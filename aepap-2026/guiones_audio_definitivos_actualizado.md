# Guiones de Audio Definitivos — AEPap 2026
# Última actualización: 2026-02-22
# Slides completadas: 0.1 a 2B.18 (audio cerrado) + 2C parcial + Bloques 3-5 parcial
# Total slides visibles: 54 (sin sub-slides)

---

## ESTADO POR SLIDE

| Slide | Deck# | Audio | Estado | Notas |
|-------|:---:|:---:|:---:|-------|
| 0.1 Portada | 1 | ✅ 1.webm | ✅ | — |
| 0.2 Conflicto de Intereses | 2 | ✅ 2.webm | ✅ | Tono neutro |
| 0.3 Objetivos | 3 | ✅ 3.webm | ✅ | Encuadre crítico |
| 0.4 Hoja de Ruta | 4 | ✅ 4.webm | ✅ | Sin tiempos |
| 1.0 Bloque 1: Ruptura | 5 | ✅ 5.webm | ✅ | — |
| 1.1 Encuesta Inicial | 6 | ✅ 6.webm | ✅ | — |
| 1.2 Pizarra/ChatGPT | 7 | ✅ | ✅ | — |
| 1.3 Bibliometría | 8 | ✅ | ✅ | — |
| 1.4 Dos historias | 9 | ✅ 9.webm | ✅ | — |
| 2A.0 Bloque 2A | 10 | ✅ 10.webm | ✅ | — |
| 2A.1 Glosario | 11 | ✅ 11.webm | ✅ | 9 conceptos |
| 2A.2 Flujo Clínico | 12 | ✅ 12.webm | ✅ | Li JMIR 2025 |
| 2A.3 ¿Qué es un LLM? | 13 | ✅ 13.webm | ✅ | ~55s |
| 2B.0 Bloque 2B | 14 | ✅ 14.webm | ✅ | ~15s |
| 2B.1 Human-in-the-Loop | 15 | ✅ 15.webm | ✅ | Goh Nat Med 2025 |
| 2B.2 Paradigma Colaboración | 16 | ✅ 16.webm | ✅ | Takita + Wang |
| 2B.3 Paradoja H+AI | 17 | ✅ 17.webm | ✅ | PI corregido: −31 a +41 |
| 2B.4 Sesgos Cognitivos | 18 | ✅ 18.webm | ✅ | Kahneman dual |
| 2B.4bis Pausa Interactiva | 19 | ✅ 19.webm | ✅ | — |
| 2B.5 Precisión Diagnóstica | 20 | ✅ 20.webm | ✅ | Takita 83 estudios |
| 2B.8 Alucinaciones | 21 | ✅ 21.webm | ✅ | Chelli + Asgari |
| 2B.8bis Brecha Exámenes vs Clínica | 22 | ✅ 22.webm | ✅ | ~55s |
| 2B.9 Riesgo: Dr. AI | 23 | ✅ 23.webm | ✅ | Bushuven + Kular |
| 2B.9bis NMA JMIR | 24 | ✅ 24.webm | ✅ | 168 estudios |
| 2B.11 RAG y sesgo | 25 | ✅ 25.webm | ✅ | NotebookLM ICC 0.27 |
| 2B.12 Diagnóstico Pediátrico | 26 | ✅ 26.webm | ✅ | Mansoor + Del Monte + Sun |
| 2B.13 Rápido ≠ fiable ≠ consistente | 27 | ✅ 27.webm | ✅ | ~68s |
| 2B.15 Sesgos Algorítmicos | 28 | ✅ 28.webm | ✅ | Hasanzadeh 2025 |
| 2B.16 Semáforo IA | 29 | ✅ 29.webm | ✅ | Detectabilidad del error |
| 2B.18 Marco Legal | 30 | ✅ 30.webm | ✅ | Fusión 2B.18+19+20. ~95s |
| 2C.0 Portada 2C | 31 | ✅ 31.webm | ✅ | Barra 5 decisiones. ~18s |
| Documentación: evidencia sólida | 32 | ✅ 32.webm | ✅ | Zhao MA + pipeline ambiental |
| Aplicaciones por dominio | 33 | ✅ 33.webm | ✅ | Tabla 4 dominios |
| Matriz de Madurez | 34 | ✅ 34.webm | ✅ | Tabla HTML nativa |
| Prompt Engineering clínico | 35 | ⚠️ sin src | ✅ txt | Audio-text existe, falta .webm |
| Técnicas de Prompting | 36 | ⚠️ sin src | ✅ txt | **RECUPERADA sesión 22/02.** CoT, RAG, Few-shot, 76pp |
| GPTs, Gems y personalización | 37 | ✅ 36.webm | ✅ | **NUEVA sesión 22/02** |
| Tu Caja de Herramientas | 38 | ✅ 37.webm | ✅ | Tabla comparativa |
| IA Agéntica | 39 | ✅ 49.webm | ✅ | **MOVIDA sesión 22/02** a Bloque 2C |
| RAG + Whitelisting | 40 | ✅ 38.webm | ✅ | Liu JAMIA. ~72s |
| Semáforo AP — Consulta | 41 | ⚠️ sin src | ✅ txt | Audio-text existe, falta .webm |
| Síntesis: 5 mensajes | 42 | ✅ 40.webm | ✅ | Bookend ✅. ~55s |
| Bloque 3: Demos en Vivo | 43 | ⚠️ sin src | ✅ txt | 2 demos: NotebookLM + Pirámide 5.0 |
| Demo: NotebookLM | 44 | ⚠️ sin src | ✅ txt | PubMed → abstracts → podcast |
| Demo: Pirámide 5.0 | 45 | ✅ 43.webm | ✅ | **NUEVA sesión 22/02.** Iframe interactivo |
| Bloque 4: Práctica RECORD | 46 | ⚠️ sin src | ✅ txt | — |
| Generador de Prompts | 47 | ⚠️ sin src | ✅ txt | — |
| Ejercicio en Grupos | 48 | ⚠️ sin src | ✅ txt | — |
| Bloque 5: Cierre | 49 | ⚠️ sin src | ✅ txt | — |
| Modelo Sándwich | 50 | ⚠️ sin src | ✅ txt | Deliberación pre-algorítmica |
| Lo que nos llevamos | 51 | ⚠️ sin src | ✅ txt | 3 takeaways |
| Pregunta Test 1 | 52 | ⚠️ sin src | ✅ txt | GEA familias |
| Pregunta Test 2 | 53 | ⚠️ sin src | ✅ txt | Verificación crítica |
| Gracias | 54 | ⚠️ sin src | ✅ txt | — |

---

## CONVENCIONES TTS

- Siglas en fonética española: "ene-pe-jota" (npj), "YAMA" (JAMA), "yámia" (JAMIA)
- Números siempre en letra: "cincuenta y dos por ciento", "ochenta y tres estudios"
- "jiuman in de lup" para human-in-the-loop
- "erre-a-ge" para RAG
- "ele-ele-eme" para LLM
- "ge-pe-te-cuatro" para GPT-4
- Evitar paréntesis, guiones largos y construcciones que TTS lea mal
- Puntos y comas para pausas naturales
- El texto en data-audio-text del HTML va en UNA SOLA LÍNEA (sin \n) porque el TTS se para
- Sin "último/más reciente/actual" (caduca). Encuadre: "según este estudio..."

---

## TEXTOS DEFINITIVOS

---

### 0.1 — Portada

Bienvenidos al seminario "La Inteligencia Artificial como
Asistente del Pediatra de Atención Primaria".

Soy médico de familia en la Comunidad de Madrid, y llevo
años combinando la práctica clínica con la formación y las
herramientas digitales.

Durante las próximas dos horas vamos a explorar juntos tres
cuestiones que considero esenciales. Primera: dónde puede
ayudarnos la IA hoy, con evidencias reales. Segunda: dónde
puede hacernos daño si la usamos mal. Y tercera, y quizá la
más práctica: cómo interactuar con ella para obtener
resultados útiles y seguros en nuestra consulta.

Será un seminario con elementos prácticos. Vais a ver demos
en vivo, vais a practicar vosotros mismos, y os vais a
llevar herramientas que podéis usar el lunes en vuestra
consulta.

Empecemos.

---

### 0.2 — Conflicto de Intereses

Antes de entrar en materia, la declaración obligada: no
tengo conflictos de interés relacionados con este contenido.
Las herramientas que mencionaré son de uso personal y no
tengo vinculación comercial con ningún proveedor de
inteligencia artificial.

Lo que sí quiero dejar claro desde el principio es el
enfoque del seminario: vamos a tratar la IA con el mismo
rigor con el que evaluamos cualquier otra intervención
clínica. Evidencia, limitaciones y aplicación práctica.

---

### 0.3 — Objetivos del Seminario

Estos que veis en pantalla son los tres objetivos del
seminario.

Quiero destacar el hilo que los conecta: no vamos a
aprender a usar ChatGPT. Eso lo podéis hacer solos en casa
en media hora. Lo que vamos a hacer es algo que requiere más
tiempo y más matices: desarrollar criterio clínico para
saber cuándo una herramienta de IA os aporta valor real,
cuándo os está dando una respuesta que parece brillante pero
es incorrecta, y cuándo directamente no deberíais estar
usándola. Y además, aprenderemos algunos trucos para
trabajar con los modelos de lenguaje que ya tenemos a
nuestro alcance y que podéis usar el lunes en vuestra
consulta.

Porque el riesgo de estas herramientas no es que sean
difíciles de usar. Al contrario, son tremendamente fáciles.
El riesgo es que esa facilidad nos haga bajar la guardia.

---

### 0.4 — Hoja de Ruta

Esta es la hoja de ruta del seminario. Cinco bloques.

Estamos ante un cambio que afecta a toda la sociedad, y la
medicina no es una excepción. Para abordarlo con rigor, la
estructura sigue una lógica deliberada: primero veremos qué
está pasando con la IA en medicina y qué dice la ciencia,
después veremos las herramientas funcionar en directo, y
finalmente pasaremos a la práctica. De la evidencia a la
demostración, y de la demostración a la aplicación.

Empecemos por el principio.

---

### 1.0 — Bloque 1: La Ruptura

Bloque uno: La Ruptura.

He llamado a este bloque así porque lo que ha ocurrido con
la inteligencia artificial en los últimos dos años no es una
evolución gradual. Es una ruptura. Y antes de analizar la
evidencia, necesitamos entender la magnitud de lo que está
pasando.

---

### 1.1 — Encuesta Inicial

Una pregunta antes de seguir, y vale la pena ser sincero
con uno mismo.

¿Has usado alguna herramienta de inteligencia artificial
esta semana para algo relacionado con tu práctica clínica?
No solo ChatGPT. Cualquier cosa: buscar información,
redactar un informe, preparar una sesión, traducir un
artículo...

Sea cual sea tu respuesta, guárdala. Al final del seminario
volveremos a esta pregunta. Y veremos si tu percepción de
lo que es "usar IA" ha cambiado después de lo que vamos a
ver hoy.

---

### 1.2 — Pizarra (ChatGPT nov 2022)

El treinta de noviembre de dos mil veintidós, OpenAI publicó
un tuit. Decía simplemente: "Presentamos ChatGPT". Un tuit
y un enlace.

En cinco días, un millón de usuarios. Para ponerlo en
perspectiva, Instagram tardó dos meses y medio en llegar a
esa cifra. En dos meses, cien millones. Ninguna aplicación
en la historia había crecido así. A día de hoy supera los
ochocientos millones de usuarios activos semanales.

Ese tuit cambió las reglas del juego. No porque la
tecnología fuese nueva, los modelos de lenguaje llevaban
años desarrollándose, sino porque por primera vez cualquier
persona podía hablar con una inteligencia artificial. Y eso
incluye a nuestros pacientes, a sus familias, y a nosotros.

---

### 1.3 — Bibliometría PubMed

Este gráfico muestra las publicaciones en PubMed sobre
inteligencia artificial y modelos de lenguaje, comparadas
con las de COVID.

En dos mil veintitrés, la IA aparecía en una de cada
doscientas quince publicaciones biomédicas. Hoy, una de
cada noventa. Y el ritmo de crecimiento ya ha superado al
que tuvo la investigación sobre COVID en su momento álgido.
Para ponerlo en cifras: en un solo año, de dos mil
veintitrés a dos mil veinticuatro, las publicaciones de IA
en salud pasaron de veintitrés mil a veintiocho mil, un
crecimiento que no muestra signos de frenarse.

Esto no es una moda pasajera. Lo que estamos viendo es un
cambio en cómo se genera, se procesa y se aplica el
conocimiento médico. No es solo que haya más artículos
sobre IA. Es que la propia forma de hacer medicina basada
en la evidencia está empezando a cambiar. Y eso nos obliga
a replantearnos algo que dábamos por sentado: cómo
decidimos en qué confiar.

---

### 1.4 — Dos historias reales

Voy a contar dos historias. Las dos son reales y
seguramente similares a las vividas por muchos de vosotros.

Primera historia. Un pediatra tiene que explicar a unos
padres que su hijo necesita una valoración por posible
trastorno del espectro autista. Es una conversación
delicada. Le pide al modelo de IA que le ayude a
estructurar los puntos clave, con un lenguaje claro y
empático, anticipando las preguntas que probablemente le
harán. Revisa la propuesta, la ajusta a la familia
concreta, y entra en la consulta mejor preparado. Diez
minutos de preparación que cambian la calidad de esa
conversación.

Segunda historia. Un modelo de IA sugiere una dosis de
medicación. El clínico no la verifica. La referencia
bibliográfica que cita el modelo no existe. Es inventada.
Una alucinación.

La diferencia entre estas dos historias no es la
herramienta. Es exactamente la misma herramienta. La
diferencia está en cómo la usamos, para qué la usamos, y
sobre todo, en si la evaluamos con el mismo rigor con el
que evaluaríamos cualquier otra tecnología sanitaria antes
de incorporarla a nuestra práctica.

Eso es exactamente lo que vamos a hacer en el siguiente
bloque: examinar qué dice la evidencia.

---

### 2A.0 — Bloque 2A: Fundamentos

Entramos en un nuevo bloque. Y empezamos por algo que puede
parecer árido pero que es más importante de lo que parece.

Desde febrero de dos mil veinticinco, la Ley Europea de
Inteligencia Artificial exige que cualquier organización
que use sistemas de IA garantice que su personal tiene un
nivel suficiente de alfabetización en IA. No es una
recomendación. Es una obligación legal. Y eso nos incluye
a nosotros como profesionales sanitarios.

Así que lo que vamos a hacer ahora no es solo un glosario
técnico. Es el mínimo necesario para entender de qué
estamos hablando cuando hablamos de IA en nuestra consulta.
Y para evaluar con criterio lo que viene después.

---

### 2A.1 — Glosario (9 conceptos)

Nueve conceptos. Los tenéis definidos en pantalla, así que
voy a centrarme en lo que no dice ahí: por qué importan en
vuestra práctica clínica.

LLM, modelo de lenguaje grande. Lo fundamental es entender
qué hace: predice la siguiente palabra más probable
basándose en patrones estadísticos. No comprende, no
razona, no verifica. Predice. Para hacernos una idea de la
escala: GPT-3 tenía ciento setenta y cinco mil millones de
parámetros. GPT-4, un billón ochocientos mil millones. Son
cifras difíciles de imaginar, pero retened la idea: más
parámetros no significa más comprensión. Significa mejor
predicción.

Prompt es la instrucción que le das al modelo. Dedicaremos
un bloque entero a esto, pero la idea clave es sencilla: si
le preguntas mal, responde mal. No es un buscador al que le
lanzas dos palabras. Es una conversación donde la precisión
de tu pregunta determina la utilidad de la respuesta.

IA generativa. Genera contenido nuevo a partir de lo que
aprendió en su entrenamiento. Y aquí hay un concepto
relacionado que conviene conocer: modelo fundacional. El
término lo acuñó Stanford en dos mil veintiuno para
describir modelos entrenados con datos masivos que luego se
adaptan a tareas muy distintas. Pensad en los cimientos de
un edificio: una misma base sobre la que se construyen
aplicaciones de texto, de imagen, de audio. El concepto es
tan importante que aparece en la legislación europea de
inteligencia artificial.

Alucinación. Probablemente el concepto más importante de
los nueve para un clínico. Cuando un LLM no tiene la
respuesta, no dice "no lo sé". Genera una respuesta
igualmente fluida, igualmente convincente, pero falsa. La
apariencia de certeza es idéntica cuando acierta y cuando
inventa. Por eso la verificación no es opcional.

RAG, generación aumentada por recuperación. Es la
diferencia entre una IA que genera desde su memoria interna
y una que primero busca en fuentes reales y luego responde.
Herramientas como Perplexity u Open Evidence funcionan así.
Es un paso crítico hacia un uso más seguro, y le
dedicaremos tiempo.

IA agéntica. Una IA que no solo responde, sino que
planifica pasos, ejecuta tareas y toma decisiones
intermedias de forma autónoma. Ya está apareciendo en
investigación clínica y cambiará la forma en que
interactuamos con estas herramientas.

Y los dos nuevos que quiero que retengáis: token y ventana
de contexto. Un token es la unidad mínima que procesa el
modelo, algo entre una letra y una palabra. Y la ventana de
contexto es su memoria de trabajo: cuánta información puede
manejar a la vez en una conversación. Cuando un modelo
"olvida" lo que le dijiste hace un rato, es porque ha
superado su ventana de contexto. Es un límite real que
afecta directamente a cómo lo usas en consulta: si quieres
que analice un informe largo, necesitas saber si cabe.

Estos nueve conceptos, junto a otros neologismos que irán
apareciendo a lo largo de esta explosión de algoritmos y
modelos, van a ser vuestro vocabulario de trabajo. Pasemos
a ver cómo encaja la IA en nuestro flujo de trabajo clínico.

---

### 2A.2 — Flujo Clínico con IA

Cuando hablamos de inteligencia artificial en la consulta,
tendemos a pensar en diagnóstico. Pero la realidad es más
amplia de lo que parece.

Esta revisión sistemática de Li y colaboradores, publicada
en dos mil veinticinco, analizó doscientos setenta estudios
y mapeó en qué tareas clínicas podía asistir GPT-4. El
resultado: setenta y uno por ciento de las subtareas del
flujo clínico, repartidas en las cinco etapas que veis en
pantalla.

Pero no todas las etapas son iguales. Donde la IA rinde
mejor es en documentación y en generación de texto:
resúmenes, informes, material educativo. Donde peor, en las
tareas que requieren integración de contexto y juicio
clínico: planificación terapéutica, decisiones con
incertidumbre, manejo de la complejidad del paciente real.
Un estudio reciente con siete modelos de razonamiento
avanzado lo confirmó: más del ochenta y cinco por ciento de
precisión en diagnósticos simples, pero el rendimiento cae
cuando hay que decidir qué exploración pedir o qué
tratamiento elegir.

Retened esta idea porque va a ser recurrente: la IA es
mejor asistente administrativo que clínico. Y eso no es una
limitación menor. Es la clave para usarla bien.

---

### 2A.3 — ¿Qué es un LLM?

Esto es lo que necesitamos entender de verdad sobre cómo
funciona un modelo de lenguaje.

Pensad en el texto predictivo de vuestro móvil. Cuando
empezáis a escribir una palabra, el teléfono os sugiere la
siguiente. Un LLM hace exactamente eso, pero a una escala
inimaginable: entrenado con billones de textos, es capaz de
generar párrafos enteros que suenan perfectos. Coherentes,
fluidos, convincentes. Pero no comprende lo que dice. No
razona. Predice.

Y aquí está la trampa para un clínico: el formato de la
respuesta es indistinguible del de un experto. Si le
preguntas por un diagnóstico diferencial, te lo presenta
con la misma estructura que usaría un adjunto experimentado.
Pero detrás no hay juicio clínico. Hay estadística. Y eso
significa que puede afirmar algo completamente falso con la
misma seguridad con que afirma algo correcto. No tiene
mecanismo interno para distinguir verdad de ficción.

Esa es la razón de fondo de todo lo que veremos a partir de
ahora en el bloque de evidencia: resultados brillantes en
unos contextos, fallos graves en otros. Y siempre con la
misma apariencia de certeza.

---

### 2B.0 — Bloque 2B: Evidencia Científica

Entramos en el bloque más denso del seminario. Y el más
necesario.

Sobre IA en medicina hay mucho ruido y mucho miedo. Las dos
posturas se basan más en titulares que en datos. Lo que
vamos a hacer ahora es lo contrario: revisar la evidencia
reciente. Veréis resultados que impresionan y fallos que
asustan. A veces en el mismo artículo.

---

### 2B.1 — Human-in-the-Loop

El modelo jiuman in de lup define cómo debe funcionar la
colaboración. La IA propone, redacta, sintetiza. El pediatra
verifica, decide, empatiza. Son roles complementarios, no
intercambiables.

En el ensayo clínico aleatorizado de Goh y colaboradores,
publicado en Nature Medicine en dos mil veinticinco, los
médicos con acceso a GPT-4 mejoraron un seis y medio por
ciento en manejo clínico. Pero tardaron dos minutos más por
caso. En una agenda de treinta pacientes, eso son sesenta
minutos adicionales.

El mensaje no es "úsalo siempre." Es "úsalo donde el
beneficio justifique el coste." En documentación, casi
siempre. En diagnóstico de rutina, rara vez. Esa decisión
estratégica es exactamente lo que veremos con el Modelo
Sándwich al final.

---

### 2B.2 — Paradigma Colaboración

El paradigma de colaboración se resume así: por separado,
médico e inteligencia artificial tienen capacidades
complementarias pero asimétricas.

El médico aporta juicio clínico, empatía y contexto del
paciente y su familia. La IA aporta velocidad y
procesamiento masivo, pero con una precisión diagnóstica
global del cincuenta y dos por ciento según el metaanálisis
de Takita con ochenta y tres estudios, publicado en
ene-pe-jota Digital Medicine. Traducido: acierta uno de
cada dos diagnósticos. Es el nivel de un médico no
especialista.

Ahora bien, juntos la cosa cambia. El metaanálisis de Wang,
también en ene-pe-jota Digital Medicine, muestra una mejora
de casi cinco puntos porcentuales en manejo clínico cuando
el médico trabaja con IA. Pero, y esto es clave, la
combinación solo funciona cuando el humano mantiene su
criterio activo. No cuando se limita a dar "ok" a lo que la
máquina propone. Esa diferencia entre supervisar y
cuestionar es exactamente el tema de la siguiente
diapositiva.

---

### 2B.3 — Paradoja H+AI

Atención a la paradoja más contraintuitiva de toda la
sesión. Wang y colaboradores, en ene-pe-jota Digital
Medicine dos mil veintiséis, muestran que humano más IA no
siempre supera a la IA sola.

El riesgo relativo es de uno coma cincuenta y nueve, pero
no significativo. Los errores persisten entre el veintiséis
y el treinta y seis por ciento incluso con supervisión. Y
el intervalo de predicción va de menos treinta y uno a más
cuarenta y un puntos, una incertidumbre que hace imposible
predecir el resultado caso por caso.

¿Por qué ocurre esto? Porque supervisar no es lo mismo que
cuestionar. Cuando leemos una respuesta bien redactada,
nuestro cerebro tiende a aceptarla. Es el mismo sesgo de
anclaje que conocemos en clínica, pero amplificado por la
elocuencia de la máquina.

Ahora bien, hay una excepción: en el ensayo clínico de Goh,
los médicos con acceso estructurado a GPT-4 mejoraron un
seis y medio por ciento en manejo clínico. La diferencia no
fue el modelo, sino el diseño: integración activa, no
supervisión pasiva. Y atención al deskilling: si un
residente delega sistemáticamente los diagnósticos
diferenciales a ChatGPT durante cuatro años, ¿qué ocurre
el día que no tiene cobertura? Esa pérdida de competencia
es acumulativa y silenciosa.

---

### 2B.4 — Sesgos Cognitivos

En la interacción humano-IA operan dos sistemas cognitivos,
los mismos que describió Kahneman. El sistema rápido acepta
o rechaza con poca fricción. El sistema analítico supervisa,
corrige y justifica. El problema es que la IA está diseñada
para reducir fricción. Y eso activa sistemáticamente el
sistema rápido.

Tres sesgos explican buena parte de los fallos que hemos
visto en la slide anterior. Sesgo de automatización: aceptar
sin crítica porque la respuesta suena bien. Anclaje
algorítmico: la primera sugerencia del modelo fija tu
razonamiento, y a partir de ahí solo buscas confirmación. Y
delegación progresiva: ceder autonomía poco a poco, sin
darte cuenta, hasta que un día no sabes hacerlo sin la
herramienta.

La regla práctica es la que veis en pantalla, y merece la
pena retenerla: antes de mirar la salida del modelo, formula
tu hipótesis. Después, compara, justifica y documenta.
Porque la IA reduce fricción cognitiva, y eso es útil. Pero
la seguridad clínica exige recuperar fricción en los puntos
críticos.

---

### 2B.4bis — Pausa: ¿Experiencias?

(Nota: la slide mantiene "levantad la mano" para uso
presencial. Este audio es la versión offline alternativa.)

Antes de seguir con más evidencia, una pausa. Piensa un
momento: ¿has usado alguna vez una herramienta de IA en tu
consulta? Y si lo has hecho, ¿verificaste la respuesta, o
la aceptaste porque sonaba bien?

No es una pregunta retórica. Después de lo que acabamos de
ver sobre sesgos de automatización y anclaje algorítmico,
merece la pena ser honesto con uno mismo. Porque la mayoría
de nosotros hemos hecho exactamente eso alguna vez: confiar
sin verificar. Y precisamente por eso necesitamos lo que
viene ahora: evidencia específica para calibrar dónde
podemos confiar y dónde no.

---

### 2B.5 — Precisión Diagnóstica (deck #20)

Hablemos de números concretos. El metaanálisis más amplio
publicado hasta ahora, Takita y colaboradores en ene-pe-jota
Digital Medicine, ochenta y tres estudios, muestra que la
precisión diagnóstica global de la IA es del cincuenta y dos
por ciento. Uno de cada dos. Eso es el nivel de un médico
no especialista, y queda quince coma ocho puntos por debajo
del especialista.

Ahora bien, esa cifra global esconde una asimetría
importante. En tareas acotadas, cribado de artículos,
extracción de datos estructurados, codificación, el
rendimiento es alto. Pero en diagnóstico complejo, donde
hay que integrar historia, exploración y contexto familiar,
la IA no llega. De hecho, un metaanálisis en red con ciento
sesenta y ocho estudios confirma que en el diagnóstico más
probable, el top uno, el humano sigue ganando.

Pensadlo así: si un padre os pregunta "¿puedo fiarme del
diagnóstico que me ha dado ChatGPT?", la respuesta honesta
es: "acertará la mitad de las veces, y no sabrás cuál
mitad." Esa incertidumbre es exactamente lo que justifica
que la IA sea copiloto, nunca piloto.

---

### 2B.8 — Alucinaciones (deck #21)

Las alucinaciones son el riesgo más crítico. Pero
necesitamos distinguir dos tipos, porque el peligro no está
donde parece.

El primero es el que más titulares genera: las citas
bibliográficas inventadas. Bard fabricaba nueve de cada
diez. GPT-4 mejora, pero aún inventa casi una de cada tres.
Y no inventa mal: inventa autores que existen, revistas que
existen, DOIs que parecen reales. Si no lo compruebas en
PubMed, no lo distingues. Regla absoluta: nunca usar una
cita de IA sin verificar el DOI en la fuente primaria.

El segundo tipo es más sutil y más relevante para nuestra
consulta. A la derecha tenéis los datos de Asgari y
colaboradores, publicado en ene-pe-jota Digital Medicine. Lo
que hicieron fue poner un escriba de IA a generar notas
clínicas a partir de transcripciones de consultas de
atención primaria. Cuatrocientas cincuenta notas, casi trece
mil frases, revisadas una por una por cincuenta médicos
marcando cada error. El resultado: las alucinaciones puras
bajan al uno y medio por ciento. Menos que el error humano
tomando notas a mano. Pero las omisiones, lo que el modelo
simplemente no incluye, llegan al tres y medio por ciento.
Y eso es lo que os tiene que preocupar más, porque una
omisión en la historia de un niño puede cambiar todo el
razonamiento diagnóstico.

La IA miente poco. Pero omite más de lo que miente. Y ni lo
uno ni lo otro se ve a simple vista. Por eso verificar no es
un consejo. Es la regla de oro.

---

### 2B.8bis — Brecha Exámenes vs Clínica (deck #22)

Los modelos de lenguaje obtienen entre ochenta y cuatro y
noventa por ciento en exámenes tipo USMLE. Impresionante.
Pero en práctica clínica real, la precisión global cae al
cincuenta y dos por ciento según el metaanálisis de Takita
con ochenta y tres estudios.

¿Por qué esa caída? Porque un examen tiene una respuesta
correcta entre cinco opciones. La consulta real tiene ruido:
un niño que llora y no se deja explorar, unos padres que
olvidan mencionar que el abuelo tiene celiaquía, una
otoscopia con cerumen que no deja ver. Nada de eso aparece
en los datos de entrenamiento.

Y el dato que más debería preocuparnos: solo el nueve coma
cuatro por ciento de los estudios publicados sobre IA
clínica evalúa errores y daño potencial al paciente. Más
del noventa por ciento de la investigación mide si la IA
acierta, pero casi nadie mide cuándo hace daño.

Si no medimos el daño, no podemos prevenirlo. Y eso nos
lleva directamente a la siguiente pregunta: qué ocurre
cuando son las familias las que consultan a la IA.

---

### 2B.9 — Riesgo: Dr. AI (deck #23)

Esto me preocupa especialmente como clínico. No por
nosotros, sino por lo que hacen las familias cuando no
estamos delante.

Los datos que veis hablan por sí solos. En un estudio con
veintidós escenarios pediátricos de emergencia, ChatGPT y
GPT-4 dieron instrucciones de primeros auxilios correctas
en menos de la mitad de los casos. Y el consejo de llamar
a emergencias fue correcto en solo uno de cada dos
escenarios. En un segundo estudio centrado exclusivamente
en reanimación cardiopulmonar pediátrica, menos del diez
por ciento de las respuestas cumplía las guías de la
American Heart Association.

Pensad en la escena: una madre a las tres de la madrugada,
el niño atragantado, el móvil en la mano. La respuesta
puede sonar impecable. Y puede estar equivocada en lo que
importa: la posición, la fuerza, la secuencia.

Nuestra responsabilidad no es solo usar bien la IA. Es
educar activamente a las familias sobre sus limitaciones.
Ante una emergencia, llama al ciento doce. La IA no salva
vidas. El ciento doce sí.

---

### 2B.9bis — NMA JMIR: Ningún modelo gana en todo (deck #24)

El meta-análisis en red más amplio publicado en JMIR hasta
ahora reúne ciento sesenta y ocho estudios y casi treinta y
seis mil preguntas clínicas. La pregunta central: ¿qué
modelo es mejor para qué?

La respuesta es que ninguno gana en todo. Para preguntas
objetivas, ChatGPT-cuatro-o. Para preguntas abiertas,
ChatGPT-cuatro. Para triaje y clasificación, Gemini.
Distintas tareas, distintos ganadores.

Pero en diagnóstico top-uno, donde hay que acertar a la
primera como hacemos cada día en consulta, los expertos
humanos siguen en cabeza. Un SUCRA de cero con noventa es
la probabilidad más alta de ocupar el primer puesto en el
ranking. Ningún modelo lo supera en esa tarea.

La implicación práctica es directa: no existe un modelo
universal. Para buscar evidencia, uno. Para redactar, otro.
Para clasificar, otro. Y para lo más crítico, la primera
sospecha diagnóstica, vuestro criterio sigue siendo el más
difícil de superar.

---

### 2B.11 — RAG y sesgo (deck #25)

¿Y si usamos RAG para evaluar sesgo? La lógica es
impecable: si el modelo trabaja con fuentes verificadas,
¿no debería juzgar mejor la calidad de un estudio?

Los datos dicen que no. Cuando se evaluó NotebookLM para
valorar riesgo de sesgo en estudios clínicos, la
concordancia con revisores humanos fue muy baja. El
coeficiente de correlación intraclase fue de cero coma
veintisiete, que en metodología se clasifica directamente
como acuerdo pobre. Y el porcentaje de coincidencia directa
con los revisores osciló entre el catorce y el veintinueve
por ciento, según la dimensión evaluada. Menos de uno de
cada tres juicios coincidía con el experto.

¿Por qué falla incluso con RAG? Porque evaluar sesgo no es
procesar texto, es inferir lo que el artículo no dice.
¿Por qué no reportan el seguimiento completo? ¿Por qué el
grupo control tiene una distribución de edad distinta? Ese
razonamiento metacrítico es experiencia metodológica, no
lingüística. Ninguna fuente verificada lo puede suplir.

Recordad este dato cuando lleguemos al semáforo: evaluar
calidad de evidencia es zona roja, con RAG o sin él.

---

### 2B.12 — Diagnóstico Pediátrico (deck #26)

Zoom al diagnóstico pediátrico. Tres estudios, dos historias
muy distintas.

Primera historia: donde la IA se acerca. Mansoor y
colaboradores entrenaron un modelo GPT-3 específicamente con
quinientos encuentros pediátricos de centros rurales en
Luisiana. Resultado: ochenta y siete coma tres por ciento de
precisión, frente al noventa y uno coma tres del pediatra.
Diferencia no significativa. Ojo: no es un chatbot genérico.
Es un modelo ajustado con datos locales. Y en urgencias
pediátricas, Del Monte evaluó ChatGPT-cuatro-o con ochenta
casos reales en Turín y obtuvo una puntuación superior a la
de los pediatras de urgencias. En tareas acotadas, con datos
estructurados, la IA compite.

Segunda historia: donde falla. Sun y colaboradores evaluaron
tres modelos con casi diez mil pacientes psiquiátricos en
seis centros de China. Hallazgo clave: la precisión de
GPT-cuatro fue del sesenta y uno por ciento en adolescentes
y del setenta y nueve por ciento en mayores de sesenta.
Dieciocho puntos de diferencia. Y en trastornos
conductuales de la infancia, la categoría que más os afecta,
solo el cuarenta y cinco por ciento de acierto. Menos de la
mitad.

El patrón es claro y merece la pena retenerlo: cuanto más
estructurada la tarea, mejor rinde la IA. Cuanto más juicio
clínico requiere, y cuanto más joven es el paciente, más
lejos queda del especialista. Y atención, porque incluso
donde se acerca, la velocidad no garantiza la fiabilidad.
Eso es lo que veremos ahora.

---

### 2B.13 — Rápido ≠ fiable ≠ consistente (deck #27)

Tres estudios recientes, tres lecciones, para entender por
qué velocidad, fiabilidad y consenso son cosas distintas. Y
por qué confundirlas es el error más frecuente cuando
evaluamos IA.

Velocidad. En escoliosis adolescente, cuatro modelos de
lenguaje clasificaron radiografías cien veces más rápido que
los cirujanos: segundos frente a once o doce minutos. Pero
cuando se repitió exactamente la misma prueba una semana
después, con las mismas imágenes y las mismas instrucciones,
los modelos dieron resultados distintos. Kappa prácticamente
cero. Es decir: el mismo paciente puede recibir una
clasificación diferente según el día. La velocidad no compró
reproducibilidad.

Fiabilidad. En otorrinolaringología pediátrica,
ChatGPT-cuatro alcanzó kappa de uno en adherencia a guías
clínicas de tubos de timpanostomía. Perfección. Pero cuando
se evaluó la misma categoría de herramienta para generar
citas bibliográficas en radiología, la tasa de referencias
inventadas osciló entre el tres y el sesenta y uno por
ciento, según el modelo. Veinte veces más error de un modelo
a otro. Perfecto donde la respuesta está codificada en una
guía. Impredecible donde tiene que construir la respuesta.

Consenso. Los cirujanos expertos coinciden el noventa y dos
por ciento de las veces. Los modelos de lenguaje, entre el
uno con seis y el diez con dos por ciento. Eso es acuerdo
por azar. Como lanzar una moneda.

La misma tecnología. Tres dimensiones. Tres perfiles de
fallo completamente distintos. Y la clave: en ninguna de las
tres puedes predecir cuándo va a fallar. Eso tiene
consecuencias directas en cómo leemos los sesgos que genera,
que es lo que vemos ahora.

---

### 2B.15 — Sesgos Algorítmicos (deck #28)

Los sesgos algorítmicos son un riesgo que debemos entender como clínicos, no solo como usuarios. Y los datos lo respaldan: según una revisión en ene-pe-jota Digital Medicine, la mitad de los estudios de inteligencia artificial sanitaria publicados hasta ahora presentan alto riesgo de sesgo. Solo uno de cada cinco tiene riesgo bajo. Aparecen en tres niveles, y cada uno tiene una consecuencia clínica distinta. En la entrada: los datos de entrenamiento pueden no representar a nuestra población. Un modelo entrenado mayoritariamente con datos de niños anglosajones puede infradiagnosticar dermatitis en piel oscura o malinterpretar patrones culturales de presentación del dolor. En el proceso: la selección automatizada amplifica patrones preexistentes. Si los datos históricos muestran menos derivaciones de TDAH en niñas, el modelo aprenderá que el TDAH es cosa de niños y perpetuará el infradiagnóstico. Y en la salida: las alucinaciones se presentan con alta confianza, sin señales de alerta. No hay un indicador luminoso que diga esto me lo estoy inventando. La validación en poblaciones infrarrepresentadas no es un ideal, es una necesidad clínica. Lo que funciona en un hospital universitario de Boston no necesariamente aplica en un centro de salud de Alcorcón. Y eso nos lleva a una pregunta práctica: qué tareas podemos delegar con seguridad y cuáles no. Eso es exactamente lo que clasifica el semáforo que viene ahora.

---

### 2B.16 — Semáforo IA (deck #29)

Os presento un semáforo de seguridad basado en toda la evidencia que hemos revisado. Y quiero que entendáis no solo los colores, sino por qué cada tarea está donde está. Luz verde: automatizar con supervisión tareas como cribado de artículos, extracción de datos tabulares y generación de resúmenes estructurados. ¿Por qué verde? Porque son tareas donde el error es detectable y reversible. Si el modelo extrae mal un dato de una tabla, lo ves al revisar. De hecho, en revisiones sistemáticas asistidas por IA, el cribado alcanza una efe-uno de cero coma noventa y dos a cero coma noventa y ocho. Luz amarilla: borradores de informes, síntesis de historiales y apoyo al diagnóstico diferencial. ¿Por qué amarillo? Porque el error puede pasar desapercibido si no cuestionas activamente. Recordad la paradoja de Wang: supervisar no es lo mismo que cuestionar. Y las omisiones, lo que el modelo simplemente no incluye, llegan al tres y medio por ciento. Son invisibles si no buscas lo que falta. Luz roja: diagnóstico primario de imagen, dosificación sin verificación, decisiones clínicas irreversibles. ¿Por qué rojo? Porque el error tiene consecuencias irreversibles para el paciente y no siempre es detectable a tiempo. Ya hemos visto que la reproducibilidad en clasificación es kappa prácticamente cero. Fijaos en que el criterio no es la IA lo hace bien o mal. El criterio es: si se equivoca, ¿puedo detectarlo antes de que cause daño? Esa es la pregunta que determina el color.

---

### 2B.18 — Marco Legal: Lo que necesitas saber (deck #30)

Antes de pasar a las aplicaciones prácticas, el marco legal. Voy a ser directo porque esto es más sencillo de lo que parece.

La norma que manda hoy es el RGPD. El Reglamento Europeo de Inteligencia Artificial clasifica la IA diagnóstica como alto riesgo, pero su calendario está en transición. Lo que sí está en vigor desde febrero de dos mil veinticinco es la obligación de alfabetización en IA. Es decir: formarse no es opcional.

Y la Asociación Médica Mundial, en Porto, octubre de dos mil veinticinco, acuñó un principio que lo resume: PITL, Physician In The Loop. El médico retiene la autoridad final sobre cualquier output de IA que afecte al cuidado clínico. Eso tiene dos caras: si sigues una recomendación errónea sin verificarla, la responsabilidad es tuya. Pero no usar herramientas que forman parte del estándar de cuidado también puede ser cuestionable.

Lo práctico. A la izquierda tenéis tres niveles según qué datos introduces. Verde: sin datos del paciente. Hojas para familias, sesiones clínicas, traducciones. Aquí vale cualquier IA comercial. Revisad siempre el output.

Amarillo: datos clínicos desidentificados. Diagnóstico diferencial, consulta sobre manejo. Lo clave aquí es cómo desidentificas. Os propongo seis pasos que se hacen en medio minuto: nombres fuera, edad por rango, fechas relativas en vez de absolutas, sin geografía, y en enfermedades raras, describir el mecanismo fisiopatológico, nunca nombrar la enfermedad, porque el propio diagnóstico puede identificar al paciente. Además, usad la configuración de privacidad de la herramienta: en ChatGPT se llama Chat Temporal, en Claude es la conversación de incógnito, en Gemini se desactiva la actividad de Gemini Apps. Y en todas podéis desactivar por separado el uso de vuestros datos para entrenar el modelo.

Rojo: datos identificables o imágenes clínicas. Aquí, solo herramientas institucionales integradas en la historia clínica electrónica.

A la derecha, dos cosas. Primero, dónde van vuestros datos. No es lo mismo. Local o europeo, máximo control. Proveedores americanos, con un acuerdo de adecuación vigente pero apelado ante el Tribunal de Justicia. Proveedores chinos como DeepSeek, esencialmente incompatibles con el RGPD para datos de salud. Italia los prohibió en enero de dos mil veinticinco.

Y segundo, un checklist de seis preguntas antes de usar cualquier IA en un contexto clínico. Lo tenéis en pantalla.

El mensaje de cierre es el que veis abajo: usar IA es legal y puede llegar a ser exigible, pero desidentifica, documenta, y mantén el juicio crítico. La lex artis no cambia. Cambia la herramienta.

---

### 2C.0 — Portada Bloque 2C (deck #31)

Hemos visto la evidencia. La hemos debatido, la hemos medido, la hemos puesto en perspectiva. Ahora toca lo concreto. En los próximos veinticinco minutos vamos a tomar cinco decisiones prácticas: dónde empezar, cómo preguntar, qué herramienta elegir, cómo verificar, y qué podemos delegar con seguridad. De la evidencia a vuestra consulta del lunes.

---

### #32 — Documentación: evidencia sólida

Empiezo con la aplicación que tiene la evidencia más sólida, y probablemente la que más os importa a nivel personal: la reducción de carga documental. Un meta-análisis de veintitrés estudios, catorce incluidos en el análisis cuantitativo, muestra que la IA reduce significativamente el tiempo de documentación, con un tamaño de efecto moderado: de-eme-de de menos cero con setenta y uno. Y la calidad de las notas generadas es comparable a la de los clínicos humanos.

---

### #33 — Aplicaciones por dominio (~80s)

La IA no solo documenta. En esta tabla tenéis cuatro dominios donde ya hay evidencia cuantitativa, y quiero que os fijéis en la columna de la derecha.

En neonatología, la detección de retinopatía del prematuro alcanza una efe-uno de cero con ochenta y nueve, y la predicción de hipoxemia perioperatoria un área bajo la curva de cero con ochenta y cinco. Son tareas de cribado con datos estructurados, donde la IA rinde bien.

En diagnóstico precoz, un sistema multi-agente para espondiloartritis consiguió sensibilidad de cero con noventa y cuatro en atención primaria. Interesante, pero es un estudio único en una patología concreta.

En salud mental, la detección de mensajes con riesgo suicida alcanza el noventa y nueve por ciento. Pero cuando hay que distinguir frustración de riesgo real, baja al ochenta y nueve. Esa diferencia entre detectar palabras y entender intención es exactamente lo que separa el cribado del juicio clínico.

Y en educación médica, los modelos generan preguntas de examen con calidad comparable a las de profesores. Pero fijaos: noventa y cuatro con cinco por ciento en preguntas tipo test, y variabilidad extrema en casos clínicos reales. Mismo patrón de siempre: tarea acotada, buen rendimiento. Juicio contextual, cae.

El patrón es claro: cuanto más estructurada la tarea y más lejos del juicio clínico, mejor rinde. Cuanto más cerca de la decisión sobre el paciente, más supervisión necesita. Eso es exactamente lo que cuantifica la siguiente slide.

---

### #34 — Matriz de Madurez (~80s)

Esta tabla es el resumen ejecutivo de todo lo que hemos visto. Cinco dominios, cinco niveles de madurez, y un patrón que a estas alturas ya debería sonaros.

En verde, lo que está listo para usar con supervisión: documentación clínica y educación. Zhao demostró que los resúmenes generados por IA tienen calidad comparable a los humanos, y Baskan que las preguntas de examen alcanzan el noventa y cuatro con cinco por ciento de precisión. Aquí la IA ya aporta valor neto.

En amarillo, lo que promete pero hay que verificar cada output. En diagnóstico de enfermedades raras pediátricas, Ilić y Sarajlija evaluaron dos modelos con cuarenta y cinco niños con displasias esqueléticas confirmadas genéticamente: los modelos alcanzaron el sesenta y dos al sesenta y cuatro por ciento de acierto en top tres, frente al ochenta y dos de los expertos. Útil como apoyo, no como sustituto. Y en investigación, la síntesis automatizada de literatura alcanza efe-uno de cero con noventa y dos a cero con noventa y ocho en cribado de artículos. Excelente para la fase mecánica, pero no para la evaluación crítica.

En rojo, salud mental. La detección de riesgo suicida en mensajes alcanza el noventa y nueve por ciento, pero distinguir frustración de riesgo real baja al ochenta y nueve. Solo investigación, nada clínico todavía.

El patrón se repite: cuanto más cerca de la decisión sobre el paciente, más lejos del uso autónomo. Ahora vamos a ver cómo hablarle a estas herramientas para sacarles el máximo partido.

---

### #35 — Prompt Engineering clínico (~95s)

La diferencia entre una respuesta clínica útil y una peligrosa no está solo en el modelo de IA. Puede estar también en cómo le preguntas. Y aquí hay una idea clave: la IA generativa no es intrínsecamente determinista; no funciona como una calculadora clínica que, con la misma entrada, siempre da exactamente la misma salida. Es un sistema probabilístico que funciona generando texto paso a paso, con variabilidad. Por eso, pequeños cambios en la forma del prompt ( es decir, en cómo preguntas) pueden cambiar el camino de la respuesta y condicionar mucho el resultado. Sin estructura, esa variabilidad se nota más: la misma pregunta, formulada de forma ligeramente distinta, puede producir recomendaciones distintas. En medicina, eso es inaceptable.

Para darle solidez, os proponemos RECORD: un acrónimo mnemotécnico de seis pasos que estructura cualquier consulta clínica a la IA. Rol, Escenario, Contexto, Objetivo, Restricciones y Diseño de salida. La mayoría de marcos similares cubren los cuatro primeros. La diferencia de RECORD son las dos últimas letras: Restricciones —lo que la IA no debe hacer ni incluir— y Diseño —el formato exacto de la respuesta—. Esos dos elementos son los que convierten un prompt genérico en un prompt controlado. En el bloque cuatro lo practicaréis con casos pediátricos.

Callens, en una revisión narrativa publicada en Acta Clinica Belgica en dos mil veintiséis, confirma esta lógica y aporta datos relevantes. Por ejemplo: un metaanálisis de treinta y seis estudios, que midió cómo rinden los modelos de lenguaje en exámenes de licenciatura médica de diez países, encontró una precisión media del setenta y dos por ciento. Es decir, un aprobado raspado. Y aquí viene lo preocupante: según este estudio, los modelos que peor puntúan muestran, paradójicamente, más confianza en sus respuestas. La IA que más se equivoca puede parecer la más segura.

¿Se puede reducir esa inestabilidad? La Clínica Mayo muestra que sí. Arriola-Montenegro y colegas evaluaron seiscientos escenarios de dosificación de anemia en hemodiálisis, un dominio donde un error de dosis puede ser grave. Con prompts sueltos, sin estructura, la IA solo seguía el protocolo institucional en el treinta y dos por ciento de los casos. Recomendaba dosis inseguras y violaba tiempos de espera entre ajustes. Entonces codificaron las reglas del protocolo directamente en la estructura del prompt, paso a paso. Resultado: cien por cien de adherencia. Trescientos de trescientos. La misma IA, distinto prompt.

Ese es el mensaje de esta diapositiva: la estructura no elimina la incertidumbre del modelo, ni vuelve determinista un sistema que no lo es; pero sí puede convertir respuestas variables en respuestas más reproducibles, más auditables y más seguras.

---

### #36 — Técnicas de Prompting

Las 4 técnicas de prompting con mayor impacto según la evidencia son: Chain-of-Thought, que mejora del 17 al 58 por ciento pidiendo "piensa paso a paso". Few-shot, dando ejemplos antes de la consulta. RAG, conectando a fuentes verificadas para reducir alucinaciones. Y Self-Consistency, generando varias respuestas y eligiendo la más frecuente. Pero atención: cambios triviales en el formato pueden causar variaciones de hasta 76 puntos porcentuales.

---

### #37 — GPTs, Gems y personalización

Ya sabéis cómo estructurar una buena consulta a la IA. Ahora, ¿cómo evitar escribir el mismo prompt cada vez que atendéis una consulta? La respuesta son los asistentes preconfigurados. En ChatGPT se llaman GPTs, en Gemini se llaman Gems, y en Claude se llaman Projects. La idea es la misma: guardas un prompt base con el rol, el contexto y las restricciones, y cada vez que lo abres, solo tienes que añadir el caso concreto. Eso elimina el prompting repetitivo y asegura consistencia entre consultas. Si el lunes usáis una estructura RECORD para un caso pediátrico y os funciona, convertidlo en un asistente. El martes solo tendréis que escribir el motivo de consulta.

---

### #38 — Tu Caja de Herramientas (~105s)

Ya tenéis un asistente configurado. Pero, ¿en qué herramienta lo habéis montado? No todas son iguales. Y la diferencia importa clínicamente. Y aquí hay un error frecuente que veo en colegas que empiezan: tratar a todas las inteligencias artificiales como si fueran lo mismo. No lo son. Y la diferencia importa clínicamente.

La tabla que tenéis en pantalla ordena las herramientas por nivel de confianza. Dejadme que la complemente con lo que no está escrito ahí.

En la fila de arriba, erre-a-ge clínico. Estas herramientas, Open Evidence, Perplexity, NotebookLM, tienen una arquitectura que impide que inventen. Primero buscan en fuentes verificadas, y solo después generan texto basándose exclusivamente en lo que han encontrado. No pueden fabricar un estudio que no existe.

En el medio, herramientas de investigación académica. Consensus, por ejemplo, busca en más de doscientos millones de artículos científicos reales y solo aplica inteligencia artificial después de la búsqueda. Elicit hace algo parecido: extrae datos de papers con una precisión validada del noventa y nueve coma cuatro por ciento. Y Scite os dice si un estudio ha sido apoyado o refutado por la literatura posterior. Ninguna de estas tres genera texto clínico: analizan lo que ya existe.

Y abajo, los modelos fundacionales: ChatGPT, Claude, Gemini, DeepSeek. Ojo, no son inútiles ni mucho menos. Todos pueden buscar en internet y citar fuentes. Gemini integra Google Search, ChatGPT tiene navegación web. Pero, y esto es clave, también pueden generar sin buscar. Y el usuario no siempre sabe en qué modo está. Por eso la confianza es variable y la supervisión no es opcional.

Ahora bien, un matiz importante que tenéis señalado en la slide. Que una herramienta cite su fuente no significa que esa fuente tenga peso metodológico. Una inteligencia artificial puede citar un artículo real de Nature y aun así producir una síntesis que no equivale a una revisión por pares. Trazabilidad no es lo mismo que evidencia preevaluada. Ese concepto lo vamos a desarrollar en la Pirámide cinco punto cero.

Un dato para poner esto en perspectiva. Según la revisión de Shool, que analizó setecientos sesenta y un estudios de cinco bases de datos, el noventa y tres por ciento evalúa modelos generalistas. De pediatría, solo diez estudios. De medicina de familia, dos. Sabemos mucho de ge-pe-te-cuatro en exámenes tipo test, y muy poco de cómo funcionan estas herramientas en nuestra consulta real.

Pero el panorama se mueve. En el panel inferior veis tres conceptos. Propietarios son modelos como ge-pe-te-cuatro o Gemini, que pertenecen a empresas y funcionan en sus servidores: tus datos salen de tu hospital. Open-source significa código público que puedes instalar en tu propio servidor: tus datos no salen. DeepSeek es open-source, y según el estudio de Sandmann, en ciento veinticinco casos clínicos rinde igual que los propietarios. Y especializados, como MedFound con ciento setenta y seis mil millones de parámetros, superan a los generalistas en ocho especialidades, pero casi nadie los evalúa todavía.

En la siguiente diapositiva vamos a ordenar todo esto visualmente con la Pirámide de Evidencia cinco punto cero adaptada a herramientas de inteligencia artificial.

---

### #39 — IA Agéntica: Impacto y Control Clínico

La IA puede amplificar el criterio clínico, pero también erosionar el razonamiento cuando delegamos sin control. En nivel asistencial, reduce carga documental; en nivel de recomendación, mejora síntesis diagnóstica; en nivel autónomo, puede actuar sin supervisión, y ahí está el riesgo. Tres niveles: asistencial, recomendación, autónomo. Solo el primero está listo para la consulta de hoy.

---

### #40 — RAG + Whitelisting (~72s)

Erre-a-ge es la diferencia entre una i-a que inventa y una que busca antes de responder. El concepto es simple: en lugar de generar desde su entrenamiento, el modelo primero busca en fuentes que tú le proporcionas y genera solo a partir de lo que ha leído. El metaanálisis de Liu en yámia, veinte estudios, muestra mejora significativa: OR de uno con treinta y cinco comparado con ele-ele-emes base.

Pero no todos los erre-a-ge son iguales. En el extremo estricto tenéis NotebookLM: solo responde desde los documentos que le subes. En el otro, los modelos con búsqueda web como Perplexity o ChatGPT, que mezclan recuperación con su propio entrenamiento. Y luego está el erre-a-ge sobre historia clínica electrónica, que es lo que usan los sistemas de documentación ambiental como los que vimos antes. Cuanto más estricto, más seguro.

Y hay un segundo paso igual de importante: el whitelisting. Restringir las fuentes a guías profesionales sube la precisión del sesenta al setenta y ocho por ciento, dieciocho puntos. Incluir una sola fuente no profesional reduce las odds de acierto a la mitad.

La regla para vuestra consulta es binaria: si la herramienta no cita la fuente primaria verificable, no la uses en clínica.

---

### #41 — Semáforo AP: Consulta (~68s)

En el bloque de evidencia vimos un semáforo para investigación. El criterio allí era: ¿puedo detectar el error antes de que cause daño? Este semáforo es distinto. Es para vuestra consulta del lunes, y el criterio es otro: ¿qué nivel de supervisión necesita esta tarea?

Luz verde: usar ahora. Administración, documentación clínica y borradores de información para familias. ¿Por qué verde? Porque son tareas donde el riesgo clínico directo es bajo, el ahorro de tiempo es alto, y la evidencia es la más sólida que hemos visto. Si el modelo redacta un informe de rutina o un borrador educativo, lo revisas en segundos y lo corriges si hace falta.

Luz amarilla: usar con cuidado. El modelo como segundo cerebro para diagnóstico diferencial complejo, y como apoyo en enfermedades raras. ¿Por qué amarillo? Porque aquí la i-a puede aportar valor real ampliando tu lista diagnóstica, pero solo si validas siempre con erre-a-ge contra fuentes profesionales. Nunca fiarse de la memoria del modelo.

Luz roja: evitar. Confianza ciega en lo que genera la i-a, y cualquier forma de sustitución del juicio clínico. No hay atajo aquí. La decisión clínica es tuya.

Fijaos en la diferencia: aquel semáforo clasificaba tareas de investigación. Este clasifica tareas de consulta. Complementarios, no redundantes.

---

### #42 — Síntesis: 5 mensajes (~55s)

Cinco mensajes. Sin datos nuevos, sin matices adicionales. Solo lo que os lleváis de este bloque para la consulta del lunes.

Primero: empezad por la documentación. Es donde la evidencia es más sólida y el riesgo más bajo. Automatizad la burocracia, revisad el resultado, y recuperad tiempo para el paciente.

Segundo: elegid herramienta por nivel de confianza, no por moda. La Pirámide cinco punto cero os da el criterio. No es lo mismo una búsqueda en Consensus que un chat abierto con un modelo generalista.

Tercero: estructurad cada consulta clínica a la IA. Rol, contexto, tarea, restricciones, salida. RECORD o Callens, da igual cuál, pero siempre con estructura. La diferencia entre el treinta y dos y el cien por cien de adherencia es exactamente eso.

Cuarto: exigid fuentes verificables. Si la herramienta no usa erre-a-ge o no podéis comprobar la referencia, no lo uséis para decisiones clínicas. Sin fuente, no hay seguridad.

Y quinto: delegad lo verde, supervisad lo amarillo, prohibid lo rojo. El semáforo de vuestra consulta no es el mismo que el de investigación. El criterio aquí es: si se equivoca, ¿puedo detectarlo antes de que afecte al paciente?

Con esto cerramos el bloque de aplicaciones. Ahora, veámoslo funcionar en directo.

---

### #43 — Bloque 3: Demos en Vivo

Ahora viene la parte más práctica. Vamos a ver dos herramientas en acción: NotebookLM para documentación clínica, y la Pirámide de Evidencia cinco punto cero interactiva para elegir herramienta.

---

### #44 — Demo: NotebookLM Actualización

Vamos a ver un caso de uso real de NotebookLM. Partimos de una búsqueda avanzada en PubMed que recoge revisiones sistemáticas y metaanálisis del último mes en las patologías más frecuentes de nuestra consulta.

---

### #45 — Demo: Pirámide 5.0

Vamos a explorar juntos la Pirámide de Evidencia cinco punto cero adaptada a herramientas de inteligencia artificial. Es interactiva: podéis pulsar en cada nivel para ver qué herramientas pertenecen a cada categoría y por qué.

---

### #46 — Bloque 4: Práctica RECORD

Ahora os toca a vosotros. Vamos a practicar el método RECORD con casos reales.

---

### #47 — Generador de Prompts

Aquí tenéis un generador de prompts para llevaros. Seleccionad un escenario y copiad el prompt generado.

---

### #48 — Ejercicio en Grupos

Formad parejas o tríos. Elegid uno de estos tres casos y escribid vuestro prompt usando el método RECORD.

---

### #49 — Bloque 5: Cierre

Llegamos al cierre. Vamos a recapitular lo que hemos aprendido y a hacer el test oficial.

---

### #50 — Modelo Sándwich

El Modelo Sándwich resume la filosofía de colaboración humano-IA. Primera capa, el pan superior: el humano define la estrategia, elige la herramienta, formula la pregunta correcta. Capa central, el relleno: la IA ejecuta, procesa, genera. Segunda capa, el pan inferior: el humano verifica, decide, actúa. Deliberación pre-algorítmica arriba, verificación post-algorítmica abajo. La IA nunca decide sola.

---

### #51 — Lo que nos llevamos

Estos son los tres mensajes clave para llevaros a casa.

---

### #52 — Pregunta Test 1

Pregunta de evaluación número uno. De las siguientes opciones, cuál representa el uso más seguro de IA en consulta.

---

### #53 — Pregunta Test 2

Pregunta de evaluación número dos. ¿Cuál es el riesgo principal de la paradoja Humano más IA que vimos en el seminario?

---

### #54 — Gracias

Muchas gracias por vuestra atención. Recordad: la mejor forma de aprender es enseñando.

---

## REFERENCIAS VERIFICADAS

| Ref | Slide | Dato clave | Estado |
|-----|-------|-----------|--------|
| Takita 2025, npj Digit Med 8:175 | 2B.2, 2B.5, 2B.16 | 83 estudios, 52.1% precisión IA | ✅ |
| Wang G 2026, npj Digit Med | 2B.2, 2B.3 | MD +4.88pp (k=2), RR 1.59 NS, PI −31 a +41 | ✅ |
| Wang L 2025, JMIR 27:e64486 | 2B.5, 2B.9bis | NMA 168 arts, humano gana top-1 dx | ✅ |
| Goh 2025, Nat Med 31:1233 | 2B.1, 2B.3 | +6.5% manejo, +119s/caso, n=92 médicos | ✅ |
| Goh 2024, JAMA Netw Open 7:e2440969 | 2B.4 | LLM NO mejoró dx reasoning | ✅ |
| Li H 2025, JMIR 27:e71916 | 2A.2 | 270 estudios, 5 etapas, 71% subtareas | ✅ |
| Chelli 2024, JMIR 26:e53164 | 2B.8 | Bard 91.4%, GPT-3.5 39.6%, GPT-4 28.6% | ✅ |
| Asgari 2025, npj Digit Med 8:274 | 2B.8, 2B.16 | 1.47% alucinaciones, 3.45% omisiones | ✅ |
| Del Monte 2025, Front Digit Health | 2B.12 | Urgencias ped Turín, GPT-4o 72.5/80 | ✅ |
| MedR-Bench 2025, Nat Comms | 2A.2 | >85% dx simple, cae en planning | ✅ |
| Bushuven 2023, J Med Syst 47:123 | 2B.9, 2B.16 | Primeros auxilios correctos 45.5%, EMS 54.5% | ✅ |
| Kular & Kumar 2025, Cureus; DOI:10.7759/cureus.89234 | 2B.9 | <10% conformidad AHA RCP pediátrica | ✅ |
| Beber et al. JPOSNA 2025 | 2B.11 | ICC 0.27, 14.8-29.6% agreement NotebookLM | ✅ |
| Mansoor 2025, JMIRx Med 6:e65263 | 2B.12 | GPT-3 fine-tuned rural, 87.3% vs 91.3% | ✅ |
| Sun 2026, JMIR Med Inform 14:e77699 | 2B.12 | 9.923 pac psiq, GPT-4 61% adolesc vs 79% ≥60a | ✅ |
| Aktan 2025, Diagnostics 15:3219 | 2B.13 | 125 AIS, 4 LLM, κ 0.001-0.036 vs 0.913 | ✅ |
| Durgut 2026, Eur Arch Otorhinolaryngol 283:117-130 | 2B.13 | ChatGPT-4 100% con prompt guía | ✅ |
| Güneş 2025, Diagn Interv Radiol; DOI:10.4274/dir.2025.253101 | 2B.13 | 480 refs, Claude 3.1% fab, Gemini 60.6% fab | ✅ |
| Hasanzadeh 2025, npj Digit Med 8:154 | 2B.15 | 50% estudios IA alto RoB, 20% bajo | ✅ |
| Lee 2025, Value Health 28(11):1655-1664 | 2B.16, #34 | A4SLR: F1 0.917-0.977 cribado | ✅ |
| Zhao 2025, BMC Med Inform Decis Mak 26:29 | #32, #34 | 23 estudios, 14 pooled, SMD −0.71 | ✅ |
| Zhang 2026, BMJ Open Ophthalmol | #33 | ROP screening, F1 0.89 | ✅ |
| Baek 2026, PLoS ONE | #33 | Hipoxemia periop, AUC 0.85 | ✅ |
| Ji 2026, npj Digit Med | #33 | Multi-agente espondiloartritis, Sens 0.94 | ✅ |
| Qadir 2026, JMIR Ment Health | #33, #34 | 99% detección riesgo suicida, 89% genuino | ✅ |
| Baskan 2025, BMC Med Educ | #33, #34 | 94.5% MCQ, variabilidad en casos | ✅ |
| Ilić & Sarajlija 2025, Genes 16(7):762 | #34 | 45 niños displasias, 62-64% top-3 vs 82% exp | ✅ |
| Callens 2026, Acta Clin Belg;1-12; DOI:10.1080/17843286.2026.2613903 | #35 | 4 elementos prompt, 72% pooled licensing | ✅ |
| Arriola-Montenegro 2025, Front Artif Intell | #35 | 32%→100% adherencia, 300 escenarios | ✅ |
| Liu S 2025, JAMIA 32(4):605-615 | #40 | 20 estudios, OR=1.35, RAG vs LLM base | ✅ |
| Masanneck 2025, JMIR; DOI:10.2196/79379 | #40 | Whitelisting: 60%→78%, OR 0.50 | ✅ |
| Moulaei 2024, Int J Med Inform 188:105474 | #41 | Scoping review, 109 estudios GAI en salud | ✅ |
| Shool 2025, BMC Med Inform Decis Mak 25(1):117 | #38 | RS 761 estudios, 93% generalistas | ✅ |
| Sandmann 2025, Nat Med 31(8):2546-2549 | #38 | DeepSeek open-source comparable | ✅ |
| Liu X 2025, Nat Med 31(3):932-942 | #38 | MedFound 176B, 8 especialidades | ✅ |
| Alper BS, Haynes RB. EBHC 2016;21(4):123-5 | #38, #45 | Pirámide evidencia 5.0 | ✅ |

---

## CORRECCIONES HTML PENDIENTES

### Resueltas (sesión 2026-02-22)
- ~~2B.3: "PI: -31.65 a +41.42"~~ → "Intervalo predicción: −31 a +41" ✅
- ~~Pirámide 5.0 audio: "Acabáis de ver"~~ → "Recordemos" ✅
- ~~Barras navegación 2C: orden incorrecto~~ → Cómo preguntar antes de Qué herramienta ✅
- ~~GPTs/Gems: contenido y numeración~~ → Nueva slide #36, renumerada ✅
- ~~Tu Caja de Herramientas: audio-src~~ → aepap-2026/audios/37.webm ✅
- ~~20 slides legacy~~ → data-visibility="hidden" ✅
- ~~IA Agéntica~~ → Movida a Bloque 2C como #39 ✅
- ~~Demo RECORD~~ → Sustituida por Demo Pirámide 5.0 ✅
- ~~Bloque 3 portada~~ → 2 demos: NotebookLM + Pirámide 5.0 ✅

### Pendientes
1. Slide 2B.4: pie "JAMA Intern Med 2024" → "JAMA Netw Open 2024"
2. Slide 2A.2: pie "Appl Clin Inform 2025" → "JMIR 2025;27:e71916"
3. Slide 2B.8bis: verificar ref pie "Zhang 2026" (9.4%)
4. Slide 2B.9bis (NMA): confirmar autoría Wang L vs Gong et al.
5. Slide 2B.14: decidir si eliminar por duplicación con #27

---

## NOTAS PARA CONTINUIDAD

- Dos Goh distintos: JAMA Netw Open 2024 (dx reasoning, negativo) vs Nat Med 2025 (management +6.5%, positivo)
- Dos Liu distintos: Liu X (MedFound, #38) vs Liu S (RAG meta-análisis, #40)
- Estilo Feynman: una tesis por slide, cada dato la construye
- PRINCIPIO: no usar "último/más reciente/actual" (caduca)
- SUCRA no es tasa de acierto: es probabilidad de ranking
- "Desidentifica en 30"" es mnemónico propio, no estándar publicado
- "Anonimizar" → "desidentificar" en todo el sub-bloque regulatorio
- Slides 2B.19 y 2B.20 del mapa original absorbidas en 2B.18 consolidada
- La slide 2B.4 planta la semilla de "deliberación pre-algorítmica" → se cierra en #50 (Sándwich)
- Pan et al. Lancet Digit Health 2025 NO encontrado — sustituido por Bushuven 2023 + Kular 2025
- El dato "17% inferior en minorías étnicas" del arsenal NO tiene fuente primaria verificable
- Si Digital Omnibus se aprueba antes del congreso: actualizar subtitle AI Act timeline
- Si TJUE invalida DPF antes del congreso: mover US a rojo en jurisdicciones
- Dato Orphanet 75% reidentificación: disponible en tooltip zona roja si preguntan
- Ji et al. (#33) NO es pediatría (espondiloartritis en adultos) — se mantiene como demo concepto multi-agente
- Ilić & Sarajlija (#34): κ intermodelo = 0.95 (ChatGPT vs DeepSeek coinciden entre sí, ambos lejos del experto)
- Dato "Elicit 99.4% precisión extracción": fuente = validación VDI/VDE (no peer-reviewed)
- 20 slides ocultas con data-visibility="hidden" (no borradas), reactivables
- IA Agéntica movida a Bloque 2C como #39, entre Tu Caja y RAG (sesión 22/02)
- Sub-slide Pirámide 5.0 eliminada de 2C (duplicada con Demo #45)
- Bloque 3 reestructurado: 2 demos (NotebookLM + Pirámide 5.0), Demo Constructor hidden (sesión 22/02)
- Barra navegación 2C: Dónde empezar → Cómo preguntar → Qué herramienta → Cómo verificar → Qué delegar

---

*Documento actualizado: 2026-02-22. Vigente hasta próxima sesión.*
