1. NPJ Digit Med. 2026 Jan 28. doi: 10.1038/s41746-026-02382-2. Online ahead of 
print.

Human-large language model collaboration in clinical medicine: a systematic 
review and meta-analysis.

Wang G(#)(1), Zhang K(#)(2), Jiang J(#)(3), Wang C(1), Bi H(4), Liang H(5), Qi 
Z(5), Huang Y(6), Li Y(7)(8), Yang X(9).

Author information:
(1)Department of hemangioma and vascular malformation, Plastic Surgery Hospital, 
Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, 
China.
(2)Department of Gastroenterology, Children's Hospital of Fudan University, 
National Children's Medical Center, Minhang District, Shanghai, China.
(3)Department of Computer Science and Engineering, The Chinese University of 
Hong Kong, Sha Tin, New Territories, Hong Kong SAR, China.
(4)Department of Internal Medicine, Plastic Surgery Hospital, Chinese Academy of 
Medical Sciences and Peking Union Medical College, Beijing, China.
(5)Department of Comprehensive Plastic Surgery, Plastic Surgery Hospital, 
Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, 
China.
(6)Department of Gastroenterology, Children's Hospital of Fudan University, 
National Children's Medical Center, Minhang District, Shanghai, China. 
yhuang815@163.com.
(7)Department of Computer Science and Engineering, The Chinese University of 
Hong Kong, Sha Tin, New Territories, Hong Kong SAR, China. liyu@cse.cuhk.edu.hk.
(8)The CUHK Research Institute, Shenzhen, China. liyu@cse.cuhk.edu.hk.
(9)Department of hemangioma and vascular malformation, Plastic Surgery Hospital, 
Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, 
China. yxnan@aliyun.com.
(#)Contributed equally

Human-AI collaboration (H + AI) using large language models (LLMs) offers a 
promising approach to enhance clinical reasoning, documentation, and 
interpretation tasks. Following PRISMA 2020 (PROSPERO registration: 
CRD420251068272), we systematically compared H + AI with human-only (H) 
workflows, searching four databases through June 28, 2025. Ten peer-reviewed 
studies met eligibility criteria, with three preprints informing sensitivity 
analyses only. Diagnostic/interpretation accuracy (k = 2) showed a positive 
trend for H + AI (Risk Ratio [RR] 1.59), but was statistically imprecise and 
non-significant (95% CI 0.08 to 32.74), with 95% prediction intervals (PI) 
crossing the null. Composite diagnostic/management scores (k = 2) showed a 
statistically significant improvement (Mean Difference [MD] +4.88 percentage 
points, 95% CI + 0.65 to +9.12), yet the PI (-31.65 to 41.42) indicates high 
real-world uncertainty. Time efficiency (k = 3) showed no overall difference 
(MD + 0.4 min, 95%CI -4.18 to +4.97; I² = 70.1%). While documentation quality 
improved, but factual error rates remained high (~26-36%), undermining quality 
gains. In three-arm settings, H + AI did not universally outperform AI-only. 
Evidence remains preliminary yet highly uncertain and context-dependent. We 
recommend preregistered, pragmatic, multicenter trials embedded in real 
workflows, with harmonized core outcomes that prioritize safety/error metrics 
and interfaces that surface uncertainty and support verification.

© 2026. The Author(s).

DOI: 10.1038/s41746-026-02382-2
PMID: 41606089

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


2. J Med Internet Res. 2026 Jan 28;28:e79677. doi: 10.2196/79677.

The Development and Use of AI Chatbots for Health Behavior Change: Scoping 
Review.

Fu L(1), Burns R(1), Xie Y(1), Shen J(2), Zhe S(3), Estabrooks P(1), Bai Y(1).

Author information:
(1)Department of Health and Kinesiology, College of Health, University of Utah, 
Salt Lake City, UT, United States.
(2)Department of Internal Medicine, School of Medicine, University of Utah, Salt 
Lake City, UT, United States.
(3)Kahlert School of Computing, University of Utah, Salt Lake City, UT, United 
States.

BACKGROUND: Artificial intelligence (AI) chatbots are technologies that 
facilitate human-computer interaction through communication in a natural 
language format. By increasing cost-effectiveness, interaction, autonomy, 
personalization, and support, mobile health interventions can benefit health 
behavior change and make it more natural and intuitive.
OBJECTIVE: This study aimed to provide an up-to-date and practical overview of 
how text-based AI chatbots are designed, developed, and evaluated across 8 
health behaviors, including their roles, theoretical foundations, health 
behavior change techniques, technology development workflow, and performance 
validation framework.
METHODS: In accordance with the PRISMA-ScR (Preferred Reporting Items for 
Systematic Reviews and Meta-Analyses extension for Scoping Reviews) framework, 
relevant studies published before March 2024 were identified from 9 
bibliographic databases (ie, PubMed, CINAHL, MEDLINE, Embase, Web of Science, 
Scopus, APA PsycINFO, IEEE Xplore, and ACM Digital Library). Two stages (ie, 
title and abstract screening followed by full-text screening) were conducted to 
screen the eligibility of the papers via Covidence software. Finally, we 
extracted the data via Microsoft Excel software and used a narrative approach, 
content analysis, and evidence map to synthesize the reported results.
RESULTS: Our systematic search initially identified 10,508 publications, 43 of 
which met our inclusion criteria. AI chatbots primarily served 2 main roles: 
routine coach (27/43, 62.79%) and on-demand assistant (12/43, 27.91%), while 4 
studies (4/43, 9.30%) integrated both roles. Frameworks like cognitive 
behavioral therapy (13/24, 54.17%) and behavior change techniques, such as goal 
setting, feedback and monitoring, and social support, guided the development of 
theory-driven AI chatbots. Noncode platforms (eg, Google Dialogflow and IBM 
Watson) integrated with social messaging platforms (eg, Facebook Messenger) were 
commonly used to develop AI chatbots (23/43, 53.49%). AI chatbots have been 
evaluated across 4 domains: technical performance (17/43, 39.53%), usability 
(17/43, 39.53%), engagement (37/43, 86.05%), and health behavior change (33/43, 
76.74%). Evidence for health behavior changes remains exploratory but promising. 
Among 33 studies with 120 comparisons, 81.67% (98/120) showed positive outcomes, 
though only 35.83% (43/120) had moderate or larger effects (Hedges g or odds 
ratio or Cohen d>0.5). Most involved nonclinical (36/43, 83.72%) and adults 
(23/43, 53.49%), and a few were randomized controlled trials (14/43, 32.56%). 
Benefits were mainly seen in physical activity, smoking cessation, stress 
management, and diet, with limited evidence for other behaviors. Findings were 
inconsistent regarding the influence of long-term effects, intervention 
duration, modality, and engagement on health behavior change outcomes.
CONCLUSIONS: The exploratory synthesis provides a roadmap for developing and 
evaluating AI chatbots in health behavior change, highlighting the need for 
further research on cost, implementation outcomes, and underexplored behaviors 
such as sleep, weight management, sedentary behavior, and alcohol use.

©Lingyi Fu, Ryan Burns, Yuhuan Xie, Jincheng Shen, Shandian Zhe, Paul 
Estabrooks, Yang Bai. Originally published in the Journal of Medical Internet 
Research (https://www.jmir.org), 28.01.2026.

DOI: 10.2196/79677
PMID: 41604667 [Indexed for MEDLINE]


3. Front Digit Health. 2026 Jan 9;7:1740557. doi: 10.3389/fdgth.2025.1740557. 
eCollection 2025.

Artificial intelligence, extended reality, and emerging AI-XR integrations in 
medical education.

Tene T(1), Vique López DF(2), García Veloz MJ(3), Rojas Oviedo BS(3), 
Tene-Fernandez R(4).

Author information:
(1)Department of Chemistry, Universidad Técnica Particular de Loja, Loja, 
Ecuador.
(2)Facultad de Salud Pública, Escuela Superior Politécnica de Chimborazo 
(ESPOCH), Riobamba, Ecuador.
(3)Facultad de Ciencias, Escuela Superior Politécnica de Chimborazo (ESPOCH), 
Riobamba, Ecuador.
(4)Hospital Metropolitano de Quito, Quito, Ecuador.

INTRODUCTION: Artificial intelligence (AI) and extended reality (XR)-including 
virtual, augmented, and mixed reality-are increasingly adopted in 
health-professions education. However, the educational impact of AI, XR, and 
especially their combined use within integrated AI-XR ecosystems remains 
incompletely characterized.
OBJECTIVE: To synthesize empirical evidence on educational outcomes and 
implementation considerations for AI-, XR-, and combined AI-XR-based 
interventions in medical and health-professions education.
METHODS: Following PRISMA and PICO guidance, we searched three databases 
(Scopus, PubMed, IEEE Xplore) and screened records using predefined eligibility 
criteria targeting empirical evaluations in health-professions education. After 
deduplication (336 records removed) and two-stage screening, 13 studies 
published between 2019 and 2024 were included. Data were extracted on learner 
population, clinical domain, AI/XR modality, comparators, outcomes, and 
implementation factors, and narratively synthesized due to heterogeneity in 
designs and measures.
RESULTS: The 13 included studies involved undergraduate and postgraduate 
learners in areas such as procedural training, clinical decision-making, and 
communication skills. Only a minority explicitly integrated AI with XR within 
the same intervention; most evaluated AI-based or XR-based approaches in 
isolation. Across this mixed body of work, studies more often than not reported 
gains in at least one outcome-knowledge or skills performance, task accuracy, 
procedural time, or learner engagement-relative to conventional instruction, 
alongside generally high acceptability. Recurrent constraints included costs, 
technical reliability, usability, faculty readiness, digital literacy, and data 
privacy and ethics concerns.
CONCLUSIONS: Current evidence on AI, XR, and emerging AI-XR integrations 
suggests promising but preliminary benefits for learning and performance. The 
small number of fully integrated AI-XR interventions and the methodological 
limitations of many primary studies substantially limit the certainty and 
generalizability of these findings. Future research should use more rigorous and 
standardized designs, explicitly compare AI-only, XR-only, and AI-XR hybrid 
approaches, and be coupled with faculty development, robust technical support, 
and alignment with competency-based assessment.

© 2026 Tene, Vique López, García Veloz, Rojas Oviedo and Tene-Fernandez.

DOI: 10.3389/fdgth.2025.1740557
PMCID: PMC12827641
PMID: 41586206

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


4. PLoS One. 2026 Jan 23;21(1):e0341631. doi: 10.1371/journal.pone.0341631. 
eCollection 2026.

User experience and safety of generative AI-based mental health chatbots: 
Scoping review protocol.

Olisaeloka L(1)(2), Richardson C(1)(2), Vigo D(1)(2).

Author information:
(1)Department of Psychiatry, Faculty of Medicine, University of British 
Columbia, Vancouver, Canada.
(2)School of Population and Public Health, Faculty of Medicine, University of 
British Columbia, Vancouver, Canada.

INTRODUCTION: Mental health problems constitute a significant global health 
challenge due to their rising prevalence and substantial treatment gap. Digital 
Mental Health Interventions (DMHIs) including mental health chatbots have 
emerged as promising solutions due to their effectiveness and scalability. 
Recent advances in Generative Artificial Intelligence (GenAI) have improved the 
conversational abilities of these chatbots, further amplifying their potential. 
However, despite instances of inadvertent harm stemming from the unpredictable 
nature of GenAI, little attention has been paid to user experience and safety of 
these chatbots.
OBJECTIVE: This proposed review will explore existing research on GenAI-based 
mental health chatbots. Specifically, it aims to identify and describe current 
chatbots, focusing on user experience, safety and risk mitigation strategies.
METHODS: The review will follow the Joanna Briggs Institute (JBI) guidelines for 
conducting scoping reviews. It will also adhere to the Preferred Reporting Items 
for Systematic Reviews and Meta-Analyses Extension for Scoping Review 
(PRISMA-ScR). A systematic database search of Medline (PubMed), Scopus, 
PsycINFO, ACM Digital Library, and IEEE Xplore will be conducted. The database 
search will be complimented by research-based search engines (Google Scholar and 
Consensus). Studies focusing on the development, evaluation or implementation of 
GenAI-based mental health chatbots will be included without limitations to 
specific disorders or population groups. Two independent reviewers will perform 
screening and data extraction. The analysis will include descriptive summary and 
thematic analysis, with results presented in tabular, graphical, and narrative 
formats.
CONCLUSION: This review will provide a comprehensive overview of GenAI-based 
mental health chatbots while identifying innovative practices and knowledge gaps 
relating to user experience and safety. Findings will inform the ethical 
development, evaluation and implementation of GenAI-based mental health 
interventions.

Copyright: © 2026 Olisaeloka et al. This is an open access article distributed 
under the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0341631
PMCID: PMC12829926
PMID: 41576176 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


5. J Med Internet Res. 2026 Jan 20;28:e70754. doi: 10.2196/70754.

Quantifying Innovation in Stroke: Large Language Model Bibliometric Analysis.

Marcus A(1)(2), Lockwood-Taylor G(3), Rueckert D(1)(4), Bentley P(2).

Author information:
(1)Department of Computing, Imperial College London, London, United Kingdom.
(2)Department of Medicine, Imperial College London, London, United Kingdom.
(3)Department of Psychology and Neuroscience, King's College London, London, 
United Kingdom.
(4)Klinikum rechts der Isar, Technical University of Munich, Munich, Germany.

BACKGROUND: Thrombolysis and mechanical thrombectomy represent the most 
successful stroke innovations over the last 30 years. Quantifying innovation in 
stroke is essential for identifying productive research lines and prioritizing 
funding, but health care lacks validated methods for measuring innovation.
OBJECTIVE: This study aimed to systematically evaluate the relationship between 
stroke-related patents and publications, demonstrate the feasibility of using 
large language models (LLMs) in this process, and identify the most rapidly 
advancing innovations in stroke care by mapping them to a theoretical innovation 
life cycle.
METHODS: The Open Patent Services (European Patent Office) and PubMed databases 
were searched between 1993 and 2023 for "stroke OR cerebrovascular." In this 
bibliometric patent-publication analysis, a 13 billion-parameter Llama LLM was 
trained to identify patents related to stroke disease, as opposed to other 
references to the word "stroke," on a manually labeled subset of 5000 patents 
and assessed using 5-fold cross-validation. The LLM filtered irrelevant results, 
and the resulting patent codes were grouped into innovation clusters. For each 
cluster, annual patent and publication counts were normalized to adjust for 
global trends. Cluster-specific growth curves were plotted to analyze the rates 
and characteristics of growth. The innovation life cycle stage for each 
innovation cluster was estimated by fitting a sigmoid curve to the patent and 
publication data consistent with the diffusion of innovations theory by Rogers.
RESULTS: The cross-validated accuracy of the LLM was 99.2%, with a sensitivity 
of 96.5% and a specificity of 99.6%. An initial bibliometric search retrieved 
237,035 patents and 486,664 research publications. A manual review of a random 
sample of patents before filtering revealed that only 11.2% (56/500) were 
relevant to stroke. After LLM filtering, of the 237,035 patents, 28,225 (11.9%) 
stroke-related patents remained. These were grouped into 7 innovation clusters: 
pharmacological treatment, alternative medicine, rehabilitation devices, medical 
imaging, diagnostic testing, surgical devices, and artificial intelligence (AI) 
methods. Patent and publication counts were strongly correlated across clusters 
(Spearman rs=0.65-0.92; P<.006) except for pharmacological treatment (rs=0.09) 
and alternative medicine (rs=0.55). Pharmacological treatments were the 
top-performing cluster over the last 30 years, accounting for 49.3% 
(36,005/73,094) of all patents, but patent activity in this area has plateaued 
since the late 2000s. AI methods, rehabilitation devices, and medical imaging 
exhibited exponential rates of patent growth, with annual normalized increases 
of 39.2%, 15.9%, and 5.8% compared to 16.9%, 5.3%, and 2.2% for publications, 
respectively.
CONCLUSIONS: Applying an LLM to publicly available patent and publication data 
provides a scalable way to quantify innovation in stroke. Pharmacological 
treatment appears to have entered a saturation phase, whereas AI methods, 
rehabilitation devices, and medical imaging remain in rapid growth, highlighting 
areas of greatest traction for future research and investment.

©Adam Marcus, Georgina Lockwood-Taylor, Daniel Rueckert, Paul Bentley. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 20.01.2026.

DOI: 10.2196/70754
PMID: 41558024 [Indexed for MEDLINE]


6. J Med Internet Res. 2026 Jan 17. doi: 10.2196/81387. Online ahead of print.

A Study on the Effectiveness of Al-Assisted Patient Health Education Using Voice 
Cloning and ChatGPT: A Prospective Randomized Controlled Trial.

Sun Y(1), Xu S(1), Jin H(1), Han X(1), Jin K(1), Zhang Y(1), Ma X(1), Wei H(2), 
Ma M(3).

Author information:
(1)Department of Thoracic Surgery, The First Hospital of Lanzhou University, 
Donggang West Road 1#, Lanzhou, CN.
(2)Outpatient Department, The First Hospital of Lanzhou University, Lanzhou, CN.
(3)Gansu International Science and Technology Cooperation Base for Development 
and Application of Thoracic Surgery Key Technologies, The First Clinical Medical 
College of Lanzhou University, Department of Thoracic Surgery, The First 
Hospital of Lanzhou University, Donggang West Road 1#Email: maminjie24@sina.com, 
Lanzhou, CN.

BACKGROUND: Traditional patient education often lacks personalization and 
engagement, potentially limiting knowledge acquisition and treatment 
adherence[1]. Advances in artificial intelligence (AI), including voice cloning 
technology and large language models such as ChatGPT, offer new opportunities to 
deliver personalized, scalable, and interactive health education[2-3]. However, 
evidence regarding the comparative effectiveness of different AI-based voice 
cloning strategies and the reliability of automated AI evaluation tools remains 
limited[4-5].
OBJECTIVE: To evaluate the effectiveness of AI-assisted patient education 
integrating voice cloning and ChatGPT, to compare physician voice cloning with 
patient self-voice cloning, and to assess the reliability of ChatGPT as an 
automated evaluation tool for education outcomes.
METHODS: A prospective, three-arm, parallel-group randomized controlled trial.A 
total of 180 hospitalized patients requiring standardized health education were 
recruited from a tertiary hospital. Inclusion criteria were: age ≥18 years, 
clear diagnosis requiring health education, clear consciousness, and voluntary 
participation with informed consent. Exclusion criteria were: severe hearing 
impairment, severe cognitive impairment, expected hospitalization <3 days, or 
prior participation in similar studies.Participants were randomly assigned 
(1:1:1) to receive (1) traditional education (control), (2) AI-assisted 
education using physician voice cloning, or (3) AI-assisted education using 
patient self-voice cloning. All groups received identical educational content 
with equal duration.The primary outcome was education content compliance, 
evaluated using ChatGPT-4 with validated prompts and verified by expert review. 
Secondary outcomes included knowledge retention, education satisfaction, 
treatment adherence, quality of life (SF-36), and psychological status (Hospital 
Anxiety and Depression Scale).Participants were randomly allocated using a 
computer-generated random sequence. Due to the nature of the intervention, 
participants were not blinded; outcome assessors and data analysts were blinded 
to group allocation.
RESULTS: Of 180 randomized participants, 174 (96.7%) completed the trial. Both 
AI-assisted groups demonstrated significantly higher education content 
compliance immediately after education compared with the control group 
(physician voice: 86.7 ± 7.3; self-voice: 92.5 ± 6.8 vs control: 73.2 ± 8.5; P < 
0.001). The patient self-voice group showed superior knowledge retention before 
discharge, higher education satisfaction, and greater treatment adherence 
compared with both the physician voice and control groups (all P ≤ 0.02). At 
one-month follow-up, the self-voice group maintained improved adherence (Cohen's 
d = 0.74) and exhibited significantly lower anxiety and depression scores (all P 
≤0.02), along with improved SF-36 quality-of-life domains. ChatGPT-based 
evaluations demonstrated high reliability compared with expert assessments 
(weighted κ = 0.87, 95% CI 0.82-0.91).
CONCLUSIONS: This study introduces an innovative patient education model 
integrating AI voice cloning and ChatGPT, representing a novel approach distinct 
from previous studies that primarily relied on standard text-to-speech or 
professionally recorded content. The key innovation lies in utilizing patients' 
own cloned voices for health education delivery, leveraging the self-reference 
effect to enhance learning outcomes. Compared with prior research focusing on 
clinician-narrated content, this study provides the first empirical evidence 
that self-voice education produces superior outcomes across multiple domains 
including compliance, satisfaction, and psychological well-being. These findings 
contribute to the field by establishing a theoretical and practical framework 
for personalized AI-driven patient education. In real-world clinical settings, 
this approach offers a scalable, cost-effective solution to enhance patient 
engagement, particularly valuable in resource-limited environments where 
individualized education is challenging to deliver.
CLINICALTRIAL: Trial Registration: Chinese Clinical Trial Registry 
(ChiCTR2500101882); registration application initiated on January 15, 2025 and 
finalized on April 30, 2025, before participant enrollment began in May 2025.

DOI: 10.2196/81387
PMID: 41548948


7. Int J Med Inform. 2025 Dec 31;209:106250. doi: 10.1016/j.ijmedinf.2025.106250. 
Online ahead of print.

Large language models versus healthcare professionals in providing medical 
information to patient questions: A systematic review.

Jacobs MMG(1), Oosterhoff JHF(2), Agricola R(3), van der Weegen W(4).

Author information:
(1)Department of Orthopedics, Radboudumc, Nijmegen, the Netherlands; Sports & 
Orthopedics Research Centre, St. Anna Hospital, Geldrop, the Netherlands. 
Electronic address: maud.jacobs@radboudumc.nl.
(2)Faculty of Technology Policy and Management, Delft University of Technology, 
Delft, the Netherlands; Department of Orthopaedic Surgery, University Medical 
Centre Groningen, Groningen, the Netherlands.
(3)Sports & Orthopedics Research Centre, St. Anna Hospital, Geldrop, the 
Netherlands; Department of Orthopedics and Sports Medicine, Erasmus University 
Medical Center, Rotterdam, the Netherlands.
(4)Sports & Orthopedics Research Centre, St. Anna Hospital, Geldrop, the 
Netherlands; Department of Anesthesiology, Pain and Palliative Medicine, 
Radboudumc, Nijmegen, the Netherlands.

OBJECTIVE: The rapid expansion of digital healthcare has heightened the volume 
of patient communication, thereby increasing the workload for healthcare 
professionals. Large Language Models (LLMs) hold promises for offering automated 
responses to patient questions relayed through eHealth platforms, yet concerns 
persist regarding their effectiveness, accuracy, and limitations in healthcare 
settings. This study aims to evaluate the current evidence on the performance 
and perceived suitability of LLMs in healthcare, focusing on their role in 
supporting clinical decision-making and patient communication.
MATERIALS AND METHODS: A systematic search in PubMed and Embase up to June 11, 
2025 identified 330 studies, of which 20 met the inclusion criteria for 
comparing the accuracy and adequacy of medical information provided by LLMs 
versus healthcare professionals and guidelines. The search strategy combined 
terms related to LLMs, healthcare professionals, and patient questions. The 
ROBINS-I tool assessed the risk of bias.
RESULTS: A total of nineteen studies focused on medical specialties and one on 
the primary care setting. Twelve studies favored the responses generated by 
LLMs, six reported mixed results, and two favored the healthcare professionals' 
response. Bias components generally scored moderate to low, indicating a low 
risk of bias.
DISCUSSION AND CONCLUSIONS: The review summarizes current evidence on the 
accuracy and adequacy of medical information provided by LLMs in response to 
patient questions, compared to healthcare professionals and clinical guidelines. 
While LLMs show potential as supportive tools in healthcare, their integration 
should be approached cautiously due to inconsistent performance and possible 
risks. Further research is essential before widespread adoption.

Copyright © 2026 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106250
PMID: 41529641

Conflict of interest statement: Declaration of competing interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: Each author certifies that they 
have no commercial associations (e.g., consultancies, stock ownership, equity 
interest, patent/licensing arrangements, etc.) that might pose a conflict of 
interest in connection with the submitted article.


8. J Am Med Inform Assoc. 2026 Jan 13:ocaf233. doi: 10.1093/jamia/ocaf233. Online 
ahead of print.

Testing and evaluation of generative large language models in electronic health 
record applications: a systematic review.

Du X(1)(2)(3), Zhou Z(4), Wang Y(4), Chuang YW(5)(6)(7), Li Y(1)(2), Yang 
R(1)(2), Zhang W(1)(2)(3), Wang X(1)(2)(3), Chen X(1)(2)(3), Guan H(1)(2), Lian 
J(1), Hong P(4), Bates DW(1)(2)(8), Zhou L(1)(2)(3).

Author information:
(1)Division of General Internal Medicine and Primary Care, Brigham and Women's 
Hospital, Boston, MA 02115, United States.
(2)Department of Medicine, Harvard Medical School, Boston, MA 02115, United 
States.
(3)Department of Biomedical Informatics, Harvard Medical School, Boston, MA 
02115, United States.
(4)Department of Computer Science, Brandeis University, Waltham, MA 02453, 
United States.
(5)Division of Nephrology, Department of Internal Medicine, Taichung Veterans 
General Hospital, Taichung 407219, Taiwan.
(6)Department of Post-Baccalaureate Medicine, College of Medicine, National 
Chung Hsing University, Taichung 402202, Taiwan.
(7)School of Medicine, College of Medicine, China Medical University, Taichung 
404328, Taiwan.
(8)Department of Health Policy and Management, Harvard T.H. Chan School of 
Public Health, Boston, MA 02115, United States.

BACKGROUND: The use of generative large language models (LLMs) with electronic 
health record (EHR) data is rapidly expanding to support clinical and research 
tasks. This systematic review characterizes the clinical fields and use cases 
that have been studied and evaluated to date.
METHODS: We followed the Preferred Reporting Items for Systematic Review and 
Meta-Analyses guidelines to conduct a systematic review of articles from PubMed 
and Web of Science published between January 1, 2023, and November 9, 2024. 
Studies were included if they used generative LLMs to analyze real-world EHR 
data and reported quantitative performance evaluations. Through data extraction, 
we identified clinical specialties and tasks for each included article, and 
summarized evaluation methods.
RESULTS: Of the 18 735 articles retrieved, 196 met our criteria. Most studies 
focused on radiology (26.0%), oncology (10.7%), and emergency medicine (6.6%). 
Regarding clinical tasks, clinical decision support made up the largest 
proportion of studies (62.2%), while summarizations and patient communications 
made up the smallest, at 5.6% and 5.1%, respectively. In addition, GPT-4 and 
GPT-3.5 were the most commonly used generative LLMs, appearing in 60.2% and 
57.7% of studies, respectively. Across these studies, we identified 22 unique 
non-NLP metrics and 35 unique NLP metrics. While NLP metrics offer greater 
scalability, none demonstrated a strong correlation with gold-standard human 
evaluations.
CONCLUSION: Our findings highlight the need to evaluate generative LLMs on EHR 
data across a broader range of clinical specialties and tasks, as well as the 
urgent need for standardized, scalable, and clinically meaningful evaluation 
frameworks.

© The Author(s) 2026. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf233
PMID: 41528313


9. J Med Syst. 2026 Jan 9;50(1):6. doi: 10.1007/s10916-025-02334-5.

Assessment of ChatGPT-5 as an Artificial Intelligence Tool for Exploring 
Emerging Dimensions of Clinical Simulation: A Proof-of-concept Study.

Rios-Garcia W(1)(2), Silva-Jiménez S(3)(4), Gálvez-Rodríguez E(5), Alberca-Naira 
Y(6), Via-Y-Rada-Torres AD(7), Rios-Garcia AA(8).

Author information:
(1)Research Network on Digital Health, Artificial Intelligence, and Education 
(NET-IA WORLD), Lima, Peru. wagner16rg@gmail.com.
(2)Hospital San Juan de Dios de Pisco, Pisco, Perú. wagner16rg@gmail.com.
(3)Facultad de Ciencias Médicas, Universidad de Cuenca, Cuenca, Ecuador.
(4)Asociación Científica de Estudiantes de Medicina de la Universidad de Cuenca 
(ASOCEM-UCuenca), Cuenca, Ecuador.
(5)Universidad Nacional de Trujillo, Trujillo, Perú.
(6)Escuela de Medicina, Universidad Nacional de Piura, Piura, 20002, Perú.
(7)Facultad de Medicina, Universidad Científica del Sur, Lima, Perú.
(8)Research Network on Digital Health, Artificial Intelligence, and Education 
(NET-IA WORLD), Lima, Peru.

Artificial intelligence (AI) and large language models (LLMs) such as ChatGPT-5 
are increasingly applied in medical education. However, their potential role in 
clinical simulation remains largely unexplored. This descriptive 
proof-of-concept study aimed to examine ChatGPT-5's ability to synthesize and 
generate educational content related to clinical simulation, focusing on the 
coherence, factual accuracy, and understandability of its outputs. Seven 
exploratory questions covering conceptual, historical, and technological aspects 
of clinical simulation were submitted to ChatGPT-5. Each query was regenerated 
three times to assess consistency. Responses were independently evaluated by 
multiple reviewers using a five-point Likert scale for content quality and 
accuracy, and the Patient Education Materials Assessment Tool (PEMAT) for 
understandability. Authenticity of AI-generated references was verified through 
PubMed and Google Scholar. ChatGPT-5 produced coherent and organized responses 
reflecting major milestones and trends in clinical simulation. Approximately 80% 
of cited references were verifiable, while some inconsistencies indicated 
residual fabrication. The average agreement score for accuracy and coherence was 
4 ("agree"), suggesting generally acceptable quality. PEMAT analysis showed that 
content was structured and clear but occasionally used complex terminology, 
limiting accessibility. Within the exploratory scope of this proof-of-concept 
study, ChatGPT-5 demonstrated potential as a supportive tool for synthesizing 
information about clinical simulation. Nonetheless, interpretive depth, citation 
reliability, and pedagogical adaptation require further refinement. Future 
research should assess the integration of LLMs into immersive simulation 
environments under robust ethical and educational frameworks.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02334-5
PMID: 41507587 [Indexed for MEDLINE]

Conflict of interest statement: Ethics Declarations. Ethical Approval: Not 
applicable. Consent to Participate: We have not worked with human participants. 
Competing Interests: The authors declare no competing interests.


10. NPJ Digit Med. 2026 Jan 7. doi: 10.1038/s41746-025-02302-w. Online ahead of 
print.

Effectiveness of large language models in preoperative and discharge education: 
a systematic review based on an evaluation framework.

Wang M(1), Ma H(2), Piao M(3).

Author information:
(1)School of Nursing, Chinese Academy of Medical Sciences & Peking Union Medical 
College, Beijing, China.
(2)School of Nursing, Chinese Academy of Medical Sciences & Peking Union Medical 
College, Beijing, China. haoming_ma@student.pumc.edu.cn.
(3)School of Nursing, Chinese Academy of Medical Sciences & Peking Union Medical 
College, Beijing, China. parkmihua@gmail.com.

Large language models (LLMs) are increasingly incorporated into preoperative and 
discharge education, yet their effectiveness and the ways in which they are 
evaluated remain inconsistent. This systematic review assessed the effectiveness 
of LLM-based interventions and identified evidence gaps relevant to 
understanding how model characteristics may influence patient outcomes. We 
searched five databases from inception to April 18, 2025, ultimately including 
twenty studies. Outcomes were narratively synthesized, and interventions were 
evaluated using a published four-dimension framework, with reporting patterns 
visualized through a heatmap. Many studies reported benefits for anxiety 
reduction and selected satisfaction domains, whereas findings for pain, 
recovery, and other satisfaction elements showed no significant differences from 
conventional materials. Reporting of evaluation sub-dimensions was uneven, with 
trustworthiness and performance rarely documented alongside clinical endpoints. 
These gaps highlight the need for future research that integrates model-centric 
and patient-centric evaluations to support responsible clinical deployment.

© 2026. The Author(s).

DOI: 10.1038/s41746-025-02302-w
PMID: 41501337

Conflict of interest statement: Competing Interests: The authors declare no 
competing interests.


11. Int J Med Inform. 2025 Dec 31;209:106248. doi: 10.1016/j.ijmedinf.2025.106248. 
Online ahead of print.

A scoping review: how evaluation methods shape our understanding of ChatGPT's 
effectiveness in healthcare.

Liu Y(1), Zhang Y(2), Mao H(3).

Author information:
(1)School of Foreign Studies, China University of Petroleum (East China), No. 66 
West Changjiang Road, Huangdao District, Qingdao, Shandong Province 266580, 
China. Electronic address: liuyuanyuan@upc.edu.cn.
(2)School of Foreign Studies, China University of Petroleum (East China), No. 66 
West Changjiang Road, Huangdao District, Qingdao, Shandong Province 266580, 
China. Electronic address: b23170001@s.upc.edu.cn.
(3)School of Foreign Studies, China University of Petroleum (East China), No. 66 
West Changjiang Road, Huangdao District, Qingdao, Shandong Province 266580, 
China. Electronic address: mao@upc.edu.cn.

BACKGROUND: The rapid growth in research on ChatGPT's healthcare applications 
has led to diverse evaluation methods and substantially heterogeneous findings, 
undermining evidence reliability and hindering clinical translation.
OBJECTIVES: This review aims to examine how different evaluation methods shape 
our understanding of ChatGPT's effectiveness in healthcare.
METHODS: Studies published between 2023 and 2024 that assess the use of ChatGPT 
in medical or healthcare-related contexts were included. Evidence was obtained 
from peer-reviewed literature analyzing ChatGPT's applications across clinical, 
educational, and diagnostic domains. Following the PRISMA guidelines, this 
systematic review analyzed 131 studies published during 2023-2024 that assess 
the use of ChatGPT in medical contexts.
RESULTS: The results indicate that predominant evaluation approaches-controlled 
trial studies, expert assessment studies, measurement-based evaluation studies, 
and prompt generation analysis studies-systematically influence conclusions 
about ChatGPT's performance due to their inherent methodological 
characteristics, such as subjectivity, objectivity, and differences in 
ecological validity. Further analysis reveals that ChatGPT's performance is 
highly context-dependent, shaped by specific application scenarios, model 
versions, and prompting strategies.
CONCLUSIONS: To address methodological heterogeneity and the lack of 
standardization, this study recommends multi-method cross-validation strategies 
and a risk-stratified, standardized evaluation framework. These steps are 
essential to enhance the scientific rigor and reliability of ChatGPT's 
assessment in healthcare and to provide a solid foundation for its clinical 
integration.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106248
PMID: 41485343

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


12. JMIR Med Inform. 2026 Jan 2;14:e79039. doi: 10.2196/79039.

Large Language Model-Based Virtual Patient Systems for History-Taking in Medical 
Education: Comprehensive Systematic Review.

Li D(1), Lebai Lutfi S(2).

Author information:
(1)Artificial Intelligence & Software Engineering, School of Computer Sciences, 
Universiti Sains Malaysia, Penang, Malaysia.
(2)Medical Informatics Department, College of Medicine and Health Sciences, 
Sultan Qaboos University, Al Seeb, Oman.

BACKGROUND: Large language models (LLMs), such as GPT-3.5 and GPT-4 (OpenAI), 
have been transforming virtual patient systems in medical education by providing 
scalable and cost-effective alternatives to standardized patients. However, 
systematic evaluations of their performance, particularly for multimorbidity 
scenarios involving multiple coexisting diseases, are still limited.
OBJECTIVE: This systematic review aimed to evaluate LLM-based virtual patient 
systems for medical history-taking, addressing four research questions: (1) 
simulated patient types and disease scope, (2) performance-enhancing techniques, 
(3) experimental designs and evaluation metrics, and (4) dataset characteristics 
and availability.
METHODS: Following PRISMA (Preferred Reporting Items for Systematic Reviews and 
Meta-Analyses) 2020, 9 databases were searched (January 1, 2020, to August 18, 
2025). Nontransformer LLMs and non-history-taking tasks were excluded. 
Multidimensional quality and bias assessments were conducted.
RESULTS: A total of 39 studies were included, screened by one computer science 
researcher under supervision. LLM-based virtual patient systems mainly simulated 
internal medicine and mental health disorders, with many addressing distinct 
single disease types but few covering multimorbidity or rare conditions. 
Techniques like role-based prompts, few-shot learning, multiagent frameworks, 
knowledge graph (KG) integration (top-k accuracy 16.02%), and fine-tuning 
enhanced dialogue and diagnostic accuracy. Multimodal inputs (eg, speech and 
imaging) improved immersion and realism. Evaluations, typically involving 10-50 
students and 3-10 experts, demonstrated strong performance (top-k accuracy: 
0.45-0.98, hallucination rate: 0.31%-5%, System Usability Scale [SUS] ≥80). 
However, small samples, inconsistent metrics, and limited controls restricted 
generalizability. Common datasets such as MIMIC-III (Medical Information Mart 
for Intensive Care-III) exhibited intensive care unit (ICU) bias and lacked 
diversity, affecting reproducibility and external validity.
CONCLUSIONS: Included studies showed moderate risk of bias, inconsistent 
metrics, small cohorts, and limited dataset transparency. LLM-based virtual 
patient systems excel in simulating multiple disease types but lack 
multimorbidity patient representation. KGs improve top-k accuracy and support 
structured disease representation and reasoning. Future research should 
prioritize hybrid KG-chain-of-thought architectures integrated with open-source 
KGs (eg, UMLS [Unified Medical Language System] and SNOMED-CT [Systematized 
Nomenclature of Medicine - Clinical Terms]), parameter-efficient fine-tuning, 
dialogue compression, multimodal LLMs, standardized metrics, larger cohorts, and 
open-access multimodal datasets to further enhance realism, diagnostic accuracy, 
fairness, and educational utility.

©Dongliang Li, Syaheerah Lebai Lutfi. Originally published in JMIR Medical 
Informatics (https://medinform.jmir.org), 02.01.2026.

DOI: 10.2196/79039
PMCID: PMC12811743
PMID: 41481915 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


13. PLoS One. 2026 Jan 2;21(1):e0339594. doi: 10.1371/journal.pone.0339594. 
eCollection 2026.

Protocol for a scoping review examining the application of large language models 
in healthcare education and public health learning spaces.

Ndukwe H(1), Otukpa EO(2).

Author information:
(1)School of Pharmacy and Medical Sciences, Griffith University, Brisbane, 
Australia.
(2)Health and Wellbeing Theme, African Population Health Research Center 
(APHRC), Nairobi, Kenya.

OBJECTIVE: Through this scoping review, we aim to explore and synthesize 
existing knowledge and evidence on the learning approaches for incorporating 
LLMs into healthcare education and public health research and learning spaces. 
Specifically, we will attempt to investigate methods for auditing prompts for 
accuracy, fairness, and effectiveness; tailoring prompts to improve 
task-specific accuracy and utility; and exploring how end-user feedback is used 
to refine and optimize LLM prompts over time. This review will provide a 
comprehensive understanding of how LLMs are being tailored and improved in these 
fields, contributing to the development of evidence-based strategies for their 
implementation. It will also identify areas for future research and innovation.
INTRODUCTION: The increasing integration of large language models (LLMs) into 
healthcare education and public health research and learning spaces, highlights 
their potential to revolutionize service delivery, decision-making, and 
ultimately patient care and outcomes. Despite these advancements, understanding 
how LLMs can be effectively tailored, audited, and refined for learning remains 
a critical area of inquiry. Key issues include, the accuracy of generated 
information, and their relevance to the medical and public health fields.
INCLUSION CRITERIA: Our focus will be on studies addressing LLM applications in 
healthcare education and public health research and learning spaces, prompt 
engineering techniques, prompt auditing methods, and processes geared towards 
integrating user feedback. Articles that do not focus on healthcare or public 
health contexts and lack relevance to LLM learning approaches will be excluded.
METHODS: The review is guided by the JBI methodology for scoping reviews 
complemented by updates from Levac et al. Databases including PubMed, Scopus, 
IEEE Xplore, and Web of Science will be searched for peer-reviewed articles, 
conference proceedings, and grey literature published in English and French from 
2015 to 2025. Data extraction will include information on study characteristics, 
LLM models, prompt engineering strategies, auditing methodologies, and user 
feedback mechanisms. We will synthesize to identify trends, gaps, and best 
practices in leveraging LLMs to generate baseline data for auditing prompts that 
optimize AI learning and education needs in the healthcare and public health 
sector.

Copyright: © 2026 Ndukwe, Otukpa. This is an open access article distributed 
under the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0339594
PMCID: PMC12758804
PMID: 41481763 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


14. PLoS One. 2026 Jan 2;21(1):e0340277. doi: 10.1371/journal.pone.0340277. 
eCollection 2026.

The use of large language models in generating multiple choice questions for 
health professions education: A systematic review and network meta-analysis.

Riehm L(1)(2), Nanji K(3)(4), Lakhani M(5), Pankiv E(1)(2), Hasanee D(6), 
Pfeifer W(2).

Author information:
(1)Department of Anesthesia and Pain Medicine, The Hospital for Sick Children, 
Toronto, Ontario, Canada.
(2)Department of Anesthesiology and Pain Medicine, University of Toronto, 
Toronto, Ontario, Canada.
(3)Division of Ophthalmology, Department of Surgery, McMaster University, 
Hamilton, Ontario, Canada.
(4)Department of Health Research Methods, Evidence and Impact, McMaster 
University, Hamilton, Ontario, Canada.
(5)Faculty of Medicine, University of Ottawa, Ottawa, Ontario, Canada.
(6)Department of Surgery, McMaster University, Hamilton, Ontario, Canada.

PURPOSE: Large language models (LLMs) have the potential to change medical 
education. Whether LLMs can generate multiple-choice questions (MCQs) that are 
of similar quality to those created by humans is unclear. This investigation 
assessed the quality of MCQs generated by LLMs compared to humans.
METHODS: This review was registered with PROSPERO (CRD42025608775). A systematic 
review and frequentist random-effects network meta-analysis (NMA) or pairwise 
meta-analysis was performed. Ovid MEDLINE, Ovid EMBASE, and Scopus were searched 
from inception to November 1, 2024. The quality of MCQs was assessed with seven 
pre-defined outcomes: question relevance, clarity, accuracy/correctness; 
distractor quality; item difficulty analysis; and item discrimination analysis 
(point biserial correlation and item discrimination index). Continuous data were 
transformed to a 10-point scale to facilitate statistical analysis and reported 
as mean differences (MD). The MERSQI and the Grade of Recommendations, 
Assessment, Development and Evaluation (GRADE) NMA guidelines were used to 
assess risk of bias and certainty of evidence assessments.
RESULTS: Five LLMs were included. NMA demonstrated that ChatGPT 4 generated 
similar quality MCQs to humans with regards to question relevance (MD -0.13; 95% 
CI: -0.44,0.18; GRADE: VERY LOW), question clarity (MD -0.03; 95% CI: 
-0.15,0.10; GRADE: VERY LOW), and distractor quality (MD -0.10; 95% CI: 
-0.24,0.04; GRADE: VERY LOW); however, MCQs generated by Llama 2 performed worse 
than humans with regards to question clarity (MD -1.21; 95% CI: -1.60,-0.82; 
GRADE: VERY LOW) and distractor quality (MD -1.50; 95% CI: -2.03,-0.97; GRADE: 
VERY LOW). Exploratory post-hoc t-tests demonstrated that ChatGPT 3.5 performed 
worse than Llama 2 and ChatGPT 4 with regards to question clarity and distractor 
quality (p < 0.001).
CONCLUSION: ChatGPT 4 may create similar quality MCQs to humans, whereas ChatGPT 
3.5 and Llama 2 may be of worse quality. Further studies that directly compare 
these LLMs to human-generated questions and administer MCQs to students are 
required.

Copyright: © 2026 Riehm et al. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0340277
PMCID: PMC12758716
PMID: 41481658 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


15. BMC Med Inform Decis Mak. 2025 Dec 24;26(1):29. doi: 10.1186/s12911-025-03324-w.

Application of artificial intelligence tools and clinical documentation burden: 
a systematic review and meta-analysis.

Zhao J(1)(2), Liu H(1)(2), Chen Y(3)(4)(5)(6), Song F(7)(8).

Author information:
(1)Department of Pediatric Research Institute, National Clinical Research Center 
for Children and Adolescents' Health and Diseases, Ministry of Education Key 
Laboratory of Child Development and Disorders, Chongqing Key Laboratory of Child 
Rare Diseases in Infection and Immunity, Children's Hospital of Chongqing 
Medical University, Chongqing, China.
(2)Chevidence Lab of Child & Adolescent Health, Children's Hospital of Chongqing 
Medical University, Chongqing, China.
(3)Department of Pediatric Research Institute, National Clinical Research Center 
for Children and Adolescents' Health and Diseases, Ministry of Education Key 
Laboratory of Child Development and Disorders, Chongqing Key Laboratory of Child 
Rare Diseases in Infection and Immunity, Children's Hospital of Chongqing 
Medical University, Chongqing, China. 483861@hospital.cqmu.edu.cn.
(4)Chevidence Lab of Child & Adolescent Health, Children's Hospital of Chongqing 
Medical University, Chongqing, China. 483861@hospital.cqmu.edu.cn.
(5)Research Unit of Evidence-Based Evaluation and Guidelines, School of Basic 
Medical Sciences, Chinese Academy of Medical Sciences (2021RU017), Lanzhou 
University, Lanzhou, China. 483861@hospital.cqmu.edu.cn.
(6)WHO Collaborating Center for Guideline Implementation and Knowledge 
Translation, Lanzhou, China. 483861@hospital.cqmu.edu.cn.
(7)Department of Pediatric Research Institute, National Clinical Research Center 
for Children and Adolescents' Health and Diseases, Ministry of Education Key 
Laboratory of Child Development and Disorders, Chongqing Key Laboratory of Child 
Rare Diseases in Infection and Immunity, Children's Hospital of Chongqing 
Medical University, Chongqing, China. fujian.song@uea.ac.uk.
(8)Norwich Medical School, University of East Anglia, Norwich, Norfolk, UK. 
fujian.song@uea.ac.uk.

BACKGROUND: Clinician burnout is a growing global concern, with heavy clinical 
documentation workload identified as a major contributor. Clinical documentation 
tasks, though essential for patient care and communication, are time-consuming 
and cognitively demanding. Recent advances in artificial intelligence (AI), 
particularly natural language processing and large language models, are being 
explored as potential tools to alleviate documentation burden, yet their 
quantitative impact has not been systematically assessed.
METHODS: We performed a systematic review and meta-analysis, registered on 
PROSPERO (CRD420250653291) and guided by PRISMA. Eligible studies included 
frontline health professionals using AI tools for clinical note creation, with 
comparators being usual practice or pre-implementation baseline. Primary 
outcomes were documentation burden, workload, burnout, and time spent on 
documentation. Searches were conducted in PubMed, Web of Science, Scopus, and 
key journals. Effect sizes were synthesized using standardized mean difference 
(SMD) under a random-effects model, with subgroup analyses by study design, AI 
tool type, task type, editing status, and data origin.
RESULTS: Of the 23 studies included, 12 were non-randomised studies with a 
concurrent control and 11 employed a before-and-after comparison design. The 
study participants varied in specialties and were mainly from ambulatory 
settings, including physicians, surgeons, pediatricians, and ICU specialists. 
Heterogeneity in results across included studies was considerable, and the 
methodological quality of the available studies was generally low. Pooling 
results of the 14 studies yielded an overall standardized mean difference (SMD) 
of -0.71 (95% confidence interval [CI]: -0.93 to -0.49), indicating a moderate 
reduction in documentation workload and related burnout. Based on results of 
studies in which clinicians reviewed and edited AI-generated drafts, AI 
applications reduced documentation time, similarly representing a moderate 
effect size (SMD= -0.72, 95% CI -0.99 to -0.45). The quality of notes generated 
by AI tools was at least comparable to those prepared manually by clinicians.
CONCLUSIONS: AI technologies offer promising benefits for reducing clinical 
documentation burden. However, their implementation must be accompanied by 
rigorous quality control and ongoing evaluation in practical settings to 
optimize their effectiveness and safeguard patient care outcomes.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-03324-w
PMCID: PMC12836966
PMID: 41444884 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. This study is a systematic review and meta-analysis 
of previously published literature and did not involve any human participants 
directly. Consent for publication: Not applicable. Competing interests: The 
authors declare no competing interests.


16. Int J Med Inform. 2026 Mar 15;208:106231. doi: 10.1016/j.ijmedinf.2025.106231. 
Epub 2025 Dec 22.

Advancing healthcare with large language models: A scoping review of 
applications and future directions.

Zhang Z(1), Momeni Nezhad MJ(2), Bagher Hosseini SM(2), Zolnour A(2), Zonour 
Z(2), Hosseini SM(2), Topaz M(3), Zolnoori M(4).

Author information:
(1)Data Science Institute, Columbia University, New York, NY 10027, United 
States; School of Nursing, Columbia University, New York, NY 10032, United 
States. Electronic address: zz3238@columbia.edu.
(2)Columbia University Irving Medical Center, New York, NY 10032, United States.
(3)Data Science Institute, Columbia University, New York, NY 10027, United 
States; School of Nursing, Columbia University, New York, NY 10032, United 
States.
(4)Data Science Institute, Columbia University, New York, NY 10027, United 
States; Columbia University Irving Medical Center, New York, NY 10032, United 
States.

BACKGROUND: The release of ChatGPT has spurred the widespread adoption of 
generative large language models (LLMs) in healthcare. This scoping review 
systematically examines their use in healthcare.
METHODS: A systematic search was conducted using PubMed, a comprehensive and 
representative database on biomedical and health science, to identify studies 
published between January 1, 2023, and July 30, 2024. Studies were included if 
they assessed the performance of generative LLMs in healthcare applications; 
review or perspective articles were excluded.
RESULTS: A total of 415 studies were included, with a significant increase in 
publications observed after April 2023. Generative LLMs were applied across 
various medical specialties, primarily supporting clinical decision-making 
(26.7%) and providing patient information (23.9%). Smaller proportions were 
focused on professional education and training (18.1%), research (16.1%), and 
workflow support (12.5%). These applications were mainly supported by three key 
NLP tasks: question answering (36.1%), text classification (27.5%), and text 
generation (26.3%). Public datasets appeared in 20% of studies, and 15% used 
clinical patient data. Of the 98 LLMs used, GPT-4 (51.3%), GPT-3.5 (36.6%), and 
ChatGPT (22.4%) were the most common. Direct prompting was the most common 
adaptation method (92.5%), with reinforcement learning rarely utilized (1.4%). 
Accuracy was the most frequently assessed metric, while errors and safety (9.4%) 
and time efficiency (7.0%) were less commonly evaluated.
CONCLUSION: LLMs hold promise across healthcare applications. Expanding their 
use in workflow optimization, trainee education, and research tools could 
enhance healthcare delivery and innovation. Comprehensive evaluation using 
standardized criteria is essential for LLMs integration into healthcare.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106231
PMID: 41443123 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


17. J Am Med Inform Assoc. 2025 Dec 23:ocaf223. doi: 10.1093/jamia/ocaf223. Online 
ahead of print.

AutoReporter: development of an artificial intelligence tool for automated 
assessment of research reporting guideline adherence.

Chen D(1)(2), Li P(3), Khoshkish E(2), Lee S(2), Ning T(2), Tahir U(2), Wong 
HCY(4), Lee MSF(5), Raman S(6)(7).

Author information:
(1)Princess Margaret Cancer Centre, Radiation Medicine Program, Toronto, ON M5G 
2C4, Canada.
(2)Temerty Faculty of Medicine, University of Toronto, Toronto, ON M5S 3K3, 
Canada.
(3)Faculty of Engineering, McMaster University, Hamilton, ON L8S 4M3, Canada.
(4)Department of Oncology, Princess Margaret Hospital, Hong Kong, China.
(5)Department of Radiation Oncology, National University Cancer Institute, 
National University Hospital, Singapore, Singapore.
(6)Department of Radiation Oncology, BC Cancer, Vancouver, BC V5Z 4E6, Canada.
(7)Division of Radiation Oncology, University of British Columbia, Vancouver, BC 
V5Z 1M9, Canada.

OBJECTIVES: To develop AutoReporter, a large language model (LLM) system that 
automates evaluation of adherence to research reporting guidelines.
MATERIALS AND METHODS: Eight prompt-engineering and retrieval strategies coupled 
with reasoning and general-purpose LLMs were benchmarked on the 
SPIRIT-CONSORT-TM corpus. The top-performing approach, AutoReporter, was 
validated on BenchReport, a novel benchmark dataset of expert-rated reporting 
guideline assessments from 10 systematic reviews.
RESULTS: AutoReporter, a zero-shot, no-retrieval prompt coupled with the o3-mini 
reasoning LLM, demonstrated strong accuracy (CONSORT 90.09%; SPIRIT: 92.07%), 
substantial agreement with humans (CONSORT Cohen's κ = 0.70, SPIRIT Cohen's 
κ = 0.77), runtime (CONSORT: 617.26 s; SPIRIT: 544.51 s), and cost (CONSORT: 
0.68 USD; SPIRIT: 0.65 USD). AutoReporter achieved a mean accuracy of 91.8% and 
substantial agreement (Cohen's κ > 0.6) with expert ratings from the BenchReport 
benchmark.
DISCUSSION: Structured prompting alone can match or exceed fine-tuned domain 
models while forgoing manually annotated corpora and computationally intensive 
training.
CONCLUSION: Large language models can feasibly automate reporting guideline 
adherence assessments for scalable quality control in scientific research 
reporting. AutoReporter is publicly accessible at 
https://autoreporter.streamlit.app.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf223
PMID: 41435508


18. J Med Internet Res. 2025 Dec 16;27:e78238. doi: 10.2196/78238.

Generative AI Mental Health Chatbots as Therapeutic Tools: Systematic Review and 
Meta-Analysis of Their Role in Reducing Mental Health Issues.

Zhang Q(1), Zhang R(2), Xiong Y(3), Sui Y(3), Tong C(3), Lin FH(3).

Author information:
(1)Department of Educational Advancement, Duke-NUS Medical School, 8 College 
Road, Singapore, 169857, Singapore, 65 66012186.
(2)Wee Kim Wee School of Communication and Information, Nanyang Technological 
University, Singapore, Singapore.
(3)School of Education, Johns Hopkins University, Baltimore, MD, United States.

BACKGROUND: In recent years, artificial intelligence (AI) has driven the rapid 
development of AI mental health chatbots. Most current reviews investigated the 
effectiveness of rule-based or retrieval-based chatbots. To date, there is no 
comprehensive review that systematically synthesizes the effect of generative AI 
(GenAI) chatbot's impact on mental health.
OBJECTIVE: This review aims to (1) narratively synthesize existing GenAI mental 
health chatbots' technical features, treatment and research designs, and sample 
characteristics through a systematic review of quantitative studies and (2) 
quantify the effectiveness and key moderators of these rigorously designed 
trials on GenAI mental health chatbots through a meta-analysis of only 
randomized controlled trials (RCTs).
METHODS: The search strategy includes 11 database searching, backward citation 
tracking, and a manual ad hoc search to update literature. This thorough 
literature search, completed in March 2025, returned 5555 records for screening. 
The systematic review included studies that (1) used generative or hybrid 
(rule/retrieval-based and generative) AI-based chatbots to deliver interventions 
and (2) quantitatively measured mental health-related outcomes. The 
meta-analysis has additional inclusion criteria: (1) studies must be RCTs, (2) 
must measure negative mental health issues, (3) the comparison group must not 
have chatbot features, and (4) must provide enough statistics for effect size 
calculation. We followed the PRISMA (Preferred Reporting Items for Systematic 
Reviews and Meta-Analyses) checklist and registered the protocol retrospectively 
during the revision process (September 18, 2025). In meta-regression, data were 
synthesized in R software using a random-effects model.
RESULTS: The narrative synthesis of 26 studies revealed that (1) GenAI chatbot 
interventions mostly took place in non-WEIRD countries (non-Western, Educated, 
Industrialized, Rich, and Democratic) and (2) there is a lack of studies 
focusing on young children and older adults. The meta-analysis of 14 RCTs showed 
a statistically significant effect (effect size [ES]=0.30, P=.047, N=6314, 95% 
CI 0.004, 0.59, 95% prediction interval [PI] -0.85, 1.67), which means that 
GenAI chatbots are, on average, effective in reducing negative mental health 
issues, such as depression, anxiety, among others. We found that social-oriented 
chatbots (ie, those that mainly provide social interactions) are more effective 
than task-oriented programs (ie, those that assist with specific tasks). Risk of 
bias in the nonrandomized studies and RCTs was assessed using Cochrane ROBINS-I 
(Risk Of Bias In Non-randomised Studies - of Interventions) and RoB2 (revised 
Cochrane risk-of-bias tool for randomized trials), respectively, indicating a 
moderate amount of risk. One main limitation of this meta-analysis is the small 
number of studies (n=14) included.
CONCLUSIONS: By identifying research gaps, we suggest that future researchers 
investigate user groups such as adolescents and older adults, outcomes other 
than depression and anxiety, cultural adaptations in non-WEIRD countries, ways 
to streamline chatbots in usual care practices, and explore applications in 
diverse settings. More importantly, we cannot ignore GenAI chatbots' risks while 
acknowledging their promise. This review also emphasized several ethical 
implications.

© Qiyang Zhang, Renwen Zhang, Yiying Xiong, Yuan Sui, Chang Tong, Fu-Hung Lin. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/78238
PMCID: PMC12707440
PMID: 41401240 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


19. Int J Med Inform. 2026 Mar 1;207:106224. doi: 10.1016/j.ijmedinf.2025.106224. 
Epub 2025 Dec 13.

Generative artificial intelligence as a source of advice on resuscitation and 
first aid for laypeople: A scoping review.

Birkun AA(1).

Author information:
(1)Department of Anaesthesiology, Resuscitation and Emergency Medicine, Medical 
Institute Named After S.I. Georgievsky of V.I. Vernadsky Crimean Federal 
University, Lenin Blvd, 5/7, Simferopol 295051, Russian Federation. Electronic 
address: birkunalexei@gmail.com.

INTRODUCTION: The performance of cutting-edge generative artificial intelligence 
(GenAI) in guiding laypeople on how to give help in health emergencies is 
attracting growing attention. This study aimed to map and summarise original 
research evidence on the quality of GenAI-synthesised advice on resuscitation 
and first aid.
METHODS: The review encompassed journal publications that reported original 
quantitative data on the quality (accuracy, correctness, completeness, 
appropriateness) of GenAI-synthesised advice on how laypeople should perform 
cardiopulmonary resuscitation or provide first aid. Relevant papers were 
identified through PubMed, Scopus, and Google Scholar. Studies were included if 
they were published in English as an article, short report, letter, or note 
during the period 2017-2025. The review was conducted following the 
recommendations of the PRISMA extension for Scoping Reviews.
RESULTS: Among the 19 eligible studies, 17 evaluated the performance of 
text-generating GenAI tools, one tested user-to-GenAI voice interaction and 
another one investigated text-to-video generation capabilities. The studies 
exhibited substantial heterogeneity in research design, methods, and reporting. 
Most of them (89.5 %) presented evidence of flaws in the generation of advice on 
resuscitation or first aid, including a failure to synthesise requested content 
(reported by 15.8 % of the studies), the creation of incomplete instructions 
(57.9 %), inaccurate instructions (57.9 %), or superfluous guidance (36.8 %), 
irrelevant or potentially harmful. The prevalence of misinformation varied from 
study to study, at times encompassing the whole sample of evaluated GenAI 
responses. Some authors did not accentuate the issue of misinformation despite 
the reported data indicating quality defects.
CONCLUSIONS: Current evidence indicates risks associated with the unsupervised 
generation of resuscitation and first aid guidance by publicly available GenAI, 
as the synthesised content often contains misinformation that may mislead users 
and induce harmful actions. There is a growing need for international 
collaboration to develop coordinated strategies to limit GenAI-driven 
misinformation and mitigate potential health risks.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106224
PMID: 41391284 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The author 
declares that the topic under review represents a particular area of research 
interest and that he served as lead investigator in four of the studies included 
in this review.


20. JMIR Med Inform. 2025 Dec 9;13:e78041. doi: 10.2196/78041.

Trends and Trajectories in the Rise of Large Language Models in Radiology: 
Scoping Review.

Al Zaabi A(1), Alshibli R(2), AlAmri A(2), AlRuheili I(2), Lutfi SL(3).

Author information:
(1)Human and Clinical Anatomy Department, College of Medicine and Health 
Sciences, Sultan Qaboos University, P.O. Box 35, Al Khodh, Muscat, 123, Oman.
(2)College of Medicine and Health Sciences, Sultan Qaboos University, Muscat, 
Oman.
(3)Medical Education and Informatics Department, College of Medicine and Health 
Sciences, Sultan Qaboos University, Muscat, Oman.

BACKGROUND: The use of large language models (LLMs) in radiology is expanding 
rapidly, offering new possibilities in report generation, decision support, and 
workflow optimization. However, a comprehensive evaluation of their 
applications, performance, and limitations across the radiology domain remains 
limited.
OBJECTIVE: This review aimed to map current applications of LLMs in radiology, 
evaluate their performance across key tasks, and identify prevailing limitations 
and directions for future research.
METHODS: A scoping review was conducted in accordance with the framework by 
Arksey and O'Malley framework and the PRISMA-ScR (Preferred Reporting Items for 
Systematic Reviews and Meta-Analyses extension for Scoping Reviews) guidelines. 
Three databases-PubMed, ScopusCOPUS, and IEEE Xplore-were searched for 
peer-reviewed studies published between January 2022 and December 2024. Eligible 
studies included empirical evaluations of LLMs applied to radiological data or 
workflows. Commentaries, reviews, and technical model proposals without 
evaluation were excluded. Two reviewers independently screened studies and 
extracted data on study characteristics, LLM type, radiological use case, data 
modality, and evaluation metrics. A thematic synthesis was used to identify key 
domains of application. No formal risk-of-bias assessment was performed, but a 
narrative appraisal of dataset representativeness and study quality was 
included.
RESULTS: A total of 67 studies were included. (n/N, %)GPT-4 was the most 
frequently used model (n=28, 42%), with text-based corpora as the primary type 
of data used (n=43, 64%). Identified use cases fell into three thematic domains: 
(1) decision support (n=39, 58%), (2) report generation and summarization (n=16, 
24%), and (3) workflow optimization (n=12, 18%). While LLMs demonstrated strong 
performance in structured-text tasks (eg, report simplification with >94% 
accuracy), diagnostic performance varied widely (16%-86%) and was limited by 
dataset bias, lack of fine tuning, and minimal clinical validation. Most studies 
(n=53, 79.1%) had single-center, proof-of-concept designs with limited 
generalizability.
CONCLUSIONS: LLMs show strong potential for augmenting radiological workflows, 
particularly for structured reporting, summarization, and educational tasks. 
However, their diagnostic performance remains inconsistent, and current 
implementations lack robust external validation. Future work should prioritize 
prospective, multicenter validation of domain-adapted and multimodal models to 
support safe clinical integration.

© Adhari Al Zaabi, Rashid Alshibli, Abdullah AlAmri, Ibrahim AlRuheili, 
Syaheerah Lebai Lutfi. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org).

DOI: 10.2196/78041
PMCID: PMC12688054
PMID: 41364806 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


21. J Med Internet Res. 2025 Dec 8;27:e77110. doi: 10.2196/77110.

Critical Appraisal Tools for Evaluating Artificial Intelligence in Clinical 
Studies: Scoping Review.

Cabello JB(1), Ruiz Garcia V(2), Torralba M(3), Maldonado Fernandez M(4), Ubeda 
M(5), Ansuategui E(6), Ramos-Ruperto L(7), Emparanza JI(8), Urreta I(8), 
Iglesias MT(8), Pijoan JI(9), Burls A(10).

Author information:
(1)Critical Appraisal Skills Program Spain, C/ Enriqueta Elizaizin, 2, E 5, 7C, 
Alicante, 03007, Spain, 34 619669243.
(2)Unidad de Hospitalización a Domicilio, Hospital Universitari i Politècnic La 
Fe, Valencia, Spain.
(3)Servicio de Medicina Interna, Hospital Universitario de Guadalajara, 
Guadalajara, Spain.
(4)Department of ENT, Hospital Vital Alvarez Buylla, Mieres, Spain.
(5)Hospital Donostia, Donostia - San Sebastian, Spain.
(6)Biblioteca virtual de salud de Euskadi, Vitoria, Spain.
(7)Unidad de VIH, Medicina Interna, Hospital Universitario La Paz, Madrid, 
Spain.
(8)Unidad de Epidemiologia Clínica e Investigación, CIBER-SP, Hospital 
Universitario Donostia, San Sebastian, Spain.
(9)Instituto de Investigación Sanitaria Biobizkaia-Hospital Universitario 
Cruces, Bizkaia, Baracaldo, Spain.
(10)City St George's, University of London, London, United Kingdom.

BACKGROUND: Health research that uses predictive and generative artificial 
intelligence (AI) is rapidly growing. As in traditional clinical studies, the 
way in which AI studies are conducted can introduce systematic errors. The 
translation of this AI evidence into clinical practice and research needs 
critical appraisal tools for clinical decision-makers and researchers.
OBJECTIVE: This study aimed to identify existing tools for the critical 
appraisal of clinical studies that use AI and to examine the concepts and 
domains these tools explore. The research question was framed using the 
Population-Concept-Context (PCC) framework. Population (P): AI clinical studies; 
Concept (C): tools for critical appraisal and associated constructs such as 
quality, reporting, validity, risk of bias, and applicability; and context (C): 
clinical practice. In addition, studies on bias classification and chatbot 
assessment were included.
METHODS: We searched medical and engineering databases (MEDLINE, Embase, CINAHL, 
PsycINFO, and IEEE) from inception to April 2024. We included clinical primary 
research with tools for critical appraisal. Classical reviews and systematic 
reviews were included in the first phase of screening and excluded in the 
secondary phase after identifying new tools by forward snowballing. We excluded 
nonhuman, computer, and mathematical research, and letters, opinion papers, and 
editorials. We used Rayyan (Qatar Computing Research Institute) for screening. 
Data extraction was done by two reviewers, and discrepancies were resolved 
through discussion. The protocol was previously registered in Open Science 
Framework. We adhered to the PRISMA-ScR (Preferred Reporting Items for 
Systematic reviews and Meta-Analyses extension for Scoping Reviews) and the 
PRISMA-S (PRISMA-Search) extension for reporting literature in systematic 
reviews.
RESULTS: We retrieved 4393 unique records for screening. After excluding 3803 
records, 119 were selected for full-text screening. From these, 59 were 
excluded. After inclusion of 10 studies via other methods, a total of 70 records 
were finally included. We found 46 tools (26 guides for reporting AI studies, 16 
tools for critical appraisal, 2 for study quality, and 2 for risk of bias). Nine 
papers focused on bias classification or mitigation. We found 15 chatbot 
assessment studies or systematic reviews of chatbot studies (6 and 9, 
respectively), which are a very heterogeneous group.
CONCLUSIONS: The results picture a landscape of evidence tools where reporting 
tools predominate, followed by critical appraisal, and a few tools for risk of 
bias. The mismatch of bias in AI and epidemiology should be considered for 
critical appraisal, especially regarding fairness and bias mitigation in AI. 
Finally, chatbot assessment studies represent a vast and evolving field in which 
progress in design, reporting, and critical appraisal is necessary and urgent.

© Juan B Cabello, Vicente Ruiz Garcia, Miguel Torralba, Miguel Maldonado 
Fernandez, Marimar Ubeda, Eukene Ansuategui, Luis Ramos-Ruperto, Jose I 
Emparanza, Iratxe Urreta, Maria Teresa Iglesias, Jose I Pijoan, Amanda Burls. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/77110
PMCID: PMC12685289
PMID: 41359958 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


22. J Med Internet Res. 2025 Dec 5;27:e80770. doi: 10.2196/80770.

Promoting Responsible DeepSeek Deployment in Health Care: Scoping Review 
Comparing Grey and White Literature.

Jiang W(#)(1), Wang D(#)(1), Zeng Y(2), Huang J(3), Xu C(4), Liu C(3).

Author information:
(1)School of Management, Hubei University of Chinese Medicine, Wuhan, China.
(2)School of Pharmacy, Huazhong University of Science and Technology, Wuhan, 
China.
(3)School of Medicine and Health Management, Huazhong University of Science and 
Technology, Hangkong Road 13, Qiaokou District, Wuhan, Hubei, 430000, China, 86 
15623423595.
(4)Intelligent Hospital Research Academy, Peking University Shenzhen Hospital, 
Shenzhen, China.
(#)Contributed equally

BACKGROUND: DeepSeek is an open-source large language model (LLM), and it has 
greatly accelerated LLM adoption in health care. Its rapid deployment has 
sparked concerns regarding its impact on patient outcomes and safety. However, 
little is known about how DeepSeek is used and regulated in health care.
OBJECTIVE: This study aimed to (1) systematically review the characteristics of 
DeepSeek deployed in the top 100 hospitals in China, and (2) compare the 
performance and risks of DeepSeek between hospital disclosures and research 
evidence.
METHODS: We searched the official websites and WeChat accounts of the top 100 
hospitals in China and the databases of Web of Science and PubMed, using the 
terms "DeepSeek" and "large language models." Searches were limited to records 
after January 15, 2025, when DeepSeek was first released. All searches were 
conducted on May 20, 2025, with an update on June 28, 2025. We extracted the 
basic characteristics of DeepSeek; its aims, evaluation approach, performance, 
and risks; and hospital regulations. A coding framework was developed covering 
the application scenarios, evaluation dimensions, and risk sources of LLMs. The 
risk of bias was assessed using the Joanna Briggs Institute checklist.
RESULTS: We identified a total of 58 DeepSeek models in 48 out of the top 100 
Chinese hospitals and found 27 studies in the literature. The first hospital 
deployment of DeepSeek was recorded on February 10, 2025, and deployment rapidly 
expanded to 37 hospitals within a month. Concurrently, most related research 
studies (20/27, 74%) were published after May 2025. Among deployments and 
studies that reported version information, DeepSeek-reasoner (R1) was the most 
frequently used model, and private deployment was the predominant approach. 
DeepSeek was mainly used to assist in clinical decision-making, including 
patient diagnosis and treatment recommendation. Among hospital disclosures, only 
36% (21/58) clearly indicated a predeployment assessment, 22% (13/58) presented 
assessment results, and 9% (5/58) identified potential risks and 
countermeasures. We found poor transparency in hospital reporting, with none of 
the disclosures presenting evaluation details. Hospitals were more likely to 
report higher performance and fewer risks for DeepSeek.
CONCLUSIONS: This is one of the first scoping reviews to reveal the rapid, 
widespread deployment of DeepSeek in China's leading hospitals, primarily for 
clinical decision support. The deployment of DeepSeek in China's leading 
hospitals poses potential risks to patient outcomes and safety. We highlight the 
urgent need for existing regulations to be expanded to downstream developers and 
users to promote the responsible use of LLMs in health care. Hospitals need to 
use a more rigorous validation process and adopt a more transparent reporting 
policy. The main limitations of this review include the restriction to top-tier 
hospitals and the inherent constraints of gray literature. These factors should 
be considered when interpreting the findings.

© Wang Jiang, Dan Wang, Yihang Zeng, Jiaqi Huang, Chang Xu, Chenxi Liu. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/80770
PMCID: PMC12680131
PMID: 41348941 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


23. J Med Internet Res. 2025 Dec 4;27:e78186. doi: 10.2196/78186.

The Efficacy of Rule-Based Versus Large Language Model-Based Chatbots in 
Alleviating Symptoms of Depression and Anxiety: Systematic Review and 
Meta-Analysis.

Du Q(1), Ren Y(#)(1), Meng ZL(#)(2), He H(1), Meng S(1).

Author information:
(1)Beijing Yuxin Technology Co., Ltd, Room 2-5, 13th Floor, Building 2, No. 48, 
Zhichun Road, Haidian District, Beijing, 100086, China, 86 010-81377053.
(2)Department of Psychology, School of Humanities and Social Sciences, Beijing 
Forestry University, Beijing, China.
(#)Contributed equally

BACKGROUND: The global mental health crisis is becoming increasingly severe. Due 
to the shortage of mental health professionals, high treatment costs, and 
insufficient accessibility of services, there is an urgent need for scalable and 
low-cost intervention methods. In recent years, chatbots have shown potential 
for psychological interventions. The efficacy differences between large language 
model (LLM)-based and rule-based chatbots have not been systematically 
evaluated, with few studies directly comparing the two; existing meta-analyses 
have notable limitations: there is high heterogeneity in intervention design 
(eg, dialogue structure, interaction frequency, and duration) across studies, 
and there is a lack of direct comparison of differentiated intervention effects 
on depressive and anxiety symptoms, making it difficult to integrate 
conclusions.
OBJECTIVE: By integrating studies from the past five years, this research 
evaluates the differences in effectiveness between LLM-based and rule-based 
chatbots in alleviating depressive and anxiety symptoms. It also analyzes the 
impacts of control group type, intervention duration, and age on intervention 
outcomes. By analyzing chatbot functionality, the study aims to provide 
evidence-based technological pathway options and optimization recommendations 
for differentiated interventions for depression and anxiety.
METHODS: A systematic search of 7 databases included 15 studies published 
between 2020 and 2025. Robust variance estimation (RVE) was used to account for 
non-independent effect sizes, and standardized mean differences (SMDs) were 
calculated using Hedges g. Based on the expectation of clinical and 
methodological heterogeneity among studies, a random-effects model was 
preselected, and the pooled effect size was estimated using restricted maximum 
likelihood estimation (REML) and interpreted according to Cohen criteria. 
Publication bias was assessed using the RVE-adjusted Egger test, funnel plot 
asymmetry, and a fail-safe N.
RESULTS: For depression, rule-based intervention achieved a small but 
significant effect (g=0.266; 95% CI 0.020-0.512; P=.04), while LLM-based 
intervention showed a nonsignificant effect with wide confidence intervals 
(g=0.407; 95% CI -0.734 to 1.550; P=.17). For anxiety, rule-based intervention 
did not yield a significant effect (g=0.147; 95% CI -0.073 to 0.367; P=.15). 
Similarly, LLM-based intervention showed a higher point estimate but also with 
nonsignificance and wide confidence intervals (g=0.711; 95% CI -0.334 to 1.760; 
P=.13). Subgroup analysis showed that the rule-based chatbot was more effective 
than the blank control for depression, with the greatest effect in the medium 
term (4-8 weeks).
CONCLUSIONS: Rule-based chatbots have a modest effect on improving depressive 
symptoms and are suitable for environments with limited psychological resources; 
4-8 weeks may be a critical intervention window. Intervention duration and 
participant age did not significantly influence intervention effectiveness. 
Limited by the sample size, robust evidence supporting the effectiveness of 
LLM-based chatbot interventions is lacking, and further sample size expansion is 
warranted.

© Qiuxue Du, Yongliang Ren, Ze-long Meng, Han He, Shasha Meng. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/78186
PMCID: PMC12677872
PMID: 41343858 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


24. Front Digit Health. 2025 Nov 17;7:1692517. doi: 10.3389/fdgth.2025.1692517. 
eCollection 2025.

Ethical and practical challenges of generative AI in healthcare and proposed 
solutions: a survey.

Tung T(1), Hasnaeen SMN(2), Zhao X(3).

Author information:
(1)Department of Biomedical Engineering, University of Tennessee, Knoxville, TN, 
United States.
(2)Bredesen Center, University of Tennessee, Knoxville, TN, United States.
(3)Department of Mechanical Engineering, University of Mississippi, Oxford, MS, 
United States.

BACKGROUND: Generative artificial intelligence (AI) is rapidly transforming 
healthcare, but its adoption introduces significant ethical and practical 
challenges. Algorithmic bias, ambiguous liability, lack of transparency, and 
data privacy risks can undermine patient trust and create health disparities, 
making their resolution critical for responsible AI integration.
OBJECTIVES: This systematic review analyzes the generative AI landscape in 
healthcare. Our objectives were to: (1) identify AI applications and their 
associated ethical and practical challenges; (2) evaluate current data-centric, 
model-centric, and regulatory solutions; and (3) propose a framework for 
responsible AI deployment.
METHODS: Following the PRISMA 2020 statement, we conducted a systematic review 
of PubMed and Google Scholar for articles published between January 2020 and May 
2025. A multi-stage screening process yielded 54 articles, which were analyzed 
using a thematic narrative synthesis.
RESULTS: Our review confirmed AI's growing integration into medical training, 
research, and clinical practice. Key challenges identified include systemic bias 
from non-representative data, unresolved legal liability, the "black box" nature 
of complex models, and significant data privacy risks. Proposed solutions are 
multifaceted, spanning technical (e.g., explainable AI), procedural (e.g., 
stakeholder oversight), and regulatory strategies.
DISCUSSION: Current solutions are fragmented and face significant implementation 
barriers. Technical fixes are insufficient without robust governance, clear 
legal guidelines, and comprehensive professional education. Gaps in global 
regulatory harmonization and frameworks ill-suited for adaptive AI persist. A 
multi-layered, socio-technical approach is essential to build trust and ensure 
the safe, equitable, and ethical deployment of generative AI in healthcare.
CONCLUSIONS: The review confirmed that generative AI has a growing integration 
into medical training, research, and clinical practice. Key challenges 
identified include systemic bias stemming from non-representative data, 
unresolved legal liability, the "black box" nature of complex models, and 
significant data privacy risks. These challenges can undermine patient trust and 
create health disparities. Proposed solutions are multifaceted, spanning 
technical (such as explainable AI), procedural (like stakeholder oversight), and 
regulatory strategies.

© 2025 Tung, Hasnaeen and Zhao.

DOI: 10.3389/fdgth.2025.1692517
PMCID: PMC12665710
PMID: 41333106

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


25. JMIR Med Inform. 2025 Dec 1;13:e78332. doi: 10.2196/78332.

Enabling Just-in-Time Clinical Oncology Analysis With Large Language Models: 
Feasibility and Validation Study Using Unstructured Synthetic Data.

May P(1), Greß J(1)(2), Seidel C(3), Sommer S(4), Schuler MK(2)(5), Nokodian 
S(1), Schröder F(2), Jung J(1)(6).

Author information:
(1)Department of Internal Medicine III, School of Medicine and Health, TUM 
University Hospital, Technical University of Munich, Ismaninger Str. 22, Munich, 
Germany, 49 89-4140-8753.
(2)MPiriQ Science Technologies GmbH, Munich, Germany.
(3)Department of Oncology, Hematology and Bone Marrow Transplantation with 
Division of Pneumology, University Medical Center Hamburg-Eppendorf, Hamburg, 
Germany.
(4)MVZ Elisenhof, Munich, Germany.
(5)Onkologischer Schwerpunkt am Oskar-Helene Heim, Berlin, Germany.
(6)Department of Hematology and Medical Oncology, University Medical Center 
Göttingen, Göttingen, Germany.

BACKGROUND: Traditional cancer registries, limited by labor-intensive manual 
data abstraction and rigid, predefined schemas, often hinder timely and 
comprehensive oncology research. While large language models (LLMs) have shown 
promise in automating data extraction, their potential to perform direct, 
just-in-time (JIT) analysis on unstructured clinical narratives-potentially 
bypassing intermediate structured databases for many analytical tasks-remains 
largely unexplored.
OBJECTIVE: This study aimed to evaluate whether a state-of-the-art LLM (Gemini 
2.5 Pro) can enable a JIT clinical oncology analysis paradigm by assessing its 
ability to (1) perform high-fidelity multiparameter data extraction, (2) answer 
complex clinical queries directly from raw text, (3) automate multistep survival 
analyses including executable code generation, and (4) generate novel, 
clinically plausible hypotheses from free-text documentation.
METHODS: A synthetic dataset of 240 unstructured clinical letters from patients 
with stage IV non-small cell lung cancer (NSCLC), embedding 14 predefined 
variables, was used. Gemini 2.5 Pro was evaluated on four core JIT capabilities. 
Performance was measured by using the following metrics: extraction accuracy 
(compared to human extraction of n=40 letters and across the full n=240 
dataset); numerical deviation for direct question answering (n=40 to 240 
letters, 5 questions); log-rank P value and Harrell concordance index for 
LLM-generated versus ground-truth Kaplan-Meier survival analyses (n=160 letters, 
overall survival and progression-free survival); and correct justification, 
novelty, and a qualitative evaluation of LLM-generated hypotheses (n=80 and 
n=160 letters).
RESULTS: For multiparameter extraction from 40 letters, the LLM achieved >99% 
average accuracy, comparable to human extraction, but in significantly less time 
(LLM: 3.7 min vs human: 133.8 min). Across the full 240-letter dataset, LLM 
multiparameter extraction maintained >98% accuracy for most variables. The LLM 
answered multiconditional clinical queries directly from raw text with a 
relative deviation rarely exceeding 1.5%, even with up to 240 letters. 
Crucially, it autonomously performed end-to-end survival analysis, generating 
text-to-R-code that produced Kaplan-Meier curves statistically indistinguishable 
from ground truth. Consistent performance was demonstrated on a small validation 
cohort of 80 synthetic acute myeloid leukemia reports. Stress testing on data 
with simulated imperfections revealed a key role of a human-in-the-loop to 
resolve AI-flagged ambiguities. Furthermore, the LLM generated several correctly 
justified, biologically plausible, and potentially novel hypotheses from 
datasets up to 80 letters.
CONCLUSIONS: This feasibility study demonstrated that a frontier LLM (Gemini 2.5 
Pro) can successfully perform high-fidelity data extraction, multiconditional 
querying, and automated survival analysis directly from unstructured text. These 
results provide a foundational proof of concept for the JIT clinical analysis 
approach. However, these findings are confined to synthetic patients, and 
rigorous validation on real-world clinical data is an essential next step before 
clinical implementation can be considered.

© Peter May, Julian Greß, Christoph Seidel, Sebastian Sommer, Markus K Schuler, 
Sina Nokodian, Florian Schröder, Johannes Jung. Originally published in JMIR 
Medical Informatics (https://medinform.jmir.org).

DOI: 10.2196/78332
PMCID: PMC12670046
PMID: 41328496 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: MKS and FS hold shares in 
Accessus Science Technologies GmbH. PM, JG, CS, and SS receive honoraria from 
Accessus Science Technologies GmbH. SN and JJ have no conflicts of interest 
related to this research.


26. J Med Internet Res. 2025 Dec 1;27:e84120. doi: 10.2196/84120.

Knowledge-Practice Performance Gap in Clinical Large Language Models: Systematic 
Review of 39 Benchmarks.

Gong EJ(1)(2)(3), Bang CS(1)(2)(3), Lee JJ(3)(4), Baik GH(1)(2).

Author information:
(1)Department of Internal Medicine, Hallym University College of Medicine, 
Chuncheon, Gangwon, Republic of Korea.
(2)Institute for Liver and Digestive Diseases, Hallym University, Chuncheon, 
Gangwon-do, Republic of Korea.
(3)Institute of New Frontier Research, Hallym University College of Medicine, 
Chuncheon, Gangwon, Republic of Korea.
(4)Department of Anesthesiology and Pain Medicine, Hallym University College of 
Medicine, Chuncheon, Gangwon, Republic of Korea.

BACKGROUND: The evaluation of large language models (LLMs) in medicine has 
undergone a shift from knowledge-based testing to practice-based assessment, 
representing an evolution in how we measure artificial intelligence readiness 
for clinical deployment. While LLMs now routinely exceed human performance on 
medical licensing examinations, their translation to clinical practice remains 
poorly characterized.
OBJECTIVE: This systematic review aims to categorize and analyze medical LLM 
benchmarks, examining performance patterns across different evaluation paradigms 
and identifying gaps in current assessment methodologies.
METHODS: The protocol was registered at PROSPERO (CRD420251139729). Four 
databases (MEDLINE/PubMed, Embase/Ovid, Cochrane Library, and arXiv) were 
searched from inception to August 31, 2025, using keywords related to clinical 
medicine benchmarks in LLMs. Studies were included if they (1) investigated 
clinical medicine benchmarks in LLMs, (2) were published in English, and (3) 
were available in full-text. Studies were excluded if they evaluated nonmedical 
domains or lacked benchmark validation. Methodological quality was assessed 
using the Mixed Methods Appraisal Tool (version 2018) by 2 independent reviewers 
(κ=0.91). Due to heterogeneity in evaluation metrics preventing meta-analysis, 
narrative synthesis was conducted using structured categorization of benchmark 
types.
RESULTS: From 3917 screened records, 39 medical LLM benchmarks were identified 
and categorized into 21 (54%) knowledge-based, 15 (38%) practice-based, and 3 
(8%) hybrid frameworks. These benchmarks collectively encompass over 2.3 million 
questions across 45 languages and 172 medical specialties. Traditional 
knowledge-based benchmarks show saturation with leading models achieving 84%-90% 
accuracy on USMLE (United States Medical Licensing Examination)-style 
examinations, approaching or exceeding average physician performance. However, 
practice-based assessments reveal performance challenges, with specific 
benchmarks showing varied results: DiagnosisArena 45.82% (95% CI 42.9%-48.8%), 
MedAgentBench 69.67% (95% CI 64.2%-74.6%), and HealthBench 60% (95% CI 
58.6%-61.3%) success rates, with practice-based benchmarks showing lower 
performance (45%-69%) compared to knowledge benchmarks (84%-90%). Task-specific 
analysis revealed differential performance patterns: factual retrieval 
maintained 85%-93% accuracy, clinical reasoning dropped to 50%-60%, diagnostic 
tasks achieved 45%-55% success, and safety assessment showed significant gaps at 
40%-50% accuracy despite being life-critical. Geographic representation spans 6 
continents with 18 (46%) benchmarks, incorporating non-English content. Quality 
assessment revealed 26% (10/39) of benchmarks had insufficient methodological 
reporting for complete evaluation.
CONCLUSIONS: This systematic review provides the first comprehensive analysis 
quantifying the significant "knowledge-practice gap" in medical artificial 
intelligence: high performance on knowledge-based examinations (84%-90%) does 
not translate to clinical competence (45%-69%), with safety assessments at 
40%-50%. Our findings provide quantitative evidence for regulators and health 
systems that examination scores are insufficient and misleading proxies for 
clinical readiness. This review concludes that autonomous deployment is not 
currently justifiable and that all evidence-based implementation strategies must 
mandate practice-oriented validation and robust human-in-the-loop oversight to 
ensure patient safety.
TRIAL REGISTRATION: PROSPERO CRD420251139729; 
https://www.crd.york.ac.uk/PROSPERO/view/CRD420251139729.

©Eun Jeong Gong, Chang Seok Bang, Jae Jun Lee, Gwang Ho Baik. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org), 
01.12.2025.

DOI: 10.2196/84120
PMCID: PMC12706444
PMID: 41325597 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


27. Artif Intell Med. 2026 Jan;171:103310. doi: 10.1016/j.artmed.2025.103310. Epub 
2025 Nov 22.

Building trustworthy large language model-driven generative recommender system 
for healthcare decision support: A scoping review of corpus sources, 
customization techniques, and evaluation frameworks.

Yang S(1), Jing M(1), Wang S(2), Huang Z(3), Wang J(3), Kou J(1), Shi M(1), Xia 
Z(4), Wei Q(5), Xing W(1), Hu Y(1), Zhu Z(6).

Author information:
(1)School of Nursing, Fudan University, Shanghai, China.
(2)School of Nursing, Dali University, Yunnan, China.
(3)School of Computer Science, Fudan University, Shanghai, China.
(4)School of Information Science and Engineering, East China University of 
Science and Technology, Shanghai, China.
(5)Peking Union Medical College Hospital, Beijing, 100730, China.
(6)School of Nursing, Fudan University, Shanghai, China. Electronic address: 
zhengzhu@fudan.edu.cn.

INTRODUCTION: Large Language Model-Driven Generative Recommender Systems 
(LLM-GRSs) are playing a growing role in healthcare, particularly in clinical 
question-answering. This study reviews their corpus sources, customization 
techniques, and evaluation metrics.
METHODS: We conducted a systematic search of PubMed, Embase, Scopus, and Web of 
Science for studies published between January 2021 and August 2025 that applied 
LLM-GRSs to deliver medical or healthcare information. Eligible studies included 
publications describing LLMs designed to emulate clinical decision-making by 
providing diagnostic or therapeutic recommendations through dialogue-based 
interfaces. Two reviewers independently screened studies and extracted data on 
corpus sources, model architectures, customization methods, and evaluation 
metrics.
RESULTS: A total of 61 articles were included. Corpus sources were grouped into 
clinical data (n = 25), literature (n = 34), open datasets (n = 37), and 
web-crawled data (n = 15), with many using multiple types. Most studies (n = 43) 
combined multiple approaches. Customization techniques included prompt 
engineering, retrieval-augmented generation and model fine-tuning. Twenty-four 
studies used a single customization technique, while 37 studies combined these 
methods during model development. The evaluation metrics were classified into 
three main domains: process metrics, usability metrics, and outcome metrics. The 
outcome metrics included both model-based and manual-assessed evaluations.
CONCLUSION: LLM-GRSs hold considerable promise in healthcare; however, their 
safety and reliability hinge on the use of evidence-based training corpora, 
transparent system design, and standardized evaluation protocols within 
real-world clinical environments.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.artmed.2025.103310
PMID: 41313967 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no conflicts of interest.


28. Int J Med Inform. 2026 Mar 1;207:106186. doi: 10.1016/j.ijmedinf.2025.106186. 
Epub 2025 Nov 26.

Healthbots for conducting clinical screening and remote monitoring with patient 
mood assessment: A scoping review.

Cesar Abrantes P(1), Netto AV(2), Kazuo Takahata A(3).

Author information:
(1)Universidade Federal do ABC (UFABC), Santo André, SP, Brazil. Electronic 
address: paulo.abrantes@ufabc.edu.br.
(2)Universidade Federal de São Paulo (UNIFESP), São Paulo, SP, Brazil.
(3)Universidade Federal do ABC (UFABC), Santo André, SP, Brazil.

BACKGROUND: Patient mood assessment is key in managing chronic diseases but is 
often overlooked. Although conversational agents enhance telemonitoring and 
engagement, few healthbots incorporate automated mood analysis into routine 
clinical workflows or hybrid care. The rise of multimodal and large language 
models presents new opportunities to embed emotional assessment into daily 
healthcare interactions.
OBJECTIVE: This scoping review aims to identify existing AI-based healthbots 
that combine clinical screening and remote monitoring with mood assessment. Its 
secondary objectives are to (1) describe their technological architectures and 
AI methods, (2) examine validation and evaluation strategies, and (3) identify 
current research gaps.
METHODS: Following the Arksey and O'Malley framework and PRISMA-ScR guidelines, 
a comprehensive search was conducted across seven databases (ACM Digital 
Library, Embase, IEEE Xplore, PubMed, Scopus, SpringerLink, Web of Science), 
covering the period from January 2020 to December 2024. Studies were included if 
they presented empirical evidence of AI-based clinical screening with mood 
assessment. Ten studies met the inclusion criteria after screening and 
deduplication. Data were charted and synthesized based on key dimensions, 
including technological features, validation methods, and limitations.
RESULTS: Ten studies, mostly in mental health, used multimodal inputs (voice, 
facial expressions, text) with CNNs, LSTMs, NLP, and LLMs via web or mobile 
platforms. Some achieved high accuracy on public data and in cross-validation, 
but few conducted external or longitudinal validation in clinical settings. 
Integration with EHRs and standards was rarely reported. Limitations included 
small, homogeneous samples, limited generalizability, insufficient 
explainability, and privacy concerns.
CONCLUSIONS: AI-driven healthbots with mood assessment show promise but are 
still immature. While they can recognize emotions, few are validated in real 
clinical settings or integrated into workflows. Broader adoption depends on 
long-term validation, explainable, bias-aware algorithms, EHR interoperability, 
and ethical standards. Progress requires collaboration among technical, 
clinical, and policy experts to ensure the safe and equitable use.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106186
PMID: 41308276 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


29. Front Digit Health. 2025 Nov 6;7:1644041. doi: 10.3389/fdgth.2025.1644041. 
eCollection 2025.

Artificial intelligence in healthcare: applications, challenges, and future 
directions. A narrative review informed by international, multidisciplinary 
expertise.

Mohajer-Bastami A(#)(1), Moin S(#)(2), Ahmad S(#)(3), Ahmed AR(4), Pouwels 
S(5)(6), Hajibandeh S(7), Yang W(8), Parmar C(9)(10)(11), Kermansaravi M(12), 
Khalil M(13), Khalid AW(14), Khamise A(14), Rawaf D(15), Hosseini F(16), Agarwal 
A(17), Lala A(17), Ahmed S(18), Patel B(18), Fyntanidou B(19), Egan R(3), 
Mougiakakou SG(20), Jakob DA(21), Ribordy V(22), Hautz WE(21), Exadaktylos 
AK(21).

Author information:
(1)Brompton Primary Care Network, London, United Kingdom.
(2)Department of General Surgery, East Surrey Hospital, London, United Kingdom.
(3)Department of Surgery, Health Education and Improvement Wales (HEIW), Wales, 
United Kingdom.
(4)Department of Surgery, Imperial College London, London, United Kingdom.
(5)Department of Surgery, Bielefeld University-Campus Detmold, Detmold, Germany.
(6)Department of Intensive Care Medicine, Elisabeth-Tweesteden Hospital, 
Tilburg, Netherlands.
(7)Department of General Surgery, Morriston Hospital, Swansea, United Kingdom.
(8)Department of Surgery, The First Affiliated Hospital of Jinan University, 
Guangzhou, China.
(9)Department of Surgery, Whittington Hospital, London, United Kingdom.
(10)Department of Surgery, University College London, London, United Kingdom.
(11)Department of Surgery, Apollo Hospitals, Telengana, India.
(12)Department of Surgery, Division of Minimally Invasive and Bariatric Surgery, 
Hazrat-e Fatemeh Hospital, School of Medicine, Iran University of Medical 
Sciences, Tehran, Iran.
(13)St James University Hospital, Leeds, United Kingdom.
(14)School of Medicine, University of Buckingham, Buckingham, United Kingdom.
(15)WHO Collaborating Centre for Public Health Education and Training, Imperial 
College London, London, United Kingdom.
(16)Kingsmill Hospital, Nottingham, United Kingdom.
(17)Department of General Surgery, Betsi Cadwaladr University Health Board, 
Wales, United Kingdom.
(18)Bart's Health NHS Trust, London, United Kingdom.
(19)Director, University Emergency Department, Aristotle University of 
Thessaloniki, Thessaloniki, Greece.
(20)ARTORG Center for Biomedical Engineering Research, AI in Health and 
Nutrition, University of Bern, Bern, Switzerland.
(21)Department of Emergency Medicine, Inselspital University Hospital of Bern, 
Bern, Switzerland.
(22)Department of Emergency Medicine, HFR Fribourg-Cantonal Hospital, 
Villars-sur-Glâne, Switzerland.
(#)Contributed equally

OBJECTIVES: This narrative review evaluates the role of artificial intelligence 
(AI) in healthcare, summarizing its historical evolution, current applications 
across medical and surgical specialties, and implications for allied health 
professions and biomedical research.
METHODS: We conducted a structured literature search in Ovid MEDLINE (2018-2025) 
using terms related to AI, machine learning, deep learning, large language 
models, generative AI, and healthcare applications. Priority was given to 
peer-reviewed articles providing novel insights, multidisciplinary perspectives, 
and coverage of underrepresented domains.
KEY FINDINGS: AI is increasingly applied to diagnostics, surgical navigation, 
risk prediction, and personalized medicine. It also holds promise in allied 
health, drug discovery, genomics, and clinical trial optimization. However, 
adoption remains limited by challenges including bias, interpretability, legal 
frameworks, and uneven global access.
CONTRIBUTIONS: This review highlights underexplored areas such as generative AI 
and allied health professions, providing an integrated multidisciplinary 
perspective.
CONCLUSIONS: With careful regulation, clinician-led design, and global equity 
considerations, AI can augment healthcare delivery and research. Future work 
must focus on robust validation, responsible implementation, and expanding 
education in digital medicine.

© 2025 Mohajer-Bastami, Moin, Ahmad, Ahmed, Pouwels, Hajibandeh, Yang, Parmar, 
Kermansaravi, Khalil, Khalid, Khamise, Rawaf, Hosseini, Agarwal, Lala, Ahmed, 
Patel, Fyntanidou, Egan, Mougiakakou, Jakob, Ribordy, Hautz and Exadaktylos.

DOI: 10.3389/fdgth.2025.1644041
PMCID: PMC12645148
PMID: 41306935

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


30. Int J Mol Sci. 2025 Nov 7;26(22):10829. doi: 10.3390/ijms262210829.

A Scoping Review of Neurotoxic and Behavioral Outcomes Following Polychlorinated 
Biphenyl (PCB) Exposure in Post-Weaned Rodents.

Breese NM(1)(2), Heim SG(1), Samuelson RJ(3), Lehmler HJ(1)(2).

Author information:
(1)Department of Occupational and Environmental Health, College of Public 
Health, The University of Iowa, Iowa City, IA 52242, USA.
(2)Interdisciplinary Graduate Program in Human Toxicology, The University of 
Iowa, Iowa City, IA 52242, USA.
(3)Hardin Library for the Health Sciences, University of Iowa Libraries, The 
University of Iowa, Iowa City, IA 52242, USA.

Polychlorinated biphenyls (PCBs) are persistent organic pollutants associated 
with neurodevelopmental toxicity, yet the effects of exposure during adolescence 
and adulthood remain underexplored. This scoping review evaluates the neurotoxic 
outcomes of post-weaning PCB exposure in rodent models. A comprehensive 
literature search was conducted across PubMed, Embase, and Scopus. Studies were 
screened according to PRISMA guidelines. Articles were included if they reported 
neurotoxic or behavioral outcomes in mice or rats exposed to PCBs during 
post-weaning stages. Thirty-five studies met the inclusion criteria, 
encompassing a variety of PCB congeners and mixtures administered via oral, 
inhalation, or intraperitoneal routes. Reported neurotoxic outcomes included 
histological and morphological brain changes, oxidative stress, disrupted 
calcium signaling, altered neurotransmitter systems, apoptosis, and gene 
expression alterations. These outcomes were assessed using diverse 
methodological approaches, including immunohistochemistry, biochemical assays, 
and gene expression profiling. Behavioral outcomes affected by PCB exposure 
included locomotion, anxiety-like behavior, learning and memory, motor 
coordination, and cognitive flexibility. Effects were often exposure-specific 
and sex-dependent, with limited female-focused studies and integrative 
molecular-behavioral assessments. These findings highlight the broad neurotoxic 
potential of PCBs following adolescent or adult exposure and underscore the need 
for further mechanistic, sex-specific research to inform health risk assessment 
and regulatory policy.

DOI: 10.3390/ijms262210829
PMCID: PMC12652203
PMID: 41303318 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest. 
The funders had no role in the design of the study; in the collection, analyses, 
or interpretation of data; in the writing of the manuscript; or in the decision 
to publish the results.


31. BMC Med Inform Decis Mak. 2025 Nov 24;25(1):427. doi: 
10.1186/s12911-025-03255-6.

Artificial intelligence in polycystic ovary syndrome: a systematic review of 
diagnostic and predictive applications.

Ghaderzadeh M(1), Garavand A(2), Salehnasab C(3).

Author information:
(1)Boukan Faculty of Medical Sciences, Urmia University of Medical Sciences, 
Urmia, Iran.
(2)School of Allied Medical Sciences, Lorestan University of Medical Sciences, 
Khorramabad, Iran.
(3)Social Determinants of Health Research Center, Yasuj University of Medical 
Sciences, Yasuj, Iran. cirruse.salehnasab@gmail.com.

BACKGROUND: Polycystic ovary syndrome (PCOS) is one of the most common endocrine 
disorders, affecting 8–13% of women of reproductive age. Its heterogeneous 
presentation and the variability of diagnostic criteria make accurate diagnosis 
and effective management challenging. Artificial intelligence (AI) methods, 
including machine learning (ML), deep learning (DL), explainable AI (XAI), and 
large language models (LLMs), have recently emerged as promising approaches to 
address these gaps.
OBJECTIVE: This systematic review aimed to provide a comprehensive synthesis of 
AI applications in PCOS, with emphasis on diagnostic performance, biomarker 
discovery, risk prediction, clinical decision support, model interpretability, 
and the emerging use of generative AI.
METHODS: Following PRISMA 2020 guidelines, PubMed, Scopus, and Web of Science 
were searched from inception to March 2025. Eligible studies applied AI 
techniques to PCOS and reported at least one performance metric. Two reviewers 
independently screened and extracted data, with quality appraisal conducted 
using QUADAS-2 and ROBIS. Given the heterogeneity of designs and outcomes, 
findings were narratively synthesized across imaging, clinical/EHR, and 
biomarker/-omics domains.
RESULTS: From 662 retrieved records, 80 studies met the inclusion criteria. 
CNN-based models dominated imaging applications, with accuracies often exceeding 
95% and occasionally reaching 98–99%. Supervised ML approaches, particularly 
random forests and support vector machines, achieved consistent high performance 
in clinical and biochemical datasets. Omics-based studies revealed novel 
biomarkers such as HDDC3, SDC2, MAP1LC3A, and OVGP1. However, only about 
one-quarter of studies applied XAI methods, limiting transparency and clinical 
trust. Early evaluations of LLMs suggested potential for patient education and 
decision support but highlighted risks of bias, hallucination, and lack of 
domain-specific training. Key limitations across studies included small sample 
sizes, class imbalance, methodological heterogeneity, and limited external 
validation.
CONCLUSIONS: AI offers substantial opportunities to advance PCOS diagnosis and 
prediction by integrating multimodal data and reducing diagnostic subjectivity. 
Yet its clinical adoption is constrained by interpretability gaps and 
insufficient validation. Future priorities include large multicenter studies, 
standardized reporting, systematic use of XAI, and careful evaluation of LLMs to 
ensure safe, equitable, and clinically meaningful integration into PCOS care.
SUPPLEMENTARY INFORMATION: The online version contains supplementary material 
available at 10.1186/s12911-025-03255-6.

AI performance Biomarker discovery Explainability Large language models Risk of 
bias Research priorities Clinical implications CNN-based imaging models and 
supervised ML classifiers consistently outperformed traditional diagnostic 
criteria, reducing subjectivity in ultrasound and clinical assessment. 
Omics-driven studies identified novel candidate genes (HDDC3, SDC2, MAP1LC3A, 
OVGP1) with potential for risk stratification. Only ~25% of studies applied XAI 
methods (SHAP, LIME, Grad-CAM). Where used, these improved interpretability and 
clinician confidence, but most models remained opaque. LLMs (ChatGPT, BERT, 
Gemini) are emerging tools for patient communication, clinical note 
summarization, and literature synthesis, but raise concerns about hallucination, 
bias, and lack of domain-specific training. QUADAS-2 and ROBIS assessments 
revealed frequent issues with patient selection, dataset representativeness, and 
lack of external validation. Kaggle datasets were overused, reducing 
generalizability. Future work should focus on multicenter collaborations, 
multimodal integration, routine incorporation of XAI, and robust evaluation of 
LLMs. AI has the potential to enhance early diagnosis and personalized 
management of PCOS, but adoption will depend on reproducibility, transparency, 
and clinician trust rather than accuracy alone.
SUPPLEMENTARY INFORMATION: The online version contains supplementary material 
available at 10.1186/s12911-025-03255-6.

DOI: 10.1186/s12911-025-03255-6
PMCID: PMC12642037
PMID: 41286838

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This review was conducted in strict accordance with the principles 
of the Declaration of Helsinki and ethical standards for research integrity and 
transparency. The study protocol was reviewed and approved by the Iran National 
Committee for Ethics in Biomedical Research (Approval Code: 
IR.YUMS.REC.1402.099). As this article is a systematic review and does not 
involve direct human participation, no individual consent was required. Consent 
for publication: Not applicable. Competing interests: The authors declare no 
competing interests.


32. J Am Med Inform Assoc. 2026 Feb 1;33(2):509-520. doi: 10.1093/jamia/ocaf206.

Gaps in artificial intelligence research for rural health in the United States: 
a scoping review.

Brown KE(1), Davis SE(1).

Author information:
(1)Department of Biomedical Informatics, Vanderbilt University Medical Center, 
Nashville, TN 37203, United States.

Update of
    medRxiv. 2025 Jun 27:2025.06.26.25330361. doi: 10.1101/2025.06.26.25330361.

OBJECTIVE: Artificial intelligence (AI) has impacted healthcare at urban and 
academic medical centers in the US. There are concerns, however, that the 
promise of AI may not be realized in rural communities. This scoping review aims 
to determine the extent of AI research in the rural US.
MATERIALS AND METHODS: We conducted a scoping review following the PRISMA 
guidelines. We included peer-reviewed, original research studies indexed in 
PubMed, Embase, and WebOfScience after January 1, 2010 and through April 29, 
2025. Studies were required to discuss the development, implementation, or 
evaluation of AI tools in rural US healthcare, including frameworks that help 
facilitate AI development (eg, data warehouses).
RESULTS: Our search strategy found 26 studies meeting inclusion criteria after 
full text screening with 14 papers discussing predictive AI models and 12 papers 
discussing data or research infrastructure. AI models most often targeted 
resource allocation and distribution. Few studies explored model deployment and 
impact. Half noted the lack of data and analytic resources as a limitation. None 
of the studies discussed examples of generative AI being trained, evaluated, or 
deployed in a rural setting.
DISCUSSION: Practical limitations may be influencing and limiting the types of 
AI models evaluated in the rural US. Validation of tools in the rural US was 
underwhelming.
CONCLUSION: With few studies moving beyond AI model design and development 
stages, there are clear gaps in our understanding of how to reliably validate, 
deploy, and sustain AI models in rural settings to advance health in all 
communities.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf206
PMCID: PMC12844595
PMID: 41285144 [Indexed for MEDLINE]

Conflict of interest statement: All authors declare no financial or 
non-financial competing interests.


33. JMIR Med Inform. 2025 Nov 24;13:e76326. doi: 10.2196/76326.

Large Language Models in Critical Care Medicine: Scoping Review.

Shi T(1)(2)(3)(4), Ma J(5), Yu Z(6), Xu H(1)(4), Yang R(1)(2)(3)(4), Xiong M(7), 
Xiao M(1)(2)(3)(4), Li Y(8), Zhao H(9), Kong G(1)(2)(3)(4).

Author information:
(1)National Institute of Health Data Science, Peking University, Beijing, China.
(2)Institute for Artificial Intelligence, Peking University, Beijing, China.
(3)Institute of Medical Technology, Peking University Health Science Center, 
Beijing, China.
(4)Advanced Institute of Information Technology, Peking University, Hangzhou, 
China.
(5)Peking University Third Hospital, Beijing, China.
(6)Department of Computer Science, University of Liverpool, Liverpool, United 
Kingdom.
(7)Johns Hopkins University School of Medicine, Baltimore, MD, United States.
(8)Fielding School of Public Health, University of California, Los Angeles, Los 
Angeles, CA, United States.
(9)Department of Critical Care Medicine, Peking University People's Hospital, 
Beijing, China.

BACKGROUND: With the rapid development of artificial intelligence, large 
language models (LLMs) have shown strong capabilities in natural language 
understanding, reasoning, and generation, attracting much research interest in 
applying LLMs to health and medicine. Critical care medicine (CCM) provides 
diagnosis and treatment for patients with critical illness who often require 
intensive monitoring and interventions in intensive care units (ICUs). Whether 
LLMs can be applied to CCM, and whether they can operate as ICU experts in 
assisting clinical decision-making rather than "stochastic parrots," remains 
uncertain.
OBJECTIVE: This scoping review aims to provide a panoramic portrait of the 
application of LLMs in CCM, identifying the advantages, challenges, and future 
potential of LLMs in this field.
METHODS: This study was conducted in accordance with the PRISMA-ScR (Preferred 
Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping 
Reviews) guidelines. Literature was searched across 7 databases, including 
PubMed, Embase, Scopus, Web of Science, CINAHL, IEEE Xplore, and ACM Digital 
Library, from the first available paper to August 22, 2025.
RESULTS: From an initial 2342 retrieved papers, 41 were selected for final 
review. LLMs played an important role in CCM through the following 3 main 
channels: clinical decision support, medical documentation and reporting, and 
medical education and doctor-patient communication. Compared to traditional 
artificial intelligence models, LLMs have advantages in handling unstructured 
data and do not require manual feature engineering. Meanwhile, applying LLMs to 
CCM has faced challenges, including hallucinations and poor interpretability, 
sensitivity to prompts, bias and alignment challenges, and privacy and ethical 
issues.
CONCLUSIONS: Although LLMs are not yet ICU experts, they have the potential to 
become valuable tools in CCM, helping to improve patient outcomes and optimize 
health care delivery. Future research should enhance model reliability and 
interpretability, improve model training and deployment scalability, integrate 
up-to-date medical knowledge, and strengthen privacy and ethical guidelines, 
paving the way for LLMs to fully realize their impact in critical care.
TRIAL REGISTRATION: OSF Registries yn328; https://osf.io/yn328/.

©Tongyue Shi, Jun Ma, Zihan Yu, Haowei Xu, Rongxin Yang, Minqi Xiong, Meirong 
Xiao, Yilin Li, Huiying Zhao, Guilan Kong. Originally published in JMIR Medical 
Informatics (https://medinform.jmir.org), 24.11.2025.

DOI: 10.2196/76326
PMCID: PMC12778902
PMID: 41284992 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


34. J Med Internet Res. 2025 Nov 21;27:e76571. doi: 10.2196/76571.

Considerations for Patient Privacy of Large Language Models in Health Care: 
Scoping Review.

Zhong X(#)(1)(2), Li S(#)(1)(2), Chen Z(3), Ge L(4)(5)(6), Yu D(7), Wang 
S(1)(2), You L(#)(1)(2), Shang H(#)(2)(8).

Author information:
(1)Dongzhimen Hospital, Beijing University of Chinese Medicine, Beijing, null, 
China.
(2)Key Laboratory of Chinese Internal Medicine of the Ministry of Education, 
Beijing University of Chinese Medicine, Beijing, null, China.
(3)Institute of Basic Research in Clinical Medicine, China Academy of Chinese 
Medical Sciences, Beijing, China.
(4)Department of Health Policy and Health Management, School of Public Health, 
Lanzhou University, Lanzhou, China.
(5)Evidence-Based Social Science Research Center, School of Public Health, 
Lanzhou University, Lanzhou, China.
(6)WHO Collaborating Center for Guideline Implementation and Knowledge 
Translation, Lanzhou, China.
(7)Second Clinical College of Guangzhou University of Chinese Medicine, 
Guangzhou, China.
(8)Dongfang Hospital, Beijing University of Chinese Medicine, Beijing, China.
(#)Contributed equally

BACKGROUND: The application of large language models (LLMs) in health care holds 
significant potential for enhancing patient care and advancing medical research. 
However, the protection of patient privacy remains a critical issue, especially 
when handling patient health information (PHI).
OBJECTIVE: This scoping review aims to evaluate the adequacy of current 
approaches and identify areas in need of improvement to ensure robust patient 
privacy protection in the existing studies about PHI-LLMs within the health care 
domain.
METHODS: A search of the literature published from January 1, 2022, to July 20, 
2025, was performed on July 20, 2025, using 2 databases (PubMed and Embase). 
This scoping review focused on the following three research questions: (1) What 
studies on the development and application of LLMs using PHI currently exist 
within the health care domain? (2) What patient privacy considerations are 
addressed in existing PHI-LLMs research, and are these measures sufficient? (3) 
How can future research on the development and application of LLMs using PHI 
better protect patient privacy? Studies were included if they focused on the 
development and application of LLMs within health care using PHI, encompassing 
activities such as model construction, fine-tuning, optimization, testing, and 
performance comparison. Eligible literature comprised original research articles 
written in English. Conversely, studies were excluded if they used publicly 
available datasets, under the assumption that such data have been adequately 
deidentified. Additionally, non-English publications, reviews, abstracts, 
incomplete reports, and preprints were excluded from the review due to the lack 
of rigorous peer review.
RESULTS: This study systematically identified 9823 studies on PHI-LLM and 
included 464 studies published between 2022 and 2025. Among the 464 studies, (1) 
a small number of studies neglected ethical review (n=45, 9.7%) and patient 
informed consent (n=148, 31.9%) during the research process, (2) more than a 
third of the studies (n=178, 38.4%) failed to report whether to implement 
effective measures to protect PHI, and (3) there was a significant lack of 
transparency and comprehensive detail in anonymization and deidentification 
methods.
CONCLUSIONS: We propose comprehensive recommendations across 3 phases-study 
design, implementation, and reporting-to strengthen patient privacy protection 
and transparency in PHI-LLM. This study emphasizes the urgent need for the 
development of stricter regulatory frameworks and the adoption of advanced 
privacy protection technologies to effectively safeguard PHI. It is anticipated 
that future applications of LLMs in the health care field will achieve a balance 
between innovation and robust patient privacy protection, thereby enhancing 
ethical standards and scientific credibility.

©Xiaoying Zhong, Siyi Li, Zhao Chen, Long Ge, Dongdong Yu, Shijia Wang, 
Liangzhen You, Hongcai Shang. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org), 21.11.2025.

DOI: 10.2196/76571
PMCID: PMC12680930
PMID: 41269747 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


35. JMIR Form Res. 2025 Nov 20;9:e73822. doi: 10.2196/73822.

Identifying Biomedical Entities for Datasets in Scientific Articles: 4-Step 
Cache-Augmented Generation Approach Using GPT-4o and PubTator 3.0.

Giuliani C(1), Benadi G(1), Engel F(1), Werner J(1), Watter M(1), Schwarzer 
G(1), Groß O(2)(3), Zeiser R(3)(4), Binder H(1)(3), Kaier K(1)(3).

Author information:
(1)Institute of Medical Biometry and Statistics, Medical Faculty and Medical 
Center, University of Freiburg, Stefan-Meier-Str. 26, Freiburg, 79104, Germany, 
49 076127083739.
(2)Institute of Neuropathology, Medical Faculty and Medical Center, University 
of Freiburg, Freiburg, Germany.
(3)Center for Integrative Biological Signaling Studies, University of Freiburg, 
Freiburg, Germany.
(4)Department of Medicine I, Medical Faculty and Medical Center, University of 
Freiburg, Freiburg, Germany.

BACKGROUND: The accurate extraction of biomedical entities in scientific 
articles is essential for effective metadata annotation of research datasets, 
ensuring data findability, accessibility, interoperability, and reusability in 
collaborative research.
OBJECTIVE: This study aimed to introduce a novel 4-step cache-augmented 
generation approach to identify biomedical entities for an automated metadata 
annotation of datasets, leveraging GPT-4o and PubTator 3.0.
METHODS: The method integrates four steps: (1) generation of candidate entities 
using GPT-4o, (2) validation via PubTator 3.0, (3) term extraction based on a 
metadata schema developed for the specific research area, and (4) a combined 
evaluation of PubTator-validated and schema-related terms. Applied to 23 
articles published in the context of the Collaborative Research Center 
OncoEscape, the process was validated through supervised, face-to-face 
interviews with article authors, allowing an assessment of annotation precision 
using random-effects meta-analysis.
RESULTS: The approach yielded a mean of 19.6 schema-related and 6.7 
PubTator-validated biomedical entities per article. Within the study's specific 
context, the overall annotation precision was 98% (95% CI 94%-100%), with most 
prediction errors concentrated in articles outside the primary basic research 
domain of the schema. In a subsample (n=20), available supplemental material was 
included in the prediction process, but it did not improve precision (98%, 95% 
CI 95%-100%). Moreover, the mean number of schema-related entities was 20.1 
(P=.56) and the mean number of PubTator-validated entities was 6.7 (P=.68); 
these values did not increase with the additional information provided in the 
supplement.
CONCLUSIONS: This study highlights the potential of large language 
model-supported metadata annotation. The findings underscore the practical 
feasibility of full-text analysis and suggest its potential for integration into 
routine workflows for biomedical metadata generation.

© Claudia Giuliani, Gita Benadi, Felix Engel, Jonas Werner, Manuel Watter, Guido 
Schwarzer, Olaf Groß, Robert Zeiser, Harald Binder, Klaus Kaier. Originally 
published in JMIR Formative Research (https://formative.jmir.org).

DOI: 10.2196/73822
PMCID: PMC12633840
PMID: 41264807 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: OG is a coinventor on 
patent applications for immunomodulators and co-founder of EMUNO Therapeutics, a 
company developing drugs that control immunity to address unmet clinical needs. 
None of the drug candidates in patenting or development were used in this study.


36. J Biomed Inform. 2025 Dec;172:104952. doi: 10.1016/j.jbi.2025.104952. Epub 2025 
Nov 14.

SemNovel - A new approach to detecting semantic novelty of biomedical 
publications using embeddings of large language models.

Peng X(1), Xie Y(2), He H(1), Ondov B(1), Raja K(1), Liu Q(2), Mei Q(3), Xu 
H(4).

Author information:
(1)Department of Biomedical Informatics and Data Science, School of Medicine, 
Yale University, 100 College St, New Haven, CT 06510, USA.
(2)School of Information, University of Michigan, 500 S. State St, Ann Arbor, MI 
48109, USA.
(3)School of Information, University of Michigan, 500 S. State St, Ann Arbor, MI 
48109, USA. Electronic address: qmei@umich.edu.
(4)Department of Biomedical Informatics and Data Science, School of Medicine, 
Yale University, 100 College St, New Haven, CT 06510, USA. Electronic address: 
hua.xu@yale.edu.

OBJECTIVE: The rapid growth of scientific literature necessitates robust methods 
to identify novel contributions. However, there is currently no 
widely-recognized measurement of novelty in biomedical research. Existing 
approaches typically quantify novelty using isolated article features, such as 
keywords, MeSH terms, or references, potentially losing important context and 
nuance from the semantic content of the text.
METHODS: We propose SemNovel, a semantic novelty detection framework that 
leverages embeddings from Large Language Models (LLMs) to capture richer 
semantic content. Specifically, we adopt LLM-embedder (BAAI/llm-embedder) for 
semantic universe construction, a unified embedding model that integrates 
Llama2-7B-Chat as its foundation and BGE base as the embedding backbone. We 
employ t-distributed Stochastic Neighbor Embedding (t-SNE) for 2D visualization 
and project the entire PubMed library into a "semantic universe". A SemNovel 
score is calculated for each article based on its distance from prior 
publications. We validated SemNovel's effectiveness through its correlation with 
future research impact and its ability to distinguish groundbreaking studies. We 
further explored its potential for analyzing trends in research trajectories and 
interdisciplinary collaboration. To enhance usability, we developed an 
interactive interface for users to analyze SemNovel scores.
RESULTS: The SemNovel score exhibited a positive correlation with future 
research impact, as measured by citation counts (ρ = 0.1782, p < 0.001, Spearman 
rank correlation), independent of factors such as journal impact factors (JIFs), 
publication years, and author counts, and outperformed previous semantic novelty 
indicators. It effectively identified highly novel papers, including Nobel 
Prize-winning studies (p < 0.001, Kolmogorov-Smirnov test). SemNovel also 
revealed trends in the evolution of scientific research, exemplified in the 
PD-1/PD-L1 field, and underscored the role of interdisciplinary collaboration in 
enhancing biomedical research novelty.
CONCLUSION: SemNovel represents a scalable and robust method for quantifying 
semantic novelty in biomedical literature. It provides a powerful tool for 
uncovering groundbreaking research, tracking scientific progress, and analyzing 
trends in innovation.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104952
PMID: 41242670 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


37. J Med Internet Res. 2025 Nov 13;27:e79091. doi: 10.2196/79091.

Embracing the Future of Medical Education With Large Language Model-Based 
Virtual Patients: Scoping Review.

Zeng J(#)(1)(2), Qi W(#)(1)(2), Shen S(1), Liu X(1), Li S(1), Wang B(1), Dong 
C(1), Zhu X(1), Shi Y(1), Lou X(1), Wang B(1), Yao J(1), Jiang G(3), Zhang Q(4), 
Cao S(1)(2).

Author information:
(1)School of Public Health and Nursing, Hangzhou Normal University, Hangzhou, 
null, China.
(2)Zhejiang Provincial Research and Evaluation Center for Educational 
Modernization, Hangzhou, null, China.
(3)Department of Psychiatry and Neuropsychology and Alzheimer Center Limburg, 
School for Mental Health and Neuroscience (MHeNS), Maastricht University, 
Maastricht, The Netherlands.
(4)Department of Nursing, Zhejiang Provincial People's Hospital, Hangzhou, 
China.
(#)Contributed equally

BACKGROUND: In recent years, large language models (LLMs) have experienced rapid 
development. LLM-based virtual patients have begun to gain attention, offering 
new opportunities for simulations in medical education.
OBJECTIVE: This study aims to systematically analyze the current applications, 
research trends, and challenges of LLM-based virtual patients in medical 
education and to explore potential future directions for development.
METHODS: This study adheres to the PRISMA-ScR (Preferred Reporting Items for 
Systematic Reviews and Meta-Analyses extension for Scoping Reviews) guidelines. 
Five databases (Web of Science Core Collection, PubMed, IEEE Xplore, Embase, and 
Scopus) were searched from January 1, 2018, to June 24, 2025, to identify 
studies related to the application of LLM-based virtual patients in medical 
education. A comprehensive analysis of LLM-based virtual patients from research 
design to application and evaluation was conducted.
RESULTS: A total of 28 studies were included in this scoping review. Analysis 
revealed that 92.9% (26/28) of the studies were published in the past 2 years, 
indicating that LLM-based virtual patient research is still in its early stages. 
The research primarily focuses on medical training and spans a wide range of 
medical disciplines. When using LLMs, advanced technologies such as social 
robots, virtual reality, and mixed reality are used to present LLM-based virtual 
patients. Combining these technologies with various supplementary tools enhances 
the realism of LLM-based virtual patients and improves user interaction. The 
evaluation of LLM-based virtual patients mainly emphasizes user experience. 
However, evaluation methods lack standardization, and only 13% (3/23) of studies 
used validated tools in assessing LLM-based virtual patients, while only 21.7% 
(5/23) of studies objectively measured learning outcomes facilitated by 
LLM-based virtual patients. All included studies expressed a positive attitude 
toward LLM-based virtual patients; however, they overlook privacy and security 
considerations in practical applications.
CONCLUSIONS: LLM-based virtual patients hold significant innovation potential in 
medical education and are still in the early stages of development. They are 
primarily applied in medical training and show promise in communication skills 
training, although they cannot replace real-world interactions. Moreover, the 
heterogeneity of research designs, the absence of nonverbal cues in 
interactions, and concerns regarding privacy and security limit their broader 
implementation. Future research should focus on improving the reliability, 
realism, safety, and scientific efficacy of LLM-based virtual patients.
TRIAL REGISTRATION: Open Science Framework Registries 10.17605/OSF.IO/DMC9Q; 
https://osf.io/DMC9Q/overview.

©Jianwen Zeng, Wenhao Qi, Shiying Shen, Xin Liu, Sixie Li, Bing Wang, Chaoqun 
Dong, Xiaohong Zhu, Yankai Shi, Xiajing Lou, Bingsheng Wang, Jiani Yao, Guowei 
Jiang, Qiong Zhang, Shihua Cao. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org), 13.11.2025.

DOI: 10.2196/79091
PMCID: PMC12661241
PMID: 41232097 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


38. J Med Internet Res. 2025 Nov 7;27:e80289. doi: 10.2196/80289.

Nurses' Perspectives on Evidence Dissemination Barriers and Large Language 
Model-Based Support: Qualitative Study Using Focus Groups and Nominal Group 
Technique.

Ruan J(1), Tang Y(1), Wei Z(2)(3), Xing W(1)(4), Hu Y(1)(4).

Author information:
(1)School of Nursing, Fudan University, Shanghai, China.
(2)School of Data Science, Fudan University, Shanghai, China.
(3)Shanghai Innovation Institute, Shanghai, China.
(4)JBI Fudan University Centre for Evidence-Based Nursing, Shanghai, China.

BACKGROUND: Current evidence dissemination methods fall short of meeting 
clinical nurses' needs, hindering the implementation of evidence-based nursing 
practice. Large language models (LLMs), with their advanced natural language 
processing capabilities, offer potential as innovative tools to facilitate 
evidence dissemination. However, general-purpose LLMs typically lack 
domain-specific knowledge, are insufficient to support effective evidence 
dissemination in clinical contexts. It is essential to develop artificial 
intelligence tools tailored to nurses' needs and preferences to enhance evidence 
dissemination.
OBJECTIVE: The aim of this study is to identify the challenges and barriers 
clinical nurses face in disseminating evidence, examine their perspectives on 
the use of existing LLMs to support evidence dissemination, and explore their 
needs and preferences regarding an LLM-based nursing evidence question-answering 
system.
METHODS: This qualitative study used a combined method of focus group 
discussions and the nominal group technique (NGT). Using purposive sampling, 
nurses with diverse specialties, professional titles, and years of experience 
were recruited, resulting in a total of 22 clinical nurses who completed the 
entire study. A total of 2 focus group discussions were conducted online via 
Tencent Meeting between November and December 2024 to explore the challenges and 
barriers nurses face in disseminating evidence, as well as their perspectives on 
using existing LLMs to support evidence dissemination. The data were analyzed 
using qualitative content analysis following the approach of Graneheim and 
Lundman. Subsequently, the NGT was used between March and April 2025 to identify 
nurses' needs and preferences for the system to be developed. To overcome 
geographical constraints and participants' busy schedules, the NGT was conducted 
entirely online, using online questionnaires and WeChat groups. Overall, 2 
rounds of voting were conducted to determine the priority ranking of the 
functionalities.
RESULTS: The focus group yielded 3 main themes and 7 subthemes. Three main 
themes were identified as (1) pathways for evidence dissemination among nurses, 
(2) barriers that hinder the effective dissemination of evidence, and (3) 
advantages and limitations of using LLMs to support evidence dissemination. The 
limitations of current LLMs served as the foundation for nurses' subsequent 
reflections in the nominal group discussions on the desired functions of a newly 
developed LLM. The NGT sessions ultimately identified 9 desired functions. After 
prioritization, the top 3 ranked functions were evidence-based, high-quality 
question-answering, evidence source provision, and personalized evidence 
recommendation.
CONCLUSIONS: The current evidence dissemination process faces multiple barriers. 
LLMs hold promise as innovative tools to support evidence dissemination, but 
require further refinement. Clinical nurses have identified key functional 
needs, guiding the development of LLMs specifically tailored to clinical nursing 
practice.

©Junyi Ruan, Yimin Tang, Zhongyu Wei, Weijie Xing, Yan Hu. Originally published 
in the Journal of Medical Internet Research (https://www.jmir.org), 07.11.2025.

DOI: 10.2196/80289
PMCID: PMC12639338
PMID: 41202291 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


39. JMIR Med Inform. 2025 Nov 6;13:e73605. doi: 10.2196/73605.

Large Language Model Versus Manual Review for Clinical Data Curation in Breast 
Cancer: Retrospective Comparative Study.

Kang YJ(1), Lee H(2), Yi JP(1), Kim H(1), Yoon CI(3), Baek JM(4), Kim YS(5), 
Jeon YW(6), Rhu J(7), Lim SH(1), Choi H(1), Oh SJ(1).

Author information:
(1)Department of Surgery, College of Medicine, The Catholic University of Korea, 
Incheon St Mary's Hospital, 56, Dongsu-ro, Bupyeong-gu, Incheon, 21431, Republic 
of Korea, 01026383847.
(2)Department of AI Health Information Management, Yonsei University (Mirae), 
Wonju, Republic of Korea.
(3)Department of Surgery, College of Medicine, The Catholic University of Korea, 
Seoul St Mary's Hospital, Seoul, Republic of Korea.
(4)Department of Surgery, College of Medicine, The Catholic University of Korea, 
Yeouido St Mary's Hospital, Seoul, Republic of Korea.
(5)Department of Surgery, College of Medicine, The Catholic University of Korea, 
Uijeongbu St Mary's Hospital, Uijeongbu, Republic of Korea.
(6)Department of Surgery, College of Medicine, The Catholic University of Korea, 
St Vincent's Hospital, Suwon, Republic of Korea.
(7)Department of Surgery, College of Medicine, The Catholic University of Korea, 
Bucheon St Mary's Hospital, Bucheon, Republic of Korea.

BACKGROUND: Manual review of electronic health records for clinical research is 
labor-intensive and prone to reviewer-dependent variations. Large language 
models (LLMs) offer potential for automated clinical data extraction; however, 
their feasibility in surgical oncology remains underexplored.
OBJECTIVE: This study aimed to evaluate the feasibility and accuracy of 
LLM-based processing compared with manual physician review for extracting 
clinical data from breast cancer records.
METHODS: We conducted a retrospective comparative study analyzing breast cancer 
records from 5 academic hospitals (January 2019-December 2019). Two data 
extraction pathways were compared: (1) manual physician review with direct 
electronic health record access (group 1: 1366/3100, 44.06%) and (2) LLM-based 
processing using Claude 3.5 Sonnet (Anthropic) on deidentified data 
automatically extracted through a clinical data warehouse platform (group 2: 
1734/3100, 55.94%). The automated extraction system provided prestructured, 
deidentified data sheets organized by clinical domains, which were then 
processed by the LLM. The LLM prompt was developed through a 3-phase iterative 
process over 2 days. Primary outcomes included missing value rates, extraction 
accuracy, and concordance between groups. Secondary outcomes included comparison 
with the Korean Breast Cancer Society national registry data, processing time, 
and resource use. Validation involved 50 stratified random samples per group 
(900 data points each), assessed by 4 breast surgical oncologists. Statistical 
analysis included chi-square tests, 2-tailed t tests, Cohen κ, and intraclass 
correlation coefficients. The accuracy threshold was set at 90%.
RESULTS: The LLM achieved 90.8% (817) accuracy in validation analysis. Missing 
data patterns differed between groups: group 2 showed better lymph node 
documentation (missing: 152/1734, 8.76% vs 294/1366, 21.52%) but higher missing 
rates for cancer staging (211/1734, 12.17% vs 43/1366, 3.15%). Both groups 
demonstrated similar breast-conserving surgery rates (1107/1734, 63.84% vs 
868/1366, 63.54%). Processing efficiency differed substantially: LLM processing 
required 12 days with 2 physicians versus 7 months with 5 physicians for manual 
review, representing a 91% reduction in physician hours (96 h vs 1025 h). The 
LLM group captured significantly more survival events (41 vs 11; P=.002). Stage 
distribution in the LLM group aligned better with national registry data (Cramér 
V=0.03 vs 0.07). Application programming interface costs totaled US $260 for 
1734 cases (US $0.15 per case).
CONCLUSIONS: LLM-based curation of automatically extracted, deidentified 
clinical data demonstrated comparable effectiveness to manual physician review 
while reducing processing time by 95% and physician hours by 91%. This 2-step 
approach-automated data extraction followed by LLM curation-addresses both 
privacy concerns and efficiency needs. Despite limitations in integrating 
multiple clinical events, this methodology offers a scalable solution for 
clinical data extraction in oncology research. The 90.8% accuracy rate and 
superior capture of survival events suggest that combining automated data 
extraction systems with LLM processing can accelerate retrospective clinical 
research while maintaining data quality and patient privacy.

© Young-Joon Kang, Hocheol Lee, Jae Pak Yi, Hyobin Kim, Chang Ik Yoon, Jong Min 
Baek, Yong-seok Kim, Ye Won Jeon, Jiyoung Rhu, Su Hyun Lim, Hoon Choi, Se Jeong 
Oh. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org).

DOI: 10.2196/73605
PMCID: PMC12599480
PMID: 41197113 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


40. J Med Internet Res. 2025 Nov 3;27:e76296. doi: 10.2196/76296.

Effectiveness of Communication Competence in AI Conversational Agents for 
Health: Systematic Review and Meta-Analysis.

Qin J(1), Nan Y(2)(3), Li Z(4), Meng J(1).

Author information:
(1)School of Communication, The Ohio State University, 154 N Oval Mall, 
Columbus, OH, 43210, United States, 1 5173034870.
(2)Division of Infectious Diseases, Department of Medicine, Heersink School of 
Medicine, The University of Alabama at Birmingham, Birmingham, AL, United 
States.
(3)Center for AIDS Research, The University of Alabama at Birmingham, 
Birmingham, AL, United States.
(4)TH Chan School of Public Health, Harvard University, Boston, MA, United 
States.

BACKGROUND: With advancements in artificial intelligence and large language 
models, researchers and designers have increasingly focused on enhancing the 
conversational capacity of health-related conversational agents (CAs). 
Communication competence, a key concept in interpersonal communication 
influencing relational and health outcomes, has been extended to human-machine 
communication to emphasize the CAs' ability to demonstrate appropriate 
communicative behaviors in managing relationships with humans.
OBJECTIVE: This review aims to summarize the operationalization of communication 
competence in health CAs and assess its impact on 4 primary outcomes: users' 
evaluations of CA, use of CA, psychological outcomes, and health outcomes.
METHODS: A systematic literature search was conducted in 7 databases (ACM 
Digital Library, APA PsycInfo, Communication and Mass Media Complete, ProQuest 
Dissertations & Theses, Scopus, Web of Science Core Collection, and PubMed). 
Studies were included if they adopted experimental designs to manipulate CAs' 
communication competence in health-related conversations, recruited human 
participants, and reported at least 1 relevant outcome. Risk of bias was 
assessed using the revised Cochrane risk-of-bias tool. The systematic review 
summarized commonly used communication competence strategies. Three-level 
random-effects meta-analytic models were used to estimate pooled effect sizes 
for 4 primary outcomes. Moderator analyses were conducted to assess whether 
effect sizes varied across publication year, participants' average age, type of 
interaction with CAs, health topics, and publication outlet.
RESULTS: Of the 8309 identified papers, 31 independent experimental studies were 
included in the systematic review. Eleven strategies were identified to enhance 
CAs' communication competence: empathetic response, contingency, humor, small 
talk, emotional expressiveness, self-disclosure, personalization, social 
etiquette, explanation, open-ended questions, and partnership. Of the 31 
studies, 25 met the criteria for meta-analysis, which involved 4525 participants 
with a mean age of 29.7 (SD 9.2) years. The meta-analytic findings showed that 
communication competence has a significant small-to-medium effect on users' 
evaluations of CAs (Hedges g=0.45, 95% CI 0.24-0.66) and psychological outcomes 
(Hedges g=0.49, 95% CI 0.19-0.78). The effect sizes on the use of CA (Hedges 
g=0.11, 95% CI -0.05 to 0.26) and health outcomes (Hedges g=0.18, 95% CI -0.13 
to 0.50) are not significant. Moderator analyses showed that the effects remain 
stable across participants' age, type of interaction, and health topics.
CONCLUSIONS: This review highlights communication competence as a critical 
component in the design of health care CAs, particularly in improving users' 
evaluations and psychological outcomes. However, the limited number of studies 
examining health outcomes restricts the robustness of its effectiveness on this 
outcome. Future research is encouraged to directly evaluate the effects on 
tangible health outcomes.

© Jiaqi Qin, Yuanfeixue Nan, Zichao Li, Jingbo Meng. Originally published in the 
Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/76296
PMCID: PMC12582511
PMID: 41183242 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


41. J Biomed Inform. 2025 Dec;172:104949. doi: 10.1016/j.jbi.2025.104949. Epub 2025 
Nov 1.

Scalable scientific interest profiling using large language models.

Liang Y(1), Zhang G(2), Sun E(3), Idnay B(2), Fang Y(2), Chen F(2), Ta C(2), 
Peng Y(4), Weng C(5).

Author information:
(1)Tandon School of Engineering, New York University, Brooklyn, NY, USA.
(2)Department of Biomedical Informatics, Columbia University, New York, NY, USA.
(3)Henry Samueli School of Engineering and Applied Science, University of 
California, Los Angeles, CA, USA.
(4)Department of Population Health Sciences, Weill Cornell Medicine, New York, 
NY, USA. Electronic address: yip4002@med.cornell.edu.
(5)Department of Biomedical Informatics, Columbia University, New York, NY, USA. 
Electronic address: cw2384@cumc.columbia.edu.

Update of
    ArXiv. 2025 Aug 19:arXiv:2508.15834v1.

OBJECTIVE: Research profiles highlight scientists' research focus, enabling 
talent discovery and fostering collaborations, but they are often outdated. 
Automated, scalable methods are urgently needed to keep these profiles current.
METHODS: In this study, we design and evaluate two Large Language Models 
(LLMs)-based methods to generate scientific interest profiles-one summarizing 
researchers' PubMed abstracts and the other generating a summary using their 
publications' Medical Subject Headings (MeSH) terms-and compare these 
machine-generated profiles with researchers' self-summarized interests. We 
collected the titles, MeSH terms, and abstracts of PubMed publications for 595 
faculty members affiliated with Columbia University Irving Medical Center 
(CUIMC), for 167 of whom we obtained human-written online research profiles. 
Subsequently, GPT-4o-mini, a state-of-the-art LLM, was prompted to summarize 
each researcher's interests. Both manual and automated evaluations were 
conducted to characterize the similarities and differences between the 
machine-generated and self-written research profiles.
RESULTS: The similarity study showed low ROUGE-L, BLEU, and METEOR scores, 
reflecting little overlap between terminologies used in machine-generated and 
self-written profiles. BERTScore analysis revealed moderate semantic similarity 
between machine-generated and reference summaries (F1: 0.542 for MeSH-based, 
0.555 for abstract-based), despite low lexical overlap. In validation, 
paraphrased summaries achieved a higher F1 of 0.851. A further comparison 
between the original and paraphrased manually written summaries indicates the 
limitations of such metrics. Kullback-Leibler (KL) Divergence of term 
frequency-inverse document frequency (TF-IDF) values (8.56 and 8.58 for profiles 
derived from MeSH terms and abstracts, respectively) suggests that 
machine-generated summaries employ different keywords than human-written 
summaries. Manual reviews further showed that 77.78% rated the overall 
impression of MeSH-based profiling as "good" or "excellent," with readability 
receiving favorable ratings in 93.44% of cases, though granularity and factual 
accuracy varied. Overall, panel reviews favored 67.86% of machine-generated 
profiles derived from MeSH terms over those derived from abstracts.
CONCLUSION: LLMs promise to automate scientific interest profiling at scale. 
Profiles derived from MeSH terms have better readability than profiles derived 
from abstracts. Overall, machine-generated summaries differ from human-written 
ones in their choice of concepts, with the latter initiating more novel ideas.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104949
PMCID: PMC12705189
PMID: 41177243 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


42. J Med Internet Res. 2025 Oct 29;27:e79379. doi: 10.2196/79379.

Evaluating Web Retrieval-Assisted Large Language Models With and Without 
Whitelisting for Evidence-Based Neurology: Comparative Study.

Masanneck L(1), Epping PZ(1), Meuth SG(1), Pawlitzki M(1).

Author information:
(1)Department of Neurology, Medical Faculty and University Hospital Düsseldorf, 
Heinrich Heine University Düsseldorf, Dusseldorf, Germany.

BACKGROUND: Large language models (LLMs) coupled with real-time web retrieval 
are reshaping how clinicians and patients locate medical evidence, and as major 
search providers fuse LLMs into their interfaces, this hybrid approach might 
become the new "gateway" to the internet. However, open-web retrieval exposes 
models to nonprofessional sources, risking hallucinations and factual errors 
that might jeopardize evidence-based care.
OBJECTIVE: We aimed to quantify the impact of guideline-domain whitelisting on 
the answer quality of 3 publicly available Perplexity web-based 
retrieval-augmented generation (RAG) models and compare their performance using 
a purpose-built, biomedical literature RAG system (OpenEvidence).
METHODS: We applied a validated 130-item question set derived from the American 
Academy of Neurology (AAN) guidelines (65 factual and 65 case based). Perplexity 
Sonar, Sonar-Pro, and Sonar-Reasoning-Pro were each queried 4 times per question 
with open-web retrieval and again with retrieval restricted to aan.com and 
neurology.org ("whitelisted"). OpenEvidence was queried 4 times. Two 
neurologists, blinded to condition, scored each response (0=wrong, 1=inaccurate, 
and 2=correct); any disagreements that arose were resolved by a third 
neurologist. Ordinal logistic models were used to assess the influence of 
question type and source category (AAN or neurology vs nonprofessional) on 
accuracy.
RESULTS: From the 3640 LLM answers that were rated (interrater agreement: 
κ=0.86), correct-answer rates were as follows (open vs whitelisted, 
respectively): Sonar, 60% vs 78%, Sonar-Pro, 80% vs 88%, and 
Sonar-Reasoning-Pro, 81% vs 89%; for OpenEvidence, the correct-answer rate was 
82%. A Friedman test on modal scores across the 7 configurations was significant 
(χ26=73.7; P<.001). Whitelisting improved mean accuracy on the 0 to 2 scale by 
0.23 for Sonar (95% CI 0.12-0.34), 0.08 for Sonar-Pro (95% CI 0.01-0.16), and 
0.08 for Sonar-Reasoning-Pro (95% CI 0.02-0.13). Including ≥1 nonprofessional 
source halved the odds of a higher rating in Sonar (odds ratio [OR] 0.50, 95% CI 
0.37-0.66; P<.001), whereas citing an AAN or neurology document doubled it (OR 
2.18, 95% CI 1.64-2.89; P<.001). Furthermore, factual questions outperformed 
case vignettes across Perplexity models (ORs ranged from 1.95, 95% CI 1.28-2.98 
[Sonar + whitelisting] to 4.28, 95% CI 2.59-7.09 [Sonar-Reasoning-Pro]; all 
P<.01) but not for OpenEvidence (OR 1.44, 95% CI 0.92-2.27; P=.11).
CONCLUSIONS: Restricting retrieval to authoritative neurology domains yielded a 
clinically meaningful 8 to 18 percentage-point gain in correctness and halved 
output variability, upgrading a consumer search assistant to a 
decision-support-level tool that at least performed on par with a specialized 
literature engine. Lightweight source control is therefore a pragmatic safety 
lever for maintaining continuously updated, web-based RAG-augmented LLMs fit for 
evidence-based neurology.

©Lars Masanneck, Paula Zoe Epping, Sven G Meuth, Marc Pawlitzki. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org), 
29.10.2025.

DOI: 10.2196/79379
PMCID: PMC12612646
PMID: 41159599 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


43. J Med Internet Res. 2025 Oct 27;27:e76947. doi: 10.2196/76947.

Evaluating Large Language Models in Ophthalmology: Systematic Review.

Zhang Z(#)(1), Zhang H(#)(1), Pan Z(2), Bi Z(2), Wan Y(2), Song X(1), Fan X(1).

Author information:
(1)State Key Laboratory of Eye Health, Department of Ophthalmology, Shanghai 
Ninth People's Hospital, Shanghai Jiao Tong University School of Medicine, 
Shanghai, China.
(2)School of Computer Science and Technology, Huazhong University of Science and 
Technology, Wuhan, China.
(#)Contributed equally

BACKGROUND: Large language models (LLMs) have the potential to revolutionize 
ophthalmic care, but their evaluation practice remains fragmented. A systematic 
assessment is crucial to identify gaps and guide future evaluation practices and 
clinical integration.
OBJECTIVE: This study aims to map the current landscape of LLM evaluations in 
ophthalmology and explore whether performance synthesis is feasible for a common 
task.
METHODS: A comprehensive search of PubMed, Web of Science, Embase, and IEEE 
Xplore was conducted up to November 17, 2024 (no language limits). Eligible 
publications quantitatively assessed an existing or modified LLM on 
ophthalmology-related tasks. Studies without full-text availability or those 
focusing solely on vision-only models were excluded. Two reviewers screened 
studies and extracted data across 6 dimensions (evaluated LLM, data modality, 
ophthalmic subspecialty, medical task, evaluation dimension, and clinical 
alignment), and disagreements were resolved by a third reviewer. Descriptive 
statistics were analyzed and visualized using Python (with NumPy, Pandas, SciPy, 
and Matplotlib libraries). The Fisher exact test compared open- versus 
closed-source models. An exploratory random-effects meta-analysis (logit 
transformation; DerSimonian-Laird τ2) was performed for the diagnosis-making 
task; heterogeneity was reported with I2 and subgrouped by model, modality, and 
subspecialty.
RESULTS: Of the 817 identified records, 187 studies met the inclusion criteria. 
Closed-source LLMs dominated: 170 for ChatGPT, 58 for Gemini, and 32 for 
Copilot. Open-source LLMs appeared in only 25 (13.4%) of studies overall, but 
they appeared in 17 (77.3%) of evaluation-after-development studies, versus 8 
(4.8%) pure-evaluation studies (P<1×10-5). Evaluations were chiefly text-only 
(n=168); image-text tasks, despite the centrality of imaging, were used in 19 
studies. Subspecialty coverage was skewed toward comprehensive ophthalmology 
(n=72), retina and vitreous (n=32), and glaucoma (n=20). Refractive surgery, 
ocular pathology and oncology, and ophthalmic pharmacology each appeared in 3 or 
fewer studies. Medical query (n=86), standardized examination (n=41), and 
diagnosis making (n=29) emerged as the 3 predominant tasks, while research 
assistance (n=5), patient triaging (n=3), and disease prediction (n=3) received 
less attention. Accuracy was reported in most studies (n=176), whereas 
calibration and uncertainty were almost absent (n=5). Real-world patient data 
(n=45), human performance comparison (n=63), non‑English testing (n=24), and 
real-world deployment (n=4) were relatively absent. Exploratory meta-analysis 
pooled 28 diagnostic evaluations from 17 studies: overall accuracy was 0.594 
(95% CI 0.488-0.692) with extreme heterogeneity (I2=94.5%). Subgroups remained 
heterogeneous (I2>80%), and findings were inconsistent (eg, pooled 
GPT-3.5>GPT-4).
CONCLUSIONS: Evidence on LLM evaluations in ophthalmology is extensive but 
heterogeneous. Most studies have tested a few closed-source LLMs on text-based 
questions, leaving open-source systems, multimodal tasks, non-English contexts, 
and real-world deployment underexamined. High methodological variability 
precludes meaningful performance aggregation, as illustrated by the 
heterogeneous meta-analysis. Standardized, multimodal benchmarks and phased 
clinical validation pipelines are urgently needed before LLMs can be safely 
integrated into eye care workflows.

©Zili Zhang, Haiyang Zhang, Zhe Pan, Zhangqian Bi, Yao Wan, Xuefei Song, Xianqun 
Fan. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 27.10.2025.

DOI: 10.2196/76947
PMCID: PMC12603593
PMID: 41144954 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


44. J Med Internet Res. 2025 Oct 22;27:e74094. doi: 10.2196/74094.

Assessing Large Language Models in Building a Structured Dataset From AskDocs 
Subreddit Data: Methodological Study.

Snell Q(#)(1), Westhoff C(#)(1), Westhoff J(#)(2), Low E(#)(1), Hanson CL(#)(1), 
Tass ESN(#)(1).

Author information:
(1)Brigham Young University, 3361 TMCB, Provo, UT, 84602, United States, 1 
8014225098.
(2)University of Nevada, Reno, Reno, NV, United States.
(#)Contributed equally

BACKGROUND: In an era marked by a growing reliance on digital platforms for 
health care consultation, the subreddit r/AskDocs has emerged as a pivotal 
forum. However, the vast, unstructured nature of forum data presents a 
formidable challenge; the extraction and meaningful analysis of such data 
require advanced tools that can navigate the complexities of language and 
context inherent in user-generated content. The emergence of large language 
models (LLMs) offers new tools for the extraction of health-related content from 
unstructured text found in social media platforms such as Reddit.
OBJECTIVE: This methodological study aimed to evaluate the use of LLMs to 
systematically transform the rich, unstructured textual data from the AskDocs 
subreddit into a structured dataset, an approach that aligns more closely with 
human cognitive processes than traditional data extraction methods.
METHODS: Human annotators and LLMs were used to extract data from 2800 randomly 
sampled r/AskDocs subreddit posts. For human annotation, at least 2 medical 
students extracted demographic information, type of inquiry (diagnosis, symptom, 
or treatment), proxy relationship, chronic condition, health care consultation 
status, and primary focus topic. For LLM data extraction, specially engineered 
prompts were created using JavaScript Object Notation and few-shot prompting. 
Prompts were used to query several state-of-the-art LLMs (eg, Llama 3, Genna, 
and GPT). Cohen κ was calculated across all human annotators, with this dataset 
serving as the gold standard for comparison with LLM data extraction. A high 
degree of human annotator reliability was observed for the coding of demographic 
information. Lower reliability was seen in coding the health-related content of 
the posts.
RESULTS: The highest performance scores compared with the gold standard were 
achieved by Llama 3 70B with 7 few-shot prompt examples (average accuracy=87.4) 
and GPT-4 with 2 few-shot prompt examples (average accuracy=87.4). Llama 3 70B 
excelled in coding health-related content while GPT-4 performed better coding 
demographic content from unstructured posts.
CONCLUSIONS: LLMs performed comparably with human annotators in extracting 
demographic and health-related information from the AskDocs subreddit 
unstructured posts. This study validates the use of LLMs for analyzing digital 
health care communications and highlights their potential as reliable tools for 
understanding online behaviors and interactions, shifting toward more 
sophisticated methodologies in digital research and practice.

© Quinn Snell, Chase Westhoff, John Westhoff, Ethan Low, Carl L Hanson, E 
Shannon Neeley Tass. Originally published in the Journal of Medical Internet 
Research (https://www.jmir.org).

DOI: 10.2196/74094
PMCID: PMC12543290
PMID: 41124662 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


45. J Am Med Inform Assoc. 2026 Feb 1;33(2):359-370. doi: 10.1093/jamia/ocaf179.

AcuKG: a comprehensive knowledge graph for medical acupuncture.

Li Y(1)(2), Peng X(3), Peng S(4), Li J(5), Pei D(6), Zhang Q(7), Lu Y(4), Hu 
Y(8), Li F(5), Zhou L(1)(2), He Y(9), Tao C(5), Xu H(3), Hong N(3).

Author information:
(1)Department of Medicine, Harvard Medical School, Boston, MA 02115, United 
States.
(2)Division of General Internal Medicine and Primary Care, Department of 
Medicine, Brigham and Women's Hospital, Boston, MA 02115, United States.
(3)Department of Biomedical Informatics and Data Science, School of Medicine, 
Yale University, New Haven, CT 06510, United States.
(4)Institute of Information on Traditional Chinese Medicine, China Academy of 
Chinese Medical Sciences, Beijing 100010, China.
(5)Department of Artificial Intelligence and Informatics, Mayo Clinic, 
Jacksonville, FL 32224, United States.
(6)The University of Texas MD Anderson Cancer Center, Houston, TX 77030, United 
States.
(7)National Science Library, Chinese Academy of Sciences, Beijing 100190, China.
(8)McWilliams School of Biomedical Informatics, The University of Texas Health 
Science Center at Houston, Houston, TX 77030, United States.
(9)Unit for Laboratory Animal Medicine, Center for Computational Medicine and 
Bioinformatics, Department of Learning Health Science, University of Michigan 
Medical School, Ann Arbor, MI 48109, United States.

BACKGROUND: Acupuncture, a key modality in traditional Chinese medicine, is 
gaining global recognition as a complementary therapy and a subject of 
increasing scientific interest. However, fragmented and unstructured acupuncture 
knowledge spread across diverse sources poses challenges for semantic retrieval, 
reasoning, and in-depth analysis. To address this gap, we developed AcuKG, a 
comprehensive knowledge graph that systematically organizes acupuncture-related 
knowledge to support sharing, discovery, and artificial intelligence-driven 
innovation in the field.
METHODS: AcuKG integrates data from multiple sources, including online 
resources, guidelines, PubMed literature, ClinicalTrials.gov, and multiple 
ontologies (SNOMED CT, UBERON, and MeSH). We employed entity recognition, 
relation extraction, and ontology mapping to establish AcuKG, with 
human-in-the-loop to ensure data quality. Two cases evaluated AcuKG's usability: 
(1) how AcuKG advances acupuncture research for obesity and (2) how AcuKG 
enhances large language model (LLM) application on acupuncture 
question-answering.
RESULTS: AcuKG comprises 1839 entities and 11 527 relations, mapped to 1836 
standard concepts in 3 ontologies. Two use cases demonstrated AcuKG's 
effectiveness and potential in advancing acupuncture research and supporting LLM 
applications. In the obesity use case, AcuKG identified highly relevant 
acupoints (eg, ST25, ST36) and uncovered novel research insights based on 
evidence from clinical trials and literature. When applied to LLMs in answering 
acupuncture-related questions, integrating AcuKG with GPT-4o and LLaMA 3 
significantly improved accuracy (GPT-4o: 46% → 54%, P = .03; LLaMA 3: 17% → 28%, 
P = .01).
CONCLUSION: AcuKG is an open dataset that provides a structured and 
computational framework for acupuncture applications, bridging traditional 
practices with acupuncture research and cutting-edge LLM technologies.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf179
PMCID: PMC12844574
PMID: 41124298 [Indexed for MEDLINE]

Conflict of interest statement: All authors declare no financial or 
non-financial competing interests.


46. PLoS One. 2025 Oct 22;20(10):e0333411. doi: 10.1371/journal.pone.0333411. 
eCollection 2025.

The ethical challenges in the integration of artificial intelligence and large 
language models in medical education: A scoping review.

Li X(1), Yan X(1), Lai H(2).

Author information:
(1)Hospital of Chengdu University of Traditional Chinese Medicine, Chengdu, 
China.
(2)Chengdu University of Traditional Chinese Medicine, Chengdu, China.

With the rapid development of artificial intelligence (AI), large language 
models (LLMs), such as ChatGPT have shown potential in medical education, 
offering personalized learning experiences. However, this integration raises 
ethical concerns, including privacy, autonomy, and transparency. This study 
employed a scoping review methodology, systematically searching relevant 
literature published between January 2010 and August 31, 2024, across three 
major databases: PubMed, Embase, and Web of Science. Through rigorous screening, 
50 articles which met inclusion criteria were ultimately selected from an 
initial pool of 1,192 records. During data processing, the Kimi AI tool was 
utilized to facilitate preliminary literature screening, extraction of key 
information, and construction of content frameworks. Data reliability was 
ensured through a stringent cross-verification process whereby two independent 
researchers validated all AI-generated content against original source 
materials. The study delineates ethical challenges and opportunities arising 
from the integration of AI and LLMs into medical education, identifying seven 
core ethical dimensions: privacy and data security, algorithmic bias, 
accountability attribution, fairness assurance, technological reliability, 
application dependency, and patient autonomy. Corresponding mitigation 
strategies were formulated for each challenge. Future research should prioritize 
establishing dedicated ethical frameworks and application guidelines for AI in 
medical education while maintaining sustained attention to the long-term ethical 
implications of these technologies in healthcare domains.

Copyright: © 2025 Li et al. This is an open access article distributed under the 
terms of the Creative Commons Attribution License, which permits unrestricted 
use, distribution, and reproduction in any medium, provided the original author 
and source are credited.

DOI: 10.1371/journal.pone.0333411
PMCID: PMC12543126
PMID: 41124146 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


47. J Med Internet Res. 2025 Oct 21;27:e80557. doi: 10.2196/80557.

Improving Large Language Model Applications in the Medical and Nursing Domains 
With Retrieval-Augmented Generation: Scoping Review.

Miao Y(#)(1), Zhao Y(#)(1), Luo Y(1), Wang H(1), Wu Y(1)(2).

Author information:
(1)School of Nursing, Capital Medical University, Bejing, China.
(2)The Chinese Institutes for Medical Research, Bejing, China.
(#)Contributed equally

BACKGROUND: Retrieval-augmented generation (RAG) is increasingly used to improve 
large language models in the medical and nursing domains. However, a 
comprehensive understanding of its specific architecture and applications in 
medical and nursing reasoning remains limited.
OBJECTIVE: We aimed to summarize the current state, existing limitations, and 
future development directions of RAG in the medical and nursing domains.
METHODS: The PubMed, Web of Science, IEEE Xplore, and arXiv databases were 
searched for relevant articles using queries that combined terms related to RAG, 
medical, and nursing domains, covering the period from November 1, 2022, to May 
31, 2025. This review was conducted following the PRISMA-ScR (Preferred 
Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping 
Reviews) guidelines.
RESULTS: A total of 917 articles were retrieved, of which 67 met the inclusion 
criteria. Most studies focused on the medical domain (63/67, 94%), while only a 
few addressed nursing applications (4/67, 6%). The RAG frameworks included in 
this review were categorized into 5 functional types: text-based RAG (36/67, 
54%), knowledge graph-enhanced RAG (17/67, 25%), agentic RAG (6/67, 9%), 
multimodal RAG (2/67, 3%), and plug-and-play RAG (6/67, 9%). On the basis of the 
Simon decision-making process theory, we divided the RAG workflow into 4 stages: 
intent recognition, knowledge retrieval, knowledge integration, and generation. 
Only 26 studies included explicit reasoning support, and few were aligned with 
real-world clinical workflows. Only 12 studies attempted to address ethical 
considerations related to RAG.
CONCLUSIONS: We identified 4 key shifts in recent RAG development: shifting from 
surface-level matching toward contextualized intent recognition, from vague 
semantics toward logic-driven dynamic retrieval, from passive toward active 
knowledge retrieval, and from simple aggregation toward coherent context 
construction. However, most RAG systems in the medical and nursing domains have 
not yet introduced reasoning methods, and those that have are still 
predominantly reliant on data‑driven associations without causal modeling. This 
highlights the need to integrate causal mechanisms for more effective and 
domain-relevant reasoning in health care.
TRIAL REGISTRATION: OSF Registries 10.17605/OSF.IO/WBSV5; https://osf.io/wbsv5.

©Yiqun Miao, Yuhan Zhao, Yuan Luo, Huiying Wang, Ying Wu. Originally published 
in the Journal of Medical Internet Research (https://www.jmir.org), 21.10.2025.

DOI: 10.2196/80557
PMCID: PMC12587015
PMID: 41118646 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


48. Front Digit Health. 2025 Sep 30;7:1659134. doi: 10.3389/fdgth.2025.1659134. 
eCollection 2025.

Large language models in real-world clinical workflows: a systematic review of 
applications and implementation.

Artsi Y(1), Sorin V(2), Glicksberg BS(3)(4)(5), Korfiatis P(2), Nadkarni 
GN(3)(4)(5), Klang E(3)(4)(5).

Author information:
(1)Azrieli Faculty of Medicine, Bar-Ilan University, Zefat, Israel.
(2)Department of Radiology, Mayo Clinic, Rochester, MN, United States.
(3)The Charles Bronfman Institute of Personalized Medicine, Icahn School of 
Medicine at Mount Sinai, New York, NY, United States.
(4)The Windreich Department of Artificial Intelligence and Human Health, Mount 
Sinai Medical Center, New York, NY, United States.
(5)The Hasso Plattner Institute for Digital Health at Mount Sinai, Icahn School 
of Medicine at Mount Sinai, New York, NY, United States.

BACKGROUND: Large language models (LLMs) offer promise for enhancing clinical 
care by automating documentation, supporting decision-making, and improving 
communication. However, their integration into real-world healthcare workflows 
remains limited and under characterized. This systematic review aims to evaluate 
the literature on real-world implementation of LLMs in clinical workflows, 
including their use cases, clinical settings, observed outcomes, and challenges.
METHODS: We searched MEDLINE, Scopus, Web of Science, and Google Scholar for 
studies published between January 2015 and April 2025 that assessed LLMs in 
real-world clinical applications. Inclusion criteria were peer-reviewed, 
full-text studies in English reporting empirical implementation of LLMs in 
clinical settings. Study quality and risk of bias were assessed using the 
PROBAST tool.
RESULTS: Four studies published between 2024 and 2025 met inclusion criteria. 
All used generative pre-trained transformers (GPTs). Reported applications 
included outpatient communication, mental health support, inbox message 
drafting, and clinical data extraction. LLM deployment was associated with 
improvements in operational efficiency, user satisfaction, and reduced workload. 
However, challenges included performance variability across data types, 
limitations in generalizability, regulatory delays, and lack of post-deployment 
monitoring.
CONCLUSIONS: Early evidence suggests that LLMs can enhance clinical workflows, 
but real-world adoption remains constrained by systemic, technical, and 
regulatory barriers. To support safe and scalable use, future efforts should 
prioritize standardized evaluation metrics, multi-site validation, human 
oversight, and implementation frameworks tailored to clinical settings.
SYSTEMATIC REVIEW REGISTRATION: 
https://www.crd.york.ac.uk/PROSPERO/recorddashboard, PROSPERO CRD420251030069.

© 2025 Artsi, Sorin, Glicksberg, Korfiatis, Nadkarni and Klang.

DOI: 10.3389/fdgth.2025.1659134
PMCID: PMC12519456
PMID: 41098649

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


49. J Am Med Inform Assoc. 2026 Feb 1;33(2):484-499. doi: 10.1093/jamia/ocaf176.

Using natural language processing to extract information from clinical text in 
electronic medical records for populating clinical registries: a systematic 
review.

Liu L(1), Blake V(1), Barman M(1), Gallego B(1), Churches T(2), Kennedy G(2)(3), 
Ooi SY(4)(5), Delaney GP(2)(3), Jorm L(1).

Author information:
(1)Centre for Big Data Research in Health, University of New South Wales, 
Sydney, NSW 2052, Australia.
(2)Ingham Institute for Applied Medical Research, Liverpool, NSW 2170, 
Australia.
(3)South Western Sydney Clinical School, University of New South Wales, Sydney, 
NSW 2052, Australia.
(4)School of Clinical Medicine, University of New South Wales, Sydney, NSW 2052, 
Australia.
(5)Prince of Wales Hospital, Randwick, NSW 2031, Australia.

OBJECTIVE: Clinical registries advance healthcare by tracking patient outcomes 
and intervention safety. Manually extracting information from clinical text for 
registries is labor- and resource-intensive and often inaccurate. Therefore, 
this systematic review aims to evaluate the use and effectiveness of natural 
language processing (NLP) methods in extracting information from clinical text 
for populating clinical registries.
MATERIALS AND METHODS: PubMed, Embase, Scopus, Web of Science, and ACM Digital 
Library were systematically searched. Studies were included if they used NLP 
techniques to populate clinical registries. The extracted data included details 
of the registry, the clinical text, the registry data elements extracted, the 
NLP methods used, and how their performance was evaluated.
RESULTS: Fifteen articles were included in the review. Since 2020, the use of 
NLP methods for extracting information to populate clinical registries has been 
increasing steadily. Initially, rule-based NLP methods dominated the field, but 
machine learning-based approaches have gradually gained popularity. However, 
only one of the included studies employed generative large language models 
(LLMs). The diversity of clinical text and extracted data elements posed 
challenges to the generalizability of the NLP methods.
CONCLUSION: To date, the application of NLP methods to clinical text for 
populating clinical registries has been limited in both the number of published 
studies and the scope of implementation. The NLP methods used thus far face 
significant challenges in effectively managing the complexity and diversity of 
clinical text and data elements. Moreover, the performance of the NLP methods 
varied significantly. This review underscores the need for a robust and 
adaptable NLP framework. Generative LLMs may provide direction for future 
research, but their use must account for challenges such as accuracy, cost, 
privacy, and limited supporting evidence.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf176
PMCID: PMC12844598
PMID: 41093296 [Indexed for MEDLINE]

Conflict of interest statement: None declared.


50. J Med Internet Res. 2025 Oct 14;27:e79217. doi: 10.2196/79217.

Evaluation Strategies for Large Language Model-Based Models in Exercise and 
Health Coaching: Scoping Review.

Lai X(1)(2), Lai Y(3), Chen J(1), Huang S(1), Gao Q(2), Huang C(1).

Author information:
(1)Research and Communication Center for Exercise and Health, Xiamen University 
of Technology, 600 Ligong Road, Jimei District, Xiamen, Fujian Province, 361024, 
China, 86 15606951380.
(2)School of Sport Medicine and Rehabilitation, Beijing Sport University, 
Beijing, China.
(3)Department of Mathematics and Digital Science, Chengyi College, Jimei 
University, Xiamen, China.

BACKGROUND: Large language model (LLM)-based artificial intelligence (AI) 
coaches show promise for personalized exercise and health interventions. 
However, the unique demands of ensuring safety and real-time, multimodal 
personalized feedback have created a fragmented evaluation landscape lacking 
standardized frameworks.
OBJECTIVE: This scoping review systematically maps current evaluation strategies 
for LLM-based AI coaches in exercise and health, identifies strengths and 
limitations, and proposes directions for robust, standardized validation.
METHODS: Following PRISMA-ScR (Preferred Reporting Items for Systematic reviews 
and Meta-Analyses extension for Scoping Reviews) guidelines, we conducted a 
systematic search across 6 major databases (eg, PubMed, Web of Science) for 
original research on LLM-based exercise and health coaching. Studies were 
included if they explicitly reported on evaluation methods. We extracted and 
synthesized data on model types, application domains, and evaluation strategies 
and developed a 5-point Evaluation Rigor Score (ERS) to quantitatively assess 
the methodological depth of the evaluation designs.
RESULTS: We included 20 studies, most using proprietary models like ChatGPT 
(75%). Evaluation strategies were highly heterogeneous, mixing human ratings 
(80%) and automated metrics (40%). Crucially, the evidence was limited by low 
methodological rigor: the median ERS was 2.5 out of 5, with 55% of studies 
classified as having low rigor. Key gaps included limited use of real-world data 
(40%) and inconsistent reliability reporting (45%).
CONCLUSIONS: The current evaluation of LLM-based health coaches is fragmented 
and methodologically weak. Future work must establish multidimensional 
validation frameworks that integrate technical benchmarks with human-centered 
methods to ensure safe, effective, and equitable deployment.

© Xiangxun Lai, Yue Lai, JiaCheng Chen, Shengqi Huang, Qi Gao, Caihua Huang. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/79217
PMCID: PMC12520646
PMID: 41086432 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


51. Artif Intell Med. 2025 Dec;170:103281. doi: 10.1016/j.artmed.2025.103281. Epub 
2025 Oct 8.

Preprocessing narrative texts in electronic medical records to identify hospital 
adverse events: A scoping review.

Jafarpour H(1), Wu G(2), Cheligeer CK(3), Yan J(4), Xu Y(5), Southern DA(6), 
Eastwood CA(7), Zeng Y(8), Quan H(9).

Author information:
(1)Concordia University, Gina Cody School of Engineering and Computer Science, 
Concordia Institute for Information Systems Engineering, 1515 Sainte Catherine 
West, Montreal, H3G 2W1, Quebec, Canada. Electronic address: 
hamed.jafarpour@concordia.ca.
(2)University of Calgary, Department of Community Health Sciences, Cumming 
School of Medicine, 2500 University Drive NW, Calgary, T2N 1N4, Alberta, Canada. 
Electronic address: Guosong.wu@ucalgary.ca.
(3)University of Calgary, Department of Community Health Sciences, Cumming 
School of Medicine, 2500 University Drive NW, Calgary, T2N 1N4, Alberta, Canada. 
Electronic address: cheligeerken@ucalgary.ca.
(4)Concordia University, Gina Cody School of Engineering and Computer Science, 
Concordia Institute for Information Systems Engineering, 1515 Sainte Catherine 
West, Montreal, H3G 2W1, Quebec, Canada. Electronic address: 
jun.yan@concordia.ca.
(5)University of Calgary, Department of Community Health Sciences, Cumming 
School of Medicine, 2500 University Drive NW, Calgary, T2N 1N4, Alberta, Canada. 
Electronic address: yuxu@ucalgary.ca.
(6)University of Calgary, Department of Community Health Sciences, Cumming 
School of Medicine, 2500 University Drive NW, Calgary, T2N 1N4, Alberta, Canada. 
Electronic address: dasouthe@ucalgary.ca.
(7)University of Calgary, Department of Community Health Sciences, Cumming 
School of Medicine, 2500 University Drive NW, Calgary, T2N 1N4, Alberta, Canada. 
Electronic address: caeastwo@ucalgary.ca.
(8)Concordia University, Gina Cody School of Engineering and Computer Science, 
Concordia Institute for Information Systems Engineering, 1515 Sainte Catherine 
West, Montreal, H3G 2W1, Quebec, Canada. Electronic address: 
yong.zeng@concordia.ca.
(9)University of Calgary, Department of Community Health Sciences, Cumming 
School of Medicine, 2500 University Drive NW, Calgary, T2N 1N4, Alberta, Canada. 
Electronic address: hquan@ucalgary.ca.

BACKGROUND: Narrative electronic medical records (EMR), which include textual 
notes created by clinicians within healthcare environments, represent a 
significant resource for documenting various facets of patient care. This form 
of text exhibits distinctive characteristics, such as the occurrence of 
grammatically incorrect sentences, abbreviations, frequent acronyms, specialized 
characters with particular meanings, negation expressions, and sporadic 
misspellings. As a result, a primary goal in processing these textual notes is 
to implement effective preprocessing techniques that enhance data quality and 
ensure consistency across all entries. Recent advancements in algorithms and 
methodologies within the fields of natural language processing (NLP), machine 
learning (ML), and large language models (LLM) have prompted researchers to 
leverage narrative EMR for the detection of hospital adverse events (HAE).
METHODS: The scoping review adhered to the PRISMA-ScR (Preferred Reporting Items 
for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) 
guidelines. A scoping review protocol was developed and utilized to guide the 
research process, clearly outlining the eligibility criteria, information 
sources, search strategies, data management, selection process, data collection 
procedures, data items, outcomes and prioritization, data synthesis, and 
meta-bias considerations. The search strategy was implemented across nine 
engineering and medical electronic databases.
RESULTS: The results have indicated that from a total of 3,264 studies 
retrieved, 48 unique studies were included in the review. Responses to the 
research questions were systematically extracted from these studies. The review 
has identified challenges associated with the preprocessing of narrative texts 
in EMR for HAE identification. Additionally, three research gaps have been 
identified: (1) the imperative need for a pipeline to preprocess narrative EMR 
for the identification of HAE, (2) the necessity for a robust system capable of 
managing the extensive volume of narrative EMR data, and (3) the requirement for 
temporal event system, which are essential for effective HAE detection. The 
study also has underscored the essential role of preprocessing tasks in 
enhancing the performance of HAE detection. The study has emphasized the 
importance of extracting N-grams from clinical text, normalizing these N-grams 
through lemmatization and/or stemming, and establishing semantic feature 
extraction in preprocessing tasks that significantly affect HAE detection 
performance. While LLM-based systems naturally incorporate tokenization and 
normalization processes within their frameworks, it remains crucial to address 
features that hold semantic relevance to the specific type of HAE during 
preprocessing.
CONCLUSION: This scoping review has provided valuable insights for researchers 
focused on HAE detection utilizing narrative EMR data. It has elucidated how 
preprocessing tasks can elevate the performance of HAE detection and draws 
attention to neglected research gaps within the field. Addressing these gaps 
will necessitate further investigation in subsequent research endeavors.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.artmed.2025.103281
PMID: 41072367 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: Hude Quan’s report received funding 
from the Canadian Institutes of Health Research. The authors declare that they 
have no known competing financial interests or personal relationships that could 
have appeared to influence the work reported in this paper. If there are other 
authors, they declare that they have no known competing financial interests or 
personal relationships that could have appeared to influence the work reported 
in this paper.


52. J Med Internet Res. 2025 Oct 9;27:e77334. doi: 10.2196/77334.

Performance of Large Language Models in Diagnosing Rare Hematologic Diseases and 
the Impact of Their Diagnostic Outputs on Physicians: Combined Retrospective and 
Prospective Study.

Yu H(#)(1), Chen T(#)(1), Zhang X(1), Yang Y(1), Liu Q(1), Yang C(1), Shen K(1), 
Li H(1), Tang W(1), Zhong X(1), Shuai X(1), Yu X(1), Liao Y(1), Wang C(1), Zhu 
H(1), Wu Y(1).

Author information:
(1)Department of Hematology, West China Hospital of Sichuan University, Guoxue 
Alley 37th, Chengdu, 610041, China, 86 102885422370, 86 102885423692.
(#)Contributed equally

BACKGROUND: Rare hematologic diseases are frequently underdiagnosed or 
misdiagnosed due to their clinical complexity. Whether new-generation large 
language models (LLMs), particularly those using chain-of-thought reasoning, can 
improve diagnostic accuracy remains unclear.
OBJECTIVE: This study aimed to evaluate the diagnostic performance of 
new-generation commercial LLMs in rare hematologic diseases and to determine 
whether the LLM output enhances physicians' diagnostic accuracy.
METHODS: We conducted a 2-phase study. In the retrospective phase, we evaluated 
7 mainstream LLMs on 158 nonpublic real-world admission records covering 9 rare 
hematologic diseases, assessed diagnostic performance using top-10 accuracy and 
mean reciprocal rank (MRR), and evaluated ranking stability via Jaccard 
similarity and entropy. Spearman rank correlation was used to examine the 
association between physicians' diagnoses and LLM-generated outputs. In the 
prospective phase, 28 physicians with varying levels of experience diagnosed 5 
cases each, gaining access to LLM-generated diagnoses across 3 sequential steps 
to assess whether LLMs can improve diagnostic accuracy.
RESULTS: In the retrospective phase, ChatGPT-o1-preview demonstrated the highest 
top-10 accuracy (70.3%) and MRR (0.577), and DeepSeek-R1 ranked second. 
Diagnostic performance was low for amyloid light-chain (AL) amyloidosis; 
Castleman disease; Erdheim-Chester disease; and polyneuropathy, organomegaly, 
endocrinopathy, monoclonal gammopathy, and skin changes (POEMS) syndrome. 
Interestingly, higher accuracy often correlated with lower ranking stability 
across most LLMs. The physician performance showed a strong correlation with 
both top-10 accuracy (ρ=0.565) and MRR (ρ=0.650). In the prospective phase, LLMs 
significantly improved the diagnostic accuracy of less-experienced physicians; 
no significant benefit was observed for specialists. However, when LLMs 
generated biased responses, physician performance often failed to improve or 
even declined.
CONCLUSIONS: Without fine-tuning, new-generation commercial LLMs, particularly 
those with chain-of-thought reasoning, can identify diagnoses of rare 
hematologic diseases with high accuracy and significantly enhance the diagnostic 
performance of less-experienced physicians. Nevertheless, biased LLM outputs may 
mislead clinicians, highlighting the need for critical appraisal and cautious 
clinical integration with appropriate safeguard systems.

© Hongbin Yu, Tian Chen, Xin Zhang, Yunfan Yang, Qinyu Liu, Chenlu Yang, Kai 
Shen, He Li, Wenjiao Tang, Xushu Zhong, Xiao Shuai, Xinmei Yu, Yi Liao, Chiyi 
Wang, Huanling Zhu, Yu Wu. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org).

DOI: 10.2196/77334
PMCID: PMC12511990
PMID: 41070713 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


53. JMIR Med Inform. 2025 Oct 7;13:e77837. doi: 10.2196/77837.

Large Language Model-Enhanced Drug Repositioning Knowledge Extraction via Long 
Chain-of-Thought: Development and Evaluation Study.

Kang H(1)(2), Li J(2), Hou L(2), Xu X(2), Zheng S(2), Li Q(1).

Author information:
(1)School of Medical Technology, Beijing Institute of Technology, No. 5 
Zhongguancun South Street, Haidian District, Beijing, 100081, China, 86 
13693067129.
(2)Medical Information Innovation Research Center, Institute of Medical 
Information, Chinese Academy of Medical Sciences and Peking Union Medical 
College, Beijing, China.

BACKGROUND: Drug repositioning is a pivotal strategy in pharmaceutical research, 
offering accelerated and cost-effective therapeutic discovery. However, 
biomedical information relevant to drug repositioning is often complex, 
dispersed, and underutilized due to limitations in traditional extraction 
methods, such as reliance on annotated data and poor generalizability. Large 
language models (LLMs) show promise but face challenges such as hallucinations 
and interpretability issues.
OBJECTIVE: This study proposed long chain-of-thought for drug repositioning 
knowledge extraction (LCoDR-KE), a lightweight and domain-specific framework to 
enhance LLMs' accuracy and adaptability in extracting structured biomedical 
knowledge for drug repositioning.
METHODS: A domain-specific schema defined 11 entities (eg, drug, disease) and 18 
relationships (eg, treats, is biomarker of). Following the established schema 
architecture, we constructed automatic annotation based on 10,000 PubMed 
abstracts via chain-of-thought prompt engineering. A total of 1000 
expert-validated abstracts were curated into a drug repositioning corpus, a 
high-quality specialized corpus, while the remaining entries were allocated for 
model training purposes. Then, the proposed LCoDR-KE framework combined 
supervised fine-tuning of the Qwen2.5-7B-Instruct model with reinforcement 
learning and dual-reward mechanisms. Performance was evaluated against 
state-of-the-art models (eg, conditional random fields, Bidirectional Encoder 
Representations From Transformers, BioBERT, Qwen2.5, DeepSeek-R1, 
OpenBioLLM-70B, and model variants) using precision, recall, and F1-score. In 
addition, the convergence of the training method was assessed by analyzing 
performance progression across iteration steps.
RESULTS: LCoDR-KE achieved an entity F1 of 81.46% (eg, drug 95.83%, disease 
90.52%) and triplet F1 of 69.04%, outperforming traditional models and rivaling 
larger LLMs (DeepSeek-R1: entity F1=84.64%, triplet F1=69.02%). Ablation studies 
confirmed the contributions of supervised fine-tuning (8.61% and 20.70% F1 drop 
if removed) and reinforcement learning (6.09% and 14.09% F1 drop if removed). 
The training process demonstrated stable convergence, validated through 
iterative performance monitoring. Qualitative analysis of the model's 
chain-of-thought outputs showed that LCoDR-KE performed structured and 
schema-aware reasoning by validating entity types, rejecting incompatible 
relations, enforcing constraints, and generating compliant JSON. Error analysis 
revealed 4 main types of mistakes and challenges for further improvement.
CONCLUSIONS: LCoDR-KE enhances LLMs' domain-specific adaptability for drug 
repositioning by offering an open-source drug repositioning corpus and a long 
chain-of-thought framework based on a lightweight LLM model. This framework 
supports drug discovery and knowledge reasoning while providing scalable, 
interpretable solutions applicable to broader biomedical knowledge extraction 
tasks.

© Hongyu Kang, Jiao Li, Li Hou, Xiaowei Xu, Si Zheng, Qin Li. Originally 
published in JMIR Medical Informatics (https://medinform.jmir.org).

DOI: 10.2196/77837
PMCID: PMC12503436
PMID: 41056561 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


54. Int J Med Inform. 2026 Jan;205:106122. doi: 10.1016/j.ijmedinf.2025.106122. Epub 
2025 Sep 26.

Enhancing healthcare worker mental health via artificial intelligence-driven 
work process improvements: a scoping review.

Dave B(1), Martin P(2), David SS(3), Kumar S(4), Chakraborty T(5).

Author information:
(1)University of Queensland-Indian Institute of Technology, Delhi (UQ-IITD) 
Research Academy, St Lucia, Queensland, Australia; Rural Clinical School, 
Medical School, The University of Queensland, Australia; Department of 
Electrical Engineering, Indian Institute of Technology Delhi, New Delhi, India. 
Electronic address: dave@student.uq.edu.au.
(2)School of Health and Medical Sciences, University of Southern Queensland, 
Australia; Rural Clinical School, Medical School, The University of Queensland, 
Australia. Electronic address: priya.martin@unisq.edu.au.
(3)Public Health, Quality and Operations, Evangelical Mission Hospital, Tilda, 
CG, India. Electronic address: sharelsingh@gmail.com.
(4)Allied Health and Human Performance, University of South Australia, 
Australia. Electronic address: Saravana.Kumar@unisa.edu.au.
(5)Department of Electrical Engineering, Indian Institute of Technology Delhi, 
New Delhi, India; Yardi School of Artificial Intelligence, Indian Institute of 
Technology Delhi, New Delhi, India. Electronic address: tanchak@iitd.ac.in.

BACKGROUND: Healthcare workers (HCWs) are exposed to higher rates of mental 
health issues, such as burnout, anxiety, cognitive overload, and stress, 
compared to the general population. These may be exacerbated by administrative 
activities like extensive paperwork and disintegrated work processes. The 
implementation of artificial intelligence (AI) in healthcare holds the potential 
to combat these challenges by streamlining workflow processes, lowering 
administrative load, and increasing efficiency. The role of AI in supporting 
HCWs' mental health is yet to be fully explored. This scoping review mapped the 
current evidence on how AI can enhance HCWs' mental health through workflow 
optimisation.
METHODS: This scoping review was informed by best practice in the conduct and 
reporting of scoping reviews. A comprehensive search of academic and grey 
literature was performed without date restrictions. A two-stage dual screening 
process was employed using Covidence. A customised data extraction tool was 
developed to systematically extract data, which was then summarised 
descriptively.
RESULTS: Twenty articles were included in the review, most of which were 
published between 2020 and 2024. These comprised empirical studies, literature 
reviews, position papers, as well as selected grey literature. The studies 
explored various AI applications such as Natural Language Processing (NLP), 
AI-integrated Electronic Health Records (EHR), Machine Learning (ML), Clinical 
Decision Support Systems (CDSS), and Generative AI-driven tools such as ChatGPT. 
Burnout was the most frequently addressed mental health issue, followed by 
stress and cognitive load. Clinical documentation emerged as the most frequently 
addressed workflow, followed by clinical decision-making and diagnostics. 
Literature indicated that AI was capable of streamlining workflows, reducing 
administrative burden, and improving job satisfaction among HCWs. However, 
challenges such as data integration, algorithmic bias, and increased oversight 
demands were noted as potential barriers to effective implementation.
CONCLUSION: AI holds significant potential to improve HCWs' mental health and 
well-being by addressing workflow inefficiencies and reducing administrative 
burden. While available evidence highlights its benefits in enhancing job 
satisfaction and mitigating burnout, challenges such as data standardisation and 
user trust must be addressed for successful adoption. Future research should 
focus on evaluating the long-term impacts of AI on HCWs' mental well-being and 
developing strategies to mitigate unintended consequences.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106122
PMID: 41037981 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: Bhavyaa Dave reports administrative 
support and writing assistance were provided by The University of Queensland. 
Bhavyaa Dave reports a relationship with The University of Queensland that 
includes: non-financial support. No conflict of interest exists with the 
proceedings of the review of the current work. If there are other authors, they 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


55. J Med Internet Res. 2025 Sep 30;27:e74177. doi: 10.2196/74177.

Large Language Models in Lung Cancer: Systematic Review.

Zhong R(1), Chen S(1), Li Z(1), Gao T(1), Su Y(1), Zhang W(1), Liu D(2), Gao 
L(#)(2), Hu K(#)(2).

Author information:
(1)Graduate School, Beijing University of Chinese Medicine, Beijing, China.
(2)Oncology Department, Dongfang Hospital, Beijing University of Chinese 
Medicine, No. 6, Fangxingyuan 1st District, Fengtai District, Beijing, China, 86 
13911650713.
(#)Contributed equally

BACKGROUND: In the era of data and intelligence, artificial intelligence has 
been widely applied in the medical field. As the most cutting-edge technology, 
the large language model (LLM) has gained popularity due to its extraordinary 
ability to handle complex tasks and interactive features.
OBJECTIVE: This study aimed to systematically review current applications of 
LLMs in lung cancer (LC) care and evaluate their potential across the full-cycle 
management spectrum.
METHODS: Following PRISMA (Preferred Reporting Items for Systematic Reviews and 
Meta-Analyses) guidelines, we conducted a comprehensive literature search across 
6 databases up to January 1, 2025. Studies were included if they satisfied the 
following criteria: (1) journal articles, conference papers, and preprints; (2) 
studies that reported the content of LLMs in LC; (3) including original data and 
LC-related data presented separately; and (4) studies published in English. The 
exclusion criteria were as follows: (1) books and book chapters, letters, 
reviews, conference proceedings; (2) studies that did not report the content of 
LLMs in LC; and (3) no original data, and LC-related data that are not presented 
separately. Studies were screened independently by 2 authors (SC and ZL) and 
assessed for quality using Quality Assessment of Diagnostic Accuracy Studies-2, 
Prediction Model Risk of Bias Assessment Tool, and Risk Of Bias in 
Non-randomized Studies - of Interventions tools, selected based on study type. 
Key data items extracted included model type, application scenario, prompt 
method, input and output format, outcome measures, and safety considerations. 
Data analysis was conducted using descriptive statistics.
RESULTS: Out of 706 studies screened, 28 were included (published between 2023 
and 2024). The ability of LLMs to automatically extract medical records, 
popularize general knowledge about LC, and assist clinical diagnosis and 
treatment has been demonstrated through the systematic review, emerging visual 
ability, and multimodal potential. Prompt engineering was a critical component, 
with varying degrees of sophistication from zero-shot to fine-tuned approaches. 
Quality assessments revealed overall acceptable methodological rigor but noted 
limitations in bias control and data security reporting.
CONCLUSIONS: LLMs show considerable potential in improving LC diagnosis, 
communication, and decision-making. However, their responsible use requires 
attention to privacy, interpretability, and human oversight.

©Ruikang Zhong, Siyi Chen, Zexing Li, Tangke Gao, Yisha Su, Wenzheng Zhang, 
Dianna Liu, Lei Gao, Kaiwen Hu. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org).

DOI: 10.2196/74177
PMCID: PMC12483341
PMID: 41026980 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


56. JMIR Med Inform. 2025 Sep 29;13:e66905. doi: 10.2196/66905.

Using Large Language Models for Chronic Disease Management Tasks: Scoping 
Review.

Serugunda HM(1), Jianquan O(2), Kasujja Namatovu H(3), Ssemaluulu P(4), Kimbugwe 
N(5), Garimoi Orach C(6), Waiswa P(7).

Author information:
(1)Department of Information Technology, School of Computing and Informatics 
Technology, Makerere University, Kampala, Uganda.
(2)School of Computer Science and School of Cyberspace, Xiangtan University, 
Engineering Building, 2nd Floor, Yuhu District, Xiangtan, Hunan, 411105, China, 
86 73158292718 ext 186.
(3)Department of Information Systems, School of Computing and Informatics 
Technology, Makerere University, Kampala, Uganda.
(4)Department of Computer Science, Faculty of Computing and Library Science, 
Kabale University, Kabale, Uganda.
(5)Department of Networks, School of Computing and Informatics Technology, 
Makerere University, Kampala, Uganda.
(6)Department of Community Health and Behavioral Sciences, School of Public 
Health, College of Health Sciences, Makerere University, Kampala, Uganda.
(7)Department of Health Policy Planning and Management, School of Public Health, 
College of Health Sciences, Makerere University, Kampala, Uganda.

BACKGROUND: Chronic diseases present significant challenges in health care, 
requiring effective management to reduce morbidity and mortality. While digital 
technologies like wearable devices and mobile applications have been widely 
adopted, large language models (LLMs) such as ChatGPT are emerging as promising 
technologies with the potential to enhance chronic disease management. However, 
the scope of their current applications in chronic disease management and 
associated challenges remains underexplored.
OBJECTIVE: This scoping review investigates LLM applications in chronic disease 
management, identifies challenges, and proposes actionable recommendations.
METHODS: A systematic search for English-language primary studies on LLM use in 
chronic disease management was conducted across PubMed, IEEE Xplore, Scopus, and 
Google Scholar to identify articles published between January 1, 2023, and 
January 15, 2025. Of the 605 screened records, 29 studies met the inclusion 
criteria. Data on study objectives, LLMs used, health care settings, study 
designs, users, disease management tasks, and challenges were extracted and 
thematically analyzed using the Preferred Reporting Items for Systematic Reviews 
and Meta-Analyses extension for Scoping Reviews guidelines.
RESULTS: LLMs were primarily used for patient-centered tasks, including patient 
education and information provision (18/29, 62%) of studies, diagnosis and 
treatment (6/29, 21%), self-management and disease monitoring (8/29, 28%), and 
emotional support and therapeutic conversations (4/29, 14%). 
Practitioner-centered tasks included clinical decision support (8/29, 28%) and 
medical predictions (6/29, 21%). Challenges identified include inaccurate and 
inconsistent LLM responses (18/29, 62%), limited datasets (6/29, 21%), 
computational and technical (6/29, 21%), usability and accessibility (9/29, 
31%), LLM evaluation (5/29, 17%), and legal, ethical, privacy, and regulatory 
(10/29, 35%). While models like ChatGPT, Llama, and Bard demonstrated use in 
diabetes management and mental health support, performance issues were evident 
across studies and use cases.
CONCLUSIONS: LLMs show promising potential for enhancing chronic disease 
management across patient and practitioner-centered tasks. However, challenges 
related to accuracy, data scarcity, usability, and ethical concerns must be 
addressed to ensure patient safety and equitable use. Future studies should 
prioritize the integration of LLMs with low-resource platforms, wearable and 
mobile technologies, developing culturally and age-appropriate interfaces, and 
establishing robust regulatory and evaluation frameworks to support safe, 
effective, and inclusive use in health care.

©Henry Mukalazi Serugunda, Ouyang Jianquan, Hasifah Kasujja Namatovu, Paul 
Ssemaluulu, Nasser Kimbugwe, Christopher Garimoi Orach, Peter Waiswa. Originally 
published in JMIR Medical Informatics (https://medinform.jmir.org).

DOI: 10.2196/66905
PMCID: PMC12479051
PMID: 41021927 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


57. Front Digit Health. 2025 Sep 11;7:1653631. doi: 10.3389/fdgth.2025.1653631. 
eCollection 2025.

A systematic review of ethical considerations of large language models in 
healthcare and medicine.

Fareed M(1), Fatima M(1), Uddin J(1), Ahmed A(1), Sattar MA(2).

Author information:
(1)Riphah School of Computing & Innovation, Riphah International University, 
Islamabad, Pakistan.
(2)Department of Computer Science, Electrical and Space Engineering, Luleå 
University of Technology, Luleå, Sweden.

The rapid integration of large language models (LLMs) into healthcare offers 
significant potential for improving diagnosis, treatment planning, and patient 
engagement. However, it also presents serious ethical challenges that remain 
incompletely addressed. In this review, we analyzed 27 peer-reviewed studies 
published between 2017 and 2025 across four major open-access databases using 
strict eligibility criteria, robust synthesis methods, and established 
guidelines to explicitly examine the ethical aspects of deploying LLMs in 
clinical settings. We explore four key aspects, including the main ethical 
issues arising from the use of LLMs in healthcare, the prevalent model 
architectures employed in ethical analyses, the healthcare application domains 
that are most frequently scrutinized, and the publication and bibliographic 
patterns characterizing this literature. Our synthesis reveals that bias and 
fairness ( n = 7 , 25.9%) are the most frequently discussed concerns, followed 
by safety, reliability, transparency, accountability, and privacy, and that the 
GPT family predominates ( n = 14 , 51.8%) among examined models. While privacy 
protection and bias mitigation received notable attention in the literature, no 
existing review has systematically addressed the comprehensive ethical issues 
surrounding LLMs. Most previous studies focus narrowly on specific clinical 
subdomains and lack a comprehensive methodology. As a systematic mapping of 
open-access literature, this synthesis identifies dominant ethical patterns, but 
it is not exhaustive of all ethical work on LLMs in healthcare. We also 
synthesize identified challenges, outline future research directions and include 
a provisional ethical integration framework to guide clinicians, developers, and 
policymakers in the responsible integration of LLMs into clinical workflows.

© 2025 Fareed, Fatima, Uddin, Ahmed and Sattar.

DOI: 10.3389/fdgth.2025.1653631
PMCID: PMC12460403
PMID: 41019285

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


58. J Med Internet Res. 2025 Sep 26;27:e72412. doi: 10.2196/72412.

Using Large Language Models to Assess the Consistency of Randomized Controlled 
Trials on AI Interventions With CONSORT-AI: Cross-Sectional Survey.

Luo X(#)(1)(2)(3)(4)(5), Li Z(#)(6), Yang Z(#)(7), Wang B(1)(2)(3)(4)(5), Ma 
Y(7)(8), Chen F(9), Wang Q(10)(11), Ge L(12), Zou J(13)(14)(15), Zhang L(6), 
Chen Y(1)(2)(3)(4)(5), Bian Z(7)(8).

Author information:
(1)Evidence-Based Medicine Center, School of Basic Medical Sciences, Lanzhou 
University, 199 Donggang West Road, Chengguan District, Lanzhou, 730000, China, 
86 13893104140.
(2)Research Unit of Evidence-Based Evaluation and Guidelines, Chinese Academy of 
Medical Sciences (2021RU017), School of Basic Medical Sciences, Lanzhou 
University, Lanzhou, China.
(3)World Health Organization Collaboration Center for Guideline Implementation 
and Knowledge Translation, Lanzhou, China.
(4)Institute of Health Data Science, Lanzhou University, Lanzhou, China.
(5)Key Laboratory of Evidence-Based Medicine of Gansu Province, Lanzhou 
University, Lanzhou, China.
(6)Department of Computer Science, Hong Kong Baptist University, Hong Kong, 
China (Hong Kong).
(7)Vincent V.C. Woo Chinese Medicine Clinical Research Institute, School of 
Chinese Medicine, Hong Kong Baptist University, Hong Kong, China (Hong Kong).
(8)Chinese EQUATOR Centre, Hong Kong, China (Hong Kong).
(9)School of Information Science & Engineering, Lanzhou University, Lanzhou, 
China.
(10)School of Nursing, Li Ka Shing Faculty of Medicine, University of Hong Kong, 
Hong Kong, China (Hong Kong).
(11)Department of Health Research Methods, Evidence and Impact, Faculty of 
Health Sciences, McMaster University, Hamilton, ON, Canada.
(12)Department of Health Policy and Management, School of Public Health, Lanzhou 
University, Lanzhou, China.
(13)Department of Biomedical Data Science, Stanford University, Stanford, CA, 
United States.
(14)Department of Electrical Engineering, Stanford University, Stanford, CA, 
United States.
(15)Department of Computer Science, Stanford University, Stanford, CA, United 
States.
(#)Contributed equally

BACKGROUND: Chatbots based on large language models (LLMs) have shown promise in 
evaluating the consistency of research. Previously, researchers used LLM to 
assess if randomized controlled trial (RCT) abstracts adhered to the 
CONSORT-Abstract guidelines. However, the consistency of artificial intelligence 
(AI) interventional RCTs aligning with the CONSORT-AI (Consolidated Standards of 
Reporting Trials-Artificial Intelligence) standards by LLMs remains unclear.
OBJECTIVE: The aim of this study is to identify the consistency of RCTs on AI 
interventions with CONSORT-AI using chatbots based on LLMs.
METHODS: This cross-sectional study employed 6 LLM models to assess the 
consistency of RCTs on AI interventions. The sample selection is based on 
articles published in JAMA Network Open, which included a total of 41 RCTs. All 
queries were submitted to LLMs through an application programming interface with 
a temperature setting of 0 to ensure deterministic responses. One researcher 
posed the questions to each model, while another independently verified the 
responses for validity before recording the results. The Overall Consistency 
Score (OCS), recall, inter-rater reliability, and consistency of contents were 
analyzed.
RESULTS: We found gpt-4-0125-preview has the best average OCS on the basis of 
the results obtained by JAMA Network Open authors and by us (86.5%, 95% CI 
82.5%-90.5% and 81.6%, 95% CI 77.6%-85.6%, respectively), followed by 
gpt-4-1106-preview (80.3%, 95% CI 76.3%-84.3% and 78.0%, 95% CI 74.0%-82.0%, 
respectively). The model with the worst average OCS is gpt-3.5-turbo-0125 on the 
basis of the results obtained by JAMA Network Open authors and by us (61.9%, 95% 
CI 57.9%-65.9% and 63.0%, 95% CI 59.0%-67.0%, respectively). Among the 11 unique 
items of CONSORT-AI, Item 2 ("State the inclusion and exclusion criteria at the 
level of the input data") received the poorest overall evaluation across the 6 
models, with an average OCS of 48.8%. For other items, those with an average OCS 
greater than 80% across the 6 models included Items 1, 5, 8, and 9.
CONCLUSIONS: GPT-4 variants demonstrate strong performance in assessing the 
consistency of RCTs with CONSORT-AI. Nonetheless, refining the prompts could 
enhance the precision and consistency of the outcomes. While AI tools like GPT-4 
variants are valuable, they are not yet fully autonomous in addressing complex 
and nuanced tasks such as adherence to CONSORT-AI standards. Therefore, 
integrating AI with higher levels of human supervision and expertise will be 
crucial to ensuring more reliable and efficient evaluations, ultimately 
advancing the quality of medical research.

© Xufei Luo, Zeming Li, Zhenhua Yang, Bingyi Wang, Yanfang Ma, Fengxian Chen, Qi 
Wang, Long Ge, James Zou, Lu Zhang, Yaolong Chen, Zhaoxiang Bian. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/72412
PMCID: PMC12466798
PMID: 41004321 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


59. Artif Intell Med. 2025 Dec;170:103268. doi: 10.1016/j.artmed.2025.103268. Epub 
2025 Sep 22.

A survey for large language models in biomedicine.

Wang C(1), Li M(2), He J(3), Wang Z(4), Darzi E(5), Chen Z(6), Ye J(7), Li T(3), 
Su Y(3), Ke J(8), Qu K(2), Li S(2), Yu Y(2), Liò P(9), Wang T(10), Wang YG(11), 
Shen Y(12).

Author information:
(1)School of Medical Engineering, Henan Medical University, Xinxiang, China; 
Engineering Technology Research Center of Neurosense and Control of Henan 
Province, Xinxiang, China; Henan International Joint Laboratory of Neural 
Information Analysis and Drug Intelligent Design, Xinxiang, China.
(2)School of Medical Engineering, Henan Medical University, Xinxiang, China.
(3)Shanghai AI Laboratory, Shanghai, China.
(4)Amazon, Palo Alto, CA, USA.
(5)Boston Children's Hospital, MA, USA; Harvard Medical School, Harvard 
University, MA, USA.
(6)Toursun Synbio, Shanghai, China.
(7)Shanghai AI Laboratory, Shanghai, China; Department of Data Science & AI, 
Faculty of IT, Monash University, Melbourne, Australia.
(8)School of Electronic Information and Electrical Engineering, Shanghai Jiao 
Tong University, Shanghai, China; School of Computer Science and Engineering, 
University of New South Wales, Sydney, Australia.
(9)Department of Computer Science and Technology, University of Cambridge, 
Cambridge, UK.
(10)School of Basic Medical Sciences, Henan Medical University, Xinxiang, China. 
Electronic address: wtianyuncn@126.com.
(11)Shanghai AI Laboratory, Shanghai, China; Toursun Synbio, Shanghai, China; 
Institute of Natural Sciences, Shanghai Jiao Tong University, Shanghai, China; 
School of Mathematics and Statistics, University of New South Wales, Sydney, 
Australia. Electronic address: yuguang.wang@sjtu.edu.cn.
(12)Department of Computer Science, Johns Hopkins University, MD, USA. 
Electronic address: yshen92@jhu.edu.

Recent breakthroughs in large language models (LLMs) offer unprecedented natural 
language understanding and generation capabilities. However, existing surveys on 
LLMs in biomedicine often focus on specific applications or model architectures, 
lacking a comprehensive analysis that integrates the latest advancements across 
various biomedical domains. This review, based on an analysis of 484 
publications sourced from databases including PubMed, Web of Science, and arXiv, 
provides an in-depth examination of the current landscape, applications, 
challenges, and prospects of LLMs in biomedicine, distinguishing itself by 
focusing on the practical implications of these models in real-world biomedical 
contexts. Firstly, we explore the capabilities of LLMs in zero-shot learning 
across a broad spectrum of biomedical tasks, including diagnostic assistance, 
drug discovery, and personalized medicine, among others, with insights drawn 
from 137 key studies. Then, we discuss adaptation strategies of LLMs, including 
fine-tuning methods for both uni-modal and multi-modal LLMs to enhance their 
performance in specialized biomedical contexts where zero-shot fails to achieve, 
such as medical question answering and efficient processing of biomedical 
literature. Finally, we discuss the challenges that LLMs face in the biomedicine 
domain including data privacy concerns, limited model interpretability, issues 
with dataset quality, and ethics due to the sensitive nature of biomedical data, 
the need for highly reliable model outputs, and the ethical implications of 
deploying AI in healthcare. To address these challenges, we also identify future 
research directions of LLM in biomedicine including federated learning methods 
to preserve data privacy and integrating explainable AI methodologies to enhance 
the transparency of LLMs. As this field of LLM rapidly evolves, continued 
research and development are essential to fully harness the capabilities of LLMs 
in biomedicine while ensuring their responsible and effective deployment.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.artmed.2025.103268
PMID: 40997586 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that the research was conducted in the absence of any commercial or 
financial relationships that could be construed as a potential conflict of 
interest.


60. J Med Internet Res. 2025 Sep 24;27:e81769. doi: 10.2196/81769.

Critical Limitations in Systematic Reviews of Large Language Models in Health 
Care.

Weizman Z(1).

Author information:
(1)Faculty of Health Sciences, Ben-Gurion University, 8 Balfour Street, 
Tel-Aviv, 6521120, Israel, 972 544888686.

Comment on
    J Med Internet Res. e82729.
    J Med Internet Res. 27:e71916.

DOI: 10.2196/81769
PMCID: PMC12459740
PMID: 40991747

Conflict of interest statement: Conflicts of Interest: None declared.


61. J Med Internet Res. 2025 Sep 24;27:e82729. doi: 10.2196/82729.

Author's Reply: Critical Limitations in Systematic Reviews of Large Language 
Models in Health Care.

Python A(1)(2)(3), Li H(1)(4), Fu JF(5)(6)(7).

Author information:
(1)Center for Data Science, Zhejiang University, Hangzhou, China.
(2)School of Medicine, Zhejiang University, Hangzhou, China.
(3)Centre for Human Genetics, Nuffield Department of Medicine, University of 
Oxford, Roosevelt Drive, Oxford, OX3 7BN, United Kingdom, 44 01865 287500.
(4)School of Mathematical Sciences, Zhejiang University, Hangzhou, China.
(5)School of Medicine, Children's Hospital of Zhejiang University, Hangzhou, 
China.
(6)National Clinical Research Center for Child Health, Hangzhou, China.
(7)National Regional Center for Children's Health, Hangzhou, China.

Comment on
    doi: 10.2196/81769.
    doi: 10.2196/71916.

DOI: 10.2196/82729
PMCID: PMC12459737
PMID: 40991734

Conflict of interest statement: Conflicts of Interest: None declared.


62. Health Informatics J. 2025 Jul-Sep;31(3):14604582251381269. doi: 
10.1177/14604582251381269. Epub 2025 Sep 22.

Performance of artificial intelligence large language models (Copilot and 
Gemini) compared to human experts in healthcare policy making: A mixed-methods 
cross-sectional study.

Khosravi M(1), Izadi R(2), Aghamaleki Sarvestani M(2), Bouzarjomehri H(3), 
Ahmadi Marzaleh M(4), Ravangard R(5).

Author information:
(1)Social Determinants of Health Research Center, Birjand University of Medical 
Sciences, Birjand, Iran.
(2)Student Research Committee, School of Health Management and Information 
Sciences, Shiraz University of Medical Sciences, Shiraz, Iran.
(3)Environmental Science and Technology Research Center, School of Public 
Health, Shahid Sadoughi University of Medical Sciences, Yazd, Iran.
(4)Department of Health in Disasters and Emergencies, Health Human Resources 
Research Center, School of Health Management and Information Sciences, Shiraz 
University of Medical Sciences, Shiraz, Iran.
(5)Department of Health Services Management, Health Human Resources Research 
Centre, School of Health Management and Information Sciences, Shiraz, Iran.

ObjectiveThis study aimed to assess the performance of Artificial Intelligence 
(AI) compared to human experts in healthcare policymaking.MethodsThis was a 
mixed-methods cross-sectional study conducted in Iran during the years 
2024-2025, comparing, and analyzing the responses of multiple AI Large Language 
Models (LLMs) including Bing AI Copilot and Gemini and a sample of 15 human 
experts-using confusion matrix analysis. This analysis provided comprehensive 
data on the respondents' ability to answer context-specific questions regarding 
healthcare policy making, evaluated through multiple parameters including 
sensitivity, specificity, negative predictive value (NPV), positive predictive 
value (PPV), and overall accuracy.ResultsCopilot demonstrated a sensitivity of 
0.867, specificity of 0, PPV of 0.722, NPV of 0, and accuracy of 0.65. In 
comparison, Gemini exhibited a sensitivity of 0.733, specificity of 0.4, PPV of 
0.786, NPV of 0.333, and also an accuracy of 0.65. Additionally, the human 
experts' responses indicated a sensitivity of 0.5808, specificity of 0.2571, PPV 
of 0.7189, NPV of 0.1579, and an accuracy of 0.5050.ConclusionThe AI LLMs 
outperformed human experts in responding to the study questionnaire. The 
findings demonstrated the considerable potential of the LLMs in enhancing 
healthcare policy-making, particularly by serving as complementary tools and 
collaborators alongside humans.

DOI: 10.1177/14604582251381269
PMID: 40977570 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of conflicting interestsThe 
author(s) declared no potential conflicts of interest with respect to the 
research, authorship, and/or publication of this article


63. J Am Med Inform Assoc. 2025 Dec 1;32(12):1877-1887. doi: 10.1093/jamia/ocaf149.

A scalable framework for benchmark embedding models in semantic health-care 
tasks.

Soffer S(1)(2), Omar M(3)(4), Gendler M(5), Glicksberg BS(3)(4), Kovatch P(6), 
Efros O(2)(7), Freeman R(3), Charney AW(3), Nadkarni GN(3)(4), Klang E(3)(4).

Author information:
(1)Institute of Hematology, Davidoff Cancer Center, Rabin Medical Center, Petah 
Tikva, 49100, Israel.
(2)Gray Faculty of Medical and Health Sciences, Tel Aviv University, Tel Aviv, 
6997801, Israel.
(3)Windreich Department of AI and Human Health, Icahn School of Medicine at 
Mount Sinai, New York, NY 10029, United States.
(4)Department of Medicine, Icahn School of Medicine at Mount Sinai, The Hasso 
Plattner Institute for Digital Health at Mount Sinai, New York, NY 10019, United 
States.
(5)Azrieli Faculty of Medicine, Bar-Ilan University, Safed, 1311502, Israel.
(6)Department of Genetics and Genomic Sciences, Icahn School of Medicine at 
Mount Sinai, New York, NY 10029, United States.
(7)National Hemophilia Center and Thrombosis Institute, Sheba Medical Center, 
Ramat Gan, 5262100, Israel.

OBJECTIVES: Text embeddings are promising for semantic tasks, such as retrieval 
augmented generation (RAG). However, their application in health care is 
underexplored due to a lack of benchmarking methods. We introduce a scalable 
benchmarking method to test embeddings for health-care semantic tasks.
MATERIALS AND METHODS: We evaluated 39 embedding models across 7 medical 
semantic similarity tasks using diverse datasets. These datasets comprised 
real-world patient data (from the Mount Sinai Health System and MIMIC IV), 
biomedical texts from PubMed, and synthetic data generated with Llama-3-70b. We 
first assessed semantic textual similarity (STS) by correlating the 
model-generated similarity scores with noise levels using Spearman rank 
correlation. We then reframed the same tasks as retrieval problems, evaluated by 
mean reciprocal rank and recall at k.
RESULTS: In total, evaluating 2000 text pairs per 7 tasks for STS and retrieval 
yielded 3.28 million model assessments. Larger models (>7b parameters), such as 
those based on Mistral-7b and Gemma-2-9b, consistently performed well, 
especially in long-context tasks. The NV-Embed-v1 model (7b parameters), 
although top in short tasks, underperformed in long tasks. For short tasks, 
smaller models such as b1ade-embed (335M parameters) performed on-par to the 
larger models. For long retrieval tasks, the larger models significantly 
outperformed the smaller ones.
DISCUSSION: The proposed benchmarking framework demonstrates scalability and 
flexibility, offering a structured approach to guide the selection of embedding 
models for a wide range of health-care tasks.
CONCLUSION: By matching the appropriate model with the task, the framework 
enables more effective deployment of embedding models, enhancing critical 
applications such as semantic search and retrieval-augmented generation (RAG).

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf149
PMCID: PMC12646376
PMID: 40977370 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
declare.


64. J Am Med Inform Assoc. 2025 Nov 1;32(11):1756-1766. doi: 10.1093/jamia/ocaf150.

Bridging language gaps in healthcare: a systematic review of the practical 
implementation of neural machine translation technologies in clinical settings.

Karakus IS(1), Strechen I(1), Gupta A(1), Nalaie K(1)(2), Chen CL(3), Hassett 
LC(4), Barwise AK(5)(6).

Author information:
(1)Department of Anesthesiology and Perioperative Medicine, Mayo Clinic, 
Rochester, MN 55905, United States.
(2)Division of Nursing Research, Department of Nursing, Mayo Clinic, Rochester, 
MN 55905, United States.
(3)Department of Internal Medicine, Mayo Clinic, Rochester, MN 55905, United 
States.
(4)Mayo Clinic Libraries, Mayo Clinic, Rochester, MN 55905, United States.
(5)Division of Pulmonary and Critical Care Medicine, Mayo Clinic, Rochester, MN 
55905, United States.
(6)Biomedical Ethics Research Program, Mayo Clinic, Rochester, MN 55905, United 
States.

OBJECTIVES: Effective communication is crucial in healthcare, and for patients 
with a non-English language preference (NELP), professional interpreters are 
recognized as the gold standard in supporting bidirectional communication. 
However, interpreters are not always readily available, prompting the 
exploration of other options for translation and interpretation. The recent 
developments in artificial intelligence-based neural network translation tools, 
namely neural machine translation (NMT) may enable robust interpretation and 
translation.
MATERIALS AND METHODS: We conducted a systematic review (SR) to evaluate the 
literature on NMT for this purpose. We did a comprehensive search of several 
databases with guidance from a professional librarian. The search was limited to 
the year 2000 onwards and English language. Title and abstract screening and 
full-text review were independently conducted by two reviewers with conflicts 
resolved by a third reviewer.
RESULTS: 2867 studies were identified with 10 studies included in the final 
analysis. Among these, six evaluated interpretation in real or simulated 
clinical settings and four examined translation of discharge materials. Google 
Translate and ChatGPT were assessed in several studies. Accuracy differed by 
language, with low-resource languages performing worse.
DISCUSSION: NMT technologies in healthcare have several advantages including 
broad language accessibility and potential cost savings for institutions. 
Despite improved accuracy of these novel tools, due to possible critical errors 
NMT tools are not yet ready for widespread clinical use.
CONCLUSION: Future studies should focus on optimizing evaluation methods as well 
as how best to integrate these technologies into real-time clinical settings.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf150
PMCID: PMC12626213
PMID: 40966445 [Indexed for MEDLINE]

Conflict of interest statement: None declared.


65. Health Informatics J. 2025 Jul-Sep;31(3):14604582251381233. doi: 
10.1177/14604582251381233. Epub 2025 Sep 17.

An innovative X-RAG technique combined with GPT-4o for summarizing medical 
information from EHR and EMR to assist doctors in clinical decision-making 
effectively and efficiently.

Wang JF(1), Chang CC(1), Chiang TM(2), Yeh TC(3), Cheng E(4), Lee YT(2), Chen 
HI(2).

Author information:
(1)Department of Electrical Engineering, National Cheng Kung University, Tainan, 
Taiwan.
(2)Doctors' Doctor Clinic, Taipei, Taiwan.
(3)DeepWave Co. Ltd, Taipei, Taiwan.
(4)C-Media Electronics Inc, Taipei, Taiwan.

Background: Large language models (LLM) still face challenges in accurately 
extracting and summarizing medical information from EHR and EMR. The variability 
in EHR and EMR formats across institutions further complicates information 
integration. Moreover, doctors need to spend a lot of time reviewing patient 
information, which affects the efficiency and effectiveness of clinical 
decision-making. Objective: This study aims to develop a medical record 
summarization system that uses the innovative X-RAG technique with GPT-4o to 
extract medical information from EHR and EMR and convert them into structured 
FHIR format. The system ultimately generates a doctor-friendly report to improve 
the efficiency and effectiveness of clinical decision-making. Methods: We 
propose an innovative X-RAG, which adds page-based chunking, chunk filtering, 
and guided extraction prompting to the basic framework of RAG and combines it 
with GPT-4o to extract medical measurement data, diagnostic reports, and 
medication history records from EHR and EMR with high accuracy. Results: The 
system achieved 96.5% accuracy in medical data extraction and reduced 
approximately 40% of the time doctors spend reviewing patient information in 
clinical applications. Conclusion: The proposed system improves the efficiency 
and effectiveness of clinical decision-making and provides a valuable tool to 
optimize medical information management and clinical workflows.

DOI: 10.1177/14604582251381233
PMID: 40961463 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of conflicting interestsThe authors 
declared no potential conflicts of interest with respect to the research, 
authorship, and/or publication of this article.


66. J Med Internet Res. 2025 Sep 16;27:e78417. doi: 10.2196/78417.

New Doc on the Block: Scoping Review of AI Systems Delivering Motivational 
Interviewing for Health Behavior Change.

Karve Z(1), Calpey J(1), Machado C(1), Knecht M(1), Mejia MC(1).

Author information:
(1)Schmidt College of Medicine, Florida Atlantic University, Boca Raton, FL, 
United States.

BACKGROUND: Artificial intelligence (AI) is increasingly used in digital health, 
particularly through large language models (LLMs), to support patient engagement 
and behavior change. One novel application is the delivery of motivational 
interviewing (MI), an evidence-based, patient-centered counseling technique 
designed to enhance motivation and resolve ambivalence around health behaviors. 
AI tools, including chatbots, mobile apps, and web-based agents, are being 
developed to simulate MI techniques at scale. While these innovations are 
promising, important questions remain about how faithfully AI systems can 
replicate MI principles or achieve meaningful behavioral impact.
OBJECTIVE: This scoping review aimed to summarize existing empirical studies 
evaluating AI-driven systems that apply MI techniques to support health behavior 
change. Specifically, we examined the feasibility of these systems; their 
fidelity to MI principles; and their reported behavioral, psychological, or 
engagement outcomes.
METHODS: We systematically searched PubMed, Embase, Scopus, Web of Science, and 
Cochrane Library for empirical studies published between January 1, 2018, and 
February 25, 2025. Eligible studies involved AI-driven systems using natural 
language generation, understanding, or computational logic to deliver MI 
techniques to users targeting a specific health behavior. We excluded studies 
using AI solely for training clinicians in MI. Three independent reviewers 
screened and extracted data on study design, AI modality and type, MI 
components, health behavior focus, MI fidelity assessment, and outcome domains.
RESULTS: Of the 1001 records identified, 15 (1.5%) met the inclusion criteria. 
Of these 15 studies, 6 (40%) were exploratory feasibility or pilot studies, and 
3 (20%) were randomized controlled trials. AI modalities included rule-based 
chatbots (9/15, 60%), LLM-based systems (4/15, 27%), and virtual or mobile 
agents (2/15, 13%). Targeted behaviors included smoking cessation (6/15, 40%), 
substance use (3/15, 20%), COVID-19 vaccine hesitancy, type 2 diabetes 
self-management, stress, mental health service use, and opioid use during 
pregnancy. Of the 15 studies, 13 (87%) reported positive findings on feasibility 
or user acceptability, while 6 (40%) assessed MI fidelity using expert review or 
structured coding, with moderate to high alignment reported. Several studies 
found that users perceived the AI systems as judgment free, supportive, and 
easier to engage with than human counselors, particularly in stigmatized 
contexts. However, limitations in empathy, safety transparency, and emotional 
nuance were commonly noted. Only 3 (20%) of the 15 studies reported 
substantially significant behavioral changes.
CONCLUSIONS: AI systems delivering MI show promise for enhancing patient 
engagement and scaling behavior change interventions. Early evidence supports 
their usability and partial fidelity to MI principles, especially in sensitive 
domains. However, most systems remain in early development, and few have been 
rigorously tested. Future research should prioritize randomized evaluations; 
standardized fidelity measures; and safeguards for LLM safety, empathy, and 
accuracy in health-related dialogue.
TRIAL REGISTRATION: OSF Registries 10.17605/OSF.IO/G9N7E; https://osf.io/g9n7e.

©Zev Karve, Jacob Calpey, Christopher Machado, Michelle Knecht, Maria Carmenza 
Mejia. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 16.09.2025.

DOI: 10.2196/78417
PMCID: PMC12485255
PMID: 40957014 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


67. J Med Syst. 2025 Sep 16;49(1):115. doi: 10.1007/s10916-025-02254-4.

Large Language Models in Neurology Treatment Decision-Making: a Scoping Review.

Shah R(1), Jotterand F(2)(3).

Author information:
(1)School of Medicine, Medical College of Wisconsin, 8701 Watertown Plank Road, 
Milwaukee, WI, 53226, USA. rshah@mcw.edu.
(2)Center for Ethics in Health Care, Oregon Health & Science University, Oregon, 
USA.
(3)Institute for Biomedical Ethics, Faculty of Medicine, University of Basel, 
Basel, Switzerland.

This scoping review evaluates the expanding role of large language models (LLMs) 
in neurology, an area drawing growing interest of researchers and clinicians 
alike. A substantial existing body of literature supports the efficacy of LLMs 
for diagnostic applications. However, clinicians' emerging point of interest now 
lies in understanding the applications of LLMs in guiding treatment decisions. 
Our study therefore aims to synthesize and evaluate existing neurological 
studies focused on LLMs in treatment decision-making. A comprehensive search was 
conducted in the electronic databases OVID/Medline, Web of Science, and the 
Cochrane Library through September 18th, 2024. Inclusion criteria included 
original studies published within the last five years focused on evaluating the 
efficacy of LLMs in treatment decision-making in neurology. The protocol was 
registered on the Open Science Framework ( https://doi.org/10.17605/OSF.IO/Y6N3E 
). Four studies were identified. ChatGPT was the LLM utilized in each article, 
though varying in model versions. Each study demonstrated positive outcomes 
across varying metrics, with models generally aligning with clinician decisions. 
However, the lack of observed studies and variability of neurological topics 
limit the generalizability of these AI tools. This scoping review analyzes the 
existing body of evidence on LLMs in treatment decision-making in neurology. 
While current studies suggest potential to support clinical care, there is 
insufficient evidence at this stage to claim outcome improvement. Findings are 
not yet generalizable across neurological practice, as existing promise appears 
limited to narrow use cases. Prospective validation across subspecialties is 
needed to support broader clinical application.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02254-4
PMID: 40956389 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics Approval : None. Competing 
interests: The authors declare no competing interests.


68. JMIR Med Inform. 2025 Sep 10;13:e70967. doi: 10.2196/70967.

Leveraging GPT-4o for Automated Extraction and Categorization of CAD-RADS 
Features From Free-Text Coronary CT Angiography Reports: Diagnostic Study.

Chen Y(#)(1), Dong M(#)(1), Sun J(#)(1), Meng Z(1), Yang Y(1), Muhetaier A(1), 
Li C(1), Qin J(1).

Author information:
(1)Departments of Radiology, The Third Affiliated Hospital, Sun Yat-Sen 
University, 600 Tianhe Road, Guangzhou, Guangdong, 510630, China, 86 
18922109279, 86 20852523108.
(#)Contributed equally

BACKGROUND: Despite the Coronary Artery Reporting and Data System (CAD-RADS) 
providing a standardized approach, radiologists continue to favor free-text 
reports. This preference creates significant challenges for data extraction and 
analysis in longitudinal studies, potentially limiting large-scale research and 
quality assessment initiatives.
OBJECTIVE: To evaluate the ability of the generative pre-trained transformer 
(GPT)-4o model to convert real-world coronary computed tomography angiography 
(CCTA) free-text reports into structured data and automatically identify 
CAD-RADS categories and P categories.
METHODS: This retrospective study analyzed CCTA reports from January 2024 and 
July 2024. A subset of 25 reports was used for prompt engineering to instruct 
the large language models (LLMs) in extracting CAD-RADS categories, P 
categories, and the presence of myocardial bridges and noncalcified plaques. 
Reports were processed using the GPT-4o API (application programming interface) 
and custom Python scripts. The ground truth was established by radiologists 
based on the CAD-RADS 2.0 guidelines. Model performance was assessed using 
accuracy, sensitivity, specificity, and F1-score. Intrarater reliability was 
assessed using Cohen κ coefficient.
RESULTS: Among 999 patients (median age 66 y, range 58-74; 650 males), CAD-RADS 
categorization showed accuracy of 0.98-1.00 (95% CI 0.9730-1.0000), sensitivity 
of 0.95-1.00 (95% CI 0.9191-1.0000), specificity of 0.98-1.00 (95% CI 
0.9669-1.0000), and F1-score of 0.96-1.00 (95% CI 0.9253-1.0000). P categories 
demonstrated accuracy of 0.97-1.00 (95% CI 0.9569-0.9990), sensitivity from 0.90 
to 1.00 (95% CI 0.8085-1.0000), specificity from 0.97 to 1.00 (95% CI 
0.9533-1.0000), and F1-score from 0.91 to 0.99 (95% CI 0.8377-0.9967). 
Myocardial bridge detection achieved an accuracy of 0.98 (95% CI 0.9680-0.9870), 
and noncalcified coronary plaques detection showed an accuracy of 0.98 (95% CI 
0.9680-0.9870). Cohen κ values for all classifications exceeded 0.98.
CONCLUSIONS: The GPT-4o model efficiently and accurately converts CCTA free-text 
reports into structured data, excelling in CAD-RADS classification, plaque 
burden assessment, and detection of myocardial bridges and calcified plaques.

© Youmei Chen, Mengshi Dong, Jie Sun, Zhanao Meng, Yiqing Yang, Abudushalamu 
Muhetaier, Chao Li, Jie Qin. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org).

DOI: 10.2196/70967
PMCID: PMC12422720
PMID: 40929727 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


69. PLoS One. 2025 Sep 9;20(9):e0330288. doi: 10.1371/journal.pone.0330288. 
eCollection 2025.

Protocol for a core outcome set for pharmacological treatments in hospitalised 
patients with acute viral respiratory infections (COSAVRI).

Devane D(1)(2)(3), Briel M(4), Bhagani S(5), Boesten N(6), Buchholz S(7)(8), De 
Luca EC(6), Guedj J(9), Koryakina A(10), Kothari K(3), Lacombe K(11), Mallon 
PW(12), Massonnaud CR(13), O'Dwyer J(14), Olsen IC(15), Saif-Ur-Rahman KM(13), 
Schwenke JM(4), Thomas J(10), Yazdanpanah Y(16), Zgaga L(17), Louw J(12).

Author information:
(1)Center for Health Research Methods, School of Nursing and Midwifery, 
University of Galway, Galway, Ireland.
(2)Health Research Board-Trials Methodology Research Network (HRB-TMRN), 
University of Galway, Galway, Ireland.
(3)Evidence Synthesis Ireland and Cochrane Ireland, University of Galway, 
Galway, Ireland.
(4)CLEAR Methods Center, Division of Clinical Epidemiology, Department of 
Clinical Research, University Hospital Basel, University of Basel, Basel, 
Switzerland.
(5)Department of Infectious Diseases and HIV Medicine, Royal Free London NHS 
Trust and Division of Infection and Immunity, University College London, London, 
United Kingdom.
(6)European Patients Forum, Brussels, Belgium.
(7)Public Health Threats Department, European Medicines Agency, Amsterdam, 
Netherlands.
(8)Division 32, Infectiology/Dermatology/Allergology, Federal Institute for 
Drugs and Medical Devices, Bonn, Germany.
(9)Université Paris Cité, INSERM, IAME, Paris, France.
(10)EPPI Centre, Social Research Institute, University College London, London, 
United Kingdom.
(11)Sorbonne Université, INSERM IPLESP, Infectious Diseases Department, St 
Antoine Hospital, APHP, Paris, France.
(12)Centre for Experimental Pathogen Host Research, School of Medicine, 
University College Dublin, Dublin, Ireland.
(13)Département d'Épidémiologie, Biostatistique et Recherche Clinique, Hôpital 
Bichat, Université Paris Cité, Inserm, IAME, APHP, Paris, France.
(14)Discipline of Pharmacy, School of Pharmacy and Medical Sciences, University 
of Galway, Galway, Ireland.
(15)Department of Research Support for Clinical Trials, Oslo University 
Hospital, Oslo, Norway.
(16)ANRS Emerging Infectious Disease, Inserm, Paris, France.
(17)Department of Public Health and Primary Care, Institute of Population 
Health, Trinity College Dublin, University of Dublin, Dublin, Ireland.

BACKGROUND: Acute viral respiratory infections (AVRIs) rank among the most 
common causes of hospitalisation worldwide, imposing significant healthcare 
burdens and driving the development of pharmacological treatments. However, 
inconsistent outcome reporting across clinical trials limits evidence synthesis 
and its translation into clinical practice. A core outcome set (COS) for 
pharmacological treatments in hospitalised adults with AVRIs is essential to 
standardise trial outcomes and improve research comparability.
OBJECTIVE: To develop an internationally agreed COS for pharmacological 
treatments in hospitalised adults ≥18 years with acute viral respiratory 
infections (COSAVRI) through stakeholder agreement.
METHODS: This protocol follows a four-stage development process in accordance 
with Core Outcome Set Handbook guidelines. Stage 1 comprises a rapid scoping 
review of randomised controlled trials (2015-2025) to systematically catalogue 
patient-relevant outcomes reported in pharmacological AVRI treatment studies. 
Semi-automated screening and data extraction will employ machine learning and 
large language models, with human verification. Stage 2 involves an online 
Real-Time Delphi survey with international stakeholders, including healthcare 
professionals, researchers, patients/caregivers, and policymakers, to prioritise 
identified outcomes using a 9-point scale. Stage 3 consists of structured online 
consensus meetings utilising anonymous electronic voting to finalise the COS. 
Stage 4 focuses on dissemination and implementation through academic 
publications, conferences, and stakeholder engagement.
EXPECTED OUTCOMES: COSAVRI will provide a standardised minimum set of outcomes 
for measuring and reporting in future pharmacological trials involving 
hospitalised adults with AVRIs. This initiative will enhance evidence synthesis, 
reduce research waste, support regulatory decision-making, and improve pandemic 
preparedness by facilitating the rapid deployment of harmonised outcomes in 
trial protocols.

Copyright: © 2025 Devane et al. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0330288
PMCID: PMC12419614
PMID: 40924748 [Indexed for MEDLINE]

Conflict of interest statement: DD, NB, SBu, ECDL, JG, AK, KK, CM, JO, ICO, 
KMSUR, JMS, JT, YY, LZ and JL have no conflicts of interests to declare. MB 
reports research grants from Moderna within the previous three years, all paid 
to the institution and all outside the submitted work. SB reports research 
grants from Abbvie, Gilead and MSD, and honoraria for educational presentations 
and advisory board attendance from Abbvie, Gilead, MSD and Pfizer. KL reports 
honoraria for educational presentations and advisory board attendance from 
Gilead, MSD and ViiV Healthcare. PM reports honoraria for educational 
presentations or advisory activities from Gilead Sciences, Janssen and MSD and 
grant income to his institution from GSK Ireland. This does not alter our 
adherence to PLOS ONE policies on sharing data and materials.


70. J Med Internet Res. 2025 Sep 8;27:e68291. doi: 10.2196/68291.

Applications of Federated Large Language Model for Adverse Drug Reactions 
Prediction: Scoping Review.

Guo D(1), Choo KR(1).

Author information:
(1)Department of Information Systems and Cybersecurity, The University of Texas 
at San Antonio, 1 UTSA Circle, San Antonio, TX, 78249, United States, 1 (210) 
458-6300.

BACKGROUND: Adverse drug reactions (ADR) present significant challenges in 
health care, where early prevention is vital for effective treatment and patient 
safety. Traditional supervised learning methods struggle to address 
heterogeneous health care data due to their unstructured nature, regulatory 
constraints, and restricted access to sensitive personal identifiable 
information.
OBJECTIVE: This review aims to explore the potential of federated learning (FL) 
combined with natural language processing and large language models (LLMs) to 
enhance ADR prediction. FL enables decentralized training across client clusters 
with limited resources, while LLMs effectively process unstructured health care 
data. By aggregating client-trained models into a global model, FL ensures 
broader data inclusion while maintaining privacy.
METHODS: A scoping review was conducted on peer-reviewed publications retrieved 
from Google Scholar and Semantic Scholar between 2019 and 2024.
RESULTS: Following the PRISMA (Preferred Reporting Items for Systematic Reviews 
and Meta-Analyses) protocol, 145 articles from PubMed, arXiv, IEEE, and ACL 
Anthology met the inclusion criteria. Of these, 12 articles were selected for an 
in-depth review to examine use cases in ADR prediction. We synthesized ADR data 
sources on structured and unstructured data types, use cases of FL integrated 
with natural language processing, and open-source frameworks for ADR 
identifications and predictions. Special attention is given to unstructured ADR 
prediction using federated learning with large language models, including 
development and deployment strategies and evaluation metrics.
CONCLUSIONS: Given the recent emergence of LLM, the integration of FL and LLM 
for ADR prediction remains in its early stage, with limited documented use 
cases. This review explored the potential applications and highlighted the 
advancements of federated learning with large language models in health care 
research, particularly in ADR prediction. Key focus areas include fine-tuning 
and merging algorithms, fairness and unbiasedness, implementation challenges, 
and real-world deployment strategies. By synthesizing current insights, this 
review aims to lay the groundwork for future research in privacy-preserving and 
scalable ADR prediction systems.

©David Guo, Kim-Kwang Raymond Choo. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/68291
PMCID: PMC12516295
PMID: 40921101 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


71. JMIR Med Inform. 2025 Sep 8;13:e76252. doi: 10.2196/76252.

Automated Literature Screening for Hepatocellular Carcinoma Treatment Through 
Integration of 3 Large Language Models: Methodological Study.

Pan C(1), Lu W(1), Chen B(1), Zhang G(1), Yang Z(1), Hao J(1).

Author information:
(1)Department of Hepatobiliary and Vascular Surgery, First Affiliated Hospital 
of Chengdu Medical College, Chengdu, China.

BACKGROUND: Primary liver cancer, particularly hepatocellular carcinoma (HCC), 
poses significant clinical challenges due to late-stage diagnosis, tumor 
heterogeneity, and rapidly evolving therapeutic strategies. While systematic 
reviews and meta-analyses are essential for updating clinical guidelines, their 
labor-intensive nature limits timely evidence synthesis.
OBJECTIVE: This study proposes an automated literature screening workflow 
powered by large language models (LLMs) to accelerate evidence synthesis for HCC 
treatment guidelines.
METHODS: We developed a tripartite LLM framework integrating Doubao-1.5-pro-32k, 
Deepseek-v3, and DeepSeek-R1-Distill-Qwen-7B to simulate collaborative 
decision-making for study inclusion and exclusion. The system was evaluated 
across 9 reconstructed datasets derived from published HCC meta-analyses, with 
performance assessed using accuracy, agreement metrics (κ and 
prevalence-adjusted bias-adjusted κ), recall, precision, F1-scores, and 
computational efficiency parameters (processing time and cost).
RESULTS: The framework demonstrated good performance, with a weighted accuracy 
of 0.96 and substantial agreement (prevalence-adjusted bias-adjusted κ=0.91), 
achieving high weighted recall (0.90) but modest weighted precision (0.15) and 
F1-scores (0.22). Computational efficiency varied across datasets (processing 
time: 248-5850 s; cost: US $0.14-$3.68 per dataset).
CONCLUSIONS: This LLM-driven approach shows promise for accelerating evidence 
synthesis in HCC care by reducing screening time while maintaining 
methodological rigor. Key limitations related to clinical context sensitivity 
and error propagation highlight the need for reinforcement learning integration 
and domain-specific fine-tuning. LLM agent architectures with reinforcement 
learning offer a practical path for streamlining guideline updates, though 
further optimization is needed to improve specialization and reliability in 
complex clinical settings.

©Chen Pan, Wei Lu, Bingliang Chen, Gang Zhang, Zhiming Yang, Jingcheng Hao. 
Originally published in JMIR Medical Informatics (https://medinform.jmir.org), 
08.09.2025.

DOI: 10.2196/76252
PMCID: PMC12455167
PMID: 40921065 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


72. PLoS One. 2025 Sep 5;20(9):e0330217. doi: 10.1371/journal.pone.0330217. 
eCollection 2025.

Frankenstein, thematic analysis and generative artificial intelligence: Quality 
appraisal methods and considerations for qualitative research.

Jowsey T(1), Stapleton P(2), Campbell S(2), Davidson A(3), McGillivray C(2), 
Maugeri I(1), Lee M(2), Keogh J(1).

Author information:
(1)Faculty of Health Sciences and Medicine, Bond University, Gold Coast, 
Australia.
(2)Faculty of Society and Design, Bond University, Gold Coast, Australia.
(3)Institute for Evidence-Based Healthcare, Bond University, Gold Coast, 
Australia.

Erratum in
    PLoS One. 2025 Nov 25;20(11):e0337734. doi: 10.1371/journal.pone.0337734.

OBJECTIVE: To determine accuracy and efficiency of using generative artificial 
intelligence (GenAI) to undertake thematic analysis.
INTRODUCTION: With the increasing use of GenAI in data analysis, testing the 
reliability and suitability of using GenAI to conduct qualitative data analysis 
is needed. We propose a method for researchers to assess reliability of GenAI 
outputs using deidentified qualitative datasets.
METHODS: We searched three databases (United Kingdom Data Service, Figshare, and 
Google Scholar) and five journals (PlosOne, Social Science and Medicine, 
Qualitative Inquiry, Qualitative Research, Sociology Health Review) to identify 
studies on health-related topics, published prior to whereby: humans undertook 
thematic analysis and published both their analysis in a peer-reviewed journal 
and the associated dataset. We prompted a closed system GenAI (Microsoft 
Copilot) to undertake thematic analysis of these datasets and analysed the GenAI 
outputs in comparison with human outputs. Measures include time (GenAI only), 
accuracy, overlap with human analysis, and reliability of selected data and 
quotes.
RESULTS: Five studies were identified that met our inclusion criteria. The 
themes identified by human researchers and Copilot showed minimal overlap, with 
human researchers often using discursive thematic analyses (40%) and Copilot 
focusing on thematic analysis (100%). Copilot's outputs often included 
fabricated quotes (58% SD = 45%) and none of the Copilot outputs provided 
participant spread by theme. Additionally, Copilot's outputs primarily drew 
themes and quotes from the first 2-3 pages of textual data, rather than from the 
entire dataset. Human researchers provided broader representation and accurate 
quotes (79% quotes were correct, SD = 27%).
CONCLUSIONS: Based on these results, we cannot recommend the current version of 
Copilot for undertaking thematic analyses. This study raises concerns about the 
validity of both human-generated and GenAI-generated qualitative data analysis 
and reporting.

Copyright: © 2025 Jowsey et al. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0330217
PMCID: PMC12412986
PMID: 40911617 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


73. J Med Syst. 2025 Sep 4;49(1):110. doi: 10.1007/s10916-025-02246-4.

How Well Do ChatGPT and Claude Perform in Study Selection for Systematic Review 
in Obstetrics.

Insuk S(1), Boonpattharatthiti K(2)(3), Booncharoen C(1), Chaipitak P(1), Rashid 
M(4), Veettil SK(5)(6), Lai NM(7), Chaiyakunapruk N(4)(8), Dhippayom T(9)(10).

Author information:
(1)Faculty of Pharmaceutical Sciences, Naresuan University, Phitsanulok, 
Thailand.
(2)The Research Unit of Evidence Synthesis (TRUES), Faculty of Pharmaceutical 
Sciences, Naresuan University, Phitsanulok, Thailand.
(3)Faculty of Pharmaceutical Sciences, Burapha University, Chon buri, Thailand.
(4)Department of Pharmacotherapy, College of Pharmacy, University of Utah, Salt 
Lake City, UT, USA.
(5)Department of Pharmacy practice, School of Pharmacy, IMU University, Kuala 
Lumpur, Malaysia.
(6)School of Medicine, Taylor's University, Selangor, Subang Jaya, Malaysia.
(7)School of Medicine, Faculty of Health and Medical Sciences, Taylor's 
University, Subang Jaya, Malaysia.
(8)IDEAS Centre, Veterans Affairs Salt Lake City Healthcare System, Salt Lake 
City, UT, USA.
(9)The Research Unit of Evidence Synthesis (TRUES), Faculty of Pharmaceutical 
Sciences, Naresuan University, Phitsanulok, Thailand. teerapond@nu.ac.th.
(10)Department of Pharmacotherapy, College of Pharmacy, University of Utah, Salt 
Lake City, UT, USA. teerapond@nu.ac.th.

The use of generative AI in systematic review workflows has gained attention for 
enhancing study selection efficiency. However, evidence on its screening 
performance remains inconclusive, and direct comparisons between different 
generative AI models are still limited. The objective of this study is to 
evaluate the performance of ChatGPT-4o and Claude 3.5 Sonnet in the study 
selection process of a systematic review in obstetrics. A literature search was 
conducted using PubMed, EMBASE, Cochrane CENTRAL, and EBSCO Open Dissertations 
from inception till February 2024. Titles and abstracts were screened using a 
structured prompt-based approach, comparing decisions by ChatGPT, Claude and 
junior researchers with decisions by an experienced researcher serving as the 
reference standard. For the full-text review, short and long prompt strategies 
were applied. We reported title/abstract screening and full-text review 
performances using accuracy, sensitivity (recall), precision, F1-score, and 
negative predictive value. In the title/abstract screening phase, human 
researchers demonstrated the highest accuracy (0.9593), followed by Claude 
(0.9448) and ChatGPT (0.9138). The F1-score was the highest among human 
researchers (0.3853), followed by Claude (0.3724) and ChatGPT (0.2755). Negative 
predictive value (NPV) was high across all screeners: ChatGPT (0.9959), Claude 
(0.9961), and human researchers (0.9924). In the full-text screening phase, 
ChatGPT with a short prompt achieved the highest accuracy (0.904), highest 
F1-score (0.90), and NPV of 1.00, surpassing the performance of Claude and human 
researchers. Generative AI models perform close to human levels in study 
selection, as evidenced in obstetrics. Further research should explore their 
integration into evidence synthesis across different fields.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02246-4
PMID: 40906005 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Human Ethics and Consent to 
Participate: Not applicable. Our research did not require Institutional Review 
Board (IRB) review or approval as it did not involve human subjects, 
identifiable private information, or biological specimens that would fall under 
IRB oversight requirements. Clinical Trial Number: Not applicable. Competing 
Interests: The authors declare no competing interests.


74. BMC Med Inform Decis Mak. 2025 Sep 1;25(1):325. doi: 10.1186/s12911-025-03035-2.

Comparison of the readability of ChatGPT and Bard in medical communication: a 
meta-analysis.

DeTemple DE(1)(2)(3), Meine TC(4)(5).

Author information:
(1)Clinic for General, Visceral and Transplant Surgery, Hannover Medical School, 
Carl-Neuberg-Strasse 1, 30625, Hannover, Germany.
(2)PRACTIS Clinician Scientist Program, Dean's Office for Academic Career 
Development, Hannover Medical School, Hannover, Germany.
(3)Clinic for General, Visceral, Pediatric and Transplantation Surgery, 
University Hospital RWTH Aachen, Aachen, Germany.
(4)PRACTIS Clinician Scientist Program, Dean's Office for Academic Career 
Development, Hannover Medical School, Hannover, Germany. 
meine.timo@mh-hannover.de.
(5)Institute for Diagnostic and Interventional Radiology, Hannover Medical 
School, Carl-Neuberg-Strasse 1, 30625, Hannover, Germany. 
meine.timo@mh-hannover.de.

BACKGROUND: To synthesize the results of various studies on the readability of 
ChatGPT and Bard in medical communication.
METHODS: Systemic literature research was conducted in PubMed, Ovid/Medline, 
CINAHL, Web-of-Science, Scopus and GoogleScholar to detect relevant publications 
(inclusion criteria: original research articles, English language, medical 
topic, ChatGPT-3.5/-4.0, Bard/Gemini, Flesch Reading Ease Score (FRE), Flesch 
Kincaid Grade Level (FKGL)). Study quality was analyzed using modified 
Downs-and-Black checklist (max. 8 points), adapted for studies on large language 
model. Analysis was performed on text simplification and/or text generation with 
ChatGPT-3.5/-4.0 versus Bard/Gemini. Meta-analysis was conducted, if outcome 
parameter was reported ≥ 3 studies. In addition, subgroup-analyses among 
different chatbot versions were performed. Publication bias was analyzed.
RESULTS: Overall, 59 studies with 2342 items were analyzed. Study quality was 
limited with a mean of 6 points for FRE and 7 points for FKGL. Meta-analysis of 
text simplification for FRE between ChatGPT-3.5/-4.0 and Bard/Gemini was not 
significant (mean difference (MD):5.03; 95%-confidence interval 
(CI):-20.05,30.11; p = 0.48). FKGL of simplified texts of ChatGPT-3.5/-4.0 and 
Bard/Gemini was borderline significant (MD:-1.59; CI:-3.15,-0.04; p = 0.05) and 
subgroup-analysis between ChatGPT-4.0 and Bard was not significant (MD:-1.68; 
CI:-3.53,0.17; p = 0.07). Focused on text acquisition, MD for FRE and FKGL of 
studies on ChatGPT-3.5/-4.0- and Bard/Gemini-generated texts were significant 
(MD:-10.36; CI:-13.08,-7.64; p < 0.01 / MD:1.62; CI:1.09,2.15; p < 0.01). 
Subgroup-analysis of FRE was significant for ChatGPT-3.5 vs. Bard (MD:-16.07, 
CI:-24.90,-7.25; p < 0.01), ChatGPT-3.5 vs. Gemini (MD:-4.51; CI:-8.73,-0.29: 
p = 0.04), ChatGPT-4.0 vs. Bard (MD:-12.01, CI:-16.22,-7.81; p < 0.01) and 
ChatGPT-4.0 vs. Gemini (MD:-7.91, CI:-11.68,-4.15; p < 0.01). Analysis of FKGL 
in the subgroups was significant for ChatGPT-3.5 vs. Bard (MD:2.85, 
CI:1.98,3.73; p < 0.01), ChatGPT-3.5 vs. Gemini (MD:1.21, CI:0.50,1.93; 
p < 0.01) and ChatGPT-4.0 vs. Gemini (MD:1.95, CI:1.05,2.86; p < 0.01), but it 
was not significant for ChatGPT-4.0 vs. Bard (MD:0.64, CI:-0.46,1.74; p = 0.24). 
Egger's test was significant in text generation for FRE and FKGL (p < 0.01 / 
p < 0.01) and in subgroup ChatGPT-4.0 vs. Bard and ChatGPT-4.0 vs. Gemini 
(p < 0.01 / p = 0.02) for FRE as well as in subgroups ChatGPT-3.5 vs. Bard and 
ChatGPT-4.0 vs. Gemini for FKGL (p < 0.01 / p < 0.01).
CONCLUSION: Readability of spontaneously generated texts by Bard/Gemini was 
slightly superior compared to ChatGPT-3.5/-4.0 and readability of simplified 
texts by ChatGPT-3.5/-4.0 tended to be improved compared to Bard. Results are 
limited due study quality and publication bias. Standardized reporting could 
improve study quality and chatbot development.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-03035-2
PMCID: PMC12403948
PMID: 40890707 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


75. J Am Med Inform Assoc. 2025 Nov 1;32(11):1718-1725. doi: 10.1093/jamia/ocaf137.

Enabling inclusive systematic reviews: incorporating preprint articles with 
large language model-driven evaluations.

Yang R(1), Tong J(2), Wang H(3), Huang H(4), Hu Z(5), Li P(3), Liu N(1), 
Lindsell CJ(3)(6), Pencina MJ(3), Chen Y(7), Hong C(3)(6).

Author information:
(1)Centre for Quantitative Medicine, Duke-NUS Medical School, Singapore, 169857, 
Singapore.
(2)Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health, 
Baltimore, MD, 21205, United States.
(3)Department of Biostatistics and Bioinformatics, Duke School of Medicine, 
Durham, NC, 27710, United States.
(4)Children's Healthcare of Atlanta, Emory University School of Medicine, 
Atlanta, GA, 30329, United States.
(5)Department of Genome Sciences, University of Virginia, Charlottesville, VA, 
22903, United States.
(6)Duke Clinical Research Institute, Duke University, Durham, NC, 27701, United 
States.
(7)Department of Biostatistics, Epidemiology and Informatics, Perelman School of 
Medicine, University of Pennsylvania, Philadelphia, PA, 19104, United States.

OBJECTIVES: Systematic reviews in comparative effectiveness research require 
timely evidence synthesis. With the rapid advancement of medical research, 
preprint articles play an increasingly important role in accelerating knowledge 
dissemination. However, as preprint articles are not peer-reviewed before 
publication, their quality varies significantly, posing challenges for evidence 
inclusion in systematic reviews.
MATERIALS AND METHODS: We developed AutoConfidenceScore (automated confidence 
score assessment), an advanced framework for predicting preprint publication, 
which reduces reliance on manual curation and expands the range of predictors, 
including three key advancements: (1) automated data extraction using natural 
language processing techniques, (2) semantic embeddings of titles and abstracts, 
and (3) large language model (LLM)-driven evaluation scores. Additionally, we 
employed two prediction models: a random forest classifier for binary outcome 
and a survival cure model that predicts both binary outcome and publication risk 
over time.
RESULTS: The random forest classifier achieved an area under the receiver 
operating characteristic curve (AUROC) of 0.747 using all features. The survival 
cure model achieved an AUROC of 0.731 for binary outcome prediction and a 
concordance index of 0.667 for time-to-publication risk.
DISCUSSION: Our study advances the framework for preprint publication prediction 
through automated data extraction and multiple feature integration. By combining 
semantic embeddings with LLM-driven evaluations, AutoConfidenceScore 
significantly enhances predictive performance while reducing manual annotation 
burden.
CONCLUSION: AutoConfidenceScore has the potential to facilitate incorporation of 
preprint articles during the appraisal phase of systematic reviews, supporting 
researchers in more effective utilization of preprint resources.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf137
PMCID: PMC12626217
PMID: 40886699 [Indexed for MEDLINE]

Conflict of interest statement: None declared.


76. Int J Med Inform. 2026 Jan;205:106091. doi: 10.1016/j.ijmedinf.2025.106091. Epub 
2025 Aug 28.

Performance and improvement strategies for adapting generative large language 
models for electronic health record applications: A systematic review.

Du X(1), Zhou Z(2), Wang Y(2), Chuang YW(3), Li Y(4), Yang R(4), Hong P(2), 
Bates DW(5), Zhou L(4).

Author information:
(1)Division of General Internal Medicine and Primary Care, Brigham and Women's 
Hospital, Boston, MA 02115, United States; Department of Medicine, Harvard 
Medical School, Boston, MA 02115, United States. Electronic address: 
xidu1@bwh.harvard.edu.
(2)Department of Computer Science, Brandeis University, Waltham, MA 02453, 
United States.
(3)Division of Nephrology, Department of Internal Medicine, Taichung Veterans 
General Hospital, Taichung 407219, Taiwan; Department of Post-Baccalaureate 
Medicine, College of Medicine, National Chung Hsing University, Taichung 402202, 
Taiwan; School of Medicine, College of Medicine, China Medical University, 
Taichung 404328, Taiwan.
(4)Division of General Internal Medicine and Primary Care, Brigham and Women's 
Hospital, Boston, MA 02115, United States; Department of Medicine, Harvard 
Medical School, Boston, MA 02115, United States.
(5)Division of General Internal Medicine and Primary Care, Brigham and Women's 
Hospital, Boston, MA 02115, United States; Department of Medicine, Harvard 
Medical School, Boston, MA 02115, United States; Department of Health Policy and 
Management, Harvard T.H. Chan School of Public Health, Boston, MA 02115, United 
States.

Update of
    medRxiv. 2025 Jun 22:2024.08.11.24311828. doi: 10.1101/2024.08.11.24311828.

PURPOSE: To synthesize performance and improvement strategies for adapting 
generative LLMs in EHR analyses and applications.
METHODS: We followed the PRISMA guidelines to conduct a systematic review of 
articles from PubMed and Web of Science published between January 1, 2023 and 
November 9, 2024. Multiple reviewers including biomedical informaticians and a 
clinician involved in the article reviewing process. Studies were included if 
they used generative LLMs to analyze real-world EHR data and reported 
quantitative performance evaluations for an improvement technique. The review 
identified key clinical applications, summarized performance and the improvement 
strategies.
RESULTS: Of the 18,735 articles retrieved, 196 met our criteria. 112 (57.1%) 
studies used generative LLMs for clinical decision support tasks, 40 (20.4%) 
studies involved documentation tasks, 39 (19.9%) studies involved information 
extraction tasks, 11 (5.6%) studies involved patient communication tasks, and 10 
(5.1%) studies included summarization tasks. Among the 196 studies, most studies 
(88.8%) did not quantitatively evaluate the LLM performance improvement 
strategies, with the rest twenty-four studies (12.2%) quantitatively evaluated 
the effectiveness of in-context learning (9 studies), fine-tuning (12 studies), 
multimodal integration (8 studies), and ensemble learning (2 studies). Three 
studies highlighted that few-shot prompting, fine-tuning, and multimodal data 
integration might not improve performance, and another two studies found that 
fine-tuning a smaller model could outperform a large model.
CONCLUSION: Applying a performance improvement strategy may not necessarily lead 
to performance improvement, and detailed guidelines regarding how to apply those 
strategies more effectively and safely are needed, which can be completed from 
more quantitative analysis in the future.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106091
PMCID: PMC12413914
PMID: 40885071 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


77. J Med Internet Res. 2025 Aug 18;27:e66100. doi: 10.2196/66100.

AI and Machine Learning Terminology in Medicine, Psychology, and Social 
Sciences: Tutorial and Practical Recommendations.

Cao B(1)(2)(3), Greiner R(1)(2)(4), Greenshaw A(1), Sui J(5).

Author information:
(1)Department of Psychiatry, University of Alberta, 4-142A Katz Group Centre for 
Research, 11315 - 87 Ave NW, Edmonton, AB, T6G 2B7, Canada, 1 7804929576.
(2)Department of Computing Science, Faculty of Science, University of Alberta, 
Edmonton, AB, Canada.
(3)School of Public Health, University of Alberta, Edmonton, AB, Canada.
(4)Alberta Machine Intelligence Institute (Amii), Edmonton, AB, Canada.
(5)School of Psychology, University of Aberdeen, Aberdeen, United Kingdom.

Recent applications of artificial intelligence (AI) and machine learning in 
medicine, psychology, and social sciences have led to common terminological 
confusions. In this paper, we review emerging evidence from systematic reviews 
documenting widespread misuse of key terms, particularly "prediction" being 
applied to studies merely demonstrating association or retrospective analysis. 
We clarify when "prediction" should be used and recommend using "prospective 
prediction" for future prediction; explain validation procedures essential for 
model generalizability; discuss overfitting and generalization in machine 
learning and traditional regression methods; clarify relationships between 
features, independent variables, predictors, risk factors, and causal factors; 
and clarify the hierarchical relationship between AI, machine learning, deep 
learning, large language models, and generative AI. We provide evidence-based 
recommendations for terminology use that can facilitate clearer communication 
among researchers from different disciplines and between the research community 
and the public, ultimately advancing the rigorous application of AI in medicine, 
psychology, and social sciences.

© Bo Cao, Russell Greiner, Andrew Greenshaw, Jie Sui. Originally published in 
the Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/66100
PMCID: PMC12360722
PMID: 40825233 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


78. JMIR Form Res. 2025 Aug 11;9:e68666. doi: 10.2196/68666.

Evaluating a Customized Version of ChatGPT for Systematic Review Data Extraction 
in Health Research: Development and Usability Study.

Sercombe J(#)(1), Bryant Z(#)(1), Wilson J(1).

Author information:
(1)The Matilda Centre for Research in Mental Health and Substance Use, 
University of Sydney, Jane Foss Russell Building (G02), Level 6, Sydney, 2006, 
Australia, 612 8627 9380.
(#)Contributed equally

BACKGROUND: Systematic reviews are essential for synthesizing research in health 
sciences; however, they are resource-intensive and prone to human error. The 
data extraction phase, in which key details of studies are identified and 
recorded in a systematic manner, may benefit from the application of automation 
processes. Recent advancements in artificial intelligence, specifically in large 
language models (LLMs) such as ChatGPT, may streamline this process.
OBJECTIVE: This study aimed to develop and evaluate a custom Generative 
Pre-Training Transformer (GPT), named Systematic Review Extractor Pro, for 
automating the data extraction phase of systematic reviews in health research.
METHODS: OpenAI's GPT Builder was used to create a GPT tailored to extract 
information from academic manuscripts. The Role, Instruction, Steps, End goal, 
and Narrowing (RISEN) framework was used to inform prompt engineering for the 
GPT. A sample of 20 studies from two distinct systematic reviews was used to 
evaluate the GPT's performance in extraction. Agreement rates between the GPT 
outputs and human reviewers were calculated for each study subsection.
RESULTS: The mean time for human data extraction was 36 minutes per study, 
compared to 26.6 seconds for GPT generation, followed by 13 minutes of human 
review. The GPT demonstrated high overall agreement rates with human reviewers, 
achieving 91.45% for review 1 and 89.31% for review 2. It was particularly 
accurate in extracting study characteristics (review 1: 95.25%; review 2: 
90.83%) and participant characteristics (review 1: 95.03%; review 2: 90.00%), 
with lower performance observed in more complex areas such as methodological 
characteristics (87.07%) and statistical results (77.50%). The GPT correctly 
extracted data in 14 instances (3.25% in review 1) and four instances (1.16% in 
review 2) when the human reviewer was incorrect.
CONCLUSIONS: The custom GPT significantly reduced extraction time and shows 
evidence that it can extract data with high accuracy, particularly for 
participant and study characteristics. This tool may offer a viable option for 
researchers seeking to reduce resource demands during the extraction phase, 
although more research is needed to evaluate test-retest reliability, 
performance across broader review types, and accuracy in extracting statistical 
data. The tool developed in the current study has been made open access.

© Jayden Sercombe, Zachary Bryant, Jack Wilson. Originally published in JMIR 
Formative Research (https://formative.jmir.org).

DOI: 10.2196/68666
PMCID: PMC12338963
PMID: 40789147 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


79. BMC Med Inform Decis Mak. 2025 Aug 7;25(1):293. doi: 10.1186/s12911-025-03138-w.

A comparative study of screening performance between abstrackr and GPT models: 
Systematic review and contextual analysis.

Xu S(1), Zhao Z(1), Liu X(1), Meng XL(2).

Author information:
(1)Department of Orthopaedic Surgery, Beijing Anzhen Hospital, Capital Medical 
University, Beijing, 100013, China.
(2)Department of Orthopaedic Surgery, Beijing Anzhen Hospital, Capital Medical 
University, Beijing, 100013, China. spinesurgeonmeng@ccmu.edu.cn.

BACKGROUND: Systematic reviews (SRs) and rapid reviews (RRs) are critical 
methodologies for synthesizing existing research evidence. However, the growing 
volume of literature has made the process of screening studies one of the most 
challenging steps in conducting systematic reviews.
METHODS: This systematic review aimed to compare the performance of Abstrackr 
and GPT models (including GPT-3.5 and GPT-4) in literature screening for 
systematic reviews. We identified relevant studies through comprehensive 
searches in PubMed, Cochrane Library, and Web of Science, focusing on those that 
provided key performance metrics such as recall, precision, specificity, and F1 
score.
RESULTS: GPT models demonstrated superior performance compared to Abstrackr in 
precision (0.51 vs. 0.21), specificity (0.84 vs. 0.71), and F1 score (0.52 vs. 
0.31), reflecting a higher overall efficiency and better balance in screening. 
This makes GPT models particularly effective in reducing false positives during 
fine-screening tasks.
CONCLUSION: Abstrackr and GPT models each offer distinct advantages in 
literature screening. Abstrackr is more suitable for the initial screening 
phases, whereas GPT models excel in fine-screening tasks. To optimize the 
efficiency and accuracy of systematic reviews, future screening tools could 
integrate the strengths of both models, potentially leading to the development 
of hybrid systems tailored to different stages of the screening process.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-03138-w
PMCID: PMC12329882
PMID: 40775694 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval: Not applicable. 
Consent for publication: Not applicable. Competing interests: The authors 
declare that they have no competing interests.


80. NPJ Digit Med. 2025 Aug 8;8(1):509. doi: 10.1038/s41746-025-01840-7.

Accelerating clinical evidence synthesis with large language models.

Wang Z(1)(2), Cao L(1), Danek B(1)(2), Jin Q(3), Lu Z(3), Sun J(4)(5)(6).

Author information:
(1)Siebel School of Computing and Data Science, University of Illinois 
Urbana-Champaign, Urbana, IL, USA.
(2)Keiji.AI Inc, Seattle, USA.
(3)Division of Intramural Research, National Library of Medicine, National 
Institutes of Health, Bethesda, MD, USA.
(4)Siebel School of Computing and Data Science, University of Illinois 
Urbana-Champaign, Urbana, IL, USA. jimeng@illinois.edu.
(5)Carle Illinois College of Medicine, University of Illinois Urbana-Champaign, 
Urbana, IL, USA. jimeng@illinois.edu.
(6)Keiji.AI Inc, Seattle, USA. jimeng@illinois.edu.

Clinical evidence synthesis largely relies on systematic reviews (SR) of 
clinical studies from medical literature. Here, we propose a generative 
artificial intelligence (AI) pipeline named TrialMind to streamline study 
search, study screening, and data extraction tasks in SR. We chose published SRs 
to build TrialReviewBench, which contains 100 SRs and 2,220 clinical studies. 
For study search, it achieves high recall rates (Ours 0.711-0.834 v.s. Human 
baseline 0.138-0.232). For study screening, TrialMind beats previous document 
ranking methods in a 1.5-2.6 fold change. For data extraction, it outperforms a 
GPT-4's accuracy by 16-32%. In a pilot study, human-AI collaboration with 
TrialMind improved recall by 71.4% and reduced screening time by 44.2%, while in 
data extraction, accuracy increased by 23.5% with a 63.4% time reduction. 
Medical experts preferred TrialMind's synthesized evidence over GPT-4's in 
62.5%-100% of cases. These findings show the promise of accelerating clinical 
evidence synthesis driven by human-AI collaboration.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01840-7
PMCID: PMC12331930
PMID: 40775042

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


81. JMIR Med Inform. 2025 Aug 7;13:e76636. doi: 10.2196/76636.

Current Landscape and Future Directions Regarding Generative Large Language 
Models in Stroke Care: Scoping Review.

Zhu X(1), Dai W(1), Evans R(2), Geng X(3), Mu A(4), Liu Z(1).

Author information:
(1)School of Medicine and Health Management, Tongji Medical College, Huazhong 
University of Science and Technology, Wuhan, China.
(2)Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada.
(3)Department of Physiology and Pathophysiology, School of Basic Medical 
Sciences, Peking University Health Science Center, Beijing, China.
(4)School of Ethnology and Sociology, Inner Mongolia University, Hohhot, China.

BACKGROUND: Stroke has a major impact on global health, causing long-term 
disability and straining health care resources. Generative large language models 
(gLLMs) have emerged as promising tools to help address these challenges, but 
their applications and reported performance in stroke care require comprehensive 
mapping and synthesis.
OBJECTIVE: The aim of this scoping review was to consolidate a fragmented 
evidence base and examine the current landscape, shortcomings, and future 
directions in the design, reporting, and evaluation of gLLM-based interventions 
in stroke care.
METHODS: In this scoping review, which adhered to the PRISMA-ScR (Preferred 
Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping 
Reviews) guidelines and the Population, Concept, and Context (PCC) framework, we 
searched 6 major scientific databases in December 2024 for gLLM-based 
interventions across the stroke care pathway, mapping their key characteristics 
and outcomes.
RESULTS: A total of 25 studies met the predefined eligibility criteria and were 
included for analysis. Retrospective designs predominated (n=16, 64%). Key 
applications of gLLMs included clinical decision-making support (n=10, 40%), 
administrative assistance (n=9, 36%), direct patient interaction (n=5, 20%), and 
automated literature review (n=1, 4%). Implementations mainly used generative 
pretrained transformer models accessed through task-prompted chat interfaces. In 
total, 5 key challenges were identified from the included studies during the 
implementation of gLLM-based interventions: ensuring factual alignment, 
maintaining system robustness, enhancing interpretability, optimizing 
efficiency, and facilitating clinical adoption.
CONCLUSIONS: The application of gLLMs in stroke care, while promising, remains 
relatively new, with most interventions reflecting early-stage or relatively 
simple implementations. Against this backdrop, critical gaps in research and 
clinical translation persist. To support the development of clinically impactful 
and trustworthy applications, we propose an actionable framework that 
prioritizes real-world evidence, mandates transparent technical reporting, 
broadens evaluation beyond output accuracy, strengthens validation of advanced 
task adaptation strategies, and investigates mechanisms for safe and effective 
human-gLLM interaction.

©XingCe Zhu, Wei Dai, Richard Evans, Xueyu Geng, Aruhan Mu, Zhiyong Liu. 
Originally published in JMIR Medical Informatics (https://medinform.jmir.org), 
07.08.2025.

DOI: 10.2196/76636
PMCID: PMC12371286
PMID: 40773746 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


82. J Biomed Inform. 2025 Sep;169:104875. doi: 10.1016/j.jbi.2025.104875. Epub 2025 
Aug 5.

Rapid review: Growing usage of Multimodal Large Language Models in healthcare.

Gupta P(1), Zhang Z(2), Song M(3), Michalowski M(3), Hu X(4), Stiglic G(5), 
Topaz M(6).

Author information:
(1)Columbia University, School of Nursing, NY, United States. Electronic 
address: pg2834@cumc.columbia.edu.
(2)Columbia University, School of Nursing, NY, United States; Columbia 
University, Data Science Institute, NY, United States.
(3)University of Minnesota, School of Nursing, Minneapolis, United States.
(4)Center for Data Science, Emory University, Atlanta, United States; Nell 
Hodgson Woodruff School of Nursing, Emory University, Atlanta, United States; 
Department of Biomedical Informatics, School of Medicine, Emory University, 
Atlanta, United States; Department of Computer Science, College of Arts and 
Sciences, Emory University, Atlanta, United States; Wallace H. Coulter 
Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta, 
United States.
(5)Faculty of Health Sciences, University of Maribor, Maribor, Slovenia; Faculty 
of Electrical Engineering and Computer Science, University of Maribor, Maribor, 
Slovenia; Usher Institute, University of Edinburgh, Edinburgh, United Kingdom.
(6)Columbia University, School of Nursing, NY, United States; Columbia 
University, Data Science Institute, NY, United States; VNS Health, NY, United 
States. Electronic address: mt3315@cumc.columbia.edu.

OBJECTIVE: Recent advancements in large language models (LLMs) have led to 
multimodal LLMs (MLLMs), which integrate multiple data modalities beyond text. 
Although MLLMs show promise, there is a gap in the literature that empirically 
demonstrates their impact in healthcare. This paper summarizes the applications 
of MLLMs in healthcare, highlighting their potential to transform health 
practices.
METHODS: A rapid literature review was conducted in August 2024 using World 
Health Organization (WHO) rapid-review methodology and PRISMA standards, with 
searches across four databases (Scopus, Medline, PubMed and ACM Digital Library) 
and top-tier conferences-including NeurIPS, ICML, AAAI, MICCAI, CVPR, ACL and 
EMNLP. Articles on MLLMs healthcare applications were included for analysis 
based on inclusion and exclusion criteria.
RESULTS: The search yielded 115 articles, 39 included in the final analysis. Of 
these, 77% appeared online (preprints and published) in 2024, reflecting the 
emergence of MLLMs. 80% of studies were from Asia and North America (mainly 
China and US), with Europe lagging. Studies split evenly between pre-built MLLMs 
evaluations (60% focused on GPT versions) and custom MLLMs/frameworks 
development with task-specific customizations. About 81% of studies examined 
MLLMs for diagnosis and reporting in radiology, pathology, and ophthalmology, 
with additional applications in education, surgery, and mental health. Prompting 
strategies, used in 80% of studies, improved performance in nearly half. 
However, evaluation practices were inconsistent with 67% reported accuracy. 
Error analysis was mostly anecdotal, with only 18% categorized failure types. 
Only 13% validated explainability through clinician feedback. Clinical 
deployment was demonstrated in just 3% of studies, and workflow integration, 
governance, and safety were rarely addressed.
DISCUSSION AND CONCLUSION: MLLMs offer substantial potential for healthcare 
transformation through multimodal data integration. Yet, methodological 
inconsistencies, limited validation, and underdeveloped deployment strategies 
highlight the need for standardized evaluation metrics, structured error 
analysis, and human-centered design to support safe, scalable, and trustworthy 
clinical adoption.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104875
PMID: 40754135 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


83. Artif Intell Med. 2025 Oct;168:103222. doi: 10.1016/j.artmed.2025.103222. Epub 
2025 Aug 1.

Reporting guideline for chatbot health advice studies: The CHART statement.

CHART Collaborative; Huo B(1), Collins G(2), Chartash D(3), Thirunavukarasu 
A(4), Flanagin A(5), Iorio A(6), Cacciamani G(7), Chen X(8), Liu N(9), Mathur 
P(10), Chan AW(11), Laine C(12), Pacella D(13), Berkwits M(14), Antoniou SA(15), 
Camaradou JC(16), Canfield C(17), Mittelman M(18), Feeney T(19), Loder E(20), 
Agha R(21), Saha A(22), Mayol J(23), Sunjaya A(24), Harvey H(25), Ng JY(26), 
McKechnie T(27), Lee Y(28), Verma N(29), Stiglic G(30), McCradden M(31), Ramji 
K(32), Boudreau V(27), Ortenzi M(33), Meerpohl J(34), Vandvik PO(35), Agoritsas 
T(36), Samuel D(37), Frankish H(38), Anderson M(39), Yao X(22), Loeb S(40), 
Lokker C(6), Liu X(41), Guallar E(42), Guyatt G(43).

Author information:
(1)Division of General Surgery, Department of Surgery, McMaster University, 
Hamilton, Canada. Electronic address: brighthuo@dal.ca.
(2)UK EQUATOR Centre, University of Oxford, Oxford, UK; Centre for Statistics in 
Medicine, Nuffield Department of Orthopaedics, Rheumatology & Musculoskeletal 
Sciences, Botnar Research Centre, University of Oxford, Oxford, UK.
(3)Department of Biomedical Informatics and Data Science, Yale University School 
of Medicine, USA.
(4)Nuffield Department of Clinical Neurosciences, Medical Sciences Division, 
University of Oxford, Oxford, UK.
(5)JAMA and JAMA Network, American Medical Association, USA.
(6)Department of Health Research Methods, Evidence, and Impact, Department of 
Medicine, McMaster University, Canada.
(7)USC Institute of Urology and Catherine and Joseph Aresty, Department of 
Urology, Keck School of Medicine, University of Southern California, Los 
Angeles, CA, USA; Artificial Intelligence Center at USC Urology, USC Institute 
of Urology, University of Southern California, Los Angeles, CA, USA.
(8)Sports Medicine Center, West China Hospital, Sichuan University, Chengdu, 
China; Department of Orthopedics and Orthopedic Research Institute, West China 
Hospital, Sichuan University, Chengdu, China.
(9)Duke-NUS Medical School, National University of Singapore, Singapore, 
Singapore.
(10)Cleveland Clinic, Case Western Reserve University, USA.
(11)Dept of Medicine, Women's College Research Institute, University of Toronto, 
Canada.
(12)Annals of Internal Medicine, American College of Physicians, USA; American 
College of Physicians, USA.
(13)Department of Public Health, University of Naples Federico II, Italy.
(14)Director, Office of Science Dissemination, Office of Science, Centers for 
Disease Control and Prevention, Atlanta, GA, USA.
(15)Department of General Surgery, Papageorgiou General Hospital, Thessaloniki, 
Greece.
(16)British Psychological Society, University of Plymouth, UK.
(17)Innovation Support Unit, Department of Family Practice, University of 
British Columbia, Canada.
(18)Patient SME, Independent Cybersecurity Professional, USA.
(19)The BMJ, London, UK; Department of Epidemiology, Gillings School of Global 
Public Health, University of North Carolina at Chapel Hill, Chapel Hill, NC, 
USA.
(20)The BMJ, London, UK; Department of Neurology, Brigham and Women's Hospital, 
Boston, MA, USA.
(21)International Journal of Surgery, USA; Eworkflow Ltd, London, UK.
(22)Department of Oncology, McMaster University, Canada.
(23)Hospital Clinico San Carlos, Instituto de Investigación Sanitaria San 
Carlos, Facultad de Medicina Universidad Complutense de Madrid, Spain.
(24)The George Institute for Global Health, Tyree Institute of Health 
Engineering, UNSW Engineering, School of Population Health, UNSW Medicine and 
Health, Australia.
(25)Hardian Health, UK.
(26)Centre for Journalology, Ottawa Hospital Research Institute, Ottawa, Canada.
(27)Division of General Surgery, Department of Surgery, McMaster University, 
Hamilton, Canada.
(28)Division of General Surgery, Department of Surgery, McMaster University, 
Hamilton, Canada; Digestive Diseases Institute, Cleveland Clinic, Cleveland, OH, 
USA.
(29)Postgraduate Institute of Medical Education and Research, Chandigarh, India.
(30)University of Maribor, Slovenia.
(31)Australian Institute for Machine Learning (AIML), Australia.
(32)Phelix AI, Canada.
(33)Università Politecnica delle Marche, Clinica di Chirurgia Generale e 
d'Urgenza, Italy.
(34)Institute for Evidence in Medicine, Medical Center & Faculty of Medicine, 
University of Freiburg, Germany; Cochrane Germany, Cochrane Germany Foundation, 
Freiburg, Germany.
(35)Cochrane Germany, Cochrane Germany Foundation, Freiburg, Germany; MAGIC 
Evidence Ecosystem Foundation, Norway.
(36)Department of Health Research Methods, Evidence, and Impact, Department of 
Medicine, McMaster University, Canada; MAGIC Evidence Ecosystem Foundation, 
Norway; University Hospitals of Geneva, Switzerland.
(37)The Lancet Digital Health, UK.
(38)The Lancet, UK.
(39)NIHR Clinical Lecturer, Health Organisation, Policy, Economics (HOPE), 
Centre for Primary Care & Health Services Research, The University of 
Manchester, UK; Senior Visiting Fellow, LSE Health, London School of Economics 
and Political Science, UK.
(40)New York University, Langone Health, USA.
(41)College of Medicine and Health, University of Birmingham, UK.
(42)School of Global Public Health, New York University, USA.
(43)Department of Health Research Methods, Evidence, and Impact, Department of 
Medicine, McMaster University, Canada; MAGIC Evidence Ecosystem Foundation, 
Norway.

The Chatbot Assessment Reporting Tool (CHART) is a reporting guideline developed 
to provide reporting recommendations for studies evaluating the performance of 
generative artificial intelligence (AI)-driven chatbots when summarizing 
clinical evidence and providing health advice, referred to as Chatbot Health 
Advice (CHA) studies. CHART was developed in several phases after performing a 
comprehensive systematic review to identify variation in the conduct, reporting 
and methodology in CHA studies. Findings from the review were used to develop a 
draft checklist that was revised through an international, multidisciplinary 
modified asynchronous Delphi consensus process of 531 stakeholders, three 
synchronous panel consensus meetings of 48 stakeholders, and subsequent pilot 
testing of the checklist. CHART includes 12 items and 39 subitems to promote 
transparent and comprehensive reporting of CHA studies. These include Title 
(subitem 1a), Abstract/Summary (subitem 1b), Background (subitems 2ab), Model 
Identifiers (subitem 3ab), Model Details (subitems 4abc), Prompt Engineering 
(subitems 5ab), Query Strategy (subitems 6abcd), Performance Evaluation 
(subitems 7ab), Sample Size (subitem 8), Data Analysis (subitem 9a), Results 
(subitems 10abc), Discussion (subitems 11abc), Disclosures (subitem 12a), 
Funding (subitem 12b), Ethics (subitem 12c), Protocol (subitem 12d), and Data 
Availability (subitem 12e). The CHART checklist and corresponding methodological 
diagram were designed to support key stakeholders including clinicians, 
researchers, editors, peer reviewers, and readers in reporting, understanding, 
and interpreting the findings of CHA studies.

Copyright © 2025 The Author(s). Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.artmed.2025.103222
PMID: 40753040 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest All authors 
have completed the ICMJE uniform disclosure form at 
www.icmje.org/disclosure-of-interest/and declare: GSC is a National Institute 
for Health and Care Research (NIHR) Senior Investigator. The views expressed in 
this article are those of the author(s) and not necessarily those of the NIHR, 
or the Department of Health and Social Care; AJT has received funding from 
HealthSense to investigate evidence-based medicine applications of large 
language models. PM is the co-founder of BrainX LLC; AS has received research 
funding from the Australian government and is co-founder of BantingMed Pty Ltd.; 
DS is the Acting Deputy Editor for the Lancet Digital Health; MM has received 
research funding from The Hospital Research Founding Group; TF sits on the 
executive committee of MDEpiNet; HF is a Senior Executive Editor for The Lancet; 
CL is the Editor in Chief of Annals of Internal Medicine; AF is Executive 
Managing Editor and Vice President, Editorial Operations, JAMA and The JAMA 
Network; TF and EL are journal editors for the BMJ; RA is the Editor in Chief of 
International Journal of Surgery; GS is an Executive Editor of Artificial 
Intelligence in Medicine; SL is a paid consultant for Astellas; DP has received 
research funding from the Italian Ministry of University and Research; MO is a 
paid consultant for Theator; TA, POV, GG are board members of the MAGIC Evidence 
Ecosystem Foundation (www.magicproject.org), a non-for profit organisation, 
which conducts research and evidence appraisal and guideline methodology and 
implementation, and which provides a authoring and publication software 
(MAGICapp) for evidence summaries, guidelines and decision aids.


84. BMJ. 2025 Aug 1;390:e083305. doi: 10.1136/bmj-2024-083305.

Reporting guidelines for chatbot health advice studies: explanation and 
elaboration for the Chatbot Assessment Reporting Tool (CHART).

CHART Collaborative.

Collaborators: Huo B, Thirunavukarasu AJ, Collins GS, Chartash D, Flanagin A, 
Iorio A, Cacciamani G, Chen X, Liu N, Mathur P, Chan AW, Laine C, Pacella D, 
Berkwits M, Antoniou SA, Camaradou JC, Canfield C, Mittelman M, Feeney T, Loder 
E, Agha R, Saha A, Mayol J, Sunjaya A, Harvey H, Ng JY, McKechnie T, Lee Y, 
Verma N, Stiglic G, McCradden M, Ramji K, Boudreau V, Ortenzi M, Meerpohl J, 
Vandvik PO, Agoritsas T, Samuel D, Frankish H, Anderson M, Yao X, Loeb S, Lokker 
C, Liu X, Guallar E, Guyatt G.

The Chatbot Assessment Reporting Tool (CHART) reporting guideline promotes 
transparent and comprehensive reporting of studies evaluating the performance of 
generative artificial intelligence (AI)-driven chatbots for the purposes of 
summarising clinical evidence and providing health advice, referred to here as 
chatbot health advice (CHA) studies. CHART is the product of an international, 
multi-phase, consensus based initiative involving various stakeholders and 
comprises a 12-item checklist with 39 subitems. The checklist includes items on 
open science, title and abstract, introduction, model identification, model 
details, prompt engineering, query strategy, performance definition and 
evaluation, statistical analysis, results, discussion, with an accompanying flow 
diagram. Each item includes distinct subitems. This explanation and elaboration 
article discusses each subitem and provides a detailed rationale for its 
inclusion in the CHART checklist.

DOI: 10.1136/bmj-2024-083305
PMCID: PMC12492839
PMID: 40750271

Conflict of interest statement: Competing interests: All authors have completed 
the ICMJE uniform disclosure form at www.icmje.org/disclosure-of-interest/ and 
declare: support from the First Cut competition and the postgraduate medical 
education committee at McMaster University for the submitted work. GSC is a 
National Institute for Health and Care Research (NIHR) Senior Investigator. The 
views expressed in this article are those of the author(s) and not necessarily 
those of the NIHR, or the Department of Health and Social Care; AJT has received 
funding from HealthSense to investigate evidence based medicine applications of 
large language models. PM is the co-founder of BrainX; AS has received research 
funding from the Australian government and is co-founder of BantingMed Pty; DS 
is the acting deputy editor for The Lancet Digital Health; MM has received 
research funding from the Hospital Research Founding Group; TF sits on the 
executive committee of MDEpiNet; HF is a senior executive editor for The Lancet; 
CL is editor-in-chief of Annals of Internal Medicine; AF is executive managing 
editor and vice president, editorial operations, at JAMA and the JAMA Network; 
TF and EL are journal editors for The BMJ; RA is the editor-in-chief of the 
International Journal of Surgery; GS is an executive editor of Artificial 
Intelligence in Medicine; SL is a paid consultant for Astellas; DP has received 
research funding from the Italian Ministry of University and Research; MO is a 
paid consultant for Theator; TA, POV, and GG are board member of the MAGIC 
Evidence Ecosystem Foundation (www.magicproject.org), a non-for profit 
organisation that conducts research and evidence appraisal and guideline 
methodology and implementation, and provides authoring and publication software 
(MAGICapp) for evidence summaries, guidelines, and decision aids.


85. PLoS One. 2025 Aug 1;20(8):e0324369. doi: 10.1371/journal.pone.0324369. 
eCollection 2025.

The role of music in ADHD: A multi-dimensional computational and theoretical 
analysis.

Achuthan K(1), Khobragade S(1).

Author information:
(1)Center for Cybersecurity Systems and Networks, Amrita Vishwa Vidyapeetham, 
Amritapuri, Kollam, Kerala, India.

Attention-Deficit Hyperactivity Disorder (ADHD), a neurodevelopmental disorder 
affecting children and adults worldwide, has seen a significant rise in 
diagnoses and medication prescriptions in recent decades. This trend has 
emphasized the need for non-pharmacological interventions such as music to aid 
ADHD management. This study explores the musical experiences of individuals with 
ADHD through a comprehensive analysis of user-generated content from the Reddit 
r/ADHD community between 2014-2024. Advanced computational techniques, including 
large language models such as Gemini 1.5 Pro and LLAMA 3.1 were employed for 
data extraction and categorization. Additionally, APIs from digital streaming 
platforms were utilized to analyze musical characteristics and lyrical content 
of 9,215 tracks across three distinct categories: focus music, stuck songs, and 
general purpose. Insights from selective attention, emotion arousal and mood 
congruence theories were used to interpret the findings. Statistical analysis 
revealed significant variations in musical characteristics, with 
instrumentalness showing the largest effect size across contexts, suggesting 
unique musical preferences among individuals with ADHD. Correlation analyses 
uncovered complex interrelationships between musical attributes, particularly in 
focus music, where energy, speechiness, and instrumental characteristics 
displayed distinctive patterns. The sentiment and popularity analysis of lyrics 
further illuminated the emotional landscape of music in ADHD experiences, 
revealing a strategic approach to musical selection as a potential cognitive and 
emotional self-regulation mechanism.

Copyright: © 2025 Achuthan, Khobragade. This is an open access article 
distributed under the terms of the Creative Commons Attribution License, which 
permits unrestricted use, distribution, and reproduction in any medium, provided 
the original author and source are credited.

DOI: 10.1371/journal.pone.0324369
PMCID: PMC12316199
PMID: 40748879 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that they have no known 
competing financial interests or personal relationships that could have appeared 
to influence the work reported in this article.


86. Int J Med Inform. 2025 Dec;204:106048. doi: 10.1016/j.ijmedinf.2025.106048. Epub 
2025 Jul 23.

Leveraging open-source large language models (LLMs) in scoping reviews: a case 
study on disability and AI applications.

Bayani A(1), Epoh Ewane LP(2), Oliveira Dos Anjos DS(1), Mac-Seing M(2), Nikiema 
JN(3).

Author information:
(1)Centre de recherche en santé publique, Université de Montréal et CIUSSS du 
Centre-Sud-de-l'Île-de-Montréal, Montréal, Canada; Laboratoire Transformation 
Numérique en Santé (LabTNS), Québec, Canada.
(2)Centre de recherche en santé publique, Université de Montréal et CIUSSS du 
Centre-Sud-de-l'Île-de-Montréal, Montréal, Canada; Department of Social and 
Preventive Medicine, School of Public Health & Centre de Recherche en Santé 
Publique, Université de Montréal, Montreal, QC, Canada; Laboratoire Solidarités 
and Global Health, Québec, Canada.
(3)Centre de recherche en santé publique, Université de Montréal et CIUSSS du 
Centre-Sud-de-l'Île-de-Montréal, Montréal, Canada; Laboratoire Transformation 
Numérique en Santé (LabTNS), Québec, Canada; Department of Management, 
Evaluation and Health Policy, School of Public Health, Université de Montréal, 
Québec, Canada. Electronic address: jean.nikiema@umontreal.ca.

BACKGROUND: Large language models (LLMs) have the potential to offer solutions 
for automating many of the manual tasks involved in scientific reviews, 
including data extraction, literature screening, summarization, and quality 
assessment.
OBJECTIVES: This study aims to evaluate the performance of LLMs in the task of 
title and abstract screening and full-text data extraction of a scoping review 
study, by identifying their effectiveness, efficiency, and potential integration 
into human-based and manual tasks.
MATERIALS AND METHOD: The following key three steps of a scientific scoping 
review were automated: 1) Title and Abstract Screening, 2) Full-Text Screening, 
and 3) Data Extraction based on nine study dimensions. The four most recent 
lightweight open-source LLMs -Mistral, Vicuna, and Llama 3.2 with 1B and 3B 
parameters- were applied and evaluated through the steps.
RESULTS: Llama 3.2-3B demonstrated the best performance in the title and 
abstract screening, achieving an accuracy of 66 %, excelling in the exclusion of 
papers. For full-text screening, it maintained the highest overall accuracy of 
65 %, effectively identifying excluded papers. In data extraction, the Mistral 
model outperformed others across most dimensions, though Llama 3.2-3B excelled 
in extracting objectives and study implications.
DISCUSSION AND CONCLUSION: The present study underscores both the potential and 
limitations of LLMs in automating scoping reviews. Automating the entire scoping 
review without human intervention is sub-optimal. Using a more controlled 
approach balances the strengths of LLMs with the need for human judgment, 
supporting not only the replication of scientific reviews but also their 
continuous refinement and follow-up over time.

Copyright © 2025 The Author(s). Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106048
PMID: 40729777 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


87. J Med Internet Res. 2025 Jul 25;27:e76964. doi: 10.2196/76964.

Foundation Models for Generative AI in Time-Series Forecasting.

Beltramin D(#)(1), Bousquet C(#)(2)(3).

Author information:
(1)Medical Information Department, Hospices Civils de Lyon, Lyon, France.
(2)Laboratory of Medical Informatics and Knowledge Engineering in e-Health, 
Sorbonne University, Paris, France.
(3)Public Health Service and Medical Information, Centre Hospitalier 
Universitaire de Saint-Étienne, Saint-Etienne, France.
(#)Contributed equally

Erratum in
    J Med Internet Res. 27:e79605.

Comment in
    J Med Internet Res. 27:e79772.

Comment on
    J Med Internet Res. 27:e59792.

DOI: 10.2196/76964
PMCID: PMC12337798
PMID: 40712136

Conflict of interest statement: Conflicts of Interest: None declared.


88. J Med Internet Res. 2025 Jul 25;27:e79772. doi: 10.2196/79772.

Authors' Reply: Foundation Models for Generative AI in Time-Series Forecasting.

He R(1)(2), Chiang J(2)(3).

Author information:
(1)Department of Computer Science, University of California, Los Angeles, Los 
Angeles, CA, United States.
(2)Department of Computational Medicine, University of California, Los Angeles, 
Los Angeles, CA, United States.
(3)Department of Neurosurgery, David Geffen School of Medicine, University of 
California, Los Angeles, Los Angeles, CA, United States.

Erratum in
    J Med Internet Res. 27:e79605.

Comment on
    J Med Internet Res. 27:e76964.
    J Med Internet Res. 27:e59792.

DOI: 10.2196/79772
PMCID: PMC12356049
PMID: 40712133

Conflict of interest statement: Conflicts of Interest: None declared.


89. PLoS One. 2025 Jul 24;20(7):e0327584. doi: 10.1371/journal.pone.0327584. 
eCollection 2025.

Evaluating GPT-4's role in critical patient management in emergency departments.

Yiğit Y(1)(2), Günay S(3), Öztürk A(3), Alkahlout B(2).

Author information:
(1)Blizard Institute, Queen Mary University, London, United Kingdom.
(2)Hamad Medical Corporation, Department of Emergency Medicine, Doha, Qatar.
(3)Hitit University, Çorum Erol Olçok Education and Research Hospital, 
Department of Emergency Medicine, Çorum, Turkey.

INTRODUCTION: Recent advancements in artificial intelligence (AI) have 
introduced tools like ChatGPT-4, capable of interpreting visual data, including 
ECGs. In our study,we aimed to investigate the effectiveness of GPT-4 in 
interpreting ECGs and managing patient care in emergency settings.
METHODS: Conducted from April to May 2024, this study evaluated GPT-4 using 
twenty case scenarios sourced from PubMed Central and the OSCE sample question 
book. These cases, categorized into common and rare scenarios, were analyzed by 
GPT-4, and its interpretations were reviewed by five experienced emergency 
medicine specialists. The accuracy of ECG interpretations and subsequent patient 
management plans were assessed using a structured evaluation framework and 
critical error identification.
RESULTS: GPT-4 made critical errors in 46% of ECG interpretations in the OSCE 
group and 50% in the PubMed group. For patient management, critical errors were 
found in 32% of the OSCE group and 14% of the PubMed group. When ECG evaluations 
were included in patient management, error rates approached 50%. The inter-rater 
reliability among evaluators indicated good agreement (ICC = 0.725, F = 3.72, 
p < 0.001).
CONCLUSION: While GPT-4 shows promise in specific applications, its current 
limitations in accurately interpreting ECGs and managing critical patient 
scenarios render it inappropriate for emergency department use. Future 
improvements and extensive validations are essential before such AI tools can be 
reliably deployed in critical healthcare settings.

Copyright: © 2025 Yiğit et al. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0327584
PMCID: PMC12288989
PMID: 40705814 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


90. Artif Intell Med. 2025 Oct;168:103220. doi: 10.1016/j.artmed.2025.103220. Epub 
2025 Jul 19.

Medical radiology report generation: A systematic review of current deep 
learning methods, trends, and future directions.

Izhar A(1), Idris N(2), Japar N(3).

Author information:
(1)Faculty of Computer Science and Information Technology, Universiti Malaya, 
Kuala Lumpur, 50603, Malaysia. Electronic address: 22086758@siswa.um.edu.my.
(2)Faculty of Computer Science and Information Technology, Universiti Malaya, 
Kuala Lumpur, 50603, Malaysia. Electronic address: norisma@um.edu.my.
(3)Faculty of Computer Science and Information Technology, Universiti Malaya, 
Kuala Lumpur, 50603, Malaysia. Electronic address: nuruljapar@um.edu.my.

Medical radiology reports play a crucial role in diagnosing various diseases, 
yet generating them manually is time-consuming and burdens clinical workflows. 
Medical radiology report generation aims to automate this process using deep 
learning to assist radiologists and reduce patient wait times. This study 
presents the most comprehensive systematic review to date on deep learning-based 
MRRG, encompassing recent advances that span traditional architectures to large 
language models. We focus on available datasets, modeling approaches, and 
evaluation practices. Following PRISMA guidelines, we retrieved 323 articles 
from major academic databases and included 78 studies after eligibility 
screening. We critically analyze key components such as model architectures, 
loss functions, datasets, evaluation metrics, and optimizers - identifying 22 
widely used datasets, 14 evaluation metrics, around 20 loss functions, over 25 
visual backbones, and more than 30 textual backbones. To support reproducibility 
and accelerate future research, we also compile links to modern models, 
toolkits, and pretrained resources. Our findings provide technical insights and 
outline future directions to address current limitations, promoting 
collaboration at the intersection of medical imaging, natural language 
processing, and deep learning to advance trustworthy AI systems in radiology.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.artmed.2025.103220
PMID: 40700862 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


91. BMC Med Inform Decis Mak. 2025 Jul 18;25(1):271. doi: 
10.1186/s12911-025-03114-4.

Application of ChatGPT-based artificial intelligence in the diagnosis and 
management of polycystic ovary syndrome.

Zhu Y(#)(1)(2), Luo D(#)(3), Shen X(1)(2), Shi Q(1)(2), Lv H(1)(2), Zhang S(4), 
Ye F(5), Kong N(6)(7).

Author information:
(1)Center for Reproductive Medicine and Obstetrics and Gynecology, Nanjing Drum 
Tower Hospital, Affiliated Hospital of Medical School, Nanjing University, 
Nanjing, China.
(2)Center for Molecular Reproductive Medicine, Nanjing University, Nanjing, 
China.
(3)School of Economics and Management, South China Normal University, Guangzhou, 
China.
(4)Guangzhou Shuntu Management Consulting Co., Ltd., Guangzhou, China.
(5)School of Chemistry, South China Normal University, Guangzhou, China.
(6)Center for Reproductive Medicine and Obstetrics and Gynecology, Nanjing Drum 
Tower Hospital, Affiliated Hospital of Medical School, Nanjing University, 
Nanjing, China. xtalkn@163.com.
(7)Center for Molecular Reproductive Medicine, Nanjing University, Nanjing, 
China. xtalkn@163.com.
(#)Contributed equally

This study systematically develops and evaluates the application value of the 
PCOS-GPT system, an artificial intelligence (AI) assistant based on ChatGPT 
technology, in the diagnosis and management of polycystic ovary syndrome (PCOS). 
The research explores innovative pathways for AI-enabled PCOS diagnosis and 
treatment, aiming to provide an adjunctive diagnostic tool for standardized 
clinical decision support. Methods: An evidence-based PCOS knowledge base was 
constructed, covering dimensions such as epidemiology, etiology, clinical 
manifestations, diagnosis, treatment, and prognosis. The PCOS-GPT system was 
developed using GPT-3.5 pretraining combined with fine-tuning on domain-specific 
datasets. Using data from 85 patients, the diagnostic and therapeutic 
performance of PCOS-GPT was evaluated multidimensionally-accuracy, readability, 
and operability-using diagnoses by three expert physicians as the gold standard. 
Compared with GPT-4, PCOS-GPT demonstrated advantages in diagnostic accuracy for 
PCOS (95.63% vs. 96.40%). Conclusion: PCOS-GPT is an intelligent diagnostic and 
therapeutic support tool with advantages in diagnostic accuracy. It holds 
promise for improving standardization in diagnosis and treatment, empowering 
patient self-management, enhancing access to high-quality healthcare resources, 
and offering comprehensive health management for PCOS patients. This innovation 
promotes the development of smart healthcare, benefiting women's health.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-03114-4
PMCID: PMC12275324
PMID: 40682078 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This retrospective study received ethical approval from the ethics 
committee of Nanjing Drum Tower Hospital (No. 2021-384-01). All patient couples 
provided written informed consent. All methods were carried out in accordance 
with relevant guidelines and regulations. Consent for publication: Informed 
consent for publication of data and findings was obtained from all participants, 
ensuring that their confidentiality and privacy were strictly maintained. 
Competing interests: The authors declare no competing interests.


92. J Am Med Inform Assoc. 2025 Sep 1;32(9):1471-1476. doi: 10.1093/jamia/ocaf117.

Automated analyses of risk of bias and critical appraisal of systematic reviews 
(ROBIS and AMSTAR 2): a comparison of the performance of 4 large language 
models.

Forero DA(1), Abreu SE(2), Tovar BE(3), Oermann MH(4).

Author information:
(1)School of Health and Sport Sciences, Fundación Universitaria del Área Andina, 
Bogotá, 110231, Colombia.
(2)Psychology Program, Fundación Universitaria del Área Andina, Medellín, 
050005, Colombia.
(3)Nursing Program, School of Health and Sport Sciences, Fundación Universitaria 
del Área Andina, Bogotá, 110231, Colombia.
(4)School of Nursing, Duke University School of Nursing, 27710, United States.

OBJECTIVES: To explore the performance of 4 large language model (LLM) chatbots 
for the analysis of 2 of the most commonly used tools for the advanced analysis 
of systematic reviews (SRs) and meta-analyses.
MATERIALS AND METHODS: We explored the performance of 4 LLM chatbots (ChatGPT, 
Gemini, DeepSeek, and QWEN) for the analysis of ROBIS and AMSTAR 2 tools (sample 
sizes: 20 SRs), in comparison with assessments by human experts.
RESULTS: Gemini showed the best agreement with human experts for both ROBIS and 
AMSTAR 2 (accuracy: 58% and 70%). The second best LLM chatbots were ChatGPT and 
QWEN, for ROBIS and AMSTAR 2, respectively.
DISCUSSION: Some LLM chatbots underestimated the risk of bias or overestimated 
the confidence of the results in published SRs, which is compatible with recent 
articles for other tools.
CONCLUSION: This is one of the first studies comparing the performance of 
several LLM chatbots for the automated analyses of ROBIS and AMSTAR 2.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf117
PMCID: PMC12361857
PMID: 40680299 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
declare.


93. NPJ Digit Med. 2025 Jul 17;8(1):450. doi: 10.1038/s41746-025-01824-7.

Large language model integrations in cancer decision-making: a systematic review 
and meta-analysis.

Hao Y(1)(2)(3), Qiu Z(4), Holmes J(5), Löckenhoff CE(4), Liu W(6), Ghassemi 
M(7), Kalantari S(4).

Author information:
(1)Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, USA. 
yh727@cornell.edu.
(2)Cornell University, Ithaca, NY, USA. yh727@cornell.edu.
(3)Massachusetts Institute of Technology, Cambridge, MA, USA. yh727@cornell.edu.
(4)Cornell University, Ithaca, NY, USA.
(5)Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, USA.
(6)Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, USA. 
liu.wei@mayo.edu.
(7)Massachusetts Institute of Technology, Cambridge, MA, USA.

Large Language Models (LLMs) are increasingly used to support cancer patients 
and clinicians in decision-making. This systematic review investigates how LLMs 
are integrated into oncology and evaluated by researchers. We conducted a 
comprehensive search across PubMed, Web of Science, Scopus, and the ACM Digital 
Library through May 2024, identifying 56 studies covering 15 cancer types. The 
meta-analysis results suggested that LLMs were commonly used to summarize, 
translate, and communicate clinical information, but performance varied: the 
average overall accuracy was 76.2%, with average diagnostic accuracy lower at 
67.4%, revealing gaps in the clinical readiness of this technology. Most 
evaluations relied heavily on quantitative datasets and automated methods 
without human graders, emphasizing "accuracy" and "appropriateness" while rarely 
addressing "safety", "harm", or "clarity". Current limitations for LLMs in 
cancer decision-making, such as limited domain knowledge and dependence on human 
oversight, demonstrate the need for open datasets and standardized evaluations 
to improve reliability.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01824-7
PMCID: PMC12271406
PMID: 40676129

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


94. J Med Internet Res. 2025 Jul 15;27:e75666. doi: 10.2196/75666.

Exploring the Dilemma of AI Use in Medical Research and Knowledge Synthesis: A 
Perspective on Deep Research Tools.

Ong AY(#)(1)(2)(3), Merle DA(#)(1)(2)(3), Wagner SK(1)(2)(3), Keane PA(1)(2)(3).

Author information:
(1)UCL Institute of Ophthalmology, Institute of Ophthalmology, University 
College London, 11-43 Bath St, London, EC1V 9EL, United Kingdom, +44 7482213973.
(2)Moorfields Eye Hospital, Moorfields Eye Hospital NHS Foundation Trust, 
London, United Kingdom.
(3)NIHR Moorfields Biomedical Research Centre, London, United Kingdom.
(#)Contributed equally

Advances in artificial intelligence (AI) promise to reshape the landscape of 
scientific inquiry. Amidst all these, OpenAI's latest tool, Deep Research, 
stands out for its potential to revolutionize how researchers engage with the 
literature. However, this leap forward presents a paradox; while AI-generated 
reviews offer speed and accessibility with minimal effort, they raise 
fundamental concerns about citation integrity, critical appraisal, and the 
erosion of deep scientific thinking. These concerns are particularly problematic 
in the context of biomedical research, where evidence quality may influence 
clinical practice and decision-making. In this piece, we present an empirical 
evaluation of Deep Research and explore both its remarkable capabilities and 
inherent limitations. Through structured experimentation, we assess its 
effectiveness in synthesizing literature, highlight key shortcomings, and 
reflect on the broader implications of these tools for research training, and 
the integrity of evidence-based practice. With AI tools increasingly blurring 
the lines between knowledge generation and critical inquiry, we argue that while 
AI democratizes access to knowledge, wisdom remains distinctly human.

© Ariel Yuhan Ong, David A Merle, Siegfried K Wagner, Pearse A Keane. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/75666
PMCID: PMC12288101
PMID: 40663724 [Indexed for MEDLINE]


95. J Med Internet Res. 2025 Jul 14;27:e64452. doi: 10.2196/64452.

Performance of Large Language Models in Numerical Versus Semantic Medical 
Knowledge: Cross-Sectional Benchmarking Study on Evidence-Based Questions and 
Answers.

Avnat E(1)(2), Levy M(3)(4), Herstain D(1), Yanko E(5), Ben Joya D(2)(6), 
Tzuchman Katz M(2), Eshel D(2), Laros S(1)(2), Dagan Y(1)(2), Barami S(1)(2), 
Mermelstein J(2), Ovadia S(2), Shomron N(1), Shalev V(1), Abdulnour RE(7).

Author information:
(1)Faculty of Medicine, Tel Aviv University, Chaim Levanon St 55, Tel Aviv, 
6997801, Israel, 972 545299622.
(2)Kahun Medical Ltd, Givatayim, Israel.
(3)Faculty of Medicine, Hebrew University of Jerusalem, Jerusalem, Israel.
(4)School of Computer Science and Engineering, The Hebrew University of 
Jerusalem, Jerusalem, Israel.
(5)The Azrieli Faculty of Medicine, Bar-Ilan University, Safed, Israel.
(6)Kaplan Medical Center, Rehovot, Israel.
(7)Division of Pulmonary and Critical Care Medicine, Department of Medicine, 
Brigham and Women's Hospital, Harvard Medical School, Boston, MA, United States.

BACKGROUND: Clinical problem-solving requires processing of semantic medical 
knowledge, such as illness scripts, and numerical medical knowledge of 
diagnostic tests for evidence-based decision-making. As large language models 
(LLMs) show promising results in many aspects of language-based clinical 
practice, their ability to generate nonlanguage evidence-based answers to 
clinical questions is inherently limited by tokenization.
OBJECTIVE: This study aimed to evaluate LLMs' performance on two question types: 
numeric (correlating findings) and semantic (differentiating entities), while 
examining differences within and between LLMs in medical aspects and comparing 
their performance to humans.
METHODS: To generate straightforward multichoice questions and answers (Q and 
As) based on evidence-based medicine (EBM), we used a comprehensive medical 
knowledge graph (containing data from more than 50,000 peer-reviewed studies) 
and created the EBM questions and answers (EBMQAs). EBMQA comprises 105,222 Q 
and As, categorized by medical topics (eg, medical disciplines) and nonmedical 
topics (eg, question length), and classified into numerical or semantic types. 
We benchmarked a dataset of 24,000 Q and As on two state-of-the-art LLMs, GPT-4 
(OpenAI) and Claude 3 Opus (Anthropic). We evaluated the LLM's accuracy on 
semantic and numerical question types and according to sublabeled topics. In 
addition, we examined the question-answering rate of LLMs by enabling them to 
choose to abstain from responding to questions. For validation, we compared the 
results for 100 unrelated numerical EBMQA questions between six human medical 
experts and the two language models.
RESULTS: In an analysis of 24,542 Q and As, Claude 3 and GPT-4 performed better 
on semantic Q and As (68.7%, n=1593 and 68.4%, n=1709), respectively. Then on 
numerical Q and As (61.3%, n=8583 and 56.7%, n=12,038), respectively, with 
Claude 3 outperforming GPT-4 in numeric accuracy (P<.001). A median accuracy gap 
of 7% (IQR 5%-10%) was observed between the best and worst sublabels per topic, 
with different LLMs excelling in different sublabels. Focusing on Medical 
Discipline sublabels, Claude 3 performed well in neoplastic disorders but 
struggled with genitourinary disorders (69%, n=676 vs 58%, n=464; P<.0001), 
while GPT-4 excelled in cardiovascular disorders but struggled with neoplastic 
disorders (60%, n=1076 vs 53%, n=704; P=.0002). Furthermore, humans (82.3%, 
n=82.3) surpassed both Claude 3 (64.3%, n=64.3; P<.001) and GPT-4 (55.8%, 
n=55.8; P<.001) in the validation test. Spearman correlation between 
question-answering and accuracy rate in both Claude 3 and GPT-4 was 
insignificant (ρ=0.12, P=.69; ρ=0.43, P=.13).
CONCLUSIONS: Both LLMs excelled more in semantic than numerical Q and As, with 
Claude 3 surpassing GPT-4 in numerical Q and As. However, both LLMs showed 
inter- and intramodel gaps in different medical aspects and remained inferior to 
humans. In addition, their ability to respond or abstain from answering a 
question does not reliably predict how accurately they perform when they do 
attempt to answer questions. Thus, their medical advice should be addressed 
carefully.

© Eden Avnat, Michal Levy, Daniel Herstain, Elia Yanko, Daniel Ben Joya, Michal 
Tzuchman Katz, Dafna Eshel, Sahar Laros, Yael Dagan, Shahar Barami, Joseph 
Mermelstein, Shahar Ovadia, Noam Shomron, Varda Shalev, Raja-Elie E Abdulnour. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/64452
PMCID: PMC12279315
PMID: 40658983 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: The authors EA, ML, DH, 
DBJ, MTK, DE, SL, YD, SB, JM, and SO are paid employees by Kahun Ltd. All other 
authors declare no financial or non-financial competing interests.


96. Digit Health. 2025 Jul 10;11:20552076251353731. doi: 10.1177/20552076251353731. 
eCollection 2025 Jan-Dec.

The effect of chatbot-based exercise interventions on physical activity, 
exercise habits, and sedentary behavior: A systematic review and meta-analysis 
of randomized controlled trials.

Wang Q(1), Yang W(2), Xu K(3), Lam LT(1)(4)(5).

Author information:
(1)Faculty of Medicine, Macau University of Science and Technology, Macau SAR, 
China.
(2)Jiangsu Research Institute of Sports Science, Nanjing, China.
(3)Nanjing Sport Institute, Nanjing, China.
(4)Faculty of Medicine and Health, The University of Sydney, Sydney, Australia.
(5)Faculty of Health, University of Technology Sydney, Sydney, Australia.

OBJECTIVES: Chatbots, transcending the limitations of space and time while 
reducing user resistance, offer a personalized and autonomous approach 
potentially enhancing digital health interventions. This systematic review aims 
to quantitatively evaluate the effectiveness of chatbot-based exercise 
interventions (EIs) in randomized controlled trials (RCTs), emphasizing physical 
activity (PA), exercise habits (EH), and sedentary behavior (SB).
METHODS: A comprehensive search was carried out across Embase, Web of Science, 
PubMed, and Cochrane databases for studies published from January 2010 to 
October 2024. The characteristics of the included studies and the employed 
chatbots were analyzed. A meta-analysis was performed to synthesize the impact 
of chatbot-based EIs on PA, EH, and SB. We carried out subgroup analyses (SAs) 
for investigating potential modifying effects.
RESULTS: This review includes 12 studies conducted between 2013 and 2024, 
encompassing 2446 participants and 240 parent-child dyads, with participant ages 
ranging from 8 to 71 years. The chatbot-based EIs significantly enhanced PA 
compared to control groups (SMD = 0.20, 95% CI = 0.04-0.37, p = 0.02; I² = 51%, 
p = 0.04). However, these interventions did not significantly impact EH 
(SMD = 0.29, 95% CI = -0.48-1.06, p = 0.46) or reduce SB (SMD = 0.32, 95% 
CI = -0.54-1.19, p = 0.46). Notably, SAs revealed a small but significant 
short-term increase in EH (SMD = 0.29, 95% CI = 0.08-0.50, p = 0.006).
CONCLUSION: Chatbot-based EIs appear promising in improving PA, particularly 
moderate to vigorous PA (MVPA). Nonetheless, further RCTs with diverse chatbot 
designs, larger sample sizes, low-resource settings, objective measures of SB 
(e.g., accelerometers), and extended follow-ups are essential to corroborate 
these findings. Future research should also explore the adaptation of chatbots 
to various socioeconomic and cultural contexts and the integration of 
technologies like ChatGPT into EIs.Trial Registration: Meta-analysis PROSPERO: 
CRD42024609852.

© The Author(s) 2025.

DOI: 10.1177/20552076251353731
PMCID: PMC12254675
PMID: 40656859

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


97. J Med Internet Res. 2025 Jul 11;27:e71916. doi: 10.2196/71916.

Implementing Large Language Models in Health Care: Clinician-Focused Review With 
Interactive Guideline.

Li H(1)(2), Fu JF(3)(4)(5), Python A(1)(6)(7).

Author information:
(1)Center for Data Science, Zhejiang University, Hangzhou, China.
(2)School of Mathematical Sciences, Zhejiang University, Hangzhou, China.
(3)School of Medicine, Children's Hospital of Zhejiang University, Hangzhou, 
China.
(4)National Clinical Research Center for Child Health, Hangzhou, China.
(5)National Regional Center for Children's Health, Hangzhou, China.
(6)School of Medicine, Zhejiang University, Hangzhou, China.
(7)Centre for Human Genetics, Nuffield Department of Medicine, University of 
Oxford, Oxford, United Kingdom.

BACKGROUND: Large language models (LLMs) can generate outputs understandable by 
humans, such as answers to medical questions and radiology reports. With the 
rapid development of LLMs, clinicians face a growing challenge in determining 
the most suitable algorithms to support their work.
OBJECTIVE: We aimed to provide clinicians and other health care practitioners 
with systematic guidance in selecting an LLM that is relevant and appropriate to 
their needs and facilitate the integration process of LLMs in health care.
METHODS: We conducted a literature search of full-text publications in English 
on clinical applications of LLMs published between January 1, 2022, and March 
31, 2025, on PubMed, ScienceDirect, Scopus, and IEEE Xplore. We excluded papers 
from journals below a set citation threshold, as well as papers that did not 
focus on LLMs, were not research based, or did not involve clinical 
applications. We also conducted a literature search on arXiv within the same 
investigated period and included papers on the clinical applications of 
innovative multimodal LLMs. This led to a total of 270 studies.
RESULTS: We collected 330 LLMs and recorded their application frequency in 
clinical tasks and frequency of best performance in their context. On the basis 
of a 5-stage clinical workflow, we found that stages 2, 3, and 4 are key stages 
in the clinical workflow, involving numerous clinical subtasks and LLMs. 
However, the diversity of LLMs that may perform optimally in each context 
remains limited. GPT-3.5 and GPT-4 were the most versatile models in the 5-stage 
clinical workflow, applied to 52% (29/56) and 71% (40/56) of the clinical 
subtasks, respectively, and they performed best in 29% (16/56) and 54% (30/56) 
of the clinical subtasks, respectively. General-purpose LLMs may not perform 
well in specialized areas as they often require lightweight prompt engineering 
methods or fine-tuning techniques based on specific datasets to improve model 
performance. Most LLMs with multimodal abilities are closed-source models and, 
therefore, lack of transparency, model customization, and fine-tuning for 
specific clinical tasks and may also pose challenges regarding data protection 
and privacy, which are common requirements in clinical settings.
CONCLUSIONS: In this review, we found that LLMs may help clinicians in a variety 
of clinical tasks. However, we did not find evidence of generalist clinical LLMs 
successfully applicable to a wide range of clinical tasks. Therefore, their 
clinical deployment remains challenging. On the basis of this review, we propose 
an interactive online guideline for clinicians to select suitable LLMs by 
clinical task. With a clinical perspective and free of unnecessary technical 
jargon, this guideline may be used as a reference to successfully apply LLMs in 
clinical settings.

©HongYi Li, Jun-Fen Fu, Andre Python. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org), 11.07.2025.

DOI: 10.2196/71916
PMCID: PMC12299950
PMID: 40644686 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


98. JMIR Med Inform. 2025 Jul 9;13:e71176. doi: 10.2196/71176.

Language Models for Multilabel Document Classification of Surgical Concepts in 
Exploratory Laparotomy Operative Notes: Algorithm Development Study.

Balch JA(1)(2), Desaraju SS(3), Nolan VJ(1), Vellanki D(3), Buchanan TR(3), 
Brinkley LM(3), Penev Y(3), Bilgili A(3), Patel A(3), Chatham CE(3), Vanderbilt 
DM(3), Uddin R(3), Bihorac A(4)(5), Efron P(1), Loftus TJ(1)(4), Rahman P(2), 
Shickel B(4)(5).

Author information:
(1)Department of Surgery, University of Florida College of Medicine, 
Gainesville, FL, United States.
(2)Department of Health Outcomes and Biomedical Informatics, University of 
Florida College of Medicine, Gainesville, FL, United States.
(3)University of Florida College of Medicine, Gainesville, FL, United States.
(4)Intelligent Clinical Care Center, University of Florida, Gainesville, FL, 
United States.
(5)Department of Medicine, University of Florida College of Medicine, 1600 SW 
Archer Road, PO Box 100224, Gainesville, FL, 32610, United States, 3522739958, 1 
3522739221.

BACKGROUND: Operative notes are frequently mined for surgical concepts in 
clinical care, research, quality improvement, and billing, often requiring hours 
of manual extraction. These notes are typically analyzed at the document level 
to determine the presence or absence of specific procedures or findings (eg, 
whether a hand-sewn anastomosis was performed or contamination occurred). 
Extracting several binary classification labels simultaneously is a multilabel 
classification problem. Traditional natural language processing 
approaches-bag-of-words (BoW) and term frequency-inverse document frequency 
(tf-idf) with linear classifiers-have been used previously for this task but are 
now being augmented or replaced by large language models (LLMs). However, few 
studies have examined their utility in surgery.
OBJECTIVE: We developed and evaluated LLMs for the purpose of expediting data 
extraction from surgical notes.
METHODS: A total of 388 exploratory laparotomy notes from a single institution 
were annotated for 21 concepts related to intraoperative findings, 
intraoperative techniques, and closure techniques. Annotation consistency was 
measured using the Cohen κ statistic. Data were preprocessed to include only the 
description of the procedure. We compared the evolution of document 
classification technologies from BoW and tf-idf to encoder-only 
(Clinical-Longformer) and decoder-only (Llama 3) transformer models. Multilabel 
classification performance was evaluated with 5-fold cross-validation with 
F1-score and hamming loss (HL). We experimented with and without context. Errors 
were assessed by manual review. Code and implementation instructions may be 
found on GitHub.
RESULTS: The prevalence of labels ranged from 0.05 (colostomy, ileostomy, active 
bleed from named vessel) to 0.50 (running fascial closure). Llama 3.3 was the 
overall best-performing model (micro F1-score 0.88, 5-fold range: 0.88-0.89; HL 
0.11, 5-fold range: 0.11-0.12). The BoW model (micro F1-score 0.68, 5-fold 
range: 0.64-0.71; HL 0.14, 5-fold range: 0.13-0.16) and Clinical-Longformer 
(micro F1-score 0.73, 5-fold range: 0.70-0.74; HL 0.11, 5-fold range: 0.10-0.12) 
had overall similar performance, with tf-idf models trailing (micro F1-score 
0.57, 5-fold range: 0.55-0.59; HL 0.27, 5-fold range: 0.25-0.29). F1-scores 
varied across concepts in the Llama model, ranging from 0.30 (5-fold range: 
0.23-0.39) for class III contamination to 0.92 (5-fold range: 0.98-0.84) for 
bowel resection. Context enhanced Llama's performance, adding an average of 0.16 
improvement to the F1-scores. Error analysis demonstrated semantic nuances and 
edge cases within operative notes, particularly when patients had references to 
prior operations in their operative notes or simultaneous operations with other 
surgical services.
CONCLUSIONS: Off-the-shelf autoregressive LLMs outperformed fined-tuned, 
encoder-only transformers and traditional natural language processing techniques 
in classifying operative notes. Multilabel classification with LLMs may 
streamline retrospective reviews in surgery, though further refinements are 
required prior to reliable use in research and quality improvement.

© Jeremy A Balch, Sasank S Desarju, Victoria J Nolan, Divya Vallanki, Timothy R 
Buchanan, Lindsey M Brinkley, Yordan Penev, Ahmet Bilgili, Aashay Patel, Corinne 
E Chatham, David M Vanderbilt, Rayon Uddin, Azra Bihorac, Philip Efron, Tyler J 
Loftus, Protiva Rahman, Benjamin Shickel. Originally published in JMIR Medical 
Informatics (https://medinform.jmir.org).

DOI: 10.2196/71176
PMCID: PMC12266303
PMID: 40632815 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


99. JMIR Form Res. 2025 Jul 8;9:e72815. doi: 10.2196/72815.

AI in Qualitative Health Research Appraisal: Comparative Study.

Landerholm A(1).

Author information:
(1)Physiotherapy Department, Healthscience Faculty, Mälardalen University, 
Avdelningen för Fysioterapi Akademin för Hälsa, Vård Och Välfärd Mälardalens 
Universitet, Västerås, 721 21, Sweden, 46 702129863.

BACKGROUND: Qualitative research appraisal is crucial for ensuring credible 
findings but faces challenges due to human variability. Artificial intelligence 
(AI) models have the potential to enhance the efficiency and consistency of 
qualitative research assessments.
OBJECTIVE: This study aims to evaluate the performance of 5 AI models (GPT-3.5, 
Claude 3.5, Sonar Huge, GPT-4, and Claude 3 Opus) in assessing the quality of 
qualitative research using 3 standardized tools: Critical Appraisal Skills 
Programme (CASP), Joanna Briggs Institute (JBI) checklist, and Evaluative Tools 
for Qualitative Studies (ETQS).
METHODS: AI-generated assessments of 3 peer-reviewed qualitative papers in 
health and physical activity-related research were analyzed. The study examined 
systematic affirmation bias, interrater reliability, and tool-dependent 
disagreements across the AI models. Sensitivity analysis was conducted to 
evaluate the impact of excluding specific models on agreement levels.
RESULTS: Results revealed a systematic affirmation bias across all AI models, 
with "Yes" rates ranging from 75.9% (145/191; Claude 3 Opus) to 85.4% (164/192; 
Claude 3.5). GPT-4 diverged significantly, showing lower agreement ("Yes": 
115/192, 59.9%) and higher uncertainty ("Cannot tell": 69/192, 35.9%). 
Proprietary models (GPT-3.5 and Claude 3.5) demonstrated near-perfect alignment 
(Cramer V=0.891; P<.001), while open-source models showed greater variability. 
Interrater reliability varied by assessment tool, with CASP achieving the 
highest baseline consensus (Krippendorff α=0.653), followed by JBI (α=0.477), 
and ETQS scoring lowest (α=0.376). Sensitivity analysis revealed that excluding 
GPT-4 increased CASP agreement by 20% (α=0.784), while removing Sonar Huge 
improved JBI agreement by 18% (α=0.561). ETQS showed marginal improvements when 
excluding GPT-4 or Claude 3 Opus (+9%, α=0.409). Tool-dependent disagreements 
were evident, particularly in ETQS criteria, highlighting AI's current 
limitations in contextual interpretation.
CONCLUSIONS: The findings demonstrate that AI models exhibit both promise and 
limitations as evaluators of qualitative research quality. While they enhance 
efficiency, AI models struggle with reaching consensus in areas requiring 
nuanced interpretation, particularly for contextual criteria. The study 
underscores the importance of hybrid frameworks that integrate AI scalability 
with human oversight, especially for contextual judgment. Future research should 
prioritize developing AI training protocols that emphasize qualitative 
epistemology, benchmarking AI performance against expert panels to validate 
accuracy thresholds, and establishing ethical guidelines for disclosing AI's 
role in systematic reviews. As qualitative methodologies evolve alongside AI 
capabilities, the path forward lies in collaborative human-AI workflows that 
leverage AI's efficiency while preserving human expertise for interpretive 
tasks.

© August Landerholm. Originally published in JMIR Formative Research 
(https://formative.jmir.org).

DOI: 10.2196/72815
PMCID: PMC12263093
PMID: 40627827 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: This study used 
ChatGPT-3.5, ChatGPT-4 (OpenAI), Claude 3.5, Claude 3 Opus (Anthropic), and 
Sonar Huge (Perplexity AI) for structured quality assessments. The author is not 
employed by, holds stock in, or has received financial compensation from these 
companies. AL holds no affiliation with JMIR journals, holds no patents related 
to this work, or has financial or personal relationships with individuals or 
organizations that could influence this research.


100. J Med Internet Res. 2025 Jul 8;27:e75347. doi: 10.2196/75347.

Psychometric Evaluation of Large Language Model Embeddings for Personality Trait 
Prediction.

Maharjan J(1), Jin R(1), Zhu J(1), Kenne D(2).

Author information:
(1)Department of Computer Science, Kent State University, 800 East Summit 
Street, Kent, OH, 44242, United States, 1 3305931365.
(2)Department of Public Health, Kent State University, Kent, OH, United States.

BACKGROUND: Recent advancements in large language models (LLMs) have generated 
significant interest in their potential for assessing psychological constructs, 
particularly personality traits. While prior research has explored LLMs' 
capabilities in zero-shot or few-shot personality inference, few studies have 
systematically evaluated LLM embeddings within a psychometric validity framework 
or examined their correlations with linguistic and emotional markers. 
Additionally, the comparative efficacy of LLM embeddings against traditional 
feature engineering methods remains underexplored, leaving gaps in understanding 
their scalability and interpretability for computational personality assessment.
OBJECTIVE: This study evaluates LLM embeddings for personality trait prediction 
through four key analyses: (1) performance comparison with zero-shot methods on 
PANDORA Reddit data, (2) psychometric validation and correlation with LIWC 
(Linguistic Inquiry and Word Count) and emotion features, (3) benchmarking 
against traditional feature engineering approaches, and (4) assessment of model 
size effects (OpenAI vs BERT vs RoBERTa). We aim to establish LLM embeddings as 
a psychometrically valid and efficient alternative for personality assessment.
METHODS: We conducted a multistage analysis using 1 million Reddit posts from 
the PANDORA Big Five personality dataset. First, we generated text embeddings 
using 3 LLM architectures (RoBERTa, BERT, and OpenAI) and trained a custom 
bidirectional long short-term memory model for personality prediction. We 
compared this approach against zero-shot inference using prompt-based methods. 
Second, we extracted psycholinguistic features (LIWC categories and National 
Research Council emotions) and performed feature engineering to evaluate 
potential performance enhancements. Third, we assessed the psychometric validity 
of LLM embeddings: reliability validity using Cronbach α and convergent validity 
analysis by examining correlations between embeddings and established linguistic 
markers. Finally, we performed traditional feature engineering on static 
psycholinguistic features to assess performance under different settings.
RESULTS: LLM embeddings trained using simple deep learning techniques 
significantly outperform zero-shot approaches on average by 45% across all 
personality traits. Although psychometric validation tests indicate moderate 
reliability, with an average Cronbach α of 0.63, correlation analyses spark a 
strong association with key linguistic or emotional markers; openness correlates 
highly with social (r=0.53), conscientiousness with linguistic (r=0.46), 
extraversion with social (r=0.41), agreeableness with pronoun usage (r=0.40), 
and neuroticism with politics-related text (r=0.63). Despite adding advanced 
feature engineering on linguistic features, the performance did not improve, 
suggesting that LLM embeddings inherently capture key linguistic features. 
Furthermore, our analyses demonstrated efficacy on larger model size with a 
computational cost trade-off.
CONCLUSIONS: Our findings demonstrate that LLM embeddings offer a robust 
alternative to zero-shot methods in personality trait analysis, capturing key 
linguistic patterns without requiring extensive feature engineering. The 
correlation between established psycholinguistic markers and the performance 
trade-off with computational cost provides a hint for future computational 
linguistic work targeting LLM for personality assessment. Further research 
should explore fine-tuning strategies to enhance psychometric validity.

© Julina Maharjan, Ruoming Jin, Jianfeng Zhu, Deric Kenne. Originally published 
in the Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/75347
PMCID: PMC12262148
PMID: 40627556 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


101. NPJ Digit Med. 2025 Jul 5;8(1):410. doi: 10.1038/s41746-025-01715-x.

A systematic literature review on integrating AI-powered smart glasses into 
digital health management for proactive healthcare solutions.

Wang B(1)(2), Zheng Y(3), Han X(4), Kong L(4), Xiao G(5), Xiao Z(6)(7), Chen 
S(8)(9).

Author information:
(1)Beijing Xiaotangshan Hospital, Beijing, China. boyuan422@foxmail.com.
(2)Department of Biomedical Sciences, City University of Hong Kong, Hong Kong, 
China. boyuan422@foxmail.com.
(3)School of Mathematical Sciences, Capital Normal University, Beijing, China.
(4)National Institute of Hospital Administration (NIHA), Beijing, China.
(5)National Institute of Hospital Administration (NIHA), Beijing, China. 
gexin_xiao@sina.com.
(6)The First Affiliated Hospital of Hunan University of Medicine, Huaihua, 
China. hyfyxzx@163.com.
(7)Hunan University of Medicine, Huaihua, China. hyfyxzx@163.com.
(8)The First Affiliated Hospital of Hunan University of Medicine, Huaihua, 
China. 1506376330@qq.com.
(9)Hunan Primary Digital Engineering Technology Research Center for Medical 
Prevention and Treatment, Huaihua, China. 1506376330@qq.com.

AI-powered smart glasses are emerging as a highly promising advancement in the 
field of digital health management, owing to their capabilities in real-time 
monitoring, chronic disease management, and personalized treatment planning. To 
comprehensively understand the current state of development, we systematically 
searched multiple databases, including Web of Science, PubMed, and IEEE Xplore, 
to collect relevant literature. This paper provides a systematic analysis of the 
current applications of smart glasses in healthcare, focusing on their potential 
benefits and limitations. Key issues discussed include user engagement, 
treatment adherence, data privacy, standardization, battery efficiency, clinical 
validation, and medical ethics. Our findings suggest that, supported by emerging 
clinical evidence, smart glasses have demonstrated significant improvements in 
areas such as assisted medical services, health management, anxiety alleviation 
in children, and telemedicine. By integrating multi-modal sensors, these devices 
are capable of accurately tracking certain physiological indicators and 
synchronizing real-time visual input, thereby enhancing the accuracy and 
timeliness of health interventions and medical services. Notably, some 
cutting-edge smart glasses have adopted advanced artificial intelligence 
algorithms, particularly large language models (LLMs) with context awareness and 
human-like interaction capabilities. These AI-powered glasses can offer 
real-time, personalized dietary and health management recommendations tailored 
to users' daily life scenarios. Building on these findings, this study further 
proposes a conceptual framework for proactive health management using smart 
glasses and explores future directions in technological development and 
practical applications. Overall, AI-enhanced smart glasses show great potential 
as a critical interface between healthcare providers and patients, poised to 
play a vital role in the future of personalized medicine and continuous health 
management.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01715-x
PMCID: PMC12228729
PMID: 40617964

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


102. J Med Syst. 2025 Jul 5;49(1):94. doi: 10.1007/s10916-025-02227-7.

Evaluating the Performance of ChatGPT on Board-Style Examination Questions in 
Ophthalmology: A Meta-Analysis.

Wei J(#)(1), Wang X(#)(1), Huang M(1), Xu Y(2)(3), Yang W(4).

Author information:
(1)School of Nursing, Southwest Medical University, Luzhou, 646099, Sichuan 
Province, China.
(2)School of Future Technology, South China University of Technology, Guangzhou, 
510641, Guangdong Province, China.
(3)Pazhou Lab, Guangzhou, 510320, Guangdong Province, China.
(4)Shenzhen Eye Hospital, Shenzhen Eye Medical Center, Southern Medical 
University, No. 18 Zetian Road, Futian District, Shenzhen, 518040, Guangdong 
Province, China. benben0606@139.com.
(#)Contributed equally

To review empirical research on ChatGPT's accuracy in answering ophthalmology 
board-style examination questions up to March 2025 and to analyze the effects of 
GPT versions, question types, language differences, and ophthalmology topics on 
accuracy. A search was conducted in PubMed, Web of Science, Embase, Scopus, and 
the Cochrane Library in March 2025. Two authors extracted data and independently 
assessed study quality. Accuracy rates were calculated with Stata 17.0. GPT-4 
had an integrated accuracy of 73%, higher than GPT-3.5's 54%. It scored 77% in 
text and 55% in image tasks. GPT-4's accuracy was 73% in English-speaking 
countries and 71% in non-English ones. In ophthalmology, General Medicine 
achieved the highest accuracy (80%), while Clinical Optics had the lowest 
performance (55%). GPT-4 outperforms GPT-3.5, but its image processing 
capability needs further validation. Performance varies by language and topic, 
suggesting the need for more research on cross-linguistic efficacy and error 
analysis.

Publisher: Not applicable.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02227-7
PMID: 40615678 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing Interests: The authors 
declare no competing interests.


103. Int J Med Inform. 2025 Nov;203:106035. doi: 10.1016/j.ijmedinf.2025.106035. Epub 
2025 Jul 1.

Enhancing AI for citation screening in literature reviews: Improving accuracy 
with ensemble models.

Zhang Z(1), Momeni Nezhad MJ(2), Gupta P(3), Zolnour A(2), Azadmaleki H(2), 
Topaz M(4), Zolnoori M(5).

Author information:
(1)Data Science Institute, Columbia University, New York, NY 10027, USA; School 
of Nursing, Columbia University, New York, NY 10032, USA. Electronic address: 
zz3238@columbia.edu.
(2)Columbia University Irving Medical Center, New York, NY 10032, USA.
(3)School of Nursing, Columbia University, New York, NY 10032, USA.
(4)Data Science Institute, Columbia University, New York, NY 10027, USA; School 
of Nursing, Columbia University, New York, NY 10032, USA.
(5)School of Nursing, Columbia University, New York, NY 10032, USA; Columbia 
University Irving Medical Center, New York, NY 10032, USA.

BACKGROUND: Healthcare literature reviews underpin evidence-based practice and 
clinical guideline development, with citation screening as a critical yet 
time-consuming step. This study evaluates the effectiveness of individual large 
language models (LLMs) versus ensemble approaches in automating citation 
screening to improve the efficiency and scalability of evidence synthesis in 
healthcare research.
METHODS: Performance was assessed across three healthcare-focused reviews: 
LLM-Healthcare (865 citations, broad scope, 49.8 % inclusion rate), MCI-Speech 
(959 citations, narrow scope, 6.5 % inclusion rate), and Multimodal-LLM (73 
citations, moderate scope, 68.5 % inclusion rate). Six LLMs (GPT-4o Mini, 
GPT-4o, Gemini Flash, Llama 3.1 8B Instruct, Llama 3.1 70B Instruct, Llama 3.1 
405B Instruct) were evaluated using zero- and few-shot learning strategies with 
PubMedBERT for demonstration selection. We compared individual model performance 
with ensemble methods, including majority voting and random forest (RF), based 
on sensitivity and specificity.
RESULTS: No individual LLM consistently outperformed others across all tasks. 
Review with narrow inclusion criteria and low inclusion rates exhibited high 
specificity but lower sensitivity. Ensemble methods consistently surpassed 
individual LLMs: the RF ensemble with GPT-4o performed best in LLM-Healthcare 
(sensitivity: 0.96, specificity: 0.89); the majority voting with 1-shot LLMs 
(sensitivity: 0.75, specificity: 0.86) and RF ensemble with 4-shot LLMs 
(sensitivity: 0.62, specificity: 0.97) excelled in MCI-Speech; and four RF 
ensembles achieved perfect classification (sensitivity: 1.0, specificity: 1.0) 
in Multimodal-LLM.
CONCLUSION: Ensemble approaches improve individual LLMs' performances in 
citation screening across diverse healthcare review tasks, highlighting their 
potential to enhance evidence synthesis workflows that support clinical 
decision-making. However, broader validation is needed before real-world 
implementation.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106035
PMID: 40609462 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


104. IEEE J Biomed Health Inform. 2025 Jun 30;PP. doi: 10.1109/JBHI.2025.3584179. 
Online ahead of print.

Precision and Personalization: How Large Language Models Redefining Diagnostic 
Accuracy in Personalized Medicine - A Systematic Literature Review.

Aththanagoda AKNL, Kulathilake KASH, Abdullah NA.

Personalized medicine aims to tailor medical treatments to the unique 
characteristics of each patient, but its effectiveness relies on achieving 
diagnostic accuracy to fully understand individual variability in disease 
response and treatment efficacy. This systematic literature review explores the 
role of large language models (LLMs) in enhancing diagnostic precision and 
supporting the advancement of personalized medicine. A comprehensive search was 
conducted across Web of Science, Science Direct, Scopus, and IEEE Xplore, 
targeting peer-reviewed articles published in English between January 2020 and 
March 2025 that applied LLMs within personalized medicine contexts. Following 
PRISMA guidelines, 39 relevant studies were selected and systematically 
analyzed. The findings indicate a growing integration of LLMs across key domains 
such as clinical informatics, medical imaging, patient-specific diagnosis, and 
clinical decision support. LLMs have shown potential in uncovering subtle data 
patterns critical for accurate diagnosis and personalized treatment planning. 
This review highlights the expanding role of LLMs in improving diagnostic 
accuracy in personalized medicine, offering insights into their performance, 
applications, and challenges, while also acknowledging limitations in 
generalizability due to variable model performance and dataset biases. The 
review highlights the importance of addressing challenges related to data 
privacy, model interpretability, and reliability across diverse clinical 
scenarios. For successful clinical integration, future research must focus on 
refining LLM technologies, ensuring ethical standards, and validating models 
continuously to safeguard effective and responsible use in healthcare 
environments.

DOI: 10.1109/JBHI.2025.3584179
PMID: 40587361


105. J Med Internet Res. 2025 Jun 24;27:e70450. doi: 10.2196/70450.

Large Language Model–Assisted Risk-of-Bias Assessment in Randomized Controlled 
Trials Using the Revised Risk-of-Bias Tool: Evaluation Study.

Huang J(1)(2), Lai H(1)(2), Zhao W(1)(2), Xia D(1)(2), Bai C(3), Sun M(4), Liu 
J(5), Liu J(1)(2), Pan B(6)(7), Tian J(6)(7), Ge L(1)(2)(6).

Author information:
(1)Department of Health Policy and Management, School of Public Health, Lanzhou 
University, Lanzhou, China.
(2)Evidence-Based Social Science Research Center, School of Public Health, 
Lanzhou University, Lanzhou, China.
(3)School of Nursing, Southern Medical University, Guangzhou, China.
(4)School of Nursing, Peking University, Beijing, China.
(5)College of Nursing, Gansu University of Traditional Chinese Medicine, 
Lanzhou, China.
(6)Evidence-Based Medicine Center, School of Basic Medical Sciences, Lanzhou 
University, Lanzhou, China.
(7)Key Laboratory of Evidence Based Medicine of Gansu Province, Lanzhou, China.

Erratum in
    J Med Internet Res. 2025 Jul 14;27:e80519. doi: 10.2196/80519.

BACKGROUND: The revised Risk-of-Bias tool (RoB2) overcomes the limitations of 
its predecessor but introduces new implementation challenges. Studies 
demonstrate low interrater reliability and substantial time requirements for 
RoB2 implementation. Large language models (LLMs) may assist in RoB2 
implementation, although their effectiveness remains uncertain.
OBJECTIVE: This study aims to evaluate the accuracy of LLMs in RoB2 assessments 
to explore their potential as research assistants for bias evaluation.
METHODS: We systematically searched the Cochrane Library (through October 2023) 
for reviews using RoB2, categorized by interest in adhering or assignment. From 
86 eligible reviews of randomized controlled trials (covering 1399 RCTs), we 
randomly selected 46 RCTs (23 per category). In addition, 3 experienced 
reviewers independently assessed all 46 RCTs using RoB2, recording assessment 
time for each trial. Reviewer judgments were reconciled through consensus. 
Furthermore, 6 RCTs (3 from each category) were randomly selected for prompt 
development and optimization. The remaining 40 trials established the internal 
validation standard, while Cochrane Reviews judgments served as external 
validation. Primary outcomes were extracted as reported in corresponding 
Cochrane Reviews. We calculated accuracy rates, Cohen κ, and time differentials.
RESULTS: We identified significant differences between Cochrane and reviewer 
judgments, particularly in domains 1, 4, and 5, likely due to different 
standards in assessing randomization and blinding. Among the 20 articles 
focusing on adhering, 18 Cochrane Reviews and 19 reviewer judgments classified 
them as "High risk," while assignment-focused RCTs showed more heterogeneous 
risk distribution. Compared with Cochrane Reviews, LLMs demonstrated accuracy 
rates of 57.5% and 70% for overall (assignment) and overall (adhering), 
respectively. When compared with reviewer judgments, LLMs' accuracy rates were 
65% and 70% for these domains. The average accuracy rates for the remaining 6 
domains were 65.2% (95% CI 57.6-72.7) against Cochrane Reviews and 74.2% (95% CI 
64.7-83.9) against reviewers. At the signaling question level, LLMs achieved 
83.2% average accuracy (95% CI 77.5-88.9), with accuracy exceeding 70% for most 
questions except 2.4 (assignment), 2.5 (assignment), 3.3, and 3.4. When domain 
judgments were derived from LLM-generated signaling questions using the RoB2 
algorithm rather than direct LLM domain judgments, accuracy improved 
substantially for Domain 2 (adhering; 55-95) and overall (adhering; 70-90). LLMs 
demonstrated high consistency between iterations (average 85.2%, 95% CI 
85.15-88.79) and completed assessments in 1.9 minutes versus 31.5 minutes for 
human reviewers (mean difference 29.6, 95% CI 25.6-33.6 minutes).
CONCLUSIONS: LLMs achieved commendable accuracy when guided by structured 
prompts, particularly through processing methodological details through 
structured reasoning. While not replacing human assessment, LLMs demonstrate 
strong potential for assisting RoB2 evaluations. Larger studies with improved 
prompting could enhance performance.

©Jiajie Huang, Honghao Lai, Weilong Zhao, Danni Xia, Chunyang Bai, Mingyao Sun, 
Jianing Liu, Jiayi Liu, Bei Pan, Jinhui Tian, Long Ge. Originally published in 
the Journal of Medical Internet Research (https://www.jmir.org), 24.06.2025.

DOI: 10.2196/70450
PMCID: PMC12238788
PMID: 40554779 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


106. J Med Internet Res. 2025 Jun 23;27:e72398. doi: 10.2196/72398.

AI in Medical Questionnaires: Scoping Review.

Luo X(#)(1), Li Y(#)(1), Xu J(#)(1), Zheng Z(1), Ying F(2)(3), Huang G(1)(4).

Author information:
(1)Faculty of Humanities and Arts, Macau University of Science and Technology, 
Macau, China.
(2)Industrial and Manufacturing Engineering, European Academy of Engineering, 
Gothenburg, Sweden.
(3)College of Computer Science and Technology, Zhejiang University, Hangzhou, 
China.
(4)Zhuhai M.U.S.T. Science and Technology Research Institute, Zhuhai, China.
(#)Contributed equally

Erratum in
    J Med Internet Res. 2025 Aug 11;27:e80644. doi: 10.2196/80644.

This systematic review aimed to explore the current applications, potential 
benefits, and issues of artificial intelligence (AI) in medical questionnaires, 
focusing on its role in 3 main functions: assessment, development, and 
prediction. The global mental health burden remains severe. The World Health 
Organization reports that >1 billion people worldwide experience mental 
disorders, with the prevalence of depression and anxiety among children and 
adolescents at 2.6% and 6.5%, respectively. However, commonly used clinical 
questionnaires such as the Hamilton Depression Rating Scale and the Beck 
Depression Inventory suffer from several problems, including the high degree of 
overlap of symptoms of depression with those of other psychiatric disorders and 
a lack of professional supervision during administration of the questionnaires, 
which often lead to inaccurate diagnoses. In the wake of the COVID-19 pandemic, 
the health care system is facing the dual challenges of a surge in patient 
numbers and the complexity of mental health issues. AI technology has now been 
shown to have great promise in improving diagnostic accuracy, assisting clinical 
decision-making, and simplifying questionnaire development and data analysis. To 
systematically assess the value of AI in medical questionnaires, this study 
searched 5 databases (PubMed, Embase, Cochrane Library, Web of Science, and 
China National Knowledge Infrastructure) for the period from database inception 
to September 2024. Of 49,091 publications, a total of 14 (0.03%) studies met the 
inclusion criteria. AI technologies showed significant advantages in assessment, 
such as distinguishing myalgic encephalomyelitis or chronic fatigue syndrome 
from long COVID-19 with 92.18% accuracy. In questionnaire development, natural 
language processing using generative models such as ChatGPT was used to 
construct culturally competent scales. In terms of disease prediction, one study 
had an area under the curve of 0.790 for cataract surgery risk prediction. 
Overall, 24 AI technologies were identified, covering traditional algorithms 
such as random forest, support vector machine, and k-nearest neighbor, as well 
as deep learning models such as convolutional neural networks, Bidirectional 
Encoder Representations From Transformers, and ChatGPT. Despite the positive 
findings, only 21% (3/14) of the studies had entered the clinical validation 
phase, whereas the remaining 79% (11/14) were still in the exploratory phase of 
research. Most of the studies (10/14, 71%) were rated as being of moderate 
methodological quality, with major limitations including lack of a control 
group, incomplete follow-up data, and inadequate validation systems. In summary, 
the integrated application of AI in medical questionnaires has significant 
potential to improve diagnostic efficiency, accelerate scale development, and 
promote early intervention. Future research should pay more attention to model 
interpretability, system compatibility, validation standardization, and ethical 
governance to effectively address key challenges such as data privacy, clinical 
integration, and transparency.
BACKGROUND: The World Health Organization reports that >1 billion people 
worldwide experience mental disorders, with the prevalence of depression and 
anxiety among children and adolescents at 2.6% and 6.5%, respectively. However, 
commonly used clinical questionnaires such as the Hamilton Depression Rating 
Scale and the Beck Depression Inventory suffer from several problems, including 
the high degree of overlap of symptoms of depression with those of other 
psychiatric disorders and a lack of professional supervision during 
administration of the questionnaires, which often lead to inaccurate diagnoses. 
In the wake of the COVID-19 pandemic, the health care system is facing the dual 
challenges of a surge in patient numbers and the complexity of mental health 
issues. Artificial Intelligence (AI) technology has now been shown to have great 
promise in improving diagnostic accuracy, assisting clinical decision-making, 
and simplifying questionnaire development and data analysis.
OBJECTIVE: This review aimed to explore the current applications, potential 
benefits, and issues of AI in medical questionnaires, focusing on its role in 3 
main functions: assessment, development, and prediction. The global mental 
health burden remains severe.
METHODS: The review included peer-reviewed studies that applied AI technologies 
to medical, psychological, or physiological questionnaires and reported 
measurable outcomes; non–peer-reviewed, non-English/Chinese, ethically 
noncompliant, or AI-unrelated studies were excluded. Five databases (PubMed, 
Embase, Cochrane Library, Web of Science, and CNKI) were searched from inception 
through September 2024. Three independent reviewers conducted data extraction, 
quality appraisal using the Joanna Briggs Institute tools, and narrative 
synthesis of AI applications across questionnaire assessment, development, and 
prediction tasks.
RESULTS: Of 49,091 publications, a total of 14 (0.03%) studies met the inclusion 
criteria. AI technologies showed advantages in assessment, such as 
distinguishing myalgic encephalomyelitis or chronic fatigue syndrome from long 
COVID-19 with 92.18% accuracy. In questionnaire development, natural language 
processing using generative models such as ChatGPT was used to construct 
culturally competent scales. In terms of disease prediction, one study had an 
area under the curve of 0.790 for cataract surgery risk prediction. Overall, 24 
AI technologies were identified, covering traditional algorithms such as random 
forest, support vector machine, and k-nearest neighbor, as well as deep learning 
models such as convolutional neural networks, Bidirectional Encoder 
Representations From Transformers, and ChatGPT. Despite the positive findings, 
only 21% (3/14) of the studies had entered the clinical validation phase, 
whereas the remaining 79% (11/14) were still in the exploratory phase of 
research. Most of the studies (10/14, 71%) were rated as being of moderate 
methodological quality, with major limitations including lack of a control 
group, incomplete follow-up data, and inadequate validation systems.
CONCLUSIONS: In summary, the integrated application of AI in medical 
questionnaires has significant potential to improve diagnostic efficiency, 
accelerate scale development, and promote early intervention. Future research 
should pay more attention to model interpretability, system compatibility, 
validation standardization, and ethical governance to effectively address key 
challenges such as data privacy, clinical integration, and transparency.

©Xuexing Luo, Yiyuan Li, Jing Xu, Zhong Zheng, Fangtian Ying, Guanghui Huang. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 23.06.2025.

DOI: 10.2196/72398
PMCID: PMC12235208
PMID: 40549427 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


107. J Biomed Inform. 2025 Aug;168:104865. doi: 10.1016/j.jbi.2025.104865. Epub 2025 
Jun 19.

Ontology enrichment using a large language model: Applying lexical, semantic, 
and knowledge network-based similarity for concept placement.

Kollapally NM(1), Geller J(2), Keloth VK(3), He Z(4), Xu J(5).

Author information:
(1)Kean University, United States. Electronic address: nmartink@kean.edu.
(2)New Jersey Institute of Technology, United States.
(3)Yale University, United States.
(4)Florida State University, United States.
(5)JX Consulting, United States.

OBJECTIVE: Ontologies are essential for representing the knowledge of a domain. 
To make ontologies useful, they must encompass a comprehensive domain view. To 
achieve ontology enrichment, there is a need to discover new concepts to be 
added, either because they were missed in the first place, or the 
state-of-the-art has advanced to develop new real-world concepts. Our goal is to 
develop an automatic enrichment pipeline using a seed ontology, a Large Language 
Model (LLM), and source of text. The pipeline is applied to the domain of Social 
Determinants of Health (SDoH), using PubMed as a source of concepts. In this 
work, the applicability and effectiveness of the enrichment pipeline is 
demonstrated by extending the SDoH Ontology called SOHOv1, however our 
methodology could be used in other domains as well.
METHODS: We first retrieved PubMed abstracts of candidate articles with existing 
SOHOv1 concepts as search terms. Next, we used GPT-4-1201 to extract semantic 
triples from the abstracts. We identified concepts from these triples utilizing 
lexical, semantic, and knowledge network-based filtering. We also compared the 
granularity of semantic triples extracted with our method to the triples in the 
SemMedDB (Semantic MEDLINE Database). The results were evaluated by human 
experts and standard ontology tools for checking consistency and semantic 
correctness.
RESULTS: We expanded SOHOv1, which contained 173 concepts and 585 axioms, 
including 207 logical axioms to SOHOv2, which contains 572 concepts, 1,542 
axioms, including 725 logical axioms. Our methods identified more concepts than 
those extracted from SemMedDB for the same task. While we have shown the 
feasibility of our approach for an SDoH ontology, the methodology is 
generalizable to other ontologies with an existing seed ontology and text 
corpus.
CONCLUSIONS: The contributions of this work are: Extracting semantic triples 
from PubMed abstracts using GPT-4-1201 utilizing prompt chaining; showing the 
superiority of triples from GPT-4-1201 over triples from SemMedDB for SDoH; 
using lexical and semantic similarity search techniques with knowledge 
network-based search to identify the concepts to be added to the ontology; 
confirming the quality of the new concepts with human experts.

Copyright © 2025 The Author(s). Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104865
PMCID: PMC12371725
PMID: 40543734 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


108. J Med Internet Res. 2025 Jun 19;27:e70315. doi: 10.2196/70315.

Large Language Model Architectures in Health Care: Scoping Review of Research 
Perspectives.

Leiser F(#)(1), Guse R(#)(1), Sunyaev A(2).

Author information:
(1)Research Group Critical Information Infrastructures, Institute of Applied 
Informatics and Formal Description Methods, Karlsruhe Institute of Technology, 
Karlsruhe, Germany.
(2)Chair of Information Infrastructures, School of Computation, Information and 
Technology, Technical University of Munich, Campus Heilbronn, Heilbronn, 
Germany.
(#)Contributed equally

BACKGROUND: Large language models (LLMs) can support health care professionals 
in their daily work, for example, when writing and filing reports or 
communicating diagnoses. With the rise of LLMs, current research investigates 
how LLMs could be applied in medical practice and their benefits for physicians 
in clinical workflows. However, most studies neglect the importance of selecting 
suitable LLM architectures.
OBJECTIVE: In this literature review, we aim to provide insights on the 
different LLM model architecture families (ie, Bidirectional Encoder 
Representations from Transformers [BERT]-based or generative pretrained 
transformer [GPT]-based models) used in previous research. We report on the 
suitability and benefits of different LLM model architecture families for 
various research foci.
METHODS: To this end, we conduct a scoping review to identify which LLMs are 
used in health care. Our search included manuscripts from PubMed, arXiv, and 
medRxiv. We used open and selective coding to assess the 114 identified 
manuscripts regarding 11 dimensions related to usage and technical facets and 
the research focus of the manuscripts.
RESULTS: We identified 4 research foci that emerged previously in manuscripts, 
with LLM performance being the main focus. We found that GPT-based models are 
used for communicative purposes such as examination preparation or patient 
interaction. In contrast, BERT-based models are used for medical tasks such as 
knowledge discovery and model improvements.
CONCLUSIONS: Our study suggests that GPT-based models are better suited for 
communicative purposes such as report generation or patient interaction. 
BERT-based models seem to be better suited for innovative applications such as 
classification or knowledge discovery. This could be due to the architectural 
differences where GPT processes language unidirectionally and BERT 
bidirectionally, allowing more in-depth understanding of the text. In addition, 
BERT-based models seem to allow more straightforward extensions of their models 
for domain-specific tasks that generally lead to better results. In summary, 
health care professionals should consider the benefits and differences of the 
LLM architecture families when selecting a suitable model for their intended 
purpose.

©Florian Leiser, Richard Guse, Ali Sunyaev. Originally published in the Journal 
of Medical Internet Research (https://www.jmir.org), 19.06.2025.

DOI: 10.2196/70315
PMCID: PMC12226782
PMID: 40536801 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


109. Digit Health. 2025 Jun 9;11:20552076251348850. doi: 10.1177/20552076251348850. 
eCollection 2025 Jan-Dec.

Answering real-world clinical questions using large language model, 
retrieval-augmented generation, and agentic systems.

Low YS(1), Jackson ML(1), Hyde RJ(1), Brown RE(1), Sanghavi NM(1), Baldwin 
JD(1), Pike CW(1), Muralidharan J(1), Hui G(1)(2), Alexander N(3), Hassan 
H(4)(5), Nene RV(6), Pike M(7), Pokrzywa CJ(8), Vedak S(9), Yan AP(3), Yao 
DH(10), Zipursky AR(3), Dinh C(1), Ballentine P(1), Derieg DC(1), Polony V(1), 
Chawdry RN(1), Davies J(1), Hyde BB(1), Shah NH(1)(9), Gombar S(1)(11).

Author information:
(1)Atropos Health, New York, NY, USA.
(2)Department of Medicine, University of California, Los Angeles, CA, USA.
(3)Department of Pediatrics, The Hospital for Sick Children, Toronto, Ontario, 
Canada.
(4)Division of Hematology/Oncology, The Hospital for Sick Children, Toronto 
Ontario, Canada.
(5)Program in Child Health Evaluative Sciences, Peter Gilgan Centre for Research 
and Learning, The Hospital for Sick Children, Toronto, Ontario, Canada.
(6)Department of Emergency Medicine, University of California, San Diego, CA, 
USA.
(7)Department of Emergency Medicine, University of Michigan, Ann Arbor, MI, USA.
(8)Department of Surgery, Columbia University, New York, NY, USA.
(9)Division of Clinical Informatics, Stanford University, Stanford, CA, USA.
(10)Department of Emergency Medicine, Stanford University, Stanford, CA, USA.
(11)Department of Pathology, Stanford University, Stanford, CA, USA.

OBJECTIVE: The practice of evidence-based medicine can be challenging when 
relevant data are lacking or difficult to contextualize for a specific patient. 
Large language models (LLMs) could potentially address both challenges by 
summarizing published literature or generating new studies using real-world 
data.
MATERIALS AND METHODS: We submitted 50 clinical questions to five LLM-based 
systems: OpenEvidence, which uses an LLM for retrieval-augmented generation 
(RAG); ChatRWD, which uses an LLM as an interface to a data extraction and 
analysis pipeline; and three general-purpose LLMs (ChatGPT-4, Claude 3 Opus, 
Gemini 1.5 Pro). Nine independent physicians evaluated the answers for 
relevance, quality of supporting evidence, and actionability (i.e., sufficient 
to justify or change clinical practice).
RESULTS: General-purpose LLMs rarely produced relevant, evidence-based answers 
(2-10% of questions). In contrast, RAG-based and agentic LLM systems, 
respectively, produced relevant, evidence-based answers for 24% (OpenEvidence) 
to 58% (ChatRWD) of questions. OpenEvidence produced actionable results for 48% 
of questions with existing evidence, compared to 37% for ChatRWD and <5% for the 
general-purpose LLMs. ChatRWD provided actionable results for 52% of questions 
that lacked existing literature compared to <10% for other LLMs.
DISCUSSION: Special-purpose LLM systems greatly outperformed general-purpose 
LLMs in producing answers to clinical questions. Retrieval-augmented 
generation-based LLM (OpenEvidence) performed well when existing data were 
available, while only the agentic ChatRWD was able to provide actionable answers 
when preexisting studies were lacking.
CONCLUSION: Synergistic systems combining RAG-based evidence summarization and 
agentic generation of novel evidence could improve the availability of pertinent 
evidence for patient care.

© The Author(s) 2025.

DOI: 10.1177/20552076251348850
PMCID: PMC12159471
PMID: 40510193

Conflict of interest statement: The authors declared the following potential 
conflicts of interest with respect to the research, authorship, and/or 
publication of this article: ChatRWD, the LLM system evaluated in this study, is 
developed by Atropos Health where many of the authors are employed. NHS is not 
an Atropos Health employee but sits on its board. OpenEvidence, another LLM 
system evaluated here, is provided by OpenEvidence whom we consulted during the 
writing of this manuscript. Non-Atropos employees NA, HH, RVN, MP, CJP, SV, APY, 
D-HY, and ARZ, have nothing to disclose.


110. J Med Syst. 2025 Jun 12;49(1):80. doi: 10.1007/s10916-025-02212-0.

Large Language Models and the Analyses of Adherence to Reporting Guidelines in 
Systematic Reviews and Overviews of Reviews (PRISMA 2020 and PRIOR).

Forero DA(1), Abreu SE(2), Tovar BE(3), Oermann MH(4).

Author information:
(1)School of Health and Sport Sciences, Fundación Universitaria del Área Andina, 
Bogotá, Colombia. dforero41@areandina.edu.co.
(2)Psychology Program, Fundación Universitaria del Área Andina, Medellín, 
Colombia.
(3)Nursing Program, School of Health and Sport Sciences, Fundación Universitaria 
del Área Andina, Bogotá, Colombia.
(4)Duke University School of Nursing, Durham, NC, USA.

In the context of Evidence-Based Practice (EBP), Systematic Reviews (SRs), 
Meta-Analyses (MAs) and overview of reviews have become cornerstones for the 
synthesis of research findings. The Preferred Reporting Items for Systematic 
Reviews and Meta-Analyses (PRISMA) 2020 and Preferred Reporting Items for 
Overviews of Reviews (PRIOR) statements have become major reporting guidelines 
for SRs/MAs and for overviews of reviews, respectively. In recent years, 
advances in Generative Artificial Intelligence (genAI) have been proposed as a 
potential major paradigm shift in scientific research. The main aim of this 
research was to examine the performance of four LLMs for the analysis of 
adherence to PRISMA 2020 and PRIOR, in a sample of 20 SRs and 20 overviews of 
reviews. We tested the free versions of four commonly used LLMs: ChatGPT 
(GPT-4o), DeepSeek (V3), Gemini (2.0 Flash) and Qwen (2.5 Max). Adherence to 
PRISMA 2020 and PRIOR was compared with scores defined previously by human 
experts, using several statistical tests. In our results, all the four LLMs 
showed a low performance for the analysis of adherence to PRISMA 2020, 
overestimating the percentage of adherence (from 23 to 30%). For PRIOR, the LLMs 
presented lower differences in the estimation of adherence (from 6 to 14%) and 
ChatGPT showed a performance similar to human experts. This is the first report 
of the performance of four commonly used LLMs for the analysis of adherence to 
PRISMA 2020 and PRIOR. Future studies of adherence to other reporting guidelines 
will be helpful in health sciences research.

© 2025. The Author(s).

DOI: 10.1007/s10916-025-02212-0
PMCID: PMC12162794
PMID: 40504403 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Research Involving Human 
Participants and/or Animals: This manuscript describes only computational 
analyses and did not describe research involving human participants or animal 
models. Declaration of generative AI in Scientific Writing: No generative AI 
tools were used for scientific writing. Competing Interests: The authors declare 
no competing interests.


111. J Med Internet Res. 2025 Jun 9;27:e72062. doi: 10.2196/72062.

Large Language Models in Medical Diagnostics: Scoping Review With Bibliometric 
Analysis.

Su H(#)(1)(2)(3), Sun Y(#)(1)(2), Li R(4), Zhang A(3), Yang Y(1)(2)(3), Xiao 
F(5), Duan Z(1)(2), Chen J(1)(2), Hu Q(1)(2), Yang T(1)(2), Xu B(1)(2), Zhang 
Q(1)(2), Zhao J(1)(2), Li Y(1)(2), Li H(1)(2).

Author information:
(1)Department of Reproductive Medicine, Xiangya Hospital Central South 
University, Changsha, China.
(2)Clinical Research Center for Women's Reproductive Health in Hunan Province, 
Changsha, China.
(3)Xiangya School of Medicine, Central South University, Changsha, China.
(4)School of Biomedical Sciences and Engineering, South China University of 
Technology, Guangzhou, China.
(5)Department of Metabolism and Endocrinology, Second Xiangya Hospital of 
Central South University, Changsha, China.
(#)Contributed equally

BACKGROUND: The integration of large language models (LLMs) into medical 
diagnostics has garnered substantial attention due to their potential to enhance 
diagnostic accuracy, streamline clinical workflows, and address health care 
disparities. However, the rapid evolution of LLM research necessitates a 
comprehensive synthesis of their applications, challenges, and future 
directions.
OBJECTIVE: This scoping review aimed to provide an overview of the current state 
of research regarding the use of LLMs in medical diagnostics. The study sought 
to answer four primary subquestions, as follows: (1) Which LLMs are commonly 
used? (2) How are LLMs assessed in diagnosis? (3) What is the current 
performance of LLMs in diagnosing diseases? (4) Which medical domains are 
investigating the application of LLMs?
METHODS: This scoping review was conducted according to the Joanna Briggs 
Institute Manual for Evidence Synthesis and adheres to the PRISMA-ScR (Preferred 
Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping 
Reviews). Relevant literature was searched from the Web of Science, PubMed, 
Embase, IEEE Xplore, and ACM Digital Library databases from 2022 to 2025. 
Articles were screened and selected based on predefined inclusion and exclusion 
criteria. Bibliometric analysis was performed using VOSviewer to identify major 
research clusters and trends. Data extraction included details on LLM types, 
application domains, and performance metrics.
RESULTS: The field is rapidly expanding, with a surge in publications after 
2023. GPT-4 and its variants dominated research (70/95, 74% of studies), 
followed by GPT-3.5 (34/95, 36%). Key applications included disease 
classification (text or image-based), medical question answering, and diagnostic 
content generation. LLMs demonstrated high accuracy in specialties like 
radiology, psychiatry, and neurology but exhibited biases in race, gender, and 
cost predictions. Ethical concerns, including privacy risks and model 
hallucination, alongside regulatory fragmentation, were critical barriers to 
clinical adoption.
CONCLUSIONS: LLMs hold transformative potential for medical diagnostics but 
require rigorous validation, bias mitigation, and multimodal integration to 
address real-world complexities. Future research should prioritize explainable 
artificial intelligence frameworks, specialty-specific optimization, and 
international regulatory harmonization to ensure equitable and safe clinical 
deployment.

©Hankun Su, Yuanyuan Sun, Ruiting Li, Aozhe Zhang, Yuemeng Yang, Fen Xiao, 
Zhiying Duan, Jingjing Chen, Qin Hu, Tianli Yang, Bin Xu, Qiong Zhang, Jing 
Zhao, Yanping Li, Hui Li. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org), 09.06.2025.

DOI: 10.2196/72062
PMCID: PMC12186007
PMID: 40489764 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


112. J Biomed Inform. 2025 Aug;168:104860. doi: 10.1016/j.jbi.2025.104860. Epub 2025 
May 28.

Do it faster with PICOS: Generative AI-Assisted systematic review screening.

Vallamchetla SK(1), Abdelkader O(1), Elnaggar A(2), Ramadan D(1), Islam Shourav 
MM(1), Riaz IB(3), Lin MP(4).

Author information:
(1)Department of Neurology, Mayo Clinic, Jacksonville, FL, USA.
(2)Zagazig University, Zagazig, Egypt.
(3)Division of Hematology and Medical Oncology, Mayo Clinic, Scottsdale, AZ, 
USA.
(4)Department of Neurology, Mayo Clinic, Jacksonville, FL, USA. Electronic 
address: lin.michelle@mayo.edu.

BACKGROUND: Systematic reviews (SRs) require substantial time and human 
resources, especially during the screening phase. Large Language Models (LLMs) 
have shown the potential to expedite screening. However, their use in generating 
structured PICOS (Population, Intervention/Exposure, Comparison, Outcome, Study 
design) summaries from title and abstract to assist human reviewers during 
screening remains unexplored.
OBJECTIVE: To assess the impact of open-source (Mistral-Nemo-Instruct-2407) 
LLM-generated structured PICOS summaries on the speed and accuracy of title and 
abstract screening.
METHODS: Four neurology trainees were grouped into two pairs based on previous 
screening experience. Pair A (A1, A2) consisted of less experienced trainees 
(1-2 SR), while Pair B (B1, B2) consisted of more experienced trainees (≥3 SR). 
Reviewers A1 and B1 received titles, abstracts, and LLM-generated structured 
PICOS summaries for each article. Reviewers A2 and B2 received only titles and 
abstracts. All reviewers independently screened the same set of 1,003 articles 
using predefined eligibility criteria. Screening times were recorded, and 
performance metrics were calculated.
RESULTS: PICOS-assisted reviewers screened significantly faster (A1: 116 min; 
B1: 90 min) than those without (A2: 463 min; B2: 370 min), with approximately 
75% reduction in screening workload. Sensitivity was perfect for PICOS-assisted 
reviewers (100%), whereas it was lower for those without assistance (88.0% and 
92.0%). Furthermore, PICOS-assisted reviewers demonstrated higher accuracy 
(99.9%), specificity (99.9), F1 scores (98.0%), and strong inter-rater 
reliability (Cohen's Kappa of 99.8%). Less experienced reviewer with PICOS 
assistance(A1) outperformed experienced reviewer(B2) without assistance in both 
efficiency and sensitivity.
CONCLUSION: LLM-generated PICOS summaries enhance the speed and accuracy of 
title and abstract screening by providing an additional layer of structured 
information. With PICOS assistance, less experienced reviewer surpassed their 
more experienced peers. Future research should explore the applicability of this 
novel method across diverse fields outside of neurology and its integration into 
fully automated systems.

Copyright © 2025. Published by Elsevier Inc.

DOI: 10.1016/j.jbi.2025.104860
PMID: 40447171 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


113. Front Digit Health. 2025 May 15;7:1500220. doi: 10.3389/fdgth.2025.1500220. 
eCollection 2025.

Personalization variables in digital mental health interventions for depression 
and anxiety in adolescents and youth: a scoping review.

Wanniarachchi VU(1), Greenhalgh C(2), Choi A(1), Warren JR(1).

Author information:
(1)School of Computer Science, University of Auckland, Auckland, New Zealand.
(2)School of Computer Science, University of Nottingham, Nottingham, United 
Kingdom.

INTRODUCTION: The impact of personalization on user engagement and adherence in 
digital mental health interventions (DMHIs) has been widely explored. However, 
there is a lack of clarity regarding the prevalence of its application, as well 
as the dimensions and mechanisms of personalization within DMHIs for adolescents 
and youth.
METHODS: To understand how personalization has been applied in DMHIs for 
adolescents and young people, a scoping review was conducted. Empirical studies 
on DMHIs for adolescents and youth with depression and anxiety, published 
between 2013 and July 2024, were extracted from PubMed and Scopus. A total of 67 
studies were included in the review. Additionally, we expanded an existing 
personalization framework, which originally classified personalization into four 
dimensions (content, order, guidance, and communication) and four mechanisms 
(user choice, provider choice, rule-based, and machine learning), by 
incorporating non-therapeutic elements.
RESULTS: The adapted framework includes therapeutic and non-therapeutic content, 
order, guidance, therapeutic and non-therapeutic communication, interfaces 
(customization of non-therapeutic visual or interactive components), and 
interactivity (personalization of user preferences), while retaining the 
original mechanisms. Half of the interventions studied used only one 
personalization dimension (51%), and more than two-thirds used only one 
personalization mechanism. This review found that personalization of therapeutic 
content (51% of the interventions) and interfaces (25%) were favored. User 
choice was the most prevalent personalization mechanism, present in 60% of 
interventions. Additionally, machine learning mechanisms were employed in a 
substantial number of cases (30%), but there were no instances of generative 
artificial intelligence (AI) among the included studies.
DISCUSSION: The findings of the review suggest that although personalization 
elements of the interventions are reported in the articles, their impact on 
younger people's experience with DMHIs and adherence to mental health protocols 
is not thoroughly addressed. Future interventions may benefit from incorporating 
generative AI, while adhering to standard clinical research practices, to 
further personalize user experiences.

© 2025 Wanniarachchi, Greenhalgh, Choi and Warren.

DOI: 10.3389/fdgth.2025.1500220
PMCID: PMC12119569
PMID: 40444184

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


114. JMIR Med Inform. 2025 May 27;13:e59309. doi: 10.2196/59309.

Using Large Language Models to Enhance Exercise Recommendations and Physical 
Activity in Clinical and Healthy Populations: Scoping Review.

Lai X(1), Chen J(2), Lai Y(3), Huang S(2)(4), Cai Y(4), Sun Z(2), Wang X(2), Pan 
K(5), Gao Q(1), Huang C(1)(2).

Author information:
(1)School of Sport Medicine and Rehabilitation, Beijing Sport University, No.48 
Xinxi Road, Haidian District, Beijing, 100084, China.
(2)Research and Communication Center for Exercise and Health, Xiamen University 
of Technology, 600 Ligong Road, Jimei District, Xiamen, 310204, China, 86 
15606951380.
(3)Department of Mathematics and Digital Science, Chengyi College, Jimei 
University, Xiamen, China.
(4)School of Physical Education and Sport Science, Fujian Normal University, 
Fuzhou, China.
(5)School of Marine Culture and Tourism, Xiamen Ocean Vocational College, 
Xiamen, China.

BACKGROUND: Regular exercise recommendations (ERs) and physical activity (PA) 
are crucial for the prevention and management of chronic diseases. However, 
creating effective exercise programs demand substantial time and specialized 
expertise from both medical and sports professionals. Large language models 
(LLMs), such as ChatGPT, offer a promising solution by helping create 
personalized ERs. While LLMs show potential, their use in exercise planning 
remains in its early stages and requires further exploration.
OBJECTIVES: This study aims to systematically review and classify the 
applications of LLMs in ERs and PA. It also seeks to identify existing gaps and 
provide insights into future research directions for optimizing LLM integration 
in personalized health interventions.
METHODS: A scoping review methodology was used to identify studies related to 
LLM applications in ERs and PA. Literature searches were conducted in Web of 
Science, PubMed, IEEE, and arXiv for English language papers published up to 
March 21, 2024. Keywords included LLMs, chatbots, ERs, PA, fitness plan, and 
related terms. Two independent reviewers (XL and CH) screened and selected 
studies based on predefined inclusion criteria. Thematic analysis was used to 
synthesize findings, which were presented narratively.
RESULTS: An initial search identified 598 papers, of which 1.8% (11/598) of 
studies were included after screening and applying selection criteria. Of these, 
ChatGPT-based models were used in 55% (6/11) of the studies. In addition, 73% 
(8/11) of the studies used expert evaluations and user feedback to assess model 
usability, and 45% (5/11) of the studies used experimental designs to evaluate 
LLM interventions in ERs and PA. Key findings indicated that LLMs can generate 
tailored ERs, save time in clinical practice, and enhance safety by 
incorporating patient-specific data. They also increased engagement and 
supported behavior change. This made PA guidance more accessible, especially in 
remote or underserved communities.
CONCLUSIONS: This review highlights the promising applications of LLMs in ERs 
and PA but emphasizes that they remain a supplement to human expertise. Expert 
validation is essential to ensure safety and mitigate risks. Future research 
should prioritize pilot testing, clinician training programs, and large-scale 
clinical trials to enhance feasibility, transparency, and ethical integration.

© Xiangxun Lai, Jiacheng Chen, Yue Lai, Shengqi Huang, Yongdong Cai, Zhifeng 
Sun, Xueding Wang, Kaijiang Pan, Qi Gao, Caihua Huang. Originally published in 
JMIR Medical Informatics (https://medinform.jmir.org).

DOI: 10.2196/59309
PMCID: PMC12133071
PMID: 40424584 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


115. Digit Health. 2025 May 20;11:20552076251344385. doi: 10.1177/20552076251344385. 
eCollection 2025 Jan-Dec.

Ethical and privacy challenges of integrating generative AI into EHR systems in 
Tanzania: A scoping review with a policy perspective.

Mwogosi A(1).

Author information:
(1)Department of Information Systems and Technology, University of Dodoma, 
Dodoma, Tanzania.

OBJECTIVES: This study examines the ethical and privacy challenges of 
integrating generative artificial intelligence (AI) into electronic health 
record (EHR) systems, focusing on Tanzania's healthcare context. It critically 
analyses the extent to which Tanzania's Policy Framework for Artificial 
Intelligence in the Health Sector (2022) addresses these challenges and proposes 
regulatory and practical safeguards for responsible generative AI deployment.
METHODS: A systematic scoping review was conducted using PubMed, IEEE Xplore, 
Scopus and Google Scholar to identify relevant studies published between 2014 
and 2024. The Preferred Reporting Items for Systematic Reviews and Meta-Analyses 
extension for Scoping Reviews (PRISMA-ScR) guidelines informed the search and 
selection process. Fourteen studies met the inclusion criteria and were 
thematically analysed to identify key ethical and privacy concerns of generative 
AI in healthcare. Moreover, a policy analysis of Tanzania's AI framework was 
conducted to assess its alignment with global best practices and regulatory 
preparedness.
RESULTS: The review identified six key ethical and privacy challenges associated 
with generative AI in EHR systems: data privacy and security risks, algorithmic 
bias and fairness concerns, transparency and accountability issues, consent and 
autonomy challenges, human oversight gaps and risks of data re-identification. 
The policy analysis revealed that while Tanzania's AI framework aligns with 
national health priorities and promotes capacity building and ethical 
governance, it lacks generative AI-specific guidelines, regulatory clarity and 
resource mobilisation strategies necessary for healthcare settings.
CONCLUSION: Integrating generative AI into Tanzania's EHR systems presents 
transformative opportunities and significant ethical and privacy risks. 
Tanzania's policy framework should incorporate AI-specific ethical guidelines, 
operationalise regulatory mechanisms, foster stakeholder engagement through 
participatory co-design and strengthen infrastructural investments. These 
measures will promote ethical integrity, enhance patient trust and position 
Tanzania as a regional leader in responsible AI use in healthcare.

© The Author(s) 2025.

DOI: 10.1177/20552076251344385
PMCID: PMC12093014
PMID: 40400763

Conflict of interest statement: The author declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


116. J Med Syst. 2025 May 17;49(1):65. doi: 10.1007/s10916-025-02197-w.

Assessment and Integration of Large Language Models for Automated Electronic 
Health Record Documentation in Emergency Medical Services.

Bai E(1), Luo X(2)(3), Zhang Z(4), Adelgais K(5), Ali H(6), Finkelstein J(7), 
Kutzin J(8).

Author information:
(1)School of Computer Science and Information Systems, Pace University, New York 
City, NY, USA.
(2)Department of Management Science and Information Systems, Oklahoma State 
University, Stillwater, OK, USA. xiao.luo@okstate.edu.
(3)School of Medicine, Indiana University, Indianapolis, IN, USA. 
xiao.luo@okstate.edu.
(4)School of Computer Science and Information Systems, Pace University, New York 
City, NY, USA. zzhang@pace.edu.
(5)School of Medicine, University of Colorado, Aurora, CO, USA.
(6)Maimonides Medical Center, New York City, NY, USA.
(7)Interfaith Medical Center, New York City, NY, USA.
(8)Mount Sinai Hospital, New York City, NY, USA.

Automating Electronic Health Records (EHR) documentation can significantly 
reduce the burden on care providers, particularly in emergency care settings 
where rapid and accurate record-keeping is crucial. A critical aspect of this 
automation involves using natural language processing (NLP) techniques to 
convert transcribed conversations into structured EHR fields. For instance, 
extracting temperature values like "102.4 Fahrenheit" from the transcribed text 
"His temperature is 39.1, which is 102.4 Fahrenheit." However, traditional 
rule-based and single-model NLP approaches often struggle with domain-specific 
medical terminology, contextual ambiguity, and numerical extraction errors. This 
study investigates the potential of integrating multiple Large Language Models 
(LLMs) to enhance EMS documentation accuracy. We developed an LLM integration 
framework and evaluated four state-of-the-art LLMs-Claude 3.5, GPT-4, Gemini, 
and Mistral-on a dataset comprising transcribed conversations from 40 EMS 
training simulations. The evaluation focused on precision, recall, and F1 score 
across zero-shot and few-shot learning scenarios. Results showed that the 
integrated LLM framework outperformed individual models, achieving overall F1 
scores of 0.78 (zero-shot) and 0.81 (few-shot). In addition to quantitative 
evaluation, a preliminary user study was conducted with domain experts to assess 
the perceived usefulness and challenges of the integrated framework. The 
findings suggest that this approach has the potential to reduce documentation 
effort compared to traditional manual documentation. However, challenges such as 
misinterpretation of medical context and occasional omissions were noted, 
highlighting areas for further refinement and future work. This research is the 
first to systematically explore and evaluate the use of LLMs for real-time EMS 
EHR documentation. By addressing key challenges in automated transcription and 
structured data extraction, our work lays a foundation for real-world 
implementation, improving efficiency and accuracy in emergency medical 
documentation.

Publisher: Not applicable.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02197-w
PMID: 40381087 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Human Ethics and Consent to 
Participate: This study has been approved by the Pace University Institutional 
Review Board (IRB# 1515261-2). All participants in the simulations agreed that 
the data could be used for research purposes, provided their identities remain 
anonymous. Competing Interests: The authors declare no competing interests.


117. J Med Internet Res. 2025 May 15;27:e68998. doi: 10.2196/68998.

Scientific Evidence for Clinical Text Summarization Using Large Language Models: 
Scoping Review.

Bednarczyk L(#)(1), Reichenpfader D(#)(2)(3), Gaudet-Blavignac C(#)(1), Ette 
AK(1)(3), Zaghir J(1)(3), Zheng Y(1)(3), Bensahla A(1)(3), Bjelogrlic M(1)(3), 
Lovis C(1)(3).

Author information:
(1)Division of Medical Information Sciences, University Hospital of Geneva, 
Geneva, Switzerland.
(2)Institute for Patient-centered Digital Health, Bern University of Applied 
Sciences, Biel, Switzerland.
(3)Faculty of Medicine, University of Geneva, Geneva, Switzerland.
(#)Contributed equally

BACKGROUND: Information overload in electronic health records requires effective 
solutions to alleviate clinicians' administrative tasks. Automatically 
summarizing clinical text has gained significant attention with the rise of 
large language models. While individual studies show optimism, a structured 
overview of the research landscape is lacking.
OBJECTIVE: This study aims to present the current state of the art on clinical 
text summarization using large language models, evaluate the level of evidence 
in existing research and assess the applicability of performance findings in 
clinical settings.
METHODS: This scoping review complied with the PRISMA-ScR (Preferred Reporting 
Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) 
guidelines. Literature published between January 1, 2019, and June 18, 2024, was 
identified from 5 databases: PubMed, Embase, Web of Science, IEEE Xplore, and 
ACM Digital Library. Studies were excluded if they did not describe 
transformer-based models, did not focus on clinical text summarization, did not 
engage with free-text data, were not original research, were nonretrievable, 
were not peer-reviewed, or were not in English, French, Spanish, or German. Data 
related to study context and characteristics, scope of research, and evaluation 
methodologies were systematically collected and analyzed by 3 authors 
independently.
RESULTS: A total of 30 original studies were included in the analysis. All used 
observational retrospective designs, mainly using real patient data (n=28, 93%). 
The research landscape demonstrated a narrow research focus, often centered on 
summarizing radiology reports (n=17, 57%), primarily involving data from the 
intensive care unit (n=15, 50%) of US-based institutions (n=19, 73%), in English 
(n=26, 87%). This focus aligned with the frequent reliance on the open-source 
Medical Information Mart for Intensive Care dataset (n=15, 50%). Summarization 
methodologies predominantly involved abstractive approaches (n=17, 57%) on 
single-document inputs (n=4, 13%) with unstructured data (n=13, 43%), yet 
reporting on methodological details remained inconsistent across studies. Model 
selection involved both open-source models (n=26, 87%) and proprietary models 
(n=7, 23%). Evaluation frameworks were highly heterogeneous. All studies 
conducted internal validation, but external validation (n=2, 7%), failure 
analysis (n=6, 20%), and patient safety risks analysis (n=1, 3%) were 
infrequent, and none reported bias assessment. Most studies used both automated 
metrics and human evaluation (n=16, 53%), while 10 (33%) used only automated 
metrics, and 4 (13%) only human evaluation.
CONCLUSIONS: Key barriers hinder the translation of current research into 
trustworthy, clinically valid applications. Current research remains exploratory 
and limited in scope, with many applications yet to be explored. Performance 
assessments often lack reliability, and clinical impact evaluations are 
insufficient raising concerns about model utility, safety, fairness, and data 
privacy. Advancing the field requires more robust evaluation frameworks, a 
broader research scope, and a stronger focus on real-world applicability.

©Lydie Bednarczyk, Daniel Reichenpfader, Christophe Gaudet-Blavignac, Amon Kenna 
Ette, Jamil Zaghir, Yuanyuan Zheng, Adel Bensahla, Mina Bjelogrlic, Christian 
Lovis. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 15.05.2025.

DOI: 10.2196/68998
PMCID: PMC12123242
PMID: 40371947 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


118. J Am Med Inform Assoc. 2025 Jul 1;32(7):1120-1129. doi: 10.1093/jamia/ocaf056.

A comparative analysis of privacy-preserving large language models for automated 
echocardiography report analysis.

Mahmoudi E(1)(2), Vahdati S(1), Chao CJ(1)(2), Khosravi B(1), Misra A(1), 
Lopez-Jimenez F(2), Erickson BJ(1).

Author information:
(1)Department of Radiology, Radiology Informatics Lab, Mayo Clinic, Rochester, 
MN 55905, United States.
(2)Department of Cardiovascular Medicine, Mayo Clinic Rochester, Rochester, MN 
55905, United States.

BACKGROUND: Automated data extraction from echocardiography reports could 
facilitate large-scale registry creation and clinical surveillance of valvular 
heart diseases (VHD). We evaluated the performance of open-source large language 
models (LLMs) guided by prompt instructions and chain of thought (CoT) for this 
task.
METHODS: From consecutive transthoracic echocardiographies performed in our 
center, we utilized 200 random reports from 2019 for prompt optimization and 
1000 from 2023 for evaluation. Five instruction-tuned LLMs (Qwen2.0-72B, 
Llama3.0-70B, Mixtral8-46.7B, Llama3.0-8B, and Phi3.0-3.8B) were guided by 
prompt instructions with and without CoT to classify prosthetic valve presence 
and VHD severity. Performance was evaluated using classification metrics against 
expert-labeled ground truth. Mean squared error (MSE) was also calculated for 
predicted severity's deviation from actual severity.
RESULTS: With CoT prompting, Llama3.0-70B and Qwen2.0 achieved the highest 
performance (accuracy: 99.1% and 98.9% for VHD severity; 100% and 99.9% for 
prosthetic valve; MSE: 0.02 and 0.05, respectively). Smaller models showed lower 
accuracy for VHD severity (54.1%-85.9%) but maintained high accuracy for 
prosthetic valve detection (>96%). Chain of thought reasoning yielded higher 
accuracy for larger models while increasing processing time from 2-25 to 67-154 
seconds per report. Based on CoT reasonings, the wrong predictions were mainly 
due to model outputs being influenced by irrelevant information in the text or 
failure to follow the prompt instructions.
CONCLUSIONS: Our study demonstrates the near-perfect performance of open-source 
LLMs for automated echocardiography report interpretation with the purpose of 
registry formation and disease surveillance. While larger models achieved 
exceptional accuracy through prompt optimization, practical implementation 
requires balancing performance with computational efficiency.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf056
PMCID: PMC12257941
PMID: 40334045 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
declare.


119. J Am Med Inform Assoc. 2025 Jun 1;32(6):1071-1086. doi: 10.1093/jamia/ocaf063.

The emergence of large language models as tools in literature reviews: a large 
language model-assisted systematic review.

Scherbakov D(1), Hubig N(1)(2), Jansari V(3), Bakumenko A(3), Lenert LA(1).

Author information:
(1)Biomedical Informatics Center, Department of Public Health Sciences, Medical 
University of South Carolina (MUSC), Charleston, SC 29403, United States.
(2)Interdisciplinary Transformation University, OG 2 A-4040 Linz, Austria.
(3)School of Computing, Clemson University, Charleston, SC 29634, United States.

OBJECTIVES: This study aims to summarize the usage of large language models 
(LLMs) in the process of creating a scientific review by looking at the 
methodological papers that describe the use of LLMs in review automation and the 
review papers that mention they were made with the support of LLMs.
MATERIALS AND METHODS: The search was conducted in June 2024 in PubMed, Scopus, 
Dimensions, and Google Scholar by human reviewers. Screening and extraction 
process took place in Covidence with the help of LLM add-on based on the OpenAI 
GPT-4o model. ChatGPT and Scite.ai were used in cleaning the data, generating 
the code for figures, and drafting the manuscript.
RESULTS: Of the 3788 articles retrieved, 172 studies were deemed eligible for 
the final review. ChatGPT and GPT-based LLM emerged as the most dominant 
architecture for review automation (n = 126, 73.2%). A significant number of 
review automation projects were found, but only a limited number of papers 
(n = 26, 15.1%) were actual reviews that acknowledged LLM usage. Most citations 
focused on the automation of a particular stage of review, such as Searching for 
publications (n = 60, 34.9%) and Data extraction (n = 54, 31.4%). When comparing 
the pooled performance of GPT-based and BERT-based models, the former was better 
in data extraction with a mean precision of 83.0% (SD = 10.4) and a recall of 
86.0% (SD = 9.8).
DISCUSSION AND CONCLUSION: Our LLM-assisted systematic review revealed a 
significant number of research projects related to review automation using LLMs. 
Despite limitations, such as lower accuracy of extraction for numeric data, we 
anticipate that LLMs will soon change the way scientific reviews are conducted.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf063
PMCID: PMC12089777
PMID: 40332983 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
declare.


120. J Med Syst. 2025 May 7;49(1):59. doi: 10.1007/s10916-025-02188-x.

ChatOCT: Embedded Clinical Decision Support Systems for Optical Coherence 
Tomography in Offline and Resource-Limited Settings.

Liu C(1), Zhang H(1), Zheng Z(2)(3), Liu W(2)(3), Gu C(1), Lan Q(1), Zhang W(1), 
Yang J(4).

Author information:
(1)School of Biomedical Engineering, Shanghai Jiao Tong University, Xuhui 
District, No. 3 Teaching Building, 1954 Huashan RD, Shanghai, China.
(2)Department of Ophthalmology, Shanghai General Hospital, Shanghai, China.
(3)National Clinical Research Center for Eye Diseases, Shanghai, China.
(4)School of Biomedical Engineering, Shanghai Jiao Tong University, Xuhui 
District, No. 3 Teaching Building, 1954 Huashan RD, Shanghai, China. 
jyangoptics@gmail.com.

Optical Coherence Tomography (OCT) is a critical imaging modality for diagnosing 
ocular and systemic conditions, yet its accessibility is hindered by the need 
for specialized expertise and high computational demands. To address these 
challenges, we introduce ChatOCT, an offline-capable, domain-adaptive clinical 
decision support system (CDSS) that integrates structured expert Q&A generation, 
OCT-specific knowledge injection, and activation-aware model compression. Unlike 
existing systems, ChatOCT functions without internet access, making it suitable 
for low-resource environments. ChatOCT is built upon LLaMA-2-7B, incorporating 
domain-specific knowledge from PubMed and OCT News through a two-stage training 
process: (1) knowledge injection for OCT-specific expertise and (2) Q&A 
instruction tuning for structured, interactive diagnostic reasoning. To ensure 
feasibility in offline environments, we apply activation-aware weight 
quantization, reducing GPU memory usage to ~ 4.74 GB, enabling deployment on 
standard OCT hardware. A novel expert answer generation framework mitigates 
hallucinations by structuring responses in a multi-step process, ensuring 
accuracy and interpretability. ChatOCT outperforms state-of-the-art baselines 
such as LLaMA-2, PMC-LLaMA-13B, and ChatDoctor by 10-15 points in coherence, 
relevance, and clinical utility, while reducing GPU memory requirements by 79%, 
while maintaining real-time responsiveness (~ 20 ms inference time). Expert 
ophthalmologists rated ChatOCT's outputs as clinically actionable and aligned 
with real-world decision-making needs, confirming its potential to assist 
frontline healthcare providers. ChatOCT represents an innovative offline 
clinical decision support system for optical coherence tomography (OCT) that 
runs entirely on local embedded hardware, enabling real-time analysis in 
resource-limited settings without internet connectivity. By offering a scalable, 
generalizable pipeline that integrates knowledge injection, instruction tuning, 
and model compression, ChatOCT provides a blueprint for next-generation, 
resource-efficient clinical AI solutions across multiple medical domains.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02188-x
PMID: 40332685 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Human Ethics and Consent to 
Participate Declarations: Not applicable. Competing interests: The authors 
declare no competing interests. Disclosures: All the authors have nothing to 
disclose and declare that the research was conducted in the absence of any 
commercial or financial relationships that could be construed as a potential 
conflict of interest.


121. J Med Internet Res. 2025 May 5;27:e69284. doi: 10.2196/69284.

The Applications of Large Language Models in Mental Health: Scoping Review.

Jin Y(#)(1), Liu J(#)(1), Li P(#)(1), Wang B(#)(1), Yan Y(2), Zhang H(3), Ni 
C(3), Wang J(4), Li Y(5), Bu Y(5), Wang Y(2).

Author information:
(1)Department of Statistics, Faculty of Arts and Sciences, Beijing Normal 
University, Beijing, China.
(2)School of Psychology, Center for Studies of Psychological Application, and 
Guangdong Key Laboratory of Mental Health and Cognitive Science, Key Laboratory 
of Brain, Cognition and Education Sciences, Ministry of Education, South China 
Normal University, Guangzhou, Guangdong, China.
(3)School of Statistics, Beijing Normal University, Beijing, China.
(4)Faculty of Computer Science, Guangdong Polytechnic Normal University, 
Guangzhou, Guangdong, China.
(5)The People's Hospital of Pingbian County, Honghe, Yunnan, China.
(#)Contributed equally

BACKGROUND: Mental health is emerging as an increasingly prevalent public issue 
globally. There is an urgent need in mental health for efficient detection 
methods, effective treatments, affordable privacy-focused health care solutions, 
and increased access to specialized psychiatrists. The emergence and rapid 
development of large language models (LLMs) have shown the potential to address 
these mental health demands. However, a comprehensive review summarizing the 
application areas, processes, and performance comparisons of LLMs in mental 
health has been lacking until now.
OBJECTIVE: This review aimed to summarize the applications of LLMs in mental 
health, including trends, application areas, performance comparisons, 
challenges, and prospective future directions.
METHODS: A scoping review was conducted to map the landscape of LLMs' 
applications in mental health, including trends, application areas, comparative 
performance, and future trajectories. We searched 7 electronic databases, 
including Web of Science, PubMed, Cochrane Library, IEEE Xplore, Weipu, CNKI, 
and Wanfang, from January 1, 2019, to August 31, 2024. Studies eligible for 
inclusion were peer-reviewed articles focused on LLMs' applications in mental 
health. Studies were excluded if they (1) were not peer-reviewed or did not 
focus on mental health or mental disorders or (2) did not use LLMs; studies that 
used only natural language processing or long short-term memory models were also 
excluded. Relevant information on application details and performance metrics 
was extracted during the data charting of eligible articles.
RESULTS: A total of 95 articles were drawn from 4859 studies using LLMs for 
mental health tasks. The applications were categorized into 3 key areas: 
screening or detection of mental disorders (67/95, 71%), supporting clinical 
treatments and interventions (31/95, 33%), and assisting in mental health 
counseling and education (11/95, 12%). Most studies used LLMs for depression 
detection and classification (33/95, 35%), clinical treatment support and 
intervention (14/95, 15%), and suicide risk prediction (12/95, 13%). Compared 
with nontransformer models and humans, LLMs demonstrate higher capabilities in 
information acquisition and analysis and efficiently generating natural language 
responses. Various series of LLMs also have different advantages and 
disadvantages in addressing mental health tasks.
CONCLUSIONS: This scoping review synthesizes the applications, processes, 
performance, and challenges of LLMs in the mental health field. These findings 
highlight the substantial potential of LLMs to augment mental health research, 
diagnostics, and intervention strategies, underscoring the imperative for 
ongoing development and ethical deliberation in clinical settings.

©Yu Jin, Jiayi Liu, Pan Li, Baosen Wang, Yangxinyu Yan, Huilin Zhang, Chenhao 
Ni, Jing Wang, Yi Li, Yajun Bu, Yuanyuan Wang. Originally published in the 
Journal of Medical Internet Research (https://www.jmir.org), 05.05.2025.

DOI: 10.2196/69284
PMCID: PMC12089884
PMID: 40324177 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


122. J Med Internet Res. 2025 Apr 30;27:e66098. doi: 10.2196/66098.

Use of Retrieval-Augmented Large Language Model for COVID-19 Fact-Checking: 
Development and Usability Study.

Li H(#)(1), Huang J(#)(1), Ji M(2), Yang Y(3), An R(4).

Author information:
(1)School of Economics and Management, Shanghai University of Sport, Shanghai, 
China.
(2)Department of Surgery, Division of Public Health Sciences, Washington 
University School of Medicine in St. Louis, St. Louis, MO, United States.
(3)Division of Computational and Data Sciences, Washington University in St. 
Louis, St. Louis, MO, United States.
(4)Constance and Martin Silver Center on Data Science and Social Equity, Silver 
School of Social Work, New York University, New York, NY, United States.
(#)Contributed equally

BACKGROUND: The COVID-19 pandemic has been accompanied by an "infodemic," where 
the rapid spread of misinformation has exacerbated public health challenges. 
Traditional fact-checking methods, though effective, are time-consuming and 
resource-intensive, limiting their ability to combat misinformation at scale. 
Large language models (LLMs) such as GPT-4 offer a more scalable solution, but 
their susceptibility to generating hallucinations-plausible yet incorrect 
information-compromises their reliability.
OBJECTIVE: This study aims to enhance the accuracy and reliability of COVID-19 
fact-checking by integrating a retrieval-augmented generation (RAG) system with 
LLMs, specifically addressing the limitations of hallucination and context 
inaccuracy inherent in stand-alone LLMs.
METHODS: We constructed a context dataset comprising approximately 130,000 
peer-reviewed papers related to COVID-19 from PubMed and Scopus. This dataset 
was integrated with GPT-4 to develop multiple RAG-enhanced models: the naïve 
RAG, Lord of the Retrievers (LOTR)-RAG, corrective RAG (CRAG), and self-RAG 
(SRAG). The RAG systems were designed to retrieve relevant external information, 
which was then embedded and indexed in a vector store for similarity searches. 
One real-world dataset and one synthesized dataset, each containing 500 claims, 
were used to evaluate the performance of these models. Each model's accuracy, 
F1-score, precision, and sensitivity were compared to assess their effectiveness 
in reducing hallucination and improving fact-checking accuracy.
RESULTS: The baseline GPT-4 model achieved an accuracy of 0.856 on the 
real-world dataset. The naïve RAG model improved this to 0.946, while the 
LOTR-RAG model further increased accuracy to 0.951. The CRAG and SRAG models 
outperformed all others, achieving accuracies of 0.972 and 0.973, respectively. 
The baseline GPT-4 model reached an accuracy of 0.960 on the synthesized 
dataset. The naïve RAG model increased this to 0.972, and the LOTR-RAG, CRAG, 
and SRAG models achieved an accuracy of 0.978. These findings demonstrate that 
the RAG-enhanced models consistently maintained high accuracy levels, closely 
mirroring ground-truth labels and significantly reducing hallucinations. The 
CRAG and SRAG models also provided more detailed and contextually accurate 
explanations, further establishing the superiority of agentic RAG frameworks in 
delivering reliable and precise fact-checking outputs across diverse datasets.
CONCLUSIONS: The integration of RAG systems with LLMs substantially improves the 
accuracy and contextual relevance of automated fact-checking. By reducing 
hallucinations and enhancing transparency by citing retrieved sources, this 
method holds significant promise for rapid, reliable information verification to 
combat misinformation during public health crises.

©Hai Li, Jingyi Huang, Mengmeng Ji, Yuyi Yang, Ruopeng An. Originally published 
in the Journal of Medical Internet Research (https://www.jmir.org), 30.04.2025.

DOI: 10.2196/66098
PMCID: PMC12079058
PMID: 40306628 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


123. J Med Internet Res. 2025 Apr 30;27:e64486. doi: 10.2196/64486.

Accuracy of Large Language Models When Answering Clinical Research Questions: 
Systematic Review and Network Meta-Analysis.

Wang L(#)(1)(2), Li J(#)(2), Zhuang B(#)(3), Huang S(#)(4), Fang M(#)(2), Wang 
C(2), Li W(1), Zhang M(2), Gong S(5).

Author information:
(1)Fuzhou University Affiliated Provincial Hospital, Shengli Clinical Medical 
College, Fujian Medical University, Fuzhou, China.
(2)School of Pharmacy, Fujian Medical University, Fuzhou, China.
(3)Fujian Center For Drug Evaluation and Monitoring, Fuzhou, China.
(4)School of Pharmacy, Fujian University of Traditional Chinese Medicine, 
Fuzhou, China.
(5)The Third Department of Critical Care Medicine, Fuzhou University Affiliated 
Provincial Hospital, Shengli Clinical Medical College, Fujian Medical 
University, Fuzhou, Fujian, China.
(#)Contributed equally

BACKGROUND: Large language models (LLMs) have flourished and gradually become an 
important research and application direction in the medical field. However, due 
to the high degree of specialization, complexity, and specificity of medicine, 
which results in extremely high accuracy requirements, controversy remains about 
whether LLMs can be used in the medical field. More studies have evaluated the 
performance of various types of LLMs in medicine, but the conclusions are 
inconsistent.
OBJECTIVE: This study uses a network meta-analysis (NMA) to assess the accuracy 
of LLMs when answering clinical research questions to provide high-level 
evidence-based evidence for its future development and application in the 
medical field.
METHODS: In this systematic review and NMA, we searched PubMed, Embase, Web of 
Science, and Scopus from inception until October 14, 2024. Studies on the 
accuracy of LLMs when answering clinical research questions were included and 
screened by reading published reports. The systematic review and NMA were 
conducted to compare the accuracy of different LLMs when answering clinical 
research questions, including objective questions, open-ended questions, top 1 
diagnosis, top 3 diagnosis, top 5 diagnosis, and triage and classification. The 
NMA was performed using Bayesian frequency theory methods. Indirect 
intercomparisons between programs were performed using a grading scale. A larger 
surface under the cumulative ranking curve (SUCRA) value indicates a higher 
ranking of the corresponding LLM accuracy.
RESULTS: The systematic review and NMA examined 168 articles encompassing 35,896 
questions and 3063 clinical cases. Of the 168 studies, 40 (23.8%) were 
considered to have a low risk of bias, 128 (76.2%) had a moderate risk, and none 
were rated as having a high risk. ChatGPT-4o (SUCRA=0.9207) demonstrated strong 
performance in terms of accuracy for objective questions, followed by 
Aeyeconsult (SUCRA=0.9187) and ChatGPT-4 (SUCRA=0.8087). ChatGPT-4 
(SUCRA=0.8708) excelled at answering open-ended questions. In terms of accuracy 
for top 1 diagnosis and top 3 diagnosis of clinical cases, human experts 
(SUCRA=0.9001 and SUCRA=0.7126, respectively) ranked the highest, while Claude 3 
Opus (SUCRA=0.9672) performed well at the top 5 diagnosis. Gemini (SUCRA=0.9649) 
had the highest rated SUCRA value for accuracy in the area of triage and 
classification.
CONCLUSIONS: Our study indicates that ChatGPT-4o has an advantage when answering 
objective questions. For open-ended questions, ChatGPT-4 may be more credible. 
Humans are more accurate at the top 1 diagnosis and top 3 diagnosis. Claude 3 
Opus performs better at the top 5 diagnosis, while for triage and 
classification, Gemini is more advantageous. This analysis offers valuable 
insights for clinicians and medical practitioners, empowering them to 
effectively leverage LLMs for improved decision-making in learning, diagnosis, 
and management of various clinical scenarios.
TRIAL REGISTRATION: PROSPERO CRD42024558245; 
https://www.crd.york.ac.uk/PROSPERO/view/CRD42024558245.

©Ling Wang, Jinglin Li, Boyang Zhuang, Shasha Huang, Meilin Fang, Cunze Wang, 
Wen Li, Mohan Zhang, Shurong Gong. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org), 30.04.2025.

DOI: 10.2196/64486
PMCID: PMC12079073
PMID: 40305085 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


124. JMIR Med Inform. 2025 Apr 25;13:e64963. doi: 10.2196/64963.

Comparing Diagnostic Accuracy of Clinical Professionals and Large Language 
Models: Systematic Review and Meta-Analysis.

Shan G(1), Chen X(1), Wang C(1), Liu L(2), Gu Y(3), Jiang H(4), Shi T(5).

Author information:
(1)Nanjing Drum Tower Hospital Clinical College of Nanjing University of Chinese 
Medicine, Nanjing, China.
(2)Jiangsu Province Hospital of Chinese Medicine, Affiliated Hospital of Nanjing 
University of Chinese Medicine, Nanjing, China.
(3)Department of Emergency, Nanjing Drum Tower Hospital, Nanjing, China.
(4)Department of Nursing, Nanjing Drum Tower Hospital, Nanjing, China.
(5)Department of Quality Management, Nanjing Drum Tower Hospital, Affiliated 
Hospital of Medical School, Nanjing University, 321 Zhongshan Road, Gulou 
District, Nanjing, 210008, China, 86 1-391-299-6998.

BACKGROUND: With the rapid development of artificial intelligence (AI) 
technology, especially generative AI, large language models (LLMs) have shown 
great potential in the medical field. Through massive medical data training, it 
can understand complex medical texts and can quickly analyze medical records and 
provide health counseling and diagnostic advice directly, especially in rare 
diseases. However, no study has yet compared and extensively discussed the 
diagnostic performance of LLMs with that of physicians.
OBJECTIVE: This study systematically reviewed the accuracy of LLMs in clinical 
diagnosis and provided reference for further clinical application.
METHODS: We conducted searches in CNKI (China National Knowledge 
Infrastructure), VIP Database, SinoMed, PubMed, Web of Science, Embase, and 
CINAHL (Cumulative Index to Nursing and Allied Health Literature) from January 
1, 2017, to the present. A total of 2 reviewers independently screened the 
literature and extracted relevant information. The risk of bias was assessed 
using the Prediction Model Risk of Bias Assessment Tool (PROBAST), which 
evaluates both the risk of bias and the applicability of included studies.
RESULTS: A total of 30 studies involving 19 LLMs and a total of 4762 cases were 
included. The quality assessment indicated a high risk of bias in the majority 
of studies, primary cause is known case diagnosis. For the optimal model, the 
accuracy of the primary diagnosis ranged from 25% to 97.8%, while the triage 
accuracy ranged from 66.5% to 98%.
CONCLUSIONS: LLMs have demonstrated considerable diagnostic capabilities and 
significant potential for application across various clinical cases. Although 
their accuracy still falls short of that of clinical professionals, if used 
cautiously, they have the potential to become one of the best intelligent 
assistants in the field of human health care.

© Guxue Shan, Xiaonan Chen, Chen Wang, Li Liu, Yuanjing Gu, Huiping Jiang, 
Tingqi Shi. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org).

DOI: 10.2196/64963
PMCID: PMC12047852
PMID: 40279517 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


125. J Biomed Inform. 2025 Jun;166:104829. doi: 10.1016/j.jbi.2025.104829. Epub 2025 
Apr 22.

Leveraging natural language processing to elucidate real-world clinical 
decision-making paradigms: A proof of concept study.

Alon Y(1), Naimi E(1), Levin C(2), Videl H(2), Saban M(3).

Author information:
(1)Nursing Department, School of Health Professions, Faculty of Medicine, Tel 
Aviv University, Tel Aviv 69978, Israel.
(2)Faculty of School of Life and Health Sciences, Nursing Department, The 
Jerusalem College of Technology-Lev Academic Center, Jerusalem, Israel.
(3)Nursing Department, School of Health Professions, Faculty of Medicine, Tel 
Aviv University, Tel Aviv 69978, Israel. Electronic address: 
morsaban1@tauex.tau.ac.il.

BACKGROUND: Understanding how clinicians arrive at decisions in actual practice 
settings is vital for advancing personalized, evidence-based care. However, 
systematic analysis of qualitative decision data poses challenges.
METHODS: We analyzed transcribed interviews with Hebrew-speaking clinicians on 
decision processes using natural language processing (NLP). Word frequency and 
characterized terminology use, while large language models (ChatGPT from OpenAI 
and Gemini by Google) identified potential cognitive paradigms.
RESULTS: Word frequency analysis of clinician interviews identified experience 
and knowledge as most influential on decision-making. NLP tentatively recognized 
heuristics-based reasoning grounded in past cases and intuition as dominant 
cognitive paradigms. Elements of shared decision-making through individualizing 
care with patients and families were also observed. Limited Hebrew clinical 
language resources required developing preliminary lexicons and dynamically 
adjusting stopwords. Findings also provided preliminary support for heuristics 
guiding clinical judgment while highlighting needs for broader sampling and 
enhanced analytical frameworks.
CONCLUSIONS: This study represents the first use of integrated qualitative and 
computational methods to systematically elucidate clinical decision-making. 
Findings supported experience-based heuristics guiding cognition. With 
methodological enhancements, similar analyses could transform global 
understanding of tailored care delivery. Standardizing interdisciplinary 
collaborations on developing NLP tools and analytical frameworks may advance 
equitable, evidence-based healthcare by elucidating real-world clinical 
reasoning processes across diverse populations and settings.

Copyright © 2025 The Author(s). Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104829
PMID: 40274037 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


126. J Biomed Inform. 2025 Jun;166:104819. doi: 10.1016/j.jbi.2025.104819. Epub 2025 
Apr 16.

RoBIn: A Transformer-based model for risk of bias inference with machine reading 
comprehension.

Dias AC(1), Moreira VP(1), Comba JLD(2).

Author information:
(1)Instituto de Informatica, Av. Bento Goncalves 9500 - Caixa Postal 15064, 
Porto Alegre, 91501-970, Rio Grande do Sul, Brazil.
(2)Instituto de Informatica, Av. Bento Goncalves 9500 - Caixa Postal 15064, 
Porto Alegre, 91501-970, Rio Grande do Sul, Brazil. Electronic address: 
joao.comba@gmail.com.

OBJECTIVE: Scientific publications are essential for uncovering insights, 
testing new drugs, and informing healthcare policies. Evaluating the quality of 
these publications often involves assessing their Risk of Bias (RoB), a task 
traditionally performed by human reviewers. The goal of this work is to create a 
dataset and develop models that allow automated RoB assessment in clinical 
trials.
METHODS: We use data from the Cochrane Database of Systematic Reviews (CDSR) as 
ground truth to label open-access clinical trial publications from PubMed. This 
process enabled us to develop training and test datasets specifically for 
machine reading comprehension and RoB inference. Additionally, we created 
extractive (RoBInExt) and generative (RoBInGen) Transformer-based approaches to 
extract relevant evidence and classify the RoB effectively.
RESULTS: RoBIn was evaluated across various settings and benchmarked against 
state-of-the-art methods, including large language models (LLMs). In most cases, 
the best-performing RoBIn variant surpasses traditional machine learning and 
LLM-based approaches, achieving a AUROC of 0.83.
CONCLUSION: This work addresses RoB assessment in clinical trials by introducing 
RoBIn, two Transformer-based models for RoB inference and evidence retrieval, 
which outperform traditional models and LLMs, demonstrating its potential to 
improve efficiency and scalability in clinical research evaluation. We also 
introduce a public dataset that is automatically annotated and can be used to 
enable future research to enhance automated RoB assessment.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104819
PMID: 40250743 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


127. J Med Internet Res. 2025 Apr 16;27:e70535. doi: 10.2196/70535.

Unveiling the Potential of Large Language Models in Transforming Chronic Disease 
Management: Mixed Methods Systematic Review.

Li C(1), Zhao Y(1), Bai Y(2), Zhao B(3), Tola YO(4), Chan CW(#)(5), Zhang 
M(#)(2), Fu X(#)(1).

Author information:
(1)The Department of Nursing, The Eighth Affiliated Hospital, Sun Yat-sen 
University, Shenzhen, China.
(2)The School of Nursing, Sun Yat-sen University, Guangzhou, China.
(3)The School of Artificial Intelligence, Sun Yat-sen University, Guangzhou, 
China.
(4)The Department of Clinical Research, Conestoga College, Kitchener, ON, 
Canada.
(5)The Nethersole School of Nursing, The Chinese University of Hong Kong, Hong 
Kong, China.
(#)Contributed equally

BACKGROUND: Chronic diseases are a major global health burden, accounting for 
nearly three-quarters of the deaths worldwide. Large language models (LLMs) are 
advanced artificial intelligence systems with transformative potential to 
optimize chronic disease management; however, robust evidence is lacking.
OBJECTIVE: This review aims to synthesize evidence on the feasibility, 
opportunities, and challenges of LLMs across the disease management spectrum, 
from prevention to screening, diagnosis, treatment, and long-term care.
METHODS: Following the PRISMA (Preferred Reporting Items for Systematic Reviews 
and Meta-Analysis) guidelines, 11 databases (Cochrane Central Register of 
Controlled Trials, CINAHL, Embase, IEEE Xplore, MEDLINE via Ovid, ProQuest 
Health & Medicine Collection, ScienceDirect, Scopus, Web of Science Core 
Collection, China National Knowledge Internet, and SinoMed) were searched on 
April 17, 2024. Intervention and simulation studies that examined LLMs in the 
management of chronic diseases were included. The methodological quality of the 
included studies was evaluated using a rating rubric designed for 
simulation-based research and the risk of bias in nonrandomized studies of 
interventions tool for quasi-experimental studies. Narrative analysis with 
descriptive figures was used to synthesize the study findings. Random-effects 
meta-analyses were conducted to assess the pooled effect estimates of the 
feasibility of LLMs in chronic disease management.
RESULTS: A total of 20 studies examined general-purpose (n=17) and 
retrieval-augmented generation-enhanced LLMs (n=3) for the management of chronic 
diseases, including cancer, cardiovascular diseases, and metabolic disorders. 
LLMs demonstrated feasibility across the chronic disease management spectrum by 
generating relevant, comprehensible, and accurate health recommendations (pooled 
accurate rate 71%, 95% CI 0.59-0.83; I2=88.32%) with retrieval-augmented 
generation-enhanced LLMs having higher accuracy rates compared to 
general-purpose LLMs (odds ratio 2.89, 95% CI 1.83-4.58; I2=54.45%). LLMs 
facilitated equitable information access; increased patient awareness regarding 
ailments, preventive measures, and treatment options; and promoted 
self-management behaviors in lifestyle modification and symptom coping. 
Additionally, LLMs facilitate compassionate emotional support, social 
connections, and health care resources to improve the health outcomes of chronic 
diseases. However, LLMs face challenges in addressing privacy, language, and 
cultural issues; undertaking advanced tasks, including diagnosis, medication, 
and comorbidity management; and generating personalized regimens with real-time 
adjustments and multiple modalities.
CONCLUSIONS: LLMs have demonstrated the potential to transform chronic disease 
management at the individual, social, and health care levels; however, their 
direct application in clinical settings is still in its infancy. A multifaceted 
approach that incorporates robust data security, domain-specific model 
fine-tuning, multimodal data integration, and wearables is crucial for the 
evolution of LLMs into invaluable adjuncts for health care professionals to 
transform chronic disease management.
TRIAL REGISTRATION: PROSPERO CRD42024545412; 
https://www.crd.york.ac.uk/PROSPERO/view/CRD42024545412.

©Caixia Li, Yina Zhao, Yang Bai, Baoquan Zhao, Yetunde Oluwafunmilayo Tola, 
Carmen WH Chan, Meifen Zhang, Xia Fu. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org), 16.04.2025.

DOI: 10.2196/70535
PMCID: PMC12044321
PMID: 40239198 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


128. Gastroenterology. 2025 Sep;169(4):715-717. doi: 10.1053/j.gastro.2025.03.034. 
Epub 2025 Apr 6.

Large Language Model-Supported Systematic Reviews to Augment Clinical Guideline 
Development: An American Gastroenterological Association Pilot.

Chung S(1), Saberzadeh-Ardestani B(2), Nigam G(3), Yuan Y(4), Singh S(5), Shung 
D(6).

Author information:
(1)Section of Digestive Diseases, Yale School of Medicine, New Haven, 
Connecticut. Electronic address: sunny.chung@yale.edu.
(2)Department of Internal Medicine, Yale School of Medicine, New Haven, 
Connecticut.
(3)Translational Gastroenterology and Liver Unit, Nuffield Department of 
Medicine, John Radcliffe Hospital, University of Oxford, Oxford, United Kingdom.
(4)Department of Medicine, Western University, London, Ontario, Canada; 
Department of Medicine, London Health Sciences Centre, London, Ontario, Canada.
(5)Division of Gastroenterology, University of California, San Diego, La Jolla, 
California.
(6)Section of Digestive Diseases, Yale School of Medicine, New Haven, 
Connecticut; Department of Biomedical Informatics and Data Science, Yale School 
of Medicine, New Haven, Connecticut.

DOI: 10.1053/j.gastro.2025.03.034
PMCID: PMC12354060
PMID: 40199387

Conflict of interest statement: Conflict of Interest Statement: The authors 
declare no conflicts of interest related to this manuscript.


129. JMIR Form Res. 2025 Apr 7;9:e64544. doi: 10.2196/64544.

Using Large Language Models to Automate Data Extraction From Surgical Pathology 
Reports: Retrospective Cohort Study.

Lee D(#)(1), Vaid A(#)(2), Menon KM(1), Freeman R(2), Matteson DS(3), Marin 
ML(1), Nadkarni GN(2).

Author information:
(1)Department of Surgery, Icahn School of Medicine at Mount Sinai, 10 Union 
Square East, Suite 2L, New York, NY, 10003, United States, 1 212 241 2891.
(2)Charles Bronfman Institute for Personalized Medicine, Icahn School of 
Medicine at Mount Sinai, New York, NY, United States.
(3)Department of Statistics and Data Science, Cornell University, Ithaca, NY, 
United States.
(#)Contributed equally

BACKGROUND: Popularized by ChatGPT, large language models (LLMs) are poised to 
transform the scalability of clinical natural language processing (NLP) 
downstream tasks such as medical question answering (MQA) and automated data 
extraction from clinical narrative reports. However, the use of LLMs in the 
health care setting is limited by cost, computing power, and patient privacy 
concerns. Specifically, as interest in LLM-based clinical applications grows, 
regulatory safeguards must be established to avoid exposure of patient data 
through the public domain. The use of open-source LLMs deployed behind 
institutional firewalls may ensure the protection of private patient data. In 
this study, we evaluated the extraction performance of a locally deployed LLM 
for automated MQA from surgical pathology reports.
OBJECTIVE: We compared the performance of human reviewers and a locally deployed 
LLM tasked with extracting key histologic and staging information from surgical 
pathology reports.
METHODS: A total of 84 thyroid cancer surgical pathology reports were assessed 
by two independent reviewers and the open-source FastChat-T5 3B-parameter LLM 
using institutional computing resources. Longer text reports were split into 
1200-character-long segments, followed by conversion to embeddings. Three 
segments with the highest similarity scores were integrated to create the final 
context for the LLM. The context was then made part of the question it was 
directed to answer. Twelve medical questions for staging and thyroid cancer 
recurrence risk data extraction were formulated and answered for each report. 
The time to respond and concordance of answers were evaluated. The concordance 
rate for each pairwise comparison (human-LLM and human-human) was calculated as 
the total number of concordant answers divided by the total number of answers 
for each of the 12 questions. The average concordance rate and associated error 
of all questions were tabulated for each pairwise comparison and evaluated with 
two-sided t tests.
RESULTS: Out of a total of 1008 questions answered, reviewers 1 and 2 had an 
average (SD) concordance rate of responses of 99% (1%; 999/1008 responses). The 
LLM was concordant with reviewers 1 and 2 at an overall average (SD) rate of 89% 
(7%; 896/1008 responses) and 89% (7.2%; 903/1008 responses). The overall time to 
review and answer questions for all reports was 170.7, 115, and 19.56 minutes 
for Reviewers 1, 2, and the LLM, respectively.
CONCLUSIONS: The locally deployed LLM can be used for MQA with considerable 
time-saving and acceptable accuracy in responses. Prompt engineering and 
fine-tuning may further augment automated data extraction from clinical 
narratives for the provision of real-time, essential clinical insights.

© Denise Lee, Akhil Vaid, Kartikeya M Menon, Robert Freeman, David S Matteson, 
Michael L Marin, Girish N Nadkarni. Originally published in JMIR Formative 
Research (https://formative.jmir.org).

DOI: 10.2196/64544
PMCID: PMC11996145
PMID: 40194317 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


130. PLoS One. 2025 Apr 3;20(4):e0320151. doi: 10.1371/journal.pone.0320151. 
eCollection 2025.

Using artificial intelligence tools to automate data extraction for living 
evidence syntheses.

Mitchell E(1), Are EB(2), Colijn C(2), Earn DJD(1).

Author information:
(1)Department of Mathematics and Statistics, McMaster University, Hamilton, 
ON,Canada.
(2)Department of Mathematics, Simon Fraser University, Burnaby, BC,Canada.

Living evidence synthesis (LES) involves repeatedly updating a systematic review 
or meta-analysis at regular intervals to incorporate new evidence into the 
summary results. It requires a considerable amount of human time investment in 
the article search, collection, and data extraction phases. Tools exist to 
automate the retrieval of relevant journal articles, but pulling data out of 
those articles is currently still a manual process. In this article, we present 
a proof-of-concept Python program that leverages artificial intelligence (AI) 
tools (specifically, ChatGPT) to parse a batch of journal articles and extract 
relevant results, greatly reducing the human time investment in this action 
without compromising on accuracy. Our program is tested on a set of journal 
articles that estimate the mean incubation period for COVID-19, an 
epidemiological parameter of importance for mathematical modelling. We also 
discuss important limitations related to the total amount of information and 
rate at which that information can be sent to the AI engine. This work 
contributes to the ongoing discussion about the use of AI and the role such 
tools can have in scientific research.

Copyright: © 2025 Mitchell et al. This is an open access article distributed 
under the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0320151
PMCID: PMC11967977
PMID: 40179121 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


131. PLoS One. 2025 Apr 1;20(4):e0321093. doi: 10.1371/journal.pone.0321093. 
eCollection 2025.

Barriers and enablers for the deployment of large language model-based 
conversational robots for older adults: A protocol for a systematic review of 
qualitative studies.

Shankar R(1), Bundele A(1)(2), Mukhopadhyay A(3).

Author information:
(1)Research and Innovation, Medical Affairs, Alexandra Hospital, National 
University Health System, Singapore.
(2)Yong Loo Lin School of Medicine, National University of Singapore, Singapore.
(3)Division of Respiratory & Critical Care Medicine, Department of Medicine, 
National University Health System, Singapore.

BACKGROUND: Artificial intelligence-powered conversational agents have immense 
potential to provide social companionship and support for older adults. However, 
the deployment of large language model (LLM)-based conversational robots for 
seniors faces various technical, user acceptance, and ethical challenges.
OBJECTIVES: This systematic review aims to synthesize insights from prior 
qualitative studies to identify key factors that influence the real-world 
application of LLM-based conversational agents for the aging population. The 
review will inform the user-centered design of these technologies, policy 
discussions on their governance, and highlight research gaps.
METHODS AND ANALYSIS: Eleven electronic databases will be searched for 
qualitative studies exploring stakeholder perspectives on using AI chatbots and 
robots to assist seniors. Two reviewers will independently screen studies, 
extract data, and appraise methodological quality using the JBI checklist. 
Thematic analysis will be conducted to identify major barriers and enablers, and 
confidence in review findings will be assessed using the GRADE-CERQual approach. 
The review will adhere to PRISMA-P and ENTREQ reporting guidelines to ensure 
transparency.
DISCUSSION: Understanding and addressing obstacles to implementing LLM-powered 
conversational agents for older adults is crucial for leveraging this technology 
to support the well-being of the rapidly aging global population. This 
systematic review will provide timely insights to guide the responsible 
development and deployment of AI companions for seniors.
TRIAL REGISTRATION: ClinicalTrials.gov CRD42024601264.

Copyright: © 2025 Shankar et al. This is an open access article distributed 
under the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0321093
PMCID: PMC11960934
PMID: 40168400 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


132. JMIR Form Res. 2025 Mar 28;9:e58366. doi: 10.2196/58366.

The AI Reviewer: Evaluating AI's Role in Citation Screening for Streamlined 
Systematic Reviews.

Ghossein J(#)(1), Hryciw BN(#)(2), Ramsay T(3)(4), Kyeremanteng K(2)(4)(5).

Author information:
(1)Interdepartmental Division of Critical Care Medicine, University of Toronto, 
Toronto, ON, Canada.
(2)Division of Critical Care, Department of Medicine, University of Ottawa, 501 
Smyth Road, Ottawa, ON, Canada, 1 (613) 798-5555 ext 16045.
(3)Faculty of Medicine, University of Ottawa, Ottawa, ON, Canada.
(4)Clinical Epidemiology, Ottawa Hospital Research Institute, Ottawa, ON, 
Canada.
(5)Institute du Savoir Montfort, Montfort Hospital, Ottawa, ON, Canada.
(#)Contributed equally

DOI: 10.2196/58366
PMCID: PMC11970706
PMID: 40153601

Conflict of interest statement: Conflicts of Interest: None declared.


133. NPJ Digit Med. 2025 Mar 25;8(1):178. doi: 10.1038/s41746-025-01566-6.

Accuracy of online symptom assessment applications, large language models, and 
laypeople for self-triage decisions.

Kopka M(1), von Kalckreuth N(2), Feufel MA(2).

Author information:
(1)Division of Ergonomics, Department of Psychology and Ergonomics (IPA), 
Technische Universität Berlin, Berlin, Germany. marvin.kopka@tu-berlin.de.
(2)Division of Ergonomics, Department of Psychology and Ergonomics (IPA), 
Technische Universität Berlin, Berlin, Germany.

Symptom-Assessment Application (SAAs, e.g., NHS 111 online) that assist 
laypeople in deciding if and where to seek care (self-triage) are gaining 
popularity and Large Language Models (LLMs) are increasingly used too. However, 
there is no evidence synthesis on the accuracy of LLMs, and no review has 
contextualized the accuracy of SAAs and LLMs. This systematic review evaluates 
the self-triage accuracy of both SAAs and LLMs and compares them to the accuracy 
of laypeople. A total of 1549 studies were screened and 19 included. The 
self-triage accuracy of SAAs was moderate but highly variable (11.5-90.0%), 
while the accuracy of LLMs (57.8-76.0%) and laypeople (47.3-62.4%) was moderate 
with low variability. Based on the available evidence, the use of SAAs or LLMs 
should neither be universally recommended nor discouraged; rather, we suggest 
that their utility should be assessed based on the specific use case and user 
group under consideration.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01566-6
PMCID: PMC11937345
PMID: 40133390

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


134. NPJ Digit Med. 2025 Mar 22;8(1):175. doi: 10.1038/s41746-025-01543-z.

A systematic review and meta-analysis of diagnostic performance comparison 
between generative AI and physicians.

Takita H(1), Kabata D(2), Walston SL(1)(3), Tatekawa H(1), Saito K(4), Tsujimoto 
Y(5)(6)(7), Miki Y(1), Ueda D(8)(9)(10).

Author information:
(1)Department of Diagnostic and Interventional Radiology, Graduate School of 
Medicine, Osaka Metropolitan University, Osaka, Japan.
(2)Center for Mathematical and Data Science, Kobe University, Kobe, Japan.
(3)Department of Artificial Intelligence, Graduate School of Medicine, Osaka 
Metropolitan University, Osaka, Japan.
(4)Center for Digital Transformation of Health Care, Graduate School of 
Medicine, Kyoto University, Kyoto, Japan.
(5)Oku Medical Clinic, Osaka, Japan.
(6)Department of Health Promotion and Human Behavior, Kyoto University Graduate 
School of Medicine/School of Public Health, Kyoto University, Kyoto, Japan.
(7)Scientific Research WorkS Peer Support Group (SRWS-PSG), Osaka, Japan.
(8)Department of Diagnostic and Interventional Radiology, Graduate School of 
Medicine, Osaka Metropolitan University, Osaka, Japan. ai.labo.ocu@gmail.com.
(9)Department of Artificial Intelligence, Graduate School of Medicine, Osaka 
Metropolitan University, Osaka, Japan. ai.labo.ocu@gmail.com.
(10)Center for Health Science Innovation, Osaka Metropolitan University, Osaka, 
Japan. ai.labo.ocu@gmail.com.

While generative artificial intelligence (AI) has shown potential in medical 
diagnostics, comprehensive evaluation of its diagnostic performance and 
comparison with physicians has not been extensively explored. We conducted a 
systematic review and meta-analysis of studies validating generative AI models 
for diagnostic tasks published between June 2018 and June 2024. Analysis of 83 
studies revealed an overall diagnostic accuracy of 52.1%. No significant 
performance difference was found between AI models and physicians overall 
(p = 0.10) or non-expert physicians (p = 0.93). However, AI models performed 
significantly worse than expert physicians (p = 0.007). Several models 
demonstrated slightly higher performance compared to non-experts, although the 
differences were not significant. Generative AI demonstrates promising 
diagnostic capabilities with accuracy varying by model. Although it has not yet 
achieved expert-level reliability, these findings suggest potential for 
enhancing healthcare delivery and medical education when implemented with 
appropriate understanding of its limitations.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01543-z
PMCID: PMC11929846
PMID: 40121370

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


135. J Am Med Inform Assoc. 2025 May 1;32(5):893-904. doi: 10.1093/jamia/ocaf050.

High-performance automated abstract screening with large language model 
ensembles.

Sanghera R(1)(2), Thirunavukarasu AJ(1)(3), El Khoury M(4)(5)(6), O'Logbon J(7), 
Chen Y(4), Watt A(8), Mahmood M(9), Butt H(4), Nishimura G(4), Soltan 
AAS(1)(10).

Author information:
(1)Oxford University Hospitals NHS Foundation Trust, Oxford OX3 9DU, United 
Kingdom.
(2)Oxford University Clinical Academic Graduate School, Medical Sciences 
Division, University of Oxford, Oxford OX3 9DU, United Kingdom.
(3)Nuffield Department of Clinical Neurosciences, Medical Sciences Division, 
University of Oxford, Oxford OX3 9DU, United Kingdom.
(4)School of Clinical Medicine, University of Cambridge, Cambridge CB2 0SP, 
United Kingdom.
(5)Georgetown University School of Medicine, Georgetown University, Washington, 
DC 20007, United States.
(6)MedStar Washington Hospital Center, Washington, DC 20010, United States.
(7)GKT School of Medical Education, King's College London, London WC2R 2LS, 
United Kingdom.
(8)Oxford Medical School, Medical Sciences Division, University of Oxford, 
Oxford OX3 9DU, United Kingdom.
(9)UCL Medical School, University College London, London WC1E 6DE, United 
Kingdom.
(10)Department of Oncology, Medical Sciences Division, University of Oxford, 
Oxford OX3 7DQ, United Kingdom.

OBJECTIVE: screening is a labor-intensive component of systematic review 
involving repetitive application of inclusion and exclusion criteria on a large 
volume of studies. We aimed to validate large language models (LLMs) used to 
automate abstract screening.
MATERIALS AND METHODS: LLMs (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Llama 3 70B, 
Gemini 1.5 Pro, and Claude Sonnet 3.5) were trialed across 23 Cochrane Library 
systematic reviews to evaluate their accuracy in zero-shot binary classification 
for abstract screening. Initial evaluation on a balanced development dataset 
(n = 800) identified optimal prompting strategies, and the best performing 
LLM-prompt combinations were then validated on a comprehensive dataset of 
replicated search results (n = 119 695).
RESULTS: On the development dataset, LLMs exhibited superior performance to 
human researchers in terms of sensitivity (LLMmax = 1.000, humanmax = 0.775), 
precision (LLMmax = 0.927, humanmax = 0.911), and balanced accuracy (LLMmax = 
0.904, humanmax = 0.865). When evaluated on the comprehensive dataset, the best 
performing LLM-prompt combinations exhibited consistent sensitivity (range 
0.756-1.000) but diminished precision (range 0.004-0.096) due to class 
imbalance. In addition, 66 LLM-human and LLM-LLM ensembles exhibited perfect 
sensitivity with a maximal precision of 0.458 with the development dataset, 
decreasing to 0.1450 over the comprehensive dataset; but conferring workload 
reductions ranging between 37.55% and 99.11%.
DISCUSSION: Automated abstract screening can reduce the screening workload in 
systematic review while maintaining quality. Performance variation between 
reviews highlights the importance of domain-specific validation before 
autonomous deployment. LLM-human ensembles can achieve similar benefits while 
maintaining human oversight over all records.
CONCLUSION: LLMs may reduce the human labor cost of systematic review with 
maintained or improved accuracy, thereby increasing the efficiency and quality 
of evidence synthesis.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf050
PMCID: PMC12012331
PMID: 40119675 [Indexed for MEDLINE]

Conflict of interest statement: AASS receives funding from the National 
Institute for Health and Care Research (NIHR) Applied Research Collaboration 
Oxford and Thames Valley at Oxford Health NHS Foundation Trust. The views 
expressed are those of the authors and not necessarily those of the NHS, the 
NIHR or the Department of Health and Social Care.


136. JMIR Form Res. 2025 Mar 19;9:e54803. doi: 10.2196/54803.

Synthetic Data-Driven Approaches for Chinese Medical Abstract Sentence 
Classification: Computational Study.

Li J(#)(1)(2)(3), Wang Z(#)(1)(2)(4), Yu L(5), Liu H(6), Song H(1)(2).

Author information:
(1)Shanghai Artificial Intelligence Research Institute Co., Ltd, Shanghai, 
China.
(2)Xiangfu Laboratory, Jiaxing, China.
(3)School of Chemistry and Chemical Engineering, Shanghai Jiao Tong University, 
Shanghai, China.
(4)Inner Mongolia Academy of Science and Technology, Hohhot, China.
(5)University of California San Diego, San Diego, CA, United States.
(6)Shanghai Civil Aviation College, Shanghai, China.
(#)Contributed equally

BACKGROUND: Medical abstract sentence classification is crucial for enhancing 
medical database searches, literature reviews, and generating new abstracts. 
However, Chinese medical abstract classification research is hindered by a lack 
of suitable datasets. Given the vastness of Chinese medical literature and the 
unique value of traditional Chinese medicine, precise classification of these 
abstracts is vital for advancing global medical research.
OBJECTIVE: This study aims to address the data scarcity issue by generating a 
large volume of labeled Chinese abstract sentences without manual annotation, 
thereby creating new training datasets. Additionally, we seek to develop more 
accurate text classification algorithms to improve the precision of Chinese 
medical abstract classification.
METHODS: We developed 3 training datasets (dataset #1, dataset #2, and dataset 
#3) and a test dataset to evaluate our model. Dataset #1 contains 15,000 
abstract sentences translated from the PubMed dataset into Chinese. Datasets #2 
and #3, each with 15,000 sentences, were generated using GPT-3.5 from 40,000 
Chinese medical abstracts in the CSL database. Dataset #2 used titles and 
keywords for pseudolabeling, while dataset #3 aligned abstracts with category 
labels. The test dataset includes 87,000 sentences from 20,000 abstracts. We 
used SBERT embeddings for deeper semantic analysis and evaluated our model using 
clustering (SBERT-DocSCAN) and supervised methods (SBERT-MEC). Extensive 
ablation studies and feature analyses were conducted to validate the model's 
effectiveness and robustness.
RESULTS: Our experiments involved training both clustering and supervised models 
on the 3 datasets, followed by comprehensive evaluation using the test dataset. 
The outcomes demonstrated that our models outperformed the baseline metrics. 
Specifically, when trained on dataset #1, the SBERT-DocSCAN model registered an 
impressive accuracy and F1-score of 89.85% on the test dataset. Concurrently, 
the SBERT-MEC algorithm exhibited comparable performance with an accuracy of 
89.38% and an identical F1-score. Training on dataset #2 yielded similarly 
positive results for the SBERT-DocSCAN model, achieving an accuracy and F1-score 
of 89.83%, while the SBERT-MEC algorithm recorded an accuracy of 86.73% and an 
F1-score of 86.51%. Notably, training with dataset #3 allowed the SBERT-DocSCAN 
model to attain the best with an accuracy and F1-score of 91.30%, whereas the 
SBERT-MEC algorithm also showed robust performance, obtaining an accuracy of 
90.39% and an F1-score of 90.35%. Ablation analysis highlighted the critical 
role of integrated features and methodologies in improving classification 
efficiency.
CONCLUSIONS: Our approach addresses the challenge of limited datasets for 
Chinese medical abstract classification by generating novel datasets. The 
deployment of SBERT-DocSCAN and SBERT-MEC models significantly enhances the 
precision of classifying Chinese medical abstracts, even when using synthetic 
datasets with pseudolabels.

© Zikai Wang, Longxuan Yu, Hui Liu, Haitao Song. Originally published in JMIR 
Formative Research (https://formative.jmir.org).

DOI: 10.2196/54803
PMCID: PMC11939029
PMID: 40106267 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


137. J Am Med Inform Assoc. 2025 May 1;32(5):835-844. doi: 10.1093/jamia/ocaf042.

Optimizing the efficiency and effectiveness of data quality assurance in a 
multicenter clinical dataset.

Fu A(1)(2), Shen T(1), Roberts SB(1)(3), Liu W(1), Vaidyanathan S(1), 
Marchena-Romero KJ(1), Lam YYP(1), Shah K(1), Mak DYF(1); GEMINI Investigators; 
Razak F(1)(2)(3), Verma AA(1)(2)(3).

Collaborators: Chin S, Stern SJ, Koppula R, Joyce LF, Pellegrino N, Harris N, Ng 
V, Srivastava S, Manikan N, Wilkinson A, Gastmeier J, Kwan JC, Byaruhanga H, 
Shaji L, George S, Handsor S, Roy RA, Kim CS, Mequanint S.

Author information:
(1)Li Ka Shing Knowledge Institute, St. Michael's Hospital, Toronto, ON M5C 3G7, 
Canada.
(2)Department of Medicine, Temerty Faculty of Medicine, University of Toronto, 
Toronto, ON M5S 1A8, Canada.
(3)Institute of Health Policy, Management and Evaluation, University of Toronto, 
Toronto, ON M5T 3M6, Canada.

OBJECTIVES: Electronic health records (EHRs) data are increasingly used for 
research and analysis, but there is little empirical evidence to inform how 
automated and manual assessments can be combined to efficiently assess data 
quality in large EHR repositories.
MATERIALS AND METHODS: The GEMINI database collected data from 462 226 patient 
admissions across 32 hospitals from 2021 to 2023. We report data quality issues 
identified through semi-automated and manual data quality assessments completed 
during the data collection phase. We conducted a simulation experiment to 
evaluate the relationship between the number of records reviewed manually, the 
detection of true data errors (true positives) and the number of manual chart 
abstraction errors (false positives) that required unnecessary investigation.
RESULTS: The semi-automated data quality assessments identified 79 data quality 
issues requiring correction, of which 14 had a large impact, affecting at least 
50% of records in the data. After resolving issues identified through 
semi-automated assessments, manual validation of 2676 patient encounters at 19 
hospitals identified 4 new meaningful data errors (3 in transfusion data and 1 
in physician identifiers), distributed across 4 hospitals. There were 365 manual 
chart abstraction errors, which required investigation by data analysts to 
identify as "false positives." These errors increased linearly with the number 
of charts reviewed manually. Simulation results demonstrate that all 3 
transfusion data errors were identified with 95% sensitivity after manual review 
of 5 records, whereas 18 records were needed for the physician's table.
DISCUSSION AND CONCLUSION: The GEMINI approach represents a scalable framework 
for data quality assessment and improvement in multisite EHR research databases. 
Manual data review is important but can be minimized to optimize the trade-off 
between true and false identification of data quality errors.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf042
PMCID: PMC12012372
PMID: 40079834 [Indexed for MEDLINE]

Conflict of interest statement: A.A.V. and F.R. report being part-time employees 
of Ontario Health, as Provincial Clinical Leads for Quality Improvement in 
General Medicine.


138. JMIR Med Inform. 2025 Mar 12;13:e64682. doi: 10.2196/64682.

GPT-3.5 Turbo and GPT-4 Turbo in Title and Abstract Screening for Systematic 
Reviews.

Oami T(1), Okada Y(2)(3), Nakada TA(1).

Author information:
(1)Department of Emergency and Critical Care Medicine, Chiba University Graduate 
School of Medicine, 1-8-1 Inohana, Chuo, Chiba, 260-8677, Japan, 81 432262372.
(2)Department of Preventive Services, Kyoto University Graduate School of 
Medicine, Kyoto, Japan.
(3)Health Services and Systems Research, Duke-NUS Medical School, National 
University of Singapore, Singapore, Singapore.

This study demonstrated that while GPT-4 Turbo had superior specificity when 
compared to GPT-3.5 Turbo (0.98 vs 0.51), as well as comparable sensitivity 
(0.85 vs 0.83), GPT-3.5 Turbo processed 100 studies faster (0.9 min vs 1.6 min) 
in citation screening for systematic reviews, suggesting that GPT-4 Turbo may be 
more suitable due to its higher specificity and highlighting the potential of 
large language models in optimizing literature selection.

© Takehiko Oami, Yohei Okada, Taka-aki Nakada. Originally published in JMIR 
Medical Informatics (https://medinform.jmir.org).

DOI: 10.2196/64682
PMCID: PMC11922487
PMID: 40073422 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


139. J Med Internet Res. 2025 Mar 11;27:e67488. doi: 10.2196/67488.

Accuracy of Large Language Models for Literature Screening in Thoracic Surgery: 
Diagnostic Study.

Dai ZY(1), Wang FQ(1), Shen C(1), Ji YL(1), Li ZY(1), Wang Y(1), Pu Q(1).

Author information:
(1)Department of Thoracic Surgery, West China Hospital of Sichuan University, 
Chengdu, China.

BACKGROUND: Systematic reviews and meta-analyses rely on labor-intensive 
literature screening. While machine learning offers potential automation, its 
accuracy remains suboptimal. This raises the question of whether emerging large 
language models (LLMs) can provide a more accurate and efficient approach.
OBJECTIVE: This paper evaluates the sensitivity, specificity, and summary 
receiver operating characteristic (SROC) curve of LLM-assisted literature 
screening.
METHODS: We conducted a diagnostic study comparing the accuracy of LLM-assisted 
screening versus manual literature screening across 6 thoracic surgery 
meta-analyses. Manual screening by 2 investigators served as the reference 
standard. LLM-assisted screening was performed using ChatGPT-4o (OpenAI) and 
Claude-3.5 (Anthropic) sonnet, with discrepancies resolved by Gemini-1.5 pro 
(Google). In addition, 2 open-source, machine learning-based screening tools, 
ASReview (Utrecht University) and Abstrackr (Center for Evidence Synthesis in 
Health, Brown University School of Public Health), were also evaluated. We 
calculated sensitivity, specificity, and 95% CIs for the title and abstract, as 
well as full-text screening, generating pooled estimates and SROC curves. LLM 
prompts were revised based on a post hoc error analysis.
RESULTS: LLM-assisted full-text screening demonstrated high pooled sensitivity 
(0.87, 95% CI 0.77-0.99) and specificity (0.96, 95% CI 0.91-0.98), with the area 
under the curve (AUC) of 0.96 (95% CI 0.94-0.97). Title and abstract screening 
achieved a pooled sensitivity of 0.73 (95% CI 0.57-0.85) and specificity of 0.99 
(95% CI 0.97-0.99), with an AUC of 0.97 (95% CI 0.96-0.99). Post hoc revisions 
improved sensitivity to 0.98 (95% CI 0.74-1.00) while maintaining high 
specificity (0.98, 95% CI 0.94-0.99). In comparison, the pooled sensitivity and 
specificity of ASReview tool-assisted screening were 0.58 (95% CI 0.53-0.64) and 
0.97 (95% CI 0.91-0.99), respectively, with an AUC of 0.66 (95% CI 0.62-0.70). 
The pooled sensitivity and specificity of Abstrackr tool-assisted screening were 
0.48 (95% CI 0.35-0.62) and 0.96 (95% CI 0.88-0.99), respectively, with an AUC 
of 0.78 (95% CI 0.74-0.82). A post hoc meta-analysis revealed comparable effect 
sizes between LLM-assisted and conventional screening.
CONCLUSIONS: LLMs hold significant potential for streamlining literature 
screening in systematic reviews, reducing workload without sacrificing quality. 
Importantly, LLMs outperformed traditional machine learning-based tools 
(ASReview and Abstrackr) in both sensitivity and AUC values, suggesting that 
LLMs offer a more accurate and efficient approach to literature screening.

©Zhang-Yi Dai, Fu-Qiang Wang, Cheng Shen, Yan-Li Ji, Zhi-Yang Li, Yun Wang, 
Qiang Pu. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 11.03.2025.

DOI: 10.2196/67488
PMCID: PMC11937709
PMID: 40068152 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


140. J Med Internet Res. 2025 Mar 10;27:e59792. doi: 10.2196/59792.

Generative AI Models in Time-Varying Biomedical Data: Scoping Review.

He R(1)(2), Sarwal V(1)(2), Qiu X(3), Zhuang Y(4), Zhang L(5), Liu Y(6), Chiang 
J(2)(7).

Author information:
(1)Department of Computer Science, University of California, Los Angeles, Los 
Angeles, CA, United States.
(2)Department of Computational Medicine, University of California, Los Angeles, 
Los Angeles, CA, United States.
(3)Division of Biomedical Sciences, School of Medicine, University of California 
Riverside, Riverside, CA, United States.
(4)Department of Biostatistics, University of Michigan, Ann Arbor, MI, United 
States.
(5)Institute for Integrative Genome Biology, University of California Riverside, 
Riverside, CA, United States.
(6)Institute for Cellular and Molecular Biology, University of Texas at Austin, 
Austin, TX, United States.
(7)Department of Neurosurgery, David Geffen School of Medicine, University of 
California, Los Angeles, Los Angeles, CA, United States.

Erratum in
    J Med Internet Res. 2025 Jul 25;27:e79605. doi: 10.2196/79605.

BACKGROUND: Trajectory modeling is a long-standing challenge in the application 
of computational methods to health care. In the age of big data, traditional 
statistical and machine learning methods do not achieve satisfactory results as 
they often fail to capture the complex underlying distributions of multimodal 
health data and long-term dependencies throughout medical histories. Recent 
advances in generative artificial intelligence (AI) have provided powerful tools 
to represent complex distributions and patterns with minimal underlying 
assumptions, with major impact in fields such as finance and environmental 
sciences, prompting researchers to apply these methods for disease modeling in 
health care.
OBJECTIVE: While AI methods have proven powerful, their application in clinical 
practice remains limited due to their highly complex nature. The proliferation 
of AI algorithms also poses a significant challenge for nondevelopers to track 
and incorporate these advances into clinical research and application. In this 
paper, we introduce basic concepts in generative AI and discuss current 
algorithms and how they can be applied to health care for practitioners with 
little background in computer science.
METHODS: We surveyed peer-reviewed papers on generative AI models with specific 
applications to time-series health data. Our search included single- and 
multimodal generative AI models that operated over structured and unstructured 
data, physiological waveforms, medical imaging, and multi-omics data. We 
introduce current generative AI methods, review their applications, and discuss 
their limitations and future directions in each data modality.
RESULTS: We followed the PRISMA-ScR (Preferred Reporting Items for Systematic 
Reviews and Meta-Analyses extension for Scoping Reviews) guidelines and reviewed 
155 articles on generative AI applications to time-series health care data 
across modalities. Furthermore, we offer a systematic framework for clinicians 
to easily identify suitable AI methods for their data and task at hand.
CONCLUSIONS: We reviewed and critiqued existing applications of generative AI to 
time-series health data with the aim of bridging the gap between computational 
methods and clinical application. We also identified the shortcomings of 
existing approaches and highlighted recent advances in generative AI that 
represent promising directions for health care modeling.

©Rosemary He, Varuni Sarwal, Xinru Qiu, Yongwen Zhuang, Le Zhang, Yue Liu, 
Jeffrey Chiang. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 10.03.2025.

DOI: 10.2196/59792
PMCID: PMC11933772
PMID: 40063929 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


141. J Am Med Inform Assoc. 2025 May 1;32(5):968-970. doi: 10.1093/jamia/ocaf041.

How the National Library of Medicine should evolve in an era of artificial 
intelligence.

Lenert LA(1).

Author information:
(1)Biomedical Informatics Center, Medical University of South Carolina, 
Charleston, SC 29405, United States.

OBJECTIVES: This article describes the challenges faced by the National Library 
of Medicine with the rise of artificial intelligence (AI) and access to human 
knowledge through large language models (LLMs).
BACKGROUND AND SIGNIFICANCE: The rise of AI as a tool for the acceleration and 
falsification of science is impacting every aspect of the transformation of data 
to information, knowledge, and wisdom through the scientific processes.
APPROACH: This perspective discusses the philosophical foundations, threats, and 
opportunities of the AI revolution with a proposal for restructuring the mission 
of the National Library of Medicine (NLM), part of the National Institutes of 
Health, with a central role as the guardian of the integrity of scientific 
knowledge in an era of AI-driven science.
RESULTS: The NLM can rise to new challenges posed by AI by working from its 
foundations in theories of Information Science and embracing new roles. Three 
paths for the NLM are proposed: (1) Become an Authentication Authority For Data, 
Information, and Knowledge through Systems of Scientific Provenance; (2) Become 
An Observatory of the State of Human Health Science supporting living systematic 
reviews; and (3) Become A hub for Culturally Appropriate Bespoke Translation, 
Transformation, and Summarization for different users (patients, the public, as 
well as scientists and clinicians) using AI technologies.
DISCUSSION: Adapting the NLM to the challenges of the Internet revolution by 
developing worldwide-web-accessible resources allowed the NLM to rise to new 
heights. Bold moves are needed to adapt the Library to the AI revolution but 
offer similar prospects of more significant impacts on the advancement of 
science and human health.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf041
PMCID: PMC12012362
PMID: 40063704 [Indexed for MEDLINE]

Conflict of interest statement: None declared.


142. BMC Med Inform Decis Mak. 2025 Mar 7;25(1):117. doi: 10.1186/s12911-025-02954-4.

A systematic review of large language model (LLM) evaluations in clinical 
medicine.

Shool S(1), Adimi S(2), Saboori Amleshi R(1), Bitaraf E(1), Golpira R(2), Tara 
M(3)(4).

Author information:
(1)Center for Technology and Innovation in Cardiovascular Informatics, Rajaie 
Cardiovascular Medical and Research Center, Iran University of Medical Sciences, 
Tehran, Iran.
(2)Rajaie Cardiovascular Medical and Research Center, Iran University of Medical 
Sciences, Tehran, 1995614331, Iran.
(3)Center for Technology and Innovation in Cardiovascular Informatics, Rajaie 
Cardiovascular Medical and Research Center, Iran University of Medical Sciences, 
Tehran, Iran. smtara@gmail.com.
(4)Rajaie Cardiovascular Medical and Research Center, Iran University of Medical 
Sciences, Tehran, 1995614331, Iran. smtara@gmail.com.

BACKGROUND: Large Language Models (LLMs), advanced AI tools based on transformer 
architectures, demonstrate significant potential in clinical medicine by 
enhancing decision support, diagnostics, and medical education. However, their 
integration into clinical workflows requires rigorous evaluation to ensure 
reliability, safety, and ethical alignment.
OBJECTIVE: This systematic review examines the evaluation parameters and 
methodologies applied to LLMs in clinical medicine, highlighting their 
capabilities, limitations, and application trends.
METHODS: A comprehensive review of the literature was conducted across PubMed, 
Scopus, Web of Science, IEEE Xplore, and arXiv databases, encompassing both 
peer-reviewed and preprint studies. Studies were screened against predefined 
inclusion and exclusion criteria to identify original research evaluating LLM 
performance in medical contexts.
RESULTS: The results reveal a growing interest in leveraging LLM tools in 
clinical settings, with 761 studies meeting the inclusion criteria. While 
general-domain LLMs, particularly ChatGPT and GPT-4, dominated evaluations 
(93.55%), medical-domain LLMs accounted for only 6.45%. Accuracy emerged as the 
most commonly assessed parameter (21.78%). Despite these advancements, the 
evidence base highlights certain limitations and biases across the included 
studies, emphasizing the need for careful interpretation and robust evaluation 
frameworks.
CONCLUSIONS: The exponential growth in LLM research underscores their 
transformative potential in healthcare. However, addressing challenges such as 
ethical risks, evaluation variability, and underrepresentation of critical 
specialties will be essential. Future efforts should prioritize standardized 
frameworks to ensure safe, effective, and equitable LLM integration in clinical 
practice.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-02954-4
PMCID: PMC11889796
PMID: 40055694 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


143. J Med Internet Res. 2025 Mar 5;27:e64364. doi: 10.2196/64364.

Retrieval Augmented Therapy Suggestion for Molecular Tumor Boards: Algorithmic 
Development and Validation Study.

Berman E(1), Sundberg Malek H(2), Bitzer M(3), Malek N(3), Eickhoff C(1).

Author information:
(1)Center for Digital Health, University Hospital Tuebingen, Tuebingen, Germany.
(2)Center for Personalized Medicine, University Hospital Tuebingen, Tuebingen, 
Germany.
(3)Department of Internal Medicine I, University Hospital Tuebingen, Tuebingen, 
Germany.

BACKGROUND: Molecular tumor boards (MTBs) require intensive manual investigation 
to generate optimal treatment recommendations for patients. Large language 
models (LLMs) can catalyze MTB recommendations, decrease human error, improve 
accessibility to care, and enhance the efficiency of precision oncology.
OBJECTIVE: In this study, we aimed to investigate the efficacy of LLM-generated 
treatments for MTB patients. We specifically investigate the LLMs' ability to 
generate evidence-based treatment recommendations using PubMed references.
METHODS: We built a retrieval augmented generation pipeline using PubMed data. 
We prompted the resulting LLM to generate treatment recommendations with PubMed 
references using a test set of patients from an MTB conference at a large 
comprehensive cancer center at a tertiary care institution. Members of the MTB 
manually assessed the relevancy and correctness of the generated responses.
RESULTS: A total of 75% of the referenced articles were properly cited from 
PubMed, while 17% of the referenced articles were hallucinations, and the 
remaining were not properly cited from PubMed. Clinician-generated LLM queries 
achieved higher accuracy through clinician evaluation than automated queries, 
with clinicians labeling 25% of LLM responses as equal to their recommendations 
and 37.5% as alternative plausible treatments.
CONCLUSIONS: This study demonstrates how retrieval augmented generation-enhanced 
LLMs can be a powerful tool in accelerating MTB conferences, as LLMs are 
sometimes capable of achieving clinician-equal treatment recommendations. 
However, further investigation is required to achieve stable results with zero 
hallucinations. LLMs signify a scalable solution to the time-intensive process 
of MTB investigations. However, LLM performance demonstrates that they must be 
used with heavy clinician supervision, and cannot yet fully automate the MTB 
pipeline.

©Eliza Berman, Holly Sundberg Malek, Michael Bitzer, Nisar Malek, Carsten 
Eickhoff. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 05.03.2025.

DOI: 10.2196/64364
PMCID: PMC11923455
PMID: 40053768 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


144. JMIR Med Inform. 2025 Feb 28;13:e62758. doi: 10.2196/62758.

Current Landscape and Future Directions for Mental Health Conversational Agents 
for Youth: Scoping Review.

Park JK(1), Singh VK(2), Wisniewski P(3).

Author information:
(1)Human-Centered Computing Division, School of Computing, Clemson University, 
Clemson, SC, United States.
(2)Department of Library and Information, School of Communication and 
Information, Rutgers University, New Brunswick, NJ, United States.
(3)Department of Computer Science, School of Engineering, Vanderbilt University, 
Nashville, TN, United States.

BACKGROUND: Conversational agents (CAs; chatbots) are systems with the ability 
to interact with users using natural human dialogue. They are increasingly used 
to support interactive knowledge discovery of sensitive topics such as mental 
health topics. While much of the research on CAs for mental health has focused 
on adult populations, the insights from such research may not apply to CAs for 
youth.
OBJECTIVE: This study aimed to comprehensively evaluate the state-of-the-art 
research on mental health CAs for youth.
METHODS: Following PRISMA (Preferred Reporting Items for Systematic Reviews and 
Meta-Analyses) guidelines, we identified 39 peer-reviewed studies specific to 
mental health CAs designed for youth across 4 databases, including ProQuest, 
Scopus, Web of Science, and PubMed. We conducted a scoping review of the 
literature to evaluate the characteristics of research on mental health CAs 
designed for youth, the design and computational considerations of mental health 
CAs for youth, and the evaluation outcomes reported in the research on mental 
health CAs for youth.
RESULTS: We found that many mental health CAs (11/39, 28%) were designed as 
older peers to provide therapeutic or educational content to promote youth 
mental well-being. All CAs were designed based on expert knowledge, with a few 
that incorporated inputs from youth. The technical maturity of CAs was in its 
infancy, focusing on building prototypes with rule-based models to deliver 
prewritten content, with limited safety features to respond to imminent risk. 
Research findings suggest that while youth appreciate the 24/7 availability of 
friendly or empathetic conversation on sensitive topics with CAs, they found the 
content provided by CAs to be limited. Finally, we found that most (35/39, 90%) 
of the reviewed studies did not address the ethical aspects of mental health 
CAs, while youth were concerned about the privacy and confidentiality of their 
sensitive conversation data.
CONCLUSIONS: Our study highlights the need for researchers to continue to work 
together to align evidence-based research on mental health CAs for youth with 
lessons learned on how to best deliver these technologies to youth. Our review 
brings to light mental health CAs needing further development and evaluation. 
The new trend of large language model-based CAs can make such technologies more 
feasible. However, the privacy and safety of the systems should be prioritized. 
Although preliminary evidence shows positive trends in mental health CAs, 
long-term evaluative research with larger sample sizes and robust research 
designs is needed to validate their efficacy. More importantly, collaboration 
between youth and clinical experts is essential from the early design stages 
through to the final evaluation to develop safe, effective, and youth-centered 
mental health chatbots. Finally, best practices for risk mitigation and ethical 
development of CAs with and for youth are needed to promote their mental 
well-being.

©Jinkyung Katie Park, Vivek K Singh, Pamela Wisniewski. Originally published in 
JMIR Medical Informatics (https://medinform.jmir.org), 28.02.2025.

DOI: 10.2196/62758
PMCID: PMC11909484
PMID: 40053735 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


145. J Am Med Inform Assoc. 2025 Apr 1;32(4):616-625. doi: 10.1093/jamia/ocaf030.

Enhancing systematic literature reviews with generative artificial intelligence: 
development, applications, and performance evaluation.

Li Y(1), Datta S(2), Rastegar-Mojarad M(2), Lee K(2), Paek H(2), Glasgow J(2), 
Liston C(2), He L(2), Wang X(2), Xu Y(1).

Author information:
(1)Regeneron Pharmaceuticals, Inc., Tarrytown, NY 10591, United States.
(2)IMO Health, Inc., Rosemont, IL 60018, United States.

OBJECTIVES: We developed and validated a large language model (LLM)-assisted 
system for conducting systematic literature reviews in health technology 
assessment (HTA) submissions.
MATERIALS AND METHODS: We developed a five-module system using abstracts 
acquired from PubMed: (1) literature search query setup; (2) study protocol 
setup using population, intervention/comparison, outcome, and study type (PICOs) 
criteria; (3) LLM-assisted abstract screening; (4) LLM-assisted data extraction; 
and (5) data summarization. The system incorporates a human-in-the-loop design, 
allowing real-time PICOs criteria adjustment. This is achieved by collecting 
information on disagreements between the LLM and human reviewers regarding 
inclusion/exclusion decisions and their rationales, enabling informed PICOs 
refinement. We generated four evaluation sets including relapsed and refractory 
multiple myeloma (RRMM) and advanced melanoma to evaluate the LLM's performance 
in three key areas: (1) recommending inclusion/exclusion decisions during 
abstract screening, (2) providing valid rationales for abstract exclusion, and 
(3) extracting relevant information from included abstracts.
RESULTS: The system demonstrated relatively high performance across all 
evaluation sets. For abstract screening, it achieved an average sensitivity of 
90%, F1 score of 82, accuracy of 89%, and Cohen's κ of 0.71, indicating 
substantial agreement between human reviewers and LLM-based results. In 
identifying specific exclusion rationales, the system attained accuracies of 97% 
and 84%, and F1 scores of 98 and 89 for RRMM and advanced melanoma, 
respectively. For data extraction, the system achieved an F1 score of 93.
DISCUSSION: Results showed high sensitivity, Cohen's κ, and PABAK for abstract 
screening, and high F1 scores for data extraction. This human-in-the-loop 
AI-assisted SLR system demonstrates the potential of GPT-4's in context learning 
capabilities by eliminating the need for manually annotated training data. In 
addition, this LLM-based system offers subject matter experts greater control 
through prompt adjustment and real-time feedback, enabling iterative refinement 
of PICOs criteria based on performance metrics.
CONCLUSION: The system demonstrates potential to streamline systematic 
literature reviews, potentially reducing time, cost, and human errors while 
enhancing evidence generation for HTA submissions.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf030
PMCID: PMC12005633
PMID: 40036547 [Indexed for MEDLINE]

Conflict of interest statement: Y.L. and Y.X. are currently employees of 
Regeneron Pharmaceuticals, Inc. S.D, M.R.-M, K.L., H.P., J.G., C.L., L.H., and 
X.W. are currently employees of Intelligence Medical Objective, Inc. The 
affiliations played no role in the design, execution, interpretation, or 
reporting of this research. The authors affirm that the manuscript is an honest, 
accurate, and transparent account of the study being reported. All views and 
opinions expressed in this manuscript are solely those of the authors and do not 
necessarily represent the views of the companies.


146. Digit Health. 2025 Mar 2;11:20552076251324444. doi: 10.1177/20552076251324444. 
eCollection 2025 Jan-Dec.

Application of large language models in healthcare: A bibliometric analysis.

Zhang L(1)(2), Zhao Q(3), Zhang D(1)(2), Song M(1)(2), Zhang Y(4), Wang X(1)(2).

Author information:
(1)Department of the Third Pulmonary Disease, Shenzhen Third People's Hospital, 
Shenzhen, Guangdong Province, China.
(2)Shenzhen Clinical Research Center for Tuberculosis, Shenzhen, Guangdong 
Province, China.
(3)Acacia Lab for Implementation Science, School of Public Health Management, 
Southern Medical University, Guangzhou, Guangdong, China.
(4)School of Humanities Changzhou Vocational Institute of Textile and Garment 
Changzhou, China.

OBJECTIVE: The objective is to provide an overview of the application of large 
language models (LLMs) in healthcare by employing a bibliometric analysis 
methodology.
METHOD: We performed a comprehensive search for peer-reviewed English-language 
articles using PubMed and Web of Science. The selected articles were 
subsequently clustered and analyzed textually, with a focus on lexical 
co-occurrences, country-level and inter-author collaborations, and other 
relevant factors. This textual analysis produced high-level concept maps that 
illustrate specific terms and their interconnections.
FINDINGS: Our final sample comprised 371 English-language journal articles. The 
study revealed a sharp rise in the number of publications related to the 
application of LLMs in healthcare. However, the development is geographically 
imbalanced, with a higher concentration of articles originating from developed 
countries like the United States, Italy, and Germany, which also exhibit strong 
inter-country collaboration. LLMs are applied across various specialties, with 
researchers investigating their use in medical education, diagnosis, treatment, 
administrative reporting, and enhancing doctor-patient communication. 
Nonetheless, significant concerns persist regarding the risks and ethical 
implications of LLMs, including the potential for gender and racial bias, as 
well as the lack of transparency in the training datasets, which can lead to 
inaccurate or misleading responses.
CONCLUSION: While the application of LLMs in healthcare is promising, the 
widespread adoption of LLMs in practice requires further improvements in their 
standardization and accuracy. It is critical to establish clear accountability 
guidelines, develop a robust regulatory framework, and ensure that training 
datasets are based on evidence-based sources to minimize risk and ensure ethical 
and reliable use.

© The Author(s) 2025.

DOI: 10.1177/20552076251324444
PMCID: PMC11873863
PMID: 40035041

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


147. J Am Med Inform Assoc. 2025 Apr 1;32(4):761-772. doi: 10.1093/jamia/ocaf029.

Deciphering genomic codes using advanced natural language processing techniques: 
a scoping review.

Cheng S(1), Wei Y(1), Zhou Y(1), Xu Z(1), Wright DN(2), Liu J(3), Peng Y(1).

Author information:
(1)Department of Population Health Sciences, Weill Cornell Medicine, New York, 
NY 10065, United States.
(2)Samuel J. Wood Library & C.V. Starr Biomedical Information Center, Weill 
Cornell Medicine, New York, NY 10065, United States.
(3)School of Public Health, Virginia Commonwealth University, Richmond, VA 
23219, United States.

Update of
    ArXiv. 2024 Nov 25:arXiv:2411.16084v1.

OBJECTIVES: The vast and complex nature of human genomic sequencing data 
presents challenges for effective analysis. This review aims to investigate the 
application of natural language processing (NLP) techniques, particularly large 
language models (LLMs) and transformer architectures, in deciphering genomic 
codes, focusing on tokenization, transformer models, and regulatory annotation 
prediction. The goal of this review is to assess data and model accessibility in 
the most recent literature, gaining a better understanding of the existing 
capabilities and constraints of these tools in processing genomic sequencing 
data.
MATERIALS AND METHODS: Following Preferred Reporting Items for Systematic 
Reviews and Meta-Analyses (PRISMA) guidelines, our scoping review was conducted 
across PubMed, Medline, Scopus, Web of Science, Embase, and ACM Digital Library. 
Studies were included if they focused on NLP methodologies applied to genomic 
sequencing data analysis, without restrictions on publication date or article 
type.
RESULTS: A total of 26 studies published between 2021 and April 2024 were 
selected for review. The review highlights that tokenization and transformer 
models enhance the processing and understanding of genomic data, with 
applications in predicting regulatory annotations like transcription-factor 
binding sites and chromatin accessibility.
DISCUSSION: The application of NLP and LLMs to genomic sequencing data 
interpretation is a promising field that can help streamline the processing of 
large-scale genomic data while also providing a better understanding of its 
complex structures. It has the potential to drive advancements in personalized 
medicine by offering more efficient and scalable solutions for genomic analysis. 
Further research is also needed to discuss and overcome current limitations, 
enhancing model transparency and applicability.
CONCLUSION: This review highlights the growing role of NLP, particularly LLMs, 
in genomic sequencing data analysis. While these models improve data processing 
and regulatory annotation prediction, challenges remain in accessibility and 
interpretability. Further research is needed to refine their application in 
genomics.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf029
PMCID: PMC12005631
PMID: 39998912 [Indexed for MEDLINE]

Conflict of interest statement: None declare.


148. PLoS One. 2025 Feb 21;20(2):e0316989. doi: 10.1371/journal.pone.0316989. 
eCollection 2025.

A systematic review of automated hyperpartisan news detection.

Maggini MJ(1), Bassi D(1), Piot P(2), Dias G(3), Otero PG(1).

Author information:
(1)Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS), 
Universidade de Santiago de Compostela, Galicia, Spain.
(2)IRLab, CITIC Research Centre, Universidade da Coruña, A Coruña, Galiza, 
Spain.
(3)Université Caen Normandie, ENSICAEN, CNRS, Normandie Univ, GREYC UMR6072, 
F-14000 Caen, France.

Hyperpartisan news consists of articles with strong biases that support specific 
political parties. The spread of such news increases polarization among readers, 
which threatens social unity and democratic stability. Automated tools can help 
identify hyperpartisan news in the daily flood of articles, offering a way to 
tackle these problems. With recent advances in machine learning and deep 
learning, there are now more methods available to address this issue. This 
literature review collects and organizes the different methods used in previous 
studies on hyperpartisan news detection. Using the PRISMA methodology, we 
reviewed and systematized approaches and datasets from 81 articles published 
from January 2015 to 2024. Our analysis includes several steps: differentiating 
hyperpartisan news detection from similar tasks, identifying text sources, 
labeling methods, and evaluating models. We found some key gaps: there is no 
clear definition of hyperpartisanship in Computer Science, and most datasets are 
in English, highlighting the need for more datasets in minority languages. 
Moreover, the tendency is that deep learning models perform better than 
traditional machine learning, but Large Language Models' (LLMs) capacities in 
this domain have been limitedly studied. This paper is the first to 
systematically review hyperpartisan news detection, laying a solid groundwork 
for future research.

Copyright: © 2025 Maggini et al. This is an open access article distributed 
under the terms of the CreativeCommonsAttributionLicense, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0316989
PMCID: PMC11845023
PMID: 39982955

Conflict of interest statement: The authors have declared that no competing 
interests exist.


149. J Med Syst. 2025 Feb 18;49(1):28. doi: 10.1007/s10916-025-02157-4.

Artificial Intelligence (AI) - Powered Documentation Systems in Healthcare: A 
Systematic Review.

Bracken A(1), Reilly C(2), Feeley A(3), Sheehan E(4), Merghani K(5), Feeley 
I(5).

Author information:
(1)Royal College of Surgeons in Ireland (RCSI), 123 Stephen's Green, Dublin 2, 
Ireland. Aislingbracken24@rcsi.com.
(2)University College Dublin (UCD), Belfield, Dublin 4, Ireland.
(3)Royal College of Surgeons in Ireland (RCSI), 123 Stephen's Green, Dublin 2, 
Ireland.
(4)School of Medicine, University of Limerick (UL), Castletroy, Limerick, 
Ireland.
(5)Midlands Regional Hospital Tullamore, Arden Rd., Tullamore, Offaly, Ireland.

Artificial Intelligence (AI) driven documentation systems are positioned to 
enhance documentation efficiency and reduce documentation burden in the 
healthcare setting. The administrative burden associated with clinical 
documentation has been identified as a major contributor to health care 
professional (HCP) burnout. The current systematic review aims to evaluate the 
efficiency, quality, and stakeholder opinion regarding the use of AI-driven 
documentation systems. Using the Preferred Reporting Items for Systematic 
Reviews and Meta-Analyses (PRISMA) guidelines a comprehensive search was 
conducted across PubMed, Embase and Cochrane library. Two independent reviewers 
applied inclusion and exclusion criteria to identify eligible studies. Details 
of AI technology, document type, document quality and stakeholder experience 
were extracted. The review included 11 studies. All included studies utilised 
Chat generated pretrained transformer (Chat GPT, Open AI, CA, USA) or an ambient 
AI technology. Both forms of AI demonstrated significant potential to improve 
documentation efficiency. Despite efficiency gains, the quality of AI-generated 
documentation varied across studies. The heterogeneity of methods utilised to 
assess document quality influenced interpretation of results. HCP opinion was 
generally positive, users highlighted ease of use and reduced task load as 
primary benefits. However, HCPs also expressed concerns about the reliability 
and validity of AI-generated documentation. Chat GPT and ambient AI show promise 
in enhancing the efficiency and quality of clinical documentation. While the 
efficiency benefits are clear, the challenges associated with accuracy and 
consistency need to be addressed. HCP experiences indicate a cautious optimism 
towards AI integration, however reliability will depend on continued refinement 
and validation of the technology.

© 2025. The Author(s).

DOI: 10.1007/s10916-025-02157-4
PMCID: PMC11835907
PMID: 39966286 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics Approval and Consent to 
Participate: Not applicable. Competing Interests: The authors declare no 
competing interests.


150. Front Digit Health. 2025 Feb 3;7:1482712. doi: 10.3389/fdgth.2025.1482712. 
eCollection 2025.

Comparative analysis of ChatGPT and Gemini (Bard) in medical inquiry: a scoping 
review.

Fattah FH(1)(2), Salih AM(1)(2), Salih AM(1)(3), Asaad SK(1)(2), Ghafour AK(1), 
Bapir R(1)(4)(5), Abdalla BA(1)(5), Othman S(5), Ahmed SM(1)(5), Hasan SJ(1), 
Mahmood YM(1), Kakamad FH(1)(2)(5).

Author information:
(1)Scientific Affairs Department, Smart Health Tower, Sulaymaniyah, Iraq.
(2)College of Medicine, University of Sulaimani, Sulaymaniyah, Iraq.
(3)Civil Engineering Department, College of Engineering, University of 
Sulaimani, Sulaymaniyah, Iraq.
(4)Department of Urology, Sulaimani Surgical Teaching Hospital, Sulaymaniyah, 
Iraq.
(5)Kscien Organization for Scientific Research (Middle East Office), 
Sulaymaniyah, Iraq.

INTRODUCTION: Artificial intelligence and machine learning are popular 
interconnected technologies. AI chatbots like ChatGPT and Gemini show 
considerable promise in medical inquiries. This scoping review aims to assess 
the accuracy and response length (in characters) of ChatGPT and Gemini in 
medical applications.
METHODS: The eligible databases were searched to find studies published in 
English from January 1 to October 20, 2023. The inclusion criteria consisted of 
studies that focused on using AI in medicine and assessed outcomes based on the 
accuracy and character count (length) of ChatGPT and Gemini. Data collected from 
the studies included the first author's name, the country where the study was 
conducted, the type of study design, publication year, sample size, medical 
speciality, and the accuracy and response length.
RESULTS: The initial search identified 64 papers, with 11 meeting the inclusion 
criteria, involving 1,177 samples. ChatGPT showed higher accuracy in radiology 
(87.43% vs. Gemini's 71%) and shorter responses (907 vs. 1,428 characters). 
Similar trends were noted in other specialties. However, Gemini outperformed 
ChatGPT in emergency scenarios (87% vs. 77%) and in renal diets with low 
potassium and high phosphorus (79% vs. 60% and 100% vs. 77%). Statistical 
analysis confirms that ChatGPT has greater accuracy and shorter responses than 
Gemini in medical studies, with a p-value of <.001 for both metrics.
CONCLUSION: This Scoping review suggests that ChatGPT may demonstrate higher 
accuracy and provide shorter responses than Gemini in medical studies.

© 2025 Fattah, Salih, Salih, Asaad, Ghafour, Bapir, Abdalla, Othman, Ahmed, 
Hasan, Mahmood and Kakamad.

DOI: 10.3389/fdgth.2025.1482712
PMCID: PMC11830737
PMID: 39963119

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


151. J Am Med Inform Assoc. 2025 Apr 1;32(4):638-647. doi: 10.1093/jamia/ocae325.

Collaborative large language models for automated data extraction in living 
systematic reviews.

Khan MA(1), Ayub U(1), Naqvi SAA(1), Khakwani KZR(2), Sipra ZBR(3), Raina A(4), 
Zhou S(1), He H(5), Saeidi A(1)(6), Hasan B(7), Rumble RB(8), Bitterman DS(9), 
Warner JL(10)(11)(12)(13), Zou J(6), Tevaarwerk AJ(14), Leventakos K(7), Kehl 
KL(15), Palmer JM(1), Murad MH(7), Baral C(6), Riaz IB(1)(16).

Author information:
(1)Department of Medicine, Mayo Clinic, Phoenix, AZ, 85054, United States.
(2)Department of Medicine, University of Arizona, Tucson, AZ, 85721, United 
States.
(3)Department of Medicine and Surgery, Rashid Latif Medical College, Lahore, 
Punjab, 54000, Pakistan.
(4)Department of Medicine, Canyon Vista Hospital, Sierra Vista, AZ, 85635, 
United States.
(5)Department of Biomedical Informatics and Data Science, Yale University, New 
Haven, CT, 06520, United States.
(6)Department of Computing and Augmented Intelligence, Arizona State University, 
Tempe, AZ, 85287, United States.
(7)Department of Medicine, Mayo Clinic, Rochester, MN, 55905, United States.
(8)American Society of Clinical Oncology, Alexandria, VA, 22314, United States.
(9)Department of Radiation Oncology, Dana-Farber Cancer Institute, Boston, MA, 
02215, United States.
(10)Department of Medicine, Brown University, Providence, RI, 02912, United 
States.
(11)Department of Biostatistics, Brown University, Providence, RI, 02912, United 
States.
(12)Department of Medicine, Rhode Island Hospital, Providence, RI, 02903, United 
States.
(13)Center for Clinical Cancer Informatics and Data Science, Legorreta Cancer 
Center, Brown University, Providence, RI, 02912, United States.
(14)Department of Oncology, Mayo Clinic, Rochester, MN, 55905, United States.
(15)Department of Medicine, Dana-Farber Cancer Institute, Boston, MA, 02215, 
United States.
(16)Department of Artificial Intelligence and Informatics, Mayo Clinic, 
Rochester, MN, 55905, United States.

Update of
    medRxiv. 2024 Sep 23:2024.09.20.24314108. doi: 10.1101/2024.09.20.24314108.

OBJECTIVE: Data extraction from the published literature is the most laborious 
step in conducting living systematic reviews (LSRs). We aim to build a 
generalizable, automated data extraction workflow leveraging large language 
models (LLMs) that mimics the real-world 2-reviewer process.
MATERIALS AND METHODS: A dataset of 10 trials (22 publications) from a published 
LSR was used, focusing on 23 variables related to trial, population, and 
outcomes data. The dataset was split into prompt development (n = 5) and 
held-out test sets (n = 17). GPT-4-turbo and Claude-3-Opus were used for data 
extraction. Responses from the 2 LLMs were considered concordant if they were 
the same for a given variable. The discordant responses from each LLM were 
provided to the other LLM for cross-critique. Accuracy, ie, the total number of 
correct responses divided by the total number of responses, was computed to 
assess performance.
RESULTS: In the prompt development set, 110 (96%) responses were concordant, 
achieving an accuracy of 0.99 against the gold standard. In the test set, 342 
(87%) responses were concordant. The accuracy of the concordant responses was 
0.94. The accuracy of the discordant responses was 0.41 for GPT-4-turbo and 0.50 
for Claude-3-Opus. Of the 49 discordant responses, 25 (51%) became concordant 
after cross-critique, increasing accuracy to 0.76.
DISCUSSION: Concordant responses by the LLMs are likely to be accurate. In 
instances of discordant responses, cross-critique can further increase the 
accuracy.
CONCLUSION: Large language models, when simulated in a collaborative, 2-reviewer 
workflow, can extract data with reasonable performance, enabling truly "living" 
systematic reviews.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocae325
PMCID: PMC12005628
PMID: 39836495 [Indexed for MEDLINE]

Conflict of interest statement: I.B.R., M.A.K., U.A., S.A.A.N., K.Z.R.K., 
Z.B.R.S., A.R., S.Z., H.H., A.H., B.H., R.B.R., J.Z., K.L.K., J.M.P., M.H.M., 
and C.B. do not have any relevant competing interests to disclose. D.S.B.: 
Editorial, unrelated to the submitted work: Associate Editor of Radiation 
Oncology, HemOnc.org (no financial compensation); Advisory and consulting, 
unrelated to the submitted work: MercurialAI. J.L.W.: Reports funding from AACR, 
NIH, Brown Physicians Incorporated, unrelated to the submitted work; consulting 
with Wested and The Lewin Group, unrelated to the submitted work; ownership in 
HemOnc.org LLC, unrelated to the submitted work. A.J.T.: Family member at Epic 
Systems, unrelated to the submitted work. K.L.: Reports consulting activities 
(honoraria to institution) with Amgen, AstraZeneca Interdisciplinary 
Corporation, Boehringer Ingelheim Pharmaceuticals, Janssen Biotech, Novartis, 
unrelated to the submitted work; advisory boards (honoraria to institution) with 
AstraZeneca, Janssen, Jazz Pharmaceuticals, Mirati Therapeutics, Regeneron, 
Takeda, and Targeted Oncology, unrelated to the submitted work; CME activities 
(honoraria to institution) with OncLive and MJH Life Sciences, MD Outlook and 
Targeted Oncology, unrelated to the submitted work; Research support (to 
institution) from AstraZeneca and Mirati Therapeutics, unrelated to the 
submitted work.


152. J Am Med Inform Assoc. 2025 Apr 1;32(4):605-615. doi: 10.1093/jamia/ocaf008.

Improving large language model applications in biomedicine with 
retrieval-augmented generation: a systematic review, meta-analysis, and clinical 
development guidelines.

Liu S(1)(2), McCoy AB(1), Wright A(1)(3).

Author information:
(1)Department of Biomedical Informatics, Vanderbilt University Medical Center, 
Nashville, TN 37212, United States.
(2)Department of Computer Science, Vanderbilt University, Nashville, TN 37212, 
United States.
(3)Department of Medicine, Vanderbilt University Medical Center, Nashville, TN 
37212, United States.

OBJECTIVE: The objectives of this study are to synthesize findings from recent 
research of retrieval-augmented generation (RAG) and large language models 
(LLMs) in biomedicine and provide clinical development guidelines to improve 
effectiveness.
MATERIALS AND METHODS: We conducted a systematic literature review and a 
meta-analysis. The report was created in adherence to the Preferred Reporting 
Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were 
performed in 3 databases (PubMed, Embase, PsycINFO) using terms related to 
"retrieval augmented generation" and "large language model," for articles 
published in 2023 and 2024. We selected studies that compared baseline LLM 
performance with RAG performance. We developed a random-effect meta-analysis 
model, using odds ratio as the effect size.
RESULTS: Among 335 studies, 20 were included in this literature review. The 
pooled effect size was 1.35, with a 95% confidence interval of 1.19-1.53, 
indicating a statistically significant effect (P = .001). We reported clinical 
tasks, baseline LLMs, retrieval sources and strategies, as well as evaluation 
methods.
DISCUSSION: Building on our literature review, we developed Guidelines for 
Unified Implementation and Development of Enhanced LLM Applications with RAG in 
Clinical Settings to inform clinical applications using RAG.
CONCLUSION: Overall, RAG implementation showed a 1.35 odds ratio increase in 
performance compared to baseline LLMs. Future research should focus on (1) 
system-level enhancement: the combination of RAG and agent, (2) knowledge-level 
enhancement: deep integration of knowledge into LLM, and (3) integration-level 
enhancement: integrating RAG systems within electronic health records.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf008
PMCID: PMC12005634
PMID: 39812777 [Indexed for MEDLINE]

Conflict of interest statement: The authors do not have conflicts of interest 
related to this study.


153. J Am Med Inform Assoc. 2025 Mar 1;32(3):526-534. doi: 10.1093/jamia/ocae314.

Utility of word embeddings from large language models in medical diagnosis.

Yazdani S(1), Henry RC(2), Byrne A(3), Henry IC(4).

Author information:
(1)Department of Pediatrics, David Geffen School of Medicine, University of 
California Los Angeles, Los Angeles, CA 90095, United States.
(2)Department of Civil Engineering, University of Southern California, Los 
Angeles, CA 90089, United States.
(3)San Francisco, CA 94129, United States.
(4)Kennewick, WA 99338, United States.

OBJECTIVE: This study evaluates the utility of word embeddings, generated by 
large language models (LLMs), for medical diagnosis by comparing the semantic 
proximity of symptoms to their eponymic disease embedding ("eponymic condition") 
and the mean of all symptom embeddings associated with a disease ("ensemble 
mean").
MATERIALS AND METHODS: Symptom data for 5 diagnostically challenging pediatric 
diseases-CHARGE syndrome, Cowden disease, POEMS syndrome, Rheumatic fever, and 
Tuberous sclerosis-were collected from PubMed. Using the Ada-002 embedding 
model, disease names and symptoms were translated into vector representations in 
a high-dimensional space. Euclidean and Chebyshev distance metrics were used to 
classify symptoms based on their proximity to both the eponymic condition and 
the ensemble mean of the condition's symptoms.
RESULTS: The ensemble mean approach showed significantly higher classification 
accuracy, correctly classifying between 80% (Cowden disease) to 100% (Tuberous 
sclerosis) of the sample disease symptoms using the Euclidean distance metric. 
In contrast, the eponymic condition approach using Euclidian distance metric and 
Chebyshev distances, in general, showed poor symptom classification performance, 
with erratic results (0%-100% accuracy), largely ranging between 0% and 3% 
accuracy.
DISCUSSION: The ensemble mean captures a disease's collective symptom profile, 
providing a more nuanced representation than the disease name alone. However, 
some misclassifications were due to superficial semantic similarities, 
highlighting the need for LLM models trained on medical corpora.
CONCLUSION: The ensemble mean of symptom embeddings improves classification 
accuracy over the eponymic condition approach. Future efforts should focus on 
medical-specific training of LLMs to enhance their diagnostic accuracy and 
clinical utility.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocae314
PMCID: PMC11833464
PMID: 39786898 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
declare.


154. IEEE J Biomed Health Inform. 2025 Sep;29(9):6143-6156. doi: 
10.1109/JBHI.2024.3483816.

Benchmarking Large Language Models in Evidence-Based Medicine.

Li J, Deng Y, Sun Q, Zhu J, Tian Y, Li J, Zhu T.

Evidence-based medicine (EBM) represents a paradigm of providing patient care 
grounded in the most current and rigorously evaluated research. Recent advances 
in large language models (LLMs) offer a potential solution to transform EBM by 
automating labor-intensive tasks and thereby improving the efficiency of 
clinical decision-making. This study explores integrating LLMs into the key 
stages in EBM, evaluating their ability across evidence retrieval (PICO 
extraction, biomedical question answering), synthesis (summarizing randomized 
controlled trials), and dissemination (medical text simplification). We 
conducted a comparative analysis of seven LLMs, including both proprietary and 
open-source models, as well as those fine-tuned on medical corpora. 
Specifically, we benchmarked the performance of various LLMs on each EBM task 
under zero-shot settings as baselines, and employed prompting techniques, 
including in-context learning, chain-of-thought reasoning, and knowledge-guided 
prompting to enhance their capabilities. Our extensive experiments revealed the 
strengths of LLMs, such as remarkable understanding capabilities even in 
zero-shot settings, strong summarization skills, and effective knowledge 
transfer via prompting. Promoting strategies such as knowledge-guided prompting 
proved highly effective (e.g., improving the performance of GPT-4 by 13.10% over 
zero-shot in PICO extraction). However, the experiments also showed limitations, 
with LLM performance falling well below state-of-the-art baselines like 
PubMedBERT in handling named entity recognition tasks. Moreover, human 
evaluation revealed persisting challenges with factual inconsistencies and 
domain inaccuracies, underscoring the need for rigorous quality control before 
clinical application. This study provides insights into enhancing EBM using LLMs 
while highlighting critical areas for further research.

DOI: 10.1109/JBHI.2024.3483816
PMID: 39437276 [Indexed for MEDLINE]