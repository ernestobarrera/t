1. JAMA. 2025 Jan 28;333(4):319-328. doi: 10.1001/jama.2024.21700.

Testing and Evaluation of Health Care Applications of Large Language Models: A 
Systematic Review.

Bedi S(1), Liu Y(2), Orr-Ewing L(2), Dash D(2)(3), Koyejo S(4), Callahan A(3), 
Fries JA(3), Wornow M(3), Swaminathan A(3), Lehmann LS(5), Hong HJ(6), Kashyap 
M(7), Chaurasia AR(3), Shah NR(2), Singh K(8), Tazbaz T(9), Milstein A(2), 
Pfeffer MA(10), Shah NH(2)(3).

Author information:
(1)Department of Biomedical Data Science, Stanford School of Medicine, Stanford, 
California.
(2)Clinical Excellence Research Center, Stanford University, Stanford, 
California.
(3)Center for Biomedical Informatics Research, Stanford University, Stanford, 
California.
(4)Department of Computer Science, Stanford University, Stanford, California.
(5)Department of Medicine, Harvard Medical School, Boston, Massachusetts.
(6)Department of Anesthesiology, Stanford University, Stanford, California.
(7)Stanford University School of Medicine, Stanford, California.
(8)Digital Health Innovation, University of California San Diego Health, San 
Diego.
(9)Digital Health Center of Excellence, US Food and Drug Administration, 
Washington, DC.
(10)Department of Medicine, Stanford University School of Medicine, Stanford, 
California.

Comment in
    doi: 10.1001/jama.2024.21451.
    doi: 10.1001/jama.2024.21772.

IMPORTANCE: Large language models (LLMs) can assist in various health care 
activities, but current evaluation approaches may not adequately identify the 
most useful application areas.
OBJECTIVE: To summarize existing evaluations of LLMs in health care in terms of 
5 components: (1) evaluation data type, (2) health care task, (3) natural 
language processing (NLP) and natural language understanding (NLU) tasks, (4) 
dimension of evaluation, and (5) medical specialty.
DATA SOURCES: A systematic search of PubMed and Web of Science was performed for 
studies published between January 1, 2022, and February 19, 2024.
STUDY SELECTION: Studies evaluating 1 or more LLMs in health care.
DATA EXTRACTION AND SYNTHESIS: Three independent reviewers categorized studies 
via keyword searches based on the data used, the health care tasks, the NLP and 
NLU tasks, the dimensions of evaluation, and the medical specialty.
RESULTS: Of 519 studies reviewed, published between January 1, 2022, and 
February 19, 2024, only 5% used real patient care data for LLM evaluation. The 
most common health care tasks were assessing medical knowledge such as answering 
medical licensing examination questions (44.5%) and making diagnoses (19.5%). 
Administrative tasks such as assigning billing codes (0.2%) and writing 
prescriptions (0.2%) were less studied. For NLP and NLU tasks, most studies 
focused on question answering (84.2%), while tasks such as summarization (8.9%) 
and conversational dialogue (3.3%) were infrequent. Almost all studies (95.4%) 
used accuracy as the primary dimension of evaluation; fairness, bias, and 
toxicity (15.8%), deployment considerations (4.6%), and calibration and 
uncertainty (1.2%) were infrequently measured. Finally, in terms of medical 
specialty area, most studies were in generic health care applications (25.6%), 
internal medicine (16.4%), surgery (11.4%), and ophthalmology (6.9%), with 
nuclear medicine (0.6%), physical medicine (0.4%), and medical genetics (0.2%) 
being the least represented.
CONCLUSIONS AND RELEVANCE: Existing evaluations of LLMs mostly focus on accuracy 
of question answering for medical examinations, without consideration of real 
patient care data. Dimensions such as fairness, bias, and toxicity and 
deployment considerations received limited attention. Future evaluations should 
adopt standardized applications and metrics, use clinical data, and broaden 
focus to include a wider range of tasks and specialties.

DOI: 10.1001/jama.2024.21700
PMCID: PMC11480901
PMID: 39405325 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest Disclosures: Dr Callahan 
reported receiving consultant fees from Atropos Health LLC outside the submitted 
work. Dr Lehmann reported being formerly employed by Google outside the 
submitted work. Dr N. R. Shah reported being co-founder of start-up company for 
AI in health care Qualified Health PBC outside the submitted work. Dr Singh 
reported receiving grants from the National Institute of Diabetes and Digestive 
and Kidney Diseases for their institution, consulting fees from Flatiron Health, 
and grants from Blue Cross Blue Shield of Michigan for their institution outside 
the submitted work. Dr Milstein reported honoraria for meeting participation 
from the Peterson Center of Healthcare, funded by a charitable foundation, 
having stock/options from Emsana Health, Amino Health, FNF Advisors, JRSL LLC, 
Embold, EZPT/Somatic Health, and Prealize outside the submitted work; and being 
a member of the Leapfrog Group Board Intermountain Healthcare Board. Dr N. H. 
Shah reported being a co-founder of Prealize Health (a predictive analytics 
company) and Atropos Health (an on-demand evidence generation company); 
receiving funding from the Gordon and Betty Moore Foundation for developing 
virtual model deployments; and being a member of the board of directors of the 
Coalition for Healthcare AI, a consensus-building organization providing 
guidelines for the responsible use of artificial intelligence in health care. No 
other disclosures were reported.


2. BMC Med Inform Decis Mak. 2025 Mar 7;25(1):117. doi: 10.1186/s12911-025-02954-4.

A systematic review of large language model (LLM) evaluations in clinical 
medicine.

Shool S(1), Adimi S(2), Saboori Amleshi R(1), Bitaraf E(1), Golpira R(2), Tara 
M(3)(4).

Author information:
(1)Center for Technology and Innovation in Cardiovascular Informatics, Rajaie 
Cardiovascular Medical and Research Center, Iran University of Medical Sciences, 
Tehran, Iran.
(2)Rajaie Cardiovascular Medical and Research Center, Iran University of Medical 
Sciences, Tehran, 1995614331, Iran.
(3)Center for Technology and Innovation in Cardiovascular Informatics, Rajaie 
Cardiovascular Medical and Research Center, Iran University of Medical Sciences, 
Tehran, Iran. smtara@gmail.com.
(4)Rajaie Cardiovascular Medical and Research Center, Iran University of Medical 
Sciences, Tehran, 1995614331, Iran. smtara@gmail.com.

BACKGROUND: Large Language Models (LLMs), advanced AI tools based on transformer 
architectures, demonstrate significant potential in clinical medicine by 
enhancing decision support, diagnostics, and medical education. However, their 
integration into clinical workflows requires rigorous evaluation to ensure 
reliability, safety, and ethical alignment.
OBJECTIVE: This systematic review examines the evaluation parameters and 
methodologies applied to LLMs in clinical medicine, highlighting their 
capabilities, limitations, and application trends.
METHODS: A comprehensive review of the literature was conducted across PubMed, 
Scopus, Web of Science, IEEE Xplore, and arXiv databases, encompassing both 
peer-reviewed and preprint studies. Studies were screened against predefined 
inclusion and exclusion criteria to identify original research evaluating LLM 
performance in medical contexts.
RESULTS: The results reveal a growing interest in leveraging LLM tools in 
clinical settings, with 761 studies meeting the inclusion criteria. While 
general-domain LLMs, particularly ChatGPT and GPT-4, dominated evaluations 
(93.55%), medical-domain LLMs accounted for only 6.45%. Accuracy emerged as the 
most commonly assessed parameter (21.78%). Despite these advancements, the 
evidence base highlights certain limitations and biases across the included 
studies, emphasizing the need for careful interpretation and robust evaluation 
frameworks.
CONCLUSIONS: The exponential growth in LLM research underscores their 
transformative potential in healthcare. However, addressing challenges such as 
ethical risks, evaluation variability, and underrepresentation of critical 
specialties will be essential. Future efforts should prioritize standardized 
frameworks to ensure safe, effective, and equitable LLM integration in clinical 
practice.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-02954-4
PMCID: PMC11889796
PMID: 40055694 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


3. Nat Med. 2025 Aug;31(8):2546-2549. doi: 10.1038/s41591-025-03727-2. Epub 2025 
Apr 23.

Benchmark evaluation of DeepSeek large language models in clinical 
decision-making.

Sandmann S(1), Hegselmann S(2), Fujarski M(1), Bickmann L(1), Wild B(2), Eils 
R(3)(4), Varghese J(5).

Author information:
(1)Institute of Medical Informatics, University of Münster, Münster, Germany.
(2)Center for Digital Health, Berlin Institute of Health, Charité - University 
Medicine Berlin, Berlin, Germany.
(3)Center for Digital Health, Berlin Institute of Health, Charité - University 
Medicine Berlin, Berlin, Germany. roland_eils@fudan.edu.cn.
(4)Intelligent Medicine Institute, Fudan University, Shanghai, China. 
roland_eils@fudan.edu.cn.
(5)Institute of Medical Data Science, Otto-von-Guericke University, Magdeburg, 
Germany.

Large language models (LLMs) are increasingly transforming medical applications. 
However, proprietary models such as GPT-4o face significant barriers to clinical 
adoption because they cannot be deployed on site within healthcare institutions, 
making them noncompliant with stringent privacy regulations. Recent advancements 
in open-source LLMs such as DeepSeek models offer a promising alternative 
because they allow efficient fine-tuning on local data in hospitals with 
advanced information technology infrastructure. Here, to demonstrate the 
clinical utility of DeepSeek-V3 and DeepSeek-R1, we benchmarked their 
performance on clinical decision support tasks against proprietary LLMs, 
including GPT-4o and Gemini-2.0 Flash Thinking Experimental. Using 125 patient 
cases with sufficient statistical power, covering a broad range of frequent and 
rare diseases, we found that DeepSeek models perform equally well and in some 
cases better than proprietary LLMs. Our study demonstrates that open-source LLMs 
can provide a scalable pathway for secure model training enabling real-world 
medical applications in accordance with data privacy and healthcare regulations.

© 2025. The Author(s).

DOI: 10.1038/s41591-025-03727-2
PMCID: PMC12353792
PMID: 40267970 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


4. Arch Pathol Lab Med. 2025 Apr 1;149(4):298-318. doi: 10.5858/arpa.2024-0215-RA.

Generative Artificial Intelligence in Anatomic Pathology.

Brodsky V(1), Ullah E(2), Bychkov A(3)(4), Song AH(5), Walk EE(6), Louis P(7), 
Rasool G(8)(9)(10), Singh RS(11), Mahmood F(5), Bui MM(12), Parwani AV(13).

Author information:
(1)From the Department of Pathology and Immunology, Washington University School 
of Medicine in St Louis, St Louis, Missouri (Brodsky).
(2)the Department of Surgery, Health New Zealand, Counties Manukau, New Zealand 
(Ullah).
(3)the Department of Pathology, Kameda Medical Center, Kamogawa City, Chiba 
Prefecture, Japan (Bychkov).
(4)the Department of Pathology, Nagasaki University, Nagasaki, Japan (Bychkov).
(5)the Department of Pathology, Brigham and Women's Hospital, Boston, 
Massachusetts (Song, Mahmood).
(6)Office of the Chief Medical Officer, PathAI, Boston, Massachusetts (Walk).
(7)the Department of Pathology and Laboratory Medicine, Rutgers Robert Wood 
Johnson Medical School, New Brunswick, New Jersey (Louis).
(8)the Department of Oncologic Sciences, Morsani College of Medicine and 
Department of Electrical Engineering, University of South Florida, Tampa 
(Rasool).
(9)the Department of Machine Learning, Moffitt Cancer Center and Research 
Institute, Tampa, Florida (Rasool).
(10)Department of Machine Learning, Neuro-Oncology, Moffitt Cancer Center and 
Research Institute, Tampa, Florida (Rasool).
(11)Dermatopathology and Digital Pathology, Summit Health, Berkley Heights, New 
Jersey (Singh).
(12)Department of Machine Learning, Pathology, Moffitt Cancer Center and 
Research Institute, Tampa, Florida (Bui).
(13)the Department of Pathology, The Ohio State University, Columbus (Parwani).

CONTEXT.—: Generative artificial intelligence (AI) has emerged as a 
transformative force in various fields, including anatomic pathology, where it 
offers the potential to significantly enhance diagnostic accuracy, workflow 
efficiency, and research capabilities.
OBJECTIVE.—: To explore the applications, benefits, and challenges of generative 
AI in anatomic pathology, with a focus on its impact on diagnostic processes, 
workflow efficiency, education, and research.
DATA SOURCES.—: A comprehensive review of current literature and recent 
advancements in the application of generative AI within anatomic pathology, 
categorized into unimodal and multimodal applications, and evaluated for 
clinical utility, ethical considerations, and future potential.
CONCLUSIONS.—: Generative AI demonstrates significant promise in various domains 
of anatomic pathology, including diagnostic accuracy enhanced through AI-driven 
image analysis, virtual staining, and synthetic data generation; workflow 
efficiency, with potential for improvement by automating routine tasks, quality 
control, and reflex testing; education and research, facilitated by AI-generated 
educational content, synthetic histology images, and advanced data analysis 
methods; and clinical integration, with preliminary surveys indicating cautious 
optimism for nondiagnostic AI tasks and growing engagement in academic settings. 
Ethical and practical challenges require rigorous validation, prompt 
engineering, federated learning, and synthetic data generation to help ensure 
trustworthy, reliable, and unbiased AI applications. Generative AI can 
potentially revolutionize anatomic pathology, enhancing diagnostic accuracy, 
improving workflow efficiency, and advancing education and research. Successful 
integration into clinical practice will require continued interdisciplinary 
collaboration, careful validation, and adherence to ethical standards to ensure 
the benefits of AI are realized while maintaining the highest standards of 
patient care.

© 2025 College of American Pathologists.

DOI: 10.5858/arpa.2024-0215-RA
PMID: 39836377 [Indexed for MEDLINE]


5. Nat Med. 2025 Mar;31(3):932-942. doi: 10.1038/s41591-024-03416-6. Epub 2025 Jan 
8.

A generalist medical language model for disease diagnosis assistance.

Liu X(#)(1), Liu H(#)(2), Yang G(#)(1), Jiang Z(#)(1), Cui S(#)(3), Zhang Z(2), 
Wang H(2), Tao L(4), Sun Y(5), Song Z(5), Hong T(6), Yang J(6), Gao T(1), Zhang 
J(1), Li X(1), Zhang J(7), Sang Y(7), Yang Z(8), Xue K(9), Wu S(10), Zhang P(1), 
Yang J(11), Song C(12), Wang G(13).

Author information:
(1)State Key Laboratory of Networking and Switching Technology, Beijing 
University of Posts and Telecommunications, Beijing, China.
(2)Department of Orthopedics, Peking University Third Hospital & Beijing Key 
Laboratory of Spinal Disease & Engineering Research Center of Bone and Joint 
Precision Medicine, Beijing, China.
(3)School of Science and Engineering (SSE), Future Network of Intelligence 
Institute (FNii) and Guangdong Provincial Key Laboratory of Future Networks of 
Intelligence, Chinese University of Hong Kong, Shenzhen, China.
(4)Research Center of Clinical Epidemiology, Peking University Third Hospital, 
Beijing, China.
(5)Department of Respiratory and Critical Care Medicine, Peking University Third 
Hospital and Research Center for Chronic Airway Diseases, Peking University 
Health Science Center, Beijing, China.
(6)Department of Endocrinology and Metabolism, Peking University Third Hospital, 
Beijing, China.
(7)Department of Cardiology, The First College of Clinical Medical Science, 
China Three Gorges University and Yichang Central People's Hospital, Yichang, 
China.
(8)Peking University First Hospital and Research Center of Public Policy, Peking 
University, Beijing, China.
(9)Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, 
UK.
(10)South China Hospital, Medical School, Shenzhen University, Shenzhen, China.
(11)Department of Cardiology, The First College of Clinical Medical Science, 
China Three Gorges University and Yichang Central People's Hospital, Yichang, 
China. yangjian@ctgu.edu.cn.
(12)Department of Orthopedics, Peking University Third Hospital & Beijing Key 
Laboratory of Spinal Disease & Engineering Research Center of Bone and Joint 
Precision Medicine, Beijing, China. schl@bjmu.edu.cn.
(13)State Key Laboratory of Networking and Switching Technology, Beijing 
University of Posts and Telecommunications, Beijing, China. 
guangyu.wang24@gmail.com.
(#)Contributed equally

The delivery of accurate diagnoses is crucial in healthcare and represents the 
gateway to appropriate and timely treatment. Although recent large language 
models (LLMs) have demonstrated impressive capabilities in few-shot or zero-shot 
learning, their effectiveness in clinical diagnosis remains unproven. Here we 
present MedFound, a generalist medical language model with 176 billion 
parameters, pre-trained on a large-scale corpus derived from diverse medical 
text and real-world clinical records. We further fine-tuned MedFound to learn 
physicians' inferential diagnosis with a self-bootstrapping strategy-based 
chain-of-thought approach and introduced a unified preference alignment 
framework to align it with standard clinical practice. Extensive experiments 
demonstrate that our medical LLM outperforms other baseline LLMs and specialized 
models in in-distribution (common diseases), out-of-distribution (external 
validation) and long-tailed distribution (rare diseases) scenarios across eight 
specialties. Further ablation studies indicate the effectiveness of key 
components in our medical LLM training approach. We conducted a comprehensive 
evaluation of the clinical applicability of LLMs for diagnosis involving 
artificial intelligence (AI) versus physician comparison, AI-assistance study 
and human evaluation framework. Our proposed framework incorporates eight 
clinical evaluation metrics, covering capabilities such as medical record 
summarization, diagnostic reasoning and risk management. Our findings 
demonstrate the model's feasibility in assisting physicians with disease 
diagnosis as part of the clinical workflow.

© 2025. The Author(s), under exclusive licence to Springer Nature America, Inc.

DOI: 10.1038/s41591-024-03416-6
PMID: 39779927 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: The authors declare no 
competing financial interests.


6. Nat Med. 2025 Oct;31(10):3270-3282. doi: 10.1038/s41591-025-03983-2. Epub 2025 
Oct 6.

Generative artificial intelligence in medicine.

Teo ZL(#)(1)(2), Thirunavukarasu AJ(#)(3), Elangovan K(1)(2), Cheng H(1)(4), 
Moova P(1)(2), Soetikno B(5), Nielsen C(6), Pollreisz A(1)(7), Ting 
DSJ(1)(4)(8)(9)(10), Morris RJT(11)(12), Shah NH(13), Langlotz CP(14), Ting 
DSW(15)(16)(17).

Author information:
(1)Singapore National Eye Centre, Singapore Eye Research Institute, Singapore, 
Singapore.
(2)AI Office, Singapore Health Services, Singapore, Singapore.
(3)Nuffield Department of Clinical Neurosciences, Medical Sciences Division, 
University of Oxford, Oxford, UK.
(4)Ophthalmology and Visual Sciences Academic Clinical Program, Duke-NUS Medical 
School, Singapore, Singapore.
(5)Department of Ophthalmology, Byers Eye Institute, Stanford, CA, USA.
(6)Cumming School of Medicine, University of Calgary, Calgary, Alberta, Canada.
(7)Department of Ophthalmology and Optometry, Medical University of Vienna, 
Vienna, Austria.
(8)Department of Inflammation and Ageing, College of Medicine and Health, 
University of Birmingham, Birmingham, UK.
(9)Birmingham and Midland Eye Centre, Sandwell and West Birmingham NHS Trust, 
Birmingham, UK.
(10)Academic Ophthalmology, School of Medicine, University of Nottingham, 
Nottingham, UK.
(11)Ministry of Health (MOH) Office for Healthcare Transformation, Singapore, 
Singapore.
(12)Yong Loo Lin School of Medicine, National University of Singapore, 
Singapore, Singapore.
(13)Department of Medicine, Stanford University, Stanford, CA, USA.
(14)Department of Radiology, Stanford University, Stanford, CA, USA.
(15)Singapore National Eye Centre, Singapore Eye Research Institute, Singapore, 
Singapore. daniel.ting@duke-nus.edu.sg.
(16)AI Office, Singapore Health Services, Singapore, Singapore. 
daniel.ting@duke-nus.edu.sg.
(17)Department of Ophthalmology, Byers Eye Institute, Stanford, CA, USA. 
daniel.ting@duke-nus.edu.sg.
(#)Contributed equally

Generative artificial intelligence (GAI) can automate a growing number of 
biomedical tasks, ranging from clinical decision support to design and analysis 
of research studies. GAI uses machine learning and transformer model 
architectures to generate useful text, images and sound data in response to user 
queries. While previous biomedical deep-learning applications have used 
general-purpose datasets and enormous volumes of labeled data for training, 
evidence now suggests that GAI models may perform better while requiring less 
training data-for example, using smaller, domain-specific datasets. Moreover, AI 
techniques have progressed from fully supervised training to less 
label-intensive approaches, such as weakly supervised or unsupervised 
fine-tuning and reinforcement learning. Recent iterations of GAI, such as 
agents, mixture-of-expert models and reasoning models, have further extended 
their capabilities to assist with complex and multistage tasks. Here, we provide 
an overview of recent technical advancements in GAI. We explore the potential of 
the latest generation of models to improve healthcare for clinicians and 
patients, and discuss validation approaches using specific examples to 
illustrate challenges and opportunities for further work.

© 2025. Springer Nature America, Inc.

DOI: 10.1038/s41591-025-03983-2
PMID: 41053447 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


7. J Med Internet Res. 2025 Jan 27;27:e64993. doi: 10.2196/64993.

Accuracy and Safety of AI-Enabled Scribe Technology: Instrument Validation 
Study.

Biro J(1), Handley JL(1), Cobb NK(2)(3), Kottamasu V(2), Collins J(2), Krevat 
S(1)(3), Ratwani RM(1)(3).

Author information:
(1)National Center for Human Factors in Healthcare, MedStar Health Research 
Institute, Washington, DC, United States.
(2)MedStar Health Institute for Innovation, Washington, DC, United States.
(3)Georgetown University School of Medicine, Washington, DC, United States.

Artificial intelligence-enabled ambient digital scribes may have many potential 
benefits, yet results from our study indicate that there are errors that must be 
evaluated to mitigate safety risks.

©Joshua Biro, Jessica L Handley, Nathan K Cobb, Varsha Kottamasu, Jeffrey 
Collins, Seth Krevat, Raj M Ratwani. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org), 27.01.2025.

DOI: 10.2196/64993
PMCID: PMC11811668
PMID: 39869899 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


8. J Med Syst. 2025 Feb 18;49(1):28. doi: 10.1007/s10916-025-02157-4.

Artificial Intelligence (AI) - Powered Documentation Systems in Healthcare: A 
Systematic Review.

Bracken A(1), Reilly C(2), Feeley A(3), Sheehan E(4), Merghani K(5), Feeley 
I(5).

Author information:
(1)Royal College of Surgeons in Ireland (RCSI), 123 Stephen's Green, Dublin 2, 
Ireland. Aislingbracken24@rcsi.com.
(2)University College Dublin (UCD), Belfield, Dublin 4, Ireland.
(3)Royal College of Surgeons in Ireland (RCSI), 123 Stephen's Green, Dublin 2, 
Ireland.
(4)School of Medicine, University of Limerick (UL), Castletroy, Limerick, 
Ireland.
(5)Midlands Regional Hospital Tullamore, Arden Rd., Tullamore, Offaly, Ireland.

Artificial Intelligence (AI) driven documentation systems are positioned to 
enhance documentation efficiency and reduce documentation burden in the 
healthcare setting. The administrative burden associated with clinical 
documentation has been identified as a major contributor to health care 
professional (HCP) burnout. The current systematic review aims to evaluate the 
efficiency, quality, and stakeholder opinion regarding the use of AI-driven 
documentation systems. Using the Preferred Reporting Items for Systematic 
Reviews and Meta-Analyses (PRISMA) guidelines a comprehensive search was 
conducted across PubMed, Embase and Cochrane library. Two independent reviewers 
applied inclusion and exclusion criteria to identify eligible studies. Details 
of AI technology, document type, document quality and stakeholder experience 
were extracted. The review included 11 studies. All included studies utilised 
Chat generated pretrained transformer (Chat GPT, Open AI, CA, USA) or an ambient 
AI technology. Both forms of AI demonstrated significant potential to improve 
documentation efficiency. Despite efficiency gains, the quality of AI-generated 
documentation varied across studies. The heterogeneity of methods utilised to 
assess document quality influenced interpretation of results. HCP opinion was 
generally positive, users highlighted ease of use and reduced task load as 
primary benefits. However, HCPs also expressed concerns about the reliability 
and validity of AI-generated documentation. Chat GPT and ambient AI show promise 
in enhancing the efficiency and quality of clinical documentation. While the 
efficiency benefits are clear, the challenges associated with accuracy and 
consistency need to be addressed. HCP experiences indicate a cautious optimism 
towards AI integration, however reliability will depend on continued refinement 
and validation of the technology.

© 2025. The Author(s).

DOI: 10.1007/s10916-025-02157-4
PMCID: PMC11835907
PMID: 39966286 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics Approval and Consent to 
Participate: Not applicable. Competing Interests: The authors declare no 
competing interests.


9. Int J Surg. 2025 Mar 1;111(3):2546-2557. doi: 10.1097/JS9.0000000000002223.

ChatGPT's role in alleviating anxiety in total knee arthroplasty consent 
process: a randomized controlled trial pilot study.

Gan W(1)(2), Ouyang J(1), She G(2), Xue Z(2), Zhu L(3), Lin A(3), Mou W(4), 
Jiang A(5), Qi C(6), Cheng Q(7), Luo P(3), Li H(8), Zheng X(2).

Author information:
(1)Department of Joint Surgery and Sports Medicine, Zhuhai People's Hospital 
(The Affiliated Hospital of Beijing Institute of Technology, Zhuhai Clinical 
Medical College of Jinan University), Zhuhai, Guangdong, China.
(2)Department of Bone and Joint Surgery and Sports Medicine Center, The First 
Affiliated Hospital, The First Affiliated Hospital of Jinan University, 
Guangzhou, China.
(3)Department of Oncology, Zhujiang Hospital, Southern Medical University, 
Guangzhou, Guangdong, China.
(4)Department of Urology, Shanghai General Hospital, Shanghai Jiao Tong 
University School of Medicine, Shanghai, China.
(5)Department of Urology, Changhai hospital, Naval Medical University (Second 
Military Medical University), Shanghai, China.
(6)The University of Hong Kong, Hong Kong, China.
(7)Department of Neurosurgery, Xiangya Hospital, Central South University, 
Changsha, Hunan, China.
(8)Department of Foot and Ankle Surgery, Beijing Jishuitan Hospital, Capital 
Medical University, Beijing, China.

BACKGROUND: Recent advancements in artificial intelligence (AI) like ChatGPT 
have expanded possibilities for patient education, yet its impact on 
perioperative anxiety in total knee arthroplasty (TKA) patients remains 
unexplored.
METHODS: In this single-blind, randomized controlled pilot study from April to 
July 2023, 60 patients were randomly allocated using sealed envelopes to either 
ChatGPT-assisted or traditional surgeon-led informed consent groups. In the 
ChatGPT group, physicians used ChatGPT 4.0 to provide standardized, 
comprehensive responses to patient queries during the consent process, while 
maintaining their role in interpreting and contextualizing the information. 
Outcomes were measured using Hospital Anxiety and Depression Scales (HADS), 
Perioperative Apprehension Scale-7 (PAS-7), Visual Analogue Scales for Anxiety 
and Pain (VAS-A, VAS-P), Western Ontario and McMaster Universities 
Osteoarthritis Index (WOMAC), and satisfaction questionnaires.
RESULTS: Of 55 patients completing the study, the ChatGPT group showed 
significantly lower anxiety scores after informed consent (HADS-A: 10.48 ± 3.84 
vs 12.75 ± 4.12, P = .04, Power = .67; PAS-7: 12.44 ± 3.70 vs 14.64 ± 2.11, P 
= .01, Power = .85; VAS-A: 5.40 ± 1.89 vs 6.71 ± 2.27, P = .02, Power = .75) and 
on the fifth postoperative day (HADS-A: 8.33 ± 3.20 vs 10.71 ± 3.83, P = .01, 
Power = .79; VAS-A: 3.41 ± 1.58 vs 4.64 ± 1.70, P = .008, Power = .85). The 
ChatGPT group also reported higher satisfaction with preoperative education 
(4.22 ± 0.51 vs 3.43 ± 0.84, P <.001, Power = .99) and overall hospitalization 
experience (4.11 ± 0.65 vs 3.46 ± 0.69, P = .001, Power = .97). No significant 
differences were found in depression scores, knee function, or pain levels.
CONCLUSIONS: ChatGPT-assisted informed consent effectively reduced perioperative 
anxiety and improved patient satisfaction in TKA patients. While these 
preliminary findings are promising, larger studies are needed to validate these 
results and explore broader applications of AI in preoperative patient 
education.

Copyright © 2025 The Author(s). Published by Wolters Kluwer Health, Inc.

DOI: 10.1097/JS9.0000000000002223
PMCID: PMC12372728
PMID: 39903546 [Indexed for MEDLINE]

Conflict of interest statement: All the authors declare to have no conflicts of 
interest relevant to this study.


10. NPJ Digit Med. 2025 Mar 22;8(1):175. doi: 10.1038/s41746-025-01543-z.

A systematic review and meta-analysis of diagnostic performance comparison 
between generative AI and physicians.

Takita H(1), Kabata D(2), Walston SL(1)(3), Tatekawa H(1), Saito K(4), Tsujimoto 
Y(5)(6)(7), Miki Y(1), Ueda D(8)(9)(10).

Author information:
(1)Department of Diagnostic and Interventional Radiology, Graduate School of 
Medicine, Osaka Metropolitan University, Osaka, Japan.
(2)Center for Mathematical and Data Science, Kobe University, Kobe, Japan.
(3)Department of Artificial Intelligence, Graduate School of Medicine, Osaka 
Metropolitan University, Osaka, Japan.
(4)Center for Digital Transformation of Health Care, Graduate School of 
Medicine, Kyoto University, Kyoto, Japan.
(5)Oku Medical Clinic, Osaka, Japan.
(6)Department of Health Promotion and Human Behavior, Kyoto University Graduate 
School of Medicine/School of Public Health, Kyoto University, Kyoto, Japan.
(7)Scientific Research WorkS Peer Support Group (SRWS-PSG), Osaka, Japan.
(8)Department of Diagnostic and Interventional Radiology, Graduate School of 
Medicine, Osaka Metropolitan University, Osaka, Japan. ai.labo.ocu@gmail.com.
(9)Department of Artificial Intelligence, Graduate School of Medicine, Osaka 
Metropolitan University, Osaka, Japan. ai.labo.ocu@gmail.com.
(10)Center for Health Science Innovation, Osaka Metropolitan University, Osaka, 
Japan. ai.labo.ocu@gmail.com.

While generative artificial intelligence (AI) has shown potential in medical 
diagnostics, comprehensive evaluation of its diagnostic performance and 
comparison with physicians has not been extensively explored. We conducted a 
systematic review and meta-analysis of studies validating generative AI models 
for diagnostic tasks published between June 2018 and June 2024. Analysis of 83 
studies revealed an overall diagnostic accuracy of 52.1%. No significant 
performance difference was found between AI models and physicians overall 
(p = 0.10) or non-expert physicians (p = 0.93). However, AI models performed 
significantly worse than expert physicians (p = 0.007). Several models 
demonstrated slightly higher performance compared to non-experts, although the 
differences were not significant. Generative AI demonstrates promising 
diagnostic capabilities with accuracy varying by model. Although it has not yet 
achieved expert-level reliability, these findings suggest potential for 
enhancing healthcare delivery and medical education when implemented with 
appropriate understanding of its limitations.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01543-z
PMCID: PMC11929846
PMID: 40121370

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


11. World Psychiatry. 2025 Jun;24(2):156-174. doi: 10.1002/wps.21299.

The evolving field of digital mental health: current evidence and implementation 
issues for smartphone apps, generative artificial intelligence, and virtual 
reality.

Torous J(1), Linardon J(2), Goldberg SB(3), Sun S(4)(5)(6), Bell I(7)(8), 
Nicholas J(5)(8), Hassan L(9), Hua Y(1)(10), Milton A(11)(12), Firth J(13).

Author information:
(1)Division of Digital Psychiatry, Beth Israel Deaconess Medical Center, Harvard 
Medical School, Boston, MA, USA.
(2)SEED Lifespan Strategic Research Centre, School of Psychology, Faculty of 
Health, Deakin University, Geelong, VIC, Australia.
(3)Department of Counseling Psychology and Center for Healthy Minds, University 
of Wisconsin, Madison, WI, USA.
(4)Department of Behavioral and Social Sciences, Brown University School of 
Public Health, Providence, RI, USA.
(5)Mindfulness Center, Brown University, Providence, RI, USA.
(6)Center for Global Public Health, Brown University, Providence, RI, USA.
(7)Orygen, Parkville, VIC, Australia.
(8)Centre for Youth Mental Health, University of Melbourne, Melbourne, VIC, 
Australia.
(9)School for Health Sciences, University of Manchester, Manchester, UK.
(10)Department of Epidemiology, Harvard T.H. Chan School of Public Health, 
Boston, MA, USA.
(11)Central Clinical School, Faculty of Medicine and Health, University of 
Sydney, Sydney, NSW, Australia.
(12)Australian Research Council (ARC) Centre of Excellence for Children and 
Families Over the Life, Sydney, NSW, Australia.
(13)Division of Psychology and Mental Health, University of Manchester, and 
Greater Manchester Mental Health NHS Foundation Trust, Manchester Academic 
Health Science Centre, Manchester, UK.

The expanding domain of digital mental health is transitioning beyond 
traditional telehealth to incorporate smartphone apps, virtual reality, and 
generative artificial intelligence, including large language models. While 
industry setbacks and methodological critiques have highlighted gaps in evidence 
and challenges in scaling these technologies, emerging solutions rooted in 
co-design, rigorous evaluation, and implementation science offer promising 
pathways forward. This paper underscores the dual necessity of advancing the 
scientific foundations of digital mental health and increasing its real-world 
applicability through five themes. First, we discuss recent technological 
advances in digital phenotyping, virtual reality, and generative artificial 
intelligence. Progress in this latter area, specifically designed to create new 
outputs such as conversations and images, holds unique potential for the mental 
health field. Given the spread of smartphone apps, we then evaluate the evidence 
supporting their utility across various mental health contexts, including 
well-being, depression, anxiety, schizophrenia, eating disorders, and substance 
use disorders. This broad view of the field highlights the need for a new 
generation of more rigorous, placebo-controlled, and real-world studies. We 
subsequently explore engagement challenges that hamper all digital mental health 
tools, and propose solutions, including human support, digital navigators, 
just-in-time adaptive interventions, and personalized approaches. We then 
analyze implementation issues, emphasizing clinician engagement, service 
integration, and scalable delivery models. We finally consider the need to 
ensure that innovations work for all people and thus can bridge digital health 
disparities, reviewing the evidence on tailoring digital tools for historically 
marginalized populations and low- and middle-income countries. Regarding digital 
mental health innovations as tools to augment and extend care, we conclude that 
smartphone apps, virtual reality, and large language models can positively 
impact mental health care if deployed correctly.

© 2025 World Psychiatric Association.

DOI: 10.1002/wps.21299
PMCID: PMC12079407
PMID: 40371757


12. J Am Med Inform Assoc. 2025 Apr 1;32(4):605-615. doi: 10.1093/jamia/ocaf008.

Improving large language model applications in biomedicine with 
retrieval-augmented generation: a systematic review, meta-analysis, and clinical 
development guidelines.

Liu S(1)(2), McCoy AB(1), Wright A(1)(3).

Author information:
(1)Department of Biomedical Informatics, Vanderbilt University Medical Center, 
Nashville, TN 37212, United States.
(2)Department of Computer Science, Vanderbilt University, Nashville, TN 37212, 
United States.
(3)Department of Medicine, Vanderbilt University Medical Center, Nashville, TN 
37212, United States.

OBJECTIVE: The objectives of this study are to synthesize findings from recent 
research of retrieval-augmented generation (RAG) and large language models 
(LLMs) in biomedicine and provide clinical development guidelines to improve 
effectiveness.
MATERIALS AND METHODS: We conducted a systematic literature review and a 
meta-analysis. The report was created in adherence to the Preferred Reporting 
Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were 
performed in 3 databases (PubMed, Embase, PsycINFO) using terms related to 
"retrieval augmented generation" and "large language model," for articles 
published in 2023 and 2024. We selected studies that compared baseline LLM 
performance with RAG performance. We developed a random-effect meta-analysis 
model, using odds ratio as the effect size.
RESULTS: Among 335 studies, 20 were included in this literature review. The 
pooled effect size was 1.35, with a 95% confidence interval of 1.19-1.53, 
indicating a statistically significant effect (P = .001). We reported clinical 
tasks, baseline LLMs, retrieval sources and strategies, as well as evaluation 
methods.
DISCUSSION: Building on our literature review, we developed Guidelines for 
Unified Implementation and Development of Enhanced LLM Applications with RAG in 
Clinical Settings to inform clinical applications using RAG.
CONCLUSION: Overall, RAG implementation showed a 1.35 odds ratio increase in 
performance compared to baseline LLMs. Future research should focus on (1) 
system-level enhancement: the combination of RAG and agent, (2) knowledge-level 
enhancement: deep integration of knowledge into LLM, and (3) integration-level 
enhancement: integrating RAG systems within electronic health records.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf008
PMCID: PMC12005634
PMID: 39812777 [Indexed for MEDLINE]

Conflict of interest statement: The authors do not have conflicts of interest 
related to this study.


13. Disabil Rehabil Assist Technol. 2025 Aug;20(6):1575-1577. doi: 
10.1080/17483107.2025.2483300. Epub 2025 Apr 3.

ChatGPT for speech-impaired assistance.

Bhamidipaty V(1), Botchu B(2), Bhamidipaty DL(3), Guntoory I(4), Iyengar KP(5).

Author information:
(1)Department of Computer Science and Engineering-Data Science, Gandhi Institute 
of Technology, Gitam University, Visakhapatnam, Andhra Pradesh, India.
(2)Solihull School, Solihull, UK.
(3)Department of Biotechnology, Manipal Institute of Technology, Manipal, 
Karnataka, India.
(4)Department of Obstetrics & Gynaecology, GIMSR, Gitam University, 
Visakhapatnam, India.
(5)Department of Orthopaedics, Southport and Ormskirk Hospitals, Mersey and West 
Lancashire Teaching NHS Trust, Southport, UK.

BACKGROUND: Speech and language impairments, though often used interchangeably, 
are two very distinct types of challenges. A speech impairment may lead to 
impaired ability to produce speech sounds whilst communication may be affected 
due to lack of fluency or articulation of words. Consequently this may affect a 
person's ability to articulate may affect academic achievement, social 
development and progress in life. ChatGPT (Generative Pretrained Transformer) is 
an open access AI (Artificial Intelligence) tool developed by Open AI® based on 
Large language models (LLMs) with the ability to respond to human prompts to 
generate texts using Supervised and Unsupervised Machine Learning (ML) 
Algorithms. This article explores the current role and future perspectives of 
ChatGPT AI Tool for Speech-Impaired Assistance.
METHODS: A cumulative search strategy using databases of PubMed, Google Scholar, 
Scopus and grey literature was conducted to generate this narrative review.
RESULTS: A spectrum of Enabling Technologies for Speech & Language Impairment 
have been explored. Augmentative and Alternative Communication technology (AAC), 
Integration with Neuroprosthesis technology and Speech therapy applications 
offer considerable potential to aid speech and language impaired individuals.
CONCLUSION: Current applications of AI, ChatGPT and other LLM's offer promising 
solutions in enhancing communication in people affected by Speech and Language 
impairment. However, further research and development is required to ensure 
affordability, accessibility and authenticity of these AI Tools in clinical 
Practice.

Plain Language Summary: Deep-learning language models could be used in 
conjunction with neuroprosthesis technology to improve BCIs (Brain-computer 
interfaces).AI like ChatGPT could help individuals without access to speech and 
language therapy, providing similar support through more inexpensive strategies 
which allow them to learn remotely.ChatGPT has considerable potential to aid 
speech-impaired individuals through improved speech and language therapy and 
more accurate neuroprosthesis technology.

DOI: 10.1080/17483107.2025.2483300
PMID: 40177878 [Indexed for MEDLINE]


14. J Med Internet Res. 2025 Jun 23;27:e72398. doi: 10.2196/72398.

AI in Medical Questionnaires: Scoping Review.

Luo X(#)(1), Li Y(#)(1), Xu J(#)(1), Zheng Z(1), Ying F(2)(3), Huang G(1)(4).

Author information:
(1)Faculty of Humanities and Arts, Macau University of Science and Technology, 
Macau, China.
(2)Industrial and Manufacturing Engineering, European Academy of Engineering, 
Gothenburg, Sweden.
(3)College of Computer Science and Technology, Zhejiang University, Hangzhou, 
China.
(4)Zhuhai M.U.S.T. Science and Technology Research Institute, Zhuhai, China.
(#)Contributed equally

Erratum in
    J Med Internet Res. 2025 Aug 11;27:e80644. doi: 10.2196/80644.

This systematic review aimed to explore the current applications, potential 
benefits, and issues of artificial intelligence (AI) in medical questionnaires, 
focusing on its role in 3 main functions: assessment, development, and 
prediction. The global mental health burden remains severe. The World Health 
Organization reports that >1 billion people worldwide experience mental 
disorders, with the prevalence of depression and anxiety among children and 
adolescents at 2.6% and 6.5%, respectively. However, commonly used clinical 
questionnaires such as the Hamilton Depression Rating Scale and the Beck 
Depression Inventory suffer from several problems, including the high degree of 
overlap of symptoms of depression with those of other psychiatric disorders and 
a lack of professional supervision during administration of the questionnaires, 
which often lead to inaccurate diagnoses. In the wake of the COVID-19 pandemic, 
the health care system is facing the dual challenges of a surge in patient 
numbers and the complexity of mental health issues. AI technology has now been 
shown to have great promise in improving diagnostic accuracy, assisting clinical 
decision-making, and simplifying questionnaire development and data analysis. To 
systematically assess the value of AI in medical questionnaires, this study 
searched 5 databases (PubMed, Embase, Cochrane Library, Web of Science, and 
China National Knowledge Infrastructure) for the period from database inception 
to September 2024. Of 49,091 publications, a total of 14 (0.03%) studies met the 
inclusion criteria. AI technologies showed significant advantages in assessment, 
such as distinguishing myalgic encephalomyelitis or chronic fatigue syndrome 
from long COVID-19 with 92.18% accuracy. In questionnaire development, natural 
language processing using generative models such as ChatGPT was used to 
construct culturally competent scales. In terms of disease prediction, one study 
had an area under the curve of 0.790 for cataract surgery risk prediction. 
Overall, 24 AI technologies were identified, covering traditional algorithms 
such as random forest, support vector machine, and k-nearest neighbor, as well 
as deep learning models such as convolutional neural networks, Bidirectional 
Encoder Representations From Transformers, and ChatGPT. Despite the positive 
findings, only 21% (3/14) of the studies had entered the clinical validation 
phase, whereas the remaining 79% (11/14) were still in the exploratory phase of 
research. Most of the studies (10/14, 71%) were rated as being of moderate 
methodological quality, with major limitations including lack of a control 
group, incomplete follow-up data, and inadequate validation systems. In summary, 
the integrated application of AI in medical questionnaires has significant 
potential to improve diagnostic efficiency, accelerate scale development, and 
promote early intervention. Future research should pay more attention to model 
interpretability, system compatibility, validation standardization, and ethical 
governance to effectively address key challenges such as data privacy, clinical 
integration, and transparency.
BACKGROUND: The World Health Organization reports that >1 billion people 
worldwide experience mental disorders, with the prevalence of depression and 
anxiety among children and adolescents at 2.6% and 6.5%, respectively. However, 
commonly used clinical questionnaires such as the Hamilton Depression Rating 
Scale and the Beck Depression Inventory suffer from several problems, including 
the high degree of overlap of symptoms of depression with those of other 
psychiatric disorders and a lack of professional supervision during 
administration of the questionnaires, which often lead to inaccurate diagnoses. 
In the wake of the COVID-19 pandemic, the health care system is facing the dual 
challenges of a surge in patient numbers and the complexity of mental health 
issues. Artificial Intelligence (AI) technology has now been shown to have great 
promise in improving diagnostic accuracy, assisting clinical decision-making, 
and simplifying questionnaire development and data analysis.
OBJECTIVE: This review aimed to explore the current applications, potential 
benefits, and issues of AI in medical questionnaires, focusing on its role in 3 
main functions: assessment, development, and prediction. The global mental 
health burden remains severe.
METHODS: The review included peer-reviewed studies that applied AI technologies 
to medical, psychological, or physiological questionnaires and reported 
measurable outcomes; non–peer-reviewed, non-English/Chinese, ethically 
noncompliant, or AI-unrelated studies were excluded. Five databases (PubMed, 
Embase, Cochrane Library, Web of Science, and CNKI) were searched from inception 
through September 2024. Three independent reviewers conducted data extraction, 
quality appraisal using the Joanna Briggs Institute tools, and narrative 
synthesis of AI applications across questionnaire assessment, development, and 
prediction tasks.
RESULTS: Of 49,091 publications, a total of 14 (0.03%) studies met the inclusion 
criteria. AI technologies showed advantages in assessment, such as 
distinguishing myalgic encephalomyelitis or chronic fatigue syndrome from long 
COVID-19 with 92.18% accuracy. In questionnaire development, natural language 
processing using generative models such as ChatGPT was used to construct 
culturally competent scales. In terms of disease prediction, one study had an 
area under the curve of 0.790 for cataract surgery risk prediction. Overall, 24 
AI technologies were identified, covering traditional algorithms such as random 
forest, support vector machine, and k-nearest neighbor, as well as deep learning 
models such as convolutional neural networks, Bidirectional Encoder 
Representations From Transformers, and ChatGPT. Despite the positive findings, 
only 21% (3/14) of the studies had entered the clinical validation phase, 
whereas the remaining 79% (11/14) were still in the exploratory phase of 
research. Most of the studies (10/14, 71%) were rated as being of moderate 
methodological quality, with major limitations including lack of a control 
group, incomplete follow-up data, and inadequate validation systems.
CONCLUSIONS: In summary, the integrated application of AI in medical 
questionnaires has significant potential to improve diagnostic efficiency, 
accelerate scale development, and promote early intervention. Future research 
should pay more attention to model interpretability, system compatibility, 
validation standardization, and ethical governance to effectively address key 
challenges such as data privacy, clinical integration, and transparency.

©Xuexing Luo, Yiyuan Li, Jing Xu, Zhong Zheng, Fangtian Ying, Guanghui Huang. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 23.06.2025.

DOI: 10.2196/72398
PMCID: PMC12235208
PMID: 40549427 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


15. Semin Nucl Med. 2025 Dec 4:S0001-2998(25)00154-0. doi: 
10.1053/j.semnuclmed.2025.11.002. Online ahead of print.

The Nuclear Nephrology Artificial Intelligence Ecosystem.

Currie GM(1), Rohren EM(2).

Author information:
(1)School of Dentistry and Medical Sciences, Charles Sturt University, NSW, 
Australia. Electronic address: gcurrie@csu.edu.au.
(2)School of Dentistry and Medical Sciences, Charles Sturt University, NSW, 
Australia; Paul L Foster School of Medicine, Texas Tech University Health 
Center, El Paso, USA.

The contemporary scope of nuclear nephrology extends from non-imaging techniques 
for glomerular filtration rate calculation through dynamic renal scintigraphy 
and cortical imaging with planar or single photon emission computed tomography 
(SPECT) approaches to emerging applications in positron emission tomography 
(PET) renography and theranostics-based renal toxicity risk assessment. 
Artificial intelligence (AI) shares a long history with nuclear nephrology that 
started with expert systems and statistical machine learning (ML) approaches, 
transitioned through feed forward neural networks (FFNN), landed with 
convolutional neural networks (CNNs) and deep learning (DL), and has emerging 
opportunities across the gamut of generative AI like large language models 
(LLMs), diffusion models, generative adversarial networks (GANs) and multimodal 
models like vision language models (VLMs). A range of AI tools across the 
nuclear nephrology ecosystem describe bespoke AI algorithms, commercial AI 
products, embedded AI tools from vendors, general-purpose and cross-domain AI 
frameworks. Applications in clinical workflow, research and development, and 
imaging are explored, highlighting the potential of AI in detection, 
classification, segmentation, prediction, data analysis and image enhancement. 
Emerging AI opportunities from generative AI, LLMs, VLMs, and segmentation 
foundation models such as the SAM, offer exciting multi-modal, few-shot learning 
that may re-imagine nuclear nephrology. There remains the need for considerable 
development and validation for widespread clinical utility of AI opportunities, 
and the need for consideration of ethical limitations and social justice.

Copyright © 2025 The Author(s). Published by Elsevier Inc. All rights reserved.

DOI: 10.1053/j.semnuclmed.2025.11.002
PMID: 41350144

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


16. J Med Internet Res. 2025 Oct 27;27:e76947. doi: 10.2196/76947.

Evaluating Large Language Models in Ophthalmology: Systematic Review.

Zhang Z(#)(1), Zhang H(#)(1), Pan Z(2), Bi Z(2), Wan Y(2), Song X(1), Fan X(1).

Author information:
(1)State Key Laboratory of Eye Health, Department of Ophthalmology, Shanghai 
Ninth People's Hospital, Shanghai Jiao Tong University School of Medicine, 
Shanghai, China.
(2)School of Computer Science and Technology, Huazhong University of Science and 
Technology, Wuhan, China.
(#)Contributed equally

BACKGROUND: Large language models (LLMs) have the potential to revolutionize 
ophthalmic care, but their evaluation practice remains fragmented. A systematic 
assessment is crucial to identify gaps and guide future evaluation practices and 
clinical integration.
OBJECTIVE: This study aims to map the current landscape of LLM evaluations in 
ophthalmology and explore whether performance synthesis is feasible for a common 
task.
METHODS: A comprehensive search of PubMed, Web of Science, Embase, and IEEE 
Xplore was conducted up to November 17, 2024 (no language limits). Eligible 
publications quantitatively assessed an existing or modified LLM on 
ophthalmology-related tasks. Studies without full-text availability or those 
focusing solely on vision-only models were excluded. Two reviewers screened 
studies and extracted data across 6 dimensions (evaluated LLM, data modality, 
ophthalmic subspecialty, medical task, evaluation dimension, and clinical 
alignment), and disagreements were resolved by a third reviewer. Descriptive 
statistics were analyzed and visualized using Python (with NumPy, Pandas, SciPy, 
and Matplotlib libraries). The Fisher exact test compared open- versus 
closed-source models. An exploratory random-effects meta-analysis (logit 
transformation; DerSimonian-Laird τ2) was performed for the diagnosis-making 
task; heterogeneity was reported with I2 and subgrouped by model, modality, and 
subspecialty.
RESULTS: Of the 817 identified records, 187 studies met the inclusion criteria. 
Closed-source LLMs dominated: 170 for ChatGPT, 58 for Gemini, and 32 for 
Copilot. Open-source LLMs appeared in only 25 (13.4%) of studies overall, but 
they appeared in 17 (77.3%) of evaluation-after-development studies, versus 8 
(4.8%) pure-evaluation studies (P<1×10-5). Evaluations were chiefly text-only 
(n=168); image-text tasks, despite the centrality of imaging, were used in 19 
studies. Subspecialty coverage was skewed toward comprehensive ophthalmology 
(n=72), retina and vitreous (n=32), and glaucoma (n=20). Refractive surgery, 
ocular pathology and oncology, and ophthalmic pharmacology each appeared in 3 or 
fewer studies. Medical query (n=86), standardized examination (n=41), and 
diagnosis making (n=29) emerged as the 3 predominant tasks, while research 
assistance (n=5), patient triaging (n=3), and disease prediction (n=3) received 
less attention. Accuracy was reported in most studies (n=176), whereas 
calibration and uncertainty were almost absent (n=5). Real-world patient data 
(n=45), human performance comparison (n=63), non‑English testing (n=24), and 
real-world deployment (n=4) were relatively absent. Exploratory meta-analysis 
pooled 28 diagnostic evaluations from 17 studies: overall accuracy was 0.594 
(95% CI 0.488-0.692) with extreme heterogeneity (I2=94.5%). Subgroups remained 
heterogeneous (I2>80%), and findings were inconsistent (eg, pooled 
GPT-3.5>GPT-4).
CONCLUSIONS: Evidence on LLM evaluations in ophthalmology is extensive but 
heterogeneous. Most studies have tested a few closed-source LLMs on text-based 
questions, leaving open-source systems, multimodal tasks, non-English contexts, 
and real-world deployment underexamined. High methodological variability 
precludes meaningful performance aggregation, as illustrated by the 
heterogeneous meta-analysis. Standardized, multimodal benchmarks and phased 
clinical validation pipelines are urgently needed before LLMs can be safely 
integrated into eye care workflows.

©Zili Zhang, Haiyang Zhang, Zhe Pan, Zhangqian Bi, Yao Wan, Xuefei Song, Xianqun 
Fan. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 27.10.2025.

DOI: 10.2196/76947
PMCID: PMC12603593
PMID: 41144954 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


17. Front Digit Health. 2025 Jun 27;7:1574287. doi: 10.3389/fdgth.2025.1574287. 
eCollection 2025.

Accuracy of ChatGPT-3.5, ChatGPT-4o, Copilot, Gemini, Claude, and Perplexity in 
advising on lumbosacral radicular pain against clinical practice guidelines: 
cross-sectional study.

Rossettini G(#)(1)(2), Bargeri S(#)(3), Cook C(4)(5)(6), Guida S(3), Palese 
A(7), Rodeghiero L(8), Pillastrini P(9)(10), Turolla A(9)(10), Castellini 
G(#)(3), Gianola S(#)(3).

Author information:
(1)School of Physiotherapy, University of Verona, Verona, Italy.
(2)Department of Physiotherapy, Faculty of Medicine, Health and Sports, 
Universidad Europea de Madrid, Madrid, Spain.
(3)Unit of Clinical Epidemiology, IRCCS Istituto Ortopedico Galeazzi, Milan, 
Italy.
(4)Department of Orthopaedics, Duke University, Durham, NC, United States.
(5)Duke Clinical Research Institute, Duke University, Durham, NC, United States.
(6)Department of Population Health Sciences, Duke University, Durham, NC, United 
States.
(7)Department of Medical Sciences, University of Udine, Udine, Italy.
(8)Department of Rehabilitation, Hospital of Merano (SABES-ASDAA), Teaching 
Hospital of Paracelsus Medical University (PMU), Merano-Meran, Italy.
(9)Department of Biomedical and Neuromotor Sciences (DIBINEM), Alma Mater 
University of Bologna, Bologna, Italy.
(10)Unit of Occupational Medicine, IRCCS Azienda Ospedaliero-Universitaria di 
Bologna, Bologna, Italy.
(#)Contributed equally

INTRODUCTION: Artificial Intelligence (AI) chatbots, which generate human-like 
responses based on extensive data, are becoming important tools in healthcare by 
providing information on health conditions, treatments, and preventive measures, 
acting as virtual assistants. However, their performance in aligning with 
clinical practice guidelines (CPGs) for providing answers to complex clinical 
questions on lumbosacral radicular pain is still unclear. We aim to evaluate AI 
chatbots' performance against CPG recommendations for diagnosing and treating 
lumbosacral radicular pain.
METHODS: We performed a cross-sectional study to assess AI chatbots' responses 
against CPGs recommendations for diagnosing and treating lumbosacral radicular 
pain. Clinical questions based on these CPGs were posed to the latest versions 
(updated in 2024) of six AI chatbots: ChatGPT-3.5, ChatGPT-4o, Microsoft 
Copilot, Google Gemini, Claude, and Perplexity. The chatbots' responses were 
evaluated for (a) consistency of text responses using Plagiarism Checker X, (b) 
intra- and inter-rater reliability using Fleiss' Kappa, and (c) match rate with 
CPGs. Statistical analyses were performed with STATA/MP 16.1.
RESULTS: We found high variability in the text consistency of AI chatbot 
responses (median range 26%-68%). Intra-rater reliability ranged from "almost 
perfect" to "substantial," while inter-rater reliability varied from "almost 
perfect" to "moderate." Perplexity had the highest match rate at 67%, followed 
by Google Gemini at 63%, and Microsoft Copilot at 44%. ChatGPT-3.5, ChatGPT-4o, 
and Claude showed the lowest performance, each with a 33% match rate.
CONCLUSIONS: Despite the variability in internal consistency and good intra- and 
inter-rater reliability, the AI Chatbots' recommendations often did not align 
with CPGs recommendations for diagnosing and treating lumbosacral radicular 
pain. Clinicians and patients should exercise caution when relying on these AI 
models, since one to two-thirds of the recommendations provided may be 
inappropriate or misleading according to specific chatbots.

© 2025 Rossettini, Bargeri, Cook, Guida, Palese, Rodeghiero, Pillastrini, 
Turolla, Castellini and Gianola.

DOI: 10.3389/fdgth.2025.1574287
PMCID: PMC12245906
PMID: 40657647

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


18. Digit Health. 2025 Mar 2;11:20552076251324444. doi: 10.1177/20552076251324444. 
eCollection 2025 Jan-Dec.

Application of large language models in healthcare: A bibliometric analysis.

Zhang L(1)(2), Zhao Q(3), Zhang D(1)(2), Song M(1)(2), Zhang Y(4), Wang X(1)(2).

Author information:
(1)Department of the Third Pulmonary Disease, Shenzhen Third People's Hospital, 
Shenzhen, Guangdong Province, China.
(2)Shenzhen Clinical Research Center for Tuberculosis, Shenzhen, Guangdong 
Province, China.
(3)Acacia Lab for Implementation Science, School of Public Health Management, 
Southern Medical University, Guangzhou, Guangdong, China.
(4)School of Humanities Changzhou Vocational Institute of Textile and Garment 
Changzhou, China.

OBJECTIVE: The objective is to provide an overview of the application of large 
language models (LLMs) in healthcare by employing a bibliometric analysis 
methodology.
METHOD: We performed a comprehensive search for peer-reviewed English-language 
articles using PubMed and Web of Science. The selected articles were 
subsequently clustered and analyzed textually, with a focus on lexical 
co-occurrences, country-level and inter-author collaborations, and other 
relevant factors. This textual analysis produced high-level concept maps that 
illustrate specific terms and their interconnections.
FINDINGS: Our final sample comprised 371 English-language journal articles. The 
study revealed a sharp rise in the number of publications related to the 
application of LLMs in healthcare. However, the development is geographically 
imbalanced, with a higher concentration of articles originating from developed 
countries like the United States, Italy, and Germany, which also exhibit strong 
inter-country collaboration. LLMs are applied across various specialties, with 
researchers investigating their use in medical education, diagnosis, treatment, 
administrative reporting, and enhancing doctor-patient communication. 
Nonetheless, significant concerns persist regarding the risks and ethical 
implications of LLMs, including the potential for gender and racial bias, as 
well as the lack of transparency in the training datasets, which can lead to 
inaccurate or misleading responses.
CONCLUSION: While the application of LLMs in healthcare is promising, the 
widespread adoption of LLMs in practice requires further improvements in their 
standardization and accuracy. It is critical to establish clear accountability 
guidelines, develop a robust regulatory framework, and ensure that training 
datasets are based on evidence-based sources to minimize risk and ensure ethical 
and reliable use.

© The Author(s) 2025.

DOI: 10.1177/20552076251324444
PMCID: PMC11873863
PMID: 40035041

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


19. Semin Plast Surg. 2025 Aug 8;39(3):190-198. doi: 10.1055/s-0045-1810062. 
eCollection 2025 Aug.

Artificial Intelligence and Machine Learning in Reconstructive Microsurgery.

Lin TC(1), Yang HA(1), Huang RW(1), Lin CH(1).

Author information:
(1)Department of Plastic and Reconstructive Surgery, Center for Vascularized 
Composite Allotransplantation, Chang Gung Memorial Hospital, Chang Gung Medical 
College and Chang Gung University, Taoyuan, Taiwan.

Artificial intelligence (AI) and machine learning (ML) technologies are 
transforming reconstructive microsurgery through data-driven approaches that 
enhance precision and standardize clinical workflows. These innovations address 
long-standing challenges, including subjective assessment methodologies, 
operator-dependent decision-making, and inconsistent monitoring protocols across 
the perioperative continuum. Contemporary applications demonstrate remarkable 
capabilities in preoperative risk stratification, with ML algorithms achieving 
high predictive accuracy for complications such as flap loss and donor site 
morbidity. CNNs have revolutionized perforator localization, with advanced 
models achieving Dice coefficients of 91.87% in anatomical structure detection 
from CT angiography. Intraoperative assistance through AI-enhanced robotic 
platforms provides submillimeter precision and tremor filtration, particularly 
beneficial in supermicrosurgery involving vessels measuring 0.3- to 0.8-mm 
diameter. Postoperative monitoring represents a particularly promising domain, 
where AI-based image analysis systems achieve 98.4% accuracy in classifying flap 
perfusion status and detecting early vascular compromise. Automated platforms 
may enable continuous surveillance with reduced clinical workload while 
maintaining superior consistency compared with traditional subjective methods. 
Patient communication benefits from AI-driven visual simulation and large 
language models (LLMs) that generate personalized educational materials, 
enhancing informed consent processes. Critical implementation challenges include 
data quality, algorithmic bias, and inherent dataset imbalance, where 
complications represent rare but clinically crucial events. Future advancement 
requires explainable AI systems, multi-institutional collaboration, and 
comprehensive regulatory frameworks. When thoughtfully integrated, AI serves as 
a powerful augmentation tool that elevates microsurgical precision and outcomes 
while preserving the fundamental importance of surgical expertise and clinical 
judgment.

Thieme. All rights reserved.

DOI: 10.1055/s-0045-1810062
PMCID: PMC12334263
PMID: 40786023

Conflict of interest statement: Conflict of Interest None declared.


20. Semin Nucl Med. 2025 May;55(3):377-386. doi: 10.1053/j.semnuclmed.2025.02.007. 
Epub 2025 Mar 11.

The Role of AI in Lymphoma: An Update.

Cairns J(1), Frood R(1), Patel C(2), Scarsbrook A(3).

Author information:
(1)Faculty of Medicine, University of Leeds, Leeds LS2 9JT, England; Department 
of Radiology, St James's University Hospital, Leeds Teaching Hospitals NHS 
Trust, Leeds, LS9 7TF, England.
(2)Department of Radiology, St James's University Hospital, Leeds Teaching 
Hospitals NHS Trust, Leeds, LS9 7TF, England.
(3)Faculty of Medicine, University of Leeds, Leeds LS2 9JT, England; Department 
of Radiology, St James's University Hospital, Leeds Teaching Hospitals NHS 
Trust, Leeds, LS9 7TF, England. Electronic address: a.f.scarsbrook@leeds.ac.uk.

Malignant lymphomas encompass a range of malignancies with incidence rising 
globally, particularly with age. In younger populations, Hodgkin and Burkitt 
lymphomas predominate, while older populations more commonly experience subtypes 
such as diffuse large B-cell, follicular, marginal zone, and mantle cell 
lymphomas. Positron emission tomography/computed tomography (PET/CT) using [18F] 
fluorodeoxyglucose (FDG) is the gold standard for staging, treatment response 
assessment, and prognostication in lymphoma. However, interpretation of PET/CT 
is complex, time-consuming, and reliant on expert imaging specialists, 
exacerbating challenges associated with workforce shortages worldwide. 
Artificial intelligence (AI) offers transformative potential across multiple 
aspects of PET/CT imaging in this setting. AI applications in appointment 
planning have demonstrated utility in reducing nonattendance rates and improving 
departmental efficiency. Advanced reconstruction techniques leveraging 
convolutional neural networks (CNNs) enable reduced injected activities of 
radiopharmaceutical and patient dose whilst maintaining diagnostic accuracy, 
particularly benefiting younger patients requiring multiple scans. Automated 
segmentation tools, predominantly using 3D U-Net architectures, have improved 
quantification of metrics such as total metabolic tumour volume (TMTV) and total 
lesion glycolysis (TLG), facilitating prognostication and treatment 
stratification. Despite these advancements, challenges remain, including 
variability in segmentation performance, impact on Deauville Score 
interpretation, and standardization of TMTV/TLG measurements. Emerging large 
language models (LLMs) also show promise in enhancing PET/CT reporting, 
converting free-text reports into structured formats, and improving patient 
communication. Further research is required to address limitations such as 
AI-induced errors, physiological uptake differentiation, and the integration of 
AI models into clinical workflows. With robust validation and harmonization, AI 
integration could significantly enhance lymphoma care, improving diagnostic 
precision, workflow efficiency, and patient outcomes.

Copyright © 2025 The Author(s). Published by Elsevier Inc. All rights reserved.

DOI: 10.1053/j.semnuclmed.2025.02.007
PMID: 40069036 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: Professor Andrew Scarsbrook reports 
financial support was provided by Cancer Research UK. Professor Andrew 
Scarsbrook reports financial support was provided by Leeds Biomedical Research 
Centre. Dr Russell Frood reports financial support was provided by Cancer 
Research UK. Dr James Cairns reports financial support was provided by National 
Institute for Health and Care Research. If there are other authors, they declare 
that they have no known competing financial interests or personal relationships 
that could have appeared to influence the work reported in this paper.


21. Int J Nurs Stud. 2025 Dec 19;176:105322. doi: 10.1016/j.ijnurstu.2025.105322. 
Online ahead of print.

The use of large language models in clinical documentation: A scoping review.

Woo BFY(1), Cato K(2), Cho H(3), You SB(3), Song J(3).

Author information:
(1)University of Pennsylvania School of Nursing, Philadelphia, United States of 
America; The Commonwealth Fund, New York, United States of America; Nursing 
Administration, Ng Teng Fong General Hospital, Singapore, Singapore. Electronic 
address: brigitte_woo@nuhs.edu.sg.
(2)University of Pennsylvania School of Nursing, Philadelphia, United States of 
America; Children's Hospital of Philadelphia, Philadelphia, United States of 
America.
(3)University of Pennsylvania School of Nursing, Philadelphia, United States of 
America.

BACKGROUND: Clinical documentation is essential for safe, high-quality care but 
has become increasingly complex, contributing to clinician burnout. Large 
language models offer potential to ease documentation by generating summaries, 
structuring data, and ensuring compliance. However, concerns remain regarding 
accuracy, bias, privacy, and regulatory risks.
OBJECTIVE: To map current literature on large language models applications in 
clinical documentation, evaluating their benefits, limitations, and ethical 
considerations.
INFORMATION SOURCES: Five electronic databases (i.e., PubMed, Scopus, CINAHL, 
Cochrane Library, and IEEE Xplore) covering peer-reviewed literature published 
in English between January 2009 and August 2025.
METHODS: This scoping review followed Arksey and OMalleys framework and was 
reported in accordance with PRISMA-ScR guidelines. Screening, data extraction, 
and quality appraisal were conducted independently by multiple reviewers using 
Joanna Briggs Institute tools. Findings were synthesized using descriptive and 
narrative approaches.
RESULTS: Forty-one studies met inclusion criteria, most originating from the 
United States. Large language models were primarily applied to clinical note 
generation, discharge summaries, and provider-patient encounter documentation. 
Key evaluation metrics included content accuracy, linguistic quality, and 
summarization performance. Large language models demonstrated potential to 
improve documentation efficiency and readability, with some studies reporting up 
to 40 % time savings. However, concerns about factual inaccuracies, 
hallucinations, and reduced performance in complex cases were common. Clinician 
perceptions were mixed. Some found notes generated by large language models 
helpful and well-structured, while others raised concerns about reliability, 
liability, and loss of clinical nuance. Ethical challenges included data 
privacy, security, and algorithmic bias, with varying levels of compliance 
across settings.
CONCLUSIONS: Large language models hold significant promise for enhancing 
clinical documentation by improving efficiency, standardization, and clarity. 
However, their safe and effective use requires rigorous attention to accuracy, 
ethical safeguards, and clinician trust. Integration must support, rather than 
supplant, clinical reasoning and patient-centered care. Co-design with 
clinicians, real-world evaluation, and artificial intelligence literacy are 
essential to ensure that these technologies augment, not erode, professional 
judgment and care quality.
REGISTRATION: Open Science Framework Registries (https://osf.io/m4h3q).

Copyright © 2025 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.ijnurstu.2025.105322
PMID: 41512731

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


22. Ther Adv Musculoskelet Dis. 2025 Apr 21;17:1759720X251331529. doi: 
10.1177/1759720X251331529. eCollection 2025.

RAGing ahead in rheumatology: new language model architectures to tame 
artificial intelligence.

Benavent D(1), Venerito V(2), Michelena X(3)(4)(5).

Author information:
(1)Department of Rheumatology, Hospital Universitari de Bellvitge, Barcelona, 
Spain.
(2)Department of Precision and Regenerative Medicine and Ionian Area, Polyclinic 
Hospital, University of Bari, Bari, Italy.
(3)Catalan Health Service, Government of Catalonia, Gran Via de les Corts 
Catalanes, 587, Barcelona 08007, Spain.
(4)Digitalization for the Sustainability of the Healthcare System (DS3), 
Barcelona, Spain.
(5)Rheumatology Research Group, Vall d'Hebron Research Institute, Barcelona, 
Spain.

Artificial intelligence (AI) is increasingly transforming rheumatology with 
research on disease detection, monitoring, and outcome prediction through the 
analysis of large datasets. The advent of generative models and large language 
models (LLMs) has expanded AI's capabilities, particularly in natural language 
processing (NLP) tasks such as question-answering and medical literature 
synthesis. While NLP has shown promise in identifying rheumatic diseases from 
electronic health records with high accuracy, LLMs face significant challenges, 
including hallucinations and a lack of domain-specific knowledge, which limit 
their reliability in specialized medical fields like rheumatology. 
Retrieval-augmented generation (RAG) emerges as a solution to these limitations 
by integrating LLMs with real-time access to external, domain-specific 
databases. RAG enhances the accuracy and relevance of AI-generated responses by 
retrieving pertinent information during the generation process, reducing 
hallucinations, and improving the trustworthiness of AI applications. This 
architecture allows for precise, context-aware outputs and can handle 
unstructured data effectively. Despite its success in other industries, the 
application of RAG in medicine, and specifically in rheumatology, remains 
underexplored. Potential applications in rheumatology include retrieving 
up-to-date clinical guidelines, summarizing complex patient histories from 
unstructured data, aiding in patient identification for clinical trials, 
enhancing pharmacovigilance efforts, and supporting personalized patient 
education. RAG also offers advantages in data privacy by enabling local data 
handling and reducing reliance on large, general-purpose models. Future 
directions involve integrating RAG with fine-tuned, smaller LLMs and exploring 
multimodal models that can process diverse data types. Challenges such as 
infrastructure costs, data privacy concerns, and the need for specialized 
evaluation metrics must be addressed. Nevertheless, RAG presents a promising 
opportunity to improve AI applications in rheumatology, offering a more precise, 
accountable, and sustainable approach to integrating advanced language models 
into clinical practice and research.

© The Author(s), 2025.

DOI: 10.1177/1759720X251331529
PMCID: PMC12033461
PMID: 40292012

Conflict of interest statement: Dr D.B. received grants/speaker/research support 
from Abbvie, Lilly, Novartis, Pfizer, and UCB. He works as a part-time advisor 
at Savana, a company focused on natural language processing in medicine. Dr V.V. 
and Dr X.M. declare no potential conflicts of interest with respect to the 
research, authorship, and/or publication of this article.


23. NPJ Digit Med. 2025 Aug 8;8(1):509. doi: 10.1038/s41746-025-01840-7.

Accelerating clinical evidence synthesis with large language models.

Wang Z(1)(2), Cao L(1), Danek B(1)(2), Jin Q(3), Lu Z(3), Sun J(4)(5)(6).

Author information:
(1)Siebel School of Computing and Data Science, University of Illinois 
Urbana-Champaign, Urbana, IL, USA.
(2)Keiji.AI Inc, Seattle, USA.
(3)Division of Intramural Research, National Library of Medicine, National 
Institutes of Health, Bethesda, MD, USA.
(4)Siebel School of Computing and Data Science, University of Illinois 
Urbana-Champaign, Urbana, IL, USA. jimeng@illinois.edu.
(5)Carle Illinois College of Medicine, University of Illinois Urbana-Champaign, 
Urbana, IL, USA. jimeng@illinois.edu.
(6)Keiji.AI Inc, Seattle, USA. jimeng@illinois.edu.

Clinical evidence synthesis largely relies on systematic reviews (SR) of 
clinical studies from medical literature. Here, we propose a generative 
artificial intelligence (AI) pipeline named TrialMind to streamline study 
search, study screening, and data extraction tasks in SR. We chose published SRs 
to build TrialReviewBench, which contains 100 SRs and 2,220 clinical studies. 
For study search, it achieves high recall rates (Ours 0.711-0.834 v.s. Human 
baseline 0.138-0.232). For study screening, TrialMind beats previous document 
ranking methods in a 1.5-2.6 fold change. For data extraction, it outperforms a 
GPT-4's accuracy by 16-32%. In a pilot study, human-AI collaboration with 
TrialMind improved recall by 71.4% and reduced screening time by 44.2%, while in 
data extraction, accuracy increased by 23.5% with a 63.4% time reduction. 
Medical experts preferred TrialMind's synthesized evidence over GPT-4's in 
62.5%-100% of cases. These findings show the promise of accelerating clinical 
evidence synthesis driven by human-AI collaboration.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01840-7
PMCID: PMC12331930
PMID: 40775042

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


24. JMIR Med Educ. 2025 Aug 15;11:e70079. doi: 10.2196/70079.

Quo Vadis, AI-Empowered Doctor?

Takahashi G(#)(1), von Liechti L(#)(1), Tarshizi E(#)(1).

Author information:
(1)Shiley-Marcos School of Engineering, University of San Diego, 5998 Alcalá 
Park, San Diego, CA, 92110, United States, 1 503 847 3079.
(#)Contributed equally

In the first decade of this century, physicians maintained considerable 
professional autonomy, enabling discretionary evaluation and implementation of 
new technologies according to individual practice requirements. The past decade, 
however, has witnessed significant restructuring of medical practice patterns in 
the United States, with most physicians transitioning to employed status. 
Concurrently, technological advances and other incentives drove the 
implementation of electronic systems into the clinic, which these physicians 
were compelled to integrate. Health care practitioners have now been introduced 
to applications based on large language models, largely driven by artificial 
intelligence (AI) developers as well as established electronic health record 
vendors eager to incorporate these innovations. Although generative AI 
assistance promises enhanced clinical efficiency and diagnostic precision, its 
rapid advancement may potentially redefine clinical provider roles and transform 
workflows, as it has already altered expectations of physician productivity, as 
well as introduced unprecedented liability considerations. Recognition of the 
input of physicians and other clinical stakeholders in this nascent stage of AI 
integration is essential. This requires a more comprehensive understanding of AI 
as a sophisticated clinical tool. Accordingly, we advocate for its systematic 
incorporation into standard medical curricula.

© Gary Takahashi, Laurentius von Liechti, Ebrahim Tarshizi. Originally published 
in JMIR Medical Education (https://mededu.jmir.org).

DOI: 10.2196/70079
PMCID: PMC12356520
PMID: 40815794 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


25. Front Digit Health. 2025 Sep 30;7:1659134. doi: 10.3389/fdgth.2025.1659134. 
eCollection 2025.

Large language models in real-world clinical workflows: a systematic review of 
applications and implementation.

Artsi Y(1), Sorin V(2), Glicksberg BS(3)(4)(5), Korfiatis P(2), Nadkarni 
GN(3)(4)(5), Klang E(3)(4)(5).

Author information:
(1)Azrieli Faculty of Medicine, Bar-Ilan University, Zefat, Israel.
(2)Department of Radiology, Mayo Clinic, Rochester, MN, United States.
(3)The Charles Bronfman Institute of Personalized Medicine, Icahn School of 
Medicine at Mount Sinai, New York, NY, United States.
(4)The Windreich Department of Artificial Intelligence and Human Health, Mount 
Sinai Medical Center, New York, NY, United States.
(5)The Hasso Plattner Institute for Digital Health at Mount Sinai, Icahn School 
of Medicine at Mount Sinai, New York, NY, United States.

BACKGROUND: Large language models (LLMs) offer promise for enhancing clinical 
care by automating documentation, supporting decision-making, and improving 
communication. However, their integration into real-world healthcare workflows 
remains limited and under characterized. This systematic review aims to evaluate 
the literature on real-world implementation of LLMs in clinical workflows, 
including their use cases, clinical settings, observed outcomes, and challenges.
METHODS: We searched MEDLINE, Scopus, Web of Science, and Google Scholar for 
studies published between January 2015 and April 2025 that assessed LLMs in 
real-world clinical applications. Inclusion criteria were peer-reviewed, 
full-text studies in English reporting empirical implementation of LLMs in 
clinical settings. Study quality and risk of bias were assessed using the 
PROBAST tool.
RESULTS: Four studies published between 2024 and 2025 met inclusion criteria. 
All used generative pre-trained transformers (GPTs). Reported applications 
included outpatient communication, mental health support, inbox message 
drafting, and clinical data extraction. LLM deployment was associated with 
improvements in operational efficiency, user satisfaction, and reduced workload. 
However, challenges included performance variability across data types, 
limitations in generalizability, regulatory delays, and lack of post-deployment 
monitoring.
CONCLUSIONS: Early evidence suggests that LLMs can enhance clinical workflows, 
but real-world adoption remains constrained by systemic, technical, and 
regulatory barriers. To support safe and scalable use, future efforts should 
prioritize standardized evaluation metrics, multi-site validation, human 
oversight, and implementation frameworks tailored to clinical settings.
SYSTEMATIC REVIEW REGISTRATION: 
https://www.crd.york.ac.uk/PROSPERO/recorddashboard, PROSPERO CRD420251030069.

© 2025 Artsi, Sorin, Glicksberg, Korfiatis, Nadkarni and Klang.

DOI: 10.3389/fdgth.2025.1659134
PMCID: PMC12519456
PMID: 41098649

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


26. Digit Health. 2025 Apr 21;11:20552076251337177. doi: 10.1177/20552076251337177. 
eCollection 2025 Jan-Dec.

Enhancing medical AI with retrieval-augmented generation: A mini narrative 
review.

Gargari OK(1), Habibi G(1).

Author information:
(1)Farzan Artificial Intelligence Team, Farzan Clinical Research Institute, 
Tehran, Iran.

Retrieval-augmented generation (RAG) is a powerful technique in artificial 
intelligence (AI) and machine learning that enhances the capabilities of large 
language models (LLMs) by integrating external data sources, allowing for more 
accurate, contextually relevant responses. In medical applications, RAG has the 
potential to improve diagnostic accuracy, clinical decision support, and patient 
care. This narrative review explores the application of RAG across various 
medical domains, including guideline interpretation, diagnostic assistance, 
clinical trial eligibility screening, clinical information retrieval, and 
information extraction from scientific literature. Studies highlight the 
benefits of RAG in providing accurate, up-to-date information, improving 
clinical outcomes, and streamlining processes. Notable applications include 
GPT-4 models enhanced with RAG to interpret hepatologic guidelines, assist in 
differential diagnosis, and aid in clinical trial screening. Furthermore, 
RAG-based systems have demonstrated superior performance over traditional 
methods in tasks such as patient diagnosis, clinical decision-making, and 
medical information extraction. Despite its advantages, challenges remain, 
particularly in model evaluation, cost-efficiency, and reducing AI 
hallucinations. This review emphasizes the potential of RAG in advancing medical 
AI applications and advocates for further optimization of retrieval mechanisms, 
embedding models, and collaboration between AI researchers and healthcare 
professionals to maximize RAG's impact on medical practice.

© The Author(s) 2025.

DOI: 10.1177/20552076251337177
PMCID: PMC12059965
PMID: 40343063

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


27. Gastroenterology. 2025 Aug;169(3):502-517.e1. doi: 10.1053/j.gastro.2025.03.038. 
Epub 2025 Apr 15.

Generative Artificial Intelligence in Clinical Medicine and Impact on 
Gastroenterology.

Soroush A(1), Giuffrè M(2), Chung S(2), Shung DL(3).

Author information:
(1)Division of Data-Driven and Digital Medicine, Icahn School of Medicine at 
Mount Sinai, New York, New York; Henry D. Janowitz Division of Gastroenterology, 
Icahn School of Medicine at Mount Sinai, New York, New York; Charles Bronfman 
Institute for Personalized Medicine, Icahn School of Medicine at Mount Sinai, 
New York, New York.
(2)Section of Digestive Diseases, Department of Medicine, Yale School of 
Medicine, New Haven, Connecticut.
(3)Section of Digestive Diseases, Department of Medicine, Yale School of 
Medicine, New Haven, Connecticut; Department of Biomedical Informatics and Data 
Science, Yale School of Medicine, New Haven, Connecticut. Electronic address: 
dennis.shung@yale.edu.

The pace of artificial intelligence (AI) integration into health care has 
accelerated with rapid advances in generative AI (genAI). Gastroenterology and 
hepatology in particular will be transformed due to the multimodal workflows 
that integrate endoscopic video, radiologic imaging, tabular data, and 
unstructured note text. GenAI will impact the entire spectrum of clinical 
experience, from administrative tasks, diagnostic guidance, and treatment 
recommendations. Unlike traditional machine learning approaches, genAI is more 
flexible, with one platform able to be used across multiple tasks. Initial 
evidence suggests benefits in lower-level administrative tasks, such as clinical 
documentation, medical billing, and scheduling; and information tasks, such as 
patient education and summarization of the medical literature. No evidence 
exists for genAI solutions for more complex tasks relevant to clinical care, 
such as clinical reasoning for diagnostic and treatment decisions that may 
affect patient outcomes. Challenges of output reliability, data privacy, and 
useful integration remain; potential solutions include robust validation, 
regulatory oversight, and "human-AI teaming" strategies to ensure safe, 
effective deployment. We remain optimistic in the potential of genAI to augment 
clinical expertise due to the adaptability of genAI to handle multiple data 
modalities to obtain and focus relevant information flows and the human-friendly 
interfaces that facilitate ease of use. We believe that the potential of genAI 
for dynamic human-algorithmic interactions may allow for a degree of 
clinician-directed customization to enhance human presence.

Copyright © 2025 AGA Institute. Published by Elsevier Inc. All rights reserved.

DOI: 10.1053/j.gastro.2025.03.038
PMID: 40245953 [Indexed for MEDLINE]


28. Diagnostics (Basel). 2025 May 24;15(11):1320. doi: 10.3390/diagnostics15111320.

Do LLMs Have 'the Eye' for MRI? Evaluating GPT-4o, Grok, and Gemini on Brain MRI 
Performance: First Evaluation of Grok in Medical Imaging and a Comparative 
Analysis.

Sozer A(1), Sahin MC(2), Sozer B(3), Erol G(4), Tufek OY(5), Nernekli K(6), 
Demirtas Z(5), Celtikci E(5).

Author information:
(1)Department of Neurosurgery, Sincan Training and Research Hospital, Ankara 
06949, Turkey.
(2)Department of Neurosurgery, Kulu State Hospital, Konya 42780, Turkey.
(3)Ankara Medipol University Faculty of Medicine, Ankara 06050, Turkey.
(4)Department of Neurosurgery, Adiyaman Training and Research Hospital, Adiyaman 
02100, Turkey.
(5)Department of Neurosurgery, Gazi University Faculty of Medicine, Ankara 
06560, Turkey.
(6)Department of Radiology, Stanford University School of Medicine, Stanford, CA 
94305, USA.

Background/Objectives: Large language models (LLMs) are revolutionizing the 
world and the field of medicine while constantly improving themselves. With 
recent advancements in image interpretation, evaluating the reasoning 
capabilities of these models and benchmarking their performance on brain MRI 
tasks has become crucial, as they may be utilized-albeit off-label-for patient 
care by both neurosurgeons and non-neurosurgeons. Methods: ChatGPT-4o, Grok, and 
Gemini were presented with 35,711 slices of brain MRI, including various 
pathologies and normal MRIs. Models were asked to identify the MRI sequence and 
determine the presence of pathology. Their individual performances were measured 
and compared with one another. Results: GPT refused to answer 28.02% of the 
slices despite three attempts, whereas Grok and Gemini provided responses on the 
first attempt for every slice. Gemini achieved 74.54% pathology prediction and 
46.38% sequence prediction accuracy. GPT-4o achieved 74.33% pathology prediction 
and 85.98% sequence prediction accuracy for questions that it had answered 
(53.50% and 61.67% in total, respectively). Grok achieved 65.64% pathology 
prediction and 66.23% sequence prediction accuracy. Conclusions: The image 
interpretation capabilities of the investigated LLMs are limited for now and 
require further refinement before competing with specifically trained and 
fine-tuned dedicated applications. Amongst them, Gemini outperforms the others 
in pathology prediction while Grok outperforms others in sequence prediction. 
These limitations should be kept in mind if use during patient care is planned.

DOI: 10.3390/diagnostics15111320
PMCID: PMC12154409
PMID: 40506892

Conflict of interest statement: The authors declare no conflicts of interest.


29. Digit Health. 2025 Nov 3;11:20552076251394631. doi: 10.1177/20552076251394631. 
eCollection 2025 Jan-Dec.

Reimagining cancer treatments in the era of generative AI.

Derbal Y(1).

Author information:
(1)Ted Rogers School of Information Technology Management, Toronto Metropolitan 
University, Toronto, ON, Canada.

Significant advances in the treatment of cancer have been achieved as reflected 
by the ever-expanding space of cancer therapeutics being available to cancer 
patients. Often, however, it is not clear which patient would respond to which 
drug and what combination of drugs will improve patient outcomes. Furthermore, 
while many of these drugs are initially effective, therapeutic resistance is 
often inevitable due to the evolving nature of cancer. Generative artificial 
intelligence (GenAI) powered by the increasingly large amount of accumulating 
clinical, molecular, and radiomics data about cancer patients and their 
treatments may serve as the kernel of rapid learning decision-support systems 
that could enable personalized cancer treatments to counter therapeutic 
resistance and overcome the shortcomings of the current standard of care. This 
perspective is explored in the context of current advances of AI applications in 
oncology and the potential of GenAI learning and inferencing capabilities to 
support patient-tailored dynamic cancer treatments. A discussion of this vision 
is elaborated with respect to issues pertinent to GenAI use in real-world 
clinical settings, including clinical validation, data curation, and sharing, 
large language model hallucinations as well as ethical concerns and 
considerations such as privacy, bias, transparency, and accountability.

© The Author(s) 2025.

DOI: 10.1177/20552076251394631
PMCID: PMC12583873
PMID: 41195370

Conflict of interest statement: The author declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


30. BMC Nurs. 2025 Jul 19;24(1):947. doi: 10.1186/s12912-025-03594-9.

Generative artificial intelligence in cardiovascular specialty care: a scoping 
review.

Zheng X(#)(1)(2), Zou H(#)(1), Wu L(1), Dong P(2), Yuan W(2), Chen Y(3).

Author information:
(1)Xiamen Cardiovascular Hospital, Xiamen University, Xiamen, China.
(2)School of Nursing, Fujian University of Traditional Chinese Medicine, Fuzhou, 
China.
(3)Xiamen Cardiovascular Hospital, Xiamen University, Xiamen, China. 
28837445@qq.com.
(#)Contributed equally

BACKGROUND: The current research on the application of generative artificial 
intelligence in has shown rapid growth, and most of the existing research 
focuses on general nursing scenarios. And is still in the exploratory stage in 
the specialty field and lacks a systematic compendium of its application in the 
field of cardiovascular specialty nursing.
OBJECTIVE: A review of studies related to the application of generative 
artificial intelligence in the field of specialized cardiovascular nursing 
informing relevant research and practice.
METHODS: This review was conducted according to the JBI guidelines for scoping 
reviews, using the PRISMA-ScR reporting tool.The databases searched included 
PubMed, Embase, Web of Science, APA PsycNetInfo (Ovid), the Cochrane Library, 
MEDLINE (Ovid), CINAHL, SinoMed, CNKI, WanFang databases, peer-reviewed studies 
written in English and Chinese from the time of database construction until 
March 29, 2025. Eligible studies were screened by title and abstract, and 
full-text screening was performed by two independent evaluators.
RESULTS: Nineteen studies were included in this review, and the main application 
tools were 9 text generation models, 3 multimodal generation models, 4 temporal 
prediction models, and 3 chatbots. The application scenarios mainly include 
clinical decision support, patient health management, and nursing education and 
counseling. Generative artificial intelligence has outstanding effects in 
reducing nursing workload and precise intervention, but there are limitations in 
the field of health education and counseling, such as logical disconnection, 
poor information quality, and lack of humanistic care.
CONCLUSIONS: Generative artificial intelligence provides technical support for 
the intelligent transformation of cardiovascular specialty care, especially 
effective in the areas of clinical decision support and patient health 
management. There is an urgent need to solve the existing problems to promote 
its in-depth application in this field, and it is suggested that future research 
focuses on the construction of specialized multimodal models.

© 2025. The Author(s).

DOI: 10.1186/s12912-025-03594-9
PMCID: PMC12275405
PMID: 40684147

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


31. Artif Intell Med. 2025 Oct;168:103222. doi: 10.1016/j.artmed.2025.103222. Epub 
2025 Aug 1.

Reporting guideline for chatbot health advice studies: The CHART statement.

CHART Collaborative; Huo B(1), Collins G(2), Chartash D(3), Thirunavukarasu 
A(4), Flanagin A(5), Iorio A(6), Cacciamani G(7), Chen X(8), Liu N(9), Mathur 
P(10), Chan AW(11), Laine C(12), Pacella D(13), Berkwits M(14), Antoniou SA(15), 
Camaradou JC(16), Canfield C(17), Mittelman M(18), Feeney T(19), Loder E(20), 
Agha R(21), Saha A(22), Mayol J(23), Sunjaya A(24), Harvey H(25), Ng JY(26), 
McKechnie T(27), Lee Y(28), Verma N(29), Stiglic G(30), McCradden M(31), Ramji 
K(32), Boudreau V(27), Ortenzi M(33), Meerpohl J(34), Vandvik PO(35), Agoritsas 
T(36), Samuel D(37), Frankish H(38), Anderson M(39), Yao X(22), Loeb S(40), 
Lokker C(6), Liu X(41), Guallar E(42), Guyatt G(43).

Author information:
(1)Division of General Surgery, Department of Surgery, McMaster University, 
Hamilton, Canada. Electronic address: brighthuo@dal.ca.
(2)UK EQUATOR Centre, University of Oxford, Oxford, UK; Centre for Statistics in 
Medicine, Nuffield Department of Orthopaedics, Rheumatology & Musculoskeletal 
Sciences, Botnar Research Centre, University of Oxford, Oxford, UK.
(3)Department of Biomedical Informatics and Data Science, Yale University School 
of Medicine, USA.
(4)Nuffield Department of Clinical Neurosciences, Medical Sciences Division, 
University of Oxford, Oxford, UK.
(5)JAMA and JAMA Network, American Medical Association, USA.
(6)Department of Health Research Methods, Evidence, and Impact, Department of 
Medicine, McMaster University, Canada.
(7)USC Institute of Urology and Catherine and Joseph Aresty, Department of 
Urology, Keck School of Medicine, University of Southern California, Los 
Angeles, CA, USA; Artificial Intelligence Center at USC Urology, USC Institute 
of Urology, University of Southern California, Los Angeles, CA, USA.
(8)Sports Medicine Center, West China Hospital, Sichuan University, Chengdu, 
China; Department of Orthopedics and Orthopedic Research Institute, West China 
Hospital, Sichuan University, Chengdu, China.
(9)Duke-NUS Medical School, National University of Singapore, Singapore, 
Singapore.
(10)Cleveland Clinic, Case Western Reserve University, USA.
(11)Dept of Medicine, Women's College Research Institute, University of Toronto, 
Canada.
(12)Annals of Internal Medicine, American College of Physicians, USA; American 
College of Physicians, USA.
(13)Department of Public Health, University of Naples Federico II, Italy.
(14)Director, Office of Science Dissemination, Office of Science, Centers for 
Disease Control and Prevention, Atlanta, GA, USA.
(15)Department of General Surgery, Papageorgiou General Hospital, Thessaloniki, 
Greece.
(16)British Psychological Society, University of Plymouth, UK.
(17)Innovation Support Unit, Department of Family Practice, University of 
British Columbia, Canada.
(18)Patient SME, Independent Cybersecurity Professional, USA.
(19)The BMJ, London, UK; Department of Epidemiology, Gillings School of Global 
Public Health, University of North Carolina at Chapel Hill, Chapel Hill, NC, 
USA.
(20)The BMJ, London, UK; Department of Neurology, Brigham and Women's Hospital, 
Boston, MA, USA.
(21)International Journal of Surgery, USA; Eworkflow Ltd, London, UK.
(22)Department of Oncology, McMaster University, Canada.
(23)Hospital Clinico San Carlos, Instituto de Investigación Sanitaria San 
Carlos, Facultad de Medicina Universidad Complutense de Madrid, Spain.
(24)The George Institute for Global Health, Tyree Institute of Health 
Engineering, UNSW Engineering, School of Population Health, UNSW Medicine and 
Health, Australia.
(25)Hardian Health, UK.
(26)Centre for Journalology, Ottawa Hospital Research Institute, Ottawa, Canada.
(27)Division of General Surgery, Department of Surgery, McMaster University, 
Hamilton, Canada.
(28)Division of General Surgery, Department of Surgery, McMaster University, 
Hamilton, Canada; Digestive Diseases Institute, Cleveland Clinic, Cleveland, OH, 
USA.
(29)Postgraduate Institute of Medical Education and Research, Chandigarh, India.
(30)University of Maribor, Slovenia.
(31)Australian Institute for Machine Learning (AIML), Australia.
(32)Phelix AI, Canada.
(33)Università Politecnica delle Marche, Clinica di Chirurgia Generale e 
d'Urgenza, Italy.
(34)Institute for Evidence in Medicine, Medical Center & Faculty of Medicine, 
University of Freiburg, Germany; Cochrane Germany, Cochrane Germany Foundation, 
Freiburg, Germany.
(35)Cochrane Germany, Cochrane Germany Foundation, Freiburg, Germany; MAGIC 
Evidence Ecosystem Foundation, Norway.
(36)Department of Health Research Methods, Evidence, and Impact, Department of 
Medicine, McMaster University, Canada; MAGIC Evidence Ecosystem Foundation, 
Norway; University Hospitals of Geneva, Switzerland.
(37)The Lancet Digital Health, UK.
(38)The Lancet, UK.
(39)NIHR Clinical Lecturer, Health Organisation, Policy, Economics (HOPE), 
Centre for Primary Care & Health Services Research, The University of 
Manchester, UK; Senior Visiting Fellow, LSE Health, London School of Economics 
and Political Science, UK.
(40)New York University, Langone Health, USA.
(41)College of Medicine and Health, University of Birmingham, UK.
(42)School of Global Public Health, New York University, USA.
(43)Department of Health Research Methods, Evidence, and Impact, Department of 
Medicine, McMaster University, Canada; MAGIC Evidence Ecosystem Foundation, 
Norway.

The Chatbot Assessment Reporting Tool (CHART) is a reporting guideline developed 
to provide reporting recommendations for studies evaluating the performance of 
generative artificial intelligence (AI)-driven chatbots when summarizing 
clinical evidence and providing health advice, referred to as Chatbot Health 
Advice (CHA) studies. CHART was developed in several phases after performing a 
comprehensive systematic review to identify variation in the conduct, reporting 
and methodology in CHA studies. Findings from the review were used to develop a 
draft checklist that was revised through an international, multidisciplinary 
modified asynchronous Delphi consensus process of 531 stakeholders, three 
synchronous panel consensus meetings of 48 stakeholders, and subsequent pilot 
testing of the checklist. CHART includes 12 items and 39 subitems to promote 
transparent and comprehensive reporting of CHA studies. These include Title 
(subitem 1a), Abstract/Summary (subitem 1b), Background (subitems 2ab), Model 
Identifiers (subitem 3ab), Model Details (subitems 4abc), Prompt Engineering 
(subitems 5ab), Query Strategy (subitems 6abcd), Performance Evaluation 
(subitems 7ab), Sample Size (subitem 8), Data Analysis (subitem 9a), Results 
(subitems 10abc), Discussion (subitems 11abc), Disclosures (subitem 12a), 
Funding (subitem 12b), Ethics (subitem 12c), Protocol (subitem 12d), and Data 
Availability (subitem 12e). The CHART checklist and corresponding methodological 
diagram were designed to support key stakeholders including clinicians, 
researchers, editors, peer reviewers, and readers in reporting, understanding, 
and interpreting the findings of CHA studies.

Copyright © 2025 The Author(s). Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.artmed.2025.103222
PMID: 40753040 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest All authors 
have completed the ICMJE uniform disclosure form at 
www.icmje.org/disclosure-of-interest/and declare: GSC is a National Institute 
for Health and Care Research (NIHR) Senior Investigator. The views expressed in 
this article are those of the author(s) and not necessarily those of the NIHR, 
or the Department of Health and Social Care; AJT has received funding from 
HealthSense to investigate evidence-based medicine applications of large 
language models. PM is the co-founder of BrainX LLC; AS has received research 
funding from the Australian government and is co-founder of BantingMed Pty Ltd.; 
DS is the Acting Deputy Editor for the Lancet Digital Health; MM has received 
research funding from The Hospital Research Founding Group; TF sits on the 
executive committee of MDEpiNet; HF is a Senior Executive Editor for The Lancet; 
CL is the Editor in Chief of Annals of Internal Medicine; AF is Executive 
Managing Editor and Vice President, Editorial Operations, JAMA and The JAMA 
Network; TF and EL are journal editors for the BMJ; RA is the Editor in Chief of 
International Journal of Surgery; GS is an Executive Editor of Artificial 
Intelligence in Medicine; SL is a paid consultant for Astellas; DP has received 
research funding from the Italian Ministry of University and Research; MO is a 
paid consultant for Theator; TA, POV, GG are board members of the MAGIC Evidence 
Ecosystem Foundation (www.magicproject.org), a non-for profit organisation, 
which conducts research and evidence appraisal and guideline methodology and 
implementation, and which provides a authoring and publication software 
(MAGICapp) for evidence summaries, guidelines and decision aids.


32. Front Digit Health. 2025 Feb 3;7:1482712. doi: 10.3389/fdgth.2025.1482712. 
eCollection 2025.

Comparative analysis of ChatGPT and Gemini (Bard) in medical inquiry: a scoping 
review.

Fattah FH(1)(2), Salih AM(1)(2), Salih AM(1)(3), Asaad SK(1)(2), Ghafour AK(1), 
Bapir R(1)(4)(5), Abdalla BA(1)(5), Othman S(5), Ahmed SM(1)(5), Hasan SJ(1), 
Mahmood YM(1), Kakamad FH(1)(2)(5).

Author information:
(1)Scientific Affairs Department, Smart Health Tower, Sulaymaniyah, Iraq.
(2)College of Medicine, University of Sulaimani, Sulaymaniyah, Iraq.
(3)Civil Engineering Department, College of Engineering, University of 
Sulaimani, Sulaymaniyah, Iraq.
(4)Department of Urology, Sulaimani Surgical Teaching Hospital, Sulaymaniyah, 
Iraq.
(5)Kscien Organization for Scientific Research (Middle East Office), 
Sulaymaniyah, Iraq.

INTRODUCTION: Artificial intelligence and machine learning are popular 
interconnected technologies. AI chatbots like ChatGPT and Gemini show 
considerable promise in medical inquiries. This scoping review aims to assess 
the accuracy and response length (in characters) of ChatGPT and Gemini in 
medical applications.
METHODS: The eligible databases were searched to find studies published in 
English from January 1 to October 20, 2023. The inclusion criteria consisted of 
studies that focused on using AI in medicine and assessed outcomes based on the 
accuracy and character count (length) of ChatGPT and Gemini. Data collected from 
the studies included the first author's name, the country where the study was 
conducted, the type of study design, publication year, sample size, medical 
speciality, and the accuracy and response length.
RESULTS: The initial search identified 64 papers, with 11 meeting the inclusion 
criteria, involving 1,177 samples. ChatGPT showed higher accuracy in radiology 
(87.43% vs. Gemini's 71%) and shorter responses (907 vs. 1,428 characters). 
Similar trends were noted in other specialties. However, Gemini outperformed 
ChatGPT in emergency scenarios (87% vs. 77%) and in renal diets with low 
potassium and high phosphorus (79% vs. 60% and 100% vs. 77%). Statistical 
analysis confirms that ChatGPT has greater accuracy and shorter responses than 
Gemini in medical studies, with a p-value of <.001 for both metrics.
CONCLUSION: This Scoping review suggests that ChatGPT may demonstrate higher 
accuracy and provide shorter responses than Gemini in medical studies.

© 2025 Fattah, Salih, Salih, Asaad, Ghafour, Bapir, Abdalla, Othman, Ahmed, 
Hasan, Mahmood and Kakamad.

DOI: 10.3389/fdgth.2025.1482712
PMCID: PMC11830737
PMID: 39963119

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


33. Nurs Ethics. 2025 Dec;32(8):2490-2515. doi: 10.1177/09697330251366600. Epub 2025 
Aug 27.

Ethical decision-making and artificial intelligence in nursing education: An 
integrative review.

Sengul T(1), Sariköse S(1), Gul A(2).

Author information:
(1)Koç University School of Nursing, Istanbul, Türkiye.
(2)Istanbul Kültür University, Faculty of Health Science, Nursing Department, 
Istanbul, Türkiye.

The integration of artificial intelligence technologies is transforming 
healthcare and nursing education, offering significant benefits for patient care 
and professional development. However, as their use grows, it is crucial to 
address the ethical implications to ensure that fundamental principles like 
justice and integrity are maintained in both clinical and educational settings. 
The aim of this integrative review is to explore the ethical challenges, risks, 
and future perspectives of integrating artificial intelligence into nursing 
education. It examines how these tools influence ethical decision-making 
processes, identifies critical barriers, and proposes strategies for ethical 
implementation. Following an integrative review methodology, 184 articles were 
identified, with 124 remaining after duplicate removal. Fifteen peer-reviewed 
studies analyzed. The methodological quality of the included studies was 
assessed using appropriate Joanna Briggs Institute Critical Appraisal Checklists 
based on study design. Searches were conducted in the PubMed, Cochrane Library, 
MEDLINE (Ovid), Scopus, Web of Science, CINAHL, and ScienceDirect databases from 
2014 to September 2024. Data was analysed using constant comparative analysis. 
The review is registered in the PROSPERO database (CRD42024609440). Guided by 
Rest's Four-Component Model of Moral Behavior, the synthesis was organized under 
four components. Moral Sensitivity included themes such as ethical and 
psychosocial effects, data privacy, equity in access, and cultural sensitivity. 
Moral Judgment covered ethical reasoning skills, AI accuracy, bias, and academic 
integrity. Moral Motivation addressed over-reliance on AI and the need for 
ethical frameworks. Moral Character highlighted educator roles and research 
priorities for ethical AI use. Artificial intelligence offers transformative 
opportunities for nursing education, but also presents significant ethical 
challenges. To ensure its responsible integration, nursing curricula must adopt 
clear ethical frameworks, equip educators with the skills to guide students and 
address disparities in access.

DOI: 10.1177/09697330251366600
PMCID: PMC12644262
PMID: 40865964 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of conflicting interestsThe authors 
declared no potential conflicts of interest with respect to the research, 
authorship, and/or publication of this article.


34. Front Psychiatry. 2025 Jul 22;16:1628216. doi: 10.3389/fpsyt.2025.1628216. 
eCollection 2025.

Implementation of generative AI for the assessment and treatment of autism 
spectrum disorders: a scoping review.

Sohn JS(1), Lee E(2), Kim JJ(2)(3), Oh HK(2), Kim E(2)(3).

Author information:
(1)Department of Medicine, Yonsei University College of Medicine, 
Seoul, Republic of Korea.
(2)Institute of Behavioral Sciences in Medicine, Yonsei University College of 
Medicine, Seoul, Republic of Korea.
(3)Department of Psychiatry, Gangnam Severance Hospital, Yonsei University 
College of Medicine, Seoul, Republic of Korea.

INTRODUCTION: Autism spectrum disorder (ASD) is characterized by persistent 
deficits in social communication and restrictive, repetitive behaviors. Current 
diagnostic and intervention pathways rely heavily on clinician expertise, 
leading to delays and limited scalability. Generative artificial intelligence 
(GenAI) offers emerging opportunities for automatically assisting and 
personalizing ASD care, though technical and ethical concerns persist.
METHODS: We conducted systematic searches in Embase, PsycINFO, PubMed, Scopus, 
and Web of Science (January 2014 to February 2025). Two reviewers independently 
screened and extracted eligible studies reporting empirical applications of 
GenAI in ASD screening, diagnosis, or intervention. Data were charted across 
GenAI architectures, application domains, evaluation metrics, and validation 
strategies. Comparative performance against baseline methods was synthesized 
where available.
RESULTS: From 553 records, 10 studies met the inclusion criteria across three 
domains: (1) screening and diagnosis (e.g., transformer-based classifiers and 
GAN-based data augmentation), (2) assessment and intervention, (e.g., multimodal 
emotion recognition and feedback systems), and (3) caregiver education and 
support (e.g., LLM-based chatbots). While most studies reported potential 
performance improvements, they also highlighted limitations such as small sample 
sizes, data biases, limited validation, and model hallucinations. Comparative 
analyses were sparse and lacked standardized metrics.
DISCUSSION: This review (i) maps GenAI applications in ASD care, (ii) compares 
GenAI and traditional approaches, (iii) highlights methodological and ethical 
challenges, and (iv) proposes future research directions. Our findings 
underscore GenAI's emerging potential in autism care and the prerequisites for 
its ethical, transparent, and clinically validated implementation.
SYSTEMATIC REVIEW REGISTRATION: https://osf.io/4gsyj/, identifier DOI: 
10.17605/OSF.IO/4GSYJ.

Copyright © 2025 Sohn, Lee, Kim, Oh and Kim.

DOI: 10.3389/fpsyt.2025.1628216
PMCID: PMC12322814
PMID: 40766925

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


35. Front Digit Health. 2025 Nov 4;7:1633577. doi: 10.3389/fdgth.2025.1633577. 
eCollection 2025.

Artificial intelligence in oncology: promise, peril, and the future of 
patient-physician interaction.

Thind BS(1), Tsao CK(2).

Author information:
(1)School of Medicine, California University of Science and Medicine, Colton, 
CA, United States.
(2)Northwell Cancer Institute, New Hyde Park, NY, United States.

Artificial intelligence (AI) is increasingly embedded in oncology. While initial 
technical evaluations emphasize diagnostic accuracy and efficiency, the impact 
on patient-physician interaction (PPI)-the foundation of trust, communication, 
comprehension, and shared decision-making-remains underexplored. In this review, 
we studied the current development of AI technology facing both physicians and 
patients with a focus in cancer care. Among different AI technologies, chatbots, 
large language model agents, and extended reality applications have shown the 
promise to date. Survey data suggest oncologists recognize AI's potential to 
augment efficiency but remain cautious about liability and the erosion of 
relational care. Key to future AI success in improving cancer care critically 
depends on design, validation, governance, and human guidance and gatekeeping in 
care delivery.

© 2025 Thind and Tsao.

DOI: 10.3389/fdgth.2025.1633577
PMCID: PMC12623327
PMID: 41262650

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


36. NPJ Digit Med. 2025 Jan 31;8(1):74. doi: 10.1038/s41746-025-01457-w.

Language models for data extraction and risk of bias assessment in complementary 
medicine.

Lai H(1)(2), Liu J(1)(2), Bai C(3), Liu H(4), Pan B(4), Luo X(4)(5)(6), Hou 
L(4)(7), Zhao W(1)(2), Xia D(1)(2), Tian J(4)(6)(8), Chen Y(4)(6)(8), Zhang 
L(9), Estill J(4)(10), Liu J(11), Liao X(12), Shi N(12), Sun X(13), Shang H(14), 
Bian Z(15), Yang K(4)(6)(8), Huang L(16)(17), Ge L(18)(19)(20); ADVANCED Working 
Group.

Collaborators: Li H, Wang Y, Zhang H, Zhu D, Peng D, Wang F, Li Y, Tang S, Liu 
H, Li Z, Yang Z, Yu X, Qin Y.

Author information:
(1)Department of Health Policy and Health Management, School of Public Health, 
Lanzhou University, Lanzhou, China.
(2)Evidence-Based Social Science Research Center, School of Public Health, 
Lanzhou University, Lanzhou, China.
(3)School of Nursing, Southern Medical University, Guangzhou, China.
(4)Evidence-Based Medicine Center, School of Basic Medical Sciences, Lanzhou 
University, Lanzhou, China.
(5)Research Unit of Evidence-Based Evaluation and Guidelines, Chinese Academy of 
Medical Science, School of Basic Medical Sciences, Lanzhou University, Lanzhou, 
China.
(6)Key Laboratory of Evidence-Based Medicine of Gansu Province, Lanzhou 
University, Lanzhou, China.
(7)Department of Health Research Methods, Evidence, and Impact, McMaster 
University, Ontario, Canada.
(8)WHO Collaborating Center for Guideline Implementation and Knowledge 
Translation, Lanzhou, China.
(9)Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, 
China.
(10)Institute of Global Health, University of Geneva, Geneva, Switzerland.
(11)Department of Oncology, Guang' anmen Hospital, China Academy of Chinese 
Medical Sciences, Beijing, China.
(12)Institute of Basic Research of Clinical Medicine, China Academy of Chinese 
Medical Sciences, Beijing, China.
(13)Chinese Evidence-Based Medicine Center, West China Hospital, Sichuan 
University, Chengdu, China.
(14)Dongzhimen Hospital, Beijing University of Chinese Medicine, Beijing, China.
(15)School of Chinese Medicine, Hong Kong Baptist University, Hong Kong SAR, 
China.
(16)China Center for Evidence Based Traditional Chinese Medicine, China Academy 
of Chinese Medical Sciences, Beijing, China. huanglugi01@126.com.
(17)National Resource Center for Chinese Materia Medica, China Academy of 
Chinese Medical Sciences, Beijing, China. huanglugi01@126.com.
(18)Department of Health Policy and Health Management, School of Public Health, 
Lanzhou University, Lanzhou, China. gelong2009@163.com.
(19)Evidence-Based Social Science Research Center, School of Public Health, 
Lanzhou University, Lanzhou, China. gelong2009@163.com.
(20)WHO Collaborating Center for Guideline Implementation and Knowledge 
Translation, Lanzhou, China. gelong2009@163.com.

Large language models (LLMs) have the potential to enhance evidence synthesis 
efficiency and accuracy. This study assessed LLM-only and LLM-assisted methods 
in data extraction and risk of bias assessment for 107 trials on complementary 
medicine. Moonshot-v1-128k and Claude-3.5-sonnet achieved high accuracy (≥95%), 
with LLM-assisted methods performing better (≥97%). LLM-assisted methods 
significantly reduced processing time (14.7 and 5.9 min vs. 86.9 and 10.4 min 
for conventional methods). These findings highlight LLMs' potential when 
integrated with human expertise.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01457-w
PMCID: PMC11785717
PMID: 39890970

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


37. NPJ Digit Med. 2025 Oct 1;8(1):588. doi: 10.1038/s41746-025-02004-3.

Large language models forecast patient health trajectories enabling digital 
twins.

Makarov N(#)(1)(2)(3), Bordukova M(#)(1)(2)(3), Quengdaeng P(2)(4), Garger 
D(2)(3), Rodriguez-Esteban R(5), Schmich F(6), Menden MP(7)(8).

Author information:
(1)Roche Innovation Center Munich (RICM), Penzberg, Germany.
(2)Computational Health Center, Helmholtz Munich, Munich, Germany.
(3)Department of Biology, Ludwig Maximilian University of Munich, Munich, 
Germany.
(4)TUM School of Computation, Information and Technology, Technical University 
of Munich, Munich, Germany.
(5)Roche Innovation Center Basel (RICB), Basel, Switzerland.
(6)Roche Innovation Center Munich (RICM), Penzberg, Germany. 
fabian.schmich@roche.com.
(7)Computational Health Center, Helmholtz Munich, Munich, Germany. 
michael.menden@unimelb.edu.au.
(8)Department of Biochemistry and Pharmacology, Bio21 Molecular Science and 
Biotechnology Institute, The University of Melbourne, Melbourne, VIC, Australia. 
michael.menden@unimelb.edu.au.
(#)Contributed equally

Generative artificial intelligence is revolutionizing digital twin development, 
enabling virtual patient representations that predict health trajectories, with 
large language models (LLMs) showcasing untapped clinical forecasting potential. 
We developed the Digital Twin-Generative Pretrained Transformer (DT-GPT), 
extending LLM-based forecasting solutions to clinical trajectory prediction. 
DT-GPT leverages electronic health records without requiring data imputation or 
normalization and overcomes real-world data challenges such as missingness, 
noise, and limited sample sizes. Benchmarking on non-small cell lung cancer, 
intensive care unit, and Alzheimer's disease datasets, DT-GPT outperformed 
state-of-the-art machine learning models, reducing the scaled mean absolute 
error by 3.4%, 1.3% and 1.8%, respectively. It maintained distributions and 
cross-correlations of clinical variables, and demonstrated explainability 
through a human-interpretable interface. Additionally, DT-GPT's ability to 
perform zero-shot forecasting highlights potential advantages of LLMs as 
clinical forecasting platforms, proposing a path towards digital twin 
applications in clinical trials, treatment selection, and adverse event 
mitigation.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-02004-3
PMCID: PMC12488890
PMID: 41034564

Conflict of interest statement: Competing interests: N.M., M.B., R.R.E. and F.S. 
are all employees of F. Hoffmann-La Roche. M.P.M. collaborates and is 
financially supported by GSK, F. Hoffmann-La Roche, and AstraZeneca. M.P.M. is 
supported by the European Union’s Horizon 2020 Research and Innovation Programme 
(Grant agreement No. 950293—COMBAT-RES). N.M., M.B., R.R.E., F.S. and M.P.M. are 
authors of an in-force patent entitled “Forecasting of subject-related 
attributes using generative machine-learning model” (patent publication number 
2025/021719, patent application number EP2024070632) owned by F. Hoffmann-La 
Roche and Helmholtz Zentrum Munich. The patent covers application of large 
language models such as DT-GPT for forecasting of clinical trajectories of 
patients during a clinical trial. The authors have no other relevant 
affiliations or financial involvement with any organization or entity with a 
financial interest in or financial conflict with the subject matter or materials 
discussed in the manuscript apart from those disclosed.


38. JMIR Med Educ. 2025 Oct 23;11:e71125. doi: 10.2196/71125.

Applications, Challenges, and Prospects of Generative Artificial Intelligence 
Empowering Medical Education: Scoping Review.

Lin Y(#)(1), Luo Z(#)(2), Ye Z(1), Zhong N(2), Zhao L(1), Zhang L(3), Li X(1), 
Chen Z(1), Chen Y(1).

Author information:
(1)Guangdong Provincial Key Laboratory of Stomatology, Hospital of Stomatology, 
Guanghua School of Stomatology, Sun Yat-sen University, No. 56, Lingyuan Road 
West, Guangzhou, 510055, China, 86 13580591020.
(2)Zhongshan School of Medicine, Sun Yat-sen University, Guangzhou, China.
(3)School of Government, Sun Yat-sen University, Guangzhou, China.
(#)Contributed equally

BACKGROUND: Nowadays, generative artificial intelligence (GAI) drives medical 
education toward enhanced intelligence, personalization, and interactivity. With 
its vast generative abilities and diverse applications, GAI redefines how 
educational resources are accessed, teaching methods are implemented, and 
assessments are conducted.
OBJECTIVE: This study aimed to review the current applications of GAI in medical 
education; analyze its opportunities and challenges; identify its strengths and 
potential issues in educational methods, assessments, and resources; and capture 
GAI's rapid evolution and multidimensional applications in medical education, 
thereby providing a theoretical foundation for future practice.
METHODS: This scoping review used PubMed, Web of Science, and Scopus to analyze 
literature from January 2023 to October 2024, focusing on GAI applications in 
medical education. Following PRISMA-ScR (Preferred Reporting Items for 
Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) guidelines, 
5991 articles were retrieved, with 1304 duplicates removed. The 2-stage 
screening (title or abstract and full-text review) excluded 4564 articles and a 
supplementary search included 8 articles, yielding 131 studies for final 
synthesis. We included (1) studies addressing GAI's applications, challenges, or 
future directions in medical education, (2) empirical research, systematic 
reviews, and meta-analyses, and (3) English-language articles. We excluded 
commentaries, editorials, viewpoints, perspectives, short reports, or 
communications with low levels of evidence, non-GAI technologies, and studies 
centered on other fields of medical education (eg, nursing). We integrated 
quantitative analysis of publication trends and Human Development Index (HDI) 
with thematic analysis of applications, technical limitations, and ethical 
implications.
RESULTS: Analysis of 131 articles revealed that 74.0% (n=97) originated from 
countries or regions with very high HDI, with the United States contributing the 
most (n=33); 14.5% (n=19) were from high HDI countries, 5.3% (n=7) from medium 
HDI countries, and 2.2% (n=3) from low HDI countries, with 3.8% (n=5) involving 
cross-HDI collaborations. ChatGPT was the most studied GAI model (n=119), 
followed by Gemini (n=22), Copilot (n=11), Claude (n=6), and LLaMA (n=4). 
Thematic analysis indicated that GAI applications in medical education mainly 
embody the diversification of educational methods, scientific evaluation of 
educational assessments, and dynamic optimization of educational resources. 
However, it also highlighted current limitations and potential future 
challenges, including insufficient scene adaptability, data quality and 
information bias, overreliance, and ethical controversies.
CONCLUSIONS: GAI application in medical education exhibits significant regional 
disparities in development, and model research statistics reflect researchers' 
certain usage preferences. GAI holds potential for empowering medical education, 
but widespread adoption requires overcoming complex technical and ethical 
challenges. Grounded in symbiotic agency theory, we advocate establishing the 
resource-method-assessment tripartite model, developing specialized models and 
constructing an integrated system of general large language models incorporating 
specialized ones, promoting resource sharing, refining ethical governance, and 
building an educational ecosystem fostering human-machine symbiosis, enabling 
deep tech-humanism integration and advancing medical education toward greater 
efficiency and human-centeredness.

© Yuhang Lin, Zhiheng Luo, Zicheng Ye, Nuoxi Zhong, Lijian Zhao, Long Zhang, 
Xiaolan Li, Zetao Chen, Yijia Chen. Originally published in JMIR Medical 
Education (https://mededu.jmir.org).

DOI: 10.2196/71125
PMCID: PMC12547994
PMID: 41128430 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


39. Can J Anaesth. 2025 Jun;72(6):1000-1014. doi: 10.1007/s12630-025-02980-w. Epub 
2025 Jun 9.

Large language models in perioperative medicine-applications and future 
prospects: a narrative review.

Mbadjeu Hondjeu AR(1)(2)(3)(4), Zhao ZY(5)(6), Newton L(5), Ajenkar A(5), 
Hladkowicz E(6)(7), Ladha K(8), Wijeysundera DN(8), McIsaac DI(5)(6)(7)(9).

Author information:
(1)Department of Anesthesiology and Pain Medicine, University of Ottawa, Ottawa, 
ON, Canada. ambadjeu@toh.ca.
(2)The Ottawa Hospital, Ottawa, ON, Canada. ambadjeu@toh.ca.
(3)Ottawa Hospital Research Institute, Ottawa, ON, Canada. ambadjeu@toh.ca.
(4)Department of Anesthesiology and Pain Medicine, The Ottawa Hospital, General 
Campus CCW1401-501 Smyth Rd, Ottawa, ON, K1H 8L6, Canada. ambadjeu@toh.ca.
(5)Department of Anesthesiology and Pain Medicine, University of Ottawa, Ottawa, 
ON, Canada.
(6)The Ottawa Hospital, Ottawa, ON, Canada.
(7)Ottawa Hospital Research Institute, Ottawa, ON, Canada.
(8)Department of Anesthesiology & Pain Medicine, University of Toronto, Toronto, 
ON, Canada.
(9)Institute for Clinical Evaluative Sciences, Ottawa, ON, Canada.

PURPOSE: Large language models (LLMs) are a subset of artificial intelligence 
(AI) and linguistics designed to help computers understand and analyze human 
language. Clinical applications of LLMs have recently been recognised for their 
potential enhanced analytic capacity. Availability and performance of LLMs are 
expected to increase substantially over time with a significant impact on 
patient care and health care provider workflow. Despite increasing recognition 
of LLMs, insights on the utilities, associated benefits and limitations are 
scarce among perioperative clinicians. In this narrative review, we delve into 
the functionalities and prospects of existing LLMs and their clinical 
application in perioperative medicine. Furthermore, we summarize challenges and 
constraints that must be addressed to fully realize the potential of LLMs.
SOURCE: We searched MEDLINE, Google Scholar, and PubMed® databases for articles 
referencing LLMs in perioperative care.
PRINCIPAL FINDINGS: We found that in the perioperative setting (from surgical 
diagnosis to discharge postoperatively), LLMs have the potential to improve the 
efficiency and accuracy of health care delivery by extracting and summarizing 
clinical data, making recommendations on the basis of these findings, as well as 
addressing patient queries. Moreover, LLMs can be used for clinical 
decision-making support, surveillance tools, predictive modelling, and 
enhancement of medical research and education.
CONCLUSIONS: The integration of LLMs into perioperative medicine presents a 
significant opportunity to enhance patient care, clinical decision-making, and 
operational efficiency. These models can streamline processes, provide 
personalized patient education, and offer robust decision support. Nevertheless, 
their clinical implementation requires addressing several key challenges, 
including managing hallucinations, ensuring data security, and mitigating 
inherent biases. If these challenges are met, LLMs can revolutionize 
perioperative practice, improving both patient outcomes and clinician workflow.

Publisher: RéSUMé: OBJECTIF: Les grands modèles de langage (LLM) sont à la 
croisée des chemins de l’intelligence artificielle (IA) et de la linguistique et 
sont conçus pour aider les ordinateurs à comprendre et analyser le langage 
humain. Les applications cliniques des LLM ont récemment été reconnues pour leur 
capacité analytique potentiellement améliorée. La disponibilité et les 
performances des LLM devraient augmenter considérablement au fil du temps, ce 
qui aura un impact significatif sur les soins à la patientèle et le flux de 
travail des prestataires de soins de santé. Malgré la reconnaissance croissante 
des LLM, les équipes cliniques périopératoires ont peu d’informations sur leurs 
utilités, ainsi que sur les avantages et limites qui y sont associés. Dans ce 
compte rendu narratif, nous nous penchons sur les fonctionnalités et les 
perspectives des LLM existants et leur application clinique en médecine 
périopératoire. Nous résumons également les défis et les contraintes qui doivent 
être abordés pour réaliser pleinement le potentiel des LLM.
SOURCES: Nous avons recherché des articles faisant référence à des LLM en soins 
périopératoires dans les bases de données MEDLINE, Google Scholar et PubMed®.
CONSTATATIONS PRINCIPALES: Nous avons constaté que dans le cadre périopératoire 
(du diagnostic chirurgical au congé postopératoire), les LLM ont le potentiel 
d’améliorer l’efficacité et la précision de la prestation des soins de santé en 
extrayant et en résumant les données cliniques, en formulant des recommandations 
sur la base de ces résultats, ainsi qu’en répondant aux questions des patients 
et patientes. De plus, les LLM peuvent être utilisés pour l’aide à la prise de 
décision clinique, les outils de surveillance, la modélisation prédictive et 
l’amélioration de la recherche médicale et de l’éducation.
CONCLUSION: L’intégration des LLM dans la médecine périopératoire représente une 
opportunité majeure d’améliorer les soins à la patientèle, la prise de décision 
clinique et l’efficacité opérationnelle. Ces modèles peuvent rationaliser les 
processus, fournir une éducation personnalisée à la patientèle et offrir une 
aide à la décision solide. Néanmoins, leur mise en œuvre clinique nécessite de 
relever plusieurs défis clés, notamment la prise en charge des hallucinations, 
la sécurité des données et l’atténuation des préjugés inhérents. Si ces défis 
sont relevés, les LLM pourraient révolutionner la pratique périopératoire, en 
améliorant à la fois les devenirs pour la patientèle et le flux de travail des 
équipes cliniques.

© 2025. The Author(s).

DOI: 10.1007/s12630-025-02980-w
PMCID: PMC12228656
PMID: 40490617 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest 
related to the content of this article.


40. Front Digit Health. 2025 Sep 11;7:1653631. doi: 10.3389/fdgth.2025.1653631. 
eCollection 2025.

A systematic review of ethical considerations of large language models in 
healthcare and medicine.

Fareed M(1), Fatima M(1), Uddin J(1), Ahmed A(1), Sattar MA(2).

Author information:
(1)Riphah School of Computing & Innovation, Riphah International University, 
Islamabad, Pakistan.
(2)Department of Computer Science, Electrical and Space Engineering, Luleå 
University of Technology, Luleå, Sweden.

The rapid integration of large language models (LLMs) into healthcare offers 
significant potential for improving diagnosis, treatment planning, and patient 
engagement. However, it also presents serious ethical challenges that remain 
incompletely addressed. In this review, we analyzed 27 peer-reviewed studies 
published between 2017 and 2025 across four major open-access databases using 
strict eligibility criteria, robust synthesis methods, and established 
guidelines to explicitly examine the ethical aspects of deploying LLMs in 
clinical settings. We explore four key aspects, including the main ethical 
issues arising from the use of LLMs in healthcare, the prevalent model 
architectures employed in ethical analyses, the healthcare application domains 
that are most frequently scrutinized, and the publication and bibliographic 
patterns characterizing this literature. Our synthesis reveals that bias and 
fairness ( n = 7 , 25.9%) are the most frequently discussed concerns, followed 
by safety, reliability, transparency, accountability, and privacy, and that the 
GPT family predominates ( n = 14 , 51.8%) among examined models. While privacy 
protection and bias mitigation received notable attention in the literature, no 
existing review has systematically addressed the comprehensive ethical issues 
surrounding LLMs. Most previous studies focus narrowly on specific clinical 
subdomains and lack a comprehensive methodology. As a systematic mapping of 
open-access literature, this synthesis identifies dominant ethical patterns, but 
it is not exhaustive of all ethical work on LLMs in healthcare. We also 
synthesize identified challenges, outline future research directions and include 
a provisional ethical integration framework to guide clinicians, developers, and 
policymakers in the responsible integration of LLMs into clinical workflows.

© 2025 Fareed, Fatima, Uddin, Ahmed and Sattar.

DOI: 10.3389/fdgth.2025.1653631
PMCID: PMC12460403
PMID: 41019285

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


41. Am J Perinatol. 2025 Dec;42(16):2094-2103. doi: 10.1055/a-2616-4182. Epub 2025 
May 20.

Application of Generative AI to Enhance Obstetrics and Gynecology Research.

Kawakita T(1), Wong MS(2)(3), Gibson KS(4), Gupta M(5)(6), Gimovsky AC(7), 
Moussa HN(8)(9), Hye HJ(10)(11); Society of Maternal-Fetal Medicine Clinical 
Informatics Committee.

Author information:
(1)Department of Obstetrics and Gynecology, Eastern Virginia Medical School, 
Norfolk, Virginia.
(2)Division of Maternal-Fetal Medicine, Department of Obstetrics and Gynecology, 
Cedars-Sinai Medical Center, Los Angeles, California.
(3)Division of Informatics, Department of Computational Biomedicine, 
Cedars-Sinai Medical Center, Los Angeles, California.
(4)Division of Maternal-Fetal Medicine, Department of Obstetrics and Gynecology, 
The MetroHealth System, Case Western Reserve University, Cleveland, Ohio.
(5)Department of Obstetrics and Gynecology, Beth Israel Deaconess Medical 
Center, Boston, Massachusetts.
(6)Department of Obstetrics, Gynecology and Reproductive Biology, Harvard 
Medical School, Boston, Massachusetts.
(7)Division of Maternal-Fetal Medicine, Department of Obstetrics and Gynecology, 
Warren Alpert School of Brown University, Women & Infants Hospital of Rhode 
Island, Providence, Rhode Island.
(8)Division of Maternal Fetal Medicine, Department of Obstetrics and Gynecology, 
University of Cincinnati, Cincinnati, Ohio.
(9)Kettering Health Maternal Fetal Medicine, Kettering, Ohio.
(10)Department of Obstetrics and Gynecology, NYU Grossman Long Island School of 
Medicine, Mineola, New York.
(11)Department of Health Informatics, NYU Langone Health, New York, New York.

The rapid evolution of large-language models such as ChatGPT, Claude, and Gemini 
is reshaping the methodological landscape of obstetrics and gynecology (OBGYN) 
research. This narrative review provides a comprehensive account of generative 
AI capabilities, key use cases, and recommended safeguards for investigators. 
First, generative AI expedites hypothesis generation, enabling researchers to 
interrogate vast corpora and surface plausible, overlooked questions. Second, it 
streamlines systematic reviews by composing optimized search strings, screening 
titles and abstracts, and identifying full-text discrepancies. Third, AI 
assistants can draft reproducible analytic code, perform preliminary descriptive 
or inferential analyses, and create publication-ready tables and figures. 
Fourth, the models support scholarly writing by suggesting journal-specific 
headings, refining prose, harmonizing references, and translating technical 
content for multidisciplinary audiences. Fifth, they augment peer-review and 
editorial workflows by delivering evidence-focused critiques. In educational 
settings, these models can create adaptive curricula and interactive simulations 
for trainees, fostering digital literacy and evidence-based practice early in 
professional development among clinicians. Integration into clinical 
decision-support pipelines is also foreseeable, warranting proactive governance. 
Notwithstanding these opportunities, responsible use demands vigilant oversight. 
Large-language models occasionally fabricate citations or misinterpret 
domain-specific data ("hallucinations"), potentially propagating misinformation. 
Outputs are highly prompt-dependent, creating a reliance on informed prompt 
engineering that may disadvantage less technical clinicians. Moreover, uploading 
protected health information or copyrighted text raises privacy, security, and 
intellectual property concerns. We outline best-practice recommendations: 
maintain human verification of all AI-generated content; cross-validate 
references with primary databases; employ privacy-preserving, on-premises 
deployments for sensitive data; document prompts for reproducibility; and 
disclose AI involvement transparently. In summary, generative AI offers a 
powerful adjunct for OBGYN scientists by accelerating topic formulation, 
evidence synthesis, data analysis, manuscript preparation, and peer review. When 
coupled with rigorous oversight and ethical safeguards, these tools can enhance 
productivity without compromising scientific integrity. Future studies should 
quantify accuracy, bias, and downstream patient impact. · Generative AI supports 
various research stages in OBGYN, such as hypothesis generation, systematic 
review assistance, data analysis, and scientific writing, demonstrating its 
potential to streamline research workflows and improve research efficiency.. · 
Generative AI has notable limitations, including the risk of generating 
inaccurate references ("hallucinations") and the need for careful supervision.. 
· Effective usage requires skill in prompt engineering, posing a challenge for 
those without technical expertise.. · Utilizing generative AI in sensitive 
fields like OBGYN raises privacy, security, and ethical concerns..

Thieme. All rights reserved.

DOI: 10.1055/a-2616-4182
PMID: 40393680 [Indexed for MEDLINE]

Conflict of interest statement: None declared.


42. J Biomed Inform. 2025 Jul;167:104850. doi: 10.1016/j.jbi.2025.104850. Epub 2025 
May 15.

Biomedical text normalization through generative modeling.

Berkowitz JS(1), Srinivasan A(1), Acitores Cortina JM(1), Fatapour Y(1), 
Tatonetti NP(2).

Author information:
(1)Department of Computational Biomedicine, Cedars-Sinai Medical Center, 700 N 
San Vicente Blvd, Pacific Design Center Suite G540, West Hollywood, CA 90069 
United States.
(2)Department of Computational Biomedicine, Cedars-Sinai Medical Center, 700 N 
San Vicente Blvd, Pacific Design Center Suite G540, West Hollywood, CA 90069 
United States. Electronic address: nicholas.tatonetti@cshs.org.

Update of
    medRxiv. 2025 Mar 05:2024.09.30.24314663. doi: 10.1101/2024.09.30.24314663.

OBJECTIVE: A large proportion of electronic health record (EHR) data consists of 
unstructured medical language text. The formatting of this text is often 
flexible and inconsistent, making it challenging to use for predictive modeling, 
clinical decision support, and data mining. Large language models' (LLMs) 
ability to understand context and semantic variations makes them promising tools 
for standardizing medical text. In this study, we develop and assess clinical 
text normalization pipelines built using large-language models.
METHODS: We implemented four LLM-based normalization strategies (Zero-Shot 
Recall, Prompt Recall, Semantic Search, and Retrieval-Augmented Generation based 
normalization [RAGnorm]) and one baseline approach using TF-IDF based String 
Matching. We evaluated performance across three datasets of SNOMED-mapped 
condition terms: [1] an oncology-specific dataset, [2] a representative sample 
of institutional medical conditions, and [3] a dataset of commonly occurring 
condition codes (>1000 uses) from our institution. We measured performance by 
recording the mean shortest path length between predicted and true SNOMED CT 
terms. Additionally, we benchmarked our models against the TAC 2017 drug label 
annotations, which normalizes terms to the Medical Dictionary for Regulatory 
Activities (MedDRA) Preferred Terms.
RESULTS: We found that RAGnorm was the most effective throughout each dataset, 
achieving a mean shortest path length of 0.21 for the domain-specific dataset, 
0.58 for the sampled dataset, and 0.90 for the top terms dataset. It achieved a 
micro F1 score of 88.01 on task 4 of the TAC2017 conference, surpassing all 
other models without viewing the provided training data.
CONCLUSION: We find that retrieval-focused approaches overcome traditional LLM 
limitations for this task. RAGnorm and related retrieval techniques should be 
explored further for the normalization of biomedical free text.

Copyright © 2025. Published by Elsevier Inc.

DOI: 10.1016/j.jbi.2025.104850
PMCID: PMC12158622
PMID: 40381869 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


43. Front Digit Health. 2025 Nov 17;7:1692517. doi: 10.3389/fdgth.2025.1692517. 
eCollection 2025.

Ethical and practical challenges of generative AI in healthcare and proposed 
solutions: a survey.

Tung T(1), Hasnaeen SMN(2), Zhao X(3).

Author information:
(1)Department of Biomedical Engineering, University of Tennessee, Knoxville, TN, 
United States.
(2)Bredesen Center, University of Tennessee, Knoxville, TN, United States.
(3)Department of Mechanical Engineering, University of Mississippi, Oxford, MS, 
United States.

BACKGROUND: Generative artificial intelligence (AI) is rapidly transforming 
healthcare, but its adoption introduces significant ethical and practical 
challenges. Algorithmic bias, ambiguous liability, lack of transparency, and 
data privacy risks can undermine patient trust and create health disparities, 
making their resolution critical for responsible AI integration.
OBJECTIVES: This systematic review analyzes the generative AI landscape in 
healthcare. Our objectives were to: (1) identify AI applications and their 
associated ethical and practical challenges; (2) evaluate current data-centric, 
model-centric, and regulatory solutions; and (3) propose a framework for 
responsible AI deployment.
METHODS: Following the PRISMA 2020 statement, we conducted a systematic review 
of PubMed and Google Scholar for articles published between January 2020 and May 
2025. A multi-stage screening process yielded 54 articles, which were analyzed 
using a thematic narrative synthesis.
RESULTS: Our review confirmed AI's growing integration into medical training, 
research, and clinical practice. Key challenges identified include systemic bias 
from non-representative data, unresolved legal liability, the "black box" nature 
of complex models, and significant data privacy risks. Proposed solutions are 
multifaceted, spanning technical (e.g., explainable AI), procedural (e.g., 
stakeholder oversight), and regulatory strategies.
DISCUSSION: Current solutions are fragmented and face significant implementation 
barriers. Technical fixes are insufficient without robust governance, clear 
legal guidelines, and comprehensive professional education. Gaps in global 
regulatory harmonization and frameworks ill-suited for adaptive AI persist. A 
multi-layered, socio-technical approach is essential to build trust and ensure 
the safe, equitable, and ethical deployment of generative AI in healthcare.
CONCLUSIONS: The review confirmed that generative AI has a growing integration 
into medical training, research, and clinical practice. Key challenges 
identified include systemic bias stemming from non-representative data, 
unresolved legal liability, the "black box" nature of complex models, and 
significant data privacy risks. These challenges can undermine patient trust and 
create health disparities. Proposed solutions are multifaceted, spanning 
technical (such as explainable AI), procedural (like stakeholder oversight), and 
regulatory strategies.

© 2025 Tung, Hasnaeen and Zhao.

DOI: 10.3389/fdgth.2025.1692517
PMCID: PMC12665710
PMID: 41333106

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


44. Clin Microbiol Infect. 2025 Aug;31(8):1336-1342. doi: 10.1016/j.cmi.2025.03.002. 
Epub 2025 Mar 19.

Comparing large language models for antibiotic prescribing in different clinical 
scenarios: which performs better?

De Vito A(1), Geremia N(2), Bavaro DF(3), Seo SK(4), Laracy J(4), Mazzitelli 
M(5), Marino A(6), Maraolo AE(7), Russo A(8), Colpani A(9), Bartoletti M(3), 
Cattelan AM(5), Mussini C(10), Parisi SG(11), Vaira LA(12), Nunnari G(6), 
Madeddu G(9).

Author information:
(1)Unit of Infectious Diseases, Department of Medicine, Surgery and Pharmacy, 
Sassari, Italy. Electronic address: andreadevitoaho@gmail.com.
(2)Unit of Infectious Diseases, Department of Clinical Medicine, Ospedale 
dell'Angelo, Venice, Italy; Unit of Infectious Diseases, Department of Clinical 
Medicine, Ospedale Civile S.S. Giovanni e Paolo, Venice, Italy.
(3)Department of Biomedical Sciences, Humanitas University, Pieve Emanuele, 
Milan, Italy; Infectious Diseases Unit - Department of Biomedical Sciences - 
Istituti di Ricovero e Cura a Carattere Scientifico (IRCCS) Humanitas Research 
Hospital, Rozzano, Milan, Italy.
(4)Infectious Diseases Service, Department of Medicine, Memorial Sloan Kettering 
Cancer Center, New York, NY, USA.
(5)Infectious and Tropical Diseases Unit, Department of Molecular Medicine, 
Padua University Hospital, Padua, Italy.
(6)Unit of Infectious Diseases, Department of Clinical and Experimental 
Medicine, Azienda Ospedaliera di Rilievo Nazionale e di Alta Specializzazione 
(ARNAS), Garibaldi Hospital, University of Catania, Catania, Italy.
(7)Section of Infectious Diseases, Department of Clinical Medicine and Surgery, 
University of Naples "Federico II," Naples, Italy.
(8)Department of Mental Health and Public Medicine-Infectious Diseases Unit, 
University of Campania Luigi Vanvitelli, Naples, Italy.
(9)Unit of Infectious Diseases, Department of Medicine, Surgery and Pharmacy, 
Sassari, Italy.
(10)Infectious Diseases Unit, Department of Surgical, Medical, Dental and 
Morphological Sciences, Azienda Ospedaliera-Universitaria of Modena, University 
of Modena and Reggio Emilia, Modena, Italy.
(11)Department of Molecular Medicine, University of Padua, Padua, Italy.
(12)Maxillofacial Surgery Operative Unit, Department of Medicine, Surgery and 
Pharmacy, University of Sassari, Sassari, Italy.

OBJECTIVES: Large language models (LLMs) show promise in clinical 
decision-making, but comparative evaluations of their antibiotic prescribing 
accuracy are limited. This study assesses the performance of various LLMs in 
recommending antibiotic treatments across diverse clinical scenarios.
METHODS: Fourteen LLMs, including standard and premium versions of ChatGPT, 
Claude, Copilot, Gemini, Le Chat, Grok, Perplexity, and Pi.ai, were evaluated 
using 60 clinical cases with antibiograms covering 10 infection types. A 
standardized prompt was used for antibiotic recommendations focusing on drug 
choice, dosage, and treatment duration. Responses were anonymized and reviewed 
by a blinded expert panel assessing antibiotic appropriateness, dosage 
correctness, and duration adequacy.
RESULTS: A total of 840 responses were collected and analysed. ChatGPT-o1 
demonstrated the highest accuracy in antibiotic prescriptions, with 71.7% 
(43/60) of its recommendations classified as correct and only one (1.7%) 
incorrect. Gemini and Claude 3 Opus had the lowest accuracy. Dosage correctness 
was highest for ChatGPT-o1 (96.7%, 58/60), followed by Perplexity Pro (90.0%, 
54/60) and Claude 3.5 Sonnet (91.7%, 55/60). In treatment duration, Gemini 
provided the most appropriate recommendations (75.0%, 45/60), whereas Claude 3.5 
Sonnet tended to over-prescribe duration. Performance declined with increasing 
case complexity, particularly for difficult-to-treat microorganisms.
DISCUSSION: There is significant variability among LLMs in prescribing 
appropriate antibiotics, dosages, and treatment durations. ChatGPT-o1 
outperformed other models, indicating the potential of advanced LLMs as 
decision-support tools in antibiotic prescribing. However, decreased accuracy in 
complex cases and inconsistencies among models highlight the need for careful 
validation before clinical utilization.

Copyright © 2025 The Author(s). Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.cmi.2025.03.002
PMID: 40113208 [Indexed for MEDLINE]


45. Nurse Educ Pract. 2025 Dec 29;91:104697. doi: 10.1016/j.nepr.2025.104697. Online 
ahead of print.

Generative artificial intelligence for teaching and assessment in health 
professions education: A scoping review.

Astbury H(1), Fortune E(1), Ponte CD(2), Lyons K(3), Shaw L(4).

Author information:
(1)Faculty of Medicine, Dentistry and Health Sciences, University of Melbourne, 
Parkville, Victoria 3052, Australia.
(2)Centre for Digital Transformation of Health, Faculty of Medicine Dentistry 
and Health Sciences, University of Melbourne, Parkville, Victoria 3052, 
Australia.
(3)Centre for Digital Transformation of Health, Faculty of Medicine Dentistry 
and Health Sciences, University of Melbourne, Parkville, Victoria 3052, 
Australia; Melbourne School of Global and Population Health, Faculty of Medicine 
Dentistry and Health Sciences, University of Melbourne, Parkville, Victoria 
3052, Australia.
(4)Centre for Digital Transformation of Health, Faculty of Medicine Dentistry 
and Health Sciences, University of Melbourne, Parkville, Victoria 3052, 
Australia; Murdoch Children's Research Institute, Royal Children's Hospital, 
Parkville, Victoria 3052, Australia. Electronic address: 
Louise.shaw@unimelb.edu.au.

AIM: To review the literature on generative artificial intelligence for teaching 
and assessment in health professions education BACKGROUND: Advancements in 
generative artificial intelligence (GenAI), such as ChatGPT, offer new 
possibilities for health professions education. These technologies offer 
potential benefits in teaching and assessment, including personalised learning 
and automated resource generation. Despite its potential, concerns about 
accuracy, ethics and reliability remain. This scoping review examines GenAI's 
implementation, benefits and challenges in teaching and assessment across health 
professions education.
DESIGN: Scoping review.
METHODS: Following Arksey and O'Malley's five-stage framework, with refinements 
based on the Joanna Briggs Institute (JBI) methodology, Medline, CINAHL and Web 
of Science Core Collection were searched for peer-reviewed studies published 
between January 2019 and June 2024. Studies were screened independently by two 
reviewers and data extraction performed systematically to ensure consistency.
RESULTS: Studies (n = 5826) were assessed for eligibility, with 23 meeting the 
inclusion criteria. All included studies were published in 2023 and 2024. The 
primary applications of GenAI were in learning resource development and 
assessment, with reported benefits such as time savings, personalised learning 
and reduced resource use. Challenges included accuracy concerns, inconsistent 
outputs, technical limitations, algorithmic bias and risks to academic 
integrity.
CONCLUSIONS: This scoping review provides an overview of how GenAI is being 
integrated into health professions education. While the technology offers 
opportunities to enhance teaching and assessment, its implementation requires 
consideration of reliability, ethical concerns and educator preparedness. This 
review is the first to examine GenAI implementation across multiple 
AHPRA-regulated health professions and proposes a practical framework (AI HPE 
checklist) to guide responsible use.

Copyright © 2025. Published by Elsevier Ltd.

DOI: 10.1016/j.nepr.2025.104697
PMID: 41477967

Conflict of interest statement: Declaration of Competing Interest The authors 
declare no conflicts of interest related to this research. No financial or 
personal relationships exist that could inappropriately influence or bias the 
content of this manuscript. The research was conducted independently, and no 
funding bodies had any role in study design, data collection, analysis, 
interpretation, manuscript preparation, or the decision to submit for 
publication.


46. BMC Nurs. 2025 Apr 29;24(1):470. doi: 10.1186/s12912-025-03115-8.

Prompts, privacy, and personalized learning: integrating AI into nursing 
education-a qualitative study.

Shen M(1)(2), Shen Y(3), Liu F(4), Jin J(5).

Author information:
(1)Shulan (Hangzhou) Hospital, Shulan International Medical College, Zhejiang 
Shuren University, Hangzhou, P. R. China. 289155169@qq.com.
(2)Zhejiang Shuren University, Hangzhou, P. R. China. 289155169@qq.com.
(3)Shulan (Hangzhou) Hospital, Shulan International Medical College, Zhejiang 
Shuren University, Hangzhou, P. R. China.
(4)Hangzhou Normal University, Hangzhou, 310010, P. R. China.
(5)Zhejiang Shuren University, Hangzhou, P. R. China.

BACKGROUND: Generative artificial intelligence (GenAI) has emerged as a powerful 
tool in nursing education, offering novel ways to enhance clinical reasoning, 
critical thinking, and personalized learning. However, questions remain 
regarding the ethical use of AI-generated outputs, data privacy concerns, and 
limitations in recognizing emotional nuances.
OBJECTIVE: This study aims to explore how nursing students utilize GenAI tools 
to develop care plans, with a particular focus on the innovative role of prompt 
engineering. By identifying both challenges and opportunities, it seeks to 
provide actionable insights into seamlessly integrating GenAI into nursing 
education while safeguarding humanistic nursing skills.
METHODS: A qualitative design was adopted, involving semi-structured interviews 
with third-year undergraduate nursing students at a single institution. 
Participants worked with anonymized clinical cases and multiple GenAI tools, 
emphasizing the iterative design of prompts to optimize care-plan outputs. Data 
were analyzed thematically to capture detailed perspectives on AI-facilitated 
learning and ethical considerations.
RESULTS: Findings indicate that GenAI tools enhanced efficiency and conceptual 
clarity, allowing students to focus more on higher-order clinical thinking. 
Prompt engineering significantly improved the accuracy and contextual relevance 
of AI-generated care plans. However, students expressed concerns about 
incomplete or imprecise responses, GenAI's limited emotional understanding, and 
privacy risks associated with sensitive healthcare data. When used with careful 
prompt refinement and critical evaluation, GenAI was viewed as a valuable 
supplement rather than a replacement for humanistic nursing competencies.
CONCLUSION: This study highlights the transformative potential of GenAI in 
nursing education, underscoring the importance of structured prompt engineering 
and ethical safeguards. By balancing technological innovation with empathy, 
communication, and cultural sensitivity, nursing educators can harness AI to 
deepen clinical reasoning and prepare students for future AI-enhanced practice. 
Further research across diverse settings is needed to validate these findings 
and refine best practices for integrating GenAI into nursing curricula.
CLINICAL TRIAL NUMBER: Not applicable. This study did not involve a clinical 
trial.

© 2025. The Author(s).

DOI: 10.1186/s12912-025-03115-8
PMCID: PMC12042552
PMID: 40301862

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was conducted in accordance with the ethical principles 
outlined in the Declaration of Helsinki and all relevant guidelines and 
regulations. Approval for the study was granted by the Research Ethics Committee 
of ZheJiang Shuren University (Approval Number: ZJSU20250103). All participants 
were fully informed about the purpose and procedures of the study. Participation 
was voluntary, and participants had the right to withdraw from the study at any 
time without any consequences. The study ensured that informed consent was 
obtained from all participants. All collected data were anonymized and kept 
confidential. Consent for publication: Not applicable. Competing interests: The 
authors declare no competing interests.


47. J Autism Dev Disord. 2025 Oct 10. doi: 10.1007/s10803-025-07059-5. Online ahead 
of print.

Efficacy of Antioxidant-Based Pharmacological Therapies in Autism Spectrum 
Disorder: A Systematic Review.

Ramírez-Guerrero S(1), Puerta-Murcia A(2), Martinez-Benavides N(2), Díaz-Quiróz 
M(1), Talero-Gutiérrez C(3).

Author information:
(1)School of Medicine and Health Sciences, Neuroscience Research Group (Neuros), 
Center of Neuroscience - Neurovitae, Universidad del Rosario, Bogotá, Colombia.
(2)School of Medicine and Health Sciences, Undergraduate Neuroscience Research 
Group (Semineuros), Center of Neuroscience - Neurovitae, Universidad del 
Rosario, Bogotá, Colombia.
(3)School of Medicine and Health Sciences, Neuroscience Research Group (Neuros), 
Center of Neuroscience - Neurovitae, Universidad del Rosario, Bogotá, Colombia. 
claudia.talero@urosario.edu.co.

PURPOSE: Autism Spectrum Disorder (ASD) is a heterogeneous neurodevelopmental 
condition characterized by persistent deficits in social interaction and 
communication. This study aimed to evaluate the efficacy of antioxidant-based 
pharmacological therapies for psychiatric symptoms in patients with ASD.
METHODS: A systematic review was conducted using four databases: PubMed, Scopus, 
ClinicalTrials.gov and Cochrane - Ovid. 20 studies were included. Risk of bias 
assessment was performed using the Cochrane Risk-of-Bias tool for randomized 
trials. A descriptive analysis synthesized the main findings. A qualitative 
value was given to each antioxidant therapy according to its effect on every 
symptom (improved, not improved or no information), data was visually reported 
using ChatGPT 4.0 and Google Collab.
PROSPERO ID: CRD42023490581.
RESULTS: There was a heterogeneous response to antioxidant therapies for ASD 
symptoms. The use of sulforaphane reported improvement in irritability, 
stereotypic/repetitive behavior, social cognition/interaction, social 
communication, hyperactivity and lethargy. Similarly, N-Acetylcysteine evidenced 
improvement in terms of irritability, stereotypic/repetitive behavior, social 
cognition and hyperactivity. L-Carnosine improved social cognition and 
communication, omega-3/omega-6 fatty acids improved social cognition, coenzyme 
q-10 showed improvement in sleep disorders, and glutathione in repetitive 
behaviors and irritability.
CONCLUSION: Antioxidant-based pharmacological therapy evidenced contrasting 
responses to a variety of core symptoms in ASD. Despite significant evidence of 
improvement in some ASD symptom scales, there is still a lack of evidence to 
justify the use of antioxidant therapies as monotherapy in individuals with ASD.

© 2025. The Author(s).

DOI: 10.1007/s10803-025-07059-5
PMID: 41071465

Conflict of interest statement: Declarations. Conflict of interest: The authors 
have no relevant financial or non-financial interests to disclose. Ethical 
Approval: This is a systematic review, therefore an ethical approval was not 
required.


48. Int J Surg. 2025 Nov 13. doi: 10.1097/JS9.0000000000004034. Online ahead of 
print.

Quality assessment of DeepSeek-R1's responses to guideline-related questions on 
hip osteoarthritis from AAOS.

Guo Z(1), Deng Y(1), Liang C(2), Zhang H(1), Li D(3), Huang M(4).

Author information:
(1)Department of Orthopedics, First Affiliated Hospital of Jinzhou Medical 
University, Jinzhou, China.
(2)School of Life and Health, Huzhou College, Huzhou, China.
(3)Department of Orthopedics, Third Affiliated Hospital of Jinzhou Medical 
University, Jinzhou, China.
(4)School of Nursing, Jinzhou Medical University, Jinzhou, China.

BACKGROUND: This study aims to evaluate the quality of responses generated by 
DeepSeek to guideline-related questions on hip osteoarthritis from the American 
Academy of Orthopaedic Surgeons (AAOS).
MATERIALS AND METHODS: Seventeen open-ended questions were derived from the 2023 
AAOS Clinical Practice Guidelines for hip osteoarthritis. Four prompt types were 
applied to DeepSeek-V3 and R1. Responses were evaluated for accuracy, 
readability, and the mention of guidelines. Automatic evaluation and qualitative 
evaluation jointly evaluated the chain-of-thought of the R1 group.
RESULTS: All the models showed superior response quality. Automatic and 
qualitative evaluations showed that the R1 group models performed better in the 
BERTScore and ROUGE-L metrics, indicating that their semantics and logic were 
closer to the reference text. This showed that the models in the R1 group could 
reflect chain-of-thought reasoning to a certain extent. The flesch-kincaid 
reading ease score of group R1 was significantly higher than that of group V3 
(P<0.001). The flesch-kincaid grade level score also showed that the R1 group 
was significantly better than the V3 group (P<0.001). However, the R1 group 
still required reading levels at the level of a 12th-grade or a college 
freshman. In addition, the guideline mention rate was higher in group R1 than in 
group V3.
CONCLUSION: DeepSeek-R1 demonstrated high accuracy in answering open-ended 
questions regarding hip osteoarthritis while effectively showcasing its 
chain-of-thought reasoning capabilities. Moreover, R1 based on background texts 
showed better readability, although its readability still required a 12th-grade 
or freshman reading level.

Copyright © 2025 The Author(s). Published by Wolters Kluwer Health, Inc.

DOI: 10.1097/JS9.0000000000004034
PMID: 41231615


49. Int J Med Inform. 2025 Dec 31;209:106248. doi: 10.1016/j.ijmedinf.2025.106248. 
Online ahead of print.

A scoping review: how evaluation methods shape our understanding of ChatGPT's 
effectiveness in healthcare.

Liu Y(1), Zhang Y(2), Mao H(3).

Author information:
(1)School of Foreign Studies, China University of Petroleum (East China), No. 66 
West Changjiang Road, Huangdao District, Qingdao, Shandong Province 266580, 
China. Electronic address: liuyuanyuan@upc.edu.cn.
(2)School of Foreign Studies, China University of Petroleum (East China), No. 66 
West Changjiang Road, Huangdao District, Qingdao, Shandong Province 266580, 
China. Electronic address: b23170001@s.upc.edu.cn.
(3)School of Foreign Studies, China University of Petroleum (East China), No. 66 
West Changjiang Road, Huangdao District, Qingdao, Shandong Province 266580, 
China. Electronic address: mao@upc.edu.cn.

BACKGROUND: The rapid growth in research on ChatGPT's healthcare applications 
has led to diverse evaluation methods and substantially heterogeneous findings, 
undermining evidence reliability and hindering clinical translation.
OBJECTIVES: This review aims to examine how different evaluation methods shape 
our understanding of ChatGPT's effectiveness in healthcare.
METHODS: Studies published between 2023 and 2024 that assess the use of ChatGPT 
in medical or healthcare-related contexts were included. Evidence was obtained 
from peer-reviewed literature analyzing ChatGPT's applications across clinical, 
educational, and diagnostic domains. Following the PRISMA guidelines, this 
systematic review analyzed 131 studies published during 2023-2024 that assess 
the use of ChatGPT in medical contexts.
RESULTS: The results indicate that predominant evaluation approaches-controlled 
trial studies, expert assessment studies, measurement-based evaluation studies, 
and prompt generation analysis studies-systematically influence conclusions 
about ChatGPT's performance due to their inherent methodological 
characteristics, such as subjectivity, objectivity, and differences in 
ecological validity. Further analysis reveals that ChatGPT's performance is 
highly context-dependent, shaped by specific application scenarios, model 
versions, and prompting strategies.
CONCLUSIONS: To address methodological heterogeneity and the lack of 
standardization, this study recommends multi-method cross-validation strategies 
and a risk-stratified, standardized evaluation framework. These steps are 
essential to enhance the scientific rigor and reliability of ChatGPT's 
assessment in healthcare and to provide a solid foundation for its clinical 
integration.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106248
PMID: 41485343

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


50. J Med Syst. 2025 Sep 16;49(1):115. doi: 10.1007/s10916-025-02254-4.

Large Language Models in Neurology Treatment Decision-Making: a Scoping Review.

Shah R(1), Jotterand F(2)(3).

Author information:
(1)School of Medicine, Medical College of Wisconsin, 8701 Watertown Plank Road, 
Milwaukee, WI, 53226, USA. rshah@mcw.edu.
(2)Center for Ethics in Health Care, Oregon Health & Science University, Oregon, 
USA.
(3)Institute for Biomedical Ethics, Faculty of Medicine, University of Basel, 
Basel, Switzerland.

This scoping review evaluates the expanding role of large language models (LLMs) 
in neurology, an area drawing growing interest of researchers and clinicians 
alike. A substantial existing body of literature supports the efficacy of LLMs 
for diagnostic applications. However, clinicians' emerging point of interest now 
lies in understanding the applications of LLMs in guiding treatment decisions. 
Our study therefore aims to synthesize and evaluate existing neurological 
studies focused on LLMs in treatment decision-making. A comprehensive search was 
conducted in the electronic databases OVID/Medline, Web of Science, and the 
Cochrane Library through September 18th, 2024. Inclusion criteria included 
original studies published within the last five years focused on evaluating the 
efficacy of LLMs in treatment decision-making in neurology. The protocol was 
registered on the Open Science Framework ( https://doi.org/10.17605/OSF.IO/Y6N3E 
). Four studies were identified. ChatGPT was the LLM utilized in each article, 
though varying in model versions. Each study demonstrated positive outcomes 
across varying metrics, with models generally aligning with clinician decisions. 
However, the lack of observed studies and variability of neurological topics 
limit the generalizability of these AI tools. This scoping review analyzes the 
existing body of evidence on LLMs in treatment decision-making in neurology. 
While current studies suggest potential to support clinical care, there is 
insufficient evidence at this stage to claim outcome improvement. Findings are 
not yet generalizable across neurological practice, as existing promise appears 
limited to narrow use cases. Prospective validation across subspecialties is 
needed to support broader clinical application.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02254-4
PMID: 40956389 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics Approval : None. Competing 
interests: The authors declare no competing interests.


51. JMIR Med Inform. 2025 Nov 24;13:e76326. doi: 10.2196/76326.

Large Language Models in Critical Care Medicine: Scoping Review.

Shi T(1)(2)(3)(4), Ma J(5), Yu Z(6), Xu H(1)(4), Yang R(1)(2)(3)(4), Xiong M(7), 
Xiao M(1)(2)(3)(4), Li Y(8), Zhao H(9), Kong G(1)(2)(3)(4).

Author information:
(1)National Institute of Health Data Science, Peking University, Beijing, China.
(2)Institute for Artificial Intelligence, Peking University, Beijing, China.
(3)Institute of Medical Technology, Peking University Health Science Center, 
Beijing, China.
(4)Advanced Institute of Information Technology, Peking University, Hangzhou, 
China.
(5)Peking University Third Hospital, Beijing, China.
(6)Department of Computer Science, University of Liverpool, Liverpool, United 
Kingdom.
(7)Johns Hopkins University School of Medicine, Baltimore, MD, United States.
(8)Fielding School of Public Health, University of California, Los Angeles, Los 
Angeles, CA, United States.
(9)Department of Critical Care Medicine, Peking University People's Hospital, 
Beijing, China.

BACKGROUND: With the rapid development of artificial intelligence, large 
language models (LLMs) have shown strong capabilities in natural language 
understanding, reasoning, and generation, attracting much research interest in 
applying LLMs to health and medicine. Critical care medicine (CCM) provides 
diagnosis and treatment for patients with critical illness who often require 
intensive monitoring and interventions in intensive care units (ICUs). Whether 
LLMs can be applied to CCM, and whether they can operate as ICU experts in 
assisting clinical decision-making rather than "stochastic parrots," remains 
uncertain.
OBJECTIVE: This scoping review aims to provide a panoramic portrait of the 
application of LLMs in CCM, identifying the advantages, challenges, and future 
potential of LLMs in this field.
METHODS: This study was conducted in accordance with the PRISMA-ScR (Preferred 
Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping 
Reviews) guidelines. Literature was searched across 7 databases, including 
PubMed, Embase, Scopus, Web of Science, CINAHL, IEEE Xplore, and ACM Digital 
Library, from the first available paper to August 22, 2025.
RESULTS: From an initial 2342 retrieved papers, 41 were selected for final 
review. LLMs played an important role in CCM through the following 3 main 
channels: clinical decision support, medical documentation and reporting, and 
medical education and doctor-patient communication. Compared to traditional 
artificial intelligence models, LLMs have advantages in handling unstructured 
data and do not require manual feature engineering. Meanwhile, applying LLMs to 
CCM has faced challenges, including hallucinations and poor interpretability, 
sensitivity to prompts, bias and alignment challenges, and privacy and ethical 
issues.
CONCLUSIONS: Although LLMs are not yet ICU experts, they have the potential to 
become valuable tools in CCM, helping to improve patient outcomes and optimize 
health care delivery. Future research should enhance model reliability and 
interpretability, improve model training and deployment scalability, integrate 
up-to-date medical knowledge, and strengthen privacy and ethical guidelines, 
paving the way for LLMs to fully realize their impact in critical care.
TRIAL REGISTRATION: OSF Registries yn328; https://osf.io/yn328/.

©Tongyue Shi, Jun Ma, Zihan Yu, Haowei Xu, Rongxin Yang, Minqi Xiong, Meirong 
Xiao, Yilin Li, Huiying Zhao, Guilan Kong. Originally published in JMIR Medical 
Informatics (https://medinform.jmir.org), 24.11.2025.

DOI: 10.2196/76326
PMCID: PMC12778902
PMID: 41284992 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


52. JHEP Rep. 2025 Mar 12;7(5):101392. doi: 10.1016/j.jhepr.2025.101392. eCollection 
2025 May.

Deep learning empowered gadolinium-free contrast-enhanced abbreviated MRI for 
diagnosing hepatocellular carcinoma.

Zhang Y(1)(2), Sheng R(1), Qian X(1), Wang H(1)(3), Wu F(1), Dai H(1), Song 
M(4), Yang C(1)(2), Zhou J(1)(3), Zhang W(4), Zeng M(1)(2).

Author information:
(1)Department of Radiology, Zhongshan Hospital, Fudan University, Shanghai, 
China.
(2)Shanghai Institute of Medical Imaging, Fudan University, Shanghai, China.
(3)Department of Radiology, Zhongshan Hospital (Xiamen), Fudan University, 
Xiamen, China.
(4)Department of Radiology, The Fourth Affiliated Hospital of Soochow 
University, Medical Center of Soochow University, Suzhou, China.

BACKGROUND & AIMS: By reducing some magnetic resonance imaging (MRI) sequences, 
abbreviated MRI (aMRI) has shown extensive promise for detecting hepatocellular 
carcinoma (HCC). We aim to develop deep learning (DL)-based gadolinium-free 
contrast-enhanced (CE) aMRI protocols (DL-aMRI) for detecting HCC.
METHODS: In total, 1,769 patients (913 with HCC) were retrospectively included 
from three institutions for training, testing, and external validation. Stable 
diffusion-based DL models were trained to generate CE-MRI, including T1-weighted 
arterial, portal venous, transitional, and hepatobiliary phase images (AP-syn, 
VP-syn, TP-syn, and HBP-syn, respectively). Non-contrast-MRI (NC-MRI), including 
T2-weighted, diffusion-weighted, and pre-contrast T1-weighted (Pre) sequences, 
along with either actual or DL-synthesized CE-MRI (AP, VP, TP, and HBP or 
AP-syn, VP-syn, TP-syn, and HBP-syn), were used to create conventional complete 
MRI (cMRI) and DL-aMRI protocols. An inter-method comparison of image quality 
between DL-aMRI and cMRI was conducted using a non-inferiority test. The 
sensitivity and specificity of DL-aMRI and cMRI for detecting HCC were 
statistically compared using the non-inferiority test and generalized estimating 
equations models.
RESULTS: DL-aMRI showed a remarkable reduction in acquisition time compared with 
cMRI (4.1 vs. 28.1 min). The image quality of DL-synthesized CE-MRI was not 
inferior to that of actual CE-MRI (p <0.001). There was an excellent 
inter-method agreement between the HCC sizes measured by the two protocols (R2 = 
0.9436-0.9683). The pooled sensitivity and specificity of cMRI and DL-aMRI were 
0.899 and 0.925 and 0.866 and 0.922, respectively. No significant differences 
were found between the sensitivity and specificity of the two protocols.
CONCLUSIONS: The proposed DL-aMRI could facilitate precise HCC diagnosis with no 
need for contrast agents, a substantial reduction in acquisition time, and 
preservation of both NC-MRI and CE-MRI data. DL-aMRI may serve as a valuable 
tool for HCC diagnosing.
IMPACT AND IMPLICATIONS: In this multi-center study involving 1,769 
participants, we developed a generative deep learning-based abbreviated MRI 
(DL-aMRI) strategy that provides an efficient, contrast-agent-free alternative 
for detecting HCC with accuracy comparable to that of conventional complete MRI, 
significantly reducing acquisition time from 28.1 min to just 4.1 min. This 
strategy is valuable for clinicians who face significant workloads resulting 
from long MRI scanning times and the potential adverse effects of contrast 
agents, as well as for researchers focused on developing cost-effective and 
accessible diagnostic tools for HCC detection. The proposed DL-aMRI protocol has 
practical implications for clinical settings, enhancing diagnostic efficiency 
while maintaining high image quality, eliminating the need for contrast agents 
and ultimately benefiting patients and healthcare providers.

© 2025 The Author(s).

DOI: 10.1016/j.jhepr.2025.101392
PMCID: PMC12056404
PMID: 40337547

Conflict of interest statement: The authors declare that there exists no 
conflict of interest. Please refer to the accompanying ICMJE disclosure forms 
for further details.


53. BMC Med Inform Decis Mak. 2025 Nov 24;25(1):427. doi: 
10.1186/s12911-025-03255-6.

Artificial intelligence in polycystic ovary syndrome: a systematic review of 
diagnostic and predictive applications.

Ghaderzadeh M(1), Garavand A(2), Salehnasab C(3).

Author information:
(1)Boukan Faculty of Medical Sciences, Urmia University of Medical Sciences, 
Urmia, Iran.
(2)School of Allied Medical Sciences, Lorestan University of Medical Sciences, 
Khorramabad, Iran.
(3)Social Determinants of Health Research Center, Yasuj University of Medical 
Sciences, Yasuj, Iran. cirruse.salehnasab@gmail.com.

BACKGROUND: Polycystic ovary syndrome (PCOS) is one of the most common endocrine 
disorders, affecting 8–13% of women of reproductive age. Its heterogeneous 
presentation and the variability of diagnostic criteria make accurate diagnosis 
and effective management challenging. Artificial intelligence (AI) methods, 
including machine learning (ML), deep learning (DL), explainable AI (XAI), and 
large language models (LLMs), have recently emerged as promising approaches to 
address these gaps.
OBJECTIVE: This systematic review aimed to provide a comprehensive synthesis of 
AI applications in PCOS, with emphasis on diagnostic performance, biomarker 
discovery, risk prediction, clinical decision support, model interpretability, 
and the emerging use of generative AI.
METHODS: Following PRISMA 2020 guidelines, PubMed, Scopus, and Web of Science 
were searched from inception to March 2025. Eligible studies applied AI 
techniques to PCOS and reported at least one performance metric. Two reviewers 
independently screened and extracted data, with quality appraisal conducted 
using QUADAS-2 and ROBIS. Given the heterogeneity of designs and outcomes, 
findings were narratively synthesized across imaging, clinical/EHR, and 
biomarker/-omics domains.
RESULTS: From 662 retrieved records, 80 studies met the inclusion criteria. 
CNN-based models dominated imaging applications, with accuracies often exceeding 
95% and occasionally reaching 98–99%. Supervised ML approaches, particularly 
random forests and support vector machines, achieved consistent high performance 
in clinical and biochemical datasets. Omics-based studies revealed novel 
biomarkers such as HDDC3, SDC2, MAP1LC3A, and OVGP1. However, only about 
one-quarter of studies applied XAI methods, limiting transparency and clinical 
trust. Early evaluations of LLMs suggested potential for patient education and 
decision support but highlighted risks of bias, hallucination, and lack of 
domain-specific training. Key limitations across studies included small sample 
sizes, class imbalance, methodological heterogeneity, and limited external 
validation.
CONCLUSIONS: AI offers substantial opportunities to advance PCOS diagnosis and 
prediction by integrating multimodal data and reducing diagnostic subjectivity. 
Yet its clinical adoption is constrained by interpretability gaps and 
insufficient validation. Future priorities include large multicenter studies, 
standardized reporting, systematic use of XAI, and careful evaluation of LLMs to 
ensure safe, equitable, and clinically meaningful integration into PCOS care.
SUPPLEMENTARY INFORMATION: The online version contains supplementary material 
available at 10.1186/s12911-025-03255-6.

AI performance Biomarker discovery Explainability Large language models Risk of 
bias Research priorities Clinical implications CNN-based imaging models and 
supervised ML classifiers consistently outperformed traditional diagnostic 
criteria, reducing subjectivity in ultrasound and clinical assessment. 
Omics-driven studies identified novel candidate genes (HDDC3, SDC2, MAP1LC3A, 
OVGP1) with potential for risk stratification. Only ~25% of studies applied XAI 
methods (SHAP, LIME, Grad-CAM). Where used, these improved interpretability and 
clinician confidence, but most models remained opaque. LLMs (ChatGPT, BERT, 
Gemini) are emerging tools for patient communication, clinical note 
summarization, and literature synthesis, but raise concerns about hallucination, 
bias, and lack of domain-specific training. QUADAS-2 and ROBIS assessments 
revealed frequent issues with patient selection, dataset representativeness, and 
lack of external validation. Kaggle datasets were overused, reducing 
generalizability. Future work should focus on multicenter collaborations, 
multimodal integration, routine incorporation of XAI, and robust evaluation of 
LLMs. AI has the potential to enhance early diagnosis and personalized 
management of PCOS, but adoption will depend on reproducibility, transparency, 
and clinician trust rather than accuracy alone.
SUPPLEMENTARY INFORMATION: The online version contains supplementary material 
available at 10.1186/s12911-025-03255-6.

DOI: 10.1186/s12911-025-03255-6
PMCID: PMC12642037
PMID: 41286838

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This review was conducted in strict accordance with the principles 
of the Declaration of Helsinki and ethical standards for research integrity and 
transparency. The study protocol was reviewed and approved by the Iran National 
Committee for Ethics in Biomedical Research (Approval Code: 
IR.YUMS.REC.1402.099). As this article is a systematic review and does not 
involve direct human participation, no individual consent was required. Consent 
for publication: Not applicable. Competing interests: The authors declare no 
competing interests.


54. Clin Kidney J. 2025 Sep 18;18(9):sfaf243. doi: 10.1093/ckj/sfaf243. eCollection 
2025 Sep.

Clinical applications and limitations of large language models in nephrology: a 
systematic review.

Unger Z(1), Soffer S(2)(3), Efros O(3)(4), Chan L(5)(6)(7), Klang E(5)(6), 
Nadkarni GN(5)(6)(7).

Author information:
(1)First Faculty of Medicine, Charles University, Prague, Czech Republic.
(2)Institute of Hematology, Davidoff Cancer Center, Rabin Medical Center, 
Petah-Tikva, Israel.
(3)School of Medicine, Tel Aviv University, Tel Aviv, Israel.
(4)National Hemophilia Center and Thrombosis Institute, Sheba Medical Center, 
Ramat Gan, Israel.
(5)Division of Data-Driven and Digital Medicine (D3M), Icahn School of Medicine 
at Mount Sinai, New York, NY, USA.
(6)Charles Bronfman Institute of Personalized Medicine, Icahn School of Medicine 
at Mount Sinai, New York, NY, USA.
(7)Barbara T. Murphy Division of Nephrology, Icahn School of Medicine at Mount 
Sinai, New York, NY, USA.

BACKGROUND: Large language models (LLMs) have emerged as potential tools in 
healthcare. This systematic review evaluates the applications of text-generative 
conversational LLMs in nephrology, with particular attention to their reported 
advantages and limitations.
METHODS: A systematic search was performed in PubMed, Web of Science, Embase and 
the Cochrane Library in accordance with the Preferred Reporting Items for 
Systematic Reviews and Meta-Analyses guidelines. Eligible studies assessed LLM 
applications in nephrology. PROSPERO registration number CRD42024550169.
RESULTS: Of 1070 records screened, 23 studies met inclusion criteria, addressing 
four clinical applications in nephrology. In patient education (n = 13), GPT-4 
improved the readability of kidney donation information from a 10th to a 4th 
grade level (9.6 ± 1.9 to 4.30 ± 1.71) and Gemini provided the most accurate 
answers to chronic kidney disease questions (Global Quality Score 3.46 ± 0.55). 
Regarding workflow optimization (n = 7), GPT-4 achieved high accuracy (90-94%) 
in managing continuous renal replacement therapy alarms and improved diagnosis 
of diabetes insipidus using chain-of-thought and retrieval-augmented prompting. 
In renal dietary guidance (n = 2), Bard AI led in classifying phosphorus and 
oxalate content of foods (100% and 84%), while GPT-4 and Bing Chat were most 
accurate for potassium classification (81%). For laboratory data interpretation 
(n = 1), Copilot significantly outperformed ChatGPT and Gemini in simulated 
nephrology datasets (median scores 5/5 compared with 4/5 and 4/5; P < .01). 
TRIPOD-LLM assessment revealed frequent omissions in data handling, prompting 
strategies and transparency.
CONCLUSIONS: While LLMs may enhance various aspects of nephrology practice, 
their widespread adoption remains premature. Input-quality dependence and 
limited external validation restrict generalizability. Further research is 
needed to confirm their real-world feasibility and ensure safe clinical 
integration.

© The Author(s) 2025. Published by Oxford University Press on behalf of the ERA.

DOI: 10.1093/ckj/sfaf243
PMCID: PMC12461145
PMID: 41018275

Conflict of interest statement: None declared.


55. Sex Med Rev. 2025 Apr 14;13(2):267-279. doi: 10.1093/sxmrev/qeaf009.

Sexual health in the era of artificial intelligence: a scoping review of the 
literature.

Abou Chawareb E(1), Im BH(2), Lu S(3), Hammad MAM(1), Huang TR(1), Chen H(4), 
Yafi FA(1).

Author information:
(1)Department of Urology, University of California, Irvine, 92697, CA, United 
States.
(2)Department of Urology, Sidney Kimmel Medical College, Thomas Jefferson 
University, Philadelphia, PA 19107, United States.
(3)Chicago Medical School, Rosalind Franklin University of Medicine and Science, 
Chicago, 60064, IL, United States.
(4)School of Osteopathic Medicine, A.T. Still University, San Diego, 92123, CA, 
United States.

INTRODUCTION: Artificial Intelligence (AI) has witnessed significant growth in 
the field of medicine, leveraging machine learning, artificial neuron networks, 
and large language models. These technologies are effective in disease 
diagnosis, education, and prevention, while raising ethical concerns and 
potential challenges. However, their utility in sexual medicine remains 
relatively unexplored.
OBJECTIVE: We aim to provide a comprehensive summary of the status of AI in the 
field of sexual medicine.
METHODS: A comprehensive search was conducted using MeSH keywords, including 
"artificial intelligence," "sexual medicine," "sexual health," and "machine 
learning." Two investigators screened articles for eligibility within the PubMed 
and MEDLINE databases, with conflicts resolved by a third reviewer. Articles in 
English language that reported on AI in sexual medicine and health were 
included. A total of 69 full-text articles were systematically analyzed based on 
predefined inclusion criteria. Data extraction included information on article 
characteristics, study design, assessment methods, and outcomes.
RESULTS: The initial search yielded 905 articles relevant to AI in sexual 
medicine. Upon assessing the full texts of 121 articles for eligibility, 52 
studies unrelated to AI in sexual health were excluded, resulting in 69 articles 
for systematic review. The analysis revealed AI's accuracy in preventing, 
diagnosing, and decision-making in sexually transmitted diseases. AI also 
demonstrated the ability to diagnose and offer precise treatment plans for male 
and female sexual dysfunction and infertility, accurately predict sex from bone 
and teeth imaging, and correctly predict and diagnose sexual orientation and 
relationship issues. AI emerged as a promising modality with significant 
implications for the future of sexual medicine.
CONCLUSIONS: Further research is essential to unlock the potential of AI in 
sexual medicine. AI presents advantages such as accessibility, 
user-friendliness, confidentiality, and a preferred source of sexual health 
information. However, it still lags human healthcare providers in terms of 
compassion and clinical expertise.

© The Author(s) 2025. Published by Oxford University Press on behalf of The 
International Society for Sexual Medicine.

DOI: 10.1093/sxmrev/qeaf009
PMID: 40121550 [Indexed for MEDLINE]


56. Integr Med Res. 2025 Jun;14(2):101150. doi: 10.1016/j.imr.2025.101150. Epub 2025 
May 4.

Evaluating and ranking guidelines on traditional and integrative medicine 
globally: Establishment of the International STAR-TIM Committee.

Yu X(1)(2), Qin Y(3)(4)(5), Estill J(3)(6), Liu H(3)(4)(5), Wang X(7), Bian 
Z(1)(2), Chen Y(2)(3)(4)(5)(7); STAR Secretariat.

Author information:
(1)Chinese EQUATOR Centre, Hong Kong Baptist University, Hong Kong, China.
(2)Vincent V.C. Woo Chinese Medicine Clinical Research Institute, School of 
Chinese Medicine, Hong Kong Baptist University, Hong Kong, China.
(3)Research Unit of Evidence-Based Evaluation and Guidelines, Chinese Academy of 
Medical Sciences (2021RU017), School of Basic Medical Sciences, Lanzhou 
University, Lanzhou, China.
(4)Key Laboratory of Evidence Based Medicine of Gansu Province, Lanzhou, China.
(5)Institute of Health Data Science, Lanzhou University, Lanzhou, China.
(6)Institute of Global Health, University of Geneva, Geneva, Switzerland.
(7)Department of Pediatric Research Institute, Chevidence Lab of Child and 
Adolescent Health, Children's Hospital of Chongqing Medical University, 
Chongqing, China.

To further enhance the quality and transparency of practice guidelines and 
consensus statements, the Scientific, Transparent, and Applicable Rankings 
(STAR) working group was officially established and commenced its work in 2021. 
The STAR tool contains 39 items grouped into 11 domains, with each domain and 
item assigned to a weight that reflects its importance. In 2021-2022, a total of 
266 Chinese Traditional and Integrative Medicine (TIM) guidelines and consensus 
statements met the inclusion criteria for STAR evaluation. After evaluating 
these TIM guidelines and consensus statements, the highest STAR score achieved 
was 96.5 (out of 100), with a median score 23.3 and a mean score of 32.0. As the 
number of TIM guidelines and consensus statements published globally continues 
to increase, the STAR Secretariat aims to establish an International STAR 
Specialty Committee for TIM. Utilizing the integrated STAR evaluation system and 
large language models, the committee seeks to enhance the efficiency of 
evaluations without compromising accuracy. This initiative aims to conduct 
evaluations of international TIM guidelines and consensus statements, thereby 
improving their quality and transparency. Additionally, it will assess the 
importance and necessity of developing the International STAR-TIM Extension.

© 2025 Korea Institute of Oriental Medicine. Published by Elsevier B.V.

DOI: 10.1016/j.imr.2025.101150
PMCID: PMC12145758
PMID: 40487779


57. J Clin Nurs. 2025 Nov 26. doi: 10.1111/jocn.70151. Online ahead of print.

Generative AI at the Bedside: An Integrative Review of Applications and 
Implications in Clinical Nursing Practice.

Watson AL(1), Bond C(2), Aveyard H(3), Smith GD(4), Jackson D(5).

Author information:
(1)College of Nursing, Brigham Young University, Provo, Utah, USA.
(2)Department of Nursing and Midwifery, School of Health and Social Care, 
Sheffield Hallam University, Sheffield, UK.
(3)School of Nursing and Midwifery, Oxford Brookes University, Oxford, UK.
(4)S. K. Yee School of Health Sciences, St. Francis University, Tseung Kwan O, 
Hong Kong.
(5)Faculty of Medicine and Health, Sydney Nursing School, The University of 
Sydney, Sydney, New South Wales (NSW), Australia.

AIM: The aim of this integrative review is to critically appraise and synthesise 
empirical evidence on the clinical applications, outcomes, and implications of 
generative artificial intelligence in nursing practice.
DESIGN: Integrative review following Whittemore and Knafl's five-stage 
framework.
METHODS: Systematic searches were performed for peer-reviewed articles and book 
chapters published between 1 January 2018 and 30 June 2025. Two reviewers 
independently screened titles/abstracts and full texts against predefined 
inclusion/exclusion criteria focused on generative artificial intelligence tools 
embedded in nursing clinical workflow (excluding nursing education-only 
applications). Data were extracted into a standardised matrix and appraised for 
quality using design-appropriate checklists. Guided by Whittemore and Knafl's 
integrative review framework, a constant comparative analysis was applied to 
derive the main themes and subthemes.
DATA SOURCES: CINAHL, MEDLINE, and Embase.
RESULTS: Included literature was a representative mix of single-group quality 
improvement pilots, mixed-method usability and feasibility studies, randomised 
controlled trials, qualitative descriptive and phenomenological studies, as well 
as preliminary and proof-of-concept observational research. Four overarching 
themes emerged: (1) Workflow Integration and Efficiency, (2) AI-Augmented 
Clinical Reasoning, (3) Patient-Facing Communication and Education, and (4) Role 
Boundaries, Ethics and Trust.
CONCLUSION: Generative artificial intelligence holds promise for enhancing 
nursing efficiency, supporting clinical decision making, and extending patient 
communication. However, consistent human validation, ethical boundary setting, 
and more rigorous, longitudinal outcome and equity evaluations are essential 
before widespread clinical adoption.
IMPLICATIONS FOR THE PROFESSION AND PATIENT CARE: Although generative artificial 
intelligence could reduce nurses' documentation workload and routine 
decision-making burden, these gains cannot be assumed. Safe and effective 
integration will require rigorous nurse training, robust governance, transparent 
labelling of AI-generated content, and ongoing evaluation of both clinical 
outcomes and equity impacts. Without these safeguards, generative artificial 
intelligence risks introducing new errors and undermining patient safety and 
trust.
REPORTING METHOD: PRISMA 2020.

© 2025 John Wiley & Sons Ltd.

DOI: 10.1111/jocn.70151
PMID: 41293898


58. J Med Internet Res. 2025 May 15;27:e68998. doi: 10.2196/68998.

Scientific Evidence for Clinical Text Summarization Using Large Language Models: 
Scoping Review.

Bednarczyk L(#)(1), Reichenpfader D(#)(2)(3), Gaudet-Blavignac C(#)(1), Ette 
AK(1)(3), Zaghir J(1)(3), Zheng Y(1)(3), Bensahla A(1)(3), Bjelogrlic M(1)(3), 
Lovis C(1)(3).

Author information:
(1)Division of Medical Information Sciences, University Hospital of Geneva, 
Geneva, Switzerland.
(2)Institute for Patient-centered Digital Health, Bern University of Applied 
Sciences, Biel, Switzerland.
(3)Faculty of Medicine, University of Geneva, Geneva, Switzerland.
(#)Contributed equally

BACKGROUND: Information overload in electronic health records requires effective 
solutions to alleviate clinicians' administrative tasks. Automatically 
summarizing clinical text has gained significant attention with the rise of 
large language models. While individual studies show optimism, a structured 
overview of the research landscape is lacking.
OBJECTIVE: This study aims to present the current state of the art on clinical 
text summarization using large language models, evaluate the level of evidence 
in existing research and assess the applicability of performance findings in 
clinical settings.
METHODS: This scoping review complied with the PRISMA-ScR (Preferred Reporting 
Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) 
guidelines. Literature published between January 1, 2019, and June 18, 2024, was 
identified from 5 databases: PubMed, Embase, Web of Science, IEEE Xplore, and 
ACM Digital Library. Studies were excluded if they did not describe 
transformer-based models, did not focus on clinical text summarization, did not 
engage with free-text data, were not original research, were nonretrievable, 
were not peer-reviewed, or were not in English, French, Spanish, or German. Data 
related to study context and characteristics, scope of research, and evaluation 
methodologies were systematically collected and analyzed by 3 authors 
independently.
RESULTS: A total of 30 original studies were included in the analysis. All used 
observational retrospective designs, mainly using real patient data (n=28, 93%). 
The research landscape demonstrated a narrow research focus, often centered on 
summarizing radiology reports (n=17, 57%), primarily involving data from the 
intensive care unit (n=15, 50%) of US-based institutions (n=19, 73%), in English 
(n=26, 87%). This focus aligned with the frequent reliance on the open-source 
Medical Information Mart for Intensive Care dataset (n=15, 50%). Summarization 
methodologies predominantly involved abstractive approaches (n=17, 57%) on 
single-document inputs (n=4, 13%) with unstructured data (n=13, 43%), yet 
reporting on methodological details remained inconsistent across studies. Model 
selection involved both open-source models (n=26, 87%) and proprietary models 
(n=7, 23%). Evaluation frameworks were highly heterogeneous. All studies 
conducted internal validation, but external validation (n=2, 7%), failure 
analysis (n=6, 20%), and patient safety risks analysis (n=1, 3%) were 
infrequent, and none reported bias assessment. Most studies used both automated 
metrics and human evaluation (n=16, 53%), while 10 (33%) used only automated 
metrics, and 4 (13%) only human evaluation.
CONCLUSIONS: Key barriers hinder the translation of current research into 
trustworthy, clinically valid applications. Current research remains exploratory 
and limited in scope, with many applications yet to be explored. Performance 
assessments often lack reliability, and clinical impact evaluations are 
insufficient raising concerns about model utility, safety, fairness, and data 
privacy. Advancing the field requires more robust evaluation frameworks, a 
broader research scope, and a stronger focus on real-world applicability.

©Lydie Bednarczyk, Daniel Reichenpfader, Christophe Gaudet-Blavignac, Amon Kenna 
Ette, Jamil Zaghir, Yuanyuan Zheng, Adel Bensahla, Mina Bjelogrlic, Christian 
Lovis. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 15.05.2025.

DOI: 10.2196/68998
PMCID: PMC12123242
PMID: 40371947 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


59. BMC Med Inform Decis Mak. 2025 Sep 1;25(1):325. doi: 10.1186/s12911-025-03035-2.

Comparison of the readability of ChatGPT and Bard in medical communication: a 
meta-analysis.

DeTemple DE(1)(2)(3), Meine TC(4)(5).

Author information:
(1)Clinic for General, Visceral and Transplant Surgery, Hannover Medical School, 
Carl-Neuberg-Strasse 1, 30625, Hannover, Germany.
(2)PRACTIS Clinician Scientist Program, Dean's Office for Academic Career 
Development, Hannover Medical School, Hannover, Germany.
(3)Clinic for General, Visceral, Pediatric and Transplantation Surgery, 
University Hospital RWTH Aachen, Aachen, Germany.
(4)PRACTIS Clinician Scientist Program, Dean's Office for Academic Career 
Development, Hannover Medical School, Hannover, Germany. 
meine.timo@mh-hannover.de.
(5)Institute for Diagnostic and Interventional Radiology, Hannover Medical 
School, Carl-Neuberg-Strasse 1, 30625, Hannover, Germany. 
meine.timo@mh-hannover.de.

BACKGROUND: To synthesize the results of various studies on the readability of 
ChatGPT and Bard in medical communication.
METHODS: Systemic literature research was conducted in PubMed, Ovid/Medline, 
CINAHL, Web-of-Science, Scopus and GoogleScholar to detect relevant publications 
(inclusion criteria: original research articles, English language, medical 
topic, ChatGPT-3.5/-4.0, Bard/Gemini, Flesch Reading Ease Score (FRE), Flesch 
Kincaid Grade Level (FKGL)). Study quality was analyzed using modified 
Downs-and-Black checklist (max. 8 points), adapted for studies on large language 
model. Analysis was performed on text simplification and/or text generation with 
ChatGPT-3.5/-4.0 versus Bard/Gemini. Meta-analysis was conducted, if outcome 
parameter was reported ≥ 3 studies. In addition, subgroup-analyses among 
different chatbot versions were performed. Publication bias was analyzed.
RESULTS: Overall, 59 studies with 2342 items were analyzed. Study quality was 
limited with a mean of 6 points for FRE and 7 points for FKGL. Meta-analysis of 
text simplification for FRE between ChatGPT-3.5/-4.0 and Bard/Gemini was not 
significant (mean difference (MD):5.03; 95%-confidence interval 
(CI):-20.05,30.11; p = 0.48). FKGL of simplified texts of ChatGPT-3.5/-4.0 and 
Bard/Gemini was borderline significant (MD:-1.59; CI:-3.15,-0.04; p = 0.05) and 
subgroup-analysis between ChatGPT-4.0 and Bard was not significant (MD:-1.68; 
CI:-3.53,0.17; p = 0.07). Focused on text acquisition, MD for FRE and FKGL of 
studies on ChatGPT-3.5/-4.0- and Bard/Gemini-generated texts were significant 
(MD:-10.36; CI:-13.08,-7.64; p < 0.01 / MD:1.62; CI:1.09,2.15; p < 0.01). 
Subgroup-analysis of FRE was significant for ChatGPT-3.5 vs. Bard (MD:-16.07, 
CI:-24.90,-7.25; p < 0.01), ChatGPT-3.5 vs. Gemini (MD:-4.51; CI:-8.73,-0.29: 
p = 0.04), ChatGPT-4.0 vs. Bard (MD:-12.01, CI:-16.22,-7.81; p < 0.01) and 
ChatGPT-4.0 vs. Gemini (MD:-7.91, CI:-11.68,-4.15; p < 0.01). Analysis of FKGL 
in the subgroups was significant for ChatGPT-3.5 vs. Bard (MD:2.85, 
CI:1.98,3.73; p < 0.01), ChatGPT-3.5 vs. Gemini (MD:1.21, CI:0.50,1.93; 
p < 0.01) and ChatGPT-4.0 vs. Gemini (MD:1.95, CI:1.05,2.86; p < 0.01), but it 
was not significant for ChatGPT-4.0 vs. Bard (MD:0.64, CI:-0.46,1.74; p = 0.24). 
Egger's test was significant in text generation for FRE and FKGL (p < 0.01 / 
p < 0.01) and in subgroup ChatGPT-4.0 vs. Bard and ChatGPT-4.0 vs. Gemini 
(p < 0.01 / p = 0.02) for FRE as well as in subgroups ChatGPT-3.5 vs. Bard and 
ChatGPT-4.0 vs. Gemini for FKGL (p < 0.01 / p < 0.01).
CONCLUSION: Readability of spontaneously generated texts by Bard/Gemini was 
slightly superior compared to ChatGPT-3.5/-4.0 and readability of simplified 
texts by ChatGPT-3.5/-4.0 tended to be improved compared to Bard. Results are 
limited due study quality and publication bias. Standardized reporting could 
improve study quality and chatbot development.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-03035-2
PMCID: PMC12403948
PMID: 40890707 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


60. J Med Internet Res. 2025 Sep 22;27:e73212. doi: 10.2196/73212.

Large Language Models in Neurological Practice: Real-World Study.

Maiorana NV(1), Marceglia S(1), Treddenti M(1)(2), Tosi M(1)(2), Guidetti M(1), 
Creta MF(1)(2), Bocci T(1)(2), Oliveri S(1)(2), Martinelli Boneschi F(1)(2), 
Priori A(1)(2).

Author information:
(1)Aldo Ravelli Center for Neurotechnology and Experimental Brain Therapeutics, 
Department of Health Sciences, University of Milan, Via Antonio di Rudinì, 8, 
Milan, 20142, Italy, 39 02 50323233.
(2)Clinical Neurology Unit, Department of Health Sciences, Azienda 
Socio-Sanitaria Territoriale Santi Paolo e Carlo, University of Milan, Milan, 
Italy.

BACKGROUND: Large language models (LLMs) such as ChatGPT (OpenAI) and Gemini 
(Google) are increasingly explored for their potential in medical diagnostics, 
including neurology. Their real-world applicability remains inadequately 
assessed, particularly in clinical workflows where nuanced decision-making is 
required.
OBJECTIVE: This study aims to evaluate the diagnostic accuracy and 
appropriateness of clinical recommendations provided by 
not-specifically-trained, freely available ChatGPT and Gemini, compared to 
neurologists, using real-world clinical cases.
METHODS: This study consisted of an experimental evaluation of LLMs' diagnostic 
performance presenting real-world neurology cases to ChatGPT and Gemini, 
comparing their performance with that of clinical neurologists. The study was 
conducted simulating a first visit using information from anonymized patient 
records from the Neurology Department of the ASST Santi Paolo e Carlo Hospital, 
ensuring a real-world clinical context. The study involved a cohort of 28 
anonymized patient cases covering a range of neurological conditions and 
diagnostic complexities representative of daily clinical practice. The primary 
outcome was diagnostic accuracy of both neurologists and LLMs, defined as 
concordance with discharge diagnoses. Secondary outcomes included the 
appropriateness of recommended diagnostic tests, interrater agreement, and the 
extent of additional prompting required for accurate responses.
RESULTS: Neurologists achieved a diagnostic accuracy of 75%, outperforming 
ChatGPT (54%) and Gemini (46%). Both LLMs demonstrated limitations in nuanced 
clinical reasoning and overprescribed diagnostic tests in 17%-25% of cases. In 
addition, complex or ambiguous cases required further prompting to refine 
artificial intelligence-generated responses. Interrater reliability analysis 
using Fleiss Kappa showed a moderate-to-substantial level of agreement among 
raters (κ=0.47, SE 0.077; z=6.14, P<.001), indicating agreement between raters.
CONCLUSIONS: While LLMs show potential as supportive tools in neurology, they 
currently lack the depth required for independent clinical decision-making when 
using freely available LLMs without previous specific training. The moderate 
agreement observed among human raters underscores the variability even in expert 
judgment and highlights the importance of rigorous validation when integrating 
artificial intelligence tools into clinical workflows. Future research should 
focus on refining LLM capabilities and developing evaluation methodologies that 
reflect the complexities of real-world neurological practice, ensuring 
effective, responsible, and safe use of such promising technologies.

© Natale Vincenzo Maiorana, Sara Marceglia, Mauro Treddenti, Mattia Tosi, Matteo 
Guidetti, Maria Francesca Creta, Tommaso Bocci, Serena Oliveri, Filippo 
Martinelli Boneschi, Alberto Priori. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/73212
PMCID: PMC12453287
PMID: 40982758 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: SM is founder, 
shareholder, and scientific advisor of Newronika Spa, a spin-off company of the 
University of Milan and of the Fondazione IRCCS Ca’Granda Ospedale Maggiore 
Policlinico of Milan, in the field of neuromodulation devices. All other authors 
declared no conflicts of interest.


61. JHEP Rep. 2025 Sep 3;7(12):101579. doi: 10.1016/j.jhepr.2025.101579. eCollection 
2025 Dec.

Evaluating large language models' performance in answering common questions on 
drug-induced liver injury.

Dong Y(1), Zhang Z(2), Zhi Y(1), Li X(1)(3), Guo T(1), He L(4), Zhao S(5), Yang 
X(5), Tang J(1), Zhong W(1), Niu Q(6), Ma M(7), Huang Z(4), Mao Y(1).

Author information:
(1)Division of Gastroenterology and Hepatology, Renji Hospital, Shanghai Jiao 
Tong University School of Medicine; NHC Key Laboratory of Digestive Diseases; 
Shanghai Research Center of Fatty Liver Disease, Shanghai 200001, China.
(2)Department of Ophthalmology, Ninth People's Hospital, Shanghai Jiao Tong 
University School of Medicine, Shanghai 200025, China.
(3)Department of Hepatology, Hepatology Research Institute, The First Affiliated 
Hospital, Fujian Medical University, Fujian Clinical Research Center for Liver 
and Intestinal Diseases, Fuzhou, Fujian 350001, China.
(4)Department of Liver Diseases, Mengchao Hepatobiliary Hospital of Fujian 
Medical University, Fuzhou, Fujian 350025, China.
(5)Tangqiao Community Health Service Center, Shanghai, China.
(6)Department of Liver Diseases, Qingdao University Hospital, Qingdao, China.
(7)Department of General Surgery, Tianjin Medical University General Hospital, 
Tianjin 300052, China.

BACKGROUND & AIMS: Drug-induced liver injury (DILI) is a complex condition often 
linked to medication behaviors, with patient education having a crucial role in 
optimizing outcomes. Large language models (LLMs) could serve as promising tools 
for scalable patient support, but their utility remains unclear. This study 
systematically evaluated the capability of six popular open- and closed-source 
LLMs in addressing common DILI-related queries, focusing on patient-centered 
education.
METHODS: Twenty-eight frequently asked DILI questions were collected with input 
from hepatologists and patients (n = 15), and categorized into six clinical 
domains. Responses from six LLMs (GPT-4, GPT-3.5, Claude-2, Claude-1.3, Gemini, 
and LLaMA-3.1-405B) were anonymized, randomized, and independently evaluated by 
three hepatologists for accuracy, comprehensiveness, and safety. Additional 
analyses included automated readability assessment, domain-specific analysis, 
detailed expert-led error analysis, and direct comparison with physician 
responses.
RESULTS: LLaMA-3.1-405B achieved the highest performance across most domains, 
with mean accuracy, comprehensiveness, and safety scores of 8.18 ± 1.68, 3.86 ± 
0.70, and 4.02 ± 0.84, respectively, significantly surpassing other models 
(Dunn's post hoc test, all p <0.05). O1-preview ranked second (accuracy, 7.29 ± 
1.38; safety, 3.80 ± 0.92), whereas GPT-3.5-Turbo consistently performed worst 
(accuracy, 4.61 ± 1.17; comprehensiveness, 2.13 ± 0.79). In direct comparison 
with physicians, both LLaMA-3.1-405B and o1-preview significantly outperformed 
residents and primary care physicians across all metrics (p <0.05). Error 
analysis showed that omission of crucial information accounted for 72% of 
errors, predominantly in GPT-3.5-Turbo, whereas hallucinations were rare (<10%) 
but notable in LLaMA outputs.
CONCLUSION: This study represents the first systematic evaluation of LLMs for 
DILI-focused patient education. High-performing, publicly accessible LLMs 
demonstrate the potential to deliver accurate, comprehensive, and safe health 
information, even surpassing physician responses.
IMPACT AND IMPLICATIONS: DILI is a complex and multidisciplinary condition where 
patient understanding has a crucial role in management outcomes, yet educational 
resources remain scarce. By systematically evaluating six widely used LLMs, 
including both open- and closed-source models, this study provides new insights 
into the potential of artificial intelligence tools to enhance patient education 
and supplement clinical communication in hepatology. These findings are 
particularly important for physicians, patient educators, and healthcare 
policymakers seeking scalable and reliable strategies to support liver disease 
management. Although further refinement and clinical oversight are necessary to 
ensure content safety and accuracy, integrating LLM-based tools into patient 
education initiatives could offer a practical pathway to improve health literacy 
and engagement in real-world settings.

© 2025 The Author(s).

DOI: 10.1016/j.jhepr.2025.101579
PMCID: PMC12657742
PMID: 41321931

Conflict of interest statement: The authors declare no conflicts of interest. 
Please refer to the accompanying ICMJE disclosure forms for further details.


62. Intensive Crit Care Nurs. 2025 Dec;91:104213. doi: 10.1016/j.iccn.2025.104213. 
Epub 2025 Sep 1.

The Ideal Human Care in Green ICU: An integrated AI framework for future ICU 
care.

Arabfard M(1), Jeffery AD(2), Moradian S(3), He HG(4), Pandian V(5), 
Vahedian-Azimi A(6).

Author information:
(1)Chemical Injuries Research Center, Systems Biology and Poisonings Institute, 
Baqiyatallah University of Medical Sciences, Tehran, Iran. Electronic address: 
arabfard@gmail.com.
(2)Biomedical Informatics and Nursing, Vanderbilt University Medical Center. 
Nashville, TN, USA. Electronic address: a.jeffery@vumc.org.
(3)Nursing Care Research Center, Clinical Sciences Institute, Nursing Faculty, 
Baqiyatallah University of Medical Sciences, Tehran, Iran. Electronic address: 
t.moradyan@yahoo.com.
(4)Alice Lee Centre for Nursing Studies, Yong Loo Lin School of Medicine, 
National University of Singapore, Level 2, Clinical Research Center, Bock MD11, 
10 Medical Drive, Singapore 117597 Singapore. Electronic address: 
nurhhg@nus.edu.sg.
(5)Associate Dean for Graduate Education and Professor of Nursing, Executive 
Director of Immersive Learning and Digital Innovation, Ross and Carol Nese 
College of Nursing, The Pennsylvania State University, University Park, PA, USA. 
Electronic address: vpandian@psu.edu.
(6)Nursing Care Research Center, Clinical Sciences Institute, Nursing Faculty, 
Baqiyatallah University of Medical Sciences, Tehran, Iran. Electronic address: 
Amirvahedian63@gmail.com.

BACKGROUND: Intensive Care Units (ICUs) present a high-stakes environment where 
timely decision-making is critical for managing patients with life-threatening 
conditions. The continuous influx of complex data often challenges clinicians, 
increasing the risk of errors. Artificial Intelligence (AI) offers 
transformative potential to enhance ICU care by supporting data analysis, 
decision-making, and workflow efficiency.
OBJECTIVE: This review aims to explore current applications of AI in ICUs, 
assess their impact on clinical outcomes, workflow optimization, and ethical 
considerations, and propose an Integrated AI Framework for enhanced critical 
care delivery.
METHODS: A literature search was conducted across PubMed, Scopus, and Web of 
Science, focusing on studies published between 2014 and 2024. The data were 
synthesized using an inductive thematic analysis approach to evaluate AI's 
impact on clinical outcomes and to identify key barriers to its integration.
RESULTS: AI has demonstrated significant advancements in ICU care, including 
early detection of sepsis, prediction of cardiac arrest, and workflow 
optimization through decision support systems. Predictive models reduced 
sepsis-related mortality by up to 20%, while workflow enhancements improved 
medication accuracy by 30% and reduced adverse events by 25%. Advanced 
techniques such as natural language processing (NLP), large language models 
(LLMs), and multimodal data integration have further streamlined ICU operations. 
However, challenges remain, including algorithmic bias, data privacy concerns, 
and integration barriers.
PROPOSED MODEL: The Ideal Human Care in Green ICU model integrates advanced AI 
technologies with multidisciplinary collaboration to provide personalized, 
evidence-based, and patient-centered care. This model emphasizes ethical AI 
practices, transparency, and family engagement to ensure responsible 
implementation.
CONCLUSION: AI can transform ICU care by improving outcomes and workflows, but 
ethical, practical, and explainable challenges must be addressed through 
diverse, validated research.
IMPLICATIONS FOR CLINICAL PRACTICE: AI integration in ICUs improves patient 
outcomes and workflows by enabling early detection and precise treatment, but 
ethical issues and real-world validation are crucial.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.iccn.2025.104213
PMID: 40897580 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


63. J Med Syst. 2025 Sep 4;49(1):110. doi: 10.1007/s10916-025-02246-4.

How Well Do ChatGPT and Claude Perform in Study Selection for Systematic Review 
in Obstetrics.

Insuk S(1), Boonpattharatthiti K(2)(3), Booncharoen C(1), Chaipitak P(1), Rashid 
M(4), Veettil SK(5)(6), Lai NM(7), Chaiyakunapruk N(4)(8), Dhippayom T(9)(10).

Author information:
(1)Faculty of Pharmaceutical Sciences, Naresuan University, Phitsanulok, 
Thailand.
(2)The Research Unit of Evidence Synthesis (TRUES), Faculty of Pharmaceutical 
Sciences, Naresuan University, Phitsanulok, Thailand.
(3)Faculty of Pharmaceutical Sciences, Burapha University, Chon buri, Thailand.
(4)Department of Pharmacotherapy, College of Pharmacy, University of Utah, Salt 
Lake City, UT, USA.
(5)Department of Pharmacy practice, School of Pharmacy, IMU University, Kuala 
Lumpur, Malaysia.
(6)School of Medicine, Taylor's University, Selangor, Subang Jaya, Malaysia.
(7)School of Medicine, Faculty of Health and Medical Sciences, Taylor's 
University, Subang Jaya, Malaysia.
(8)IDEAS Centre, Veterans Affairs Salt Lake City Healthcare System, Salt Lake 
City, UT, USA.
(9)The Research Unit of Evidence Synthesis (TRUES), Faculty of Pharmaceutical 
Sciences, Naresuan University, Phitsanulok, Thailand. teerapond@nu.ac.th.
(10)Department of Pharmacotherapy, College of Pharmacy, University of Utah, Salt 
Lake City, UT, USA. teerapond@nu.ac.th.

The use of generative AI in systematic review workflows has gained attention for 
enhancing study selection efficiency. However, evidence on its screening 
performance remains inconclusive, and direct comparisons between different 
generative AI models are still limited. The objective of this study is to 
evaluate the performance of ChatGPT-4o and Claude 3.5 Sonnet in the study 
selection process of a systematic review in obstetrics. A literature search was 
conducted using PubMed, EMBASE, Cochrane CENTRAL, and EBSCO Open Dissertations 
from inception till February 2024. Titles and abstracts were screened using a 
structured prompt-based approach, comparing decisions by ChatGPT, Claude and 
junior researchers with decisions by an experienced researcher serving as the 
reference standard. For the full-text review, short and long prompt strategies 
were applied. We reported title/abstract screening and full-text review 
performances using accuracy, sensitivity (recall), precision, F1-score, and 
negative predictive value. In the title/abstract screening phase, human 
researchers demonstrated the highest accuracy (0.9593), followed by Claude 
(0.9448) and ChatGPT (0.9138). The F1-score was the highest among human 
researchers (0.3853), followed by Claude (0.3724) and ChatGPT (0.2755). Negative 
predictive value (NPV) was high across all screeners: ChatGPT (0.9959), Claude 
(0.9961), and human researchers (0.9924). In the full-text screening phase, 
ChatGPT with a short prompt achieved the highest accuracy (0.904), highest 
F1-score (0.90), and NPV of 1.00, surpassing the performance of Claude and human 
researchers. Generative AI models perform close to human levels in study 
selection, as evidenced in obstetrics. Further research should explore their 
integration into evidence synthesis across different fields.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02246-4
PMID: 40906005 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Human Ethics and Consent to 
Participate: Not applicable. Our research did not require Institutional Review 
Board (IRB) review or approval as it did not involve human subjects, 
identifiable private information, or biological specimens that would fall under 
IRB oversight requirements. Clinical Trial Number: Not applicable. Competing 
Interests: The authors declare no competing interests.


64. BMC Emerg Med. 2025 Mar 12;25(1):43. doi: 10.1186/s12873-024-01159-8.

AI-assisted decision-making in mild traumatic brain injury.

Yigit Y(1)(2), Kaynak MF(3), Alkahlout B(4), Ahmed S(4), Günay S(5), Ozbek 
AE(6).

Author information:
(1)Department of Emergency Medicine, Hamad Medical Corporation, Hamad General 
Hospital, Doha, 3050, Qatar. yyigit@hamad.qa.
(2)Blizard Institute, Queen Mary University, London, UK. yyigit@hamad.qa.
(3)Primary Health Care Corporation, Doha, Qatar.
(4)Department of Emergency Medicine, Hamad Medical Corporation, Hamad General 
Hospital, Doha, 3050, Qatar.
(5)Çorum Erol Olçok Education and Research Hospital, Department of Emergency 
Medicine, Hitit University, Çorum, Turkey.
(6)Department of Emergency Medicine, Kocaeli City Hospital, Kocaeli, Turkey.

OBJECTIVE: This study evaluates the potential use of ChatGPT in aiding clinical 
decision-making for patients with mild traumatic brain injury (TBI) by assessing 
the quality of responses it generates for clinical care.
METHODS: Seventeen mild TBI case scenarios were selected from PubMed Central, 
and each case was analyzed by GPT-4 (March 21, 2024, version) between April 11 
and April 20, 2024. Responses were evaluated by four emergency medicine 
specialists, who rated the ease of understanding, scientific adequacy, and 
satisfaction with each response using a 7-point Likert scale. Evaluators were 
also asked to identify critical errors, defined as mistakes in clinical care or 
interpretation that could lead to morbidity or mortality. The readability of 
GPT-4's responses was also assessed using the Flesch Reading Ease and 
Flesch-Kincaid Grade Level tools.
RESULTS: There was no significant difference in the ease of understanding 
between responses with and without critical errors (p = 0.133). However, 
responses with critical errors significantly reduced satisfaction and scientific 
adequacy (p < 0.001). GPT-4 responses were significantly more difficult to read 
than the case descriptions (p < 0.001).
CONCLUSION: GPT-4 demonstrates potential utility in clinical decision-making for 
mild TBI management, offering scientifically appropriate and comprehensible 
responses. However, critical errors and readability issues limit its immediate 
implementation in emergency settings without oversight by experienced medical 
professionals.

© 2024. The Author(s).

DOI: 10.1186/s12873-024-01159-8
PMCID: PMC11905459
PMID: 40075297 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


65. Dentomaxillofac Radiol. 2025 Nov 1;54(8):613-631. doi: 10.1093/dmfr/twaf060.

The performance of large language models in dentomaxillofacial radiology: a 
systematic review.

Liu Z(1), Nalley A(1), Hao J(1), Ai QYH(2), Yeung AWK(1), Tanaka R(1), Hung 
KF(1).

Author information:
(1)Oral and Maxillofacial Radiology, Division of Applied Oral Sciences and 
Community Dental Care, Faculty of Dentistry, The University of Hong Kong, Hong 
Kong SAR, 999077, China.
(2)Department of Diagnostic Radiology, Li Ka Shing Faculty of Medicine, The 
University of Hong Kong, Hong Kong SAR, 999077, China.

OBJECTIVES: This study aimed to systematically review the current performance of 
large language models (LLMs) in dento-maxillofacial radiology (DMFR).
METHODS: Five electronic databases were used to identify studies that developed, 
fine-tuned, or evaluated LLMs for DMFR-related tasks. Data extracted included 
study purpose, LLM type, images/text source, applied language, dataset 
characteristics, input and output, performance outcomes, evaluation methods, and 
reference standards. Customized assessment criteria adapted from the TRIPOD-LLM 
reporting guideline were used to evaluate the risk-of-bias in the included 
studies specifically regarding the clarity of dataset origin, the robustness of 
performance evaluation methods, and the validity of the reference standards.
RESULTS: The initial search yielded 1621 titles, and 19 studies were included. 
These studies investigated the use of LLMs for tasks including the production 
and answering of DMFR-related qualification exams and educational questions 
(n = 8), diagnosis and treatment recommendations (n = 7), and radiology report 
generation and patient communication (n = 4). LLMs demonstrated varied 
performance in diagnosing dental conditions, with accuracy ranging from 37% to 
92.5% and expert ratings for differential diagnosis and treatment planning 
between 3.6 and 4.7 on a 5-point scale. For DMFR-related qualification exams and 
board-style questions, LLMs achieved correctness rates between 33.3% and 86.1%. 
Automated radiology report generation showed moderate performance with accuracy 
ranging from 70.4% to 81.3%.
CONCLUSIONS: LLMs demonstrate promising potential in DMFR, particularly for 
diagnostic, educational, and report generation tasks. However, their current 
accuracy, completeness, and consistency remain variable. Further development, 
validation, and standardization are needed before LLMs can be reliably 
integrated as supportive tools in clinical workflows and educational settings.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
British Institute of Radiology and the International Association of 
Dentomaxillofacial Radiology.

DOI: 10.1093/dmfr/twaf060
PMCID: PMC12653761
PMID: 40796316 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflict of interest.


66. Laryngoscope Investig Otolaryngol. 2025 Mar 22;10(2):e70130. doi: 
10.1002/lio2.70130. eCollection 2025 Apr.

Evaluation of Large Language Models' Concordance With Guidelines on Olfaction.

Shaari AL(1), Saad AM(1), Patil D(1), Yanik J(1), Hsueh WD(1), Eloy 
JA(1)(2)(3)(4)(5), Filimonov A(1).

Author information:
(1)Department of Otolaryngology - Head and Neck Surgery Rutgers New Jersey 
Medical School Newark New Jersey USA.
(2)Center for Skull Base and Pituitary Surgery, Neurological Institute of New 
Jersey, Rutgers New Jersey Medical School Newark New Jersey USA.
(3)Department of Neurological Surgery Rutgers New Jersey Medical School Newark 
New Jersey USA.
(4)Department of Ophthalmology and Visual Science Rutgers New Jersey Medical 
School Newark New Jersey USA.
(5)Department of Otolaryngology and Facial Plastic Surgery Saint Barnabas 
Medical Center - RWJBarnabas Health Livingston New Jersey USA.

OBJECTIVE: To assess the concordance of artificial intelligence (AI)-generated 
information with the 2022 International Consensus Statement on Allergy and 
Rhinology: Olfaction (ICAR-O).
METHODS: Forty-two guidelines were extracted from the ICAR-O. Each guideline was 
converted into a question, which was presented to ChatGPT version 4.o and Google 
Gemini. Concordance was deemed an agreement between the AI response and the 
clinical recommendation. Credibility was granted if the AI platform provided a 
credible resource. Accuracy was graded on a Likert scale (0: entirely inaccurate 
information, 1: mix of accurate and inaccurate information, 2: entirely accurate 
information). Statistical analysis was performed.
RESULTS: A total of 84 responses were generated. The mean accuracy of the 
ChatGPT and Gemini responses was 1.85 and 1.48 out of 2, respectively, 
indicating that the responses contained a mix of accurate and inaccurate 
information. ChatGPT responses were significantly more accurate than Gemini 
responses (p = 0.001). Of the ChatGPT responses, 78.57% (N = 33) were concordant 
with the ICAR-O guidelines and 100% (N = 42) cited a credible resource. Of the 
Gemini responses, 66.67% (N = 28) were concordant and 97.62% (N = 41) cited a 
credible resource. There were no significant differences in concordance 
(p = 0.22) or credibility (p = 0.31) between the AI platforms.
CONCLUSION: ChatGPT provided more accurate information than Gemini on olfaction. 
However, overall, both platforms did not consistently align with clinical 
guidelines. AI platforms require further evaluation before clinical 
implementation or use as educational adjuncts.
LEVEL OF EVIDENCE: N/A.

© 2025 The Author(s). Laryngoscope Investigative Otolaryngology published by 
Wiley Periodicals LLC on behalf of The Triological Society.

DOI: 10.1002/lio2.70130
PMCID: PMC11929122
PMID: 40124251

Conflict of interest statement: The authors declare no conflicts of interest.


67. J Med Internet Res. 2025 Jul 11;27:e71916. doi: 10.2196/71916.

Implementing Large Language Models in Health Care: Clinician-Focused Review With 
Interactive Guideline.

Li H(1)(2), Fu JF(3)(4)(5), Python A(1)(6)(7).

Author information:
(1)Center for Data Science, Zhejiang University, Hangzhou, China.
(2)School of Mathematical Sciences, Zhejiang University, Hangzhou, China.
(3)School of Medicine, Children's Hospital of Zhejiang University, Hangzhou, 
China.
(4)National Clinical Research Center for Child Health, Hangzhou, China.
(5)National Regional Center for Children's Health, Hangzhou, China.
(6)School of Medicine, Zhejiang University, Hangzhou, China.
(7)Centre for Human Genetics, Nuffield Department of Medicine, University of 
Oxford, Oxford, United Kingdom.

BACKGROUND: Large language models (LLMs) can generate outputs understandable by 
humans, such as answers to medical questions and radiology reports. With the 
rapid development of LLMs, clinicians face a growing challenge in determining 
the most suitable algorithms to support their work.
OBJECTIVE: We aimed to provide clinicians and other health care practitioners 
with systematic guidance in selecting an LLM that is relevant and appropriate to 
their needs and facilitate the integration process of LLMs in health care.
METHODS: We conducted a literature search of full-text publications in English 
on clinical applications of LLMs published between January 1, 2022, and March 
31, 2025, on PubMed, ScienceDirect, Scopus, and IEEE Xplore. We excluded papers 
from journals below a set citation threshold, as well as papers that did not 
focus on LLMs, were not research based, or did not involve clinical 
applications. We also conducted a literature search on arXiv within the same 
investigated period and included papers on the clinical applications of 
innovative multimodal LLMs. This led to a total of 270 studies.
RESULTS: We collected 330 LLMs and recorded their application frequency in 
clinical tasks and frequency of best performance in their context. On the basis 
of a 5-stage clinical workflow, we found that stages 2, 3, and 4 are key stages 
in the clinical workflow, involving numerous clinical subtasks and LLMs. 
However, the diversity of LLMs that may perform optimally in each context 
remains limited. GPT-3.5 and GPT-4 were the most versatile models in the 5-stage 
clinical workflow, applied to 52% (29/56) and 71% (40/56) of the clinical 
subtasks, respectively, and they performed best in 29% (16/56) and 54% (30/56) 
of the clinical subtasks, respectively. General-purpose LLMs may not perform 
well in specialized areas as they often require lightweight prompt engineering 
methods or fine-tuning techniques based on specific datasets to improve model 
performance. Most LLMs with multimodal abilities are closed-source models and, 
therefore, lack of transparency, model customization, and fine-tuning for 
specific clinical tasks and may also pose challenges regarding data protection 
and privacy, which are common requirements in clinical settings.
CONCLUSIONS: In this review, we found that LLMs may help clinicians in a variety 
of clinical tasks. However, we did not find evidence of generalist clinical LLMs 
successfully applicable to a wide range of clinical tasks. Therefore, their 
clinical deployment remains challenging. On the basis of this review, we propose 
an interactive online guideline for clinicians to select suitable LLMs by 
clinical task. With a clinical perspective and free of unnecessary technical 
jargon, this guideline may be used as a reference to successfully apply LLMs in 
clinical settings.

©HongYi Li, Jun-Fen Fu, Andre Python. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org), 11.07.2025.

DOI: 10.2196/71916
PMCID: PMC12299950
PMID: 40644686 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


68. Int J Med Inform. 2025 Dec 31;209:106250. doi: 10.1016/j.ijmedinf.2025.106250. 
Online ahead of print.

Large language models versus healthcare professionals in providing medical 
information to patient questions: A systematic review.

Jacobs MMG(1), Oosterhoff JHF(2), Agricola R(3), van der Weegen W(4).

Author information:
(1)Department of Orthopedics, Radboudumc, Nijmegen, the Netherlands; Sports & 
Orthopedics Research Centre, St. Anna Hospital, Geldrop, the Netherlands. 
Electronic address: maud.jacobs@radboudumc.nl.
(2)Faculty of Technology Policy and Management, Delft University of Technology, 
Delft, the Netherlands; Department of Orthopaedic Surgery, University Medical 
Centre Groningen, Groningen, the Netherlands.
(3)Sports & Orthopedics Research Centre, St. Anna Hospital, Geldrop, the 
Netherlands; Department of Orthopedics and Sports Medicine, Erasmus University 
Medical Center, Rotterdam, the Netherlands.
(4)Sports & Orthopedics Research Centre, St. Anna Hospital, Geldrop, the 
Netherlands; Department of Anesthesiology, Pain and Palliative Medicine, 
Radboudumc, Nijmegen, the Netherlands.

OBJECTIVE: The rapid expansion of digital healthcare has heightened the volume 
of patient communication, thereby increasing the workload for healthcare 
professionals. Large Language Models (LLMs) hold promises for offering automated 
responses to patient questions relayed through eHealth platforms, yet concerns 
persist regarding their effectiveness, accuracy, and limitations in healthcare 
settings. This study aims to evaluate the current evidence on the performance 
and perceived suitability of LLMs in healthcare, focusing on their role in 
supporting clinical decision-making and patient communication.
MATERIALS AND METHODS: A systematic search in PubMed and Embase up to June 11, 
2025 identified 330 studies, of which 20 met the inclusion criteria for 
comparing the accuracy and adequacy of medical information provided by LLMs 
versus healthcare professionals and guidelines. The search strategy combined 
terms related to LLMs, healthcare professionals, and patient questions. The 
ROBINS-I tool assessed the risk of bias.
RESULTS: A total of nineteen studies focused on medical specialties and one on 
the primary care setting. Twelve studies favored the responses generated by 
LLMs, six reported mixed results, and two favored the healthcare professionals' 
response. Bias components generally scored moderate to low, indicating a low 
risk of bias.
DISCUSSION AND CONCLUSIONS: The review summarizes current evidence on the 
accuracy and adequacy of medical information provided by LLMs in response to 
patient questions, compared to healthcare professionals and clinical guidelines. 
While LLMs show potential as supportive tools in healthcare, their integration 
should be approached cautiously due to inconsistent performance and possible 
risks. Further research is essential before widespread adoption.

Copyright © 2026 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106250
PMID: 41529641

Conflict of interest statement: Declaration of competing interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: Each author certifies that they 
have no commercial associations (e.g., consultancies, stock ownership, equity 
interest, patent/licensing arrangements, etc.) that might pose a conflict of 
interest in connection with the submitted article.


69. J Allergy Clin Immunol Pract. 2025 Nov;13(11):2927-2935. doi: 
10.1016/j.jaip.2025.09.012. Epub 2025 Sep 19.

Preparing Allergists to Practice in 2050 Using Artificial Intelligence.

Khoury P(1), Oppenheimer J(2), Bunyavanich S(3), Ciaccio CE(4), Portnoy J(5).

Author information:
(1)Laboratory of Allergic Diseases, Division of Intramural Research, National 
Institute of Allergy and Infectious Diseases/National Institutes of Health, 
Bethesda, Md.
(2)Division of Allergy/Immunology, UMDNJ-Rutgers University, Morristown, NJ.
(3)Division of Allergy and Immunology, Department of Pediatrics, Department of 
Genetics and Genomic Sciences, Icahn School of Medicine at Mount Sinai, New 
York, NY.
(4)Department of Pediatrics, University of Chicago, Chicago, Ill; Department of 
Medicine, University of Chicago, Chicago, Ill.
(5)Division of Allergy, Asthma, Pulmonary, and Sleep Medicine, Children's Mercy 
Hospital, Kansas City, Mo. Electronic address: Jportnoy@cmh.edu.

As artificial intelligence (AI) becomes deeply embedded in clinical practice, 
the field of allergy and immunology is poised for transformation by 2050. 
Artificial intelligence is expected to evolve from a decision support tool to a 
collaborative partner in diagnostics, treatment personalization, and medical 
education. Allergy training programs will need to prepare fellows for a 
technologically advanced landscape by integrating AI literacy, data science, and 
virtual simulation into curricula. Fellowship programs will need to adopt 
adaptive learning platforms, high-fidelity simulations, and AI-powered clinical 
decision support to improve diagnostic acumen, procedural competency, and 
patient care. This evolution also demands attention to the ethical and legal 
challenges of AI implementation, including preserving patient autonomy, 
addressing algorithmic bias, and safeguarding data privacy. Fellows must develop 
skills to evaluate AI outputs critically and uphold transparent, human-centered 
care. Artificial intelligence will probably also reshape research practices 
through predictive analytics, digital twins, and automated trial matching, 
accelerating discovery in allergic and immunologic disease. Despite these 
advances, limitations such as the black box problem, lack of emotional 
intelligence, and misinformed patient self-diagnoses pose challenges. Clinicians 
will require new communication strategies, including brief cognitive behavioral 
interventions, to address AI-derived misconceptions and maintain trust. Rather 
than replacing allergists, AI is likely to expand their roles, freeing time for 
patient interaction while reinforcing their responsibility as interpreters, 
educators, and ethical stewards of digital tools. This review explores how 
graduate medical education and clinical practice in allergy and immunology must 
evolve to ensure that future allergists remain competent, compassionate, and 
technologically fluent in a dynamic AI-enhanced health care environment.

Copyright © 2025 American Academy of Allergy, Asthma & Immunology. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jaip.2025.09.012
PMCID: PMC12697010
PMID: 40976355 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: Dr. Khoury receives 
royalties from UpToDate. Dr. Oppenheimer reports: serving as a 
consultant/advisor for Aimmune, Amgen, ARS Pharmaceuticals, Aquestive 
Therapeutics, and GlaxoSmithKline; on the adjudication/DSMB for AbbVie, 
AstraZeneca, GlaxoSmithKline, and Sanofi; reviewer for UpToDate; executive 
editor for Annals of Allergy, Asthma, and Immunology. Dr. Bunyavanich has no 
conflicts to disclose. Dr. Ciaccio has served as a consultant/advisor for 
Novartis, Opella, Clostrabio and Siolta Therapeutics; a speaker for Genentech; 
and receives research funding from Genentech, FARE and the NIH. Dr. Portnoy 
receives royalties from UpToDate


70. J Anesth. 2025 Sep 10. doi: 10.1007/s00540-025-03575-x. Online ahead of print.

Generative AI in perioperative medicine and anesthesiology: ethical integration, 
educational innovation, and the future of clinical professionalism.

Komasawa N(1)(2).

Author information:
(1)Community Medicine Education Promotion Office, Faculty of Medicine, Kagawa 
University Ikenobe, 1750-1, Miki-Cho, Kagawa, 761-0793, Japan. 
komasawa.nobuyasu@kagawa-u.ac.jp.
(2)Department of Medical Education, Faculty of Medicine, Kagawa University, 
Miki-Cho, Kagawa, 761-0793, Japan. komasawa.nobuyasu@kagawa-u.ac.jp.

Generative artificial intelligence (AI) is rapidly transforming perioperative 
medicine, particularly anesthesiology, by enabling novel applications, such as 
real-time data synthesis, individualized risk prediction, and automated 
documentation. These capabilities enhance clinical decision-making, patient 
communication, and workflow efficiency in the operating room. In education, 
generative AI offers immersive simulations and tailored learning experiences 
that improve both technical skills and professional judgment. However, 
overreliance without critical appraisal may compromise patient safety and 
humanistic care. This paper introduces a novel professionalism framework for 
anesthesiology in the AI era, comprising three pillars: critical AI literacy, 
human-centered care, and digital accountability. The model supports resident 
training, certification, and lifelong learning by integrating AI competencies 
with ethical awareness and reflective practice. By encouraging anesthesiologists 
to critically engage with AI tools, the framework ensures safe, effective, and 
compassionate perioperative care.

© 2025. The Author(s) under exclusive licence to Japanese Society of 
Anesthesiologists.

DOI: 10.1007/s00540-025-03575-x
PMID: 40931244

Conflict of interest statement: Declarations. Conflict of interest: The authors 
have no affiliation with any manufacturer of any device described in the 
manuscript and declare no financial interest in relation to the material 
described in the manuscript.


71. J Med Syst. 2025 Mar 25;49(1):39. doi: 10.1007/s10916-025-02170-7.

Large Language Models' Responses to Spinal Cord Injury: A Comparative Study of 
Performance.

Li J(#)(1)(2), Chang C(#)(1)(2), Li Y(#)(3), Cui S(1)(2), Yuan F(1)(2), Li Z(4), 
Wang X(5), Li K(1)(2), Feng Y(6), Wang Z(7)(8), Wei Z(9), Jian F(10)(11).

Author information:
(1)Department of Neurosurgery, Xuanwu Hospital, Capital Medical University, No. 
45 Changchun Street, Xicheng District, Beijing, 100053, China.
(2)Spine Center, China International Neuroscience Institute (CHINA-INI), 
Beijing, China.
(3)Center for Integrative Medicine, Beijing Ditan Hospital, Capital Medical 
University, Beijing, China.
(4)School of Artificial Intelligence, Beijing University of Posts and 
Telecommunications, Beijing, China.
(5)Baylor College of Medicine, Houston, TX, USA.
(6)Capital Medical University, Beijing, China.
(7)Department of Neurosurgery, Xuanwu Hospital, Capital Medical University, No. 
45 Changchun Street, Xicheng District, Beijing, 100053, China. wzw6855@163.com.
(8)Spine Center, China International Neuroscience Institute (CHINA-INI), 
Beijing, China. wzw6855@163.com.
(9)Department of Orthopaedics, Qilu Hospital of Shandong University, Shandong 
University, No. 107 Wenhua West Road, Lixia District, 250012, Jinan, China. 
weizhijian2002@126.com.
(10)Department of Neurosurgery, Xuanwu Hospital, Capital Medical University, No. 
45 Changchun Street, Xicheng District, Beijing, 100053, China. 
jianfengzeng@xwh.ccmu.edu.cn.
(11)Spine Center, China International Neuroscience Institute (CHINA-INI), 
Beijing, China. jianfengzeng@xwh.ccmu.edu.cn.
(#)Contributed equally

With the increasing application of large language models (LLMs) in the medical 
field, their potential in patient education and clinical decision support is 
becoming increasingly prominent. Given the complex pathogenesis, diverse 
treatment options, and lengthy rehabilitation periods of spinal cord injury 
(SCI), patients are increasingly turning to advanced online resources to obtain 
relevant medical information. This study analyzed responses from four 
LLMs-ChatGPT-4o, Claude-3.5 sonnet, Gemini-1.5 Pro, and Llama-3.1-to 37 
SCI-related questions spanning pathogenesis, risk factors, clinical features, 
diagnostics, treatments, and prognosis. Quality and readability were assessed 
using the Ensuring Quality Information for Patients (EQIP) tool and 
Flesch-Kincaid metrics, respectively. Accuracy was independently scored by three 
senior spine surgeons using consensus scoring. Performance varied among the 
models. Gemini ranked highest in EQIP scores, suggesting superior information 
quality. Although the readability of all four LLMs was generally low, requiring 
a college-level reading comprehension ability, they were all able to effectively 
simplify complex content. Notably, ChatGPT led in accuracy, achieving 
significantly higher "Good" ratings (83.8%) compared to Claude (78.4%), Gemini 
(54.1%), and Llama (62.2%). Comprehensiveness scores were high across all 
models. Furthermore, the LLMs exhibited strong self-correction abilities. After 
being prompted for revision, the accuracy of ChatGPT and Claude's responses 
improved by 100% and 50%, respectively; both Gemini and Llama improved by 67%. 
This study represents the first systematic comparison of leading LLMs in the 
context of SCI. While Gemini excelled in response quality, ChatGPT provided the 
most accurate and comprehensive responses.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02170-7
PMID: 40128385 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics Approval: This was not a 
study involving human subjects but rather a comparative analysis of responses of 
large language models to spinal cord injury problems. Therefore, obtaining 
institutional review board approval or informed consent was not required. 
Competing Interests: The authors declare no competing interests.


72. Eur Arch Otorhinolaryngol. 2025 Aug;282(8):4325-4334. doi: 
10.1007/s00405-025-09504-8. Epub 2025 Jun 6.

Clinical decision support using large language models in otolaryngology: a 
systematic review.

Filali Ansary R(1), Lechien JR(2)(3)(4)(5).

Author information:
(1)Department of Surgery, Faculty of Medicine, UMONS Research Institute for 
Health Sciences and Technology, University of Mons (UMons), University of Mons, 
6, Mons, B7000, Belgium.
(2)Department of Surgery, Faculty of Medicine, UMONS Research Institute for 
Health Sciences and Technology, University of Mons (UMons), University of Mons, 
6, Mons, B7000, Belgium. Jerome.Lechien@umons.ac.be.
(3)Department of Otolaryngology-Head and Neck Surgery, School of Medicine, Foch 
Hospital, University Paris Saclay, Paris, France. Jerome.Lechien@umons.ac.be.
(4)Department of Otolaryngology-Head and Neck Surgery, CHU Saint-Pierre, 
Brussels, Belgium. Jerome.Lechien@umons.ac.be.
(5)Department of Otolaryngology, Elsan Hospital, Paris, France. 
Jerome.Lechien@umons.ac.be.

OBJECTIVE: This systematic review evaluated the diagnostic accuracy of large 
language models (LLMs) in otolaryngology-head and neck surgery clinical 
decision-making.
DATA SOURCES: PubMed/MEDLINE, Cochrane Library, and Embase databases were 
searched for studies investigating clinical decision support accuracy of LLMs in 
otolaryngology.
REVIEW METHODS: Three investigators searched the literature for peer-reviewed 
studies investigating the application of LLMs as clinical decision support for 
real clinical cases according to the Preferred Reporting Items for Systematic 
Reviews and Meta-Analyses (PRISMA) guidelines. The following outcomes were 
considered: diagnostic accuracy, additional examination and treatment 
recommendations. Study quality was assessed using the modified Methodological 
Index for Non-Randomized Studies (MINORS).
RESULTS: Of the 285 eligible publications, 17 met the inclusion criteria, 
accounting for 734 patients across various otolaryngology subspecialties. 
ChatGPT-4 was the most evaluated LLM (n = 14/17), followed by Claude-3/3.5 
(n = 2/17), and Gemini (n = 2/17). Primary diagnostic accuracy ranged from 45.7 
to 80.2% across different LLMs, with Claude often outperforming ChatGPT. LLMs 
demonstrated lower accuracy in recommending appropriate additional examinations 
(10-29%) and treatments (16.7-60%), with substantial subspecialty variability. 
Treatment recommendation accuracy was highest in head and neck oncology (55-60%) 
and lowest in rhinology (16.7%). There was substantial heterogeneity across 
studies for the inclusion criteria, information entered in the application 
programming interface, and the methods of accuracy assessment.
CONCLUSIONS: LLMs demonstrate promising moderate diagnostic accuracy in 
otolaryngology clinical decision support, with higher performance in providing 
diagnoses than in suggesting appropriate additional examinations and treatments. 
Emerging findings support that Claude often outperforms ChatGPT. Methodological 
standardization is needed for future research.
LEVEL OF EVIDENCE: NA.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00405-025-09504-8
PMID: 40481345 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethic committee: Not required. 
Informed consent: Not required. Competing interests: The author Jerome R. 
Lechien was not involved with the peer review process of this article.


73. Ann Biomed Eng. 2025 Nov;53(11):3202-3207. doi: 10.1007/s10439-025-03832-w. Epub 
2025 Sep 9.

ChatGPT in Nursing: Applications, Advantages, and Challenges in Education, 
Research, and Clinical Practice.

Gerçek A(1), Çiftci N(2), Durmuş M(1), Sarman A(3), Taşcı Ö(1), Yıldız M(4).

Author information:
(1)Department of Nursing, Faculty of Health Sciences, Muş Alparslan University, 
49100, Muş, Turkey.
(2)Department of Nursing, Faculty of Health Sciences, Muş Alparslan University, 
49100, Muş, Turkey. necmettin2387@hotmail.com.
(3)Department of Nursing, Faculty of Health Sciences, Bingöl University, 12100, 
Bingöl, Turkey.
(4)Department of Midwifery, Faculty of Health Sciences, Sakarya University, 
54100, Sakarya, Turkey.

The incorporation of AI-supported language models into the healthcare sector 
holds significant potential to revolutionize nursing education, research, and 
clinical practice. Within this framework, ChatGPT has emerged as a valuable tool 
for personalizing educational materials, enhancing academic productivity, 
expediting clinical decision-making processes, and optimizing research 
efficiency. In the realm of nursing education, ChatGPT offers numerous 
advantages, including the preparation of course content, facilitation of student 
assessments, and the development of simulation-based learning environments. 
Furthermore, it enhances pedagogical effectiveness through its ability to engage 
with students in real time and provide tailored learning experiences. In 
research processes, it supports researchers in tasks such as literature review, 
data analysis, and preparation of ethical documents; it also facilitates 
labor-intensive processes such as thematic coding, especially in qualitative 
analysis. In clinical applications, it can be used to prepare patient 
information materials, create care plans, and strengthen nurse-patient 
communication. However, ChatGPT has limitations, such as information 
reliability, ethical appropriateness, lack of empathy, and the risk of users 
developing over-reliance. Therefore, a careful, critical, and ethical approach 
should be adopted for the integration of ChatGPT into the nursing discipline. In 
conclusion, ChatGPT offers significant opportunities to enhance nursing 
education, research, and practice, yet it should not substitute for professional 
judgment or human compassion. Its application must be steered by ethical 
standards and a critical mindset. Incorporating AI literacy into nursing 
curricula and establishing clear guidelines for its use can ensure its effective 
and responsible implementation while maintaining the profession's humanistic 
essence.

© 2025. The Author(s) under exclusive licence to Biomedical Engineering Society.

DOI: 10.1007/s10439-025-03832-w
PMID: 40924308 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare that they have no conflict of interest. Ethical Approval: This study did 
not require approval from an ethics committee, as it exclusively utilized data 
obtained from publicly accessible databases and did not involve research on 
human or animal subjects. The study was conducted with strict adherence to 
ethical principles and relied solely on a literature analysis. Consent for 
Publication: Not applicable. This study is a review article and does not include 
any individual data or identifiable images.


74. JMIR Med Inform. 2025 Sep 29;13:e66905. doi: 10.2196/66905.

Using Large Language Models for Chronic Disease Management Tasks: Scoping 
Review.

Serugunda HM(1), Jianquan O(2), Kasujja Namatovu H(3), Ssemaluulu P(4), Kimbugwe 
N(5), Garimoi Orach C(6), Waiswa P(7).

Author information:
(1)Department of Information Technology, School of Computing and Informatics 
Technology, Makerere University, Kampala, Uganda.
(2)School of Computer Science and School of Cyberspace, Xiangtan University, 
Engineering Building, 2nd Floor, Yuhu District, Xiangtan, Hunan, 411105, China, 
86 73158292718 ext 186.
(3)Department of Information Systems, School of Computing and Informatics 
Technology, Makerere University, Kampala, Uganda.
(4)Department of Computer Science, Faculty of Computing and Library Science, 
Kabale University, Kabale, Uganda.
(5)Department of Networks, School of Computing and Informatics Technology, 
Makerere University, Kampala, Uganda.
(6)Department of Community Health and Behavioral Sciences, School of Public 
Health, College of Health Sciences, Makerere University, Kampala, Uganda.
(7)Department of Health Policy Planning and Management, School of Public Health, 
College of Health Sciences, Makerere University, Kampala, Uganda.

BACKGROUND: Chronic diseases present significant challenges in health care, 
requiring effective management to reduce morbidity and mortality. While digital 
technologies like wearable devices and mobile applications have been widely 
adopted, large language models (LLMs) such as ChatGPT are emerging as promising 
technologies with the potential to enhance chronic disease management. However, 
the scope of their current applications in chronic disease management and 
associated challenges remains underexplored.
OBJECTIVE: This scoping review investigates LLM applications in chronic disease 
management, identifies challenges, and proposes actionable recommendations.
METHODS: A systematic search for English-language primary studies on LLM use in 
chronic disease management was conducted across PubMed, IEEE Xplore, Scopus, and 
Google Scholar to identify articles published between January 1, 2023, and 
January 15, 2025. Of the 605 screened records, 29 studies met the inclusion 
criteria. Data on study objectives, LLMs used, health care settings, study 
designs, users, disease management tasks, and challenges were extracted and 
thematically analyzed using the Preferred Reporting Items for Systematic Reviews 
and Meta-Analyses extension for Scoping Reviews guidelines.
RESULTS: LLMs were primarily used for patient-centered tasks, including patient 
education and information provision (18/29, 62%) of studies, diagnosis and 
treatment (6/29, 21%), self-management and disease monitoring (8/29, 28%), and 
emotional support and therapeutic conversations (4/29, 14%). 
Practitioner-centered tasks included clinical decision support (8/29, 28%) and 
medical predictions (6/29, 21%). Challenges identified include inaccurate and 
inconsistent LLM responses (18/29, 62%), limited datasets (6/29, 21%), 
computational and technical (6/29, 21%), usability and accessibility (9/29, 
31%), LLM evaluation (5/29, 17%), and legal, ethical, privacy, and regulatory 
(10/29, 35%). While models like ChatGPT, Llama, and Bard demonstrated use in 
diabetes management and mental health support, performance issues were evident 
across studies and use cases.
CONCLUSIONS: LLMs show promising potential for enhancing chronic disease 
management across patient and practitioner-centered tasks. However, challenges 
related to accuracy, data scarcity, usability, and ethical concerns must be 
addressed to ensure patient safety and equitable use. Future studies should 
prioritize the integration of LLMs with low-resource platforms, wearable and 
mobile technologies, developing culturally and age-appropriate interfaces, and 
establishing robust regulatory and evaluation frameworks to support safe, 
effective, and inclusive use in health care.

©Henry Mukalazi Serugunda, Ouyang Jianquan, Hasifah Kasujja Namatovu, Paul 
Ssemaluulu, Nasser Kimbugwe, Christopher Garimoi Orach, Peter Waiswa. Originally 
published in JMIR Medical Informatics (https://medinform.jmir.org).

DOI: 10.2196/66905
PMCID: PMC12479051
PMID: 41021927 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


75. JMIR Form Res. 2025 May 20;9:e63857. doi: 10.2196/63857.

Comparison of ChatGPT and Internet Research for Clinical Research and 
Decision-Making in Occupational Medicine: Randomized Controlled Trial.

Weuthen FA(1), Otte N(1), Krabbe H(2), Kraus T(1), Krabbe J(1)(3).

Author information:
(1)Institute of Occupational, Social and Environmental Medicine, Medical 
Faculty, Rheinisch-Westfälische Technische Hochschule Aachen University, Aachen, 
Germany.
(2)Department of Vascular Surgery, St. Josef Hospital Bochum, Katholisches 
Klinikum Bochum, Medical Faculty, Ruhr University Bochum, Bochum, Germany.
(3)Institute for Prevention and Occupational Medicine of the German Social 
Accident Insurance, Medical Faculty, Ruhr University Bochum, Bürkle de la Camp 
Platz 1, Bochum, 44789, Germany, 49 3013001 ext 4110.

BACKGROUND: Artificial intelligence is becoming a part of daily life and the 
medical field. Generative artificial intelligence models, such as GPT-4 and 
ChatGPT, are experiencing a surge in popularity due to their enhanced 
performance and reliability. However, the application of these models in 
specialized domains, such as occupational medicine, remains largely unexplored.
OBJECTIVE: This study aims to assess the potential suitability of a generative 
large language model, such as ChatGPT, as a support tool for medical research 
and even clinical decisions in occupational medicine in Germany.
METHODS: In this randomized controlled study, the usability of ChatGPT for 
medical research and clinical decision-making was investigated using a web 
application developed for this purpose. Eligibility criteria were being a 
physician or medical student. Participants (N=56) were asked to work on 3 cases 
of occupational lung diseases and answer case-related questions. They were 
allocated via coin weighted for proportions of physicians in each group into 2 
groups. One group researched the cases using an integrated chat application 
similar to ChatGPT based on the latest GPT-4-Turbo model, while the other used 
their usual research methods, such as Google, Amboss, or DocCheck. The primary 
outcome was case performance based on correct answers, while secondary outcomes 
included changes in specific question accuracy and self-assessed occupational 
medicine expertise before and after case processing. Group assignment was not 
traditionally blinded, as the chat window indicated membership; participants 
only knew the study examined web-based research, not group specifics.
RESULTS: Participants of the ChatGPT group (n=27) showed better performance in 
specific research, for example, for potentially hazardous substances or 
activities (eg, case 1: ChatGPT group 2.5 hazardous substances that cause 
pleural changes versus 1.8 in a group with own research; P=.01; Cohen r=-0.38), 
and led to an increase in self-assessment with regard to specialist knowledge 
(from 3.9 to 3.4 in the ChatGPT group vs from 3.5 to 3.4 in the own research 
group; German school grades between 1=very good and 6=unsatisfactory; P=.047). 
However, clinical decisions, for example, whether an occupational disease report 
should be filed, were more often made correctly as a result of the participant's 
own research (n=29; eg, case 1: Should an occupational disease report be filed? 
Yes for 7 participants in the ChatGPT group vs 14 in their own research group; 
P=.007; odds ratio 6.00, 95% CI 1.54-23.36).
CONCLUSIONS: ChatGPT can be a useful tool for targeted medical research, even 
for rather specific questions in occupational medicine regarding occupational 
diseases. However, clinical decisions should currently only be supported and not 
made by the large language model. Future systems should be critically assessed, 
even if the initial results are promising.

© Felix A Weuthen, Nelly Otte, Hanif Krabbe, Thomas Kraus, Julia Krabbe. 
Originally published in JMIR Formative Research (https://formative.jmir.org).

DOI: 10.2196/63857
PMCID: PMC12112251
PMID: 40393042 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


76. J Surg Educ. 2025 Aug;82(8):103519. doi: 10.1016/j.jsurg.2025.103519. Epub 2025 
May 15.

The Current Landscape of Artificial Intelligence in Plastic Surgery Education 
and Training: A Systematic Review.

Genovese A(1), Borna S(1), Gomez-Cabello CA(1), Haider SA(1), Prabha S(1), 
Trabilsy M(1), Forte AJ(2).

Author information:
(1)Division of Plastic Surgery, Mayo Clinic, Jacksonville, Florida.
(2)Division of Plastic Surgery, Mayo Clinic, Jacksonville, Florida; Center for 
Digital Health, Mayo Clinic, Rochester, Minnesota. Electronic address: 
ajvforte@yahoo.com.br.

OBJECTIVE: Artificial intelligence (AI) shows promise in surgery, but its role 
in plastic surgery education remains underexplored. This review evaluates the 
current landscape of AI in plastic surgery education.
DESIGN: A systematic search was conducted on August 11, 2024, across PubMed, 
CINAHL, IEEE, Scopus, Web of Science, and Google Scholar using terms related to 
AI, plastic surgery, and education. Original research articles focusing on AI in 
plastic surgery education were included, excluding correspondence, reviews, book 
chapters, theses, corrections, and non-peer-reviewed or non-English articles. 
Two investigators independently screened studies and synthesized data. ROBINS-I 
was used to assess bias.
RESULTS: Fifteen studies were included, with 13 evaluating large language models 
(LLMs) such as ChatGPT, Microsoft Bing, and Google Bard. ChatGPT-4 outperformed 
other models on In-Service Examinations (average score of 72.7%) and 
demonstrated potential as a teaching assistant in plastic surgery education. 
AI-generated personal statements were comparable to human-written ones. However, 
ChatGPT showed inaccuracies in generating surgical protocols. ChatGPT 
demonstrated its ability to provide qualitative predictions, forecasting survey 
results that indicated limited current use of AI in plastic surgery education 
but support for further AI research. a study combined ChatGPT with DALL-E 2, a 
generative model, to create acceptable educational images. Machine learning was 
used in 1 study for evaluating surgical skill and providing real-time feedback 
during liposuction. Nine studies had low risk of bias, while 6 had moderate 
risk.
CONCLUSIONS: AI demonstrates potential as an educational tool in plastic 
surgery. However, limitations of evidence, such as AI model uncertainties, 
introduce ambiguity. While AI cannot replicate the expertise of seasoned 
surgeons, it shows promise for foundational learning and skill assessment. 
Developing authenticity guidelines and enhancing AI capabilities are essential 
for its effective, ethical integration into plastic surgery education.

Copyright © 2025 Association of Program Directors in Surgery. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jsurg.2025.103519
PMID: 40378641 [Indexed for MEDLINE]


77. JAMA Netw Open. 2025 Oct 1;8(10):e2538427. doi: 
10.1001/jamanetworkopen.2025.38427.

Large Language Model Assistant for Emergency Department Discharge Documentation.

Song JW(1), Park J(2), Kim JH(2)(3), You SC(3)(4).

Author information:
(1)Yonsei University College of Medicine, Seoul, South Korea.
(2)Department of Emergency Medicine, Yonsei University College of Medicine, 
Seoul, South Korea.
(3)Institute for Innovation in Digital Health, Yonsei University, Seoul, South 
Korea.
(4)Department of Biomedical Systems Informatics, Yonsei University College of 
Medicine, Seoul, South Korea.

Comment in
    doi: 10.1001/jamanetworkopen.2025.38437.

IMPORTANCE: Emergency department (ED) discharge documentation is time-consuming 
and often incomplete.
OBJECTIVE: To develop a large language model (LLM) assistant that generates ED 
discharge notes and to evaluate its effectiveness on documentation quality and 
workflow efficiency.
DESIGN, SETTING, AND PARTICIPANTS: This comparative effectiveness study, which 
was conducted at a 2400-bed tertiary care hospital in South Korea, consisted of 
2 primary phases: a development phase and sequential validation of the LLM 
assistant. In the randomized sequential prospective validation, 6 emergency 
physicians first wrote discharge notes manually (session 1), then edited 
LLM-generated drafts after a 1-hour washout period (session 2). Three 
independent physicians evaluated 300 note sets (each containing a manual note, 
an LLM draft, and an LLM-assisted note). For model development and validation, 
patient records from ED visits between September 1, 2022, and August 31, 2023, 
were used. The inclusion criteria encompassed adult patients (aged ≥17 years) 
and pediatric patients with nondisease conditions (eg, trauma, poisoning, or 
burns). Emergency physicians selected 592 representative cases for training and 
50 for validation.
EXPOSURE: A commercially available text generation transformer model was used as 
a core LLM, fine-tuned using the 592 training cases. Two distinct processing 
pipelines were implemented within the LLM assistant due to different input data: 
(1) for patients managed solely by emergency physicians, using the ED initial 
record and prescription list, and (2) for those requiring specialty 
consultations, using the ED initial record and consultation request form.
MAIN OUTCOMES AND MEASURES: Quality of notes using 4C metrics (completeness, 
correctness, conciseness, and clinical utility) on a Likert scale ranging from 1 
to 5 and time taken to complete the notes manually and with the LLM assistant.
RESULTS: Of the 50 test cases, the mean (SD) patient age was 57.7 (23.1) years, 
and 28 patients (56%) were female. LLM-assisted notes achieved higher scores 
than manual notes in completeness (4.23 [95% CI, 4.17-4.28] vs 4.03 [95% CI, 
3.96-4.09]), correctness (4.38 [95% CI, 4.33-4.42] vs 4.20 [95% CI, 4.14-4.26]), 
conciseness (4.23 [95% CI, 4.18-4.28] vs 4.11 [95% CI, 4.05-4.17]), and clinical 
utility (4.17 [95% CI, 4.11-4.23] vs 3.85 [95% CI, 3.78-3.91]) (all P < .001). 
When compared with LLM drafts, LLM-assisted notes excelled in conciseness (4.23 
vs 3.98 [95% CI, 3.91-4.04]; P < .001) and maintained equivalent clinical 
utility (4.17 vs 4.16 [95% CI, 4.11-4.21]; P > .99), but scored lower in 
completeness (4.23 vs 4.34 [95% CI, 4.29-4.39]; P = .001) and correctness (4.38 
vs 4.45 [95% CI, 4.41-4.49]; P < .001). The median documentation time per note 
dropped from 69.5 (95% CI, 65.5-78.0) seconds for manual notes to 32.0 (95% CI, 
29.5-36.0) seconds for LLM-assisted notes (P < .001).
CONCLUSION: In this comparative effectiveness study, use of an on-site LLM 
assistant was associated with reduced writing time for ED discharge notes 
compared with manual note-taking, without compromising documentation quality, 
representing a critical advancement in the use of artificial intelligence for 
clinical practice.

DOI: 10.1001/jamanetworkopen.2025.38427
PMCID: PMC12541540
PMID: 41118162 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest Disclosures: Dr You 
reported receiving personal fees from PHI Digital Healthcare during the conduct 
of the study and grant support from Daiichi Sankyo outside the submitted work, 
and having patents 2025-0039190, a 2025-0039191, 2025-0039192, 2025-0039193, and 
2025-0039194 pending. No other disclosures were reported.


78. NPJ Digit Med. 2025 May 17;8(1):290. doi: 10.1038/s41746-025-01689-w.

Development and prospective implementation of a large language model based 
system for early sepsis prediction.

Shashikumar SP(1), Mohammadi S(1), Krishnamoorthy R(1), Patel A(2), Wardi 
G(2)(3), Ahn JC(1)(4), Singh K(1)(5), Aronoff-Spencer E(6), Nemati S(7).

Author information:
(1)Division of Biomedical Informatics, UC San Diego, San Diego, CA, USA.
(2)Department of Emergency Medicine, UC San Diego, San Diego, CA, USA.
(3)Division of Pulmonary, Critical Care and Sleep Medicine, UC San Diego, San 
Diego, CA, USA.
(4)Division of Gastroenterology and Hepatology, Mayo Clinic, Rochester, NY, USA.
(5)Jacobs Center for Health Innovation, UC San Diego Health, San Diego, CA, USA.
(6)Division of Infectious Diseases and Global Public Health, UC San Diego, San 
Diego, CA, USA.
(7)Division of Biomedical Informatics, UC San Diego, San Diego, CA, USA. 
snemati@health.ucsd.edu.

Update of
    medRxiv. 2025 Mar 11:2025.03.07.25323589. doi: 10.1101/2025.03.07.25323589.

Sepsis is a dysregulated host response to infection with high mortality and 
morbidity. Early detection and intervention have been shown to improve patient 
outcomes, but existing computational models relying on structured electronic 
health record data often miss contextual information from unstructured clinical 
notes. This study introduces COMPOSER-LLM, an open-source large language model 
(LLM) integrated with the COMPOSER model to enhance early sepsis prediction. For 
high-uncertainty predictions, the LLM extracts additional context to assess 
sepsis-mimics, improving accuracy. Evaluated on 2500 patient encounters, 
COMPOSER-LLM achieved a sensitivity of 72.1%, positive predictive value of 
52.9%, F-1 score of 61.0%, and 0.0087 false alarms per patient hour, 
outperforming the standalone COMPOSER model. Prospective validation yielded 
similar results. Manual chart review found 62% of false positives had bacterial 
infections, demonstrating potential clinical utility. Our findings suggest that 
integrating LLMs with traditional models can enhance predictive performance by 
leveraging unstructured data, representing a significant advance in healthcare 
analytics.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01689-w
PMCID: PMC12084535
PMID: 40379845

Conflict of interest statement: Competing interests: S.N. and S.P.S. are 
co-founders of a UCSD start-up, Clairyon Inc. (formerly Healcisio Inc.), which 
is focused on commercialization of advanced analytical decision support tools, 
and formed in compliance with UCSD conflict of interest policies. The remaining 
authors declare no competing interests.


79. J Allergy Clin Immunol. 2025 Jul;156(1):81-87. doi: 10.1016/j.jaci.2025.02.004. 
Epub 2025 Feb 14.

Evaluating large language model performance to support the diagnosis and 
management of patients with primary immune disorders.

Rider NL(1), Li Y(2), Chin AT(3), DiGiacomo DV(4), Dutmer C(5), Farmer JR(6), 
Roberts K(7), Savova G(2), Ong MS(8).

Author information:
(1)Department of Health Systems and Implementation Science, Virginia Tech 
Carilion School of Medicine, Roanoke, Va; Section of Allergy and Immunology, 
Department of Medicine, Carilion Clinic, Roanoke, Va. Electronic address: 
nick70@vt.edu.
(2)Computational Health Informatics Program, Harvard Medical School and Boston 
Children's Hospital, Boston, Mass.
(3)Department of Pediatrics, Division of Immunology, Allergy, and Rheumatology, 
University of California, Los Angeles, Los Angeles, Calif.
(4)Department of Pediatrics, K. Hovnanian Children's Hospital, Jersey Shore 
University Medical Center, Neptune, NJ; Hackensack Meridian School of Medicine, 
Nutley, NJ.
(5)Department of Pediatrics, University of Colorado School of Medicine, Denver, 
Colo; Section of Allergy & Immunology, Children's Hospital Colorado, Denver, 
Colo.
(6)Clinical Immunodeficiency Program of Beth Israel Lahey Health, Division of 
Allergy and Immunology, Lahey Hospital & Medical Center, Burlington, Mass; 
Department of Medicine, UMass Chan Medical School, Burlington, Mass.
(7)McWilliams School of Biomedical Informatics, University of Texas Health 
Science Center at Houston, Houston, Tex.
(8)Department of Population Medicine, Harvard Pilgrim Health Care Institute, 
Harvard Medical School, Boston, Mass.

BACKGROUND: Generative artificial intelligence (GAI) is transforming health care 
in a variety of ways; however, the present utility of GAI for supporting 
clinicians who treat rare disease such as primary immune disorders (PIs) is not 
well studied. We evaluated the ability of 6 state-of-the-art large language 
models (LLMs) for providing clinical guidance about PIs.
OBJECTIVE: To quantitatively and qualitatively measure the utility of current, 
open-source LLMs for diagnosing and providing helpful clinical decision support 
about PIs.
METHODS: Five expert clinical immunologists each provided 5 real-world, 
anonymized PI case vignettes via multi-turn prompting to 6 LLMs (OpenAI GPT-4o, 
Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct, Mistral-7B-Instruct-v0.3, 
Mistral-Large-Instruct-2407, Mixtral-8x7B-Instruct-v0.1). We assessed the 
diagnostic accuracy of the LLMs and the quality of clinical reasoning using the 
Revised-IDEA (R-IDEA) score. Qualitative LLM assessment was made by immunologist 
narratives.
RESULTS: Performance accuracy (>88%) and R-IDEA scores (≥8) were superior for 3 
models (GPT-4o, Llama-3.1-70B-Instruct, Mistral-Large-Instruct-2407), with 
GPT-4o achieving the highest diagnostic accuracy (96.2%). Conversely, the 
remaining 3 models fell below acceptable accuracy rates near 60% or lower and 
had poor R-IDEA scores (≤0.55), with Mistral-7B-Instruct-v0.3 attaining the 
worst diagnostic accuracy (42.3%). Compared with the 3 best-performing LLMs, the 
3 worst-performing LLMs had a substantially lower median R-IDEA score (P < 
.001). Interclass correlation coefficient for R-IDEA score assignments varied 
substantially by LLM, ranging from good to poor agreement, and did not appear to 
correlate with either diagnostic accuracy or median R-IDEA score. Qualitatively, 
immunologists identified several themes (eg, correctness, differential diagnosis 
appropriateness, relative conciseness of explanations) of relevance to PIs.
CONCLUSIONS: LLM can support diagnosis and management of PIs; however, further 
tuning is needed to optimize LLMs for best practice recommendations.

Copyright © 2025 American Academy of Allergy, Asthma & Immunology. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jaci.2025.02.004
PMCID: PMC12229761
PMID: 39956279 [Indexed for MEDLINE]

Conflict of interest statement: Disclosure statement N.L.R. receives funding 
from National Institute of Allergy and Infectious Diseases (R21AI164100), the 
Centers for Disease Control and Prevention (NE11CD000002) via the Jeffrey Modell 
Foundation, and Takeda Pharmaceuticals. J.R.F. was supported by a faculty 
development award from the American Academy of Allergy, Asthma & Immunology and 
the National Institute on Minority Health and Health Disparities of the National 
Institutes of Health (NIH) (R01MD017816). Y.L. and G.S. receive funding from the 
NIH (R01GM114355). M.-S.O. receives funding from the NIH (R01MD017816) and 
Patient-Centered Outcomes Research Institute (PCORI) (ME-2022C2-25573). Research 
reported in this publication was partially funded through a PCORI award 
(ME-2022C2-25573). Disclosure of potential conflict of interest: N. L. Rider 
serves on scientific advisory committees for Takeda, Pharming, and Grifols. He 
receives royalties from Wolters Kluwer for contributions to UpToDate. J. R. 
Farmer is an ongoing consultant for Pharming and has received 
investigator-initiated research grants from Pfizer, Bristol Myers Squibb, and 
Pharming with no direct relation to the work presented. M.-S. Ong has received 
an investigator-initiated research grant from Pharming with no direct relation 
to the work presented. The rest of the authors declare that they have no 
relevant conflicts of interest.


80. Laryngoscope Investig Otolaryngol. 2025 Nov 17;10(6):e70296. doi: 
10.1002/lio2.70296. eCollection 2025 Dec.

Artificial Intelligence in Laryngeal Cancer Management: Enhancing Guidelines or 
Redefining Standards?

Dedivitis RA(1), de Castro MAF(2), Matos LL(3), Ribeiro DA(4), Duarte BP(2), 
Kowalski LP(5).

Author information:
(1)Department of Head and Neck Surgery, Hospital das Clínicas, Faculdade de 
Medicina Universidade de São Paulo (HCFMUSP) São Paulo Brazil.
(2)Metropolitan University of Santos Santos Brazil.
(3)Department of Head and Neck Surgery Instituto do Câncer do Estado de São 
Paulo, Hospital das Clínicas, Faculdade de Medicina, Universidade de São Paulo 
(ICESP HCFMUSP) São Paulo Brazil.
(4)Department of Biosciences Federal University of São Paulo - Campus Baixada 
Santista Santos Brazil.
(5)Department of Head and Neck Surgery University of São Paulo School of 
Medicine São Paulo Brazil.

OBJECTIVE: To evaluate the accuracy of artificial intelligence (AI) in 
establishing clinical decision-making in the treatment of advanced laryngeal 
cancer.
METHODS: A structured question was elaborated for each of the seven 
recommendations chosen. Each Large Language Model (LLM) platform answered the 
questions. The Claude platform identified the differences between the guidelines 
and the responses generated and three specialists evaluated the impact of such 
differences.
RESULTS: Of the 28 analyzed responses, 22 (78.6%) demonstrated content 
similarity with existing guidelines. Two responses showed that guidelines 
contained significantly more comprehensive content, three responses from LLMs 
provided additional content not demonstrated in the guidelines, and one response 
showed direct disagreement with established guidelines.
CONCLUSION: There was a 78.6% overlap in responses between guideline 
recommendations and LLMs. Therefore, while AI holds promise for transforming 
guideline creation, its integration into clinical practice must be carefully 
evaluated to ensure that it complements, rather than replaces, established 
expert-driven protocols.
LEVEL OF EVIDENCE: 4.

© 2025 The Author(s). Laryngoscope Investigative Otolaryngology published by 
Wiley Periodicals LLC on behalf of The Triological Society.

DOI: 10.1002/lio2.70296
PMCID: PMC12620662
PMID: 41255779

Conflict of interest statement: The authors declare no conflicts of interest.


81. Oral Surg Oral Med Oral Pathol Oral Radiol. 2025 Oct;140(4):405-415. doi: 
10.1016/j.oooo.2025.03.004. Epub 2025 Mar 10.

Artificial intelligence for medication-related osteonecrosis of the jaw: a 
scoping review.

Mine Y(1), Okazaki S(2), Yamasaki S(3), Kakimoto N(4), Yanamoto S(3), Murayama 
T(2).

Author information:
(1)Project Research Center for Integrating Digital Dentistry, Hiroshima 
University, Hiroshima, Japan; Department of Medical Systems Engineering, 
Graduate School of Biomedical and Health Sciences, Hiroshima University, 
Hiroshima, Japan. Electronic address: mine@hiroshima-u.ac.jp.
(2)Project Research Center for Integrating Digital Dentistry, Hiroshima 
University, Hiroshima, Japan; Department of Medical Systems Engineering, 
Graduate School of Biomedical and Health Sciences, Hiroshima University, 
Hiroshima, Japan.
(3)Department of Oral Oncology, Graduate School of Biomedical and Health 
Sciences, Hiroshima University, Hiroshima, Japan.
(4)Department of Oral and Maxillofacial Radiology, Graduate School of Biomedical 
and Health Sciences, Hiroshima University, Hiroshima, Japan.

OBJECTIVES: To describe the current state of research on the application of 
artificial intelligence (AI) to the prediction, diagnosis, and management of 
medication-related osteonecrosis of the jaw (MRONJ).
STUDY DESIGN: A scoping review was conducted using PubMed/MEDLINE, Scopus, and 
Web of Science databases through March 1, 2024, and updated to December 31, 
2024, according to PRISMA-SCR guidelines. Data on AI tasks, models, patient 
information, datasets, and performance metrics were extracted from eligible 
studies.
RESULTS: Eight studies met the inclusion criteria, focusing on MRONJ onset 
prediction (n = 5), diagnosis (n = 2), and patient education (n = 1). For onset 
prediction, machine learning models (support vector machines, random forests, 
gradient boosting machines) achieved area under the curve values of 0.793-0.973. 
For diagnosis, deep learning models using radiographic images achieved 96% 
accuracy with 93% precision and recall. For patient education, a large language 
model was evaluated with moderate to high response quality.
CONCLUSIONS: While AI shows promise in the prediction and diagnosis of MRONJ, 
challenges related to data quality, validation, and clinical integration need to 
be addressed. Future research should focus on developing standardized, 
explainable AI models and establishing implementation guidelines for clinical 
practice.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.oooo.2025.03.004
PMID: 40393880 [Indexed for MEDLINE]


82. Nurse Educ Pract. 2026 Jan 13;91:104724. doi: 10.1016/j.nepr.2026.104724. Online 
ahead of print.

Artificial intelligence-enhanced clinical reasoning in nurse practitioners: A 
systematic review.

Yu SY(1), Lin HP(1), Kung YM(2), Chen SC(3).

Author information:
(1)Department of Nursing, College of Nursing, Chang Gung University of Science 
and Technology, Taoyuan, Taiwan.
(2)College of Nursing, Florida State University, FL, United States.
(3)School of Nursing and Geriatric and Long-Term Care Research Center, College 
of Nursing, Chang Gung University of Science and Technology, Taoyuan, Taiwan; 
Department of Nursing, College of Medicine, Chang Gung University, Taoyuan, 
Taiwan; Department of Radiation Oncology and Proton and Radiation Therapy 
Center, Chang Gung Medical Foundation, Linkou Chang Gung Memorial Hospital, 
Taoyuan, Taiwan. Electronic address: shuching@gw.cgust.edu.tw.

AIMS: This systematic review aimed to synthesize the current evidence on 
artificial intelligence (AI)-enhanced clinical reasoning among nurse 
practitioners (NPs).
BACKGROUND: NPs require strong clinical reasoning skills and AI-based tools may 
support the development of these competencies; however, empirical evidence 
regarding their effectiveness remains limited. The strengthened literature 
review clearly identifies a critical gap, the absence of a prior systematic 
review specifically examining AI-enhanced clinical reasoning among NPs and will 
provid a strong rationale for the present review.
DESIGN: Systematic review following PRISMA 2020 guidelines.
METHODS: Searches were conducted in PubMed, Embase and CINAHL through July 2025. 
Of the 429 records retrieved, 13 met inclusion criteria. Eligible studies 
examined AI interventions targeting clinical reasoning among NPs. Risk of bias 
was assessed using the Critical Appraisal Skills Programme checklists and Joanna 
Briggs Institute tools. Data were extracted on study design, population, AI 
application domain, outcomes and quality appraisal.
RESULTS: Thirteen studies were included: seven quantitative quasi-experimental, 
intervention validation, or retrospective cohort studies; three qualitative 
studies; and three systematic reviews. AI applications ranged from real-time 
monitoring and decision-support systems to simulation platforms and large 
language models, which supported clinical reasoning domains such as data 
gathering, hypothesis generation, diagnostic justification and reflective 
judgment. Quantitative studies showed improvements in diagnostic accuracy, 
consistency, efficiency and data collection, while qualitative studies found 
that NPs view AI as a supportive tool that enhances diagnostic reasoning and 
patient-centered care, while emphasizing the need for transparency, 
interpretability and workflow integration.
CONCLUSIONS: AI tools may strengthen NPs' clinical reasoning by improving 
diagnostic accuracy, decision consistency and care efficiency, but their safe 
use requires rigorous validation, standardized evaluation, ethical safeguards 
and digital literacy training. Limitations include heterogeneous AI applications 
across professional groups and a predominance of simulation-based evidence over 
real-world clinical evaluations.

Copyright © 2026 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.nepr.2026.104724
PMID: 41548327

Conflict of interest statement: Declaration of Competing Interest None declared.


83. JMIR Form Res. 2025 Oct 14;9:e77482. doi: 10.2196/77482.

Explainable AI-Driven Analysis of Radiology Reports Using Text and Image Data: 
Experimental Study.

Zamir MT(#)(1), Khan SU(#)(2), Gelbukh A(1), Felipe Riverón EM(1), Gelbukh I(1).

Author information:
(1)Centro de Investigación en Computación (CIC), Instituto Politécnico Nacional 
(IPN), Ciudad de México, CDMX, Mexico.
(2)Department of Cell Biology, Center for Research and Advanced Studies of the 
National Polytechnic Institute, Ciudad de México, CDMX, Mexico.
(#)Contributed equally

BACKGROUND: Artificial intelligence (AI) is increasingly being integrated into 
clinical diagnostics; yet, its lack of transparency hinders trust and adoption 
among health care professionals. The explainable artificial intelligence (XAI) 
has the potential to improve the interpretability and reliability of AI-based 
decisions in clinical practice.
OBJECTIVE: This study evaluates the use of XAI for interpreting radiology 
reports to improve health care practitioners' confidence and comprehension of 
AI-assisted diagnostics.
METHODS: This study used the Indiana University chest x-ray dataset containing 
3169 textual reports and 6471 images. Textual data were being classified as 
either normal or abnormal by using a range of machine learning approaches. This 
includes traditional machine learning models and ensemble methods, deep learning 
models (long short-term memory network), and advanced transformer-based language 
models (GPT-2, T5, LLaMA-2, and LLaMA-3.1). For image-based classifications, 
convolutional neural networks, including DenseNet121 and DenseNet169, were used. 
Top-performing models were interpreted using XAI methods SHAP (Shapley Adaptive 
Explanations) and Local Interpretable Model-Agnostic Explanations to support 
clinical decision making by enhancing transparency and trust in model 
predictions.
RESULTS: The LLaMA-3.1 model achieved the highest accuracy of 98% in classifying 
the textual radiology reports. Statistical analysis confirmed the model's 
robustness, with Cohen κ (k=0.981) indicating near-perfect agreement beyond 
chance. Both the chi-square and Fisher exact tests revealed a highly significant 
association between the actual and predicted labels (P<.001). Although the 
McNemar Test yielding a nonsignificant result (P=.25) suggests a balanced class 
performance, the highest accuracy of 84% was achieved in the analysis of imaging 
data using the DenseNet169 and DenseNet121 models. To assess explainability, 
Local Interpretable Model-Agnostic Explanations and SHAP were applied to the 
best-performing models. These models consistently highlighted that the 
medical-related terms such as "opacity," "consolidation," and "pleural" are 
clear indications for abnormal findings in textual reports.
CONCLUSIONS: The research underscores that explainability is an essential 
component of any AI systems used in diagnostics and is helpful in the design and 
implementation of AI in the health care sector. Such an approach improves the 
accuracy of the diagnosis and builds confidence in health workers, who in the 
future will use XAI in clinical settings, particularly in the application of AI 
explainability for medical purposes.

©Muhammad Tayyab Zamir, Safir Ullah Khan, Alexander Gelbukh, Edgardo Manuel 
Felipe Riverón, Irina Gelbukh. Originally published in JMIR Formative Research 
(https://formative.jmir.org), 14.10.2025.

DOI: 10.2196/77482
PMCID: PMC12569488
PMID: 40997754 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


84. Nurs Open. 2025 Jun;12(6):e70253. doi: 10.1002/nop2.70253.

ChatGPT Applications in Nursing: Current Status and Future Perspectives.

Peng W(1), Cheng X(1), Deng J(1), Zhang X(1).

Author information:
(1)Chongqing College of Humanities, Science & Technology, Chongqing, China.

AIM: With the rapid advancement of generative artificial intelligence 
technologies, natural language processing tools represented by ChatGPT have been 
increasingly used. This narrative review explored the application, challenges, 
and future directions of ChatGPT in nursing.
DESIGN: Narrative review.
METHODS: The searches were conducted in PubMed, Web of Science and Google 
Scholar. The empirical studies of ChatGPT in nursing were selected and explored.
RESULTS: ChatGPT has been integrated into clinical nursing support, nurse 
education and patient service optimisation, demonstrating potential in improving 
efficiency and patient outcomes. However, technical limitations, ethical-legal 
issues and implementation barriers pose challenges to its widespread adoption. 
In future, technology iteration, standard development and multimodal convergence 
are needed to promote the construction of trusted artificial intelligence 
nursing systems.
PATIENT OR PUBLIC CONTRIBUTION: This narrative review is based on a secondary 
analysis of existing literature and does not directly involve patient or public 
contributions.

© 2025 The Author(s). Nursing Open published by John Wiley & Sons Ltd.

DOI: 10.1002/nop2.70253
PMCID: PMC12145163
PMID: 40482056 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


85. J Surg Res. 2025 Mar;307:129-138. doi: 10.1016/j.jss.2025.01.015. Epub 2025 Feb 
27.

Testing ChatGPT's Ability to Provide Patient and Physician Information on Aortic 
Aneurysm.

Bertges DJ(1), Beck AW(2), Schermerhorn M(3), Eskandari MK(4), Eldrup-Jorgensen 
J(5), Liebscher S(6), Guinto R(6), Ferris M(6), Stanley A(6), Steinthorsson 
G(6), Alef M(7), Scali ST(8).

Author information:
(1)Division of Vascular Surgery, University of Vermont Medical Center, 
Burlington, Vermont. Electronic address: daniel.bertges@uvmhealth.org.
(2)Division of Vascular Surgery and Endovascular Therapy, University of Alabama 
at Birmingham, Birmingham, Alabama.
(3)Division of Vascular and Endovascular Surgery Boston, Beth Israel Deaconess 
Medical Center, Massachusetts.
(4)Division of Vascular Surgery, Northwestern Medicine, Chicago, Illinois.
(5)Maine Medical Center, Portland, Maine.
(6)Division of Vascular Surgery, University of Vermont Medical Center, 
Burlington, Vermont.
(7)Dartmouth Hitchcock Medical Center, Lebanon, New Hampshire.
(8)Division of Vascular Surgery and Endovascular Therapy, University of Florida, 
Gainesville, Florida.

INTRODUCTION: Our objective was to test the ability of ChatGPT 4.0 to provide 
accurate information for patients and physicians about abdominal aortic 
aneurysms (AAA) and to assess its alignment with Society for Vascular Surgery 
(SVS) clinical practice guidelines (CPG) for AAA care.
MATERIAL AND METHODS: Fifteen patient-level questions, 37 questions selected to 
reflect 28 SVS CPGs and 4 questions regarding AAA rupture risk were posed to 
ChatGPT 4.0. Single responses were recorded and graded for accuracy and quality 
by ten board-certified vascular surgeons as well as two fellow trainees using a 
5-point Likert scale; 1 = very poor, 2 = poor, 3 = fair, 4 = good, and 
5 = excellent.
RESULTS: The mean of the means (MoM) accuracy rating across all 15 patient-level 
questions was 4.4 (SD 0.4, quartile range (QR) 4.2-4.7). ChatGPT 4.0 
demonstrated good alignment with SVS practice guidelines (MoM: 4.2, SD: 0.4, QR: 
3.9-4.5). The accuracy of responses was consistent across guideline categories; 
screening or surveillance (4.2), indications for surgery (4.5), preoperative 
risk assessment (4.5), perioperative coronary revascularization (4.1), and 
perioperative management (4.2). The generative artificial intelligence bot 
demonstrated only fair performance in answering the annual AAA rupture risk 
(MoM: 3.4, SD: 1.2, QR: 2.3-4.3).
CONCLUSIONS: ChatGPT 4.0 provided accurate responses to a variety of 
patient-level questions regarding AAA. Responses were well-aligned with current 
SVS CPGs except for inaccuracies in the risk of AAA rupture at varying 
diameters. The emergence of generative artificial intelligence bots presents an 
opportunity for study of applications in patient education and to determine 
their ability to augment the vascular specialist's knowledge base.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jss.2025.01.015
PMID: 40014909 [Indexed for MEDLINE]


86. BMC Neurol. 2025 Jul 1;25(1):264. doi: 10.1186/s12883-025-04280-8.

Evaluating ChatGPT and DeepSeek in postdural puncture headache management: a 
comparative study with international consensus guidelines.

Deng J(#)(1)(2), Qiu X(#)(1)(2), Dong C(1)(2), Xu L(1)(2), Dong X(3)(4), Yang 
S(5), Li Q(1)(2), Mei T(1)(2), Chen S(1)(2), Wu Y(1)(2), Sun J(1)(2), He F(6), 
Wang H(7)(8), Yu L(9)(10).

Author information:
(1)The Fourth Clinical School of Medicine, Zhejiang Chinese Medical 
University，Hangzhou First People's Hospital, Hangzhou, China.
(2)Department of Pain, The Affiliated Hangzhou First People's Hospital, Westlake 
University School of Medicine, Hangzhou, China.
(3)Department of Neurology, Shanghai General Hospital, Shanghai Jiao Tong 
University School of Medicine, No.86, Wujin Road, Shanghai, 200080, China.
(4)Program in Developmental and Stem Cell Biology, The Hospital for Sick 
Children, 686 Bay Street, Toronto, ON, M5G 0A4, Canada.
(5)Department of Anaesthesiology, First Affiliated Hospital of Soochow 
University, Suzhou, Jiangsu, China.
(6)Department of Pain Management, Center for Intracranial Hypotension 
Management, Sir Run Run Shaw Hospital, Medical College of Zhejiang University, 
Hangzhou, China. hefeifang@zju.edu.cn.
(7)The Fourth Clinical School of Medicine, Zhejiang Chinese Medical 
University，Hangzhou First People's Hospital, Hangzhou, China. 
wanghanbin@hospital.westlake.edu.cn.
(8)Department of Pain, The Affiliated Hangzhou First People's Hospital, Westlake 
University School of Medicine, Hangzhou, China. 
wanghanbin@hospital.westlake.edu.cn.
(9)The Fourth Clinical School of Medicine, Zhejiang Chinese Medical 
University，Hangzhou First People's Hospital, Hangzhou, China. 
yuliang@hospital.westlake.edu.cn.
(10)Department of Pain, The Affiliated Hangzhou First People's Hospital, 
Westlake University School of Medicine, Hangzhou, China. 
yuliang@hospital.westlake.edu.cn.
(#)Contributed equally

OBJECTIVE: To evaluate the use of ChatGPT and DeepSeek in clinical practice to 
provide healthcare professionals with accurate information on the prevention, 
diagnosis, and management of post-dural puncture headache (PDPH), in particular 
to evaluate ChatGPT-4o, ChatGPT-4o mini, DeepSeek-V3 and DeepSeek with Deep 
Think(R1)'s responses with consensus practice guidelines for headache after 
dural puncture.
BACKGROUND: Post-dural puncture headache (PDPH) is a common complication of 
dural puncture. Currently, there is a lack of evidence-based guidance on the 
prevention, diagnosis and management of PDPH. The 2023 Consensus guidelines 
provide comprehensive information. With the development and popularization of 
AI, more and more people are using ai models, including patients and doctors. 
However, the quality of the answers provided by ai has not yet been tested.
METHODS: Responses from ChatGPT-4o, ChatGPT-4o mini, DeepSeek-V3, and 
DeepSeek-R1 were evaluated against PDPH guidelines using four dimensions: 
Accuracy (guideline adherence), Overconclusiveness (unjustified 
recommendations), Supplementary information (additional relevant details), and 
Incompleteness (omission of critical guidelines). A 5-point Likert scale further 
assessed response accuracy and completeness.
RESULTS: All four models show high accuracy and completeness.Of the 10 clinical 
guidelines evaluated,ChatGPT-4o, ChatGPT-4o mini, DeepSeek-V3 and DeepSeek-R1 
all showed 100% accuracy in responses (10/10)(p = 1). None of the four models 
showed overly conclusive results(p = 1). In terms of supplementary information, 
ChatGPT-4o,ChatGPT-4o mini and DeepSeek-R1 are 100% (10/10), DeepSeek-V3 is 90% 
(9/10)(p = 1). In terms of incompleteness, ChatGPT-4o is 80%(8/10), DeepSeek-R1 
is 70%(7/10), ChatGPT-4o mini and DeepSeek-V3 are 60% (6/10) (p = 0.729).
CONCLUSION: All four AI models demonstrate clinical validity, with ChatGPT-4o 
and DeepSeek-R1 showing stronger guideline alignment. Though largely accurate, 
their responses achieve only 60-80% completeness relative to medical guidelines. 
Healthcare professionals must exercise caution when using AI tools and should 
critically evaluate outputs before clinical application. While promising, their 
partial guideline coverage requires careful human oversight. Further validation 
research is essential before these models can reliably support clinical 
decision-making for complex conditions like PDPH.

© 2025. The Author(s).

DOI: 10.1186/s12883-025-04280-8
PMCID: PMC12211737
PMID: 40597769 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


87. JACC Cardiovasc Interv. 2025 Oct 27;18(20):2458-2467. doi: 
10.1016/j.jcin.2025.08.019.

Performance of Large Language Models on the Acute Coronary Syndrome Guidelines 
Using Retrieval-Augmented Generation.

Alexandrou M(1), Kumar S(2), Mahtani AU(3), Strepkos D(1), Carvalho PEP(1), 
Mutlu D(1), Ser OS(1), Rempakos A(4), Mastrodemos OC(1), Rangan BV(1), Jalli 
S(1), Sandoval Y(1), Brilakis ES(5).

Author information:
(1)Minneapolis Heart Institute and Minneapolis Heart Institute Foundation, 
Abbott Northwestern Hospital, Minneapolis, Minnesota, USA.
(2)Department of Cardiology, Creighton University School of Medicine, Phoenix, 
Arizona, USA.
(3)Department of Cardiology, Virginia Commonwealth University, Richmond, 
Virginia, USA.
(4)Department of Internal Medicine, Corewell Health William Beaumont University 
Hospital, Royal Oak, Michigan, USA.
(5)Minneapolis Heart Institute and Minneapolis Heart Institute Foundation, 
Abbott Northwestern Hospital, Minneapolis, Minnesota, USA. Electronic address: 
esbrilakis@gmail.com.

BACKGROUND: Large language models (LLMs) are increasingly applied in 
interventional cardiology, but hallucinations limit their clinical utility.
OBJECTIVES: The aim of this study was to assess whether retrieval-augmented 
generation (RAG), a technique that allows LLMs to access guideline content 
during response generation, improves accuracy when answering questions on the 
basis of the guidelines for acute coronary syndromes.
METHODS: The accuracy of ChatGPT-4o, DeepSeek R1, and Med-PaLM 2 was compared 
using a set of 38 open-ended cardiology guideline-based questions and answers. 
ChatGPT-4o and DeepSeek R1 were evaluated with and without RAG, while Med-PaLM 
2, a medicine-specific LLM, was tested without RAG. Model outputs were compared 
against guideline recommendations using an artificial intelligence-powered 
similarity score tool.
RESULTS: DeepSeek R1 with RAG achieved the highest accuracy (94.7%; 95% CI: 
82.7%-98.5%), followed by ChatGPT-4o with RAG (92.1%; 95% CI: 79.2%-97.3%) (P = 
0.922). ChatGPT-4o without RAG achieved 71.1% accuracy (95% CI: 55.2%-83.0%), 
which significantly improved with RAG (P = 0.017). Among non-RAG models, 
DeepSeek R1 demonstrated the highest accuracy (78.9%; 95% CI: 63.7%-88.9%), 
followed by ChatGPT-4o without RAG (71.1%) (P = 0.083). Med-PaLM 2 had the 
lowest accuracy (68.4%; 95% CI: 52.5%-80.9%). Spearman correlation analysis 
revealed a strong correlation between DeepSeek R1 without RAG and Med-PaLM 2 (r 
= 0.646; 95% CI: 0.411-0.800; P < 0.001), indicating similar response patterns. 
Scatterplot analysis further revealed that RAG disproportionately improved lower 
scoring questions in DeepSeek R1 while improving scores more evenly in 
ChatGPT-4o.
CONCLUSIONS: Embedding guideline content into LLM workflows via RAG can enhance 
LLM accuracy for clinical applications, particularly in scenarios common to 
interventional cardiology. These results support the potential for LLMs, when 
enhanced with domain-specific knowledge, to optimize clinical decision making 
and increase alignment with guidelines.

Copyright © 2025 American College of Cardiology Foundation. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jcin.2025.08.019
PMID: 41161918 [Indexed for MEDLINE]

Conflict of interest statement: Funding Support and Author Disclosures Dr 
Sandoval reports the following disclosures: Abbott (consultant, advisory board, 
speaker), CathWorks (consultant, speaker), Cleerly (speaker, research grant), GE 
Healthcare (consultant, advisory board), HeartFlow (consultant, speaker), 
Medtronic (speaker), Philips (consultant, advisory board, speaker), Roche 
Diagnostics (consultant, advisory board, speaker), and Zoll (advisory board); 
owner, Systole LLC. He is an Associate Editor for JACC Advances. He and others 
hold patent 20210401347. Patent 20210401347. Dr Brilakis has received consulting 
and speaking honoraria from Abbott Vascular, American Heart Association 
(associate editor, Circulation), Biotronik, Boston Scientific, the 
Cardiovascular Innovations Foundation (Board of Directors), Cordis, 
Cardiovascular Systems, Elsevier, GE Healthcare, Haemonetics, IMDS, Medtronic, 
and Teleflex; has received research support from Boston Scientific and GE 
Healthcare; is an owner of Hippocrates; and is a shareholder in MHI Ventures, 
Cleerly Health, Stallion Medical, and TrueVue. All other authors have reported 
that they have no relationships relevant to the contents of this paper to 
disclose.


88. J Am Coll Emerg Physicians Open. 2025 Jan 13;6(1):100031. doi: 
10.1016/j.acepjo.2024.100031. eCollection 2025 Feb.

Addressing Note Bloat: Solutions for Effective Clinical Documentation.

Genes N(1), Sills J(2), Heaton HA(3), Shy BD(4), Scofi J(5).

Author information:
(1)Department of Emergency Medicine Ronald O. Perelman, NYU Grossman School of 
Medicine, New York, New York, USA.
(2)UMass Chan Medical School - Baystate, Springfield, Massachusetts, USA.
(3)Mayo Clinic, Rochester, Minnesota, USA.
(4)University of Colorado School of Medicine, Aurora, Colorado, USA.
(5)Emergency Medicine Service Line, Northwell Health, New Hyde Park, New York, 
USA.

Clinical documentation in the United States has grown longer and more difficult 
to read, a phenomenon described as "note bloat." This issue is especially 
pronounced in emergency medicine, where high diagnostic uncertainty and brief 
evaluations demand focused, efficient chart review to inform decision-making. 
Note bloat arises from multiple factors: efforts to enhance billing, mitigate 
malpractice risk, and leverage electronic health record tools that improve speed 
and completeness. We discuss best practices based on available evidence and 
expert opinion to improve note clarity and concision. Recent E/M coding reforms 
aim to streamline documentation by prioritizing medical decision-making over 
details of historical and physical examination, though implementation varies. 
New technologies such as generative artificial intelligence present 
opportunities and challenges for documentation practices. Addressing note bloat 
will require ongoing effort from clinical leadership, electronic health record 
vendors, and professional organizations.

© 2024 The Author(s).

DOI: 10.1016/j.acepjo.2024.100031
PMCID: PMC11852943
PMID: 40012671

Conflict of interest statement: All authors have affirmed they have no conflicts 
of interest to declare.


89. Surgery. 2025 Jul;183:109393. doi: 10.1016/j.surg.2025.109393. Epub 2025 May 9.

A literature review on the role of artificial intelligence-based chatbots in 
patient education in colorectal surgery.

Garcia Garcia L(1), Emile SH(2), Linkeshwaran L(3), Wignakumar A(4), Wexner 
SD(5).

Author information:
(1)Servicio de Cirugía General y Digestiva. Complejo Hospitalario Universitario 
Materno Infantil de Gran Canaria, Las Palmas de Gran Canaria, Spain.
(2)Ellen Leifer Shulman and Steven Shulman Digestive Disease Center, Cleveland 
Clinic Florida, Weston, FL; Colorectal Surgery Unit, General Surgery Department, 
Mansoura University Hospitals, Mansoura, Egypt. Electronic address: 
https://twitter.com/dr_samehhany81.
(3)UCL Medical School, University College London, London, United Kingdom.
(4)Ellen Leifer Shulman and Steven Shulman Digestive Disease Center, Cleveland 
Clinic Florida, Weston, FL. Electronic address: https://twitter.com/AWignakumar.
(5)Ellen Leifer Shulman and Steven Shulman Digestive Disease Center, Cleveland 
Clinic Florida, Weston, FL. Electronic address: wexners@ccf.org.

INTRODUCTION: Artificial intelligence-based chatbots are becoming increasingly 
used in patient education, in the realm of colorectal diseases. Perhaps, not 
surprisingly, concerns about the appropriateness of chatbot answers have been 
raised by healthcare professionals. Numerous studies have explored the utility 
and accuracy of chatbots in providing information in several clinical 
disciplines. This review aimed to summarize the findings of published studies, 
highlighting the strengths and limitations of chatbots used in medical education 
for colorectal surgery.
METHODS: We searched MEDLINE via PubMed and Scopus in February 2025 for original 
articles evaluating artificial intelligence-based chatbots in patient education 
related to colorectal surgery, categorizing them into 3 groups: colorectal 
cancer, inflammatory bowel diseases, and other colorectal conditions.
RESULTS: We identified 15 studies, 9 assessed chatbot utility in patient 
education in colorectal cancer, 4 assessed their utility in inflammatory bowel 
diseases, 1 involved benign anal conditions, and another involved intestinal 
stomas. Our findings indicated that chatbots, particularly ChatGPT, can improve 
patient education by providing accessible information on common questions. 
However, we also identified several limitations of the ability of chatbots to 
address complex medical issues which underscored that these tools may complement 
rather than replace professional medical guidance.
CONCLUSION: Chatbots may be useful for patient education related to simple and 
basic information, but not in complex and patient-specific settings. Future 
research should focus on refining chatbot algorithms to enhance the accuracy and 
depth of their responses, ensuring they effectively support patient education 
while maintaining the crucial role of healthcare providers.

Copyright © 2025 The Author(s). Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.surg.2025.109393
PMID: 40347684 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest/Disclosure Dr Wexner is a 
consultant for ActivSurgical, Arthrex, Baxter, Becton, Dickinson and Co, Glaxo 
Smith Kline, Intuitive Surgical, OstomyCure, Takeda, and Virtual Ports; has 
consulting agreements with stock options for consulting with GI View, 
OstomyCure, and Virtual Ports; is a member of the Data Safety Monitoring Board 
of JSR/WCG/ACI (chair) and Polypoid (chair); and receives royalties from 
Intuitive Surgical, Karl Storz Endoscopy America Inc, and Unique Surgical 
Solutions, LLC. Dr Emile is a consultant for Becton, Dickinson and Co. None of 
the other authors report any conflicts of interest.


90. Arthroscopy. 2025 Apr;41(4):874-885. doi: 10.1016/j.arthro.2024.12.011. Epub 
2024 Dec 15.

Ethical Application of Generative Artificial Intelligence in Medicine.

Hasan SS(1), Fury MS(2), Woo JJ(3), Kunze KN(4), Ramkumar PN(5).

Author information:
(1)Rush Medical College, Chicago, Illinois, U.S.A.
(2)Baton Rouge Orthopaedic Clinic, Baton Rouge, Louisiana, U.S.A.
(3)Brown University/The Warren Alpert School of Brown University, Providence, 
Rhode Island, U.S.A.
(4)Hospital for Special Surgery, New York, New York, U.S.A.
(5)Commons Clinic, Long Beach, California, U.S.A.. Electronic address: 
premramkumar@gmail.com.

Generative artificial intelligence (AI) may revolutionize health care, providing 
solutions that range from enhancing diagnostic accuracy to personalizing 
treatment plans. However, its rapid and largely unregulated integration into 
medicine raises ethical concerns related to data integrity, patient safety, and 
appropriate oversight. One of the primary ethical challenges lies in generative 
AI's potential to produce misleading or fabricated information, posing risks of 
misdiagnosis or inappropriate treatment recommendations, which underscore the 
necessity for robust physician oversight. Transparency also remains a critical 
concern, as the closed-source nature of many large-language models prevents both 
patients and health care providers from understanding the reasoning behind 
AI-generated outputs, potentially eroding trust. The lack of regulatory approval 
for AI as a medical device, combined with concerns around the security of 
patient-derived data and AI-generated synthetic data, further complicates its 
safe integration into clinical workflows. Furthermore, synthetic datasets 
generated by AI, although valuable for augmenting research in areas with scarce 
data, complicate questions of data ownership, patient consent, and scientific 
validity. In addition, generative AI's ability to streamline administrative 
tasks risks depersonalizing care, further distancing providers from patients. 
These challenges compound the deeper issues plaguing the health care system, 
including the emphasis of volume and speed over value and expertise. The use of 
generative AI in medicine brings about mass scaling of synthetic information, 
thereby necessitating careful adoption to protect patient care and medical 
advancement. Given these considerations, generative AI applications warrant 
regulatory and critical scrutiny. Key starting points include establishing 
strict standards for data security and transparency, implementing oversight akin 
to institutional review boards to govern data usage, and developing 
interdisciplinary guidelines that involve developers, clinicians, and ethicists. 
By addressing these concerns, we can better align generative AI adoption with 
the core foundations of humanistic health care, preserving patient safety, 
autonomy, and trust while harnessing AI's transformative potential. LEVEL OF 
EVIDENCE: Level V, expert opinion.

Copyright © 2024 Arthroscopy Association of North America. Published by Elsevier 
Inc. All rights reserved.

DOI: 10.1016/j.arthro.2024.12.011
PMID: 39689842 [Indexed for MEDLINE]

Conflict of interest statement: Disclosures The authors declare the following 
financial interests/personal relationships which may be considered as potential 
competing interests: P.N.R. reports equity or stocks from Intelligent Health 
Analytics and Overture Orthopaedics; consulting/advisor for Pacira BioSciences, 
Stryker, and Globus Medical; and editorial board of Arthroscopy. K.N.K. reports 
editorial board of Arthroscopy. All other authors (S.S.H., M.S.F., J.J.W.) 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


91. Int J Med Inform. 2025 Dec;204:106088. doi: 10.1016/j.ijmedinf.2025.106088. Epub 
2025 Aug 23.

Diagnostic performance of newly developed large language models in critical 
illness cases: A comparative study.

Wu X(1), Huang Y(2), He Q(1).

Author information:
(1)Department of Intensive Care Medicine, Affiliated Hospital of Southwest 
Jiaotong University, The Third People's Hospital of Chengdu, Chengdu, Sichuan, 
China.
(2)Department of Intensive Care Medicine, Affiliated Hospital of Southwest 
Jiaotong University, The Third People's Hospital of Chengdu, Chengdu, Sichuan, 
China. Electronic address: huangyu0120er@126.com.

BACKGROUND: Large language models (LLMs) are increasingly used in clinical 
decision support, and newly developed models have demonstrated promising 
potential, yet their diagnostic performance for critically ill patients in 
intensive care unit (ICU) settings remains underexplored. This study evaluated 
the diagnostic accuracy, differential diagnosis quality, and response quality in 
critical illness cases of four newly developed LLMs.
METHODS: In this cross-sectional comparative study, four newly developed 
LLMs-ChatGPT-4o, ChatGPT-o3, DeepSeek-V3, and DeepSeek-R1-were evaluated using 
50 critical illness cases in ICU settings from published literature. Diagnostic 
accuracy and response quality were compared across models.
RESULTS: A total of 50 critical illness cases were included. ChatGPT-o3 achieved 
the top diagnosis accuracy at 72 % (36/50; 95 % CI 0.600-0.840), followed by 
DeepSeek-R1 at 68 % (34/50; 95 % CI 0.540-0.800), ChatGPT-4o at 64 % (32/50; 95 
% CI 0.500-0.760), and DeepSeek-V3 at 32 % (16/50; 95 % CI 0.200-0.460). 
ChatGPT-o3, DeepSeek-R1, and ChatGPT-4o all significantly outperformed 
DeepSeek-V3, with no significant differences among the three. The median 
differential quality score was 5.0 for ChatGPT-o3 (IQR 5.0-5.0; 95 % CI 
5.0-5.0), DeepSeek-R1 (IQR 5.0-5.0; 95 % CI 5.0-5.0), and ChatGPT-4o (IQR 
4.0-5.0; 95 % CI 4.5-5.0), and 4.0 for DeepSeek-V3 (IQR 3.0-5.0; 95 % CI 
4.0-5.0). ChatGPT-o3 and DeepSeek-R1 scored significantly higher than 
DeepSeek-V3; ChatGPT-4o showed a non-significant trend toward better 
performance.All models received high Likert ratings for response completeness, 
clarity, and usefulness. ChatGPT-o3, DeepSeek-R1, and ChatGPT-4o each showed a 
trend toward better response quality compared to DeepSeek-V3, although no 
significant differences were observed among the models.
CONCLUSIONS: The newly developed models, especially the reasoning models, 
demonstrated strong potential in supporting diagnosis in critical illness cases 
in ICU settings. With further domain-specific fine-tuning, their diagnostic 
accuracy could be further enhanced. Notably, the open-source reasoning model 
DeepSeek-R1 performed competitively, suggesting strong potential for scalable 
deployment in resource-limited settings.

Copyright © 2025 The Author(s). Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106088
PMID: 40865411 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


92. Integr Med Res. 2025 Dec;14(4):101217. doi: 10.1016/j.imr.2025.101217. Epub 2025 
Aug 6.

Using artificial intelligence for the development of a living evidence map: The 
pharmacopuncture example.

Kwon CY(1)(2).

Author information:
(1)Department of Oriental Neuropsychiatry, Dong-eui University College of Korean 
Medicine, 52-57, Yangjeong-ro, Busanjin-gu, Busan, Republic of Korea.
(2)Anti-Aging Research Center, Dong-eui University, Busan, Republic of Korea.

BACKGROUND: Evidence map is a tool that visualizes the research status to 
identify research gaps and set priorities, but it has the limitation of the 
burden of continuous literature monitoring. Pharmacopuncture is a therapeutic 
modality used in Korean medicine that involves the injection of medicinal 
extracts into acupoints. This study aimed to develop an artificial intelligence 
(AI)-based automated system for building and maintaining a living evidence map 
in the field of pharmacopuncture research and verify its performance.
METHODS: A web-based system that automates literature search, selection, data 
extraction, and classification using PubMed API and Gemini AI was developed. The 
accuracy of nine tasks was evaluated and time efficiency was measured using 
manual review by experts as a standard reference. A visualization system using 
interactive bubble charts was implemented to provide a research gap 
identification function.
RESULTS: The AI system achieved an overall accuracy of 94.00% (error rate of 
6.00%) for 202 articles, including detailed data extraction for 90 articles. 
Task-specific performance varied from sample size extraction (0% error rate) to 
pharmacopuncture name extraction (22.22% error rate), with high accuracy of over 
90% in most tasks. Time efficiency was improved by 68.9% (190 vs. 59 minutes, 
including quality control), demonstrating that daily updates are practically 
feasible.
CONCLUSIONS: The developed visualization system significantly improves the 
existing static evidence organization method by intuitively identifying research 
gaps. The AI-based living evidence map enables continuous evidence monitoring in 
the field of pharmacopuncture research with high accuracy and significant time 
savings.

© 2025 Korea Institute of Oriental Medicine. Published by Elsevier B.V.

DOI: 10.1016/j.imr.2025.101217
PMCID: PMC12395373
PMID: 40896349


93. Eur J Surg Oncol. 2025 Aug;51(8):110096. doi: 10.1016/j.ejso.2025.110096. Epub 
2025 Apr 24.

ChatGPT-4o outperforms gemini advanced in assisting multidisciplinary 
decision-making for advanced gastric cancer.

Li H(1), Huang J(2), Liu K(2), Liu J(2), Liu Q(3), Zhou Z(2), Zong Z(2), Mao 
S(4).

Author information:
(1)Department of General Surgery, The Second Affiliated Hospital of Nanchang 
University, Nanchang, Jiangxi, China. Electronic address: lhzdoctor182@163.com.
(2)Department of General Surgery, The Second Affiliated Hospital of Nanchang 
University, Nanchang, Jiangxi, China.
(3)Department of Medical Oncology, The Second Affiliated Hospital of Nanchang 
University, Nanchang, Jiangxi, China.
(4)Department of General Surgery, The Second Affiliated Hospital of Nanchang 
University, Nanchang, Jiangxi, China. Electronic address: maoshengxun@126.com.

BACKGROUND & AIMS: The treatment of advanced gastric cancer (GC) requires 
precise and comprehensive clinical decision-making. Artificial intelligence (AI) 
chatbots offer potential tools to enhance multidisciplinary team (MDT) 
discussions. This study aims to compare the performances of ChatGPT-4o and 
Gemini Advanced in generating treatment recommendations for advanced GC.
METHODS: The study involved three steps: (1) evaluating responses to ten 
critical clinical questions, (2) analyzing clinical cases from MDT meetings at 
our institution, and (3) reviewing rare GC cases from PubMed. It included 95 
advanced GC patients discussed between November 2022 and July 2024, and 14 rare 
cases from PubMed. Prompts designed from advanced GC cases were submitted to 
ChatGPT-4o and Gemini Advanced using a standardized format. Outputs were 
evaluated for accuracy and completeness using a structured 4-point Likert scale. 
Interrater reliability was calculated to ensure consistency among evaluators.
RESULTS: For the ten clinical questions, ChatGPT-4o achieved better performances 
compared to Gemini Advanced. In MDT cases, ChatGPT-4o provided more valuable 
recommendations in surgical suggestion, chemotherapy recommendation, and 
chemotherapy regimens. Subgroup analysis confirmed these findings in both 
routine and complex cases with high interrater reliability. ChatGPT-4o also 
outperformed Gemini Advanced in the analysis of rare GC cases from PubMed, 
showing superior accuracy with high interrater reliability.
CONCLUSIONS: While our findings suggest that AI chatbots can generate clinically 
relevant and guideline-based treatment recommendations, their use in MDT 
decision-making should be viewed as supportive rather than autonomous. We 
emphasize that while AI chatbots have potential as decision-support tools, but 
they should be integrated only under expert supervision in a real-world clinical 
context.

Copyright © 2025. Published by Elsevier Ltd.

DOI: 10.1016/j.ejso.2025.110096
PMID: 40294561 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of interests The authors declare 
that they have no known competing financial interests or personal relationships 
that could have appeared to influence the work reported in this paper.


94. Comput Med Imaging Graph. 2025 Apr;121:102498. doi: 
10.1016/j.compmedimag.2025.102498. Epub 2025 Feb 19.

A novel generative model for brain tumor detection using magnetic resonance 
imaging.

da Costa Nascimento JJ(1), Marques AG(2), do Nascimento Souza L(3), de Mattos 
Dourado Junior CMJ(4), da Silva Barros AC(5), de Albuquerque VHC(6), de Freitas 
Sousa LF(7).

Author information:
(1)Universidade Federal do Ceará, Fortaleza, 60455-760, CE, Brazil. Electronic 
address: jerovane@alu.ufc.br.
(2)Instituto Federal de Educação, Ciência e Tecnologia do Ceará - Campus 
Fortaleza, Fortaleza, 60040-531, CE, Brazil. Electronic address: 
adriell.gomes.marques07@aluno.ifce.edu.br.
(3)Universidade Federal do Cariri, Juazeiro do Norte, 63048-080, CE, Brazil. 
Electronic address: lucas.nascimento@aluno.ufca.edu.br.
(4)Instituto Federal de Educação, Ciência e Tecnologia do Ceará - Campus 
Fortaleza, Fortaleza, 60040-531, CE, Brazil. Electronic address: 
mauriciodourado@ifce.edu.br.
(5)Universidade da Integração Internacional da Lusofonia Afro-Brasileira, 
Redenção, 43900-000, CE, Brazil. Electronic address: carlosbarros@unilab.edu.br.
(6)Universidade Federal do Ceará, Fortaleza, 60455-760, CE, Brazil. Electronic 
address: victor.albuquerque@ieee.org.
(7)Universidade Federal do Cariri, Juazeiro do Norte, 63048-080, CE, Brazil. 
Electronic address: fabricio.freitas@ufca.edu.br.

Brain tumors are a disease that kills thousands of people worldwide each year. 
Early identification through diagnosis is essential for monitoring and treating 
patients. The proposed study brings a new method through intelligent 
computational cells that are capable of segmenting the tumor region with high 
precision. The method uses deep learning to detect brain tumors with the "You 
only look once" (Yolov8) framework, and a fine-tuning process at the end of the 
network layer using intelligent computational cells capable of traversing the 
detected region, segmenting the edges of the brain tumor. In addition, the 
method uses a classification pipeline that combines a set of classifiers and 
extractors combined with grid search, to find the best combination and the best 
parameters for the dataset. The method obtained satisfactory results above 98% 
accuracy for region detection, and above 99% for brain tumor segmentation and 
accuracies above 98% for binary classification of brain tumor, and segmentation 
time obtaining less than 1 s, surpassing the state of the art compared to the 
same database, demonstrating the effectiveness of the proposed method. The new 
approach proposes the classification of different databases through data fusion 
to classify the presence of tumor in MRI images, as well as the patient's life 
span. The segmentation and classification steps are validated by comparing them 
with the literature, with comparisons between works that used the same dataset. 
The method addresses a new generative AI for brain tumor capable of generating a 
pre-diagnosis through input data through Large Language Model (LLM), and can be 
used in systems to aid medical imaging diagnosis. As a contribution, this study 
employs new detection models combined with innovative methods based on digital 
image processing to improve segmentation metrics, as well as the use of Data 
Fusion, combining two tumor datasets to enhance classification performance. The 
study also utilizes LLM models to refine the pre-diagnosis obtained 
post-classification. Thus, this study proposes a Computer-Aided Diagnosis (CAD) 
method through AI with PDI, CNN, and LLM.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.compmedimag.2025.102498
PMID: 39985841 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: Victor Hugo C. de Albuquerque 
reports financial support was provided by Federal University of Ceara. Victor 
Hugo C. de Albuquerque reports a relationship with Federal University of Ceara 
that includes: employment. If there are other authors, they declare that they 
have no known competing financial interests or personal relationships that could 
have appeared to influence the work reported in this paper.


95. Radiology. 2025 Sep;316(3):e241516. doi: 10.1148/radiol.241516.

Best Practices for the Safe Use of Large Language Models and Other Generative AI 
in Radiology.

Yi PH(1), Haver HL(2), Jeudy JJ(3), Kim W(4), Kitamura FC(5)(6), Oluyemi ET(7), 
Smith AD(1), Moy L(8), Parekh VS(9).

Author information:
(1)Department of Radiology, St. Jude Children's Research Hospital, 262 Danny 
Thomas Pl, Memphis, TN 38105-3678.
(2)Department of Radiology, Brigham and Women's Hospital, Boston, Mass.
(3)Department of Diagnostic Radiology and Nuclear Medicine, University of 
Maryland School of Medicine, Baltimore, Md.
(4)Department of Radiology, Palo Alto VA Medical Center, Palo Alto, Calif.
(5)Department of Diagnostic Imaging, Universidade Federal de São Paulo, São 
Paulo, Brazil.
(6)Bunkerhill Health, San Francisco, Calif.
(7)The Russell H. Morgan Department of Radiology and Radiological Science, Johns 
Hopkins University School of Medicine, Baltimore, Md.
(8)Department of Radiology, New York University Grossman School of Medicine, New 
York, NY.
(9)Department of Diagnostic & Interventional Imaging, McGovern Medical School, 
University of Texas Health Science Center, Houston, Tex.

As large language models (LLMs) and other generative artificial intelligence 
(AI) models are rapidly integrated into radiology workflows, unique pitfalls 
threatening their safe use have emerged. Problems with AI are often identified 
only after public release, highlighting the need for preventive measures to 
mitigate negative impacts and ensure safe, effective deployment into clinical 
settings. This article summarizes best practices for the safe use of LLMs and 
other generative AI models in radiology, focusing on three key areas that can 
lead to pitfalls if overlooked: regulatory issues, data privacy, and bias. To 
address these areas and minimize risk to patients, radiologists must examine all 
potential failure modes and ensure vendor transparency. These best practices are 
based on the best available evidence and the experiences of leaders in the 
field. Ultimately, this article provides actionable guidelines for radiologists, 
radiology departments, and vendors using and integrating generative AI into 
radiology workflows, offering a framework to prevent these problems.

© RSNA, 2025.

DOI: 10.1148/radiol.241516
PMCID: PMC12501631
PMID: 40985835 [Indexed for MEDLINE]

Conflict of interest statement: Disclosures of conflicts of interest: P.H.Y. 
Grants from the National Institutes of Health/National Cancer Institute, 
American College of Radiology, and RSNA; associate editor for Radiology: 
Artificial Intelligence and vice chair of the Society for Imaging Informatics in 
Medicine (SIIM) Annual Meeting Planning Committee; stock or stock options in 
Bunkerhill Health. H.L.H. No relevant relationships. J.J.J. No relevant 
relationships. W.K. Consulting fees from Infiniti Medical, Hyperfine Research, 
Nuance Communications, and ClariPi; honoraria from the Radiology Business 
Management Association, University of Pennsylvania, and American College of 
Radiology; owner of Equium Intelligence and employee of Rad AI; patents with 
Equium Intelligence and Rad AI; advisory board member for Alara Imaging, 
ImageBiopsy Lab, Braid Health, Luxsonic Technologies, Within Health, and 
Inference Analytics; former member of the board of directors for SIIM; stock or 
stock options in Equium Intelligence, Nuance Communications, and Rad AI. F.C.K. 
Consulting fees from Bunkerhill Health, GE HealthCare, and MD.ai; speaker fees 
from Sharing Progress in Cancer Care; early career consultant to the editor of 
Radiology; editor of Radiology: Artificial Intelligence; vice chair of the SIIM 
Machine Learning Committee; member of the RSNA AI Committee; and member of the 
RSNA Radiology Informatics Council. E.T.O. Grants from GE HealthCare and the 
Bristol Myers Squibb Foundation. A.D.S. Associate editor for Radiology. L.M. 
Grants from Siemens, Gordon and Betty Moore Foundation, Mary Kay Foundation, and 
Google; consulting fees from Lunit, ICAD, and Guerbet; payment for lectures from 
ICAD and Guerbet; board member for the International Society for Magnetic 
Resonance in Medicine and Society of Breast Imaging; stock or stock options in 
Lunit. V.S.P. No relevant relationships.


96. Br J Oral Maxillofac Surg. 2025 Jun;63(5):357-362. doi: 
10.1016/j.bjoms.2025.03.006. Epub 2025 Mar 24.

The impact of the large language model ChatGPT in oral and maxillofacial 
surgery: a systematic review.

de Menezes Torres LM(1), de Morais EF(2), Fernandes Almeida DRM(3), Pagotto 
LEC(4), de Santana Santos T(5).

Author information:
(1)Department of Oral and Maxillofacial Surgery, Universidade de Pernambuco, 
Recife, PE, Brazil.
(2)Department of Oral Diagnosis, and Graduate Program in Oral Biology, 
Piracicaba Dental School, University of Campinas, Piracicaba, São Paulo, Brazil. 
Electronic address: evertonfreitas2@hotmail.com.
(3)School of Dentistry, Federal University of Alfenas (Unifal-MG), Alfenas, 
Minas Gerais, Brazil.
(4)Oral and Maxillofacial Surgeon, Hospital Sírio-Libanes, São Paulo, Brazil.
(5)Institute Maxillofacial Education, Aracaju, Sergipe, Brazil.

This systematic review evaluates the impact of the large language model (LLM) 
ChatGPT in oral and maxillofacial surgery. Following PRISMA guidelines and 
registered in PROSPERO (CRD42024625882), the study involved a comprehensive 
search across PubMed/Medline, Embase, Scopus, and Science Direct. Inclusion 
criteria focused on ChatGPT's use in clinical decision-making, surgical 
planning, patient education, and research. Ten studies were reviewed, assessing 
ChatGPT's performance in diagnostic accuracy, surgical efficiency, and patient 
satisfaction. GPT-4 achieved the highest accuracy (76.8%) in multiple-choice 
questions but showed variability, with lower performance in pharmacology and 
complex clinical scenarios. It excelled in generating informed consent 
documents, outperforming other AI models and human residents in accuracy, 
completeness, and readability. It also provided accurate and supportive 
responses in postoperative follow up. However, limitations were noted in 
handling complex clinical decisions and providing personalised advice for cases 
such as oral cancer and orthognathic surgery. While ChatGPT shows potential in 
improving patient communication, reducing healthcare workloads, and providing 
up-to-date information, its current limitations in accuracy and personalisation 
underscore the need for human supervision and integration with clinical 
databases. AI tools like ChatGPT can complement, but should not replace, human 
judgment in specialised fields such as oral and maxillofacial surgery.

Copyright © 2025 The British Association of Oral and Maxillofacial Surgeons. 
Published by Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.bjoms.2025.03.006
PMID: 40251084 [Indexed for MEDLINE]


97. Transl Lung Cancer Res. 2025 May 30;14(5):1830-1847. doi: 10.21037/tlcr-24-801. 
Epub 2025 May 23.

Opportunities and challenges in lung cancer care in the era of large language 
models and vision language models.

Luo Y(1), Hooshangnejad H(1)(2), Ngwa W(2), Ding K(2).

Author information:
(1)Department of Biomedical Engineering, Johns Hopkins University, Baltimore, 
MD, USA.
(2)Department of Radiation Oncology and Molecular Radiation Sciences, Johns 
Hopkins University, Baltimore, MD, USA.

Lung cancer remains the leading cause of cancer-related deaths globally. Over 
the past decade, the development of artificial intelligence (AI) has 
significantly propelled lung cancer care, particularly in areas such as lung 
cancer early diagnosis, survival prediction, recurrence prediction, medical 
image processing, medical image registration, medical visual question answering, 
clinical report writing, medical image generation, and multimodal integration. 
This review aims to provide a comprehensive summary of the various AI methods 
utilized in lung cancer care, with a particular emphasis on machine learning and 
deep learning techniques. Moreover, with the advent and widespread application 
of large language models (LLMs), vision language models (VLMs), and multimodal 
integration for downstream clinical tasks, we explore the current landscape 
these cutting-edge AI tools offer. However, it also presents both significant 
challenges and opportunities, including data privacy risks, inherent biases that 
may exacerbate healthcare disparities, model hallucinations, ethical 
implications, implementation costs, and the lack of standardized evaluation 
metrics. Furthermore, the translation of these technologies from experimental 
research to clinical implementation demands comprehensive validation protocols 
and multidisciplinary collaboration to guarantee patient safety, therapeutic 
efficacy, and equitable healthcare delivery. This review emphasizes the critical 
role of AI in enhancing our understanding and management of lung cancer, 
ultimately striving for precision medicine and equitable healthcare worldwide.

Copyright © 2025 AME Publishing Company. All rights reserved.

DOI: 10.21037/tlcr-24-801
PMCID: PMC12170128
PMID: 40535072

Conflict of interest statement: Conflicts of Interest: All authors have 
completed the ICMJE uniform disclosure form (available at 
https://tlcr.amegroups.com/article/view/10.21037/tlcr-24-801/coif). K.D. serves 
as an unpaid editorial board member of Translational Lung Cancer Research from 
February 2024 to January 2026. The other authors have no conflicts of interest 
to declare.


98. NPJ Digit Med. 2025 Dec 15;9(1):42. doi: 10.1038/s41746-025-02216-7.

Evaluating commercial multimodal AI for diabetic eye screening and implications 
for an alternative regulatory pathway.

Hunt MS(1), Dai T(2)(3)(4), Abràmoff MD(5)(6)(7).

Author information:
(1)Department of Ophthalmology and Visual Sciences. University of Iowa, Iowa 
City, IA, USA.
(2)Carey Business School, Johns Hopkins University, Baltimore, MD, USA.
(3)Hopkins Business of Health Initiative, Johns Hopkins University, Washington, 
DC, USA.
(4)School of Nursing, Johns Hopkins University, Baltimore, MD, USA.
(5)Department of Ophthalmology and Visual Sciences. University of Iowa, Iowa 
City, IA, USA. michael-abramoff@uiowa.edu.
(6)Department of Electrical and Computer Engineering, University of Iowa, Iowa 
City, IA, USA. michael-abramoff@uiowa.edu.
(7)Veterans Affairs Medical Center, Iowa City, IA, USA. 
michael-abramoff@uiowa.edu.

Autonomous AI for diabetic eye examination is among the most validated and 
trusted medical AI systems, supported by extensive real-world evidence 
demonstrating safety, efficacy, improved outcomes, increased productivity, and 
cost savings. Yet its adoption remains limited. In contrast, commercially 
available off-the-shelf generative AI models (OTSAIs) are being rapidly tested 
in medical settings despite a lack of such real-world validation. These models 
have shown strong performance on medical reasoning tasks, prompting interest in 
their potential for clinical deployment. We evaluated four OTSAIs-GPT-4o and 
GPT-4o-mini (OpenAI, San Francisco, CA), Grok (xAI, San Francisco, CA), and 
Gemini (Google, Mountain View, CA)-on a specific diagnostic task: diabetic eye 
examination. The OTSAIs were bundled to ensure consistency, and performance was 
assessed using a level 3 reference standard, the publicly available Messidor-2 
dataset. GPT-4o achieved the highest area under the receiver operator 
characteristic curve (AUC), 0.83. Grok achieved 0.63, and AUC was not calculable 
for Gemini. The AUC of retina specialists on the same task was estimated at 
0.94, so the emergent performance of OTSAIs does not match that of clinical 
experts, nor does it approach FDA endpoints for consideration as a medical 
device. Nevertheless, as the performance of these OTSAIs approaches theoretical 
limits in the future, there might be a regulatory path through task-specific 
licensing by State Medical Boards for specific clinical tasks. This path may be 
modeled after licensing for physician assistants, where trust in the bundled 
OTSAI, to be used in an assistive fashion, is achieved through rigorous 
validation for safety and efficacy according to widely accepted regulatory 
considerations for both patient-facing AI, as well as for SaMD processes.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-02216-7
PMCID: PMC12800013
PMID: 41398461

Conflict of interest statement: Competing interests: M.D.A. reports the 
following conflicts of interest: patents and patent applications assigned to the 
University of Iowa and Digital Diagnostics relevant to the subject; Director, 
Consultant, shareholder, Digital Diagnostics Inc; Executive Secretary, 
Healthcare AI Coalition, Washington DC; Treasurer, Collaborative Community on 
Ophthalmic Imaging, Washington DC; member, American Academy of Ophthalmology 
(AAO) AI Committee; member, AI Workgroup Digital Medicine Payment Advisory Group 
(DMPAG) of the American Medical Association. T.D. declares no competing 
financial interests and a nonfinancial interest as a member of multiple study 
teams using LumineticsCore from Digital Diagnostics and as co-lead of Johns 
Hopkins University’s Bloomberg Distinguished Professorship Cluster on Global 
Advances in Medical Artificial Intelligence. T.D. is an Editor for npj Digital 
Medicine and played no role in the internal review or decision to publish this 
article. M.S.H. reports no financial or non-financial competing interests.


99. J Med Internet Res. 2025 Feb 26;27:e67010. doi: 10.2196/67010.

Stroke Diagnosis and Prediction Tool Using ChatGLM: Development and Validation 
Study.

Song X(#)(1), Wang J(#)(2), He F(3), Yin W(4), Ma W(#)(5), Wu J(#)(1).

Author information:
(1)Department of Neurology, Beijing Tsinghua Changgung Hospital, School of 
Clinical Medicine, Tsinghua University, Beijing, China.
(2)Harbin Institute of Technology, Harbin, China.
(3)Department of Neurology, Beijing Geriatric Hospital, Beijing, China.
(4)School of Biomedical Engineeering, Tsinghua Medicine, Tsinghua University, 
Beijing, China.
(5)Institute for AI Industry Research, Tsinghua University, Beijing, China.
(#)Contributed equally

BACKGROUND: Stroke is a globally prevalent disease that imposes a significant 
burden on health care systems and national economies. Accurate and rapid stroke 
diagnosis can substantially increase reperfusion rates, mitigate disability, and 
reduce mortality. However, there are considerable discrepancies in the diagnosis 
and treatment of acute stroke.
OBJECTIVE: The aim of this study is to develop and validate a stroke diagnosis 
and prediction tool using ChatGLM-6B, which uses free-text information from 
electronic health records in conjunction with noncontrast computed tomography 
(NCCT) reports to enhance stroke detection and treatment.
METHODS: A large language model (LLM) using ChatGLM-6B was proposed to 
facilitate stroke diagnosis by identifying optimal input combinations, using 
external tools, and applying instruction tuning and low-rank adaptation (LoRA) 
techniques. A dataset containing details of 1885 patients with and those without 
stroke from 2016 to 2024 was used for training and internal validation; another 
335 patients from two hospitals were used as an external test set, including 230 
patients from the training hospital but admitted at different periods, and 105 
patients from another hospital.
RESULTS: The LLM, which is based on clinical notes and NCCT, demonstrates 
exceptionally high accuracy in stroke diagnosis, achieving 99% in the internal 
validation dataset and 95.5% and 79.1% in two external test cohorts. It 
effectively distinguishes between ischemia and hemorrhage, with an accuracy of 
100% in the validation dataset and 99.1% and 97.1% in the other test cohorts. In 
addition, it identifies large vessel occlusions (LVO) with an accuracy of 80% in 
the validation dataset and 88.6% and 83.3% in the other test cohorts. 
Furthermore, it screens patients eligible for intravenous thrombolysis (IVT) 
with an accuracy of 89.4% in the validation dataset and 60% and 80% in the other 
test cohorts.
CONCLUSIONS: We developed an LLM that leverages clinical text and NCCT to 
identify strokes and guide recanalization therapy. While our results necessitate 
validation through widespread deployment, they hold the potential to enhance 
stroke identification and reduce reperfusion time.

©Xiaowei Song, Jiayi Wang, Feifei He, Wei Yin, Weizhi Ma, Jian Wu. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org), 
26.02.2025.

DOI: 10.2196/67010
PMCID: PMC11904371
PMID: 40009850 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


100. JMIR Med Educ. 2025 Dec 31;11:e81517. doi: 10.2196/81517.

AI in Psychiatric Education and Training From 2016 to 2024: Scoping Review of 
Trends.

Weightman MJ(1)(2)(3), Chur-Hansen A(3), Clark SR(1)(2).

Author information:
(1)Discipline of Psychiatry, Faculty of Health and Medical Sciences, The 
University of Adelaide, Level 6, Adelaide Health and Medical Sciences Building, 
Corner of North Terrace and George Street, Adelaide, SA, 5000, Australia, +61 
883138163.
(2)Central Adelaide Local Health Network, Adelaide, SA, Australia.
(3)School of Psychology, Faculty of Health and Medical Sciences, The University 
of Adelaide, Adelaide, SA, Australia.

BACKGROUND: Artificial intelligence (AI) is rapidly changing both clinical 
psychiatry and the education of medical professionals. However, little is 
currently known about how AI is being discussed in the education and training of 
psychiatry for medical students and doctors around the world.
OBJECTIVE: This paper aims to provide a snapshot of the available data on this 
subject as of 2024. A deliberately broad definition of AI was adopted to capture 
the widest range of relevant literature and applications, including machine 
learning, natural language processing, and generative AI tools.
METHODS: A scoping review was conducted using both peer-reviewed publications 
from PubMed, Embase, PsycINFO, and Scopus databases, and gray literature 
sources. The criterion for inclusion was a description of how AI could be 
applied to education or training in psychiatry.
RESULTS: A total of 26 records published between 2016 and 2024 were included. 
The key themes identified were (1) the imperative for an AI curriculum for 
students or doctors training in psychiatry, (2) uses of AI to develop 
educational resources, (3) uses of AI to develop clinical skills, (4) uses of AI 
for assessments, (5) academic integrity or ethical considerations surrounding 
the use of AI, and (6) tensions relating to competing priorities and directions.
CONCLUSIONS: Although a nascent field, it is clear that AI will increasingly 
impact assessment, clinical skills training, and the development of teaching 
resources in psychiatry. Training curricula will need to reflect the new 
knowledge and skills required for future clinical practice. Educators will need 
to be mindful of academic integrity risks and to emphasize development of 
critical thinking skills. Attitudes of psychiatrists toward the rise of AI in 
training remain underexplored.

© Michael James Weightman, Anna Chur-Hansen, Scott Richard Clark. Originally 
published in JMIR Medical Education (https://mededu.jmir.org).

DOI: 10.2196/81517
PMCID: PMC12755346
PMID: 41474738 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: MJW and ACH declared no 
competing interests with respect to the research, authorship, and/or publication 
of this paper. SRC has participated in advisory and educational boards and 
received speaker’s fees from Janssen-Cilag, Lundbeck, Otsuka, and Servier; 
research funding from Janssen-Cilag, Lundbeck, Otsuka, and Gilead; and data 
sharing from Viatris Australia.


101. IEEE J Biomed Health Inform. 2025 Dec 9;PP. doi: 10.1109/JBHI.2025.3641931. 
Online ahead of print.

Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical 
Practice Guidelines.

Li W, Zhang H, Zhang H, Li Z, Dong Z, Chen Y, Bidargaddi N, Liu H.

Current medical language models, adapted from large language models, typically 
predict ICD code-based diagnosis from electronic health records (EHRs) because 
these labels are readily available. However, ICD codes do not capture the 
nuanced, context-rich reasoning clinicians use for diagnosis. Clinicians 
synthesize diverse patient data and reference clinical practice guidelines 
(CPGs) to make evidence-based decisions. This misalignment limits the clinical 
utility of existing models. We introduce GARMLE-G, a Generation-Augmented 
Retrieval framework that grounds medical language model outputs in authoritative 
CPGs. Unlike conventional Retrieval-Augmented Generation based approaches, 
GARMLE-G enables hallucination-free outputs by directly retrieving authoritative 
guideline content without relying on model-generated text. It (1) integrates LLM 
predictions with EHR data to create semantically rich queries, (2) retrieves 
relevant CPG knowledge snippets via embedding similarity, and (3) fuses 
guideline content with model output to generate clinically aligned 
recommendations. A prototype system for hypertension and coronary heart disease 
diagnosis was developed and evaluated on multiple metrics, demonstrating 
superior retrieval precision, semantic relevance, and clinical guideline 
adherence compared to RAG-based baselines, while maintaining a lightweight 
architecture suitable for localized healthcare deployment. This work provides a 
scalable, low-cost, and hallucination-free method for grounding medical language 
models in evidence-based clinical practice, with strong potential for broader 
clinical deployment.

DOI: 10.1109/JBHI.2025.3641931
PMID: 41364573


102. Int J Med Inform. 2026 Mar 1;207:106172. doi: 10.1016/j.ijmedinf.2025.106172. 
Epub 2025 Nov 12.

Evaluating the performance of Large language models in rheumatology for 
connective tissue Diseases: DeepSeek-R1, ChatGPT-4.0, Copilot, and Gemini-2.0.

Wang G(1), Yang R(1), Zhang Y(1), Wen X(1), Liu C(1), Liu E(1), Tang M(1), Xue 
L(1), Liu Z(2).

Author information:
(1)Department of Rheumatology and Immunology, The Second Affiliated Hospital of 
Soochow University, Suzhou, Jiangsu 215000, China.
(2)Department of Rheumatology and Immunology, The Second Affiliated Hospital of 
Soochow University, Suzhou, Jiangsu 215000, China. Electronic address: 
zcliurheu@suda.edu.cn.

BACKGROUND: Large language models (LLMs) demonstrate significant potential in 
medical information provision and may serve as valuable tools for patients 
seeking health information. Existing research primarily focuses on individual 
models or general medical inquiries, with no systematic evaluation of mainstream 
LLMs' performance. Particularly noteworthy is the absence of cross-comparison 
studies involving Chinese AI model DeepSeek-R1. This research gap may hinder the 
effective translation of artificial intelligence technology into clinical 
practice for rheumatic diseases.
OBJECTIVE: This study aims to assess the accuracy, completeness, readability, 
and level of detail in the responses provided by LLMs to common questions 
related to connective tissue disease (CTD).
METHODS: This cross-sectional study analyzed the responses to 250 common 
questions related to CTD, covering topics such as etiology and pathogenesis, 
risk factors, clinical manifestations, diagnostic criteria and differential 
diagnosis, treatment, prevention, and prognosis. These questions were 
collaboratively developed by three experienced clinicians and piloted by two 
rheumatology residents. Between February 18 and February 20, 2025, the questions 
were input as prompts into DeepSeek-R1, ChatGPT-4.0 (OpenAI), Copilot 
(Microsoft), and Gemini-2.0 (Google). The accuracy, completeness, readability, 
level of detail, and inclusion of health advice disclaimers in the responses 
were evaluated. Two experienced clinicians conducted a double-blind evaluation 
using four standardized scoring tools, with the average score serving as the 
final result. In cases of conflict or significant discrepancies in scores for 
the same question, the final score for each answer was determined by majority 
consensus.
RESULTS: A total of 1000 responses (4000 scores) were generated, with an average 
accuracy score of 5.12 (0.78), and an average completeness score of 1.98 (0.56). 
The answers provided by the LLMs were "easy" to read, with an average FRES score 
of 80.46 (7.19). The average level of detail score was 79.38 (8.14). Overall, 
DeepSeek-R1 and ChatGPT-4.0 performed the best, with similar scores in accuracy, 
completeness, readability, and level of detail. Health advice disclaimers were 
included in 83%-94% of the responses.
CONCLUSION: Using LLMs as tools for education and consultation in rheumatic 
diseases, particularly CTD shows promising potential, but the results are 
varied, indicating room for further improvement. DeepSeek-R1 and ChatGPT-4.0 
scored similarly, performing the best in terms of accuracy, completeness, 
readability, and level of detail. The study results provide a basis for 
decision-making regarding the integration of the Chinese AI model DeepSeek-R1 
into global patient education and support systems.
LIMITATION & FUTURE DIRECTION: This study did not establish a mechanism to 
assess the dynamic updating ability of LLMs, and the rapid evolution of medical 
knowledge could affect the accuracy of model outputs. Furthermore, this study is 
limited to single-turn questions and does not simulate the progressive dialogue 
in real clinical scenarios. Future research should focus on further improving 
the accuracy, completeness, and readability of LLMs to better serve clinical 
practice and patient education.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106172
PMID: 41252762 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


103. Int J Med Inform. 2025 Dec;204:106053. doi: 10.1016/j.ijmedinf.2025.106053. Epub 
2025 Jul 19.

LLM-powered breast cancer staging from PET/CT reports: a comparative performance 
study.

Spitzl D(1), Mergen M(2), Braren R(3), Endrös L(2), Eiber M(4), Steinhelfer 
L(5).

Author information:
(1)Department of Diagnostic and Interventional Radiology, TUM University 
Hospital, School of Medicine, Technical University of Munich, Munich, Germany. 
Electronic address: danieljan.spitzl@mri.tum.de.
(2)Department of Diagnostic and Interventional Radiology, TUM University 
Hospital, School of Medicine, Technical University of Munich, Munich, Germany.
(3)Department of Diagnostic and Interventional Radiology, TUM University 
Hospital, School of Medicine, Technical University of Munich, Munich, Germany; 
German Cancer Consortium (DKTK), Partner-site Munich, a Partnership between DKFZ 
and Klinikum rechts der Isar, Munich, Germany; Bavarian Cancer Research Center 
(BZKF), Munich, Germany.
(4)Department of Nuclear Medicine, TUM University Hospital, School of Medicine 
and Health, Technical University Munich, Munich, Germany; German Cancer 
Consortium (DKTK), Partner-site Munich, a Partnership between DKFZ and Klinikum 
rechts der Isar, Munich, Germany; Bavarian Cancer Research Center (BZKF), 
Munich, Germany.
(5)Department of Nuclear Medicine, TUM University Hospital, School of Medicine 
and Health, Technical University Munich, Munich, Germany; Technical University 
of Munich, School of Medicine and Health, Department of Diagnostic and 
Interventional Neuroradiology, TUM University Hospital, Munich, Germany.

PURPOSE: Imaging reports are crucial in breast cancer management, with the 
tumor-node-metastasis (TNM) classification serving as a widely used model for 
assessing disease severity, guiding treatment decisions, and predicting patient 
outcomes. Large language models (LLMs) offer a potential solution by extracting 
standardized UICC TNM classifications and the corresponding UICC stage directly 
from existing PET/CT reports. This approach holds promise to enhance staging 
accuracy, streamline multidisciplinary discussions, and improve patient 
outcomes.
METHODS: Here, we evaluated four LLMs-ChatGPT-4o, DeepSeek V3, Claude 3.5 
Sonnet, and Gemini 2.0 Flash-for their capacity to determine TNM staging based 
on UICC/AJCC breast cancer guidelines. A total of 111 fictitious PET/CT reports 
were analyzed, and each model's outputs were measured against expert-generated 
TNM classifications and stage categorizations.
RESULTS: Among the tested models, Claude 3.5 Sonnet demonstrated superior F1 
scores of 0.95%, 0.95%, 1.00% and 0.92% for T, N, M classification and UICC 
stage classification, respectively.
CONCLUSIONS: These findings underscore the ability of advanced natural language 
processing (NLP) technologies to support reliable cancer staging, potentially 
aiding clinicians. Despite the encouraging performance, prospective clinical 
trials and validation across diverse practice settings remain critical to 
confirming these preliminary outcomes. Nonetheless, this study highlights the 
promise of LLM-based systems in reinforcing the accuracy of oncologic workflows 
and lays the groundwork for broader adoption of AI-driven tools in breast cancer 
management.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106053
PMID: 40706196 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


104. Laryngoscope. 2025 Sep;135(9):3049-3063. doi: 10.1002/lary.32198. Epub 2025 May 
1.

Applications of Natural Language Processing in Otolaryngology: A Scoping Review.

Banyi N(1), Ma B(2), Amanian A(3), Bur A(4), Abdalkhani A(3).

Author information:
(1)The University of British Columbia, Faculty of Medicine, Vancouver, Canada.
(2)Department of Cellular & Physiological Sciences, University of British 
Columbia, Vancouver, Canada.
(3)Division of Otolaryngology-Head and Neck Surgery, Department of Surgery, 
University of British Columbia, Vancouver, Canada.
(4)Department of Otolaryngology-Head and Neck Surgery, University of Kansas 
Medical Centre, Kansas City, Kansas, USA.

OBJECTIVE: To review the current literature on the applications of natural 
language processing (NLP) within the field of otolaryngology.
DATA SOURCES: MEDLINE, EMBASE, SCOPUS, Cochrane Library, Web of Science, and 
CINAHL.
METHODS: The preferred reporting Items for systematic reviews and meta-analyzes 
extension for scoping reviews checklist was followed. Databases were searched 
from the date of inception up to Dec 26, 2023. Original articles on the 
application of language-based models to otolaryngology patient care and 
research, regardless of publication date, were included. The studies were 
classified under the 2011 Oxford CEBM levels of evidence.
RESULTS: One-hundred sixty-six papers with a median publication year of 2024 
(range 1982, 2024) were included. Sixty-one percent (102/166) of studies used 
ChatGPT and were published in 2023 or 2024. Sixty studies used NLP for clinical 
education and decision support, 42 for patient education, 14 for electronic 
medical record improvement, 5 for triaging, 4 for trainee education, 4 for 
patient monitoring, 3 for telemedicine, and 1 for medical translation. For 
research, 37 studies used NLP for extraction, classification, or analysis of 
data, 17 for thematic analysis, 5 for evaluating scientific reporting, and 4 for 
manuscript preparation.
CONCLUSION: The role of NLP in otolaryngology is evolving, with ChatGPT passing 
OHNS board simulations, though its clinical application requires improvement. 
NLP shows potential in patient education and post-treatment monitoring. NLP is 
effective at extracting data from unstructured or large data sets. There is 
limited research on NLP in trainee education and administrative tasks. 
Guidelines for NLP use in research are critical.

© 2025 The Author(s). The Laryngoscope published by Wiley Periodicals LLC on 
behalf of The American Laryngological, Rhinological and Otological Society, Inc.

DOI: 10.1002/lary.32198
PMCID: PMC12368917
PMID: 40309961 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


105. BMC Nurs. 2025 Nov 27;24(1):1515. doi: 10.1186/s12912-025-04171-w.

Readability, reliability, and quality of nursing care plan texts generated by 
ChatGPT.

Gokalp MG(1)(2), Yucel SC(3), Cakir Z(4), Kargi R(5).

Author information:
(1)Department of Nursing, Faculty of Health Sciences, Amasya University, Amasya, 
05100, Turkey. mucahide.gokcen@amasya.edu.tr.
(2)Department of Fundamentals of Nursing, Faculty of Health Sciences, Amasya 
University, Amasya, 05100, Turkey. mucahide.gokcen@amasya.edu.tr.
(3)Department of Fundamentals of Nursing, Faculty of Nursing, Ege University, 
Izmir, 35100, Turkey.
(4)Merzifon Kara Mustafa Paşa State Hospıtal, Amasya, 35100, Turkey.
(5)Sabuncuoğlu Şerefeddin Traınıng and Research Hospıtal, Amasya, 35100, Turkey.

BACKGROUND: Nursing care plans require clinical reasoning, prioritization, and 
patient-centered decision-making, which distinguishes them from more general 
AI-generated educational texts. As large language models such as ChatGPT are 
increasingly used to support nursing education and care planning, it is 
essential to evaluate the readability, reliability, and quality of the nursing 
care plans they produce.
PURPOSE: This study aims to evaluate the readability, reliability, and quality 
of nursing care plan texts generated by ChatGPT.
METHODS: The study sample consisted of 50 texts generated by ChatGPT (version 
4.0) based on selected nursing diagnoses from NANDA 2021–2023. These texts were 
evaluated using a descriptive criteria form, the DISCERN tool, and readability 
indices including the Flesch Reading Ease Score (FRES), Simple Measure of 
Gobbledygook (SMOG), Gunning Fog Index, and Flesch-Kincaid Grade Level (FKGL).
RESULTS: The analysis demonstrated that the nursing care plans generated by 
ChatGPT showed a moderate level of quality and reliability. However, the 
readability levels were generally higher than what is desirable for clinical and 
educational use, indicating that the texts may be difficult for some users to 
understand without adaptation. The findings also suggest that the presence of 
verifiable references contributes positively to the overall quality and 
reliability of the generated care plans.
CONCLUSION: Evaluating the readability, reliability, and quality of AI-generated 
nursing care plans is essential for ensuring their safe and meaningful use in 
nursing education and clinical practice. These findings highlight the importance 
of guiding and refining AI-supported care planning to better align with 
professional standards and patient-centered care needs.

DOI: 10.1186/s12912-025-04171-w
PMCID: PMC12751850
PMID: 41310693

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The version of artificial intelligence used in this study was an 
openly accessible “free research preview,” and therefore did not require ethics 
committee approval or institutional permission. Owing to the accessible and 
verifiable nature of the data, the procedures for using AI—such as selecting, 
randomizing, and listing the diagnoses—were clearly described in the study. The 
research was conducted in accordance with the principles of the Declaration of 
Helsinki. Human ethics and consent to participate: Not applicable. Consent for 
publication: Not applicable. Competing interests: The authors declare no 
competing interests.


106. JAMA Netw Open. 2025 Aug 1;8(8):e2530220. doi: 
10.1001/jamanetworkopen.2025.30220.

Reporting Guideline for Chatbot Health Advice Studies: The CHART Statement.

CHART Collaborative; Huo B(1), Collins GS(2)(3), Chartash D(4), Thirunavukarasu 
AJ(5), Flanagin A(6), Iorio A(7), Cacciamani G(8)(9), Chen X(10)(11), Liu N(12), 
Mathur P(13), Chan AW(14), Laine C(15)(16), Pacella D(17), Berkwits M(18), 
Antoniou SA(19), Camaradou JC(20), Canfield C(21), Mittelman M(22), Feeney 
T(23)(24), Loder EW(23)(25), Agha R(26)(27), Saha A(28), Mayol J(29), Sunjaya 
A(30), Harvey H(31), Ng JY(32), McKechnie T(1), Lee Y(1)(33), Verma N(34), 
Stiglic G(35), McCradden M(36), Ramji K(37), Boudreau V(1), Ortenzi M(38), 
Meerpohl JJ(39)(40), Vandvik PO(40)(41), Agoritsas T(7)(41)(42), Samuel D(43), 
Frankish H(44), Anderson M(45)(46), Yao X(28), Loeb S(47), Lokker C(7), Liu 
X(48), Guallar E(49), Guyatt GH(7)(41).

Author information:
(1)Division of General Surgery, Department of Surgery, McMaster University, 
Hamilton, Ontario, Canada.
(2)UK EQUATOR Centre, University of Oxford, Oxford, United Kingdom.
(3)Centre for Statistics in Medicine, Nuffield Department of Orthopaedics, 
Rheumatology and Musculoskeletal Sciences, Botnar Research Centre, University of 
Oxford, Oxford, United Kingdom.
(4)Department of Biomedical Informatics and Data Science, Yale University School 
of Medicine, New Haven, Connecticut.
(5)Nuffield Department of Clinical Neurosciences, Medical Sciences Division, 
University of Oxford, Oxford, United Kingdom.
(6)JAMA and JAMA Network, American Medical Association, Chicago, Illinois.
(7)Department of Health Research Methods, Evidence, and Impact; Department of 
Medicine; McMaster University, Hamilton, Ontario, Canada.
(8)USC Institute of Urology and Catherine and Joseph Aresty Department of 
Urology, Keck School of Medicine, University of Southern California, Los 
Angeles.
(9)Artificial Intelligence Center at USC Urology, USC Institute of Urology, 
University of Southern California, Los Angeles.
(10)Sports Medicine Center, West China Hospital, Sichuan University, Chengdu, 
China.
(11)Department of Orthopedics and Orthopedic Research Institute, West China 
Hospital, Sichuan University, Chengdu, China.
(12)Duke-NUS Medical School, National University of Singapore, Singapore, 
Singapore.
(13)Cleveland Clinic, Case Western Reserve University, Cleveland, Ohio.
(14)Department of Medicine, Women's College Research Institute, University of 
Toronto, Toronto, Ontario, Canada.
(15)Annals of Internal Medicine, American College of Physicians, Philadelphia, 
Pennsylvania.
(16)American College of Physicians, Philadelphia, Pennsylvania.
(17)Department of Public Health, University of Naples Federico II, Naples, 
Italy.
(18)Director, Office of Science Dissemination, Office of Science, Centers for 
Disease Control and Prevention, Atlanta, Georgia.
(19)Department of General Surgery, Papageorgiou General Hospital, Thessaloniki, 
Greece.
(20)British Psychological Society, University of Plymouth, Plymouth, United 
Kingdom.
(21)Innovation Support Unit, Department of Family Practice, University of 
British Columbia, Vancouver, British Columbia, Canada.
(22)Patient SME, Independent Cybersecurity Professional, Philadelphia, 
Pennsylvania.
(23)The BMJ, London, United Kingdom.
(24)Department of Epidemiology, Gillings School of Global Public Health, 
University of North Carolina at Chapel Hill, Chapel Hill, North Carolina.
(25)Department of Neurology, Brigham and Women's Hospital, Boston, 
Massachusetts.
(26)International Journal of Surgery, London, United Kingdom.
(27)Eworkflow Ltd, London, United Kingdom.
(28)Department of Oncology, McMaster University, Hamilton, Ontario, Canada.
(29)Hospital Clinico San Carlos, Instituto de Investigación Sanitaria San 
Carlos, Facultad de Medicina Universidad Complutense de Madrid, Spain.
(30)The George Institute for Global Health; Tyree Institute of Health 
Engineering, UNSW Engineering; School of Population Health, University of New 
South Wales Medicine and Health, Sydney, Australia.
(31)Hardian Health, Haywards Heath, United Kingdom.
(32)Centre for Journalology, Ottawa Hospital Research Institute, Ottawa, Canada.
(33)Digestive Diseases Institute, Cleveland Clinic, Cleveland, Ohio.
(34)Postgraduate Institute of Medical Education and Research, Chandigarh, India.
(35)University of Maribor, Maribor, Slovenia.
(36)Australian Institute for Machine Learning, Adelaide, South Australia, 
Australia.
(37)Phelix AI, Hamilton, Ontario, Canada.
(38)Università Politecnica delle Marche, Clinica di Chirurgia Generale e 
d'Urgenza, Ancona, Italy.
(39)Institute for Evidence in Medicine, Medical Center and Faculty of Medicine, 
University of Freiburg, Germany.
(40)Cochrane Germany, Cochrane Germany Foundation, Freiburg, Germany.
(41)MAGIC Evidence Ecosystem Foundation, Oslo, Norway.
(42)University Hospitals of Geneva, Geneva, Switzerland.
(43)The Lancet Digital Health, London, United Kingdom.
(44)The Lancet, London, United Kingdom.
(45)NIHR Clinical Lecturer, Health Organisation, Policy, Economics (HOPE), 
Centre for Primary Care and Health Services Research, The University of 
Manchester.
(46)Senior Visiting Fellow, LSE Health, London School of Economics and Political 
Science, London, United Kingdom.
(47)New York University Langone Health.
(48)College of Medicine and Health, University of Birmingham, United Kingdom.
(49)School of Public Health, New York University.

IMPORTANCE: The rise in chatbot health advice (CHA) studies is accompanied by 
heterogeneity in reporting standards, impacting their interpretability.
OBJECTIVE: To provide reporting recommendations for studies evaluating the 
performance of generative artificial intelligence (AI)-driven chatbots when 
summarizing clinical evidence and providing health advice.
DESIGN, SETTING, AND PARTICIPANTS: CHART was developed in several phases after 
performing a comprehensive systematic review to identify variation in the 
conduct, reporting, and methodology in CHA studies. Findings from the review 
were used to develop a draft checklist that was revised through an 
international, multidisciplinary modified asynchronous Delphi consensus process 
of 531 stakeholders, 3 synchronous panel consensus meetings of 48 stakeholders, 
and subsequent pilot testing of the checklist.
RESULTS: CHART includes 12 items and 39 subitems to promote transparent and 
comprehensive reporting of CHA studies. These include title (subitem 1a), 
abstract or summary (subitem 1b), background (subitems 2ab), model identifiers 
(subitem 3ab), model details (subitems 4abc), prompt engineering (subitems 5ab), 
query strategy (subitems 6abcd), performance evaluation (subitems 7ab), sample 
size (subitem 8), data analysis (subitem 9a), results (subitems 10abc), 
discussion (subitems 11abc), disclosures (subitem 12a), funding (subitem 12b), 
ethics (subitem 12c), protocol (subitem 12d), and data availability (subitem 
12e).
CONCLUSIONS AND RELEVANCE: The CHART checklist and corresponding methodological 
diagram were designed to support key stakeholders including clinicians, 
researchers, editors, peer reviewers, and readers in reporting, understanding, 
and interpreting the findings of CHA studies.

DOI: 10.1001/jamanetworkopen.2025.30220
PMID: 40747871 [Indexed for MEDLINE]


107. NPJ Digit Med. 2025 Jul 21;8(1):466. doi: 10.1038/s41746-025-01845-2.

Evaluating the role of large language models in traditional Chinese medicine 
diagnosis and treatment recommendations.

Liu Y(1), Yuan Y(2), Yan K(1), Li Y(1), Sacca V(1), Hodges S(1), Cannistra M(1), 
Jeong P(1), Wu J(1), Kong J(3).

Author information:
(1)Department of Psychiatry, Massachusetts General Hospital and Harvard Medical 
School, Charlestown, MA, USA.
(2)Beijing University of Chinese Medicine, Beijing, China.
(3)Department of Psychiatry, Massachusetts General Hospital and Harvard Medical 
School, Charlestown, MA, USA. jkong2@mgh.harvard.edu.

Digital health technologies hold significant potential for reducing global 
healthcare disparities. Large language models (LLMs) offer new opportunities to 
enhance access to culturally specific healthcare, including traditional Chinese 
medicine (TCM). This study evaluated the diagnostic and treatment performance of 
seven publicly available LLMs using a real-world acupuncture case, comparing 
their outputs with three professional acupuncturists across five domains: 
Western diagnosis, TCM diagnosis, acupoint selection, needling technique, and 
herbal medicine. Twenty-eight expert evaluators from China, South Korea, and the 
United States assessed the responses using a multilingual survey. LLMs performed 
comparably to acupuncturists in Western diagnosis and showed variable 
performance in TCM-specific tasks. GPT-4o, Qwen 2.5 Max, and Doubao 1.5 Pro 
demonstrated the highest alignment with expert evaluations, particularly in TCM 
diagnosis and acupoint selection. These findings highlight the potential of 
general-purpose LLMs to support culturally grounded medical decision-making and 
reduce access barriers in TCM care systems.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01845-2
PMCID: PMC12279949
PMID: 40691277

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


108. JMIR Med Inform. 2025 Apr 25;13:e64963. doi: 10.2196/64963.

Comparing Diagnostic Accuracy of Clinical Professionals and Large Language 
Models: Systematic Review and Meta-Analysis.

Shan G(1), Chen X(1), Wang C(1), Liu L(2), Gu Y(3), Jiang H(4), Shi T(5).

Author information:
(1)Nanjing Drum Tower Hospital Clinical College of Nanjing University of Chinese 
Medicine, Nanjing, China.
(2)Jiangsu Province Hospital of Chinese Medicine, Affiliated Hospital of Nanjing 
University of Chinese Medicine, Nanjing, China.
(3)Department of Emergency, Nanjing Drum Tower Hospital, Nanjing, China.
(4)Department of Nursing, Nanjing Drum Tower Hospital, Nanjing, China.
(5)Department of Quality Management, Nanjing Drum Tower Hospital, Affiliated 
Hospital of Medical School, Nanjing University, 321 Zhongshan Road, Gulou 
District, Nanjing, 210008, China, 86 1-391-299-6998.

BACKGROUND: With the rapid development of artificial intelligence (AI) 
technology, especially generative AI, large language models (LLMs) have shown 
great potential in the medical field. Through massive medical data training, it 
can understand complex medical texts and can quickly analyze medical records and 
provide health counseling and diagnostic advice directly, especially in rare 
diseases. However, no study has yet compared and extensively discussed the 
diagnostic performance of LLMs with that of physicians.
OBJECTIVE: This study systematically reviewed the accuracy of LLMs in clinical 
diagnosis and provided reference for further clinical application.
METHODS: We conducted searches in CNKI (China National Knowledge 
Infrastructure), VIP Database, SinoMed, PubMed, Web of Science, Embase, and 
CINAHL (Cumulative Index to Nursing and Allied Health Literature) from January 
1, 2017, to the present. A total of 2 reviewers independently screened the 
literature and extracted relevant information. The risk of bias was assessed 
using the Prediction Model Risk of Bias Assessment Tool (PROBAST), which 
evaluates both the risk of bias and the applicability of included studies.
RESULTS: A total of 30 studies involving 19 LLMs and a total of 4762 cases were 
included. The quality assessment indicated a high risk of bias in the majority 
of studies, primary cause is known case diagnosis. For the optimal model, the 
accuracy of the primary diagnosis ranged from 25% to 97.8%, while the triage 
accuracy ranged from 66.5% to 98%.
CONCLUSIONS: LLMs have demonstrated considerable diagnostic capabilities and 
significant potential for application across various clinical cases. Although 
their accuracy still falls short of that of clinical professionals, if used 
cautiously, they have the potential to become one of the best intelligent 
assistants in the field of human health care.

© Guxue Shan, Xiaonan Chen, Chen Wang, Li Liu, Yuanjing Gu, Huiping Jiang, 
Tingqi Shi. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org).

DOI: 10.2196/64963
PMCID: PMC12047852
PMID: 40279517 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


109. Digit Health. 2025 Nov 10;11:20552076251393291. doi: 10.1177/20552076251393291. 
eCollection 2025 Jan-Dec.

Effectiveness of ChatGPT to provide esophageal cancer information: A 
SERVQUAL-based analysis.

Cheng W(1)(2), Liu Y(3), Zhou C(4), Xie C(1).

Author information:
(1)Department of Gastroenterology, The First Affiliated Hospital of Nanchang 
University, Jiangxi, China.
(2)Queen Mary College, Nanchang University, Jiangxi, China.
(3)The First Clinical Medical College, Nanchang University, Jiangxi, China.
(4)West China School of Public Health and West China Forth Hospital, Sichuan 
University, Sichuan, China.

BACKGROUND: ChatGPT and other AI-driven language models are increasingly used in 
healthcare for disseminating medical information. However, their performance in 
providing accurate and empathetic responses to patients with specific diseases 
remains uncertain.
OBJECTIVE: This study aimed to evaluate the effectiveness and reliability of 
ChatGPT in providing esophageal cancer-related information using the SERVQUAL 
framework, focusing on five dimensions: Tangibles, Reliability, Responsiveness, 
Assurance, and Empathy.
METHODS: Ten representative questions on esophageal cancer were selected through 
search engine analysis and specialist consultation. ChatGPT generated responses, 
which were evaluated by 48 gastroenterologists using a 5-point Likert scale 
aligned with the SERVQUAL model. Statistical analysis was conducted using R 
4.4.0 to compare responses between specialist and non-specialist physicians.
RESULTS: ChatGPT performed well in providing structured, logical, and generally 
informative responses, particularly in the prevention domain. However, 
limitations were identified in its responsiveness and empathy. Significant 
differences were observed between specialists and non-specialists in evaluating 
certain answers, especially regarding reliability and cutting-edge knowledge. 
ChatGPT often failed to reflect the latest clinical guidelines or regional 
risk-specific recommendations.
CONCLUSION: While ChatGPT shows potential in patient education for esophageal 
cancer, its current outputs lack clinical specificity and up-to-date medical 
insight. AI tools should be continuously improved with dynamic data integration 
and specialist supervision to ensure reliability and relevance in real-world 
healthcare scenarios.

© The Author(s) 2025.

DOI: 10.1177/20552076251393291
PMCID: PMC12602916
PMID: 41229931

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


110. Orthop J Sports Med. 2025 Apr 30;13(4):23259671251332596. doi: 
10.1177/23259671251332596. eCollection 2025 Apr.

Are Large Language Model-Based Chatbots Effective in Providing Reliable Medical 
Advice for Achilles Tendinopathy? An International Multispecialist Evaluation.

Liang Z(1), Wang M(1), Abdelatif NMN(2), Arunakul M(3), Borbon CAV(4), Chong 
KW(5), Chow MW(1), Hua Y(6), Oji D(7), Ahumada X(8), Siu KM(9), Tan KJ(10), 
Tanaka Y(11), Taniguchi A(11), Yung PS(1), Ling SK(1)(12).

Author information:
(1)Department of Orthopaedics and Traumatology, The Chinese University of Hong 
Kong, Hong Kong, SAR, China.
(2)DrNasef OrthoClinic for Foot and Ankle Orthopedic Disorders, Cairo, Egypt.
(3)Department of Orthopedic Surgery, Faculty of Medicine, Thammasat University, 
Pathumthani, Thailand.
(4)Makati Medical Center, Makati, Philippines.
(5)Duke-NUS Graduate Medical School, Singapore.
(6)Department of Sports Medicine, Huashan Hospital, Fudan University, Shanghai, 
China.
(7)Foot and Ankle Surgery, Department of Orthopaedic Surgery, Stanford 
University School of Medicine, Redwood City, California, USA.
(8)Hospital Clínico San Borja Arriarán, Santiago, Chile.
(9)Department of Orthopaedics and Traumatology, Princess Margaret Hospital, Hong 
Kong, China.
(10)OrthoSports Clinic for Orthopedic Surgery and Sports Medicine, Mt. Elizabeth 
Novena Specialist Centre, Singapore.
(11)Department of Orthopaedic Surgery, Nara Medical University, Kashihara, Nara, 
Japan.
(12)Investigation performed at The Chinese University of Hong Kong, Hong Kong.

BACKGROUND: Large language model (LLM)-based chatbots have shown potential in 
providing health information and patient education. However, the reliability of 
these chatbots in offering medical advice for specific conditions like Achilles 
tendinopathy remains uncertain. Mixed outcomes in the field of orthopaedics 
highlight the need for further examination of these chatbots' reliability.
HYPOTHESIS: Three leading LLM-based chatbots can provide accurate and complete 
responses to inquiries related to Achilles tendinopathy.
STUDY DESIGN: Cross-sectional study.
METHODS: Eighteen questions derived from the Dutch clinical guideline on 
Achilles tendinopathy were posed to 3 leading LLM-based chatbots: ChatGPT 4.0, 
Claude 2, and Gemini. The responses were incorporated into an online survey 
assessed by orthopaedic surgeons specializing in Achilles tendinopathy. 
Responses were evaluated using a 4-point scoring system, where 1 indicates 
unsatisfactory and 4 indicates excellent. The total scores for the 18 responses 
were aggregated for each rater and compared across the chatbots. The intraclass 
correlation coefficient was calculated to assess consistency among the raters' 
evaluations.
RESULTS: Thirteen specialists from 9 diverse countries and regions participated. 
Analysis showed no significant difference in the mean total scores among the 
chatbots: ChatGPT (59.7 ± 5.5), Claude 2 (53.4 ± 9.7), and Gemini (53.6 ± 8.4). 
The proportions of unsatisfactory responses (score 1) were low and comparable 
across chatbots: 0.9% for ChatGPT 4.0, 3.4% for Claude 2, and 3.4% for Gemini. 
In terms of excellent responses (score 4), ChatGPT 4.0 outperformed the others, 
with 43.6% of the responses rated as excellent, significantly higher than Claude 
2 at 27.4% and Gemini at 25.2% (P < .001 for both comparisons). Intraclass 
correlation coefficients indicated poor reliability for ChatGPT 4.0 (0.420) and 
moderate reliability for Claude 2 (0.522) and Gemini (0.575).
CONCLUSION: While LLM-based chatbots such as ChatGPT 4.0 can deliver 
high-quality responses to queries regarding Achilles tendinopathy, the 
inconsistency among specialist evaluations and the absence of standardized 
assessment criteria significantly challenge our ability to draw definitive 
conclusions. These issues underscore the need for a cautious and standardized 
approach when considering the integration of LLM-based chatbots into clinical 
settings.

© The Author(s) 2025.

DOI: 10.1177/23259671251332596
PMCID: PMC12046157
PMID: 40322749

Conflict of interest statement: The authors declared that they have no conflicts 
of interest in the authorship and publication of this contribution. AOSSM checks 
author disclosures against the Open Payments Database (OPD). AOSSM has not 
conducted an independent investigation on the OPD and disclaims any liability or 
responsibility relating thereto. Ethical approval was not sought for the present 
study.


111. World J Urol. 2025 Oct 29;43(1):641. doi: 10.1007/s00345-025-06019-z.

Large language model chatbots for patient education in kidney stones: a scoping 
review.

Goudrar R(1), Zekraoui O(1), Moussa I(1), Nguyen DD(2)(3), Bouhadana D(4), Li 
T(2)(5), Gauhar V(6)(7), Yuen SKK(8), Bhojani N(9).

Author information:
(1)Faculty of Medicine, University of Montreal, Montreal, QC, Canada.
(2)Division of Urology, University of Toronto, Toronto, ON, Canada.
(3)Institute of Health Policy, Management and Evaluation, Dalla Lana School of 
Public Health, University of Toronto, Toronto, ON, Canada.
(4)Division of Urology, McGill University, Montreal, QC, Canada.
(5)Institute of Medical Science, Temerty Faculty of Medicine, University of 
Toronto, Toronto, Canada.
(6)Department of Urology, Ng Teng Fong Hospital, Singapore, Singapore.
(7)Asian Institute of nephrourology (AINU), Hyderabad, India.
(8)Department of Surgery, SH Ho Urology Centre, The Chinese University of Hong 
Kong, Shatin, Hong Kong, China.
(9)Division of Urology, University of Montreal Hospital Center, 900, rue 
Saint-Denis, pavillon R, Montréal, QC, H2X 0A9, Canada. 
naeem.bhojani.1@umontreal.ca.

PURPOSE: In 2024, 17% of adults reported using an artificial intelligence (AI) 
chatbot at least once a month as a source of health information, rising to 25% 
among those under 30. We aim to conduct a scoping review of the existing 
literature assessing the performance of large language model (LLM) chatbots for 
patient education in kidney stone disease (KSD).
METHODS: The Joanna Briggs Institute methodology was followed. Ovid MEDLINE, 
Embase, CENTRAL, Web of Science, CINAHL, and Google Scholar were searched for 
studies in all languages, from 2015 up to February 16th 2025, evaluating LLM 
chatbots to create educational content on KSD. Two independent reviewers 
completed screening, full-text review, and data extraction, with conflicts 
resolved by a third reviewer.
RESULTS: Of the 281 search results, 17 were included. Five of six studies 
assessing readability found that LLM responses exceeded the recommended 6th-8th 
grade reading level, though effective prompting can help meet this target. Six 
out of eight studies reported adequate to very good accuracy performance, with 
two showing comparable performance to traditional information sources. 
Understandability and actionability performance was poor. Quality performance 
was variable across seven studies. Three studies assessed patients' perception, 
revealing a mixed but generally favorable experience. Two studies noted notable 
deviations from established clinical guidelines.
CONCLUSION: LLM chatbots show potential for KSD patient education and physician 
workload reduction, but currently have limitations in readability, guideline 
adherence, understandability, and actionability. This could be mitigated by 
prompting and the development of urology-specific tools trained on validated 
content and evaluated with patient involvement.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00345-025-06019-z
PMID: 41160267 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Confict of interest: The authors 
declare no competing interests.


112. NPJ Digit Med. 2025 Jul 17;8(1):450. doi: 10.1038/s41746-025-01824-7.

Large language model integrations in cancer decision-making: a systematic review 
and meta-analysis.

Hao Y(1)(2)(3), Qiu Z(4), Holmes J(5), Löckenhoff CE(4), Liu W(6), Ghassemi 
M(7), Kalantari S(4).

Author information:
(1)Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, USA. 
yh727@cornell.edu.
(2)Cornell University, Ithaca, NY, USA. yh727@cornell.edu.
(3)Massachusetts Institute of Technology, Cambridge, MA, USA. yh727@cornell.edu.
(4)Cornell University, Ithaca, NY, USA.
(5)Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, USA.
(6)Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, USA. 
liu.wei@mayo.edu.
(7)Massachusetts Institute of Technology, Cambridge, MA, USA.

Large Language Models (LLMs) are increasingly used to support cancer patients 
and clinicians in decision-making. This systematic review investigates how LLMs 
are integrated into oncology and evaluated by researchers. We conducted a 
comprehensive search across PubMed, Web of Science, Scopus, and the ACM Digital 
Library through May 2024, identifying 56 studies covering 15 cancer types. The 
meta-analysis results suggested that LLMs were commonly used to summarize, 
translate, and communicate clinical information, but performance varied: the 
average overall accuracy was 76.2%, with average diagnostic accuracy lower at 
67.4%, revealing gaps in the clinical readiness of this technology. Most 
evaluations relied heavily on quantitative datasets and automated methods 
without human graders, emphasizing "accuracy" and "appropriateness" while rarely 
addressing "safety", "harm", or "clarity". Current limitations for LLMs in 
cancer decision-making, such as limited domain knowledge and dependence on human 
oversight, demonstrate the need for open datasets and standardized evaluations 
to improve reliability.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01824-7
PMCID: PMC12271406
PMID: 40676129

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


113. JAMA Ophthalmol. 2025 May 1;143(5):410-419. doi: 
10.1001/jamaophthalmol.2025.0351.

Evaluation of AI Summaries on Interdisciplinary Understanding of Ophthalmology 
Notes.

Tailor PD(1)(2), D'Souza HS(1), Castillejo Becerra CM(1), Dahl HM(1), Patel 
NR(3), Kaplan TM(1), Kohli D(1), Bothun ED(1), Mohney BG(1), Tooley AA(1), 
Baratz KH(1), Iezzi R(1), Barkmeier AJ(1), Bakri SJ(1), Roddy GW(1), Hodge D(4), 
Sit AJ(1), Starr MR(1), Chen JJ(1)(5).

Author information:
(1)Department of Ophthalmology, Mayo Clinic, Rochester, Minnesota.
(2)Jules Stein Eye Institute and Department of Ophthalmology, David Geffen 
School of Medicine at UCLA, University of California, Los Angeles, Los Angeles, 
California.
(3)Department of Allergy & Immunology, ICAHN School of Medicine at Mount Sinai, 
New York, New York.
(4)Department of Quantitative Health Sciences, Mayo Clinic, Jacksonville, 
Florida.
(5)Department of Neurology, Mayo Clinic, Rochester, Minnesota.

Comment on
    JAMA Ophthalmol. 2025 May 1;143(5):419-420. doi: 
10.1001/jamaophthalmol.2025.0352.

IMPORTANCE: Specialized ophthalmology terminology limits comprehension for 
nonophthalmology clinicians and professionals, hindering interdisciplinary 
communication and patient care. The clinical implementation of large language 
models (LLMs) into practice has to date been relatively unexplored.
OBJECTIVE: To evaluate LLM-generated plain language summaries (PLSs) integrated 
into standard ophthalmology notes (SONs) in improving diagnostic understanding, 
satisfaction, and clarity.
DESIGN, SETTING, AND PARTICIPANTS: Randomized quality improvement study 
conducted from February 1, 2024, to May 31, 2024, including data from inpatient 
and outpatient encounters in a single tertiary academic center. Participants 
were nonophthalmology clinicians and professionals and ophthalmologists. The 
single inclusion criterion was any encounter note generated by an 
ophthalmologist during the study dates. Exclusion criteria were (1) lack of 
established nonophthalmology clinicians and professionals for outpatient 
encounters and (2) procedure-only patient encounters.
INTERVENTION: Addition of LLM-generated plain language summaries to 
ophthalmology notes.
MAIN OUTCOMES AND MEASURES: The primary outcome was survey responses from 
nonophthalmology clinicians and professionals assessing understanding, 
satisfaction, and clarity of ophthalmology notes. Secondary outcomes were survey 
responses from ophthalmologists evaluating PLS in terms of clinical workflow and 
accuracy, objective measures of semantic quality, and safety analysis.
RESULTS: A total of 362 (85%) nonophthalmology clinicians and professionals 
(33.0% response rate) preferred the PLS to SON. Demographic data on age, race 
and ethnicity, and sex were not collected. Nonophthalmology clinicians and 
professionals reported enhanced diagnostic understanding (percentage point 
increase, 9.0; 95% CI, 0.3-18.2; P = .01), increased note detail satisfaction 
(percentage point increase, 21.5; 95% CI, 11.4-31.5; P < .001), and improved 
explanation clarity (percentage point increase, 23.0; 95% CI, 12.0-33.1; 
P < .001) for notes containing a PLS. The addition of a PLS was associated with 
reduced comprehension gaps between clinicians who were comfortable and 
uncomfortable with ophthalmology terminology (from 26.1% [95% CI, 13.7%-38.6%; 
P < .001] to 14.4% [95% CI, 4.3%-24.6%; P > .06]). PLS semantic analysis found 
high meaning preservation (bidirectional encoder representations from 
transformers score mean F1 score: 0.85) with greater readability than SONs 
(Flesch Reading Ease: 51.8 vs 43.6; Flesch-Kincaid Grade Level: 10.7 vs 11.9). 
Ophthalmologists (n = 489; 84% response rate) reported high PLS accuracy (90% 
[320 of 355] a great deal) with minimal review time burden (94.9% [464 of 489] 
≤1 minute). PLS error rate on ophthalmologist review was 26% (126 of 489). A 
total of 83.9% (104 of 126) of errors were deemed low risk for harm and none had 
a risk of severe harm or death.
CONCLUSIONS AND RELEVANCE: In this study, use of LLM-generated PLSs was 
associated with enhanced comprehension and satisfaction among nonophthalmology 
clinicians and professionals, which might aid interdisciplinary communication. 
Careful implementation and safety monitoring are recommended for clinical 
integration given the persistence of errors despite physician review.

DOI: 10.1001/jamaophthalmol.2025.0351
PMCID: PMC11969348
PMID: 40178837 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest Disclosures: Dr Iezzi 
reported receiving personal fees from Johnson and Johnson as consultant outside 
the submitted work. Dr Sit reported receiving personal fees from Globe 
Biomedical, Inc. and Injectsense Inc, grants from Nicox Ophthalmics Inc, 
personal fees from PolyActiva, Pty, Qlaris Bio Inc, and Santen Pharmaceuticals 
Asia, Pty outside the submitted work. Dr Starr reported receiving nonfinancial 
support from AbbVie, Gyroscope Therapeutics, Evolve Medical, and Alimera 
Sciences outside the submitted work. No other disclosures were reported.


114. Nurse Educ Today. 2025 Oct;153:106822. doi: 10.1016/j.nedt.2025.106822. Epub 
2025 Jul 5.

Patterns, advances, and gaps in using ChatGPT and similar technologies in 
nursing education: A PAGER scoping review.

Amankwaa I(1), Ekpor E(2), Cudjoe D(3), Kobiah E(4), Fuseini AJ(5), Diebieri 
M(6), Gyamfi S(7), Brownie S(8).

Author information:
(1)Department of Nursing, Auckland University of Technology, Auckland, New 
Zealand. Electronic address: Isaac.amankwaa@aut.ac.nz.
(2)School of Nursing and Midwifery, University of Ghana, Accra, Ghana.
(3)St. Michael Catholic Nursing and Midwifery Training College, Pramso, Ghana.
(4)Department of Nursing, Kwame Nkrumah University of Science and Technology, 
Kumasi, Ghana.
(5)Nursing Department, Health School, University of Algarve, 8000 Faro, 
Portugal.
(6)Nursing and Midwifery Training College, Kpembe, Salaga, Ghana.
(7)Faculty of Nursing, University of Windsor, Windsor, Ontario, Canada.
(8)School of Health Sciences, Swinburne University of Technology, Hawthorn, VIC, 
Australia.

BACKGROUND AND AIM: Generative AI (GenAI) can transform nursing education and 
modernise content delivery. However, the rapid integration of these tools has 
raised concerns about academic integrity and teaching quality. Previous reviews 
have either looked broadly at artificial intelligence or focused narrowly on 
single tools like ChatGPT. This scoping review uses a structured framework to 
identify patterns, advances, gaps, evidence, and recommendations for 
implementing GenAI in nursing education.
METHODS: This scoping review followed the JBI methodology and PRISMA-ScR 
guidelines. We searched PubMed, CINAHL, SCOPUS, ERIC, and grey literature 
(October to November 2024). Data synthesis utilised the PAGER framework as a 
mapping tool to organise and describe patterns, advances, gaps, evidence for 
practice, and recommendations.
RESULTS: Analysis of 107 studies revealed GenAI implementation across four key 
domains: assessment and evaluation, clinical simulation, educational content 
development, and faculty/student support. Three distinct implementation patterns 
emerged: restrictive, integrative, and hybrid approaches, with hybrid models 
demonstrating superior adoption outcomes. Technical advances showed significant 
improvement from GPT-3.5 (75.3 % accuracy) to GPT-4 (88.67 % accuracy) in 
NCLEX-style assessments, with enhanced capabilities in multilingual assessment, 
clinical scenario generation, and adaptive content creation. Major gaps included 
limited methodological rigour (29.0 % of empirical studies), inconsistent 
quality control, verification challenges, equity concerns, and inadequate 
faculty training. Geographic distribution showed North American (42.1 %) and 
Asian (29.9 %) dominance, with ChatGPT representing 83.2 % of tool 
implementations. Key recommendations include developing institutional policies, 
establishing quality verification protocols, enhancing faculty training 
programs, and addressing digital equity concerns to optimise GenAI integration 
in nursing education.
CONCLUSIONS: GenAI has transformative potential in nursing education. To realise 
its full potential and ensure responsible use, research should focus on 
developing standardised governance frameworks, empirically validating outcomes, 
developing faculty in AI literacy, and improving technical infrastructure for 
low-income contexts. Such efforts should involve international collaboration, 
highlighting the importance of the audience's role in the global healthcare 
community.

Copyright © 2025 The Author(s). Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.nedt.2025.106822
PMID: 40644844 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


115. Front Digit Health. 2025 May 15;7:1500220. doi: 10.3389/fdgth.2025.1500220. 
eCollection 2025.

Personalization variables in digital mental health interventions for depression 
and anxiety in adolescents and youth: a scoping review.

Wanniarachchi VU(1), Greenhalgh C(2), Choi A(1), Warren JR(1).

Author information:
(1)School of Computer Science, University of Auckland, Auckland, New Zealand.
(2)School of Computer Science, University of Nottingham, Nottingham, United 
Kingdom.

INTRODUCTION: The impact of personalization on user engagement and adherence in 
digital mental health interventions (DMHIs) has been widely explored. However, 
there is a lack of clarity regarding the prevalence of its application, as well 
as the dimensions and mechanisms of personalization within DMHIs for adolescents 
and youth.
METHODS: To understand how personalization has been applied in DMHIs for 
adolescents and young people, a scoping review was conducted. Empirical studies 
on DMHIs for adolescents and youth with depression and anxiety, published 
between 2013 and July 2024, were extracted from PubMed and Scopus. A total of 67 
studies were included in the review. Additionally, we expanded an existing 
personalization framework, which originally classified personalization into four 
dimensions (content, order, guidance, and communication) and four mechanisms 
(user choice, provider choice, rule-based, and machine learning), by 
incorporating non-therapeutic elements.
RESULTS: The adapted framework includes therapeutic and non-therapeutic content, 
order, guidance, therapeutic and non-therapeutic communication, interfaces 
(customization of non-therapeutic visual or interactive components), and 
interactivity (personalization of user preferences), while retaining the 
original mechanisms. Half of the interventions studied used only one 
personalization dimension (51%), and more than two-thirds used only one 
personalization mechanism. This review found that personalization of therapeutic 
content (51% of the interventions) and interfaces (25%) were favored. User 
choice was the most prevalent personalization mechanism, present in 60% of 
interventions. Additionally, machine learning mechanisms were employed in a 
substantial number of cases (30%), but there were no instances of generative 
artificial intelligence (AI) among the included studies.
DISCUSSION: The findings of the review suggest that although personalization 
elements of the interventions are reported in the articles, their impact on 
younger people's experience with DMHIs and adherence to mental health protocols 
is not thoroughly addressed. Future interventions may benefit from incorporating 
generative AI, while adhering to standard clinical research practices, to 
further personalize user experiences.

© 2025 Wanniarachchi, Greenhalgh, Choi and Warren.

DOI: 10.3389/fdgth.2025.1500220
PMCID: PMC12119569
PMID: 40444184

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


116. JMIR Med Inform. 2026 Jan 3. doi: 10.2196/78567. Online ahead of print.

Improving Clinical Decision-Making in Treating Airway Diseases with an Expert 
System Built Upon the Free AI Tool Google NotebookLM®.

Hsu CH(1), Hsu CL(2), Tsou CH(3)(4), Hsu KF(2), Yang HY(5)(6)(1).

Author information:
(1)Department of Medicine, College of Medicine, Taipei Medical University, 
Taipei, TW.
(2)Miaoli General Hospital, Ministry of Health and Welfare, Taiwan, Miaoli City, 
TW.
(3)Department of Pulmonology, Jen-Ai Branch, Taipei City Hospital, Taipei, TW.
(4)School of Medicine, College of Medicine, National Yang Ming Chiao Tung 
University, Taipei, TW.
(5)Department of Cardiology, Wan Fang Hospital, Taipei Medical University, 
No.111, Sec. 3, Xinglong Rd.,Wenshan Dist.,, Taipei, TW.
(6)Biomedical Data Center, Wan Fang Hospital, Taipei Medical University, Taipei, 
TW.

Objective: We employed the free artificial intelligence (AI) tool Google 
NotebookLM®, powered by the large language model (LLM) Gemini 2.0, to construct 
a medical decision-making aid for diagnosing and managing airway diseases, and 
subsequently evaluated its functionality and performance in clinical workflow. 
Methods: After feeding this tool with relevant published clinical guidelines for 
these diseases, we evaluated the feasibility of the system regarding its 
behavior, ability, and potential, and made simulated cases and used this system 
to solve associated medical problems. The test and simulation questions were 
designed by a pulmonologist, and the appropriateness (focusing on accuracy and 
completeness) of AI responses were judged by three pulmonologists independently. 
The system was then deployed in an emergency department (ED) setting, where it 
was tested by medical staff (n=20) to see how it affected the process of 
clinical consultation. Test opinions were collected through questionnaire. 
Results: Most (58/84=66.7%) of the specialists' ratings regarding AI responses 
were above average. The inter-rater reliability was moderate on accuracy 
(Intraclass correlation coefficient (ICC)=0.612, P<.001) and good on 
completeness (ICC=0.773, P<.001). When deployed in an ED setting, this system 
could respond with reasonable answers, enhance the literacy of personnel about 
these diseases. The potential to save the time spent in consultation did not 
reach statistical significance (Kolmogorov-Smirnov D=.223, P=.237>.05) across 
all participants, but indicated a favorable outcome if we analyzed only 
physicians' responses. Conclusions: This system is customizable, cost-efficient, 
and accessible by clinicians and allied professionals without any computer 
coding experience in treating airway diseases. It provides convincing 
guideline-based recommendations, increases the staff's medical literacy, and 
potentially saves physicians' time spent on consultation. It warrants further 
evaluation in other medical disciplines and healthcare environments.

DOI: 10.2196/78567
PMID: 41483468


117. J Am Med Inform Assoc. 2025 Apr 1;32(4):638-647. doi: 10.1093/jamia/ocae325.

Collaborative large language models for automated data extraction in living 
systematic reviews.

Khan MA(1), Ayub U(1), Naqvi SAA(1), Khakwani KZR(2), Sipra ZBR(3), Raina A(4), 
Zhou S(1), He H(5), Saeidi A(1)(6), Hasan B(7), Rumble RB(8), Bitterman DS(9), 
Warner JL(10)(11)(12)(13), Zou J(6), Tevaarwerk AJ(14), Leventakos K(7), Kehl 
KL(15), Palmer JM(1), Murad MH(7), Baral C(6), Riaz IB(1)(16).

Author information:
(1)Department of Medicine, Mayo Clinic, Phoenix, AZ, 85054, United States.
(2)Department of Medicine, University of Arizona, Tucson, AZ, 85721, United 
States.
(3)Department of Medicine and Surgery, Rashid Latif Medical College, Lahore, 
Punjab, 54000, Pakistan.
(4)Department of Medicine, Canyon Vista Hospital, Sierra Vista, AZ, 85635, 
United States.
(5)Department of Biomedical Informatics and Data Science, Yale University, New 
Haven, CT, 06520, United States.
(6)Department of Computing and Augmented Intelligence, Arizona State University, 
Tempe, AZ, 85287, United States.
(7)Department of Medicine, Mayo Clinic, Rochester, MN, 55905, United States.
(8)American Society of Clinical Oncology, Alexandria, VA, 22314, United States.
(9)Department of Radiation Oncology, Dana-Farber Cancer Institute, Boston, MA, 
02215, United States.
(10)Department of Medicine, Brown University, Providence, RI, 02912, United 
States.
(11)Department of Biostatistics, Brown University, Providence, RI, 02912, United 
States.
(12)Department of Medicine, Rhode Island Hospital, Providence, RI, 02903, United 
States.
(13)Center for Clinical Cancer Informatics and Data Science, Legorreta Cancer 
Center, Brown University, Providence, RI, 02912, United States.
(14)Department of Oncology, Mayo Clinic, Rochester, MN, 55905, United States.
(15)Department of Medicine, Dana-Farber Cancer Institute, Boston, MA, 02215, 
United States.
(16)Department of Artificial Intelligence and Informatics, Mayo Clinic, 
Rochester, MN, 55905, United States.

Update of
    medRxiv. 2024 Sep 23:2024.09.20.24314108. doi: 10.1101/2024.09.20.24314108.

OBJECTIVE: Data extraction from the published literature is the most laborious 
step in conducting living systematic reviews (LSRs). We aim to build a 
generalizable, automated data extraction workflow leveraging large language 
models (LLMs) that mimics the real-world 2-reviewer process.
MATERIALS AND METHODS: A dataset of 10 trials (22 publications) from a published 
LSR was used, focusing on 23 variables related to trial, population, and 
outcomes data. The dataset was split into prompt development (n = 5) and 
held-out test sets (n = 17). GPT-4-turbo and Claude-3-Opus were used for data 
extraction. Responses from the 2 LLMs were considered concordant if they were 
the same for a given variable. The discordant responses from each LLM were 
provided to the other LLM for cross-critique. Accuracy, ie, the total number of 
correct responses divided by the total number of responses, was computed to 
assess performance.
RESULTS: In the prompt development set, 110 (96%) responses were concordant, 
achieving an accuracy of 0.99 against the gold standard. In the test set, 342 
(87%) responses were concordant. The accuracy of the concordant responses was 
0.94. The accuracy of the discordant responses was 0.41 for GPT-4-turbo and 0.50 
for Claude-3-Opus. Of the 49 discordant responses, 25 (51%) became concordant 
after cross-critique, increasing accuracy to 0.76.
DISCUSSION: Concordant responses by the LLMs are likely to be accurate. In 
instances of discordant responses, cross-critique can further increase the 
accuracy.
CONCLUSION: Large language models, when simulated in a collaborative, 2-reviewer 
workflow, can extract data with reasonable performance, enabling truly "living" 
systematic reviews.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocae325
PMCID: PMC12005628
PMID: 39836495 [Indexed for MEDLINE]

Conflict of interest statement: I.B.R., M.A.K., U.A., S.A.A.N., K.Z.R.K., 
Z.B.R.S., A.R., S.Z., H.H., A.H., B.H., R.B.R., J.Z., K.L.K., J.M.P., M.H.M., 
and C.B. do not have any relevant competing interests to disclose. D.S.B.: 
Editorial, unrelated to the submitted work: Associate Editor of Radiation 
Oncology, HemOnc.org (no financial compensation); Advisory and consulting, 
unrelated to the submitted work: MercurialAI. J.L.W.: Reports funding from AACR, 
NIH, Brown Physicians Incorporated, unrelated to the submitted work; consulting 
with Wested and The Lewin Group, unrelated to the submitted work; ownership in 
HemOnc.org LLC, unrelated to the submitted work. A.J.T.: Family member at Epic 
Systems, unrelated to the submitted work. K.L.: Reports consulting activities 
(honoraria to institution) with Amgen, AstraZeneca Interdisciplinary 
Corporation, Boehringer Ingelheim Pharmaceuticals, Janssen Biotech, Novartis, 
unrelated to the submitted work; advisory boards (honoraria to institution) with 
AstraZeneca, Janssen, Jazz Pharmaceuticals, Mirati Therapeutics, Regeneron, 
Takeda, and Targeted Oncology, unrelated to the submitted work; CME activities 
(honoraria to institution) with OncLive and MJH Life Sciences, MD Outlook and 
Targeted Oncology, unrelated to the submitted work; Research support (to 
institution) from AstraZeneca and Mirati Therapeutics, unrelated to the 
submitted work.


118. J Med Internet Res. 2025 May 12;27:e67253. doi: 10.2196/67253.

Large Language Models and Artificial Neural Networks for Assessing 1-Year 
Mortality in Patients With Myocardial Infarction: Analysis From the Medical 
Information Mart for Intensive Care IV (MIMIC-IV) Database.

Shi B(#)(1), Chen L(#)(1), Pang S(1), Wang Y(1), Wang S(1), Li F(1), Zhao W(1), 
Guo P(1), Zhang L(1), Fan C(1), Zou Y(1), Wu X(1).

Author information:
(1)Department of Cardiology, Beijing Anzhen Hospital, Capital Medical 
University, Beijing, China.
(#)Contributed equally

BACKGROUND: Accurate mortality risk prediction is crucial for effective 
cardiovascular risk management. Recent advancements in artificial intelligence 
(AI) have demonstrated potential in this specific medical field. Qwen-2 and 
Llama-3 are high-performance, open-source large language models (LLMs) available 
online. An artificial neural network (ANN) algorithm derived from the SWEDEHEART 
(Swedish Web System for Enhancement and Development of Evidence-Based Care in 
Heart Disease Evaluated According to Recommended Therapies) registry, termed 
SWEDEHEART-AI, can predict patient prognosis following acute myocardial 
infarction (AMI).
OBJECTIVE: This study aims to evaluate the 3 models mentioned above in 
predicting 1-year all-cause mortality in critically ill patients with AMI.
METHODS: The Medical Information Mart for Intensive Care IV (MIMIC-IV) database 
is a publicly available data set in critical care medicine. We included 2758 
patients who were first admitted for AMI and discharged alive. SWEDEHEART-AI 
calculated the mortality rate based on each patient's 21 clinical variables. 
Qwen-2 and Llama-3 analyzed the content of patients' discharge records and 
directly provided a 1-decimal value between 0 and 1 to represent 1-year death 
risk probabilities. The patients' actual mortality was verified using follow-up 
data. The predictive performance of the 3 models was assessed and compared using 
the Harrell C-statistic (C-index), the area under the receiver operating 
characteristic curve (AUROC), calibration plots, Kaplan-Meier curves, and 
decision curve analysis.
RESULTS: SWEDEHEART-AI demonstrated strong discrimination in predicting 1-year 
all-cause mortality in patients with AMI, with a higher C-index than Qwen-2 and 
Llama-3 (C-index 0.72, 95% CI 0.69-0.74 vs C-index 0.65, 0.62-0.67 vs C-index 
0.56, 95% CI 0.53-0.58, respectively; all P<.001 for both comparisons). 
SWEDEHEART-AI also showed high and consistent AUROC in the time-dependent ROC 
curve. The death rates calculated by SWEDEHEART-AI were positively correlated 
with actual mortality, and the 3 risk classes derived from this model showed 
clear differentiation in the Kaplan-Meier curve (P<.001). Calibration plots 
indicated that SWEDEHEART-AI tended to overestimate mortality risk, with an 
observed-to-expected ratio of 0.478. Compared with the LLMs, SWEDEHEART-AI 
demonstrated positive and greater net benefits at risk thresholds below 19%.
CONCLUSIONS: SWEDEHEART-AI, a trained ANN model, demonstrated the best 
performance, with strong discrimination and clinical utility in predicting 
1-year all-cause mortality in patients with AMI from an intensive care cohort. 
Among the LLMs, Qwen-2 outperformed Llama-3 and showed moderate predictive 
value. Qwen-2 and SWEDEHEART-AI exhibited comparable classification 
effectiveness. The future integration of LLMs into clinical decision support 
systems holds promise for accurate risk stratification in patients with AMI; 
however, further research is needed to optimize LLM performance and address 
calibration issues across diverse patient populations.

©Boqun Shi, Liangguo Chen, Shuo Pang, Yue Wang, Shen Wang, Fadong Li, Wenxin 
Zhao, Pengrong Guo, Leli Zhang, Chu Fan, Yi Zou, Xiaofan Wu. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org), 
12.05.2025.

DOI: 10.2196/67253
PMCID: PMC12107198
PMID: 40354652 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


119. JMIR Form Res. 2025 Jul 8;9:e72815. doi: 10.2196/72815.

AI in Qualitative Health Research Appraisal: Comparative Study.

Landerholm A(1).

Author information:
(1)Physiotherapy Department, Healthscience Faculty, Mälardalen University, 
Avdelningen för Fysioterapi Akademin för Hälsa, Vård Och Välfärd Mälardalens 
Universitet, Västerås, 721 21, Sweden, 46 702129863.

BACKGROUND: Qualitative research appraisal is crucial for ensuring credible 
findings but faces challenges due to human variability. Artificial intelligence 
(AI) models have the potential to enhance the efficiency and consistency of 
qualitative research assessments.
OBJECTIVE: This study aims to evaluate the performance of 5 AI models (GPT-3.5, 
Claude 3.5, Sonar Huge, GPT-4, and Claude 3 Opus) in assessing the quality of 
qualitative research using 3 standardized tools: Critical Appraisal Skills 
Programme (CASP), Joanna Briggs Institute (JBI) checklist, and Evaluative Tools 
for Qualitative Studies (ETQS).
METHODS: AI-generated assessments of 3 peer-reviewed qualitative papers in 
health and physical activity-related research were analyzed. The study examined 
systematic affirmation bias, interrater reliability, and tool-dependent 
disagreements across the AI models. Sensitivity analysis was conducted to 
evaluate the impact of excluding specific models on agreement levels.
RESULTS: Results revealed a systematic affirmation bias across all AI models, 
with "Yes" rates ranging from 75.9% (145/191; Claude 3 Opus) to 85.4% (164/192; 
Claude 3.5). GPT-4 diverged significantly, showing lower agreement ("Yes": 
115/192, 59.9%) and higher uncertainty ("Cannot tell": 69/192, 35.9%). 
Proprietary models (GPT-3.5 and Claude 3.5) demonstrated near-perfect alignment 
(Cramer V=0.891; P<.001), while open-source models showed greater variability. 
Interrater reliability varied by assessment tool, with CASP achieving the 
highest baseline consensus (Krippendorff α=0.653), followed by JBI (α=0.477), 
and ETQS scoring lowest (α=0.376). Sensitivity analysis revealed that excluding 
GPT-4 increased CASP agreement by 20% (α=0.784), while removing Sonar Huge 
improved JBI agreement by 18% (α=0.561). ETQS showed marginal improvements when 
excluding GPT-4 or Claude 3 Opus (+9%, α=0.409). Tool-dependent disagreements 
were evident, particularly in ETQS criteria, highlighting AI's current 
limitations in contextual interpretation.
CONCLUSIONS: The findings demonstrate that AI models exhibit both promise and 
limitations as evaluators of qualitative research quality. While they enhance 
efficiency, AI models struggle with reaching consensus in areas requiring 
nuanced interpretation, particularly for contextual criteria. The study 
underscores the importance of hybrid frameworks that integrate AI scalability 
with human oversight, especially for contextual judgment. Future research should 
prioritize developing AI training protocols that emphasize qualitative 
epistemology, benchmarking AI performance against expert panels to validate 
accuracy thresholds, and establishing ethical guidelines for disclosing AI's 
role in systematic reviews. As qualitative methodologies evolve alongside AI 
capabilities, the path forward lies in collaborative human-AI workflows that 
leverage AI's efficiency while preserving human expertise for interpretive 
tasks.

© August Landerholm. Originally published in JMIR Formative Research 
(https://formative.jmir.org).

DOI: 10.2196/72815
PMCID: PMC12263093
PMID: 40627827 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: This study used 
ChatGPT-3.5, ChatGPT-4 (OpenAI), Claude 3.5, Claude 3 Opus (Anthropic), and 
Sonar Huge (Perplexity AI) for structured quality assessments. The author is not 
employed by, holds stock in, or has received financial compensation from these 
companies. AL holds no affiliation with JMIR journals, holds no patents related 
to this work, or has financial or personal relationships with individuals or 
organizations that could influence this research.


120. J Am Med Inform Assoc. 2025 May 1;32(5):893-904. doi: 10.1093/jamia/ocaf050.

High-performance automated abstract screening with large language model 
ensembles.

Sanghera R(1)(2), Thirunavukarasu AJ(1)(3), El Khoury M(4)(5)(6), O'Logbon J(7), 
Chen Y(4), Watt A(8), Mahmood M(9), Butt H(4), Nishimura G(4), Soltan 
AAS(1)(10).

Author information:
(1)Oxford University Hospitals NHS Foundation Trust, Oxford OX3 9DU, United 
Kingdom.
(2)Oxford University Clinical Academic Graduate School, Medical Sciences 
Division, University of Oxford, Oxford OX3 9DU, United Kingdom.
(3)Nuffield Department of Clinical Neurosciences, Medical Sciences Division, 
University of Oxford, Oxford OX3 9DU, United Kingdom.
(4)School of Clinical Medicine, University of Cambridge, Cambridge CB2 0SP, 
United Kingdom.
(5)Georgetown University School of Medicine, Georgetown University, Washington, 
DC 20007, United States.
(6)MedStar Washington Hospital Center, Washington, DC 20010, United States.
(7)GKT School of Medical Education, King's College London, London WC2R 2LS, 
United Kingdom.
(8)Oxford Medical School, Medical Sciences Division, University of Oxford, 
Oxford OX3 9DU, United Kingdom.
(9)UCL Medical School, University College London, London WC1E 6DE, United 
Kingdom.
(10)Department of Oncology, Medical Sciences Division, University of Oxford, 
Oxford OX3 7DQ, United Kingdom.

OBJECTIVE: screening is a labor-intensive component of systematic review 
involving repetitive application of inclusion and exclusion criteria on a large 
volume of studies. We aimed to validate large language models (LLMs) used to 
automate abstract screening.
MATERIALS AND METHODS: LLMs (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Llama 3 70B, 
Gemini 1.5 Pro, and Claude Sonnet 3.5) were trialed across 23 Cochrane Library 
systematic reviews to evaluate their accuracy in zero-shot binary classification 
for abstract screening. Initial evaluation on a balanced development dataset 
(n = 800) identified optimal prompting strategies, and the best performing 
LLM-prompt combinations were then validated on a comprehensive dataset of 
replicated search results (n = 119 695).
RESULTS: On the development dataset, LLMs exhibited superior performance to 
human researchers in terms of sensitivity (LLMmax = 1.000, humanmax = 0.775), 
precision (LLMmax = 0.927, humanmax = 0.911), and balanced accuracy (LLMmax = 
0.904, humanmax = 0.865). When evaluated on the comprehensive dataset, the best 
performing LLM-prompt combinations exhibited consistent sensitivity (range 
0.756-1.000) but diminished precision (range 0.004-0.096) due to class 
imbalance. In addition, 66 LLM-human and LLM-LLM ensembles exhibited perfect 
sensitivity with a maximal precision of 0.458 with the development dataset, 
decreasing to 0.1450 over the comprehensive dataset; but conferring workload 
reductions ranging between 37.55% and 99.11%.
DISCUSSION: Automated abstract screening can reduce the screening workload in 
systematic review while maintaining quality. Performance variation between 
reviews highlights the importance of domain-specific validation before 
autonomous deployment. LLM-human ensembles can achieve similar benefits while 
maintaining human oversight over all records.
CONCLUSION: LLMs may reduce the human labor cost of systematic review with 
maintained or improved accuracy, thereby increasing the efficiency and quality 
of evidence synthesis.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf050
PMCID: PMC12012331
PMID: 40119675 [Indexed for MEDLINE]

Conflict of interest statement: AASS receives funding from the National 
Institute for Health and Care Research (NIHR) Applied Research Collaboration 
Oxford and Thames Valley at Oxford Health NHS Foundation Trust. The views 
expressed are those of the authors and not necessarily those of the NHS, the 
NIHR or the Department of Health and Social Care.


121. J Med Internet Res. 2025 Apr 16;27:e70535. doi: 10.2196/70535.

Unveiling the Potential of Large Language Models in Transforming Chronic Disease 
Management: Mixed Methods Systematic Review.

Li C(1), Zhao Y(1), Bai Y(2), Zhao B(3), Tola YO(4), Chan CW(#)(5), Zhang 
M(#)(2), Fu X(#)(1).

Author information:
(1)The Department of Nursing, The Eighth Affiliated Hospital, Sun Yat-sen 
University, Shenzhen, China.
(2)The School of Nursing, Sun Yat-sen University, Guangzhou, China.
(3)The School of Artificial Intelligence, Sun Yat-sen University, Guangzhou, 
China.
(4)The Department of Clinical Research, Conestoga College, Kitchener, ON, 
Canada.
(5)The Nethersole School of Nursing, The Chinese University of Hong Kong, Hong 
Kong, China.
(#)Contributed equally

BACKGROUND: Chronic diseases are a major global health burden, accounting for 
nearly three-quarters of the deaths worldwide. Large language models (LLMs) are 
advanced artificial intelligence systems with transformative potential to 
optimize chronic disease management; however, robust evidence is lacking.
OBJECTIVE: This review aims to synthesize evidence on the feasibility, 
opportunities, and challenges of LLMs across the disease management spectrum, 
from prevention to screening, diagnosis, treatment, and long-term care.
METHODS: Following the PRISMA (Preferred Reporting Items for Systematic Reviews 
and Meta-Analysis) guidelines, 11 databases (Cochrane Central Register of 
Controlled Trials, CINAHL, Embase, IEEE Xplore, MEDLINE via Ovid, ProQuest 
Health & Medicine Collection, ScienceDirect, Scopus, Web of Science Core 
Collection, China National Knowledge Internet, and SinoMed) were searched on 
April 17, 2024. Intervention and simulation studies that examined LLMs in the 
management of chronic diseases were included. The methodological quality of the 
included studies was evaluated using a rating rubric designed for 
simulation-based research and the risk of bias in nonrandomized studies of 
interventions tool for quasi-experimental studies. Narrative analysis with 
descriptive figures was used to synthesize the study findings. Random-effects 
meta-analyses were conducted to assess the pooled effect estimates of the 
feasibility of LLMs in chronic disease management.
RESULTS: A total of 20 studies examined general-purpose (n=17) and 
retrieval-augmented generation-enhanced LLMs (n=3) for the management of chronic 
diseases, including cancer, cardiovascular diseases, and metabolic disorders. 
LLMs demonstrated feasibility across the chronic disease management spectrum by 
generating relevant, comprehensible, and accurate health recommendations (pooled 
accurate rate 71%, 95% CI 0.59-0.83; I2=88.32%) with retrieval-augmented 
generation-enhanced LLMs having higher accuracy rates compared to 
general-purpose LLMs (odds ratio 2.89, 95% CI 1.83-4.58; I2=54.45%). LLMs 
facilitated equitable information access; increased patient awareness regarding 
ailments, preventive measures, and treatment options; and promoted 
self-management behaviors in lifestyle modification and symptom coping. 
Additionally, LLMs facilitate compassionate emotional support, social 
connections, and health care resources to improve the health outcomes of chronic 
diseases. However, LLMs face challenges in addressing privacy, language, and 
cultural issues; undertaking advanced tasks, including diagnosis, medication, 
and comorbidity management; and generating personalized regimens with real-time 
adjustments and multiple modalities.
CONCLUSIONS: LLMs have demonstrated the potential to transform chronic disease 
management at the individual, social, and health care levels; however, their 
direct application in clinical settings is still in its infancy. A multifaceted 
approach that incorporates robust data security, domain-specific model 
fine-tuning, multimodal data integration, and wearables is crucial for the 
evolution of LLMs into invaluable adjuncts for health care professionals to 
transform chronic disease management.
TRIAL REGISTRATION: PROSPERO CRD42024545412; 
https://www.crd.york.ac.uk/PROSPERO/view/CRD42024545412.

©Caixia Li, Yina Zhao, Yang Bai, Baoquan Zhao, Yetunde Oluwafunmilayo Tola, 
Carmen WH Chan, Meifen Zhang, Xia Fu. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org), 16.04.2025.

DOI: 10.2196/70535
PMCID: PMC12044321
PMID: 40239198 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


122. J Am Med Inform Assoc. 2025 Dec 16:ocaf219. doi: 10.1093/jamia/ocaf219. Online 
ahead of print.

Knowledge graph-augmented large language models for reconstructing life course 
risk pathways: a gestational diabetes mellitus-to-dementia case study.

Wang S(1), Zhang Y(2), Gao Y(2), He X(1), Deng G(1), Du J(1).

Author information:
(1)National Institute of Health Data Science, Peking University, Beijing 100191, 
China.
(2)Department of Endocrinology, Peking University First Hospital, Beijing 
100034, China.

OBJECTIVES: To develop and evaluate a knowledge graph-augmented large language 
model (LLM) framework that synthesizes epidemiological evidence to infer 
life-course exposure-outcome pathways, using gestational diabetes mellitus (GDM) 
and dementia as a case study.
MATERIALS AND METHODS: We constructed a causal knowledge graph by extracting 
empirical epidemiological associations from scientific literature, excluding 
hypothetical assertions. The graph was integrated with GPT-4 through four graph 
retrieval-augmented generation (GRAG) strategies to infer bridging variables 
between early-life exposure (GDM) and later-life outcome (dementia). Semantic 
triples served as structured inputs to support LLM reasoning. Each GRAG strategy 
was evaluated by human clinical experts and three LLM-based reviewers (GPT-4o, 
Llama 3-70B, and Gemini Advanced), assessing scientific reliability, novelty, 
and clinical relevance.
RESULTS: The GRAG strategy using a minimal set of abstracts specifically related 
to GDM-dementia bridging variables performed comparably to the strategy using 
broader sub-community abstracts, and both significantly outperformed approaches 
using the full GDM- or dementia-related corpus or baseline GPT-4 without 
external augmentation. The knowledge graph-augmented LLM identified 108 maternal 
candidate mediators, including validated risk factors such as chronic kidney 
disease and physical inactivity. The structured approach improved accuracy and 
reduced confabulation compared to standard LLM outputs.
DISCUSSION: Our findings suggest that augmenting LLMs with epidemiological 
knowledge graphs enables effective reasoning over fragmented literature and 
supports the reconstruction of progressive risk pathways. Expert assessments 
revealed that LLMs may overestimate clinical relevance, highlighting the need 
for human-AI collaboration in interpretation and application.
CONCLUSION: Integrating semantic epidemiological knowledge with LLMs via GRAG 
strategies provides a promising framework for life-course epidemiology, enabling 
early detection of modifiable risk factors and guiding variable selection in 
cohort study design.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf219
PMID: 41411018


123. Reg Anesth Pain Med. 2025 Feb 16:rapm-2024-106358. doi: 
10.1136/rapm-2024-106358. Online ahead of print.

Human versus artificial intelligence: evaluating ChatGPT's performance in 
conducting published systematic reviews with meta-analysis in chronic pain 
research.

Purewal A(1), Fautsch K(2), Klasova J(2), Hussain N(3), D'Souza RS(4).

Author information:
(1)Department of Orthopedic Surgery and Rehabilitation Medicine, SUNY Downstate 
Health Sciences University, Brooklyn, New York, USA.
(2)Department of Anesthesiology and Perioperative Medicine, Mayo Clinic, 
Rochester, Minnesota, USA.
(3)The Ohio State University, Columbus, Ohio, USA.
(4)Department of Anesthesiology and Perioperative Medicine, Mayo Clinic, 
Rochester, Minnesota, USA DSouza.Ryan@mayo.edu.

INTRODUCTION: Artificial intelligence (AI), particularly large-language models 
like Chat Generative Pre-Trained Transformer (ChatGPT), has demonstrated 
potential in streamlining research methodologies. Systematic reviews and 
meta-analyses, often considered the pinnacle of evidence-based medicine, are 
inherently time-intensive and demand meticulous planning, rigorous data 
extraction, thorough analysis, and careful synthesis. Despite promising 
applications of AI, its utility in conducting systematic reviews with 
meta-analysis remains unclear. This study evaluated ChatGPT's accuracy in 
conducting key tasks of a systematic review with meta-analysis.
METHODS: This validation study used data from a published meta-analysis on 
emotional functioning after spinal cord stimulation. ChatGPT-4o performed 
title/abstract screening, full-text study selection, and data pooling for this 
systematic review with meta-analysis. Comparisons were made against 
human-executed steps, which were considered the gold standard. Outcomes of 
interest included accuracy, sensitivity, specificity, positive predictive value, 
and negative predictive value for screening and full-text review tasks. We also 
assessed for discrepancies in pooled effect estimates and forest plot 
generation.
RESULTS: For title and abstract screening, ChatGPT achieved an accuracy of 
70.4%, sensitivity of 54.9%, and specificity of 80.1%. In the full-text 
screening phase, accuracy was 68.4%, sensitivity 75.6%, and specificity 66.8%. 
ChatGPT successfully pooled data for five forest plots, achieving 100% accuracy 
in calculating pooled mean differences, 95% CIs, and heterogeneity estimates (I2 
score and tau-squared values) for most outcomes, with minor discrepancies in 
tau-squared values (range 0.01-0.05). Forest plots showed no significant 
discrepancies.
CONCLUSION: ChatGPT demonstrates modest to moderate accuracy in screening and 
study selection tasks, but performs well in data pooling and meta-analytic 
calculations. These findings underscore the potential of AI to augment 
systematic review methodologies, while also emphasizing the need for human 
oversight to ensure accuracy and integrity in research workflows.

© American Society of Regional Anesthesia & Pain Medicine 2025. Re-use permitted 
under CC BY-NC. No commercial re-use. Published by BMJ Group.

DOI: 10.1136/rapm-2024-106358
PMID: 39956557

Conflict of interest statement: Competing interests: RSD'S received 
investigator-initiated research grant funding from Nevro Corp and Saol 
Therapeutics paid to his institution. Other authors declare no conflicts of 
interest.


124. J Am Med Inform Assoc. 2025 Jun 1;32(6):1071-1086. doi: 10.1093/jamia/ocaf063.

The emergence of large language models as tools in literature reviews: a large 
language model-assisted systematic review.

Scherbakov D(1), Hubig N(1)(2), Jansari V(3), Bakumenko A(3), Lenert LA(1).

Author information:
(1)Biomedical Informatics Center, Department of Public Health Sciences, Medical 
University of South Carolina (MUSC), Charleston, SC 29403, United States.
(2)Interdisciplinary Transformation University, OG 2 A-4040 Linz, Austria.
(3)School of Computing, Clemson University, Charleston, SC 29634, United States.

OBJECTIVES: This study aims to summarize the usage of large language models 
(LLMs) in the process of creating a scientific review by looking at the 
methodological papers that describe the use of LLMs in review automation and the 
review papers that mention they were made with the support of LLMs.
MATERIALS AND METHODS: The search was conducted in June 2024 in PubMed, Scopus, 
Dimensions, and Google Scholar by human reviewers. Screening and extraction 
process took place in Covidence with the help of LLM add-on based on the OpenAI 
GPT-4o model. ChatGPT and Scite.ai were used in cleaning the data, generating 
the code for figures, and drafting the manuscript.
RESULTS: Of the 3788 articles retrieved, 172 studies were deemed eligible for 
the final review. ChatGPT and GPT-based LLM emerged as the most dominant 
architecture for review automation (n = 126, 73.2%). A significant number of 
review automation projects were found, but only a limited number of papers 
(n = 26, 15.1%) were actual reviews that acknowledged LLM usage. Most citations 
focused on the automation of a particular stage of review, such as Searching for 
publications (n = 60, 34.9%) and Data extraction (n = 54, 31.4%). When comparing 
the pooled performance of GPT-based and BERT-based models, the former was better 
in data extraction with a mean precision of 83.0% (SD = 10.4) and a recall of 
86.0% (SD = 9.8).
DISCUSSION AND CONCLUSION: Our LLM-assisted systematic review revealed a 
significant number of research projects related to review automation using LLMs. 
Despite limitations, such as lower accuracy of extraction for numeric data, we 
anticipate that LLMs will soon change the way scientific reviews are conducted.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf063
PMCID: PMC12089777
PMID: 40332983 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
declare.


125. NPJ Digit Med. 2025 Jul 5;8(1):410. doi: 10.1038/s41746-025-01715-x.

A systematic literature review on integrating AI-powered smart glasses into 
digital health management for proactive healthcare solutions.

Wang B(1)(2), Zheng Y(3), Han X(4), Kong L(4), Xiao G(5), Xiao Z(6)(7), Chen 
S(8)(9).

Author information:
(1)Beijing Xiaotangshan Hospital, Beijing, China. boyuan422@foxmail.com.
(2)Department of Biomedical Sciences, City University of Hong Kong, Hong Kong, 
China. boyuan422@foxmail.com.
(3)School of Mathematical Sciences, Capital Normal University, Beijing, China.
(4)National Institute of Hospital Administration (NIHA), Beijing, China.
(5)National Institute of Hospital Administration (NIHA), Beijing, China. 
gexin_xiao@sina.com.
(6)The First Affiliated Hospital of Hunan University of Medicine, Huaihua, 
China. hyfyxzx@163.com.
(7)Hunan University of Medicine, Huaihua, China. hyfyxzx@163.com.
(8)The First Affiliated Hospital of Hunan University of Medicine, Huaihua, 
China. 1506376330@qq.com.
(9)Hunan Primary Digital Engineering Technology Research Center for Medical 
Prevention and Treatment, Huaihua, China. 1506376330@qq.com.

AI-powered smart glasses are emerging as a highly promising advancement in the 
field of digital health management, owing to their capabilities in real-time 
monitoring, chronic disease management, and personalized treatment planning. To 
comprehensively understand the current state of development, we systematically 
searched multiple databases, including Web of Science, PubMed, and IEEE Xplore, 
to collect relevant literature. This paper provides a systematic analysis of the 
current applications of smart glasses in healthcare, focusing on their potential 
benefits and limitations. Key issues discussed include user engagement, 
treatment adherence, data privacy, standardization, battery efficiency, clinical 
validation, and medical ethics. Our findings suggest that, supported by emerging 
clinical evidence, smart glasses have demonstrated significant improvements in 
areas such as assisted medical services, health management, anxiety alleviation 
in children, and telemedicine. By integrating multi-modal sensors, these devices 
are capable of accurately tracking certain physiological indicators and 
synchronizing real-time visual input, thereby enhancing the accuracy and 
timeliness of health interventions and medical services. Notably, some 
cutting-edge smart glasses have adopted advanced artificial intelligence 
algorithms, particularly large language models (LLMs) with context awareness and 
human-like interaction capabilities. These AI-powered glasses can offer 
real-time, personalized dietary and health management recommendations tailored 
to users' daily life scenarios. Building on these findings, this study further 
proposes a conceptual framework for proactive health management using smart 
glasses and explores future directions in technological development and 
practical applications. Overall, AI-enhanced smart glasses show great potential 
as a critical interface between healthcare providers and patients, poised to 
play a vital role in the future of personalized medicine and continuous health 
management.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01715-x
PMCID: PMC12228729
PMID: 40617964

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


126. J Am Med Inform Assoc. 2025 Jun 1;32(6):1050-1060. doi: 10.1093/jamia/ocaf068.

Development and validation of the provider documentation summarization quality 
instrument for large language models.

Croxford E(1), Gao Y(2), Pellegrino N(3), Wong K(3)(4), Wills G(4), First E(3), 
Schnier M(3), Burton K(4), Ebby C(5), Gorski J(4)(6), Kalscheur M(4)(7), Khalil 
S(4), Pisani M(7), Rubeor T(4), Stetson P(8), Liao F(4)(6), Goswami C(4), 
Patterson B(4)(6), Afshar M(1)(4)(7).

Author information:
(1)Department of Biostatistics and Medical Informatics, University of Wisconsin, 
Madison, WI 53792, United States.
(2)Department of Biomedical Informatics, University of Colorado-Anschutz 
Medical, Aurora, CO 80045, United States.
(3)Epic Systems, Verona, WI 53593, United States.
(4)UW Health, Madison, WI 53726, United States.
(5)Department of Pediatrics, University of Wisconsin, Madison, WI 53792, United 
States.
(6)BerbeeWalsh Department of Emergency Medicine, University of Wisconsin, 
Madison, WI 53792, United States.
(7)Department of Medicine, University of Wisconsin, Madison, WI 53792, United 
States.
(8)Memorial Sloan Kettering Cancer Center, New York, NY 10065, United States.

OBJECTIVES: As large language models (LLMs) are integrated into electronic 
health record (EHR) workflows, validated instruments are essential to evaluate 
their performance before implementation and as models and documentation 
practices evolve. Existing instruments for provider documentation quality are 
often unsuitable for the complexities of LLM-generated text and lack validation 
on real-world data. The Provider Documentation Summarization Quality Instrument 
(PDSQI-9) was developed to evaluate LLM-generated clinical summaries. This study 
aimed to validate the PDSQI-9 across key aspects of construct validity.
MATERIALS AND METHODS: Multi-document summaries were generated from real-world 
EHR data across multiple specialties using several LLMs (GPT-4o, Mixtral 8x7b, 
and Llama 3-8b). Validation included Pearson correlation analyses for 
substantive validity, factor analysis and Cronbach's α for structural validity, 
inter-rater reliability (ICC and Krippendorff's α) for generalizability, a 
semi-Delphi process for content validity, and comparisons of high- versus 
low-quality summaries for discriminant validity. Raters underwent standardized 
training to ensure consistent application of the instrument.
RESULTS: Seven physician raters evaluated 779 summaries and answered 8329 
questions, achieving over 80% power for inter-rater reliability. The PDSQI-9 
demonstrated strong internal consistency (Cronbach's α = 0.879; 95% CI, 
0.867-0.891) and high inter-rater reliability (ICC = 0.867; 95% CI, 
0.867-0.868), supporting structural validity and generalizability. Factor 
analysis identified a 4-factor model explaining 58% of the variance, 
representing organization, clarity, accuracy, and utility. Substantive validity 
was supported by correlations between note length and scores for Succinct (ρ = 
-0.200, P = .029) and Organized (ρ = -0.190, P = .037). The semi-Delphi process 
ensured clinically relevant attributes, and discriminant validity distinguished 
high- from low-quality summaries (P<.001).
DISCUSSION: The PDSQI-9 showed high inter-rater reliability, internal 
consistency, and a meaningful factor structure that reliably captured key 
dimensions of documentation quality. It distinguished between high- and 
low-quality summaries, supporting its practical utility for health systems 
needing an evaluation instrument for LLMs.
CONCLUSIONS: The PDSQI-9 demonstrates robust construct validity, supporting its 
use in clinical practice to evaluate LLM-generated summaries and facilitate 
safer, more effective integration of LLMs into healthcare workflows.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf068
PMCID: PMC12089781
PMID: 40323321 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
declare.


127. J Am Med Inform Assoc. 2025 Dec 1;32(12):1888-1898. doi: 10.1093/jamia/ocaf141.

Large language models accurately identify immunosuppression in intensive care 
unit patients.

Guggilla V(1), Kang M(2), Bak MJ(3), Tran SD(1), Pawlowski A(4), Nannapaneni 
P(4), Rasmussen LV(5), Schneider D(4), Donnelly HK(2), Agrawal A(6), Liebovitz 
D(3), Misharin AV(2)(7), Budinger GRS(2)(7), Wunderink RG(2)(7), Walunas 
TL(1)(3), Gao CA(2); NU SCRIPT Study Investigators.

Collaborators: Hauser AR, Peltekian A, Misharin AV, Wolfe AR, Szabo AL, 
Choudhary A, Ludwig A, Moghadam AA, Yeldandi AV, Agrawal A, Bharat A, Pawlowski 
AE, Joudi AM, Tambe AP, Smith-Nunez AJ, Singer BD, Ulrich BJ, Tran B, Gottardi 
CJ, Gao CA, Qi C, Pickens CO, Schroedl CJ, Meza D, Schneider D, Garcia DS, Ozer 
EA, Gusman E, Shanes ED, Leibenguth EM, Olson EM, Hartmann EM, Korth EA, Diaz E, 
Guzman ER, Martinez FJ, Matias G, Budinger GRS, Donnelly HK, Abdala-Valencia H, 
Sumner JT, Sznajder JI, Kruser JM, Glowala J, Walter JM, Rowell JH, Arnold JM, 
Coleman J, Lomasney JW, Bailey JI, Hultquist J, Fiala JA, Starren J, Ridge KM, 
Senkow KJ, Helmin KA, Gates KL, Simmons L, Pinzon L, Gradone LD, Wolfe LF, Luo 
L, Morales-Nebreda L, Rasmussen LV, Jain M, Sala MA, Schleck M, Ross MH, Querrey 
M, Kang M, Cuttica MJ, Prickett MH, Nadig N, Rhodes N, Chandel NS, Markov NS, 
Sporn PHS, Nannapaneni P, Liu Q, Kadar RB, Medernach RL, Lorenzo-Redondo R, 
Kalhan R, Clepp RK, Wunderink RG, Morimoto RI, Grant RA, Mylvaganam RJ, Fenske 
S, Laurenzo SA, Smith S, Han SH, Nozick S, Panchamukhi S, Eisenbarth SC, 
Swaminathan S, Russell SR, Poor TA, Cybulski TR, Lombardo TA, Walunas TL, Bolig 
T, Stoeger T, Doan T, Rowe T, Guggilla V, Liao WT, Luo Y, Sokolenko Y, Yu Z, Lu 
Z.

Author information:
(1)Institute for Artificial Intelligence in Medicine, Northwestern University 
Feinberg School of Medicine, Chicago, IL 60611, United States.
(2)Division of Pulmonary and Critical Care, Department of Medicine, Northwestern 
University Feinberg School of Medicine, Chicago, IL 60611, United States.
(3)Division of General Internal Medicine, Department of Medicine, Northwestern 
University Feinberg School of Medicine, Chicago, IL 60611, United States.
(4)Northwestern Medicine Enterprise Data Warehouse, Northwestern University 
Feinberg School of Medicine, Chicago, IL 60611, United States.
(5)Department of Preventive Medicine, Northwestern University Feinberg School of 
Medicine, Chicago, IL 60611, United States.
(6)Department of Electrical and Computer Engineering, Northwestern University 
McCormick School of Engineering, Evanston, IL 60208, United States.
(7)Simpson Querrey Lung Institute for Translational Science (SQLIFTS), 
Northwestern University Feinberg School of Medicine, Chicago, IL 60611, United 
States.

OBJECTIVE: Rule-based structured data algorithms and natural language processing 
(NLP) approaches applied to unstructured clinical notes have limited accuracy 
and poor generalizability for identifying immunosuppression. Large language 
models (LLMs) may effectively identify patients with heterogenous types of 
immunosuppression from unstructured clinical notes. We compared the performance 
of LLMs applied to unstructured notes for identifying patients with 
immunosuppressive conditions or immunosuppressive medication use against 2 
baselines: (1) structured data algorithms using diagnosis codes and medication 
orders and (2) NLP approaches applied to unstructured notes.
MATERIALS AND METHODS: We used hospital admission notes from a primary cohort of 
827 intensive care unit (ICU) patients at Northwestern Memorial Hospital and a 
validation cohort of 200 ICU patients at Beth Israel Deaconess Medical Center, 
along with diagnosis codes and medication orders from the primary cohort. We 
evaluated the performance of structured data algorithms, NLP approaches, and 
LLMs in identifying 7 immunosuppressive conditions and 6 immunosuppressive 
medications.
RESULTS: In the primary cohort, structured data algorithms achieved peak F1 
scores ranging from 0.30 to 0.97 for identifying immunosuppressive conditions 
and medications. NLP approaches achieved peak F1 scores ranging from 0 to 1. 
GPT-4o outperformed or matched structured data algorithms and NLP approaches 
across all conditions and medications, with F1 scores ranging from 0.51 to 1. 
GPT-4o also performed impressively in our validation cohort (F1 = 1 for 8/13 
variables).
DISCUSSION: LLMs, particularly GPT-4o, outperformed structured data algorithms 
and NLP approaches in identifying immunosuppressive conditions and medications 
with robust external validation.
CONCLUSION: LLMs can be applied for improved cohort identification for research 
purposes.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf141
PMCID: PMC12490808
PMID: 40977378 [Indexed for MEDLINE]


128. IEEE J Biomed Health Inform. 2025 Sep;29(9):6366-6378. doi: 
10.1109/JBHI.2024.3464555.

RDguru: A Conversational Intelligent Agent for Rare Diseases.

Yang J, Shu L, Duan H, Li H.

Large language models (LLMs) hold significant promise in clinical practice, yet 
their real-world adoption is constrained by their propensity to produce 
erroneous and occasionally harmful outputs, particularly in the intricate domain 
of rare diseases (RDs). This study introduces RDguru, a conversational 
intelligent agent leveraging the LangChain framework and powered by 
GPT-3.5-turbo. RDguru offers a comprehensive suite of functionalities, 
encompassing evidence-traceable knowledge Q&A and professional medical 
consultations for differential diagnosis (DDX), integrating authoritative 
knowledge sources and reliable tools. A novel multi-source fusion diagnostic 
model, rooted in deep Q-network, amalgamates three diagnostic recommendation 
strategies (GPT-4, PheLR, and phenotype matching) to enhance diagnostic recall 
during medical consultations. Through tailored tools and advanced algorithms for 
retrieval-augmented generation, RDguru excels in knowledge Q&A, automated 
phenotype annotation, and RD DDX. A multi-aspect Q&A analysis demonstrates 
RDguru outperforms ChatGPT in generating descriptions aligned with authoritative 
knowledge, quantified by ROUGE scores, GPT-4-based automatic rating, and RAGAs 
evaluation metrics. Testing on 238 published RD cases reveals that RDguru's top 
5 multi-source fusion diagnoses recapture 63.87% of actual diagnoses, marking a 
5.47% improvement over the state-of-the-art diagnostic method PheLR. 
Furthermore, RDguru's consultation strategy proves effective in eliciting 
diagnostically beneficial phenotypes and refining the prioritization of genuine 
diagnoses through multi-round phenotype-orient questioning. Evaluations against 
established benchmarks and real-world patient data demonstrate RDguru's efficacy 
and reliability, highlighting its potential to enhance clinical decision-making 
in the realm of RDs.

DOI: 10.1109/JBHI.2024.3464555
PMID: 39298307 [Indexed for MEDLINE]


129. J Allergy Clin Immunol Pract. 2025 Nov;13(11):2960-2967. doi: 
10.1016/j.jaip.2025.07.047. Epub 2025 Aug 6.

Development and Reliability Assessment of an Artificial Intelligence-Driven 
Urticaria Support (AIDUS) Chatbot.

Aulenbacher F(1), Gutsche A(1), Bihlmaier B(2), Bonnekoh H(1), Cherrez-Ojeda 
I(3), Fluhr JW(1), Kolkhir P(1), Magerl M(1), Metz M(1), Pyatilova P(1), 
Siebenhaar F(1), Zuberbier T(1), Neisinger S(4).

Author information:
(1)Institute of Allergology, Charité-Universitätsmedizin Berlin, Corporate 
Member of Freie Universität Berlin and Humboldt-Universität zu Berlin, Berlin, 
Germany; Immunology and Allergology, Fraunhofer Institute for Translational 
Medicine and Pharmacology, Berlin, Germany.
(2)Institute of Allergology, Charité-Universitätsmedizin Berlin, Corporate 
Member of Freie Universität Berlin and Humboldt-Universität zu Berlin, Berlin, 
Germany.
(3)Institute of Allergology, Charité-Universitätsmedizin Berlin, Corporate 
Member of Freie Universität Berlin and Humboldt-Universität zu Berlin, Berlin, 
Germany; Immunology and Allergology, Fraunhofer Institute for Translational 
Medicine and Pharmacology, Berlin, Germany; Department of Allergy and 
Pulmonology, Universidad Espiritu Santo, Samborondón, Ecuador.
(4)Institute of Allergology, Charité-Universitätsmedizin Berlin, Corporate 
Member of Freie Universität Berlin and Humboldt-Universität zu Berlin, Berlin, 
Germany; Immunology and Allergology, Fraunhofer Institute for Translational 
Medicine and Pharmacology, Berlin, Germany. Electronic address: 
sophia.neisinger@charite.de.

BACKGROUND: Chronic urticaria (CU) severely impairs patients' quality of life. 
Correctly diagnosing and treating CU can take years, so patients seek answers 
from the Internet to manage the condition.
OBJECTIVE: We aimed to build a chatbot, Artificial Intelligence-Driven Urticaria 
Support (AIDUS) for patients with CU and treating physicians and to evaluate its 
reliability in providing high-quality CU-specific information compared with Chat 
Generative Pre-Trained Transformer (ChatGPT)-3.5 and ChatGPT-4o.
METHODS: AIDUS was developed by an expert committee of urticaria and artificial 
intelligence specialists using JavaScript and OpenAI's 
(https://www.clay.com/dossier/openai-headquarters-office-locations) ChatGPT 
large language model. PubMed was systematically reviewed to ensure AIDUS 
contained high-quality information. The chatbot was populated exclusively with 
selected peer-reviewed CU publications authored by the Charité University, 
Berlin research group, published after 2014. A total of 254 publications were 
integrated using ChatGPT-3.5 as the underlying algorithm. We developed A set of 
100 validated questions based on current CU knowledge to evaluate the 
performance of AIDUS. The program was run on the same questions several times 
and compared for consistency. We tested performance with different chunk- and 
overlap-size settings to optimize AIDUS's efficiency and accuracy.
RESULTS: AIDUS outperformed general ChatGPT models in terms of accuracy, 
consistency, and stability in answering CU-specific questions. AIDUS 
demonstrated higher average accuracy (94.6%) across multiple test runs compared 
with ChatGPT-3.5 (42.6%) and ChatGPT-4o (85.7%).
CONCLUSIONS: AIDUS provides reliable, high-quality information about CU, 
addressing patients' and physicians' needs for accurate, relevant answers based 
on peer-reviewed medical literature. AIDUS remains a means of assistance and 
does not replace consultation with a physician.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jaip.2025.07.047
PMID: 40780383 [Indexed for MEDLINE]


130. J Plast Reconstr Aesthet Surg. 2025 Jun;105:189-195. doi: 
10.1016/j.bjps.2025.03.057. Epub 2025 Apr 3.

Evaluating artificial intelligence in decision-making for surgical treatment of 
benign breast conditions.

De Pellegrin L(1), Weinzierl A(2), Kappos EA(3), Lindenblatt N(2), Zucal I(4), 
Harder Y(5).

Author information:
(1)Department of Plastic, Reconstructive and Aesthetic Surgery, Ospedale 
Regionale di Lugano, Ente Ospedaliero Cantonale (EOC), Lugano, Switzerland; 
Department of Plastic and Hand Surgery, Inselspital University Hospital Bern, 
University of Bern, Bern, Switzerland.
(2)Department of Plastic Surgery and Hand Surgery, University Hospital Zurich, 
Zurich, Switzerland.
(3)Department of Plastic, Reconstructive, Aesthetic and Handsurgery, University 
Hospital Basel, Basel, Switzerland; Medical Faculty, University of Basel, 
Switzerland.
(4)Department of Plastic, Reconstructive and Aesthetic Surgery, Ospedale 
Regionale di Lugano, Ente Ospedaliero Cantonale (EOC), Lugano, Switzerland.
(5)Department of Plastic, Reconstructive and Aesthetic Surgery and Hand Surgery, 
University Hospital of Lausanne (CHUV), Lausanne, Switzerland; Faculty of 
Biology and Medicine, University of Lausanne, Lausanne, Switzerland. Electronic 
address: yves.harder@chuv.ch.

INTRODUCTION: Determining the ideal surgical strategy for breast pathologies can 
sometimes be challenging, in particular if personalized solutions are required. 
Moreover, acquiring the necessary expertise takes years. Artificial intelligence 
(AI) may help streamline decision-making and improve treatment approaches. This 
study assessed the potential role of AI in therapeutic planning for common 
breast pathologies.
METHODS: Five clinical cases representing common breast pathologies were 
presented to ChatGPT-4o, an advanced AI model. Its responses were evaluated by 
16 board-certified plastic surgeons and 9 residents for accuracy, relevance, and 
completeness using a Likert scale. Readability scores were also assessed by 
considering word length, syllable count, and sentence complexity.
RESULTS: AI-generated responses were found to be medically accurate, 
well-structured, and comprehensible. However, they lacked depth in surgical 
planning and risk assessment. The readability scores were Flesch Reading Ease 
(27.9), Flesch-Kincaid Grade Level (14.3), and Coleman-Liau Index (15.0), 
indicating a relatively high reading level.
CONCLUSION: ChatGPT-4o can assist in analyzing breast pathologies; however, it 
does not account for individual patient factors and surgical nuances. Although 
ChatGPT-4o is not yet suitable for independent decision-making, further 
refinement, training, and expert validation could enhance the AI's role in 
future clinical applications.

Copyright © 2025 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.bjps.2025.03.057
PMID: 40305890 [Indexed for MEDLINE]


131. JMIR Med Inform. 2025 Aug 7;13:e76636. doi: 10.2196/76636.

Current Landscape and Future Directions Regarding Generative Large Language 
Models in Stroke Care: Scoping Review.

Zhu X(1), Dai W(1), Evans R(2), Geng X(3), Mu A(4), Liu Z(1).

Author information:
(1)School of Medicine and Health Management, Tongji Medical College, Huazhong 
University of Science and Technology, Wuhan, China.
(2)Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada.
(3)Department of Physiology and Pathophysiology, School of Basic Medical 
Sciences, Peking University Health Science Center, Beijing, China.
(4)School of Ethnology and Sociology, Inner Mongolia University, Hohhot, China.

BACKGROUND: Stroke has a major impact on global health, causing long-term 
disability and straining health care resources. Generative large language models 
(gLLMs) have emerged as promising tools to help address these challenges, but 
their applications and reported performance in stroke care require comprehensive 
mapping and synthesis.
OBJECTIVE: The aim of this scoping review was to consolidate a fragmented 
evidence base and examine the current landscape, shortcomings, and future 
directions in the design, reporting, and evaluation of gLLM-based interventions 
in stroke care.
METHODS: In this scoping review, which adhered to the PRISMA-ScR (Preferred 
Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping 
Reviews) guidelines and the Population, Concept, and Context (PCC) framework, we 
searched 6 major scientific databases in December 2024 for gLLM-based 
interventions across the stroke care pathway, mapping their key characteristics 
and outcomes.
RESULTS: A total of 25 studies met the predefined eligibility criteria and were 
included for analysis. Retrospective designs predominated (n=16, 64%). Key 
applications of gLLMs included clinical decision-making support (n=10, 40%), 
administrative assistance (n=9, 36%), direct patient interaction (n=5, 20%), and 
automated literature review (n=1, 4%). Implementations mainly used generative 
pretrained transformer models accessed through task-prompted chat interfaces. In 
total, 5 key challenges were identified from the included studies during the 
implementation of gLLM-based interventions: ensuring factual alignment, 
maintaining system robustness, enhancing interpretability, optimizing 
efficiency, and facilitating clinical adoption.
CONCLUSIONS: The application of gLLMs in stroke care, while promising, remains 
relatively new, with most interventions reflecting early-stage or relatively 
simple implementations. Against this backdrop, critical gaps in research and 
clinical translation persist. To support the development of clinically impactful 
and trustworthy applications, we propose an actionable framework that 
prioritizes real-world evidence, mandates transparent technical reporting, 
broadens evaluation beyond output accuracy, strengthens validation of advanced 
task adaptation strategies, and investigates mechanisms for safe and effective 
human-gLLM interaction.

©XingCe Zhu, Wei Dai, Richard Evans, Xueyu Geng, Aruhan Mu, Zhiyong Liu. 
Originally published in JMIR Medical Informatics (https://medinform.jmir.org), 
07.08.2025.

DOI: 10.2196/76636
PMCID: PMC12371286
PMID: 40773746 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


132. Eur Arch Otorhinolaryngol. 2025 Sep;282(9):4815-4824. doi: 
10.1007/s00405-025-09524-4. Epub 2025 Jun 17.

ChatGPT versus DeepSeek in head and neck cancer staging and treatment planning: 
guideline-based study.

Vural Camalan B(1), Doluoglu S(2), Taraf NH(2), Gunay MM(2), Ozlugedik S(2).

Author information:
(1)Department of Otorhinolaryngology Head and Neck Surgery, University of Health 
Sciences Ankara Etlik City Hospital, Varlık Mahallesi, Halil Sezai Erkut 
Caddesi, No:5, 06170, Yenimahalle, Ankara, Turkey. burcuvural@gmail.com.
(2)Department of Otorhinolaryngology Head and Neck Surgery, University of Health 
Sciences Ankara Etlik City Hospital, Varlık Mahallesi, Halil Sezai Erkut 
Caddesi, No:5, 06170, Yenimahalle, Ankara, Turkey.

Comment in
    Eur Arch Otorhinolaryngol. 2025 Nov 23. doi: 10.1007/s00405-025-09846-3.

PURPOSE: This prospective simulation study was conducted to evaluate and compare 
the performance of ChatGPT (o1, 2023) and DeepSeek (V3, 2024) in staging and 
treatment planning for head and neck cancers.
METHODS: This prospective simulation study was conducted in March 2025 to 
evaluate and compare the performance of two advanced artificial intelligence 
(AI) models, ChatGPT (o1, 2023) and DeepSeek (V3, 2024), in clinical 
decision-making for head and neck malignancies. A total of 50 hypothetical, 
guideline-based clinical scenarios were carefully designed in English by two 
otorhinolaryngologists in alignment with the National Comprehensive Cancer 
Network® (NCCN®) Guidelines Version 2.2025.
RESULTS: In the overall analysis of treatment planning performance, DeepSeek 
(V3, 2024) demonstrated statistically superior accuracy compared to ChatGPT (o1, 
2023) (p = 0.04). Both models showed comparable performance in tumor staging 
(p = 0.83). Both DeepSeek (p = 0.0001) and ChatGPT (p = 0.02) were statistically 
successful in respect of staging accuracy and providing fully correct answers on 
the subject of treatment.
CONCLUSION: Although DeepSeek V3 demonstrated promising capability for clinical 
decision support in head and neck oncology, these artificial intelligence tools 
cannot replace multidisciplinary tumor boards. However, they can significantly 
streamline clinical workflows by rapidly organizing patient data, thereby 
enhancing board efficiency. Future efforts should prioritize the development and 
integration of secure, institution-specific, local large language models 
tailored for oncological decision-making.

© 2025. The Author(s).

DOI: 10.1007/s00405-025-09524-4
PMCID: PMC12423241
PMID: 40523995 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Conflict of interest: The authors declare that they 
have no conflict of interest.


133. Front Digit Health. 2025 Jul 1;7:1624786. doi: 10.3389/fdgth.2025.1624786. 
eCollection 2025.

Diagnostic efficacy of large language models in the pediatric emergency 
department: a pilot study.

Del Monte F(#)(1), Barolo R(#)(2), Circhetta M(3), Delmonaco AG(1), Castagno 
E(1), Pivetta E(4), Bergamasco L(3)(5), Franco M(6), Olmo G(3), Bondone C(1).

Author information:
(1)Department of Pediatric Emergency, Regina Margherita Children's 
Hospital-A.O.U. Città Della Salute e Della Scienza di Torino, Turin, Italy.
(2)Department of Public Health and Pediatrics, Postgraduate School of 
Pediatrics, University of Turin, Turin, Italy.
(3)Department of Control and Computer Engineering, Politecnico di Torino, Turin, 
Italy.
(4)Division of Emergency Medicine and High Dependency Unit, Department of 
Medical Sciences, Città Della Salute e Della Scienza di Torino and University of 
Turin, Turin, Italy.
(5)LINKS Foundation, Turin, Italy.
(6)Department of Clinical and Biological Sciences, University of Turin, 
Orbassano, Turin, Italy.
(#)Contributed equally

Erratum in
    Front Digit Health. 2025 Jul 16;7:1658635. doi: 10.3389/fdgth.2025.1658635.

BACKGROUND: The Pediatric Emergency Department (PED) faces significant 
challenges, such as high patient volumes, time-sensitive decisions, and complex 
diagnoses. Large Language Models (LLMs) have the potential to enhance patient 
care; however, their effectiveness in supporting the diagnostic process remains 
uncertain, with studies showing mixed results regarding their impact on clinical 
reasoning. We aimed to assess LLM-based chatbots performance in realistic PED 
scenarios, and to explore their use as diagnosis-making assistants in pediatric 
emergency.
METHODS: We evaluated the diagnostic effectiveness of 5 LLMs (ChatGPT-4o, Gemini 
1.5 Pro, Gemini 1.5 Flash, Llama-3-8B, and ChatGPT-4o mini) compared to 23 
physicians (including 10 PED physicians, 6 PED residents, and 7 Emergency 
Medicine residents). Both LLMs and physicians had to provide one primary 
diagnosis and two differential diagnoses for 80 real-practice pediatric clinical 
cases from the PED of a tertiary care Children's Hospital, with three different 
levels of diagnostic complexity. The responses from both LLMs and physicians 
were compared to the final diagnoses assigned upon patient discharge; two 
independent experts evaluated the answers using a five-level accuracy scale. 
Each physician or LLM received a total score out of 80, based on the sum of all 
answer points.
RESULTS: The best performing chatbots were ChatGPT-4o (score: 72.5) and Gemini 
1.5 Pro (score: 62.75), the first performing better (p < 0.05) than PED 
physicians (score: 61.88). Emergency Medicine residents performed worse (score: 
43.75) than both the other physicians and chatbots (p < 0.01). Chatbots' 
performance was inversely proportional to case difficulty, but ChatGPT-4o 
managed to match the majority of the correct answers even for highly difficult 
cases.
DISCUSSION: ChatGPT-4o and Gemini 1.5 Pro could be a valid tool for ED 
physicians, supporting clinical decision-making without replacing the 
physician's judgment. Shared protocols for effective collaboration between AI 
chatbots and healthcare professionals are needed.

© 2025 Del Monte, Barolo, Circhetta, Delmonaco, Castagno, Pivetta, Bergamasco, 
Franco, Olmo and Bondone.

DOI: 10.3389/fdgth.2025.1624786
PMCID: PMC12259579
PMID: 40666107

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


134. JMIR Form Res. 2025 Apr 24;9:e53335. doi: 10.2196/53335.

Guideline-Incorporated Large Language Model-Driven Evaluation of Medical Records 
Using MedCheckLLM.

Schubert MC(1), Soyka S(1), Wick W(1), Venkataramani V.

Author information:
(1)Department of Neurology, University Hospital Heidelberg, Im Neuenheimer Feld 
400, Heidelberg, 69120, Germany, 49 6221548630.

The study introduces MedCheckLLM, a large language model-driven framework that 
enhances medical record evaluation through a guideline-in-the-loop approach by 
integrating evidence-based guidelines.

© Marc Cicero Schubert, Stella Soyka, Wolfgang Wick, Varun Venkataramani. 
Originally published in JMIR Formative Research (https://formative.jmir.org).

DOI: 10.2196/53335
PMCID: PMC12045122
PMID: 40272831 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


135. BMC Nurs. 2025 Apr 8;24(1):394. doi: 10.1186/s12912-025-03010-2.

Can large language models facilitate the effective implementation of nursing 
processes in clinical settings?

Cao Y(#)(1), Hu L(#)(1), Cao X(2), Peng J(3).

Author information:
(1)Nursing Department, Department of Neurosurgery, The First Affiliated Hospital 
of Chongqing Medical University, Chongqing, 400016, People's Republic of China.
(2)Chong Qing Wondertek Software Corporation, Room 2, 10th Floor, Building C, 
Qilin, Huangshan Avenue Middle Section, Yubei District, Chongqing, 401121, 
People's Republic of China.
(3)Nursing Department, Department of Neurosurgery, The First Affiliated Hospital 
of Chongqing Medical University, Chongqing, 400016, People's Republic of China. 
202599@hospital.cqmu.edu.cn.
(#)Contributed equally

BACKGROUND: The quality of generative nursing diagnoses and plans reported in 
existing research remains a topic of debate, and previous studies have primarily 
utilized ChatGPT as the sole large language mode.
PURPOSE: To explore the quality of nursing diagnoses and plans generated by a 
prompt framework across different large language models (LLMs) and assess the 
potential applicability of LLMs in clinical settings.
METHODS: We designed a structured nursing assessment template and iteratively 
developed a prompt framework incorporating various prompting techniques. We then 
evaluated the quality of nursing diagnoses and care plans generated by this 
framework across two distinct LLMs(ERNIE Bot 4.0 and Moonshot AI), while also 
assessing their clinical utility.
RESULTS: The scope and nature of the nursing diagnoses generated by ERNIE Bot 
4.0 and Moonshot AI were similar to the "gold standard" nursing diagnoses and 
care plans.The structured assessment template effectively and comprehensively 
captures the key characteristics of neurosurgical patients, while the strategic 
use of prompting techniques has enhanced the generalization capabilities of the 
LLMs.
CONCLUSION: Our research further confirms the potential of LLMs in clinical 
nursing practice.However, significant challenges remain in the effective 
integration of LLM-assisted nursing processes into clinical environments.

© 2025. The Author(s).

DOI: 10.1186/s12912-025-03010-2
PMCID: PMC11980118
PMID: 40200247

Conflict of interest statement: Declarations. Ethical approval and consent to 
participate: The study was conducted in compliance with the Declaration of 
Helsinki and received ethical approval from the Medical Ethics Committee of the 
First Affiliated Hospital of Chongqing Medical University (Ethics review batch 
number K2023-191). All of the participants provided informed consent to 
participate in the study. Large Language Models were only used to generate 
nursing diagnoses and care plans in this study. Large Language Models were not 
utilized for the writing of the manuscript. All writing was completed by human 
authors. This is hereby stated for clarification. Consent for publication: Not 
applicable. Clinical trial number: Not applicable. Competing interests: The 
authors declare no competing interests.


136. J Med Educ Curric Dev. 2025 Oct 29;12:23821205251391969. doi: 
10.1177/23821205251391969. eCollection 2025 Jan-Dec.

Perceptions and Use of Generative Artificial Intelligence in Medical Students: A 
Multicenter Survey.

Tran C(1), Hryciw BN(2), Moore SW(3), Chaput A(4), Seely AJE(5)(6).

Author information:
(1)Department of Medicine, Faculty of Medicine, University of Ottawa, Ottawa, 
Ontario, Canada.
(2)Department of Critical Care, The Ottawa Hospital, Ottawa, Ontario, Canada.
(3)Emergency Medicine Section, Division of Clinical Sciences, Northern Ontario 
School of Medicine University, Thunder Bay, Ontario, Canada.
(4)Department of Anesthesiology and Pain Medicine, The Ottawa Hospital, 
University of Ottawa, Ottawa, Ontario, Canada.
(5)Division of Thoracic Surgery, Department of Surgery, The Ottawa Hospital, 
Ottawa, Ontario, Canada.
(6)Divisions of Thoracic Surgery and Critical Care Medicine, University of 
Ottawa, Ottawa, Ontario, Canada.

INTRODUCTION: Generative artificial intelligence (AI) has transformative 
potential in medical training, and its role in medicine holds drastic 
implications for patients, healthcare providers, and society; however, its 
current use by medical students is unknown. The study aims to characterize the 
use, frequency of use, and perceptions of generative AI by Canadian medical 
students.
METHODS: A cross-sectional survey was distributed to 6 medical schools in 
Ontario, Canada, to investigate how medical students use generative AI in 
education, clinical settings, and for communication, and to assess the perceived 
barriers and enablers that influence their use.
RESULTS: A total of 167 respondents completed the survey (60.8% female, 69.3% in 
first and second year), and over 78.9% of respondents reported using generative 
AI, with ChatGPT being the most popular model; 53.0% of respondents were 
frequent users and reported using generative AI tools at least once a week. In 
clinical settings, students report using generative AI for learning and 
reviewing medical content, summarizing clinical guidelines, and generating 
differential diagnoses; 92.8% of students were willing to learn how to use 
generative AI to integrate it into their future clinical practice. At the same 
time, most medical students appreciated the limitations of generative AI in 
terms of its risk for inaccuracy (91.6%) and bias (78.9%); 75.9% of participants 
agreed that generative AI should be implemented as a resource or formal teaching 
topic in medical training.
DISCUSSION: The findings of this study may help guide medical education 
institutions in adapting curricula and developing policies to promote the 
ethical and appropriate use of generative AI in medicine.

© The Author(s) 2025.

DOI: 10.1177/23821205251391969
PMCID: PMC12576227
PMID: 41181167

Conflict of interest statement: The authors declared the following potential 
conflicts of interest with respect to the research, authorship, and/or 
publication of this article: Dr Hryciw is a co-founder of Osler AI Inc., a 
medical training and evaluation company. Dr Seely is founder and CEO of 
Therapeutic Monitoring Systems (TMS) Inc., a company that manages several 
point-of-care decision support software's. Dr Seely manages this active COI with 
agreements with the Ottawa Health Research Institute (OHRI) and disclosures on 
grants, presentations, and publications.


137. J Am Med Inform Assoc. 2025 Apr 1;32(4):616-625. doi: 10.1093/jamia/ocaf030.

Enhancing systematic literature reviews with generative artificial intelligence: 
development, applications, and performance evaluation.

Li Y(1), Datta S(2), Rastegar-Mojarad M(2), Lee K(2), Paek H(2), Glasgow J(2), 
Liston C(2), He L(2), Wang X(2), Xu Y(1).

Author information:
(1)Regeneron Pharmaceuticals, Inc., Tarrytown, NY 10591, United States.
(2)IMO Health, Inc., Rosemont, IL 60018, United States.

OBJECTIVES: We developed and validated a large language model (LLM)-assisted 
system for conducting systematic literature reviews in health technology 
assessment (HTA) submissions.
MATERIALS AND METHODS: We developed a five-module system using abstracts 
acquired from PubMed: (1) literature search query setup; (2) study protocol 
setup using population, intervention/comparison, outcome, and study type (PICOs) 
criteria; (3) LLM-assisted abstract screening; (4) LLM-assisted data extraction; 
and (5) data summarization. The system incorporates a human-in-the-loop design, 
allowing real-time PICOs criteria adjustment. This is achieved by collecting 
information on disagreements between the LLM and human reviewers regarding 
inclusion/exclusion decisions and their rationales, enabling informed PICOs 
refinement. We generated four evaluation sets including relapsed and refractory 
multiple myeloma (RRMM) and advanced melanoma to evaluate the LLM's performance 
in three key areas: (1) recommending inclusion/exclusion decisions during 
abstract screening, (2) providing valid rationales for abstract exclusion, and 
(3) extracting relevant information from included abstracts.
RESULTS: The system demonstrated relatively high performance across all 
evaluation sets. For abstract screening, it achieved an average sensitivity of 
90%, F1 score of 82, accuracy of 89%, and Cohen's κ of 0.71, indicating 
substantial agreement between human reviewers and LLM-based results. In 
identifying specific exclusion rationales, the system attained accuracies of 97% 
and 84%, and F1 scores of 98 and 89 for RRMM and advanced melanoma, 
respectively. For data extraction, the system achieved an F1 score of 93.
DISCUSSION: Results showed high sensitivity, Cohen's κ, and PABAK for abstract 
screening, and high F1 scores for data extraction. This human-in-the-loop 
AI-assisted SLR system demonstrates the potential of GPT-4's in context learning 
capabilities by eliminating the need for manually annotated training data. In 
addition, this LLM-based system offers subject matter experts greater control 
through prompt adjustment and real-time feedback, enabling iterative refinement 
of PICOs criteria based on performance metrics.
CONCLUSION: The system demonstrates potential to streamline systematic 
literature reviews, potentially reducing time, cost, and human errors while 
enhancing evidence generation for HTA submissions.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf030
PMCID: PMC12005633
PMID: 40036547 [Indexed for MEDLINE]

Conflict of interest statement: Y.L. and Y.X. are currently employees of 
Regeneron Pharmaceuticals, Inc. S.D, M.R.-M, K.L., H.P., J.G., C.L., L.H., and 
X.W. are currently employees of Intelligence Medical Objective, Inc. The 
affiliations played no role in the design, execution, interpretation, or 
reporting of this research. The authors affirm that the manuscript is an honest, 
accurate, and transparent account of the study being reported. All views and 
opinions expressed in this manuscript are solely those of the authors and do not 
necessarily represent the views of the companies.


138. Pediatr Transplant. 2025 May;29(3):e70068. doi: 10.1111/petr.70068.

The Potential of ChatGPT as a Source of Information for Kidney Transplant 
Recipients and Their Caregivers.

Demirbaş KC(1), Saygılı S(2), Yılmaz EK(2), Gülmez R(3), Ağbaş A(2), Taşdemir 
M(4), Canpolat N(2).

Author information:
(1)Department of Pediatrics, Istanbul University-Cerrahpaşa, Cerrahpaşa School 
of Medicine, Istanbul, Türkiye.
(2)Division of Pediatric Nephrology, Department of Pediatrics, Istanbul 
University-Cerrahpaşa, Cerrahpaşa School of Medicine, Istanbul, Türkiye.
(3)Division of Pediatric Nephrology, Department of Pediatrics, Istanbul Prof. 
Dr. Suleyman Yalcin Research and Training Hospital, Istanbul, Türkiye.
(4)Division of Pediatric Nephrology, Department of Pediatrics, Istinye 
University School of Medicine, Istanbul, Türkiye.

BACKGROUND: Education and enhancing the knowledge of adolescents who will 
undergo kidney transplantation are among the primary objectives of their care. 
While there are specific interventions in place to achieve this, they require 
extensive resources. The rise of large language models like ChatGPT-3.5 offers 
potential assistance for providing information to patients. This study aimed to 
evaluate the accuracy, relevance, and safety of ChatGPT-3.5's responses to 
patient-centered questions about pediatric kidney transplantation. The objective 
was to assess whether ChatGPT-3.5 could be a supplementary educational tool for 
adolescents and their caregivers in a complex medical context.
METHODS: A total of 37 questions about kidney transplantation were presented to 
ChatGPT-3.5, which was prompted to respond as a health professional would to a 
layperson. Five pediatric nephrologists independently evaluated the outputs for 
accuracy, relevance, comprehensiveness, understandability, readability, and 
safety.
RESULTS: The mean accuracy, relevancy, and comprehensiveness scores for all 
outputs were 4.51, 4.56, and 4.55, respectively. Out of 37 outputs, four were 
rated as completely accurate, and seven were completely relevant and 
comprehensive. Only one output had an accuracy, relevancy, and comprehensiveness 
score below 4. Twelve outputs were considered potentially risky, but only three 
had a risk grade of moderate or higher. Outputs that were considered risky had 
an accuracy and relevancy below the average.
CONCLUSION: Our findings suggest that ChatGPT could be a useful tool for 
adolescents or caregivers of individuals waiting for kidney transplantation. 
However, the presence of potentially risky outputs underscores the necessity for 
human oversight and validation.

© 2025 Wiley Periodicals LLC.

DOI: 10.1111/petr.70068
PMID: 40078030 [Indexed for MEDLINE]


139. J Cardiothorac Vasc Anesth. 2025 Dec;39(12):3534-3540. doi: 
10.1053/j.jvca.2025.07.026. Epub 2025 Jul 22.

Dependability of Large Language Models in Cardiovascular Medicine: A Scoping 
Review.

Jia YY(1), Pang LY(2), Bi MM(3), Yang XL(4), Song JP(5).

Author information:
(1)Nursing Department, The Second Affiliated Hospital of Zhejiang University 
School of Medicine, Hangzhou, China; School of Medicine, Zhejiang University, 
Hangzhou, China; West China School of Medicine, Chengdu, China.
(2)School of Nursing, Lan Zhou University, Lanzhou, China.
(3)Nursing Department, Jiaxing University Affiliated Xin'an International 
Hospital, Jiaxing, China.
(4)Nursing Department, Zhejiang University of Traditional Chinese Medicine, 
Hangzhou, China.
(5)Nursing Department, The Second Affiliated Hospital of Zhejiang University 
School of Medicine, Hangzhou, China. Electronic address: zrxwk1@zju.edu.cn.

BACKGROUND: The adoption of large language models (LLMs) in both clinical and 
consumer healthcare settings has surged exponentially. However, there remains 
limited evidence on their reliability and impact in cardiovascular practice.
OBJECTIVES: This scoping review was designed to consolidate the existing 
biomedical literature on applicability, reliability, and quality improvement 
strategies for the integration of LLMs into the cardiovascular domain. Following 
Cochrane methodology and Preferred Reporting Items for Systematic Reviews and 
Meta-analyses guidelines, three electronic databases (PubMed, Web of Science, 
and Embase) were systematically searched to identify pertinent studies published 
between August 2020 and February 2025. Articles addressing the development, 
implementation, and assessment of LLMs in cardiovascular medicine were selected 
for comprehensive analysis.
RESULTS: Twenty-five eligible publications evaluated the performance of LLMs in 
responding to cardiology-related questions, encompassing parameters such as 
accuracy, response latency, indirectness, completeness, and so on. The 
assessment methodology varied considerably across studies. LLMs demonstrated 
potential utility in cardiovascular decision-making, myocarditis management, 
cardiac arrest diagnosis and treatment, and image differentiation.
CONCLUSIONS: Although some LLM-generated responses to cardiovascular-related 
questions exhibit acceptable levels of quality, significant drawbacks persist. 
These include verbosity, inaccuracies, occasional misinformation, inconsistent 
outputs to identical questions, bias, and poor reproducibility. Overall, this 
work highlights the urgent need for continued refinement and validation.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1053/j.jvca.2025.07.026
PMID: 40835552 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


140. Aesthetic Plast Surg. 2025 Oct;49(19):5641-5642. doi: 
10.1007/s00266-025-05038-w. Epub 2025 Jun 25.

Toward Transparent AI-Enabled Patient Selection in Cosmetic Surgery by 
Integrating Reasoning and Medical LLMs.

Ray PP(1).

Author information:
(1)Department of Computer Applications, Sikkim University, Gangtok, Sikkim, 
India. ppray@cus.ac.in.

Existing AI solutions-like the XGBoost tool by Li et al.-show potential for 
preoperative screening but rely on fixed questionnaires and opaque feature 
weighting. We introduce a hybrid framework that combines reasoning LLMs (OpenAI 
o3, DeepSeek R1, Google Gemini 2.5, Anthropic Claude 3.7 Sonnet) with specialty 
medical models (Baichuan-M1, Zhipu AI GLM-4-9B-Chat, OpenBioLLM-Llama-70B, 
MedLLaMA3-v20, Med-PaLM 2, SurgeryLLM). Patient inputs-structured and 
free-text-are ingested via a secure mobile app and processed through a 
retrieval-augmented pipeline. Reasoning LLMs expose chain-of-thought steps for 
full transparency, while medical LLMs validate each risk factor against clinical 
guidelines. An ensemble then delivers a composite suitability score, complete 
with an audit trail of data points and citations. We address key hurdles-model 
recency, hallucination control, data privacy, and fairness-and recommend a 
medical-device regulatory approach with independent validation, ongoing bias 
monitoring, and co-design with multidisciplinary stakeholders.Level of Evidence 
V This journal requires that authors assign a level of evidence to each article. 
For a full description of these Evidence-Based Medicine ratings, please refer to 
the Table of Contents or the online Instructions to Authors 
www.springer.com/00266 .

© 2025. Springer Science+Business Media, LLC, part of Springer Nature and 
International Society of Aesthetic Plastic Surgery.

DOI: 10.1007/s00266-025-05038-w
PMID: 40563006 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The author 
declares that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper. Ethical Approval: Ethical approval is not applicable as the study does 
not involve human participants or sensitive data collection. Consent for 
Publication: Not applicable. Human and Animal Rights: This letter does not 
contain any studies with animals performed by any of the authors. Human 
Participants: Not applicable since no human participants were involved in this 
article. Informed Consent: Not applicable.


141. Digit Health. 2025 Jul 10;11:20552076251353731. doi: 10.1177/20552076251353731. 
eCollection 2025 Jan-Dec.

The effect of chatbot-based exercise interventions on physical activity, 
exercise habits, and sedentary behavior: A systematic review and meta-analysis 
of randomized controlled trials.

Wang Q(1), Yang W(2), Xu K(3), Lam LT(1)(4)(5).

Author information:
(1)Faculty of Medicine, Macau University of Science and Technology, Macau SAR, 
China.
(2)Jiangsu Research Institute of Sports Science, Nanjing, China.
(3)Nanjing Sport Institute, Nanjing, China.
(4)Faculty of Medicine and Health, The University of Sydney, Sydney, Australia.
(5)Faculty of Health, University of Technology Sydney, Sydney, Australia.

OBJECTIVES: Chatbots, transcending the limitations of space and time while 
reducing user resistance, offer a personalized and autonomous approach 
potentially enhancing digital health interventions. This systematic review aims 
to quantitatively evaluate the effectiveness of chatbot-based exercise 
interventions (EIs) in randomized controlled trials (RCTs), emphasizing physical 
activity (PA), exercise habits (EH), and sedentary behavior (SB).
METHODS: A comprehensive search was carried out across Embase, Web of Science, 
PubMed, and Cochrane databases for studies published from January 2010 to 
October 2024. The characteristics of the included studies and the employed 
chatbots were analyzed. A meta-analysis was performed to synthesize the impact 
of chatbot-based EIs on PA, EH, and SB. We carried out subgroup analyses (SAs) 
for investigating potential modifying effects.
RESULTS: This review includes 12 studies conducted between 2013 and 2024, 
encompassing 2446 participants and 240 parent-child dyads, with participant ages 
ranging from 8 to 71 years. The chatbot-based EIs significantly enhanced PA 
compared to control groups (SMD = 0.20, 95% CI = 0.04-0.37, p = 0.02; I² = 51%, 
p = 0.04). However, these interventions did not significantly impact EH 
(SMD = 0.29, 95% CI = -0.48-1.06, p = 0.46) or reduce SB (SMD = 0.32, 95% 
CI = -0.54-1.19, p = 0.46). Notably, SAs revealed a small but significant 
short-term increase in EH (SMD = 0.29, 95% CI = 0.08-0.50, p = 0.006).
CONCLUSION: Chatbot-based EIs appear promising in improving PA, particularly 
moderate to vigorous PA (MVPA). Nonetheless, further RCTs with diverse chatbot 
designs, larger sample sizes, low-resource settings, objective measures of SB 
(e.g., accelerometers), and extended follow-ups are essential to corroborate 
these findings. Future research should also explore the adaptation of chatbots 
to various socioeconomic and cultural contexts and the integration of 
technologies like ChatGPT into EIs.Trial Registration: Meta-analysis PROSPERO: 
CRD42024609852.

© The Author(s) 2025.

DOI: 10.1177/20552076251353731
PMCID: PMC12254675
PMID: 40656859

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


142. JMIR Med Educ. 2025 Sep 19;11:e68070. doi: 10.2196/68070.

Evaluating the Potential and Accuracy of ChatGPT-3.5 and 4.0 in Medical 
Licensing and In-Training Examinations: Systematic Review and Meta-Analysis.

Jaleel A(#)(1), Aziz U(#)(1), Farid G(#)(1), Zahid Bashir M(1), Mirza TR(1), 
Khizar Abbas SM(1), Aslam S(#)(1), Sikander RMH(#)(1).

Author information:
(1)Shalamar Medical and Dental College, Lahore, Pakistan.
(#)Contributed equally

BACKGROUND: Artificial intelligence (AI) has significantly impacted health care, 
medicine, and radiology, offering personalized treatment plans, simplified 
workflows, and informed clinical decisions. ChatGPT (OpenAI), a conversational 
AI model, has revolutionized health care and medical education by simulating 
clinical scenarios and improving communication skills. However, inconsistent 
performance across medical licensing examinations and variability between 
countries and specialties highlight the need for further research on contextual 
factors influencing AI accuracy and exploring its potential to enhance technical 
proficiency and soft skills, making AI a reliable tool in patient care and 
medical education.
OBJECTIVE: This systematic review aims to evaluate and compare the accuracy and 
potential of ChatGPT-3.5 and 4.0 in medical licensing and in-training residency 
examinations across various countries and specialties.
METHODS: A systematic review and meta-analysis were conducted, adhering to the 
PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) 
guidelines. Data were collected from multiple reputable databases (Scopus, 
PubMed, JMIR Publications, Elsevier, BMJ, and Wiley Online Library), focusing on 
studies published from January 2023 to July 2024. Analysis specifically targeted 
research assessing ChatGPT's efficacy in medical licensing exams, excluding 
studies not related to this focus or published in languages other than English. 
Ultimately, 53 studies were included, providing a robust dataset for comparing 
the accuracy rates of ChatGPT-3.5 and 4.0.
RESULTS: ChatGPT-4 outperformed ChatGPT-3.5 in medical licensing exams, 
achieving a pooled accuracy of 81.8%, compared to ChatGPT-3.5's 60.8%. In 
in-training residency exams, ChatGPT-4 achieved an accuracy rate of 72.2%, 
compared to 57.7% for ChatGPT-3.5. The forest plot presented a risk ratio of 
1.36 (95% CI 1.30-1.43), demonstrating that ChatGPT-4 was 36% more likely to 
provide correct answers than ChatGPT-3.5 across both medical licensing and 
residency exams. These results indicate that ChatGPT-4 significantly outperforms 
ChatGPT-3.5, but the performance advantage varies depending on the exam type. 
This highlights the importance of targeted improvements and further research to 
optimize ChatGPT-4's performance in specific educational and clinical settings.
CONCLUSIONS: ChatGPT-4.0 and 3.5 show promising results in enhancing medical 
education and supporting clinical decision-making, but they cannot replace the 
comprehensive skill set required for effective medical practice. Future research 
should focus on improving AI's capabilities in interpreting complex clinical 
data and enhancing its reliability as an educational resource.

©Anila Jaleel, Umair Aziz, Ghulam Farid, Muhammad Zahid Bashir, Tehmasp Rehman 
Mirza, Syed Mohammad Khizar Abbas, Shiraz Aslam, Rana Muhammad Hassaan Sikander. 
Originally published in JMIR Medical Education (https://mededu.jmir.org), 
19.09.2025.

DOI: 10.2196/68070
PMCID: PMC12495368
PMID: 40973108 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


143. NPJ Digit Med. 2025 Jun 4;8(1):332. doi: 10.1038/s41746-025-01746-4.

Racial bias in AI-mediated psychiatric diagnosis and treatment: a qualitative 
comparison of four large language models.

Bouguettaya A(1)(2), Stuart EM(3), Aboujaoude E(4)(5).

Author information:
(1)Department of Biomedical Sciences, Cedars-Sinai Medical Center, Los Angeles, 
CA, USA.
(2)School of Nursing and Midwifery, Monash University, Melbourne, VIC, 
Australia.
(3)Jonathan Jaques Children's Cancer Institute, Miller Children's & Women's 
Hospital Long Beach, Long Beach, CA, USA.
(4)Department of Biomedical Sciences, Cedars-Sinai Medical Center, Los Angeles, 
CA, USA. elias.aboujaoude@cshs.org.
(5)Department of Psychiatry and Behavioral Sciences, Stanford University, 
Stanford, CA, USA. elias.aboujaoude@cshs.org.

Artificial intelligence (AI), particularly large language models (LLMs), is 
increasingly integrated into mental health care. This study examined racial bias 
in psychiatric diagnosis and treatment across four leading LLMs: Claude, 
ChatGPT, Gemini, and NewMes-15 (a local, medical-focused LLaMA 3 variant). Ten 
psychiatric patient cases representing five diagnoses were presented to these 
models under three conditions: race-neutral, race-implied, and race-explicitly 
stated (i.e., stating patient is African American). The models' diagnostic 
recommendations and treatment plans were qualitatively evaluated by a clinical 
psychologist and a social psychologist, who scored 120 outputs for bias by 
comparing responses generated under race-neutral, race-implied, and 
race-explicit conditions. Results indicated that LLMs often proposed inferior 
treatments when patient race was explicitly or implicitly indicated, though 
diagnostic decisions demonstrated minimal bias. NewMes-15 exhibited the highest 
degree of racial bias, while Gemini showed the least. These findings underscore 
critical concerns about the potential for AI to perpetuate racial disparities in 
mental healthcare, emphasizing the necessity of rigorous bias assessment in 
algorithmic medical decision support systems.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01746-4
PMCID: PMC12137607
PMID: 40467886

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


144. Liver Int. 2025 Oct;45(10):e70349. doi: 10.1111/liv.70349.

From Guidelines to Real-Time Conversation: Expert-Validated Retrieval-Augmented 
and Fine-Tuned GPT-4 for Hepatitis C Management.

Giuffrè M(1)(2), Pugliese N(3)(4), Kresevic S(5), Ajcevic M(5), Negro F(6), 
Puoti M(7)(8), Forns X(9)(10), Pawlotsky JM(11)(12)(13), Shung DL(1), Aghemo 
A(3)(4).

Author information:
(1)Department of Internal Medicine (Section of Digestive Diseases), Yale School 
of Medicine, Yale University, New Haven, Connecticut, USA.
(2)Department of Medicine, Surgery and Health Sciences, University of Trieste, 
Trieste, Italy.
(3)Department of Biomedical Sciences, Humanitas University, Milano, Italy.
(4)Division of Internal Medicine and Hepatology, Department of Gastroenterology, 
IRCCS Humanitas Research Hospital, Milano, Italy.
(5)Department of Engineering and Architecture, University of Trieste, Trieste, 
Italy.
(6)Division of Gastroenterology, University Hospitals of Geneva, Geneva, 
Switzerland.
(7)Division of Infectious Diseases, ASST GOM Niguarda, Milan, Italy.
(8)Department of Medicine, University of Milan Bicocca, Milan, Italy.
(9)Liver Unit, Hospital Clínic, University of Barcelona, IDIBAPS, Barcelona, 
Spain.
(10)Consorcio de Investigación Biomédica en Red de Enfermedades Hepáticas y 
Digestivas, ISCIII, Madrid, Spain.
(11)Université Paris Est Créteil, INSERM, IMRB, Créteil, France.
(12)Inserm, U955, Team18, Créteil, France.
(13)Department of Digestive and Hepatobiliary Surgery, Assistance 
Publique-Hôpitaux de Paris, Paul Brousse University Hospital, Paris, France.

BACKGROUND AND AIMS: Advances in artificial intelligence, particularly large 
language models (LLMs), hold promise for transforming chronic disease management 
such as Hepatitis C Virus (HCV) infection. This study evaluates the impact of 
retrieval-augmented generation (RAG) and supervised fine-tuning (SFT) on both 
open-ended question answering (accuracy and clarity) and on LLM-recommended 
treatment regimens for clinical scenarios.
METHODS: We employed OpenAI's GPT-4 Turbo in four configurations-baseline, 
RAG-Top1, RAG-Top 10 and SFT-using the 2020 EASL HCV guidelines as external 
knowledge or fine-tuning data. For the question set, guidelines were segmented 
at the paragraph level and encoded into 3072-dimensional embeddings. Fifteen 
questions covering general, patient and physician perspectives were scored on a 
10-point accuracy scale and binary accuracy/clarity by four experts. Separately, 
we created 25 simulated clinical scenarios; a consensus of four hepatologists 
defined the gold-standard DAA regimens. Model performance on these cases was 
measured by two metrics: 'partial accuracy' (≥ one correct DAA without errors) 
and 'complete accuracy' (all correct DAAs without errors).
RESULTS: On open-ended questions, RAG-Top10 outperformed baseline in accuracy 
(91.7% vs. 36.6%; p < 0.001) and clarity (91.7% vs. 46.6%; p < 0.001). RAG-Top1 
achieved 81.7% accuracy and 86.6% clarity (both p < 0.001), while SFT reached 
71.7% accuracy and 88.3% clarity (p < 0.001). Similarly, RAG-Top10 achieved the 
highest performance in prescribing the correct DAA regimen according to expert 
consensus in 76% of cases (vs. 24% for baseline model, p < 0.001).
CONCLUSIONS: Both RAG-Top10 and SFT markedly enhance LLM performance in 
guideline-driven HCV management-improving not only response accuracy and clarity 
but also DAA selection in clinical scenarios. RAG-Top10's broader context 
retrieval confers the greatest gains, while SFT underscores the value of 
domain-specific alignment. Rigorous, expert-informed evaluation frameworks are 
essential for the safe integration of LLMs into clinical practice.

© 2025 The Author(s). Liver International published by John Wiley & Sons Ltd.

DOI: 10.1111/liv.70349
PMCID: PMC12442523
PMID: 40960299 [Indexed for MEDLINE]

Conflict of interest statement: F.N.: Advisor for Gilead Sciences, IQVIA, 
AbbVie, Medicine Patents Pool, International Agency for Research on Cancer; 
Speaker for Gilead Sciences, Roche Diagnostics; Travel Grant from Gilead 
Sciences. A.A.: Advisory board fees from: Gilead, MSD, Abbvie, Ipsen; Speaker's 
fees from: Gilead, Abbvie, Ipsen. X.F.: acted as advisor for Gilead. The other 
authors declare no conflicts of interest with the present work. M.P.: Advisory 
Board/Speaker Bureau/Travel grants for Gilead Sciences, Abbvie, Merck, ViiV, 
GSK, Astra Zeneca, Infectofos, Pfizer, Menarini, Angelini. J.‐M.P.: advisor or 
speaker for Abbott, Abbvie, Gilead and GSK. None of the authors received 
consultancy fees from OpenAI.


145. Eur Arch Otorhinolaryngol. 2025 May;282(5):2669-2677. doi: 
10.1007/s00405-025-09255-6. Epub 2025 Feb 20.

ChatGPT 4.0 and algor in generating concept maps: an observational study.

Maniaci A(1)(2)(3)(4), Gagliano C(5), Salerno V(5), Cilia N(5), Lavalle S(5), 
Saibene AM(6)(7), Cammaroto G(6)(8), Chiesa-Estomba C(6)(9), Radulesco T(6)(10), 
Vaira L(6)(11), Iannella G(6)(12), Fakhry N(6)(13), Lechien JR(6)(14).

Author information:
(1)Department of Medicine and Surgery, University of Enna "Kore", Enna, 94100, 
Italy. antonino.maniaci@unikore.it.
(2)Study Group of Young-Otolaryngologists of the International Federations of 
Oto-rhino-laryngological Societies (YO-IFOS), Paris, France. 
antonino.maniaci@unikore.it.
(3)Department of Anatomy and Experimental Oncology, Mons School of Medicine, 
UMONS Research Institute for Health Sciences and Technology, University of Mons 
(UMons), Mons, 7000, Belgium. antonino.maniaci@unikore.it.
(4)School of Medicine University Enna Kore, Enna, Italy. 
antonino.maniaci@unikore.it.
(5)Department of Medicine and Surgery, University of Enna "Kore", Enna, 94100, 
Italy.
(6)Study Group of Young-Otolaryngologists of the International Federations of 
Oto-rhino-laryngological Societies (YO-IFOS), Paris, France.
(7)Otolaryngology Unit, Santi Paolo e Carlo Hospital, Department of Health 
Sciences, Università degli Studi di Milano, Milan, 26900, Italy.
(8)Department of Otolaryngology-Head and Neck Surgery, Forli Hospital, Forli, 
47122, Italy.
(9)Department of Otolaryngology-Head and Neck Surgery, San Sebastian University 
Hospital, San Sebastian, Spain.
(10)APHM, CNRS, IUSTI, La Conception University Hospital, ENT-HNS Department, 
Aix Marseille University, Marseille, 13005, France.
(11)Maxillofacial Surgery Operative Unit, Department of Medicine Surgery and 
Pharmacy, University of Sassari, Sassari, 07100, Italy.
(12)Department of 'Organi di Senso', University "Sapienza", Viale dell' 
Università, 33, Rome, 00185, Italy.
(13)Department of Otolaryngology and Head and Neck Surgery, Aix-Marseille 
University, AP-HM, La Conception 33 Hospital, Marseille, 13005, France.
(14)Department of Anatomy and Experimental Oncology, Mons School of Medicine, 
UMONS Research Institute for Health Sciences and Technology, University of Mons 
(UMons), Mons, 7000, Belgium.

BACKGROUND: To evaluate the performance of two AI systems, ChatGPT 4.0 and 
Algor, in generating concept maps from validated otolaryngology clinical 
practice guidelines.
METHODS: Concept maps were generated by ChatGPT 4.0 and Algor from four American 
Academy of Otolaryngology-Head and Neck Surgery Foundation (AAO-HNSF) clinical 
practice guidelines. Eight otolaryngology specialists evaluated the generated 
concept maps using the AI-Map questionnaire, covering concept identification, 
relationship establishment, hierarchical structure representation, and visual 
presentation. Chi-square tests and Kendall's tau coefficient were used for 
statistical analysis.
RESULTS: While no consistent superiority was observed across all guidelines, 
both AI systems demonstrated unique strengths. ChatGPT excelled in representing 
cross-connections between concepts and layout optimization, particularly for the 
Rhinoplasty guidelines (χ²=6.000, p = 0.050 for cross-connections). Algor showed 
strengths in capturing main themes and distinguishing general/abstract concepts, 
especially in the BPVV and Tympanostomy Tube guidelines (χ²=8.000, p = 0.046 for 
main themes in BPVV). Statistically significant differences were found in 
representing dynamic nature (favouring H&NMass-GPT, χ²=7.571, p = 0.023) and 
overall value and usefulness (favouring H&NMass-Algor, χ²=7.905, p = 0.019) for 
the H&N Masses guidelines.
CONCLUSION: AI systems showed potential in automating concept map creation from 
otolaryngology guidelines, with performance varying across different medical 
topics and evaluation criteria. Further research is required to optimize AI 
systems for medical education and knowledge representation, highlighting their 
promise and current limitations.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00405-025-09255-6
PMID: 39979628 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The author 
Jerome R. Lechien was not involved with the peer review process of this article.


146. J Law Med Ethics. 2025 Mar 27;53(1):1-6. doi: 10.1017/jme.2025.41. Online ahead 
of print.

The Need for Prospective Integrity Standards for the Use of Generative AI in 
Research.

Spector-Bagdady K(1).

Author information:
(1)University of Michigan, Ann Arbor, MI, United States.

The federal government has a long history of trying to find the right balance in 
supporting scientific and medical research while protecting the public and other 
researchers from potential harms. To date, this balance has been generally 
calibrated differently across contexts - including in clinical care, human 
subjects research, and research integrity. New challenges continue to face this 
disparate model of regulation, including novel Generative Artificial 
Intelligence (GenAI) tools. Because of potential increases in unintentional 
fabrication, falsification, and plagiarism using GenAI - and challenges 
establishing both these errors and intentionality in retrospect - this article 
argues that we should instead move toward a system that sets accepted community 
standards for the use of GenAI in research as prospective requirements.

DOI: 10.1017/jme.2025.41
PMCID: PMC12179527
PMID: 40143603


147. Front Psychiatry. 2025 Jun 6;16:1585702. doi: 10.3389/fpsyt.2025.1585702. 
eCollection 2025.

The effect of Artificial Intelligence Health Education Accurately Linking System 
on childhood asthma: study protocol for a pilot randomized controlled trial.

Wang HF(#)(1), Li YH(#)(2), Zhang Q(3), Han L(3), Wang L(3), Zhang L(4), Bai 
X(1), Cheng M(1), Zhang T(1), Zhao F(1), Li H(#)(5), Wang XY(#)(6).

Author information:
(1)Nursing Department, Inner Mongolia Maternal and Child Health Care Hospital, 
Hohhot, China.
(2)College of Education, Chengdu College of Arts and Sciences, Chengdu, China.
(3)Pediatric Internal Medicine Department, Inner Mongolia Maternal and Child 
Health Care Hospital, Hohhot, China.
(4)Pediatric Outpatient Department, Inner Mongolia Maternal and Child Health 
Care Hospital, Hohhot, China.
(5)Nursing Department, Shandong Provincial Hospital Affiliated to Shandong First 
Medical University, Jinan, China.
(6)Pediatric Department, Inner Mongolia Maternal and Child Health Care Hospital, 
Hohhot, China.
(#)Contributed equally

BACKGROUND: Childhood asthma is a prevalent chronic disease that affects 
millions of children worldwide. Managing this disease demands not only medical 
treatment but also the long-term self-management efforts of both children and 
their parents. Conventional self-management education typically depends on 
face-to-face approaches, often failing to take into account the personalized 
requirements and ongoing support needed. Nevertheless, with the evolution of 
artificial intelligence (AI) technology, fresh prospects have emerged to boost 
the effectiveness of self-management for childhood asthma. Based on it, we have 
designed an AI Health Education Accurately Linkage System (AI-HEALS) to explore 
whether AI-driven interventions can improve self-management capabilities of 
families with asthmatic children, thereby helping them control the disease and 
reduce medical costs.
METHODS: This research is a pilot single-blind randomized controlled trial (RCT) 
intended to gauge the efficacy of the AI-HEALS intervention delivered via the 
WeChat platform in enhancing the self-management abilities of families with 
asthmatic children. Participants will be recruited from eligible families whose 
children have been diagnosed with asthma and randomly allocated to either the 
intervention group or the control group. The control group will receive standard 
treatment, whereas the intervention group will receive both standard treatment 
and the AI-HEALS intervention. The intervention includes an AI-enabled, 
voice-activated interactive question-and-answer system, as well as monitoring 
and recording of physiological indicators, regular reminders, and customized 
educational articles. All components of the intervention will mainly be provided 
through a WeChat official account named "Children's Asthma Health Management 
Expert." AI-HEALS will construct its knowledge base according to pediatric 
asthma treatment guidelines to enhance the accuracy and reliability of the 
information it offers. The primary outcome measure is the alteration in asthma 
symptom control levels, while secondary outcomes comprise a variety of other 
physiological indicators related to asthma, parents' self-management behaviors, 
and mental health conditions.
DISCUSSION: This study combines AI and mobile health technology to develop the 
AI-HEALS system, with the aim of assisting families of children with asthma in 
controlling the disease symptoms. The primary objective is to evaluate whether 
the intervention can improve asthma symptom control.
CLINICAL TRIAL REGISTRATION: The study is scheduled to begin in April 2025 and 
is expected to conclude in December 2026. This research protocol is the first 
version and was registered with the China Clinical Trial Registration Center on 
February 14, 2025 (Registration Number: ChiCTR2500097233).

Copyright © 2025 Wang, Li, Zhang, Han, Wang, Zhang, Bai, Cheng, Zhang, Zhao, Li 
and Wang.

DOI: 10.3389/fpsyt.2025.1585702
PMCID: PMC12179151
PMID: 40547124

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


148. J Biomed Inform. 2025 Aug;168:104860. doi: 10.1016/j.jbi.2025.104860. Epub 2025 
May 28.

Do it faster with PICOS: Generative AI-Assisted systematic review screening.

Vallamchetla SK(1), Abdelkader O(1), Elnaggar A(2), Ramadan D(1), Islam Shourav 
MM(1), Riaz IB(3), Lin MP(4).

Author information:
(1)Department of Neurology, Mayo Clinic, Jacksonville, FL, USA.
(2)Zagazig University, Zagazig, Egypt.
(3)Division of Hematology and Medical Oncology, Mayo Clinic, Scottsdale, AZ, 
USA.
(4)Department of Neurology, Mayo Clinic, Jacksonville, FL, USA. Electronic 
address: lin.michelle@mayo.edu.

BACKGROUND: Systematic reviews (SRs) require substantial time and human 
resources, especially during the screening phase. Large Language Models (LLMs) 
have shown the potential to expedite screening. However, their use in generating 
structured PICOS (Population, Intervention/Exposure, Comparison, Outcome, Study 
design) summaries from title and abstract to assist human reviewers during 
screening remains unexplored.
OBJECTIVE: To assess the impact of open-source (Mistral-Nemo-Instruct-2407) 
LLM-generated structured PICOS summaries on the speed and accuracy of title and 
abstract screening.
METHODS: Four neurology trainees were grouped into two pairs based on previous 
screening experience. Pair A (A1, A2) consisted of less experienced trainees 
(1-2 SR), while Pair B (B1, B2) consisted of more experienced trainees (≥3 SR). 
Reviewers A1 and B1 received titles, abstracts, and LLM-generated structured 
PICOS summaries for each article. Reviewers A2 and B2 received only titles and 
abstracts. All reviewers independently screened the same set of 1,003 articles 
using predefined eligibility criteria. Screening times were recorded, and 
performance metrics were calculated.
RESULTS: PICOS-assisted reviewers screened significantly faster (A1: 116 min; 
B1: 90 min) than those without (A2: 463 min; B2: 370 min), with approximately 
75% reduction in screening workload. Sensitivity was perfect for PICOS-assisted 
reviewers (100%), whereas it was lower for those without assistance (88.0% and 
92.0%). Furthermore, PICOS-assisted reviewers demonstrated higher accuracy 
(99.9%), specificity (99.9), F1 scores (98.0%), and strong inter-rater 
reliability (Cohen's Kappa of 99.8%). Less experienced reviewer with PICOS 
assistance(A1) outperformed experienced reviewer(B2) without assistance in both 
efficiency and sensitivity.
CONCLUSION: LLM-generated PICOS summaries enhance the speed and accuracy of 
title and abstract screening by providing an additional layer of structured 
information. With PICOS assistance, less experienced reviewer surpassed their 
more experienced peers. Future research should explore the applicability of this 
novel method across diverse fields outside of neurology and its integration into 
fully automated systems.

Copyright © 2025. Published by Elsevier Inc.

DOI: 10.1016/j.jbi.2025.104860
PMID: 40447171 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


149. Plast Reconstr Surg Glob Open. 2025 Aug 11;13(8):e7057. doi: 
10.1097/GOX.0000000000007057. eCollection 2025 Aug.

Evaluating Artificial Intelligence's Role in Developing Research Questions in 
Head and Neck Reconstruction.

Holm S(1)(2), Zambrana M(1), Berner JE(3), Tabrisi R(1)(2), Landström 
F(1)(2)(4), Hanoon D(1), Zdolsek J(1)(2).

Author information:
(1)From the Department of Plastic and Reconstructive Surgery, Örebro University 
Hospital, Örebro, Sweden.
(2)Faculty of Medicine and Health, Örebro University, Örebro, Sweden.
(3)Division of Plastic Surgery, Department of Surgery, University of Texas 
Health Science Center at San Antonio, San Antonio, TX.
(4)Department of Otorhinolaryngology, Örebro University Hospital, Örebro, 
Sweden.

Generative artificial intelligence (AI) large language models are an emerging 
technology, with ChatGPT and Gemini being 2 well-known examples. The current 
literature discusses clinical applications and limitations of AI, but its role 
in research has not yet been extensively evaluated. This study aimed to assess 
the role of ChatGPT and Gemini in developing novel and clinically relevant 
research ideas (RIs) for systematic reviews (SRs) in head and neck 
reconstruction. ChatGPT and Gemini were prompted to provide 10 novel and 
clinically relevant RIs for SRs in the following domains: head and neck 
reconstruction in general, microsurgery, and complications in reconstructive 
head and neck procedures. A comprehensive search was then performed for SRs in 
MEDLINE, Cochrane Library, and Embase to determine the novelty of the RIs 
generated. A total of 60 RIs were generated, with half created by ChatGPT and 
the other half by Gemini. Overall, 3613 entries were found through the 
literature search. After deduplication and screening, a total of 50 studies that 
partially addressed the AI-generated RIs were identified and were included in 
the present review. Out of the 60 AI-generated RIs, 42 had not been previously 
studied and were therefore considered novel. No statistically significant 
differences were found between the outputs generated by Gemini and ChatGPT. Both 
ChatGPT and Gemini were able to effectively generate novel and clinically 
relevant RIs for SRs, although their suggestions were generally broad. This 
study demonstrated that AI could potentially aid in the process of conducting 
novel SRs.

Copyright © 2025 The Authors. Published by Wolters Kluwer Health, Inc. on behalf 
of The American Society of Plastic Surgeons.

DOI: 10.1097/GOX.0000000000007057
PMCID: PMC12333708
PMID: 40791742

Conflict of interest statement: The authors have no financial interest to 
declare in relation to the content of this article.


150. Mov Disord. 2025 May;40(5):906-916. doi: 10.1002/mds.30168. Epub 2025 Mar 22.

Artificial Intelligence-Based Virtual Assistant for the Diagnostic Approach of 
Chronic Ataxias.

Alessandro L(1)(2), Bianciotti N(3), Salama L(3), Volmaro S(3), Navarrine V(3), 
Ameghino L(4), Arena J(4), Bestoso S(5), Bruno V(6), Castillo Torres S(7), 
Chamorro M(8), Couto B(9), De La Riestra T(10), Echeverria F(11), Genco J(12), 
Gonzalez Del Boca F(13), Guarnaschelli M(14), Giugni JC(15), Laffue A(1), 
Martinez Villota V(16), Medina Escobar A(17), Paez Maggio M(18), Rauek S(19), 
Rodriguez Quiroga S(20), Tela M(4), Villa C(21), Sanguinetti O(22), Kauffman 
M(23)(24), Fernandez Slezak D(2)(25)(26), Farez MF(2)(27), Rossi M(4)(28).

Author information:
(1)Departmento de Neurologia, Fleni, Buenos Aires, Argentina.
(2)Entelai, Buenos Aires, Argentina.
(3)Facultad de Medicina, Universidad de Buenos Aires, Buenos Aires, Argentina.
(4)Sección de Movimientos Anormales, Departamento de Neurología, Fleni, Buenos 
Aires, Argentina.
(5)Sección Parkinson y Trastornos del Movimiento del Hospital Italiano Buenos 
Aires (HIBA).
(6)Department of Clinical Neurosciences, University of Calgary Hotchkiss Brain 
Institute, Calgary, Alberta, Canada.
(7)Servicio de Neurología, Hospital Universitario Dr. Jose Eleuterio Gonzalez, 
Universidad Autónoma de Nuevo Leon, Monterrey, Mexico.
(8)Sanatorio Parque, Servicio de Neurología. INECO Neurociencias Oroño. Clínica 
de Movimientos Anormales, Unidad de DBS, Rosario, Argentina.
(9)Instituto de Neurociencia Cognitiva y Traslacional (INECO-CONICET-Favaloro), 
Ciudad de Buenos Aires, Argentina.
(10)Ineco Neurociencias Oroño, Rosario, Argentina.
(11)Hospital Privado de Rosario, Rosario, Argentina.
(12)Consultorio de Trastornos del Movimiento, Servicio de Neurología y 
Neurocirugía, Hospital Luis Carlos Lagomaggiore, Mendoza, Argentina.
(13)Departamento de Movimientos Anormales, Sanatorio Allende, Servicio de 
Neurología, Córdoba, Argentina.
(14)Facultad de Ciencias de la Salud, Universidad Adventista del Plata, 
Libertador de San Martin, Entre Rios, Argentina.
(15)Hospital Dr. Guillermo Rawson, San Juan, Argentina.
(16)Servicio de Neurología Clínica, Hospital Universitario Departamental de 
Nariño, Pasto, Colombia.
(17)Moncton Interdisciplinary Neurodegenerative Diseases Clinic, Horizon Health 
Network, Moncton, New Brunswick, Canada.
(18)Seccion de Movimientos anormales. Departamento de Neurología, Hospital 
Britanico, Buenos Aires, Argentina.
(19)Hospital Universitario de Mendoza, Mendoza, Argentina.
(20)Departamento de Neurología Hospital J.M. Ramos Mejia, Unidad de Movimientos 
Anormales y Neurogenética, Buenos Aires, Argentina.
(21)Hospital San Bernardo, Salta, Argentina.
(22)Instituto Tecnológico de Buenos Aires, Buenos Aires, Argentina.
(23)IIMT-FCB-Universidad Austral-CONICET, Buenos Aires, Argentina.
(24)Consultorio y Laboratorio de Neurogenética Hospital J.M Ramos Mejia, Buenos 
Aires, Argentina.
(25)Departamento de Computación, Facultad de Ciencias Exactas y Naturales, 
Universidad de Buenos Aires, Buenos Aires, Argentina.
(26)Instituto de Investigacion en Ciencias de la Computación, CONICET-UBA, 
Buenos Aires, Argentina.
(27)Centro de Investigación en Enfermedades Neuroinmunológicas (CIEN), Buenos 
Aires, Argentina.
(28)Instituto Fleni-CONICET (INEU) Buenos Aire, Buenos Aires, Argentina.

BACKGROUND: Chronic ataxias, a complex group of over 300 diseases, pose 
significant diagnostic challenges because of their clinical and genetic 
heterogeneity. Here, we propose that artificial intelligence (AI) can aid in the 
identification and understanding of these disorders through the utilization of a 
smart virtual assistant.
OBJECTIVES: The aim is to develop and validate an AI-powered virtual assistant 
for diagnosing chronic ataxias.
METHODS: A non-commercial virtual assistant was developed using advanced 
algorithms, decision trees, and large language models. In the validation 
process, 453 clinical cases from the literature were selected from 151 causes of 
chronic ataxia. The diagnostic accuracy was compared with that of 21 
neurologists specializing in movement disorders and GPT-4. Usability regarding 
time and number of questions needed were also evaluated.
RESULTS: The virtual assistant accuracy was 90.9%, higher than neurologists 
(18.3%), and GPT-4 (19.4%). It also significantly outperformed in causes of 
ataxia distributed by age, inheritance, frequency, associated clinical 
manifestations, and treatment availability. Neurologists and GPT-4 mentioned 110 
incorrect diagnoses, 83.6% of which were made by GPT-4, which also generated 
seven data hallucinations. The virtual assistant required an average of 14 
questions and 1.5 minutes to generate a list of differential diagnoses, 
significantly faster than the neurologists (mean, 19.4 minutes).
CONCLUSIONS: The virtual assistant proved to be accurate and easy fast-use for 
the diagnosis of chronic ataxias, potentially serving as a support tool in 
neurological consultation. This diagnostic approach could also be expanded to 
other neurological and non-neurological diseases. © 2025 International Parkinson 
and Movement Disorder Society.

© 2025 International Parkinson and Movement Disorder Society.

DOI: 10.1002/mds.30168
PMID: 40119570 [Indexed for MEDLINE]


151. J Med Internet Res. 2025 Apr 30;27:e64486. doi: 10.2196/64486.

Accuracy of Large Language Models When Answering Clinical Research Questions: 
Systematic Review and Network Meta-Analysis.

Wang L(#)(1)(2), Li J(#)(2), Zhuang B(#)(3), Huang S(#)(4), Fang M(#)(2), Wang 
C(2), Li W(1), Zhang M(2), Gong S(5).

Author information:
(1)Fuzhou University Affiliated Provincial Hospital, Shengli Clinical Medical 
College, Fujian Medical University, Fuzhou, China.
(2)School of Pharmacy, Fujian Medical University, Fuzhou, China.
(3)Fujian Center For Drug Evaluation and Monitoring, Fuzhou, China.
(4)School of Pharmacy, Fujian University of Traditional Chinese Medicine, 
Fuzhou, China.
(5)The Third Department of Critical Care Medicine, Fuzhou University Affiliated 
Provincial Hospital, Shengli Clinical Medical College, Fujian Medical 
University, Fuzhou, Fujian, China.
(#)Contributed equally

BACKGROUND: Large language models (LLMs) have flourished and gradually become an 
important research and application direction in the medical field. However, due 
to the high degree of specialization, complexity, and specificity of medicine, 
which results in extremely high accuracy requirements, controversy remains about 
whether LLMs can be used in the medical field. More studies have evaluated the 
performance of various types of LLMs in medicine, but the conclusions are 
inconsistent.
OBJECTIVE: This study uses a network meta-analysis (NMA) to assess the accuracy 
of LLMs when answering clinical research questions to provide high-level 
evidence-based evidence for its future development and application in the 
medical field.
METHODS: In this systematic review and NMA, we searched PubMed, Embase, Web of 
Science, and Scopus from inception until October 14, 2024. Studies on the 
accuracy of LLMs when answering clinical research questions were included and 
screened by reading published reports. The systematic review and NMA were 
conducted to compare the accuracy of different LLMs when answering clinical 
research questions, including objective questions, open-ended questions, top 1 
diagnosis, top 3 diagnosis, top 5 diagnosis, and triage and classification. The 
NMA was performed using Bayesian frequency theory methods. Indirect 
intercomparisons between programs were performed using a grading scale. A larger 
surface under the cumulative ranking curve (SUCRA) value indicates a higher 
ranking of the corresponding LLM accuracy.
RESULTS: The systematic review and NMA examined 168 articles encompassing 35,896 
questions and 3063 clinical cases. Of the 168 studies, 40 (23.8%) were 
considered to have a low risk of bias, 128 (76.2%) had a moderate risk, and none 
were rated as having a high risk. ChatGPT-4o (SUCRA=0.9207) demonstrated strong 
performance in terms of accuracy for objective questions, followed by 
Aeyeconsult (SUCRA=0.9187) and ChatGPT-4 (SUCRA=0.8087). ChatGPT-4 
(SUCRA=0.8708) excelled at answering open-ended questions. In terms of accuracy 
for top 1 diagnosis and top 3 diagnosis of clinical cases, human experts 
(SUCRA=0.9001 and SUCRA=0.7126, respectively) ranked the highest, while Claude 3 
Opus (SUCRA=0.9672) performed well at the top 5 diagnosis. Gemini (SUCRA=0.9649) 
had the highest rated SUCRA value for accuracy in the area of triage and 
classification.
CONCLUSIONS: Our study indicates that ChatGPT-4o has an advantage when answering 
objective questions. For open-ended questions, ChatGPT-4 may be more credible. 
Humans are more accurate at the top 1 diagnosis and top 3 diagnosis. Claude 3 
Opus performs better at the top 5 diagnosis, while for triage and 
classification, Gemini is more advantageous. This analysis offers valuable 
insights for clinicians and medical practitioners, empowering them to 
effectively leverage LLMs for improved decision-making in learning, diagnosis, 
and management of various clinical scenarios.
TRIAL REGISTRATION: PROSPERO CRD42024558245; 
https://www.crd.york.ac.uk/PROSPERO/view/CRD42024558245.

©Ling Wang, Jinglin Li, Boyang Zhuang, Shasha Huang, Meilin Fang, Cunze Wang, 
Wen Li, Mohan Zhang, Shurong Gong. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org), 30.04.2025.

DOI: 10.2196/64486
PMCID: PMC12079073
PMID: 40305085 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


152. Int J Gynaecol Obstet. 2025 Feb;168(2):419-427. doi: 10.1002/ijgo.15869. Epub 
2024 Aug 20.

Exploring the role of artificial intelligence, large language models: Comparing 
patient-focused information and clinical decision support capabilities to the 
gynecologic oncology guidelines.

Reicher L(1)(2)(3)(4), Lutsker G(3)(4), Michaan N(1)(2), Grisaru D(1)(2), Laskov 
I(1)(2).

Author information:
(1)Department of Gynecologic Oncology, Lis Hospital for Women, Tel Aviv Medical 
Center, Tel Aviv, Israel.
(2)Sackler School of Medicine, Department of Gynecology, Tel Aviv University, 
Tel Aviv, Israel.
(3)Department of Molecular Cell Biology, Weizmann Institute of Science, Rehovot, 
Israel.
(4)Department of Computer Science and Applied Mathematics, Weizmann Institute of 
Science, Rehovot, Israel.

Gynecologic cancer requires personalized care to improve outcomes. Large 
language models (LLMs) hold the potential to provide intelligent 
question-answering with reliable information about medical queries in clear and 
plain English, which can be understood by both healthcare providers and 
patients. We aimed to evaluate two freely available LLMs (ChatGPT and Google's 
Bard) in answering questions regarding the management of gynecologic cancer. The 
LLMs' performances were evaluated by developing a set questions that addressed 
common gynecologic oncologic findings from a patient's perspective and more 
complex questions to elicit recommendations from a clinician's perspective. Each 
question was presented to the LLM interface, and the responses generated by the 
artificial intelligence (AI) model were recorded. The responses were assessed 
based on the adherence to the National Comprehensive Cancer Network and European 
Society of Gynecological Oncology guidelines. This evaluation aimed to determine 
the accuracy and appropriateness of the information provided by LLMs. We showed 
that the models provided largely appropriate responses to questions regarding 
common cervical cancer screening tests and BRCA-related questions. Less useful 
answers were received to complex and controversial gynecologic oncology cases, 
as assessed by reviewing the common guidelines. ChatGPT and Bard lacked 
knowledge of regional guideline variations, However, it provided practical and 
multifaceted advice to patients and caregivers regarding the next steps of 
management and follow up. We conclude that LLMs may have a role as an adjunct 
informational tool to improve outcomes.

© 2024 The Author(s). International Journal of Gynecology & Obstetrics published 
by John Wiley & Sons Ltd on behalf of International Federation of Gynecology and 
Obstetrics.

DOI: 10.1002/ijgo.15869
PMCID: PMC11726133
PMID: 39161265 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no conflicts of interest.


153. J Clin Epidemiol. 2025 Oct;186:111903. doi: 10.1016/j.jclinepi.2025.111903. Epub 
2025 Jul 18.

Lack of methodological rigor and limited coverage of generative artificial 
intelligence in existing artificial intelligence reporting guidelines: a scoping 
review.

Luo X(1), Wang B(1), Shi Q(2), Wang Z(1), Lai H(3), Liu H(1), Qin Y(1), Chen 
F(4), Song X(3), Ge L(3), Zhang L(5), Bian Z(6), Chen Y(7); ADVANCED working 
group.

Collaborators: He H, Wang Y, Li H, Zhang H, Zhu D, Yao Y, Peng D, Li Z, Zhang J, 
Qin Y, Wang F, Tang Z, Li Y, Liu H, Zhao J.

Author information:
(1)Evidence-Based Medicine Center, School of Basic Medical Sciences, Lanzhou 
University, Lanzhou, China; Research Unit of Evidence-Based Evaluation and 
Guidelines, Chinese Academy of Medical Sciences (2021RU017), School of Basic 
Medical Sciences, Lanzhou University, Lanzhou, China; World Health Organization 
Collaboration Center for Guideline Implementation and Knowledge Translation, 
Lanzhou, China; Institute of Health Data Science, Lanzhou University, Lanzhou, 
China; Key Laboratory of Evidence Based Medicine of Gansu Province, Lanzhou 
University, Lanzhou, China.
(2)The First School of Clinical Medicine, Lanzhou University, Lanzhou, China.
(3)Department of Health Policy and Health Management, School of Public Health, 
Lanzhou University, Lanzhou, China; Evidence-Based Social Science Research 
Center, School of Public Health, Lanzhou University, Lanzhou, China.
(4)School of Information Science & Engineering, Lanzhou University, Lanzhou, 
China.
(5)Department of Computer Science, Hong Kong Baptist University, Hongkong, 
China.
(6)Vincent V.C. Woo Chinese Medicine Clinical Research Institute, School of 
Chinese Medicine, Hong Kong Baptist University, Hong Kong, China; Chinese 
EQUATOR Centre, Hong Kong, China.
(7)Evidence-Based Medicine Center, School of Basic Medical Sciences, Lanzhou 
University, Lanzhou, China; Research Unit of Evidence-Based Evaluation and 
Guidelines, Chinese Academy of Medical Sciences (2021RU017), School of Basic 
Medical Sciences, Lanzhou University, Lanzhou, China; World Health Organization 
Collaboration Center for Guideline Implementation and Knowledge Translation, 
Lanzhou, China; Institute of Health Data Science, Lanzhou University, Lanzhou, 
China; Key Laboratory of Evidence Based Medicine of Gansu Province, Lanzhou 
University, Lanzhou, China. Electronic address: chevidence@lzu.edu.cn.

OBJECTIVES: This study aimed to systematically map the development methods, 
scope, and limitations of existing artificial intelligence (AI) reporting 
guidelines in medicine and to explore their applicability to generative AI (GAI) 
tools, such as large language models (LLMs).
STUDY DESIGN AND SETTING: We reported a scoping review adhering to the Preferred 
Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping 
Reviews. Five information sources were searched, including MEDLINE (via PubMed), 
Enhancing the QUAlity and Transparency Of health Research (EQUATOR) Network, 
China National Knowledge Infrastructure, FAIRsharing, and Google Scholar, from 
inception to December 31, 2024. Two reviewers independently screened records and 
extracted data using a predefined Excel template. Data included guideline 
characteristics (eg, development methods, target audience, AI domain), adherence 
to EQUATOR Network recommendations, and consensus methodologies. Discrepancies 
were resolved by a third reviewer.
RESULTS: Sixty-eight AI reporting guidelines were included; 48.5% focused on 
general AI, whereas only 7.4% addressed GAI/LLMs. Methodological rigor was 
limited; 39.7% described development processes, 42.6% involved multidisciplinary 
experts, and 33.8% followed EQUATOR recommendations. Significant overlap 
existed, particularly in medical imaging (20.6% of guidelines). GAI-specific 
guidelines (14.7%) lacked comprehensive coverage and methodological 
transparency.
CONCLUSION: Existing AI reporting guidelines in medicine have suboptimal 
methodological rigor, redundancy, and insufficient coverage of GAI applications. 
Future and updated guidelines should prioritize standardized development 
processes, multidisciplinary collaboration, and expanded focus on emerging AI 
technologies like LLMs.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jclinepi.2025.111903
PMID: 40684889 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest There are no 
competing interests for any authors.


154. J Craniofac Surg. 2025 Aug 14. doi: 10.1097/SCS.0000000000011779. Online ahead 
of print.

Comparing GPT-4o and o1 in Otolaryngology: An Evaluation of Guideline Adherence 
and Accuracy.

Prasad S(1), DiStefano N, Khuu N, Deluca N, Thaller S.

Author information:
(1)Department of Otolaryngology, University of Miami Miller School of Medicine, 
Miami, FL.

Artificial-intelligence chatbots are gaining prominence in otolaryngology, yet 
their clinical safety depends on strict adherence to practice guidelines. The 
authors compared the accuracy of OpenAI's general-purpose GPT-4o model with the 
specialty-tuned o1 model on 100 otolaryngology questions drawn from national 
guidelines and common clinical scenarios spanning 7 subspecialty domains. 
Blinded otolaryngologists graded each answer as correct, partially correct, 
incorrect, or non-answer (scores 1, 0.5, 0, respectively), and paired 
statistical tests assessed performance differences. The o1 model delivered fully 
correct responses for 73% of questions, partially correct for 26%, and incorrect 
for 1%, yielding a mean accuracy score of 0.86. GPT-4o produced 64% correct and 
36% partially correct answers with no incorrect responses, for a mean score of 
0.82. The 4-point gap was not statistically significant (paired t test P=0.165; 
Wilcoxon P=0.157). Pediatric questions had the highest correctness (o1=92.9%, 
GPT-4o=78.6%). No domain showed systematic critical errors. Both models thus 
supplied predominantly guideline-concordant information, and specialty tuning 
conferred only a modest, nonsignificant benefit in this data set. These findings 
suggest contemporary large-language models may approach reliability thresholds 
suitable for supervised decision support in otolaryngology, but continual 
validation and oversight remain essential before routine deployment.

Copyright © 2025 by Mutaz B. Habal, MD.

DOI: 10.1097/SCS.0000000000011779
PMID: 40811069

Conflict of interest statement: The authors report no conflicts of interest.


155. BMC Med Inform Decis Mak. 2025 Nov 14;25(1):423. doi: 
10.1186/s12911-025-03258-3.

Can general purpose large language models assist pediatricians in predicting 
infants with serious bacterial infection?

Šimunović I(1), Rezić K(1), Franić N(2), Boduljak G(3), Batinić M(4), Jukić 
I(4), Jelovina I(4), Biočić J(1)(4), Pogorelić Z(5)(6), Markić J(7)(8).

Author information:
(1)School of Medicine, University of Split, Split, 21000, Croatia.
(2)Faculty of Electrical Engineering, Mechanical Engineering and Naval 
Architecture, University of Split, Split, 21000, Croatia.
(3)Department of Computer Science, University of Oxford, Oxford, UK.
(4)Department of Pediatrics, University Hospital of Split, Split, 21000, 
Croatia.
(5)School of Medicine, University of Split, Split, 21000, Croatia. 
zenon.pogorelic@mefst.hr.
(6)Department of Pediatric Surgery, University Hospital of Split, Split, 21000, 
Croatia. zenon.pogorelic@mefst.hr.
(7)School of Medicine, University of Split, Split, 21000, Croatia. 
jmarkic@mefst.hr.
(8)Department of Pediatrics, University Hospital of Split, Split, 21000, 
Croatia. jmarkic@mefst.hr.

BACKGROUND: Serious Bacterial Infection (SBI) in neonates and young infants 
often exhibit nonspecific symptoms and clinical signs in the early stages of 
illness, making early diagnosis challenging. Timely recognition and appropriate 
treatment are essential to prevent adverse outcomes. While several clinical 
algorithms are widely used for SBI risk stratification, these tools have 
limitations, particularly low positive predictive value. This study evaluates 
the diagnostic accuracy of general-purpose large language models (LLMs) in 
detecting SBI in neonates and infants under 90 days of age admitted to the 
emergency department. Our objective is to improve diagnostic precision, reduce 
unnecessary interventions, and enhance patient outcomes. LLM performance was 
compared against traditional machine learning models, state-of-the-art 
rule-based methods, and an ensemble of physicians to assess their potential as 
clinical decision-support tools in scenarios of diagnostic uncertainty.
RESULTS: On a dataset of 742 patients, LLMs demonstrated diagnostic accuracy 
comparable to traditional machine learning models and state-of-the-art 
rule-based methods. The optimized CatBoost (class-weighted) model achieved the 
best overall performance, with a PPV of 0.70, NPV of 0.90, sensitivity of 0.54, 
specificity of 0.95, F1-score of 0.60, and MCC of 0.54, outperforming the 
baseline CatBoost model and achieving results on par with large language models 
(LLMs) and physicians. When optimally prompted, LLMs performed on par with 
ensembles of experienced clinicians. Additionally, LLMs exhibited effective 
medical reasoning and provided credible diagnostic predictions, particularly 
valuable in cases of clinician uncertainty. The models achieved balanced 
performance across multiple evaluation metrics, including PPV, NPV, sensitivity, 
specificity, F1-score, and Matthew’s correlation coefficient (MCC). ChatGPT-4o 
achieved a sensitivity of 0.65 and specificity of 0.83, with an MCC of 0.41. 
Claude Sonnet 3.5 reached a sensitivity of 0.60 and specificity of 0.86, MCC 
0.42 and Google Gemini 2.0 Flash had lower sensitivity (0.43) but the highest 
specificity (0.94), with an MCC of 0.43. In comparison, the best-performing 
individual pediatrician achieved a higher sensitivity (0.74) but lower 
specificity (0.68), with an MCC of 0.33, while the pediatricians’ majority vote 
yielded sensitivity of 0.69, specificity of 0.81, and MCC of 0.43 — comparable 
to the top-performing LLMs.
CONCLUSIONS: These Artificial intelligence tools offer a promising direction for 
SBI risk prediction, achieving performance comparable to that of experienced 
pediatric specialists, while maintaining simplicity of use/data-preprocessing 
for potential real-world applications.
SUPPLEMENTARY INFORMATION: The online version contains supplementary material 
available at 10.1186/s12911-025-03258-3.

DOI: 10.1186/s12911-025-03258-3
PMCID: PMC12619361
PMID: 41239388

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The study was conducted in full compliance with the ethical 
principles set forth in the Declaration of Helsinki, the foundational guideline 
of the World Medical Association for research involving human participants. 
Written consent to participate in the study was obtained from all parents or 
legal guardians of the patients. This research was approved by the Ethical 
Committee of the University Hospital of Split, Croatia, ensuring compliance with 
ethical standards and regulations for medical research (approval number: 
2181 − 147/01–06/LJ.Z.-24-02, Date of Approval: 30 August 2024). Consent for 
publication: Not applicable. Competing interests: The authors declare no 
competing interests.


156. J Assist Reprod Genet. 2025 Nov;42(11):3565-3579. doi: 
10.1007/s10815-025-03679-z. Epub 2025 Sep 25.

The potential of large language models in the field of infertility: a systematic 
review.

Li W(1), Ur-Rehman A(1), Ge MW(1), Shen LT(1), Peng XY(1), Zhong K(1), Feng 
R(1), Gao S(1), Hu FH(1), Jia YJ(1), Chen HL(2).

Author information:
(1)School of Nursing and Rehabilitation, Nantong University, Nantong, Jiangsu, 
PR China.
(2)School of Nursing and Rehabilitation, Nantong University, Nantong, Jiangsu, 
PR China. honglinyjs@126.com.

PURPOSE: Large language models have demonstrated significant potential across a 
wide range of medical fields and may potentially induce a transformation in the 
field of infertility and sterility. This systematic review aims to synthesize 
existing research and explore the current strengths and limitations of large 
language models in the field of infertility and sterility.
METHODS: Researchers conducted a comprehensive search across three databases and 
employed a thematic synthesis approach for data analysis.
RESULTS: The analysis included a total of 13 studies. Large language models 
exhibited advantages in the accuracy and reproducibility of information output 
and consultation, demonstrated robust learning capabilities, and were able to 
provide satisfactory recommendations to patients. However, there were 
significant variations in the performance of different large language models, 
and the readability of the output information was poor, making it difficult to 
provide comprehensive answers.
CONCLUSION: In the domain of infertility, large language models have not yet 
achieved full reliability and should be regarded solely as information sources 
whose outputs require careful verification; they are presently incapable of 
substituting for clinical diagnosis. Their development requires increased 
investment in relevant technologies and the use of authoritative, accurate 
information to unlock their potential in this field and other medical areas.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10815-025-03679-z
PMCID: PMC12640383
PMID: 40999290 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare no competing interests.


157. JMIR Med Educ. 2025 May 12;11:e58801. doi: 10.2196/58801.

Global Health care Professionals' Perceptions of Large Language Model Use In 
Practice: Cross-Sectional Survey Study.

Ozkan E(1), Tekin A(2), Ozkan MC(1), Cabrera D(3), Niven A(4), Dong Y(2).

Author information:
(1)Department of Medicine, Jersey Shore University Medical Center, 1945 NJ-33, 
Neptune, NJ, 07753, United States, 1 5078843064.
(2)Department of Anesthesiology, Mayo Clinic College of Medicine, Rochester, MN, 
United States.
(3)Department of Emergency Medicine, Mayo Clinic College of Medicine, Rochester, 
MN, United States.
(4)Department of Pulmonary and Critical Care Medicine, Mayo Clinic College of 
Medicine, Rochester, MN, United States.

BACKGROUND: ChatGPT is a large language model-based chatbot developed by OpenAI. 
ChatGPT has many potential applications to health care, including enhanced 
diagnostic accuracy and efficiency, improved treatment planning, and better 
patient outcomes. However, health care professionals' perceptions of ChatGPT and 
similar artificial intelligence tools are not well known. Understanding these 
attitudes is important to inform the best approaches to exploring their use in 
medicine.
OBJECTIVE: Our aim was to evaluate the health care professionals' awareness and 
perceptions regarding potential applications of ChatGPT in the medical field, 
including potential benefits and challenges of adoption.
METHODS: We designed a 33-question online survey that was distributed among 
health care professionals via targeted emails and professional Twitter and 
LinkedIn accounts. The survey included a range of questions to define 
respondents' demographic characteristics, familiarity with ChatGPT, perceptions 
of this tool's usefulness and reliability, and opinions on its potential to 
improve patient care, research, and education efforts.
RESULTS: One hundred and fifteen health care professionals from 21 countries 
responded to the survey, including physicians, nurses, researchers, and 
educators. Of these, 101 (87.8%) had heard of ChatGPT, mainly from peers, social 
media, and news, and 77 (76.2%) had used ChatGPT at least once. Participants 
found ChatGPT to be helpful for writing manuscripts (n=31, 45.6%), emails (n=25, 
36.8%), and grants (n=12, 17.6%); accessing the latest research and 
evidence-based guidelines (n=21, 30.9%); providing suggestions on diagnosis or 
treatment (n=15, 22.1%); and improving patient communication (n=12, 17.6%). 
Respondents also felt that the ability of ChatGPT to access and summarize 
research articles (n=22, 46.8%), provide quick answers to clinical questions 
(n=15, 31.9%), and generate patient education materials (n=10, 21.3%) was 
helpful. However, there are concerns regarding the use of ChatGPT, for example, 
the accuracy of responses (n=14, 29.8%), limited applicability in specific 
practices (n=18, 38.3%), and legal and ethical considerations (n=6, 12.8%), 
mainly related to plagiarism or copyright violations. Participants stated that 
safety protocols such as data encryption (n=63, 62.4%) and access control (n=52, 
51.5%) could assist in ensuring patient privacy and data security.
CONCLUSIONS: Our findings show that ChatGPT use is widespread among health care 
professionals in daily clinical, research, and educational activities. The 
majority of our participants found ChatGPT to be useful; however, there are 
concerns about patient privacy, data security, and its legal and ethical issues 
as well as the accuracy of its information. Further studies are required to 
understand the impact of ChatGPT and other large language models on clinical, 
educational, and research outcomes, and the concerns regarding its use must be 
addressed systematically and through appropriate methods.

© Ecem Ozkan, Aysun Tekin, Mahmut Can Ozkan, Daniel Cabrera, Alexander Niven, 
Yue Dong. Originally published in JMIR Medical Education 
(https://mededu.jmir.org).

DOI: 10.2196/58801
PMCID: PMC12088617
PMID: 40354644 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


158. Int J Med Inform. 2025 Nov;203:106047. doi: 10.1016/j.ijmedinf.2025.106047. Epub 
2025 Jul 12.

AI in primary care: Comparing ChatGPT and family physicians on patient queries.

İnan M(1), Suvak Ö(2), Aypak C(2).

Author information:
(1)Department of Family Medicine, Ankara Etlik City Hospital, Ankara, Türkiye. 
Electronic address: muhammedinan.md@gmail.com.
(2)Department of Family Medicine, Ankara Etlik City Hospital, Ankara, Türkiye.

OBJECTIVE: The integration of artificial intelligence (AI) in medicine has led 
to growing interest in its applications for primary care. This study evaluates 
and compares the responses of ChatGPT-4o and family physicians to 200 commonly 
asked clinical questions in family medicine.
METHODS: This was a comparative, observational, cross-sectional study was 
conducted using a dataset of 200 primary care-related questions generated 
through literature review and expert validation. Three experienced family 
physicians and ChatGPT-4o independently provided responses. The responses were 
anonymized and randomly assessed by three independent family medicine experts. 
Evaluations were based on Likert scales for appropriateness (1-6), accuracy 
(1-6), comprehensiveness (1-3), and empathy (1-5). Word counts were also 
recorded.
RESULTS: ChatGPT-4o outperformed family physicians across all evaluation metrics 
(p < 0.01). ChatGPT-4o received higher scores for appropriateness (mean 
5.8 ± 0.5 vs. 4.3 ± 1.0), accuracy (5.8 ± 0.5 vs. 4.5 ± 1.1), comprehensiveness 
(2.4 ± 0.6 vs. 1.4 ± 0.7). and empathy (4.8 ± 0.4 vs. 4.0 ± 0.8). The average 
word count of ChatGPT's responses (298.8 ± 82.3 words) was significantly longer 
than that of physicians (106.1 ± 95.0 words). In topic-specific analysis, 
ChatGPT-4o outperformed physicians, except in General Consultation and Child 
Infections (p = 0.07, 0.08 respectively).
CONCLUSION: The findings suggest that ChatGPT-4o has the potential to enhance 
patient education, medical training, and clinical decision support. Future 
research should explore AI's real-world clinical impact, its role in improving 
medical education, and strategies to refine AI-generated responses for 
conciseness and cultural relevance.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106047
PMID: 40664020 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


159. Sports Med. 2025 Oct 3. doi: 10.1007/s40279-025-02303-5. Online ahead of print.

"Can We Trust Them?" An Expert Evaluation of Large Language Models to Provide 
Sleep and Jet Lag Recommendations for Athletes.

Vitale J(1)(2), McCall A(3)(4)(5), Cina A(6)(7); Athlete Travel, Sleep Interest 
Group (ATSIG); van Rensburg DCJ(#)(8)(9), Halson S(#)(10)(11).

Collaborators: Alonso JM, Bender A, Biggins M, Charest J, Dohi M, Filip-Stachnik 
A, Fullagar H, Grandner M, Gupta L, Lastella M, Miller D, Nedelec M, van 
Rensburg AJ, Roach GD, Rossiter A, Sargent C, Skorski S.

Author information:
(1)Schulthess Klinik, Lengghalde 2, 8008, Zurich, Switzerland. 
jacopo.vitale@kws.ch.
(2)Institute of Sports Sciences, University of Physical Culture in Cracow, 
31-571, Cracow, Poland. jacopo.vitale@kws.ch.
(3)School of Applied Sciences, Edinburgh Napier University, Edinburgh, UK.
(4)FIFA Medical, Fédération Internationale de Football Association, Zurich, 
Switzerland.
(5)School of Sport, Exercise and Rehabilitation, University of Technology 
Sydney, Sydney, NSW, Australia.
(6)Schulthess Klinik, Lengghalde 2, 8008, Zurich, Switzerland.
(7)ETH Zurich, Zurich, Switzerland.
(8)Section Sports Medicine and SEMLI, Faculty of Health Sciences, University of 
Pretoria, Pretoria, South Africa.
(9)Medical Advisory Panel, World Netball, Manchester, UK.
(10)School of Behavioural and Health Sciences, Australian Catholic University, 
Brisbane, QLD, Australia.
(11)Sports Performance, Recovery, Injury and New Technologies (SPRINT) Research 
Centre, Australian Catholic University, Brisbane, QLD, Australia.
(#)Contributed equally

BACKGROUND: With the increasing use of artificial intelligence in healthcare and 
sports science, large language models (LLMs) are being explored as tools for 
delivering personalized, evidence-based guidance to athletes.
OBJECTIVE: This study evaluated the capabilities of LLMs (ChatGPT-3.5, 
ChatGPT-4, and Google Bard) to deliver evidence-based advice on sleep and jet 
lag for athletes.
METHODS: Conducted in two phases between January and June 2024, the study first 
identified ten frequently asked questions on these topics with input from 
experts and LLMs. In the second phase, 20 experts (mean age 43.9 ± 9.0 years; 
ten females, ten males) assessed LLM responses using Google Forms surveys 
administered at two intervals (T1 and T2). Inter-rater reliability was evaluated 
using Fleiss' Kappa, and intra-rater agreement using the Jaccard Similarity 
Index (JSI), and content validity through the content validity ratio (CVR). 
Differences among LLMs were analyzed using Friedman and Chi-square tests.
RESULTS: Experts' response rates were high (100% at T1 and 95% at T2). 
Inter-rater reliability was minimal (Fleiss' Kappa: 0.21-0.39), while 
intra-rater agreement was high, with 53% of experts achieving a JSI ≥ 0.75. 
ChatGPT-4 had the highest CVR for sleep (0.67) and was the only model with a 
valid CVR for jet lag (0.68). Google Bard showed the lowest CVR for jet lag 
(0%), with significant differences compared to ChatGPT-3.5 (p = 0.0073) and 
ChatGPT-4 (p < 0.0001). Reasons for inappropriate responses varied significantly 
for jet lag (p < 0.0001), with Google Bard criticized for insufficient 
information and frequent errors. ChatGPT-4 outperformed other models overall.
CONCLUSIONS: This study highlights the potential of LLMs, particularly 
ChatGPT-4, to provide evidence-based advice on sleep but underscores the need 
for improved accuracy and validation for jet lag recommendations.

© 2025. The Author(s), under exclusive licence to Springer Nature Switzerland 
AG.

DOI: 10.1007/s40279-025-02303-5
PMID: 41042486

Conflict of interest statement: Declarations. Funding: This research did not 
receive any specific grant from funding agencies in the public, commercial, or 
not-for-profit sectors. Conflict of interest: Shona Halson is an Editorial Board 
member of Sports Medicine. She was not involved in the selection of peer 
reviewers for the manuscript nor in any of the subsequent editorial decisions 
All other authors declare no conflict of interest or competing interests. 
Availability of data and material: The datasets generated and analyzed during 
the current study are available from the corresponding author, Jacopo Vitale 
(jacopo.vitale@kws.ch), upon reasonable request. Ethics approval: As officially 
specified by a letter from the local Ethical Committee of the Principal 
Investigator (JV, dated 15/12/2023), ethical approval was not required for the 
present study because the research project does not fall within the scope of the 
Human Research Act. Consent to participate: Not applicable. Consent for 
publication: Not applicable. Author contributions: The study was conceptualized 
and designed by JV, DCJvR, and SH. JV, SH, DCJvR, AC, and AM developed the 
methodology, including FAQ identification and pilot testing. JV, DCJvR, and SH 
conducted data collection, curation, and survey administration. Statistical 
analyses were performed by JV, with critical input from AC and AM. The ATSIG 
contributed to the evaluation and interpretation of the LLM responses and 
provided critical insights during manuscript preparation. The initial draft of 
the manuscript was written by JV, with substantial revisions contributed by AM, 
DCJvR, SH, and AC. All authors, including ATSIG members, reviewed and critically 
revised the manuscript, providing intellectual content. All authors approved the 
final version of the manuscript. JV is the guarantor of this work.


160. J Med Syst. 2025 Nov 10;49(1):158. doi: 10.1007/s10916-025-02282-0.

Evaluating the Performance of DeepSeek-R1 as a Patient Education Tool.

Hu J(#)(1), Wang J(#)(2), He L(3), Qiu Z(4), Sun S(1), Peng F(5).

Author information:
(1)Department of Nursing, Second Affiliated Hospital of Naval Medical 
University, Shanghai, China.
(2)Department of Cardiothoracic Surgery, Second Affiliated Hospital of Naval 
Medical University, Shanghai, China.
(3)Department of Infectious Diseases, Second Affiliated Hospital of Naval 
Medical University, Shanghai, China.
(4)Department of Anesthesiology, Ruijin Hospital, Shanghai Jiao Tong University 
School of Medicine, Shanghai, China.
(5)Department of Nursing, Second Affiliated Hospital of Naval Medical 
University, Shanghai, China. zhaozichenpf@163.com.
(#)Contributed equally

The cost-effective open-source artificial intelligence (AI) model DeepSeek-R1 in 
China holds significant potential for healthcare applications. As a health 
education tool, it could help patients acquire health science knowledge and 
improve health literacy. Low back pain (LBP), the most common musculoskeletal 
problem globally, has seen increasing use of large language model (LLM)-based AI 
chatbots by patients to access health information, making it critical to further 
examine the quality of such information. This study aimed to evaluate the 
response quality and readability of answers generated by DeepSeek-R1 to common 
patient questions about LBP. Ten questions were formulated using inductive 
methods based on literature analysis and Baidu Index data, which were presented 
to DeepSeek-R1 on March 10, 2025. The evaluation spanned readability, 
understandability, actionability, clinician assessment, and reference 
assessment. Readability was measured using the Flesch-Kincaid Grade Level, 
Flesch Reading Ease Scale, Gunning Fog Index, Coleman-Liau Index, and Simple 
Measure of Gobbledygook (SMOG Index). Understandability and actionability were 
assessed via the Patient Education Materials and Assessment Tool for Printable 
Materials (PEMAT-P). Clinicians evaluated accuracy, completeness, and 
correlation. A reference evaluation tool was used to assess reference quality 
and the presence of hallucinations. Readability analysis indicated that 
DeepSeek's responses were overall "difficult to read", with Flesch-Kincaid Grade 
Level (mean 12.39, SD 1.91), Flesch Reading Ease Scale (mean 19.55, Q1 12.94, Q3 
29.78), Gunning Fog Index (mean 13.95, SD 2.61), Coleman-Liau Index (mean 17.46, 
SD 2.30), and SMOG Index (mean 11.04, SD 1.37). PEMAT-P revealed good 
understandability but weak actionability. Consensus among five clinicians 
confirmed satisfactory accuracy, completeness, and relevance. References 
Assessment identified 9 instances (14.8%) of hallucinated references, while 
Supporting was rated as moderate, with most references sourced from 
authoritative platforms. Our study demonstrates the potential of DeepSeek-R1 in 
the educational content for patients with LBP. It can be employed as a 
supplement to patient education tools rather than substituting for clinical 
judgment.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02282-0
PMID: 41212429 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Human Ethics: Not applicable. 
Consent to Participate: Not applicable. Competing interests: The authors declare 
no competing interests.


161. Can J Anaesth. 2025 Jun;72(6):904-922. doi: 10.1007/s12630-025-02973-9. Epub 
2025 Jun 16.

The applications of ChatGPT and other large language models in anesthesiology 
and critical care: a systematic review.

[Article in English]

Daccache N(1), Zako J(1), Morisson L(1), Laferrière-Langlois P(2).

Author information:
(1)Maisonneuve-Rosemont Hospital Research Centre, Université de Montréal, 
Montreal, QC, Canada.
(2)Maisonneuve-Rosemont Hospital Research Centre, Université de Montréal, 
Montreal, QC, Canada. pascal.laferriere-langlois@umontreal.ca.

PURPOSE: ChatGPT and other large language models (LLMs) have gained immense 
popularity since their commercial release in 2022, with applications in various 
sectors including health care. We sought to evaluate their deployment in 
anesthesiology and critical care in a systematic review. Our aim was to describe 
the integration of LLMs in the field by showcasing and categorizing their 
current applications, assessing their performance in patient care, and reviewing 
application-specific ethical and practical challenges in deployment.
METHODS: Respecting Preferred Reporting Items for Systematic Reviews and 
Meta-analyses (PRISMA) guidelines, we systematically searched through PubMed®, 
Embase, the Cochrane Central Register of Controlled Trials, and Web of Science®, 
from inception until 1 August 2024. We extracted all papers investigating LLMs 
in anesthesiology or critical care and reporting results. We segmented the 
literature into major themes and highlighted key findings and limitations.
RESULTS: From 480 retrieved articles, we included 45 papers. The evaluated 
models (GPT-4, GPT-3.5, Google Bard [now Gemini], LLaMA, and others) showed 
diverse applications in four segments: intensive care unit, patient education, 
medical education, and perioperative care. Large language models, especially 
newer models, are promising in predicting clinical scores, navigating simple 
clinical scenarios, and managing preoperative anxiety. Their performance remains 
below the clinician level in predicting outcomes, solving complex clinical 
scenarios (i.e., airway management), board examinations, and generating 
patient-directed documents, although newer models performed better than older 
ones.
CONCLUSION: While LLMs are not yet equipped to fully assist physicians in 
anesthesiology and critical care, they have significant potential, and their 
capabilities are rapidly improving. Supervised use for select tasks can 
streamline patient care. Further trials are warranted as new versions of models 
become available.
STUDY REGISTRATION: PROSPERO ( CRD42024567380 ); first submitted 22 July 2024.

Publisher: RéSUMé: OBJECTIF: ChatGPT et d’autres grands modèles de langage (LLM) 
ont acquis une immense popularité depuis leur sortie commerciale en 2022, avec 
des applications dans divers secteurs, y compris dans les soins de santé. Nous 
avons cherché à évaluer leur déploiement en anesthésiologie et en soins 
intensifs dans le cadre d’une revue systématique. Notre objectif était de 
décrire l’intégration des LLM sur le terrain en présentant et en catégorisant 
leurs applications actuelles, en évaluant leurs performances dans les soins à la 
patientèle et en examinant les défis éthiques et pratiques spécifiques aux 
applications lors de leur déploiement. MéTHODE: Conformément aux directives 
PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-analyses), 
nous avons réalisé des recherches systématiques dans les bases de données 
PubMed®, Embase, le registre Cochrane des essais contrôlés et Web of Science®, 
depuis leur création jusqu’au 1er août 2024. Nous avons extrait tous les 
articles portant sur les LLM en anesthésiologie ou en soins intensifs et 
rapporté les résultats. Nous avons segmenté la littérature en grands thèmes et 
mis en évidence les principales constatations et limites. RéSULTATS: Sur 480 
articles récupérés, nous avons inclus 45 articles. Les modèles évalués (GPT-4, 
GPT-3.5, Google Bard [maintenant Gemini], LLaMA et autres) ont montré des 
applications diverses dans quatre segments : unité de soins intensifs, éducation 
de la patientèle, éducation médicale et soins périopératoires. Les grands 
modèles de langage, en particulier les modèles les plus récents, sont 
prometteurs pour prédire les scores cliniques, naviguer des cas cliniques 
simples et gérer l’anxiété préopératoire. Leur performance reste inférieure à 
celle des cliniciens et cliniciennes en ce qui concerne la prédiction des 
devenirs, la résolution de cas cliniques complexes (c’est-à-dire la prise en 
charge des voies aériennes), les examens de certification et la génération de 
documents destinés à la patientèle, bien que les modèles les plus récents aient 
donné de meilleurs résultats comparativement aux modèles plus anciens. 
CONCLUSION: Bien que les LLM ne soient pas encore équipés pour assister 
complètement les médecins en anesthésiologie et en soins intensifs, ils ont un 
potentiel important et leurs capacités s’améliorent rapidement. L’utilisation 
supervisée pour certaines tâches peut rationaliser les soins à la patientèle. 
D’autres études sont nécessaires au fur et à mesure que de nouvelles versions de 
modèles seront déployées. ENREGISTREMENT DE L’éTUDE: PROSPERO ( CRD42024567380 
); première soumission le 22 juillet 2024.

© 2025. Canadian Anesthesiologists' Society.

DOI: 10.1007/s12630-025-02973-9
PMID: 40524117 [Indexed for MEDLINE]


162. Aesthetic Plast Surg. 2025 Apr;49(7):1947-1953. doi: 10.1007/s00266-024-04515-y. 
Epub 2024 Nov 26.

Performance of Artificial Intelligence Chatbots in Answering Clinical Questions 
on Japanese Practical Guidelines for Implant-based Breast Reconstruction.

Shiraishi M(1), Sowa Y(2), Tomita K(3), Terao Y(4), Satake T(5), Muto 
M(5)(6)(7), Morita Y(8)(9), Higai S(8), Toyohara Y(8), Kurokawa Y(8), Sunaga 
A(8), Okazaki M(1).

Author information:
(1)Department of Plastic and Reconstructive Surgery, The University of Tokyo 
Hospital, Tokyo, Japan.
(2)Department of Plastic Surgery, Jichi Medical University, Yakushiji, 
Shimotsuke, Tochigi, Japan. ysowawan@gmail.com.
(3)Department of Plastic and Reconstructive Surgery, Kindai University, Osaka, 
Japan.
(4)Department of Plastic and Reconstructive Surgery, Tokyo Metropolitan Cancer 
and Infectious Diseases Center, Komagome Hospital, Tokyo, Japan.
(5)Department of Plastic, Reconstructive and Aesthetic Surgery, Toyama 
University Hospital, Toyama, Japan.
(6)Lala Breast Reconstruction Clinic Yokohama, Yokohama, Japan.
(7)Department of Plastic Surgery, Yokohama City University Medical Center, 
Yokohama, Japan.
(8)Department of Plastic Surgery, Jichi Medical University, Yakushiji, 
Shimotsuke, Tochigi, Japan.
(9)Japanese Red Cross Koga Hospital, Koga, Japan.

BACKGROUND: Artificial intelligence (AI) chatbots, including ChatGPT-4 (GPT-4) 
and Grok-1 (Grok), have been shown to be potentially useful in several medical 
fields, but have not been examined in plastic and aesthetic surgery. The aim of 
this study is to evaluate the responses of these AI chatbots for clinical 
questions (CQs) related to the guidelines for implant-based breast 
reconstruction (IBBR) published by the Japan Society of Plastic and 
Reconstructive Surgery (JSPRS) in 2021.
METHODS: CQs in the JSPRS guidelines were used as question sources. Responses 
from two AI chatbots, GPT-4 and Grok, were evaluated for accuracy, 
informativeness, and readability by five Japanese Board-certified breast 
reconstruction specialists and five Japanese clinical fellows of plastic 
surgery.
RESULTS: GPT-4 outperformed Grok significantly in terms of accuracy (p < 0.001), 
informativeness (p < 0.001), and readability (p < 0.001) when evaluated by 
plastic surgery fellows. Compared to the original guidelines, Grok scored 
significantly lower in all three areas (all p < 0.001). The accuracy of GPT-4 
was rated to be significantly higher based on scores given by plastic surgery 
fellows compared to those of breast reconstruction specialists (p = 0.012), 
whereas there was no significant difference between these scores for Grok.
CONCLUSIONS: The study suggests that GPT-4 has the potential to assist in 
interpreting and applying clinical guidelines for IBBR but importantly there is 
still a risk that AI chatbots can misinform. Further studies are needed to 
understand the broader role of current and future AI chatbots in breast 
reconstruction surgery.
LEVEL OF EVIDENCE IV: This journal requires that authors assign a level of 
evidence to each article. For a full description of these Evidence-Based 
Medicine Ratings, please refer to Table of Contents or online Instructions to 
Authors www.springer.com/00266 .

© 2024. Springer Science+Business Media, LLC, part of Springer Nature and 
International Society of Aesthetic Plastic Surgery.

DOI: 10.1007/s00266-024-04515-y
PMID: 39592492 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare that they have no conflicts of interest to disclose. Human and Animal 
Rights: The study does not use human subjects or animals. Informed Consent: Not 
required for this type of study.


163. Br J Ophthalmol. 2025 Jul 22;109(8):962-966. doi: 10.1136/bjo-2024-326254.

Can large language models fully automate or partially assist paper selection in 
systematic reviews?

Chen H(#)(1), Jiang Z(#)(1), Liu X(2), Xue CC(3), Yew SME(4)(5), Sheng B(6)(7), 
Zheng YF(3)(8), Wang X(9), Wu Y(10)(11), Sivaprasad S(12), Wong TY(1)(3)(13), 
Chaudhary V(14), Tham YC(15)(4)(5)(16).

Author information:
(1)Tsinghua Medicine, Tsinghua University, Beijing, China.
(2)Institute of Medical Technology, Peking University Health Science Center, 
Beijing, China.
(3)Singapore Eye Research Institute, Singapore National Eye Centre, Singapore.
(4)Centre for Innovation and Precision Eye Health, Yong Loo Lin School of 
Medicine, National University of Singapore, Singapore.
(5)Department of Ophthalmology, Yong Loo Lin School of Medicine, National 
University of Singapore, Singapore.
(6)MOE Key Laboratory of AI, School of Electronic, Information, and Electrical 
Engineering, Shanghai Jiao Tong University, Shanghai, China.
(7)Department of Computer Science and Engineering, School of Electronic, 
Information, and Electrical Engineering, Shanghai Jiao Tong University, 
Shanghai, China.
(8)State Key Laboratory of Ophthalmology, Zhongshan Ophthalmic Center, Sun 
Yat-sen University, Guangzhou, China.
(9)Beijing Advanced Innovation Center for Biomedical Engineering, Key Laboratory 
for Biomechanics and Mechanobiology of Ministry of Education, School of 
Biological Science and Medical Engineering, Beihang University, Beijing, China.
(10)Tsinghua University School of Medicine, Beijing, China.
(11)Department of Health Policy and Management, Johns Hopkins University, 
Baltimore, Maryland, USA.
(12)Moorfields Eye Hospital City Road Campus, London, UK.
(13)School of Clinical Medicine, Beijing Tsinghua Changgung Hospital, Tsinghua 
University, Beijing, China.
(14)Department of Surgery, Division of Ophthalmology, McMaster University, 
Hamilton, Ontario, Canada.
(15)Singapore Eye Research Institute, Singapore National Eye Centre, Singapore 
thamyc@nus.edu.sg.
(16)Ophthalmology and Visual Science Academic Clinical Program, Duke-NUS Medical 
School, Singapore.
(#)Contributed equally

BACKGROUND/AIMS: Large language models (LLMs) have substantial potential to 
enhance the efficiency of academic research. The accuracy and performance of 
LLMs in a systematic review, a core part of evidence building, has yet to be 
studied in detail.
METHODS: We introduced two LLM-based approaches of systematic review: an 
LLM-enabled fully automated approach (LLM-FA) utilising three different GPT-4 
plugins (Consensus GPT, Scholar GPT and GPT web browsing modes) and an 
LLM-facilitated semi-automated approach (LLM-SA) using GPT4's Application 
Programming Interface (API). We benchmarked these approaches using three 
published systematic reviews that reported the prevalence of diabetic 
retinopathy across different populations (general population, pregnant women and 
children).
RESULTS: The three published reviews consisted of 98 papers in total. Across 
these three reviews, in the LLM-FA approach, Consensus GPT correctly identified 
32.7% (32 out of 98) of papers, while Scholar GPT and GPT4's web browsing modes 
only identified 19.4% (19 out of 98) and 6.1% (6 out of 98), respectively. On 
the other hand, the LLM-SA approach not only successfully included 82.7% (81 out 
of 98) of these papers but also correctly excluded 92.2% of 4497 irrelevant 
papers.
CONCLUSIONS: Our findings suggest LLMs are not yet capable of autonomously 
identifying and selecting relevant papers in systematic reviews. However, they 
hold promise as an assistive tool to improve the efficiency of the paper 
selection process in systematic reviews.

© Author(s) (or their employer(s)) 2025. Re-use permitted under CC BY-NC. No 
commercial re-use. See rights and permissions. Published by BMJ Group.

DOI: 10.1136/bjo-2024-326254
PMCID: PMC12320601
PMID: 39814458 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: TYW declares consulting 
fees from Aldropika Therapeutics, Bayer, Boehringer Ingelheim, Genentech, Iveric 
Bio, Novartis, Plano, Oxurion, Roche, Sanofi and Shanghai Henlius; funding from 
the National Key R&D Program, China (grant number 2022YFC2502802); and being an 
inventor, patent holder and co-founder of the start-up companies EyRiS and 
Visre. All other authors declare no competing interests.


164. JMIR Med Educ. 2025 Apr 10;11:e67244. doi: 10.2196/67244.

Large Language Models in Biochemistry Education: Comparative Evaluation of 
Performance.

Bolgova O(1), Shypilova I(2), Mavrych V(1).

Author information:
(1)College of Medicine, Alfaisal University, Al Takhassousi St, Riyadh, 11533, 
Saudi Arabia.
(2)School of Medicine, St Mathews University, George Town, Cayman Islands.

BACKGROUND: Recent advancements in artificial intelligence (AI), particularly in 
large language models (LLMs), have started a new era of innovation across 
various fields, with medicine at the forefront of this technological revolution. 
Many studies indicated that at the current level of development, LLMs can pass 
different board exams. However, the ability to answer specific subject-related 
questions requires validation.
OBJECTIVE: The objective of this study was to conduct a comprehensive analysis 
comparing the performance of advanced LLM chatbots-Claude (Anthropic), GPT-4 
(OpenAI), Gemini (Google), and Copilot (Microsoft)-against the academic results 
of medical students in the medical biochemistry course.
METHODS: We used 200 USMLE (United States Medical Licensing Examination)-style 
multiple-choice questions (MCQs) selected from the course exam database. They 
encompassed various complexity levels and were distributed across 23 distinctive 
topics. The questions with tables and images were not included in the study. The 
results of 5 successive attempts by Claude 3.5 Sonnet, GPT-4-1106, Gemini 1.5 
Flash, and Copilot to answer this questionnaire set were evaluated based on 
accuracy in August 2024. Statistica 13.5.0.17 (TIBCO Software Inc) was used to 
analyze the data's basic statistics. Considering the binary nature of the data, 
the chi-square test was used to compare results among the different chatbots, 
with a statistical significance level of P<.05.
RESULTS: On average, the selected chatbots correctly answered 81.1% (SD 12.8%) 
of the questions, surpassing the students' performance by 8.3% (P=.02). In this 
study, Claude showed the best performance in biochemistry MCQs, correctly 
answering 92.5% (185/200) of questions, followed by GPT-4 (170/200, 85%), Gemini 
(157/200, 78.5%), and Copilot (128/200, 64%). The chatbots demonstrated the best 
results in the following 4 topics: eicosanoids (mean 100%, SD 0%), bioenergetics 
and electron transport chain (mean 96.4%, SD 7.2%), hexose monophosphate pathway 
(mean 91.7%, SD 16.7%), and ketone bodies (mean 93.8%, SD 12.5%). The Pearson 
chi-square test indicated a statistically significant association between the 
answers of all 4 chatbots (P<.001 to P<.04).
CONCLUSIONS: Our study suggests that different AI models may have unique 
strengths in specific medical fields, which could be leveraged for targeted 
support in biochemistry courses. This performance highlights the potential of AI 
in medical education and assessment.

© Olena Bolgova, Inna Shypilova, Volodymyr Mavrych. Originally published in JMIR 
Medical Education (https://mededu.jmir.org).

DOI: 10.2196/67244
PMCID: PMC12005600
PMID: 40209205 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


165. J Med Internet Res. 2025 Jun 12;27:e66055. doi: 10.2196/66055.

Knowledge Graph-Enhanced Deep Learning Model (H-SYSTEM) for Hypertensive 
Intracerebral Hemorrhage: Model Development and Validation.

Xia Y(1), Li J(2), Deng B(1), Huang Q(2), Cai F(2), Xie Y(1), Sun X(1), Shi 
Q(1), Dan W(1), Zhan Y(1), Jiang L(1).

Author information:
(1)Department of Neurosurgery, The First Affiliated Hospital of Chongqing 
Medical University, No. 1 Youyi Road, Yuanjiagang, Yuzhong District, Chongqing, 
China, 86 13638354200.
(2)School of Computer Science and Engineering, Chongqing University of Science 
and Technology, Chongqing, China.

BACKGROUND: Although much progress has been made in artificial intelligence 
(AI), several challenges remain substantial obstacles to the development and 
translation of AI systems into clinical practice. Even large language models, 
which show excellent performance on various tasks, have progressed slowly in 
clinical practice tasks. Providing precise and explainable treatment plans with 
personalized details remains a big challenge for AI systems due to both the 
highly specialized medical knowledge required and patients' complicated 
conditions.
OBJECTIVE: This study aimed to develop an explainable and efficient decision 
support system named H-SYSTEM to assist neurosurgeons in diagnosing and treating 
patients with hypertensive intracerebral hemorrhage. The system was designed to 
address the limitations of existing AI systems by integrating a medical domain 
knowledge graph to enhance decision-making accuracy and explainability.
METHODS: The H-SYSTEM consists of 3 main modules: the key named entity 
recognition (NER) module, the semantic analysis and representation module, and 
the reasoning module. Furthermore, we constructed a medical domain knowledge 
graph for hypertensive intracerebral hemorrhage, named HKG, which served as an 
external knowledge brain of the H-SYSTEM to enhance its text recognition and 
automated decision-making capability. The HKG was exploited to guide the 
training of the semantic analysis and representation module and reasoning 
module, which makes the output of the H-SYSTEM more explainable., To assess the 
performance of the H-SYSTEM, we compared it with doctors and different large 
language models.
RESULTS: The outputs based on HKG showed reliable performance as compared with 
neurosurgical doctors, with an overall accuracy of 94.87%. The bidirectional 
encoder representations from transformers, inflated dilated convolutional neural 
network, bidirectional long short-term memory, and conditional random fields 
(BERT-IDCNN-BiLSTM-CRF) model was used as the key NER module of the H-SYSTEM due 
to its fast convergence and efficient extraction of key named entities, achieved 
the highest performance among 7 key NER models (precision=92.03, recall=90.22, 
and F1-score=91.11), significantly outperforming the others. The H-SYSTEM 
achieved an overall accuracy of 91.74% in treatment plans, showing significant 
consistency with the gold standard (P<.05), with diagnostic measures achieving 
88.18% accuracy, 97.03% area under the curve (AUC), and a κ of 0.874; surgical 
therapy achieving 98.53% accuracy, 98.53% AUC, and a κ of 0.971; and rescue 
therapies achieving 89.50% accuracy, 94.67% AUC, and a κ of 0.923 (all P<.05). 
Furthermore, the H-SYSTEM showed high reliability and efficiency when compared 
to doctors and ChatGPT, achieving statistically higher accuracy (95.26% vs 
91.48%, P<.05). Additionally, the H-SYSTEM achieved a total accuracy of 92.22% 
(ranging from 91.14% to 95.35%) in treatment plans for 605 additional patients 
from 6 different medical centers.
CONCLUSIONS: The H-SYSTEM showed significantly high efficiency and 
generalization capacity in processing electronic medical records, and it 
provided explainable and elaborate treatment plans. Therefore, it has the 
potential to provide neurosurgeons with rapid and reliable decision support, 
especially in emergency conditions. The knowledge graph-enhanced deep-learning 
model exhibited excellent performance in the clinical practice tasks.

© Yulong Xia, Jie Li, Bo Deng, Qilin Huang, Fenglin Cai, Yanfeng Xie, Xiaochuan 
Sun, Quanhong Shi, Wei Dan, Yan Zhan, Li Jiang. Originally published in the 
Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/66055
PMCID: PMC12203281
PMID: 40505141 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


166. Clin Kidney J. 2025 Oct 6;18(10):sfaf308. doi: 10.1093/ckj/sfaf308. eCollection 
2025 Oct.

ECOSBot: a multicenter validation pilot study of a generative AI tool for 
OSCE-based nephrology training.

Bentegeac R(1)(2), Florens N(3)(4)(5), Maanaoui M(6)(7), Maisons V(8), Lanot 
A(9)(10), Bobot M(11)(12), Brilland B(13)(14), Glowacki F(6), Gérard E(15), 
Hazzan M(6), Amouyel P(1)(2), Le Guellec B(2)(16)(17), Hamroun A(1)(2)(5)(6).

Author information:
(1)Lille University, Department of Public Health - Epidemiology, Lille 
University Hospital Center, Lille, France.
(2)UMR1167 RID-AGE, Pasteur Institute of Lille, Inserm, Lille University, Lille 
University Hospital Center, Lille, France.
(3)Nephrology Department, Hôpitaux Universitaires de Strasbourg, Strasbourg, 
France.
(4)Inserm UMR_S 1109 Immuno-Rhumatology Laboratory, Translational Medicine 
Federation of Strasbourg (FMTS), FHU Target, Faculté de Médecine, Université de 
Strasbourg, Strasbourg, France.
(5)INI-CRCT (Cardiovascular and Renal Trialists), F-CRIN Network, Strasbourg, 
France.
(6)Lille University, Department of Nephrology, Lille University Hospital Center, 
Lille, France.
(7)U1190 Translational Research for Diabetes, INSERM, Pasteur Institute of 
Lille, Lille University, Lille, France.
(8)Service de Néphrologie, CHU de Tours, Tours, France; U1246, INSERM, SPHERE, 
Université de Tours, Université de Nantes, Tours, Nantes, France.
(9)Normandie Université, UNICAEN, CHU de Caen Normandie, Néphrologie, CAEN, 
France.
(10)ANTICIPE U1086 INSERM-UCN, Centre François Baclesse, Caen, France.
(11)Centre de Néphrologie et Transplantation Rénale, Hôpital de la Conception, 
AP-HM, Marseille, France.
(12)Aix Marseille Univ., INSERM 1263, INRAE 1260, C2VN, CERIMED, Marseille, 
France.
(13)Service de Néphrologie-Dialyse-Transplantation, CHU Angers, Angers, France.
(14)Univ. Angers, Nantes Université, Inserm, CNRS, CRCI2NA, SFR ICAT, Angers, 
France.
(15)Université de Lille, CHU Lille, ULR 2694-METRICS: Évaluation des 
Technologies de Santé et des Pratiques Médicales, Lille, France.
(16)Lille University, Department of Neuroradiology, CHU Lille, Salengro 
Hospital,  Lille, France.
(17)INSERM, U1172-LilNCog-Lille Neuroscience & Cognition, Université de Lille, 
Lille, France.

BACKGROUND: Developing diagnostic reasoning in nephrology is particularly 
challenging due to its pathophysiological complexity and reliance on abstract 
clinical data. Objective Structured Clinical Examinations (OSCEs) are pivotal 
for nephrology training but remain resource-intensive and difficult to scale. 
Generative artificial intelligence (AI) offers a promising alternative, yet its 
capacity to emulate nephrology-specific OSCEs has not been formally assessed.
METHODS: We developed ECOSBot, a web-based tool powered by GPT-4o, to simulate 
both standardized patients and examiners for nephrology-focused OSCEs. In this 
multicenter prospective study, undergraduate medical students from five French 
medical schools interacted with ECOSBot across four clinical stations. All 
interactions were double-rated by nephrology faculty members to establish a gold 
standard. ECOSBot's performance was evaluated against this standard using four 
criteria (script coverage, authenticity, correctness and relevance) for patient 
simulation, and via checklists and competency-based ratings for examiner 
scoring. Usability was assessed using the Chatbot Usability Questionnaire (CUQ), 
adapted to include six items on feedback quality.
RESULTS: Ninety-one students generated 2939 prompts across 184 OSCE sessions. 
ECOSBot demonstrated high fidelity in patient simulation: authenticity 98.6% 
[95% confidence interval (CI) 98.2-99.0], correctness 98.3% (95% CI 97.9-98.7) 
and relevance 99.2% (95% CI 98.9-99.5), including during exchanges not 
explicitly covered by the pre-specified scenario. As an examiner, ECOSBot showed 
strong agreement with human raters on global scores [intraclass correlation 
coefficient (ICC) = 0.94, 95% CI 0.91-0.96], consistent across case formats, 
training levels and institutions. However, scoring of attitude and communication 
skills was less reliable (ICC = 0.44, 95% CI 0.28-0.58). Median CUQ score was 
69.7/100, with 91.7% of students finding the tool highly useful for OSCE 
preparation in nephrology.
CONCLUSIONS: ECOSBot reliably simulated both roles in nephrology OSCEs with high 
fidelity and strong alignment with expert rating. While challenges remain for 
subjective skill assessment, this tool offers a scalable and autonomous solution 
to enhance nephrology education.

© The Author(s) 2025. Published by Oxford University Press on behalf of the ERA.

DOI: 10.1093/ckj/sfaf308
PMCID: PMC12541372
PMID: 41133180

Conflict of interest statement: The authors declare no competing interests 
related to the content of this article.


167. JMIR Med Inform. 2025 Dec 9;13:e78041. doi: 10.2196/78041.

Trends and Trajectories in the Rise of Large Language Models in Radiology: 
Scoping Review.

Al Zaabi A(1), Alshibli R(2), AlAmri A(2), AlRuheili I(2), Lutfi SL(3).

Author information:
(1)Human and Clinical Anatomy Department, College of Medicine and Health 
Sciences, Sultan Qaboos University, P.O. Box 35, Al Khodh, Muscat, 123, Oman.
(2)College of Medicine and Health Sciences, Sultan Qaboos University, Muscat, 
Oman.
(3)Medical Education and Informatics Department, College of Medicine and Health 
Sciences, Sultan Qaboos University, Muscat, Oman.

BACKGROUND: The use of large language models (LLMs) in radiology is expanding 
rapidly, offering new possibilities in report generation, decision support, and 
workflow optimization. However, a comprehensive evaluation of their 
applications, performance, and limitations across the radiology domain remains 
limited.
OBJECTIVE: This review aimed to map current applications of LLMs in radiology, 
evaluate their performance across key tasks, and identify prevailing limitations 
and directions for future research.
METHODS: A scoping review was conducted in accordance with the framework by 
Arksey and O'Malley framework and the PRISMA-ScR (Preferred Reporting Items for 
Systematic Reviews and Meta-Analyses extension for Scoping Reviews) guidelines. 
Three databases-PubMed, ScopusCOPUS, and IEEE Xplore-were searched for 
peer-reviewed studies published between January 2022 and December 2024. Eligible 
studies included empirical evaluations of LLMs applied to radiological data or 
workflows. Commentaries, reviews, and technical model proposals without 
evaluation were excluded. Two reviewers independently screened studies and 
extracted data on study characteristics, LLM type, radiological use case, data 
modality, and evaluation metrics. A thematic synthesis was used to identify key 
domains of application. No formal risk-of-bias assessment was performed, but a 
narrative appraisal of dataset representativeness and study quality was 
included.
RESULTS: A total of 67 studies were included. (n/N, %)GPT-4 was the most 
frequently used model (n=28, 42%), with text-based corpora as the primary type 
of data used (n=43, 64%). Identified use cases fell into three thematic domains: 
(1) decision support (n=39, 58%), (2) report generation and summarization (n=16, 
24%), and (3) workflow optimization (n=12, 18%). While LLMs demonstrated strong 
performance in structured-text tasks (eg, report simplification with >94% 
accuracy), diagnostic performance varied widely (16%-86%) and was limited by 
dataset bias, lack of fine tuning, and minimal clinical validation. Most studies 
(n=53, 79.1%) had single-center, proof-of-concept designs with limited 
generalizability.
CONCLUSIONS: LLMs show strong potential for augmenting radiological workflows, 
particularly for structured reporting, summarization, and educational tasks. 
However, their diagnostic performance remains inconsistent, and current 
implementations lack robust external validation. Future work should prioritize 
prospective, multicenter validation of domain-adapted and multimodal models to 
support safe clinical integration.

© Adhari Al Zaabi, Rashid Alshibli, Abdullah AlAmri, Ibrahim AlRuheili, 
Syaheerah Lebai Lutfi. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org).

DOI: 10.2196/78041
PMCID: PMC12688054
PMID: 41364806 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


168. Eur Arch Otorhinolaryngol. 2025 Mar;282(3):1619-1629. doi: 
10.1007/s00405-024-08947-9. Epub 2024 Sep 13.

Prediction of tumor board procedural recommendations using large language 
models.

Aubreville M(1)(2), Ganz J(3), Ammeling J(3), Rosbach E(3), Gehrke T(4), 
Scherzad A(4), Hackenberg S(4), Goncalves M(4).

Author information:
(1)Flensburg University of Applied Sciences, Flensburg, Germany. 
marc.aubreville@hs-flensburg.de.
(2)Technische Hochschule Ingolstadt, Ingolstadt, Germany. 
marc.aubreville@hs-flensburg.de.
(3)Technische Hochschule Ingolstadt, Ingolstadt, Germany.
(4)Department of Otorhinolaryngology, Plastic, Aesthetic and Reconstructive Head 
and Neck Surgery, University Hospital Würzburg, Würzburg, Germany.

INTRODUCTION: Multidisciplinary tumor boards are meetings where a team of 
medical specialists, including medical oncologists, radiation oncologists, 
radiologists, surgeons, and pathologists, collaborate to determine the best 
treatment plan for cancer patients. While decision-making in this context is 
logistically and cost-intensive, it has a significant positive effect on overall 
cancer survival. METHODS : We evaluated the quality and accuracy of predictions 
by several large language models for recommending procedures by a Head and Neck 
Oncology tumor board, which we adapted for the task using parameter-efficient 
fine-tuning or in-context learning. Records were divided into two sets: n=229 
used for training and n=100 records for validation of our approaches. 
Randomized, blinded, manual human expert classification was used to evaluate the 
different models. RESULTS : Treatment line congruence varied depending on the 
model, reaching up to 86%, with medically justifiable recommendations up to 98%. 
Parameter-efficient fine-tuning yielded better outcomes than in-context 
learning, and larger/commercial models tend to perform better.
CONCLUSION: Providing precise, medically justifiable procedural recommendations 
for complex oncology patients is feasible. Extending the data corpus to a larger 
patient cohort and incorporating the latest guidelines, assuming the model can 
handle sufficient context length, could result in more factual and 
guideline-aligned responses and is anticipated to enhance model performance. We, 
therefore, encourage further research in this direction to improve the efficacy 
and reliability of large language models as support in medical decision-making 
processes.

© 2024. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00405-024-08947-9
PMID: 39266750 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare no Conflict of interest.


169. Am J Ophthalmol. 2025 Mar;271:445-454. doi: 10.1016/j.ajo.2024.12.021. Epub 2024 
Dec 30.

Ophthalmology Journals' Guidelines on Generative Artificial Intelligence: A 
Comprehensive Analysis.

Huang W(1), Liang Y(1), Wei X(2), Du Y(3).

Author information:
(1)From the Department of Ophthalmology (W.H., Y.L., Y.D.), The First Affiliated 
Hospital of Guangxi Medical University, Nanning, Guangxi Zhuang Autonomous 
Region, China.
(2)Department of Ophthalmology (X.W.), Liuzhou Red Cross Hospital/Eye Hospital 
of Liuzhou City, Liuzhou, Guangxi Zhuang Autonomous Region, China.
(3)From the Department of Ophthalmology (W.H., Y.L., Y.D.), The First Affiliated 
Hospital of Guangxi Medical University, Nanning, Guangxi Zhuang Autonomous 
Region, China. Electronic address: duyi@gxmu.edu.cn.

PURPOSE: The integration of generative artificial intelligence (GAI) into 
scientific research and academic writing has generated considerable controversy. 
Currently, standards for using GAI in academic medicine remain undefined. This 
study aims to conduct a comprehensive analysis of the guidance provided for 
authors regarding the use of GAI in ophthalmology scientific journals.
DESIGN: Cross-sectional bibliometric analysis.
PARTICIPANTS: A total of 140 ophthalmology journals listed in the Scimago 
Journal and Country Rankings, regardless of language or origin.
METHODS: We systematically searched and screened the 140 ophthalmology journals' 
websites on October 19 and 20, 2024, and conducted updates on November 19 and 
20, 2024.
MAIN OUTCOME MEASURES: The content of GAI guidelines from the websites of the 
140 ophthalmology journals.
RESULTS: Of 140 journals reviewed, 96 (69%) provide explicit guidelines for 
authors regarding the use of GAI. Among these, nearly all journals agree on 3 
key points: (1) 94 journals (98%) have established specific guidelines 
prohibiting GAI from being listed as an author; (2) 94 journals (98%) emphasize 
that human authors are responsible for the outputs generated by GAI tools; and 
(3) all 96 journals require authors to disclose any use of GAI. In addition, 20 
journals (21%) specify that their guidelines pertain solely to the writing 
process with GAI. Furthermore, 92 journals (66%) have developed guidelines 
concerning GAI-generated images, with 63 journals (68%) permitting their use and 
29 (32%) prohibiting them. Among those that prohibit GAI images, 27 journals 
(93%) allow their use under specific conditions.
CONCLUSION: Although there is considerable ethical consensus among ophthalmology 
journals regarding the use of GAI, notable variations exist in terms of 
permissible use and disclosure practices. Establishing standardized guidelines 
is essential to safeguard the originality and integrity of scientific research. 
Researchers must uphold high standards of academic ethics and integrity when 
using GAI.

Copyright © 2024 The Author(s). Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.ajo.2024.12.021
PMID: 39742908 [Indexed for MEDLINE]


170. J Voice. 2025 Nov 21:S0892-1997(25)00466-7. doi: 10.1016/j.jvoice.2025.10.044. 
Online ahead of print.

Assessing the Performance and Reliability of ChatGPT in Answering Patients' 
Questions on Voice Disorders across Time.

Saeedi S(1), Rong P(2).

Author information:
(1)Independent Researcher in Laryngology, Voice Pathology, and Speech- Language 
Pathology, Tehran, Iran.
(2)Department of Speech-Language-Hearing: Sciences & Disorders, The University 
of Kansas, Lawrence. Electronic address: prong@ku.edu.

OBJECTIVES/HYPOTHESIS: The objective of this study is to evaluate ChatGPT's 
responses in addressing common inquiries about voice disorders across two time 
slots.
METHODS: In this exploratory study, 30 frequently asked questions about voice 
disorders were gathered from a licensed clinical speech-language pathologist 
specialized in voice disorders and reputable online patient education sources. 
These questions were entered into the GPT‑4o mini at two different time slots 
(ie, November 2024 and April 2025), using a customized prompt that directed the 
model to act as a specialized voice-assistance chatbot, referred to as 
"VoiceHelp." The authors conducted independent evaluations of ChatGPT's 
responses, focusing on the accuracy, potential harm, and extent, alignment with 
medical consensus, empathy, as well as the overall quality. The readability of 
the responses was assessed with the Flesch Reading Ease Score (FRES), Gunning 
Fog Scale Level (GFSL), and Dale-Chall Score (D-CS), word count, sentence count, 
words per sentence, and characters per word.
RESULTS: Most generated responses (91.7%) were free from inaccurate or 
inappropriate content; 92.5% were rated as harmless, and 80% were consistent 
with the consensus. Although 38.3% of responses lacked empathy, the majority 
(92.5%) were scored between acceptable and very good in overall quality. The 
average scores for FRES, GFSL, D-CS, words count, sentence count, words per 
sentence, and characters per word at time slot 1 were 37.08, 15.76, 10.05, 
117.53, 6.90, 17.48, and 5.47, respectively, indicating a high level of reading 
complexity for a general audience. The corresponding scores at time slot 2 were 
45.09, 15.04, 9.26, 266.20, 13.6, 20.23, and 5.14, respectively.
CONCLUSIONS: ChatGPT consistently provided accurate and informative responses to 
common questions on voice disorders; however, the readability level of its 
responses was relatively low for the general public. This limitation appeared to 
be improved in the more recent version of the model. Further research is 
warranted before recommending ChatGPT as a reliable source of medical 
information for voice-disordered patients.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jvoice.2025.10.044
PMID: 41274812

Conflict of interest statement: Declaration of Competing Interest All authors 
affirm that they have no competing interests to disclose.


171. Int J Audiol. 2025 Oct 23:1-11. doi: 10.1080/14992027.2025.2569927. Online ahead 
of print.

Validity, reliability, and readability of Artificial Intelligence chatbots as 
public sources of information on hearing loss: a comparative evaluation of 
ChatGPT, Bing, Gemini, and Perplexity.

Pourhoseingholi MA(1)(2), Killan C(1)(2), Rafiee S(3), Hoare DJ(1)(2), Wray 
N(1)(2), Bateman P(1)(2).

Author information:
(1)Hearing Sciences, Mental Health and Clinical Neurosciences, School of 
Medicine, University of Nottingham, Nottingham, United Kingdom.
(2)National Institute for Health and Care Research (NIHR) Nottingham Biomedical 
Research Centre, Nottingham, United Kingdom.
(3)School of Medicine, Shahid Beheshti University of Medical Sciences, Tehran, 
Iran.

OBJECTIVE: To assess the validity, reliability and readability of four AI 
chatbots for hearing-health information.
DESIGN AND STUDY SAMPLE: Three audiologists created 100 questions covering adult 
hearing loss, paediatric hearing, hearing aids, tinnitus and cochlear implants 
(20 each). Questions were submitted twice to ChatGPT-3.5, Bing AI, Gemini and 
Perplexity. Answers were scored for factual accuracy and completeness on a 
five-point Global Quality Score. Validity was defined using low (score = 5) and 
high (score ≥ 4) thresholds. Internal consistency was estimated with Cronbach's 
α; readability with the Flesch Reading Ease Score (FRES) and Flesch-Kincaid 
Grade Level (FKGL). All scoring was completed independently by two blinded 
reviewers; discrepancies were resolved by consensus.
RESULTS: Under the low threshold ChatGPT-3.5 and Perplexity were most valid (84% 
and 79%); high-threshold validity fell to 37% and 34%. Perplexity had the 
highest overall reliability (α = 0.83) yet α dropped below 0.70 for 
cochlear-implant, tinnitus and hearing-aid questions. 84% percent of outputs 
were "Difficult"/"Very Difficult" and 68% read at college level.
CONCLUSIONS: AI chatbots deliver generally accurate hearing-health content, but 
high-threshold accuracy, domain-specific reliability and readability remain 
suboptimal. They should supplement, not replace the professional counselling. 
Continued optimisation and external validation are needed before routine 
clinical recommendation.

DOI: 10.1080/14992027.2025.2569927
PMID: 41129664


172. J Med Syst. 2025 Jul 5;49(1):94. doi: 10.1007/s10916-025-02227-7.

Evaluating the Performance of ChatGPT on Board-Style Examination Questions in 
Ophthalmology: A Meta-Analysis.

Wei J(#)(1), Wang X(#)(1), Huang M(1), Xu Y(2)(3), Yang W(4).

Author information:
(1)School of Nursing, Southwest Medical University, Luzhou, 646099, Sichuan 
Province, China.
(2)School of Future Technology, South China University of Technology, Guangzhou, 
510641, Guangdong Province, China.
(3)Pazhou Lab, Guangzhou, 510320, Guangdong Province, China.
(4)Shenzhen Eye Hospital, Shenzhen Eye Medical Center, Southern Medical 
University, No. 18 Zetian Road, Futian District, Shenzhen, 518040, Guangdong 
Province, China. benben0606@139.com.
(#)Contributed equally

To review empirical research on ChatGPT's accuracy in answering ophthalmology 
board-style examination questions up to March 2025 and to analyze the effects of 
GPT versions, question types, language differences, and ophthalmology topics on 
accuracy. A search was conducted in PubMed, Web of Science, Embase, Scopus, and 
the Cochrane Library in March 2025. Two authors extracted data and independently 
assessed study quality. Accuracy rates were calculated with Stata 17.0. GPT-4 
had an integrated accuracy of 73%, higher than GPT-3.5's 54%. It scored 77% in 
text and 55% in image tasks. GPT-4's accuracy was 73% in English-speaking 
countries and 71% in non-English ones. In ophthalmology, General Medicine 
achieved the highest accuracy (80%), while Clinical Optics had the lowest 
performance (55%). GPT-4 outperforms GPT-3.5, but its image processing 
capability needs further validation. Performance varies by language and topic, 
suggesting the need for more research on cross-linguistic efficacy and error 
analysis.

Publisher: Not applicable.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02227-7
PMID: 40615678 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing Interests: The authors 
declare no competing interests.


173. Digit Health. 2025 Mar 14;11:20552076251326014. doi: 10.1177/20552076251326014. 
eCollection 2025 Jan-Dec.

Advancing personalized medicine in digital health: The role of artificial 
intelligence in enhancing clinical interpretation of 24-h ambulatory blood 
pressure monitoring.

Alam SF(1)(2), Thongprayoon C(1)(2), Miao J(1)(2), Pham JH(1)(2), Sheikh 
MS(1)(2), Garcia Valencia OA(1)(2), Schwartz GL(1)(2), Craici IM(1)(2), Gonzalez 
Suarez ML(2), Cheungpasitporn W(2).

Author information:
(1)Division of Nephrology and Hypertension, Mayo Clinic Minnesota, Rochester, 
MN, USA.
(2)*Maria L. Gonzalez Suarez and Wisit Cheungpasitporn are senior co-authors.

Background: The use of artificial intelligence (AI) for interpreting ambulatory 
blood pressure monitoring (ABPM) data is gaining traction in clinical practice. 
Evaluating the accuracy of AI models, like ChatGPT 4.0, in clinical settings can 
inform their integration into healthcare processes. However, limited research 
has been conducted to validate the performance of such models against expert 
interpretations in real-world clinical scenarios. Methods: A total of 53 ABPM 
records from Mayo Clinic, Minnesota, were analyzed. ChatGPT 4.0's 
interpretations were compared with consensus results from two experienced 
nephrologists, based on the American College of Cardiology/American Heart 
Association (ACC/AHA) guidelines. The study assessed ChatGPT's accuracy and 
reliability over two rounds of testing, with a three-month interval between 
rounds. Results: ChatGPT achieved an accuracy of 87% for identifying 
hypertension, 89% for nocturnal hypertension, 81% for nocturnal dipping, and 94% 
for abnormal heart rate. ChatGPT correctly identified all conditions in 60% of 
ABPM records. The percentage agreement between the first and second round of 
ChatGPT's analysis was 81% in identifying hypertension, 85% in nocturnal 
hypertension, 89% in nocturnal dipping, and 94% in abnormal heart rate. There 
was no significant difference in accuracy between the first and second round 
(all p > 0.05). The Kappa statistic was 0.63 for identifying hypertension, 0.66 
for nocturnal hypertension, 0.76 for nocturnal dipping, and 0.70 for abnormal 
heart rate. Conclusions: ChatGPT 4.0 demonstrates potential as a reliable tool 
for interpreting 24-h ABPM data, achieving substantial agreement with expert 
nephrologists. These findings underscore the potential for AI integration into 
hypertension management workflows, while highlighting the need for further 
validation in larger, diverse cohorts.

© The Author(s) 2025.

DOI: 10.1177/20552076251326014
PMCID: PMC11909660
PMID: 40093710

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


174. J Clin Epidemiol. 2025 Nov;187:111944. doi: 10.1016/j.jclinepi.2025.111944. Epub 
2025 Aug 25.

Use of artificial intelligence to support the assessment of the methodological 
quality of systematic reviews.

Marques-Cruz M(1), Pinto F(2), Vieira RJ(3), Bognanni A(4), Perestrelo P(5), 
Gil-Mata S(3), Duarte VH(3), Barbosa JP(3), Cardoso-Fernandes A(6), 
Martinho-Dias D(3), Franco-Pego F(7), Germini F(4), Arienti C(8), Chu AWL(9), 
Riera-Serra P(10), Jemioło P(11), Rodrigues PP(3), Fonseca JA(3), Azevedo LF(3), 
Schünemann HJ(12), Cruz-Correia R(3), Jankin S(13), Sousa-Pinto B(3).

Author information:
(1)Faculty of Medicine, MEDCIDS - Department of Community Medicine, Information 
and Health Decision Sciences, University of Porto, Porto, Portugal; 
CINTESIS@RISE - Health Research Network, University of Porto, Porto, Portugal; 
Public Health Unit Marão e Douro Norte, Local Health Unit Trás-os-Montes e Alto 
Douro, Vila Real, Portugal.
(2)Faculty of Medicine, MEDCIDS - Department of Community Medicine, Information 
and Health Decision Sciences, University of Porto, Porto, Portugal; Knok 
Healthcare, Porto, Portugal.
(3)Faculty of Medicine, MEDCIDS - Department of Community Medicine, Information 
and Health Decision Sciences, University of Porto, Porto, Portugal; 
CINTESIS@RISE - Health Research Network, University of Porto, Porto, Portugal.
(4)Department of Health Research Methods, Evidence and Impact, McMaster 
University, Hamilton, Ontario, Canada; Clinical Epidemiology and Research Center 
(CERC), IRCCS Humanitas Research Hospital & Department of Biomedical Sciences, 
Humanitas University, Milan, Italy.
(5)Faculty of Medicine, MEDCIDS - Department of Community Medicine, Information 
and Health Decision Sciences, University of Porto, Porto, Portugal; 
CINTESIS@RISE - Health Research Network, University of Porto, Porto, Portugal; 
Oncology Department, Local Health Unit of Trás-os-Montes e Alto Douro, Vila 
Real, Portugal.
(6)Faculty of Medicine, MEDCIDS - Department of Community Medicine, Information 
and Health Decision Sciences, University of Porto, Porto, Portugal; 
CINTESIS@RISE - Health Research Network, University of Porto, Porto, Portugal; 
Internal Medicine Department, Hospital of Santa Luzia, Local Health Unit of Alto 
Minho, Viana do Castelo, Portugal.
(7)Faculty of Medicine, MEDCIDS - Department of Community Medicine, Information 
and Health Decision Sciences, University of Porto, Porto, Portugal; 
CINTESIS@RISE - Health Research Network, University of Porto, Porto, Portugal; 
Almada-Seixal Public Health Unit, Almada-Seixal Local Health Unit, Almada, 
Portugal.
(8)Clinical Epidemiology and Research Center (CERC), IRCCS Humanitas Research 
Hospital & Department of Biomedical Sciences, Humanitas University, Milan, 
Italy.
(9)Department of Medicine, McMaster University, Hamilton, Ontario, Canada.
(10)Health Research Institute of the Balearic Islands (IdISBa), Son Espases 
University Hospital, Palma, Balearic Islands, Spain.
(11)AGH University of Krakow, Kraków, Poland; Department of Hygiene and 
Dietetics, Faculty of Medicine Jagiellonian University Medical College, Krakow, 
Poland.
(12)Clinical Epidemiology and Research Center (CERC), IRCCS Humanitas Research 
Hospital & Department of Biomedical Sciences, Humanitas University, Milan, 
Italy; Fraunhofer Institute for Translational Medicine and Pharmacology ITMP, 
Allergology and Immunology, Berlin, Germany. Electronic address: 
schuneh@mcmaster.ca.
(13)School of Government and School of Computer Science, University of 
Birmingham, Birmingham, United Kingdom.

OBJECTIVES: Published systematic reviews display a heterogeneous methodological 
quality, which can impact decision-making. Large language models (LLMs) can 
support and make the assessment of the methodological quality of systematic 
reviews more efficient, aiding in the incorporation of their evidence in 
guideline recommendations. We aimed to develop an LLM-based tool for supporting 
the assessment of the methodological quality of systematic reviews.
METHODS: We assessed the performance of 8 LLMs in evaluating the methodological 
quality of systematic reviews. In particular, we provided 100 systematic reviews 
for eight LLMs (five base models and three fine-tuned models) to evaluate their 
methodological quality based on a 27-item validated tool (Reported 
Methodological Quality (ReMarQ)). The fine-tuned models had been trained with a 
different sample of 300 manually assessed systematic reviews. We compared the 
answers provided by LLMs with those independently provided by human reviewers, 
computing the accuracy, kappa coefficient and F1-score for this comparison.
RESULTS: The best performing LLM was a fine-tuned GPT-3.5 model (mean accuracy = 
96.5% [95% CI = 89.9%-100%]; mean kappa coefficient = 0.90 [95% CI = 0.71-1.00]; 
mean F1-score = 0.91 [95% CI = 0.83-1.00]). This model displayed an accuracy 
>80% and a kappa coefficient >0.60 for all individual items. When we made this 
LLM assess 60 times the same set of systematic reviews, answers to 18 of 27 
items were always consistent (ie, were always the same) and only 11% of assessed 
systematic reviews showed inconsistency.
CONCLUSION: Overall, LLMs have the potential to accurately support the 
assessment of the methodological quality of systematic reviews based on a 
validated tool comprising dichotomous items.

Copyright © 2025 The Author(s). Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jclinepi.2025.111944
PMID: 40865587 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest There are no 
competing interests for any other author.


175. J Med Internet Res. 2025 Dec 1;27:e84120. doi: 10.2196/84120.

Knowledge-Practice Performance Gap in Clinical Large Language Models: Systematic 
Review of 39 Benchmarks.

Gong EJ(1)(2)(3), Bang CS(1)(2)(3), Lee JJ(3)(4), Baik GH(1)(2).

Author information:
(1)Department of Internal Medicine, Hallym University College of Medicine, 
Chuncheon, Gangwon, Republic of Korea.
(2)Institute for Liver and Digestive Diseases, Hallym University, Chuncheon, 
Gangwon-do, Republic of Korea.
(3)Institute of New Frontier Research, Hallym University College of Medicine, 
Chuncheon, Gangwon, Republic of Korea.
(4)Department of Anesthesiology and Pain Medicine, Hallym University College of 
Medicine, Chuncheon, Gangwon, Republic of Korea.

BACKGROUND: The evaluation of large language models (LLMs) in medicine has 
undergone a shift from knowledge-based testing to practice-based assessment, 
representing an evolution in how we measure artificial intelligence readiness 
for clinical deployment. While LLMs now routinely exceed human performance on 
medical licensing examinations, their translation to clinical practice remains 
poorly characterized.
OBJECTIVE: This systematic review aims to categorize and analyze medical LLM 
benchmarks, examining performance patterns across different evaluation paradigms 
and identifying gaps in current assessment methodologies.
METHODS: The protocol was registered at PROSPERO (CRD420251139729). Four 
databases (MEDLINE/PubMed, Embase/Ovid, Cochrane Library, and arXiv) were 
searched from inception to August 31, 2025, using keywords related to clinical 
medicine benchmarks in LLMs. Studies were included if they (1) investigated 
clinical medicine benchmarks in LLMs, (2) were published in English, and (3) 
were available in full-text. Studies were excluded if they evaluated nonmedical 
domains or lacked benchmark validation. Methodological quality was assessed 
using the Mixed Methods Appraisal Tool (version 2018) by 2 independent reviewers 
(κ=0.91). Due to heterogeneity in evaluation metrics preventing meta-analysis, 
narrative synthesis was conducted using structured categorization of benchmark 
types.
RESULTS: From 3917 screened records, 39 medical LLM benchmarks were identified 
and categorized into 21 (54%) knowledge-based, 15 (38%) practice-based, and 3 
(8%) hybrid frameworks. These benchmarks collectively encompass over 2.3 million 
questions across 45 languages and 172 medical specialties. Traditional 
knowledge-based benchmarks show saturation with leading models achieving 84%-90% 
accuracy on USMLE (United States Medical Licensing Examination)-style 
examinations, approaching or exceeding average physician performance. However, 
practice-based assessments reveal performance challenges, with specific 
benchmarks showing varied results: DiagnosisArena 45.82% (95% CI 42.9%-48.8%), 
MedAgentBench 69.67% (95% CI 64.2%-74.6%), and HealthBench 60% (95% CI 
58.6%-61.3%) success rates, with practice-based benchmarks showing lower 
performance (45%-69%) compared to knowledge benchmarks (84%-90%). Task-specific 
analysis revealed differential performance patterns: factual retrieval 
maintained 85%-93% accuracy, clinical reasoning dropped to 50%-60%, diagnostic 
tasks achieved 45%-55% success, and safety assessment showed significant gaps at 
40%-50% accuracy despite being life-critical. Geographic representation spans 6 
continents with 18 (46%) benchmarks, incorporating non-English content. Quality 
assessment revealed 26% (10/39) of benchmarks had insufficient methodological 
reporting for complete evaluation.
CONCLUSIONS: This systematic review provides the first comprehensive analysis 
quantifying the significant "knowledge-practice gap" in medical artificial 
intelligence: high performance on knowledge-based examinations (84%-90%) does 
not translate to clinical competence (45%-69%), with safety assessments at 
40%-50%. Our findings provide quantitative evidence for regulators and health 
systems that examination scores are insufficient and misleading proxies for 
clinical readiness. This review concludes that autonomous deployment is not 
currently justifiable and that all evidence-based implementation strategies must 
mandate practice-oriented validation and robust human-in-the-loop oversight to 
ensure patient safety.
TRIAL REGISTRATION: PROSPERO CRD420251139729; 
https://www.crd.york.ac.uk/PROSPERO/view/CRD420251139729.

©Eun Jeong Gong, Chang Seok Bang, Jae Jun Lee, Gwang Ho Baik. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org), 
01.12.2025.

DOI: 10.2196/84120
PMCID: PMC12706444
PMID: 41325597 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


176. J Craniofac Surg. 2025 Sep 10. doi: 10.1097/SCS.0000000000011930. Online ahead 
of print.

Multicriteria Assessment of Text Quality in Large Language Model-Generated 
Gynecomastia Materials: DeepSeek Versus OpenAI Versus Claude.

Zang T(1), Li J(2), Wei L(2), Wang Y(2).

Author information:
(1)Department of Aesthetic Plastic Surgery and Laser Medicine, Beijing Anzhen 
Hospital Affiliated to Capital Medical University.
(2)Department of Breast Plastic Surgery, Plastic Surgery Hospital, Chinese 
Academy of Medical Sciences and Peking Union Medical College, Shijingshan, 
Beijing, China.

BACKGROUND: With the development of artificial intelligence, obtaining 
patient-centered medical information through large language models (LLMs) is 
crucial for patient education. However, existing digital resources in online 
health care have heterogeneous quality, and the reliability and readability of 
content generated by various AI models need to be evaluated to meet the needs of 
patients with different levels of cultural literacy.
OBJECTIVE: This study aims to compare the accuracy and readability of different 
LLMs in providing medical information related to gynecomastia, and explore the 
most promising science education tools in practical clinical applications.
METHODS: This study selected 10 most frequently searched questions about 
gynecomastia from PubMed and Google Trends. Responses were generated using 3 
LLMs (DeepSeek-R1, OpenAI-O3, Claude-4-Sonnet), with text quality assessed using 
the DISCERN-AI and PEMAT-AI scales. Text readability and legibility were 
comprehensively evaluated through metrics including word count, syllable count, 
Flesch-Kincaid Grade Level (FKGL), Flesch Kincaid Reading Ease (FKRE), SMOG 
index, and Automated Readability Index (ARI).
RESULTS: In terms of quality evaluation, among the 10 items of the DISCERN-AI 
scale, only the overall content quality score showed a statistically significant 
difference (P = 0.001), with DeepSeek-R1 demonstrating the best performance at a 
median score of 5 (5,5). Regarding readability, DeepSeek-R1 exhibited the 
highest average word count and syllable count, both with P-values of 0.000. The 
3 models showed no significant differences in FKGL, FKRE, or automatic 
readability indices. Specifically, the averaged FKGL scores of DeepSeek-R1 was 
14.08, OpenAI-O3 was 14.1, and Claude-4-sonnet was 13.31. The SOMG evaluation 
revealed that Claude-4-sonnet demonstrated the strongest readability, the 
average value is 11 with a P-value of 0.028.
CONCLUSION: DeepSeek-R1 demonstrated the highest overall quality in content 
generation, followed by Claude-4-sonnet. Evaluations using FKGL, SMOG index, and 
ARI all indicated that Claude-4-sonnet exhibited the best readability. Given 
that improvements in quality and readability can enhance patient engagement and 
reduce anxiety, these 2 models should be prioritized for patient education 
applications. Future efforts should focus on integrating these advantages to 
develop more reliable large-scale medical language models.

Copyright © 2025 by Mutaz B. Habal, MD.

DOI: 10.1097/SCS.0000000000011930
PMID: 40929657

Conflict of interest statement: The authors report no conflicts of interest.


177. J Am Geriatr Soc. 2025 Aug;73(8):2422-2430. doi: 10.1111/jgs.19545. Epub 2025 
May 15.

A Large Language Model Approach to Identifying Preoperative Frailty Among Older 
Adults From Clinical Notes.

Zhou YQ(1), Litake O(1), Meineke MN(1), Tully JL(1), Xu N(2), Abdou W(2), 
Gabriel RA(1)(3).

Author information:
(1)Division of Perioperative Informatics, Department of Anesthesiology, 
University of California, San Diego, La Jolla, California, USA.
(2)School of Medicine, University of California, San Diego, La Jolla, 
California, USA.
(3)Division of Biomedical Informatics, Department of Medicine, University of 
California, San Diego, La Jolla, California, USA.

BACKGROUND: Patients with frailty have a higher risk of major postoperative 
mortality and morbidity. Identifying frailty from the medical record, however, 
is not straightforward since it is a multifactorial state based on multiple 
organ systems and a sum of factors accumulated over time. The objective of this 
study was to develop a large language model-based binary classifier using 
accurately phenotyped datasets to identify preoperative frailty from clinical 
notes.
METHODS: We trained various large language models to identify frailty from 
anesthesia preoperative clinic notes. There were two development datasets used: 
(1) patients undergoing spine surgery whose frailty was characterized by patient 
responses to the Vulnerable Elders-13 Survey (VES-13); and (2) patients 
undergoing surgery whose frailty was characterized by their calculated 
electronic frailty index (eFI) score.
RESULTS: When trained on our VES-13 development set and tested on our VES-13 
validation set, the area under the receiver operating characteristics curve 
(AUC) for the RoBERTa, BERT, BioBERT, and PubMedBERT models was 0.99, 0.64, 
0.67, and 0.73, respectively. When tested on the eFI validation set, the AUCs 
were 0.63, 0.83, 0.87, and 0.87, respectively. Models trained on the eFI 
development dataset did not discriminate frailty adequately when tested on the 
VES-13 validation set.
CONCLUSION: We report the development and validation of a classifier that 
detects older adults at risk for preoperative frailty from preoperative 
anesthesia clinical notes. Large language models can be used to accurately 
identify a difficult-to-quantify and multifactorial characteristic such as 
frailty in patients by using readily available unstructured information from 
clinical notes.

© 2025 The American Geriatrics Society.

DOI: 10.1111/jgs.19545
PMID: 40372061 [Indexed for MEDLINE]


178. PLoS One. 2025 Apr 3;20(4):e0320151. doi: 10.1371/journal.pone.0320151. 
eCollection 2025.

Using artificial intelligence tools to automate data extraction for living 
evidence syntheses.

Mitchell E(1), Are EB(2), Colijn C(2), Earn DJD(1).

Author information:
(1)Department of Mathematics and Statistics, McMaster University, Hamilton, 
ON,Canada.
(2)Department of Mathematics, Simon Fraser University, Burnaby, BC,Canada.

Living evidence synthesis (LES) involves repeatedly updating a systematic review 
or meta-analysis at regular intervals to incorporate new evidence into the 
summary results. It requires a considerable amount of human time investment in 
the article search, collection, and data extraction phases. Tools exist to 
automate the retrieval of relevant journal articles, but pulling data out of 
those articles is currently still a manual process. In this article, we present 
a proof-of-concept Python program that leverages artificial intelligence (AI) 
tools (specifically, ChatGPT) to parse a batch of journal articles and extract 
relevant results, greatly reducing the human time investment in this action 
without compromising on accuracy. Our program is tested on a set of journal 
articles that estimate the mean incubation period for COVID-19, an 
epidemiological parameter of importance for mathematical modelling. We also 
discuss important limitations related to the total amount of information and 
rate at which that information can be sent to the AI engine. This work 
contributes to the ongoing discussion about the use of AI and the role such 
tools can have in scientific research.

Copyright: © 2025 Mitchell et al. This is an open access article distributed 
under the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0320151
PMCID: PMC11967977
PMID: 40179121 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


179. J Med Internet Res. 2025 Aug 18;27:e66100. doi: 10.2196/66100.

AI and Machine Learning Terminology in Medicine, Psychology, and Social 
Sciences: Tutorial and Practical Recommendations.

Cao B(1)(2)(3), Greiner R(1)(2)(4), Greenshaw A(1), Sui J(5).

Author information:
(1)Department of Psychiatry, University of Alberta, 4-142A Katz Group Centre for 
Research, 11315 - 87 Ave NW, Edmonton, AB, T6G 2B7, Canada, 1 7804929576.
(2)Department of Computing Science, Faculty of Science, University of Alberta, 
Edmonton, AB, Canada.
(3)School of Public Health, University of Alberta, Edmonton, AB, Canada.
(4)Alberta Machine Intelligence Institute (Amii), Edmonton, AB, Canada.
(5)School of Psychology, University of Aberdeen, Aberdeen, United Kingdom.

Recent applications of artificial intelligence (AI) and machine learning in 
medicine, psychology, and social sciences have led to common terminological 
confusions. In this paper, we review emerging evidence from systematic reviews 
documenting widespread misuse of key terms, particularly "prediction" being 
applied to studies merely demonstrating association or retrospective analysis. 
We clarify when "prediction" should be used and recommend using "prospective 
prediction" for future prediction; explain validation procedures essential for 
model generalizability; discuss overfitting and generalization in machine 
learning and traditional regression methods; clarify relationships between 
features, independent variables, predictors, risk factors, and causal factors; 
and clarify the hierarchical relationship between AI, machine learning, deep 
learning, large language models, and generative AI. We provide evidence-based 
recommendations for terminology use that can facilitate clearer communication 
among researchers from different disciplines and between the research community 
and the public, ultimately advancing the rigorous application of AI in medicine, 
psychology, and social sciences.

© Bo Cao, Russell Greiner, Andrew Greenshaw, Jie Sui. Originally published in 
the Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/66100
PMCID: PMC12360722
PMID: 40825233 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


180. J Am Med Inform Assoc. 2025 Dec 23:ocaf223. doi: 10.1093/jamia/ocaf223. Online 
ahead of print.

AutoReporter: development of an artificial intelligence tool for automated 
assessment of research reporting guideline adherence.

Chen D(1)(2), Li P(3), Khoshkish E(2), Lee S(2), Ning T(2), Tahir U(2), Wong 
HCY(4), Lee MSF(5), Raman S(6)(7).

Author information:
(1)Princess Margaret Cancer Centre, Radiation Medicine Program, Toronto, ON M5G 
2C4, Canada.
(2)Temerty Faculty of Medicine, University of Toronto, Toronto, ON M5S 3K3, 
Canada.
(3)Faculty of Engineering, McMaster University, Hamilton, ON L8S 4M3, Canada.
(4)Department of Oncology, Princess Margaret Hospital, Hong Kong, China.
(5)Department of Radiation Oncology, National University Cancer Institute, 
National University Hospital, Singapore, Singapore.
(6)Department of Radiation Oncology, BC Cancer, Vancouver, BC V5Z 4E6, Canada.
(7)Division of Radiation Oncology, University of British Columbia, Vancouver, BC 
V5Z 1M9, Canada.

OBJECTIVES: To develop AutoReporter, a large language model (LLM) system that 
automates evaluation of adherence to research reporting guidelines.
MATERIALS AND METHODS: Eight prompt-engineering and retrieval strategies coupled 
with reasoning and general-purpose LLMs were benchmarked on the 
SPIRIT-CONSORT-TM corpus. The top-performing approach, AutoReporter, was 
validated on BenchReport, a novel benchmark dataset of expert-rated reporting 
guideline assessments from 10 systematic reviews.
RESULTS: AutoReporter, a zero-shot, no-retrieval prompt coupled with the o3-mini 
reasoning LLM, demonstrated strong accuracy (CONSORT 90.09%; SPIRIT: 92.07%), 
substantial agreement with humans (CONSORT Cohen's κ = 0.70, SPIRIT Cohen's 
κ = 0.77), runtime (CONSORT: 617.26 s; SPIRIT: 544.51 s), and cost (CONSORT: 
0.68 USD; SPIRIT: 0.65 USD). AutoReporter achieved a mean accuracy of 91.8% and 
substantial agreement (Cohen's κ > 0.6) with expert ratings from the BenchReport 
benchmark.
DISCUSSION: Structured prompting alone can match or exceed fine-tuned domain 
models while forgoing manually annotated corpora and computationally intensive 
training.
CONCLUSION: Large language models can feasibly automate reporting guideline 
adherence assessments for scalable quality control in scientific research 
reporting. AutoReporter is publicly accessible at 
https://autoreporter.streamlit.app.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf223
PMID: 41435508


181. Int J Med Inform. 2025 Nov;203:106026. doi: 10.1016/j.ijmedinf.2025.106026. Epub 
2025 Jun 25.

Comparing the accuracy of large language models and prompt engineering in 
diagnosing realworld cases.

Yao G(1), Zhang W(2), Zhu Y(2), Wong UK(2), Zhang Y(2), Yang C(3), Shen G, Li 
Z(4), Gao H(5).

Author information:
(1)School of Information Engineering, Beijing Polytechnic College, Beijing, 
China.
(2)Department of Rheumatology and Immunology, Peking University International 
Hospital, Beijing, China.
(3)Department of Surgery, University Medicine Mannheim, Medical Faculty 
Mannheim, University of Heidelberg, Mannheim, Germany; AI Health Innovation 
Cluster, German Cancer Research Center (DKFZ), Heidelberg, Germany.
(4)Department of Rheumatology and Immunology, Peking University People's 
Hospital, Beijing, China. Electronic address: li99@bjmu.edu.cn.
(5)Department of Rheumatology and Immunology, Peking University International 
Hospital, Beijing, China. Electronic address: gaohui_2025@163.com.

IMPORTANCE: Large language models (LLMs) hold potential in clinical 
decision-making, especially for complex and rare disease diagnoses. However, 
real-world applications require further evaluation for accuracy and utility.
OBJECTIVE: To evaluate the diagnostic performance of four LLMs (GPT-4o mini, 
GPT-4o, ERNIE, and Llama-3) using real-world inpatient medical records and 
assess the impact of different prompt engineering methods.
METHOD: This single-center, retrospective study was conducted at Peking 
University International Hospital. It involved 1,122 medical records categorized 
into common rheumatic autoimmune diseases, rare rheumatic autoimmune diseases, 
and non-rheumatic diseases. Four LLMs were evaluated using two prompt 
engineering methods: few-shot and chain-of-thought prompting. Diagnostic 
accuracy (hit1) was defined as the inclusion of the first final diagnosis from 
the medical record in the model's top prediction.
RESULTS: Hit1 of four LLMs were as follows: GPT-4omini (81.8 %), GPT-4o 
(82.4 %), ERNIE (82.9 %) and Llama-3 (82.7 %). Few-shot prompting significantly 
improved GPT-4o's hit1 (85.9 %) compared to its base model (p = 0.02), 
outperforming other models (all p < 0.05). Chain-of-thought prompting showed no 
significant improvement. Hit1 for both common and rare rheumatic diseases was 
consistently higher than that for non-rheumatic disease. Few-shot prompting 
increased costs per correct diagnosis for GPT-4o by approximately ¥4.54.
CONCLUSIONS: LLMs, including GPT-4o, demonstrate promising diagnostic accuracy 
on real medical records. Few-shot prompting enhances performance but at higher 
costs, underscoring the need for accuracy improvements and cost management. 
These findings inform LLM development in Chinese medical contexts and highlight 
the necessity for further multi-center validation.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106026
PMID: 40617017 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


182. J Plast Reconstr Aesthet Surg. 2025 May;104:414-416. doi: 
10.1016/j.bjps.2025.03.053. Epub 2025 Mar 31.

Evidence-based artificial intelligence: Implementing retrieval-augmented 
generation models to enhance clinical decision support in plastic surgery.

Ozmen BB(1), Mathur P(2).

Author information:
(1)Department of Plastic Surgery, Cleveland Clinic, Cleveland, OH, USA. 
Electronic address: ozmenb@ccf.org.
(2)Department of General Anesthesiology, Cleveland Clinic, Cleveland, OH, USA; 
BrainXAI ReSearch, BrainX LLC, Cleveland, OH, USA.

The rapid advancement of large language models (LLMs) has generated significant 
enthusiasm within healthcare, especially in supporting clinical decision-making 
and patient management. However, inherent limitations including hallucinations, 
outdated clinical context, and unreliable references pose serious concerns for 
their clinical utility. Retrieval-Augmented Generation (RAG) models address 
these limitations by integrating validated, curated medical literature directly 
into AI workflows, significantly enhancing the accuracy, relevance, and 
transparency of generated outputs. This viewpoint discusses how RAG frameworks 
can specifically benefit plastic and reconstructive surgery by providing 
contextually accurate, evidence-based, and clinically grounded support for 
decision-making. Potential clinical applications include clinical decision 
support, efficient evidence synthesis, customizable patient education, informed 
consent materials, multilingual capabilities, and structured surgical 
documentation. By querying specialized databases that incorporate contemporary 
guidelines and literature, RAG models can markedly reduce inaccuracies and 
increase the reliability of AI-generated responses. However, the implementation 
of RAG technology demands rigorous database curation, regular updating with 
guidelines from surgical societies, and ongoing validation to maintain clinical 
relevance. Addressing challenges related to data privacy, governance, ethical 
considerations, and user training remains critical for successful clinical 
adoption. In conclusion, RAG models represent a significant advancement in 
overcoming traditional LLM limitations, promoting transparency and clinical 
accuracy with great potential for plastic surgery. Plastic surgeons and 
researchers are encouraged to explore and integrate these innovative generative 
AI frameworks to enhance patient care, surgical outcomes, communication, 
documentation quality, and education.

Copyright © 2025 The Author(s). Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.bjps.2025.03.053
PMID: 40174259 [Indexed for MEDLINE]


183. Aesthetic Plast Surg. 2025 Apr;49(7):2154-2155. doi: 10.1007/s00266-024-04193-w. 
Epub 2024 Jun 18.

ChatGPT and Clinical Questions on the Practical Guideline of Blepharoptosis: 
Reply.

Shiraishi M(1), Tomioka Y(2), Okazaki M(2).

Author information:
(1)Department of Plastic and Reconstructive Surgery, The University of Tokyo 
Hospital, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8655, Japan. 
shiraishi-kyf@umin.ac.jp.
(2)Department of Plastic and Reconstructive Surgery, The University of Tokyo 
Hospital, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8655, Japan.

In a recent Letter to the Editor authored by Daungsupawong et al. in Aesthetic 
Plastic Surgery, titled "ChatGPT and Clinical Questions on the Practical 
Guideline of Blepharoptosis: Correspondence," the authors emphasized important 
points regarding the input language differences between input and output 
references. However, advanced versions, such as GPT-4, have shown marginal 
differences between English and Chinese inputs, possibly because of the use of 
larger training data. To address this issue, non-English-language-oriented large 
language models (LLMs) have been developed. The ability of LLMs to refer to 
existing references varies, with newer models, such as GPT-4, showing higher 
reference rates than GPT-3.5. Future research should focus on addressing the 
current limitations and enhancing the effectiveness of emerging LLMs in 
providing accurate and informative answers to medical questions across multiple 
languages.Level of Evidence V This journal requires that authors assign a level 
of evidence to each article. For a full description of these Evidence-Based 
Medicine ratings, please refer to the Table of Contents or the online 
Instructions to Authors www.springer.com/00266 .

© 2024. Springer Science+Business Media, LLC, part of Springer Nature and 
International Society of Aesthetic Plastic Surgery.

DOI: 10.1007/s00266-024-04193-w
PMID: 38890161 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare that they have no conflicts of interest to disclose. Human and Animal 
Rights: This article does not contain any studies with human participants or 
animals performed by any of the authors. Informed Consent: For this type of 
study informed consent is not required.


184. IEEE J Biomed Health Inform. 2025 Nov 13;PP. doi: 10.1109/JBHI.2025.3631538. 
Online ahead of print.

Applications of Large Language Models and Prompt Optimization for Knowledge 
Extraction from Biological Pathway Figures.

Azam M, Zeng S, Aldihis H, Popescu M, Xu D.

Recent developments in Large Language Models (LLMs) have demonstrated remarkable 
capabilities for image comprehension. This study aims to automate and enhance 
the extraction of gene interactions from biological pathway images by 
integrating LLMs and a Genetic Algorithm (GA). A dataset of 200 tumor signaling 
pathway figures from the recent biological literature was employed to assess the 
performance of four AI chatbots: GPT-4oV, Claude-3.5V, Gemini-1.5V, and 
Llama-3.2V, with GA used to optimize prompts for each model. Model performance 
was evaluated on both directional and non-directional gene relationship 
extraction. GA-optimized prompts significantly improved extraction accuracies 
across all LLMs, with GPT 4oV achieving an F1-score of 0.645 (±0.055) and 
Llama-3.2V achieving an F1-score of 0.616 (±0.068). For non-directional 
interactions, GPT-4oV outperformed other models, reaching a precision of 0.805, 
a recall of 0.695, and an F1 score of 0.757, followed by Llama-3.2V and 
Claude-3.5V with F1-scores of 0.702 and 0.697, respectively, while Gemini-1.5V 
lagged with 0.612. In directional interaction predictions, all models performed 
lower, with GPT-4oV leading at 0.687 F1-score, followed by Llama-3.2V at 0.656, 
Claude-3.5V at 0.641, and Gemini-1.5V at 0.573. While these results demonstrate 
substantial improvements over traditional OCR-based approaches, further advances 
in model accuracy and explainability are needed for widespread adoption in 
critical biomedical applications. Nevertheless, these findings provide a 
valuable benchmark for the research community and a foundation for future 
development of specialized, fine-tuned models and scalable multimodal AI 
frameworks in biomedical data analysis. The source code is publicly available on 
https://github.com/Muh-aza/LLM_GPV.

DOI: 10.1109/JBHI.2025.3631538
PMID: 41231698


185. BMC Med Educ. 2025 Aug 19;25(1):1175. doi: 10.1186/s12909-025-07750-2.

Effectiveness of generative artificial intelligence-based teaching versus 
traditional teaching methods in medical education: a meta-analysis of randomized 
controlled trials.

Li J(#)(1)(2), Yin K(#)(1)(2), Wang Y(3), Jiang X(4)(5)(6), Chen D(7)(8).

Author information:
(1)Department of Anesthesiology, West China Second University Hospital, Sichuan 
University, Chengdu, China.
(2)Key Laboratory of Birth Defects and Related Diseases of Women and Children, 
Ministry of Education, Chengdu, China.
(3)Key Laboratory of BioResource and Eco-Environment of Ministry of Education, 
College of Life Science, Sichuan University, Chengdu, China.
(4)Department of Anesthesiology, West China Second University Hospital, Sichuan 
University, Chengdu, China. 1598862657jxq@scu.edu.cn.
(5)Key Laboratory of Birth Defects and Related Diseases of Women and Children, 
Ministry of Education, Chengdu, China. 1598862657jxq@scu.edu.cn.
(6)Department of Anesthesiology, Chengdu Hi-Tech Zone Hospital for Women and 
Children, Chengdu, China. 1598862657jxq@scu.edu.cn.
(7)Department of Anesthesiology, West China Second University Hospital, Sichuan 
University, Chengdu, China. scucdx@foxmail.com.
(8)Key Laboratory of Birth Defects and Related Diseases of Women and Children, 
Ministry of Education, Chengdu, China. scucdx@foxmail.com.
(#)Contributed equally

BACKGROUND: Artificial intelligence (AI) has demonstrated remarkable 
capabilities across diverse medical applications, potentially revolutionizing 
healthcare delivery systems. This systematic review and meta-analysis 
investigated the comparative effectiveness of generative artificial intelligence 
(GAI)-based teaching methodologies versus conventional pedagogical approaches on 
educational outcomes among medical students.
METHODS: We conducted a comprehensive literature search across multiple 
electronic databases, including PubMed, Cochrane Library, EMBASE, and Web of 
Science, encompassing studies published from January 2014 through January 2025. 
The review focused on randomized controlled trials (RCTs) that compared 
GAI-based teaching interventions with traditional instructional teaching methods 
in medical students.
RESULTS: The meta-analysis incorporated 11 eligible RCTs, comprising 786 medical 
students. Pooled analysis revealed no statistically significant difference in 
knowledge acquisition scores between GAI-based and traditional teaching 
approaches (standardized mean difference [SMD] 0.27, 95% confidence interval 
[CI] -0.31 to 0.85; p = 0.36). However, subgroup analysis indicated enhanced 
knowledge performance in the GAI group specifically for extended learning 
periods (exceeding one week) and practice-oriented courses. GAI-based 
instruction demonstrated superior outcomes in practical skill development 
compared to conventional methods (SMD 0.63, 95% CI 0.10-1.16; p = 0.02). 
Students in the GAI group reported significantly higher satisfaction scores with 
their learning experience.
CONCLUSION: While theoretical knowledge acquisition remains comparable between 
teaching modalities, the distinctive advantages of GAI-based approaches in 
practical skill development warrant their integration into medical curricula. 
Future research should focus on optimizing the integration of GAI-based teaching 
methods, standardizing implementation protocols, and evaluating long-term 
educational outcomes.
TRIAL REGISTRATION: This protocol was registered on the International Platform 
of Registered Systematic Review and Meta-analysis Protocols (INPLASY) with the 
registration number INPLASY202510006. Registered on 2 January 2025.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-07750-2
PMCID: PMC12362899
PMID: 40830476 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Ethical approval and patient consent were not required as all 
analyses were based on previously published studies. Consent for publication: 
Not applicable. Competing interests: The authors declare no competing interests.


186. Acad Radiol. 2025 Oct;32(10):5869-5877. doi: 10.1016/j.acra.2025.07.041. Epub 
2025 Aug 8.

LLM-Based Extraction of Imaging Features from Radiology Reports: Automating 
Disease Activity Scoring in Crohn's Disease.

Dehdab R(1), Mankertz F(1), Brendel JM(1), Maalouf N(1), Kaya K(2), Afat S(1), 
Kolahdoozan S(3), Radmard AR(4).

Author information:
(1)Department of Radiology, Tuebingen University Hospital, University of 
Tuebingen, Tuebingen, Germany (R.D., F.M., J.M.B., N.M., S.A.).
(2)Institute for Diagnostic and Interventional Radiology, Faculty of Medicine 
and University Hospital Cologne, University of Cologne, Cologne, Germany (K.K.).
(3)Digestive Diseases Research Center, Digestive Diseases Research Institute, 
Tehran University of Medical Sciences, Tehran, Iran (S.K.).
(4)Department of Radiology, Shariati Hospital, Tehran University of Medical 
Sciences, Tehran, Iran (A.R.R.). Electronic address: amir.radmard@gmail.com.

RATIONALE AND OBJECTIVES: Large Language Models (LLMs) offer a promising 
solution for extracting structured clinical information from free-text radiology 
reports. The Simplified Magnetic Resonance Index of Activity (sMARIA) is a 
validated scoring system used to quantify Crohn's disease (CD) activity based on 
Magnetic Resonance Enterography (MRE) findings. This study aims to evaluate the 
performance of two advanced LLMs in extracting key imaging features and 
computing sMARIA scores from free-text MRE reports.
MATERIALS AND METHODS: This retrospective study included 117 anonymized 
free-text MRE reports from patients with confirmed CD. ChatGPT (GPT-4o) and 
DeepSeek (DeepSeek-R1) were prompted using a structured input designed to 
extract four key radiologic features relevant to sMARIA: bowel wall thickness, 
mural edema, perienteric fat stranding, and ulceration. LLM outputs were 
evaluated against radiologist annotations at both the segment and feature 
levels. Segment-level agreement was assessed using accuracy, mean absolute error 
(MAE) and Pearson correlation. Feature-level performance was evaluated using 
sensitivity, specificity, precision, and F1-score. Errors including 
confabulations were recorded descriptively.
RESULTS: ChatGPT achieved a segment-level accuracy of 98.6%, MAE of 0.17, and 
Pearson correlation of 0.99. DeepSeek achieved 97.3% accuracy, MAE of 0.51, and 
correlation of 0.96. At the feature level, ChatGPT yielded an F1-score of 98.8% 
(precision 97.8%, sensitivity 99.9%), while DeepSeek achieved 97.9% (precision 
96.0%, sensitivity 99.8%).
CONCLUSIONS: LLMs demonstrate near-human accuracy in extracting structured 
information and computing sMARIA scores from free-text MRE reports. This enables 
automated assessment of CD activity without altering current reporting 
workflows, supporting longitudinal monitoring and large-scale research. 
Integration into clinical decision support systems may be feasible in the 
future, provided appropriate human oversight and validation are ensured.

Copyright © 2025 The Association of University Radiologists. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.acra.2025.07.041
PMID: 40783343 [Indexed for MEDLINE]


187. IEEE J Biomed Health Inform. 2025 Dec 11;PP. doi: 10.1109/JBHI.2025.3631841. 
Online ahead of print.

An LLM Method for Understanding Traditional Chinese Medicine: Mechanism 
Exploration and Innovative Application.

Li YX, Elnaffar S, Chen HY, Chen NJ, Lai PY, Li NQ, Chong Y, Qiao J, Liu T, Peng 
ZB, Xu DY, Lai JH, Wang CD, Hu JQ.

Large language models (LLMs) show promise in medical knowledge representation 
but struggle with dynamic clinical workflows and personalized treatment in 
complex systems like Traditional Chinese Medicine (TCM). We propose an efficient 
and novel LLM framework for TCM mechanism exploration and clinical application, 
combining incremental domain-specific pre-training, multi-task supervised 
fine-tuning, and Chain-of-Thought (CoT) reasoning. Our two-stage 
approach-"Understanding and Inheritance" followed by "Exploration and 
Innovation"-uniquely leverages a heterogeneous database of 100,538 records from 
19 TCM physicians to model the core TCM principle of "different treatments for 
the same disease". Six downstream tasks assess clinical capabilities, including 
personalized prescription generation (Task 3). After incremental pre-training, 
the model improves BLEU-4 by 1,313% over baseline, reaching 41.26-43.21 after 
fine-tuning. We quantify physician-specific variations and formally validate the 
decisive role of basic formulas-removing them causes a 23.9% performance drop. 
Cross-school evaluations confirm robust generalization, with 22.8 BLEU-4 on 
external data. CoT annotation boosts performance by 20% using only 10% labeled 
data, demonstrating high data efficiency. The model captures TCM's "different 
treatments for the same disease" principle and preserves school-specific 
diagnostic logic. This work advances intelligent TCM inheritance and paves the 
way for AI-driven personalized medicine.

DOI: 10.1109/JBHI.2025.3631841
PMID: 41379900


188. Plast Reconstr Surg Glob Open. 2025 Oct 2;13(10):e7176. doi: 
10.1097/GOX.0000000000007176. eCollection 2025 Oct.

Reliability and Accuracy of Generative Artificial Intelligence Tools in 
Providing General Information on Migraine Surgery.

Raposio E(1)(2), Baldelli I(1)(2).

Author information:
(1)From the Department of Surgical Sciences and Integrated Diagnostics (DISC), 
University of Genova, Genova, Italy.
(2)Plastic and Reconstructive Surgery Division, IRCCS Ospedale Policlinico San 
Martino, Genova, Italy.

BACKGROUND: Numerous publicly available generative artificial intelligence tools 
have been introduced, aimed at research use or for the general public. The aim 
of this study was to investigate what information could be obtained by querying 
9 of these publicly available software tools regarding the possible outcomes and 
complications of migraine surgery.
METHODS: We consulted 9 of the most well-known and widely used generative 
artificial intelligence tools: ChatGPT, Gemini, Perplexity, Elicit, SciSpace, 
Consensus, PaperPal, Julius, and Mistral AI. Each tool was asked the same 
question: "Detail the outcomes and complications of migraine surgery."
RESULTS: The results we obtained were, on the whole, very satisfactory. All the 
tools used, in addition to being extremely fast, provided sensible and 
scientifically reliable answers, albeit with varying degrees of accuracy. The 
accuracy of the answers provided was found to be directly proportional to the 
number of scientific references each tool used to generate its response. The 
tools that provided the most complete and detailed answers were, in order, 
Consensus, SciSpace, Perplexity, and ChatGPT.
CONCLUSIONS: In the answers provided by all the software, the universally 
accepted principles of migraine surgery were accurately listed, both in terms of 
outcomes and the rate and severity of complications. Regarding outcomes, the 
high success rate of the procedure was consistently emphasized.

Copyright © 2025 The Authors. Published by Wolters Kluwer Health, Inc. on behalf 
of The American Society of Plastic Surgeons.

DOI: 10.1097/GOX.0000000000007176
PMCID: PMC12490650
PMID: 41050980

Conflict of interest statement: The authors have no financial interest to 
declare in relation to the content of this article.


189. JMIR Med Educ. 2025 Nov 14;11:e73469. doi: 10.2196/73469.

Evaluating the Performance of DeepSeek-R1 and DeepSeek-V3 Versus OpenAI Models 
in the Chinese National Medical Licensing Examination: Cross-Sectional 
Comparative Study.

Wang W(#)(1), Zhou Y(#)(1)(2), Fu J(#)(3), Hu K(1).

Author information:
(1)Department of Radiation Oncology, Peking Union Medical College Hospital, 
Chinese Academy of Medical Sciences & Peking Union Medical College, Beijing, 
China.
(2)Tsinghua Medicine, School of Medicine, Tsinghua University, Beijing, China.
(3)Department of clinical laboratory, Xuanwu Hospital, Capital Medical 
University, Beijing, China.
(#)Contributed equally

BACKGROUND: Deepseek-R1, an open-source large language model (LLM), has 
generated significant global interest in the past months.
OBJECTIVE: This study aimed to compare the performance of DeepSeek and OpenAI 
LLMs on the Chinese National Medical Licensing Examination (NMLE) and evaluate 
their potential in medical education.
METHODS: This cross-sectional study assessed 2 DeepSeek models (DeepSeek-R1 and 
DeepSeek-V3), 3 OpenAI models (ChatGPT-o1 pro, ChatGPT-o3 mini, and GPT-4o), and 
2 additional Chinese LLMs (ERNIE 4.5 Turbo and Qwen 3) using the 2021 NMLE. 
Model performance was evaluated based on overall accuracy, accuracy across 
question types (A1, A2, A3 and A4, and B1), case analysis and non-case analysis 
questions, medical specialties, and accuracy consensus between different model 
combinations.
RESULTS: All LLMs successfully passed the NMLE. DeepSeek-R1 achieved the highest 
accuracy (573/597, 96%), followed by DeepSeek-V3 (558/600, 93%), both of which 
significantly outperformed ChatGPT-o1 pro (450/600, 75%), ChatGPT-o3 mini 
(455/600, 75.8%), and GPT-4o (452/600, 75.3%; P<.001 for all comparisons). 
Performance disparities were consistent across various question types (A1, A2, 
A3 and A4, and B1), case analysis and non-case analysis questions, different 
types of case analyses, and medical specialties. The accuracy consensus between 
DeepSeek-R1 and DeepSeek-V3 reached 97.7% (544/557), significantly outperforming 
DeepSeek-R1 alone (P=.04). Two additional Chinese LLMs, ERNIE 4.5 Turbo 
(572/600, 95.3%) and Qwen 3 (555/600, 92.5%), also exhibited significantly 
better performance compared to the 3 OpenAI models (all P<.001).
CONCLUSIONS: This study demonstrates that DeepSeek-R1 and DeepSeek-V3 
significantly outperform OpenAI models on the NMLE. DeepSeek models show promise 
as tools for medical education and exam preparation in the Chinese language.

©Weiping Wang, Yuchen Zhou, Jingxuan Fu, Ke Hu. Originally published in JMIR 
Medical Education (https://mededu.jmir.org), 14.11.2025.

DOI: 10.2196/73469
PMCID: PMC12663704
PMID: 41237388 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


190. Int J Med Inform. 2025 Mar;195:105746. doi: 10.1016/j.ijmedinf.2024.105746. Epub 
2024 Dec 4.

CPRS: a clinical protocol recommendation system based on LLMs.

Ruan J(1), Su Q(2), Chen Z(1), Huang J(3), Li Y(4).

Author information:
(1)School of Electronic and Electrical Engineering, Shanghai University of 
Engineering Science, ShangHai, China.
(2)School of Electronic and Electrical Engineering, Shanghai University of 
Engineering Science, ShangHai, China; College of Computer Science and 
Technology, Xinjiang Normal University, China. Electronic address: 
suqm@sues.edu.cn.
(3)Center for Drug Clinical Research, Shanghai University of Traditional Chinese 
Medicine, ShangHai, China.
(4)Department of Hepatology Longhua Hospital, Shanghai University of Traditional 
Chinese Medicine, ShangHai, China.

BACKGROUND: As fundamental documents in clinical trials, clinical trial 
protocols are intended to ensure that trials are conducted according to the 
objectives set by researchers. The advent of large models with superior semantic 
performance compared to traditional models provides fresh perspectives for 
research recommendations in clinical trial protocols.
METHOD: A clinical trial protocol recommendation system based on Large Language 
Models (LLMs) is proposed in this paper, combining GPT-4 and knowledge graph to 
assist in clinical trial protocol recommendations. Using knowledge graphs as an 
auxiliary tool, a finite set of clinical trial projects with similar features is 
identified. Subsequently, through the semantic capabilities of GPT-4, targeted 
recommendations are made to patients.
RESULTS: Experiments were conducted to compare GPT-4 and multiple models from 
the SBERT family that handle semantic similarity. The results indicate that 
GPT-4 is capable of better sorting clinical trial protocols based on similarity 
criteria and offering targeted recommendations to patients. Consequently, this 
capability meets the matching requirements between projects and patients and 
enhances the automation of clinical trial protocol recommendations. 
Additionally, in the future, personal factors of patients will be fully 
considered during the recommendation process to provide more accurate and 
personalized protocol recommendations.
CONCLUSION: By integrating knowledge graphs and LLMs, a better understanding and 
processing of clinical trial protocol information can be achieved, enabling the 
recommendation of appropriate protocols for patients and enhancing both matching 
efficiency and accuracy. Furthermore, the application of this system contributes 
to the automation of clinical trial protocol recommendations, playing a crucial 
role in medical research institutions such as clinical trial research institutes 
and public health management departments. Additionally, it significantly aids in 
advancing the development of clinical trials and the medical field at large.

Copyright © 2024 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2024.105746
PMID: 39644792 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: QianMin SU reports financial 
support was provided by Shanghai Municipal Administration of Traditional Chinese 
Medicine Clinical Project. If there are other authors, they declare that they 
have no known competing financial interests or personal relationships that could 
have appeared to influence the work reported in this paper.


191. J Med Internet Res. 2025 May 7;27:e67830. doi: 10.2196/67830.

Comparing Artificial Intelligence-Generated and Clinician-Created Personalized 
Self-Management Guidance for Patients With Knee Osteoarthritis: Blinded 
Observational Study.

Du K(#)(1), Li A(#)(1), Zuo QH(1), Zhang CY(1), Guo R(2), Chen P(2), Du WS(2), 
Li SM(2).

Author information:
(1)Beijing University of Chinese Medicine, Beijing, China.
(2)Beijing Hospital of Traditional Chinese Medicine, Beijing, China.
(#)Contributed equally

BACKGROUND: Knee osteoarthritis is a prevalent, chronic musculoskeletal disorder 
that impairs mobility and quality of life. Personalized patient education aims 
to improve self-management and adherence; yet, its delivery is often limited by 
time constraints, clinician workload, and the heterogeneity of patient needs. 
Recent advances in large language models offer potential solutions. GPT-4 
(OpenAI), distinguished by its long-context reasoning and adoption in clinical 
artificial intelligence research, emerged as a leading candidate for 
personalized health communication. However, its application in generating 
condition-specific educational guidance remains underexplored, and concerns 
about misinformation, personalization limits, and ethical oversight remain.
OBJECTIVE: We evaluated GPT-4's ability to generate individualized 
self-management guidance for patients with knee osteoarthritis in comparison 
with clinician-created content.
METHODS: This 2-phase, double-blind, observational study used data from 50 
patients previously enrolled in a registered randomized trial. In phase 1, 2 
orthopedic clinicians each generated personalized education materials for 25 
patient profiles using anonymized clinical data, including history, symptoms, 
and lifestyle. In phase 2, the same datasets were processed by GPT-4 using 
standardized prompts. All content was anonymized and evaluated by 2 independent, 
blinded clinical experts using validated scoring systems. Evaluation criteria 
included efficiency, readability (Flesch-Kincaid, Gunning Fog, Coleman-Liau, and 
Simple Measure of Gobbledygook), accuracy, personalization, and 
comprehensiveness and safety. Disagreements between reviewers were resolved 
through consensus or third-party adjudication.
RESULTS: GPT-4 outperformed clinicians in content generation speed (530.03 vs 
37.29 words per min, P<.001). Readability was better on the Flesch-Kincaid (mean 
11.56, SD 1.08 vs mean 12.67 SD 0.95), Gunning Fog (mean 12.47, SD 1.36 vs mean 
14.56, SD 0.93), and Simple Measure of Gobbledygook (mean 13.33, SD 1.00 vs mean 
13.81 SD 0.69) indices (all P<.001), though GPT-4 scored slightly higher on the 
Coleman-Liau Index (mean 15.90, SD 1.03 vs mean 15.15, SD 0.91). GPT-4 also 
outperformed clinicians in accuracy (mean 5.31, SD 1.73 vs mean 4.76, SD 1.10; 
P=.05, personalization (mean 54.32, SD 6.21 vs mean 33.20, SD 5.40; P<.001), 
comprehensiveness (mean 51.74, SD 6.47 vs mean 35.26, SD 6.66; P<.001), and 
safety (median 61, IQR 58-66 vs median 50, IQR 47-55.25; P<.001).
CONCLUSIONS: GPT-4 could generate personalized self-management guidance for knee 
osteoarthritis with greater efficiency, accuracy, personalization, 
comprehensiveness, and safety than clinician-generated content, as assessed 
using standardized, guideline-aligned evaluation frameworks. These findings 
underscore the potential of large language models to support scalable, 
high-quality patient education in chronic disease management. The observed 
lexical complexity suggests the need to refine outputs for populations with 
limited health literacy. As an exploratory, single-center study, these results 
warrant confirmation in larger, multicenter cohorts with diverse demographic 
profiles. Future implementation should be guided by ethical and operational 
safeguards, including data privacy, transparency, and the delineation of 
clinical responsibility. Hybrid models integrating artificial 
intelligence-generated content with clinician oversight may offer a pragmatic 
path forward.

©Kai Du, Ao Li, Qi-Heng Zuo, Chen-Yu Zhang, Ren Guo, Ping Chen, Wei-Shuai Du, 
Shu-Ming Li. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 07.05.2025.

DOI: 10.2196/67830
PMCID: PMC12096024
PMID: 40332991 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


192. J Am Med Inform Assoc. 2025 Sep 1;32(9):1471-1476. doi: 10.1093/jamia/ocaf117.

Automated analyses of risk of bias and critical appraisal of systematic reviews 
(ROBIS and AMSTAR 2): a comparison of the performance of 4 large language 
models.

Forero DA(1), Abreu SE(2), Tovar BE(3), Oermann MH(4).

Author information:
(1)School of Health and Sport Sciences, Fundación Universitaria del Área Andina, 
Bogotá, 110231, Colombia.
(2)Psychology Program, Fundación Universitaria del Área Andina, Medellín, 
050005, Colombia.
(3)Nursing Program, School of Health and Sport Sciences, Fundación Universitaria 
del Área Andina, Bogotá, 110231, Colombia.
(4)School of Nursing, Duke University School of Nursing, 27710, United States.

OBJECTIVES: To explore the performance of 4 large language model (LLM) chatbots 
for the analysis of 2 of the most commonly used tools for the advanced analysis 
of systematic reviews (SRs) and meta-analyses.
MATERIALS AND METHODS: We explored the performance of 4 LLM chatbots (ChatGPT, 
Gemini, DeepSeek, and QWEN) for the analysis of ROBIS and AMSTAR 2 tools (sample 
sizes: 20 SRs), in comparison with assessments by human experts.
RESULTS: Gemini showed the best agreement with human experts for both ROBIS and 
AMSTAR 2 (accuracy: 58% and 70%). The second best LLM chatbots were ChatGPT and 
QWEN, for ROBIS and AMSTAR 2, respectively.
DISCUSSION: Some LLM chatbots underestimated the risk of bias or overestimated 
the confidence of the results in published SRs, which is compatible with recent 
articles for other tools.
CONCLUSION: This is one of the first studies comparing the performance of 
several LLM chatbots for the automated analyses of ROBIS and AMSTAR 2.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf117
PMCID: PMC12361857
PMID: 40680299 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
declare.


193. Health Informatics J. 2025 Oct-Dec;31(4):14604582251387649. doi: 
10.1177/14604582251387649. Epub 2025 Nov 6.

Integrating clinical guidelines with large language models for improved sepsis 
mortality prediction.

Zhao Z(1), An B(2), Zhang T(1), Zhu R(1), Fan Z(1), Wang G(1).

Author information:
(1)Department of Emergency Medicine, Beijing Friendship Hospital affiliated 
Capital Medical University, China.
(2)Chinese Academy of Social Sciences, Institute of Ethnology and Anthropology, 
Beijing, China.

We develop and validate a clinical guideline-integrated LLM for enhanced sepsis 
mortality prediction. Using MIMIC-IV data from 24,237 ICU sepsis patients, we 
fine-tuned a large language model with Low-Rank Adaptation, embedding clinical 
guidelines into the training process. The model's predictive performance was 
evaluated using accuracy, F1-score, sensitivity, specificity, and area under the 
receiver operating characteristic curve (AUC). Ablation studies assessed the 
specific contributions of clinical guideline integration. The guideline-enhanced 
fine-tuned LLM demonstrated moderately higher performance across all evaluation 
metrics including predictive accuracy (0.819), F1-score (0.815), sensitivity 
(0.815), specificity (0.822), and AUC (0.852) in predicting mortality risk for 
septic patients compared to traditional machine learning (highest accuracy: 
0.774, AUC: 0.850) and deep learning methods (highest accuracy: 0.762, AUC: 
0.841). Ablation experiments demonstrated that explicit integration of clinical 
guideline knowledge substantially improved performance over both direct 
prompting (accuracy: 0.709, AUC: 0.706) and fine-tuning without clinical 
guidelines (accuracy: 0.786, AUC: 0.801). These findings demonstrate 
that incorporating clinical guidelines into the fine-tuning of large language 
models outperforms both traditional and deep learning baselines across multiple 
metrics in sepsis mortality prediction, highlighting the value of explicit 
domain knowledge integration for clinical AI's robustness.

DOI: 10.1177/14604582251387649
PMID: 41195684 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of conflicting interestsThe authors 
declared no potential conflicts of interest with respect to the research, 
authorship, and/or publication of this article.


194. J Am Board Fam Med. 2025 Sep 15;38(3):599-602. doi: 10.3122/jabfm.2024.240385R1.

Leveraging Large Language Models to Advance Certification, Physician Learning, 
and Diagnostic Excellence.

Wang T(1), Price DW(2), Bazemore AW(2).

Author information:
(1)From the American Board of Family Medicine, Lexington, KY (TW, DWP, AWB); 
Department of Family Medicine, University of Colorado School Anschutz School of 
Medicine, Aurora, CO (DWP). twang@theabfm.org.
(2)From the American Board of Family Medicine, Lexington, KY (TW, DWP, AWB); 
Department of Family Medicine, University of Colorado School Anschutz School of 
Medicine, Aurora, CO (DWP).

Diagnostic errors are a significant challenge in health care, often resulting 
from gaps in physicians' knowledge and misalignment between confidence and 
diagnostic accuracy. Traditional educational methods have not sufficiently 
addressed these issues. This commentary explores how large language models 
(LLMs), a subset of artificial intelligence, can enhance diagnostic education by 
improving learning transfer and physicians' diagnostic accuracy. The American 
Board of Family Medicine (ABFM) is integrating LLMs into its Continuous 
Knowledge Self-Assessment (CKSA) platform to generate high-quality cloned 
diagnostic questions, implement effective spaced repetition strategies, and 
provide personalized feedback. By leveraging LLMs for efficient question 
generation and individualized learning, the initiative aims to transform 
continuous certification and lifelong learning, ultimately enhancing diagnostic 
accuracy and patient care.

© 2025 Copyright by the American Board of Family Medicine.

DOI: 10.3122/jabfm.2024.240385R1
PMCID: PMC12616789
PMID: 40854705 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of interest: None.


195. Psychol Bull. 2025 Oct;151(10):1280-1306. doi: 10.1037/bul0000501.

Data extraction by generative artificial intelligence: Assessing determinants of 
accuracy using human-extracted data from systematic review databases.

Jansen T(1), Liebenow LW(1), Mertens U(1), Schmidt FTC(2), Lohmann JF(1), 
Fleckenstein J(2), Meyer J(1).

Author information:
(1)Leibniz Institute for Science and Mathematics Education, Kiel University.
(2)Department of Applied Educational Science, University of Hildesheim.

Psychological science requires reliable measures. Within systematic literature 
reviews, reliability hinges on high interrater agreement during data extraction. 
Yet, the extraction process has been time-consuming. Efforts to accelerate the 
process using technology have shown limited success until generative artificial 
intelligence (genAI), particularly large language models (LLMs), accurately 
extracted variables from medical studies. Nonetheless, for psychological 
researchers, it remains unclear how to utilize genAI for data extraction, given 
the range of tested variables, the medical context, and the variability in 
accuracy. We systematically assessed extraction accuracy and error patterns 
across domains in psychology by comparing genAI-extracted and human-extracted 
data from 22 systematic review databases published in the Psychological 
Bulletin. Eight LLMs extracted 312,329 data points from 2,179 studies on 186 
variables. LLM extractions achieved unacceptable accuracy on all metrics for 20% 
of variables. For 46% of variables, accuracy was acceptable for some metrics and 
unacceptable for others. LLMs reached acceptable but not high accuracy on all 
metrics in 15%, high but not excellent in 8%, and excellent accuracy in 12% of 
variables. Accuracy varied most between variables, less between systematic 
reviews, and least between LLMs. Moderator analyses using a hierarchical 
logistic regression, hierarchical linear model, and meta-analysis revealed that 
accuracy was higher for variables describing studies' context and moderator 
variables compared to variables for effect size calculation. Also, accuracy was 
higher in systematic reviews with more detailed variable descriptions and 
positively correlated with model sizes. We discuss directions for investigating 
ways to use genAI to accelerate data extractions while ensuring meaningful human 
control. (PsycInfo Database Record (c) 2025 APA, all rights reserved).

DOI: 10.1037/bul0000501
PMID: 41396533 [Indexed for MEDLINE]


196. Int J Med Inform. 2026 Jan;205:106122. doi: 10.1016/j.ijmedinf.2025.106122. Epub 
2025 Sep 26.

Enhancing healthcare worker mental health via artificial intelligence-driven 
work process improvements: a scoping review.

Dave B(1), Martin P(2), David SS(3), Kumar S(4), Chakraborty T(5).

Author information:
(1)University of Queensland-Indian Institute of Technology, Delhi (UQ-IITD) 
Research Academy, St Lucia, Queensland, Australia; Rural Clinical School, 
Medical School, The University of Queensland, Australia; Department of 
Electrical Engineering, Indian Institute of Technology Delhi, New Delhi, India. 
Electronic address: dave@student.uq.edu.au.
(2)School of Health and Medical Sciences, University of Southern Queensland, 
Australia; Rural Clinical School, Medical School, The University of Queensland, 
Australia. Electronic address: priya.martin@unisq.edu.au.
(3)Public Health, Quality and Operations, Evangelical Mission Hospital, Tilda, 
CG, India. Electronic address: sharelsingh@gmail.com.
(4)Allied Health and Human Performance, University of South Australia, 
Australia. Electronic address: Saravana.Kumar@unisa.edu.au.
(5)Department of Electrical Engineering, Indian Institute of Technology Delhi, 
New Delhi, India; Yardi School of Artificial Intelligence, Indian Institute of 
Technology Delhi, New Delhi, India. Electronic address: tanchak@iitd.ac.in.

BACKGROUND: Healthcare workers (HCWs) are exposed to higher rates of mental 
health issues, such as burnout, anxiety, cognitive overload, and stress, 
compared to the general population. These may be exacerbated by administrative 
activities like extensive paperwork and disintegrated work processes. The 
implementation of artificial intelligence (AI) in healthcare holds the potential 
to combat these challenges by streamlining workflow processes, lowering 
administrative load, and increasing efficiency. The role of AI in supporting 
HCWs' mental health is yet to be fully explored. This scoping review mapped the 
current evidence on how AI can enhance HCWs' mental health through workflow 
optimisation.
METHODS: This scoping review was informed by best practice in the conduct and 
reporting of scoping reviews. A comprehensive search of academic and grey 
literature was performed without date restrictions. A two-stage dual screening 
process was employed using Covidence. A customised data extraction tool was 
developed to systematically extract data, which was then summarised 
descriptively.
RESULTS: Twenty articles were included in the review, most of which were 
published between 2020 and 2024. These comprised empirical studies, literature 
reviews, position papers, as well as selected grey literature. The studies 
explored various AI applications such as Natural Language Processing (NLP), 
AI-integrated Electronic Health Records (EHR), Machine Learning (ML), Clinical 
Decision Support Systems (CDSS), and Generative AI-driven tools such as ChatGPT. 
Burnout was the most frequently addressed mental health issue, followed by 
stress and cognitive load. Clinical documentation emerged as the most frequently 
addressed workflow, followed by clinical decision-making and diagnostics. 
Literature indicated that AI was capable of streamlining workflows, reducing 
administrative burden, and improving job satisfaction among HCWs. However, 
challenges such as data integration, algorithmic bias, and increased oversight 
demands were noted as potential barriers to effective implementation.
CONCLUSION: AI holds significant potential to improve HCWs' mental health and 
well-being by addressing workflow inefficiencies and reducing administrative 
burden. While available evidence highlights its benefits in enhancing job 
satisfaction and mitigating burnout, challenges such as data standardisation and 
user trust must be addressed for successful adoption. Future research should 
focus on evaluating the long-term impacts of AI on HCWs' mental well-being and 
developing strategies to mitigate unintended consequences.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106122
PMID: 41037981 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: Bhavyaa Dave reports administrative 
support and writing assistance were provided by The University of Queensland. 
Bhavyaa Dave reports a relationship with The University of Queensland that 
includes: non-financial support. No conflict of interest exists with the 
proceedings of the review of the current work. If there are other authors, they 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


197. Acad Radiol. 2025 Dec;32(12):6981-6988. doi: 10.1016/j.acra.2025.09.008. Epub 
2025 Sep 24.

Performance Comparison of Cutting-Edge Large Language Models on the ACR 
In-Training Examination: An Update for 2025.

Young A(1), Paloka R(2), Islam A(3), Prasanna P(4), Hill V(5), Payne D(6).

Author information:
(1)Northwell Health, Mather Hospital, Port Jefferson, New York (A.Y.). 
Electronic address: austinyoungmd@gmail.com.
(2)Department of Radiology, Stony Brook University Hospital, Stony Brook, New 
York (R.P.).
(3)New York Institute of Technology College of Medicine, Old Westbury, New York 
(A.I.).
(4)Department of Biomedical Informatics, Stony Brook University Hospital, Stony 
Brook, New York (P.P.).
(5)Department of Radiology, Northwestern University Hospital, Chicago, Illinois 
(V.H.).
(6)Department of Radiology, Northwestern University Hospital, Chicago, Illinois 
(D.P.).

PURPOSE: This study represents a continuation of prior work by Payne et al. 
evaluating large language model (LLM) performance on radiology board-style 
assessments, specifically the ACR diagnostic radiology in-training examination 
(DXIT). Building upon earlier findings with GPT-4, we assess the performance of 
newer, cutting-edge models, such as GPT-4o, GPT-o1, GPT-o3, Claude, Gemini, and 
Grok on standardized DXIT questions. In addition to overall performance, we 
compare model accuracy on text-based versus image-based questions to assess 
multi-modal reasoning capabilities. As a secondary aim, we investigate the 
potential impact of data contamination by comparing model performance on 
original versus revised image-based questions.
METHODS: Seven LLMs - GPT-4, GPT-4o, GPT-o1, GPT-o3, Claude 3.5 Sonnet, Gemini 
1.5 Pro, and Grok 2.0-were evaluated using 106 publicly available DXIT 
questions. Each model was prompted using a standardized instruction set to 
simulate a radiology resident answering board-style questions. For each 
question, the model's selected answer, rationale, and confidence score were 
recorded. Unadjusted accuracy (based on correct answer selection) and 
logic-adjusted accuracy (based on clinical reasoning pathways) were calculated. 
Subgroup analysis compared model performance on text-based versus image-based 
questions. Additionally, 63 image-based questions were revised to test novel 
reasoning while preserving the original diagnostic image to assess the impact of 
potential training data contamination.
RESULTS: Across 106 DXIT questions, GPT-o1 demonstrated the highest unadjusted 
accuracy (71.7%), followed closely by GPT-4o (69.8%) and GPT-o3 (68.9%). GPT-4 
(59.4%) and Grok 2.0 exhibited similar scores (59.4% and 52.8%). Claude 3.5 
Sonnet had the lowest unadjusted accuracies (34.9%). Similar trends were 
observed for logic-adjusted accuracy, with GPT-o1 (60.4%), GPT-4o (59.4%), and 
GPT-o3 (59.4%) again outperforming other models, while Grok 2.0 and Claude 3.5 
Sonnet lagged behind (34.0% and 30.2%, respectively). GPT-4o's performance was 
significantly higher on text-based questions compared to image-based ones. 
Unadjusted accuracy for the revised DXIT questions was 49.2%, compared to 56.1% 
on matched original DXIT questions. Logic-adjusted accuracy for the revised DXIT 
questions was 40.0% compared to 44.4% on matched original DXIT questions. No 
significant difference in performance was observed between original and revised 
questions.
CONCLUSION: Modern LLMs, especially those from OpenAI, demonstrate strong and 
improved performance on board-style radiology assessments. Comparable 
performance on revised prompts suggests that data contamination may have played 
a limited role. As LLMs improve, they hold strong potential to support radiology 
resident learning through personalized feedback and board-style question review.

Copyright © 2025 The Association of University Radiologists. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.acra.2025.09.008
PMID: 40998658 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


198. J Oral Pathol Med. 2025 Apr;54(4):241-247. doi: 10.1111/jop.13616. Epub 2025 Mar 
9.

The Need to Improve the Medical Subject Headings (MeSH) and the Excerpta Medica 
Tree (EMTREE) Thesauri to Perform Systematic Review on Oral Potentially 
Malignant Disorders.

Caponio VCA(1)(2), Musella G(1), Pérez-Sayáns M(3)(4)(5), Lo Muzio L(1), Amaral 
Mendes R(6)(7)(8)(9), López-Pintor RM(2).

Author information:
(1)Department of Clinical and Experimental Medicine, University of Foggia, 
Foggia, Italy.
(2)ORALMED Research Group, Department of Dental Clinical Specialties, School of 
Dentistry, Complutense University, Madrid, Spain.
(3)Oral Medicine, Oral Surgery and Implantology Unit (MedOralRes), Faculty of 
Medicine and Dentistry, Santiago de Compostela University, Santiago de 
Compostela, Spain.
(4)ORALRES Group, Health Research Institute of Santiago de Compostela (FIDIS), 
Santiago de Compostela, Spain.
(5)Instituto de los materiales de Santiago de Compostela (iMATUS), Avenida do 
Mestre Mateo, Santiago de Compostela, Spain.
(6)Department of Community Medicine, Information and Health Decision Sciences 
(MEDCIDS), Faculty of Medicine of the University of Porto, Porto, Portugal.
(7)RISE-Health, PerMed Research Group, Faculty of Medicine of the University of 
Porto, Porto, Portugal.
(8)RISE-Laboratorio Associado, LT2-Clinical and Translational Research in 
Oncology, Faculty of Medicine of the University of Porto, Porto, Portugal.
(9)Department of Oral and Maxillofacial Medicine and Diagnostic Sciences, Case 
Western Reserve University, Cleveland, OH, USA.

BACKGROUND: Despite recent advancements in the understanding and classification 
of oral potentially malignant disorders (OPMD), their terminology remains 
inconsistent and heterogeneous throughout the scientific literature, thus 
affecting evidence-based decision-making relevant for clinical management of 
these disorders. Updating this classification represents a necessity to improve 
the indexing and retrieval of OPMD publications, in particular for systematic 
reviews and meta-analysis.
METHODS: Through a critical appraisal of the Medical Subject Headings (MeSH) and 
Excerpta Medica Tree (EMTREE) thesauri, we assessed gaps in the indexing for 
OPMD literature and propose improvements for enhanced categorisation and 
retrieval.
RESULTS: The present study identifies inconsistencies and limitations in the 
classification of these disorders across the major medical databases, which may 
be summarized in the following findings: a) The MeSH database lacks a dedicated 
subject heading for "oral potentially malignant disorders"; b) EMTREE indexing 
is incomplete, with only 5 out of 11 recognised OPMD having corresponding terms; 
c) Incoherent controlled vocabulary mappings hinder systematic literature 
retrieval.
CONCLUSION: To ensure accurate evidence synthesis, the authors recommend 
searching both PubMed and Embase for OPMD studies. Moreover, the use of Embase's 
PubMed query translator and Large Language Models, such as ChatGPT, may lead to 
retrieval biases due to indexing discrepancies, posing challenges for 
early-career researchers and students. We recommend introducing "oral 
potentially malignant disorders" as a standardised subject heading. 
Evidence-based medicine underpins clinical decision support systems, which rely 
on standardised clinical coding for reliable health information. Enhanced 
medical ontologies will facilitate structured clinical coding, ensuring 
interoperability and improving clinical decision support systems.

© 2025 The Author(s). Journal of Oral Pathology & Medicine published by John 
Wiley & Sons Ltd.

DOI: 10.1111/jop.13616
PMCID: PMC11986563
PMID: 40059391 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


199. Aesthet Surg J. 2025 Nov 14:sjaf238. doi: 10.1093/asj/sjaf238. Online ahead of 
print.

A Systematic Review of Applications, Challenges, and Future Trajectories of 
Artificial Intelligence in Cosmetic Surgery.

Shoham G(1), Naveh S(1), Confino I(1), Zoabi T(1), Govrin O(1), Fliss E(1), 
Barnea Y(1).

Author information:
(1)Department of Plastic and Reconstructive Surgery, Tel Aviv Sourasky Medical 
Center, Tel Aviv, Israel.

The global demand for cosmetic procedures is accelerating, with over 1.6 million 
aesthetic surgical procedures performed in the US in 2023. Concurrently, AI is 
transforming surgical practice through advanced analytics, predictive modeling, 
and computer vision. Cosmetic surgery, characterized by subjective outcomes and 
limited standardized metrics, presents a unique opportunity for AI integration 
to enhance precision, objectivity, and patient communication. Following PRISMA 
2020 guidelines, we systematically searched MEDLINE/PubMed, Embase, and the 
Cochrane Library (January 2020-July 2025) for studies applying AI, machine 
learning, deep learning, computer vision, or large language models to cosmetic 
or aesthetic procedures. Eligible designs included randomized controlled trials, 
observational studies, diagnostic accuracy studies, feasibility studies, and 
prediction model development. Two reviewers independently screened 
titles/abstracts, assessed full texts, extracted data, and evaluated risk of 
bias using ROBINS-I for non-randomized studies.Of 3,941 records, 38 met the 
inclusion criteria. AI applications spanned preoperative planning (predictive 
risk modeling, 3D outcome simulation), intraoperative guidance (augmented 
reality overlays), and postoperative monitoring (smartphone-based complication 
detection, objective aesthetic scoring). Benefits included improved 
patient-surgeon communication, enhanced risk stratification, and standardized 
outcome measurement. However, most studies were early-phase, with limited 
external validation, heterogeneous datasets, and inconsistent outcome metrics. 
Risk of bias was moderate to serious in most studies. AI in cosmetic surgery 
shows significant potential but remains in early clinical adoption. Progress 
requires multicenter validation, standardized datasets, explainable algorithms, 
and clear regulatory frameworks. Large language model-driven tools may 
accelerate development and integration, provided ethical, equitable, and 
patient-centered principles guide implementation.

© The Author(s) 2025. Published by Oxford University Press on behalf of The 
Aesthetic Society. All rights reserved. For commercial re-use, please contact 
reprints@oup.com for reprints and translation rights for reprints. All other 
permissions can be obtained through our RightsLink service via the Permissions 
link on the article page on our site—for further information please contact 
journals.permissions@oup.com.

DOI: 10.1093/asj/sjaf238
PMID: 41234118


200. BMC Emerg Med. 2025 Sep 1;25(1):176. doi: 10.1186/s12873-025-01337-2.

Performance of ChatGPT, Gemini and DeepSeek for non-critical triage support 
using real-world conversations in emergency department.

Lee S(#)(1), Jung S(#)(2), Park JH(1), Cho H(1), Moon S(1), Ahn S(3)(4).

Author information:
(1)Department of Emergency Medicine, Korea University Ansan Hospital, Ansan-si, 
15355, Republic of Korea.
(2)Core Research & Development Center, Korea University Ansan Hospital, 
Ansan-si, 15355, Republic of Korea.
(3)Department of Emergency Medicine, Korea University Ansan Hospital, Ansan-si, 
15355, Republic of Korea. sejoongahn@naver.com.
(4)Emergency Department, Korea University Ansan Hospital, 123, Jeokgeum-ro, 
Danwon-gu, Ansan-si, Gyeonggi-do, Republic of Korea. sejoongahn@naver.com.
(#)Contributed equally

BACKGROUND: Timely and accurate triage is crucial for the emergency department 
(ED) care. Recently, there has been growing interest in applying large language 
models (LLMs) to support triage decision-making. However, most existing studies 
have evaluated these models using simulated scenarios rather than real-world 
clinical cases. Therefore, we evaluated the performance of multiple commercial 
LLMs for non-critical triage support in ED using real-world clinical 
conversations.
METHODS: We retrospectively analyzed real-world triage conversations 
prospectively collected from three tertiary hospitals in South Korea. Multiple 
commercial LLMs-including OpenAI GPT-4o, GPT-4.1, O3, Google Gemini 2.0 flash, 
Gemini 2.5 flash, Gemini 2.5 pro, DeepSeek V3, and DeepSeek R1-were evaluated 
for the accuracy in triaging patient urgency based solely on unsummarized 
dialogue. The Korean Triage and Acuity Scale (KTAS) assigned by triage nurses 
was used as the gold standard for evaluating the LLM classifications. Model 
performance was assessed under both a zero-shot prompting condition and a 
few-shot prompting condition that included representative examples.
RESULTS: A total of 1,057 triage cases were included in the analysis. Among the 
models, Gemini 2.5 flash achieved the highest accuracy (73.8%), specificity 
(88.9%), and PPV (94.0%). Gemini 2.5 pro demonstrated the highest sensitivity 
(90.9%) and F1-score (82.4%), though with lower specificity (23.3%). GPT-4.1 
also showed balanced high accuracy (70.6%) and sensitivity (81.3%) with 
practical response times (1.79s). Performance varied widely between models and 
even between different versions from the same vendor. With few-shot prompting, 
most models showed further improvements in accuracy and F1-score.
CONCLUSIONS: LLMs can accurately triage ED patient urgency using real-world 
clinical conversations. Several models demonstrated both high sensitivity and 
acceptable response times, supporting the feasibility of LLM in non-critical 
triage support tools in diverse clinical environments. These findings apply to 
non-critical patients (KTAS 3-5), and further research should address 
integration with objective clinical data and real-time workflow.

© 2025. The Author(s).

DOI: 10.1186/s12873-025-01337-2
PMCID: PMC12403343
PMID: 40890624 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was approved by the Institutional Review Board of Korea 
University Ansan Hospital (IRB No. 2025AS0116) and conducted in accordance with 
the principles of the Declaration of Helsinki. The requirement for informed 
consent was waived due to the retrospective analysis of publicly available 
anonymized data. Consent for publication: Not applicable. Competing interests: 
The authors declare no competing interests.


201. JMIR Form Res. 2025 Nov 28;9:e79534. doi: 10.2196/79534.

Comparison of ChatGPT and DeepSeek on a Standardized Audiologist Qualification 
Examination in Chinese: Observational Study.

Qi B(1), Zheng Y(1), Wang Y(1), Xu L(2).

Author information:
(1)Beijing Tongren Hospital, Capital Medical University, Key Laboratory of 
Otolaryngology - Head and Neck Surgery (Capital Medical University), Ministry of 
Education, Beijing, China, Beijing, China.
(2)Department of Hearing, Speech and Language Sciences, Ohio University, Athens, 
OH, United States.

BACKGROUND: Generative artificial intelligence (GenAI), exemplified by ChatGPT 
and DeepSeek, is rapidly advancing and reshaping human-computer interaction with 
its growing reasoning capabilities and broad applications across fields such as 
medicine and education.
OBJECTIVE: This study aimed to evaluate the performance of 2 GenAI models (ie, 
GPT-4-turbo and DeepSeek-R1) on a standardized audiologist qualification 
examination in Chinese and to explore their potential applicability in audiology 
education and clinical training.
METHODS: The 2024 Taiwan Audiologist Qualification Examination, comprising 300 
multiple-choice questions across 6 subject areas (ie, basic hearing science, 
behavioral audiology, electrophysiological audiology, principles and practice of 
hearing devices, health and rehabilitation of the auditory and balance systems, 
and hearing and speech communication disorders [including professional ethics]), 
was used to assess the performance of the 2 GenAI models. The complete answering 
process and reasoning paths of the models were recorded, and performance was 
analyzed by overall accuracy, subject-specific scores, and question-type scores. 
Statistical comparisons were performed at the item level using the McNemar test.
RESULTS: ChatGPT and DeepSeek achieved overall accuracies of 80.3% (241/300) and 
79.3% (238/300), respectively, which are higher than the passing criterion of 
the Taiwan Audiologist Qualification Examination (ie, 60% correct answers). The 
accuracies for the 6 subject areas were 88% (44/50), 70% (35/50), 86% (43/50), 
76% (38/50), 82% (41/50), and 80% (40/50) for ChatGPT and 82% (41/50), 72% 
(36/50), 78% (39/50), 80% (40/50), 80% (40/50), and 84% (41/50) for DeepSeek. No 
significant differences were found between the two models at the item level 
(overall P=.79), with a small effect size (accuracy difference=+1%, Cohen 
h=0.02, odds ratio 0.90, 95% CI 0.53-1.52) and substantial agreement (κ=0.71). 
ChatGPT scored highest in basic hearing science (88%), whereas DeepSeek 
performed the best in hearing and speech communication disorders (84%). Both 
models scored lowest in behavioral audiology (ChatGPT: 70% and DeepSeek: 72%). 
Question-type analysis revealed that both models performed well on reverse logic 
questions (ChatGPT: 79/95, 83%; DeepSeek: 80/95, 84%) but performed moderately 
on complex multiple-choice questions (ChatGPT: 9/17, 53%; DeepSeek: 11/17, 65%). 
However, both models performed poorly on graph-based questions (ChatGPT: 2/11, 
18%; DeepSeek: 4/11, 36%).
CONCLUSIONS: Both GenAI models demonstrated strong professional knowledge and 
stable reasoning ability, meeting the basic requirements of clinical 
audiologists and suggesting their potential as supportive tools in audiology 
education. However, the presence of errors underscores the need for cautious use 
under educator supervision. Future research should explore their performance in 
open-ended, real-world clinical scenarios to assess practical applicability and 
limitations.

©Beier Qi, Yan Zheng, Yuanyuan Wang, Li Xu. Originally published in JMIR 
Formative Research (https://formative.jmir.org), 28.11.2025.

DOI: 10.2196/79534
PMCID: PMC12701348
PMID: 41313805 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


202. Disabil Rehabil Assist Technol. 2025 Sep 19:1-11. doi: 
10.1080/17483107.2025.2561248. Online ahead of print.

AI-Empowered assistive technology for optimizing specimen submission in 
obstetrics and gynecology: integrating DeepSeek with the ADDIE model.

Wan L(1), Gao W(1), Xi P(2), Xu K(1), Li T(1), Wu J(1), Wu D(1).

Author information:
(1)Department of Obstetrics and Gynecology, Jinling Hospital Affiliated to 
Nanjing University Medical School, Nanjing, Jiangsu, China.
(2)Department of Hepatology and Infectious Diseases, Jinling Hospital Affiliated 
to Nanjing University Medical School, Nanjing, Jiangsu, China.

This study leverages AI-empowered assistive technology to enhance augment 
clinician training through AI-assisted education clinical workflows by 
integrating DeepSeek's data-driven intelligence with the ADDIE educational 
model. We aimed to optimize the obstetrics and gynecology specimen submission 
process, reduce errors, and improve operational efficiency through predictive 
analytics, interactive training, and automated feedback. By analyzing 444 
specimen return events (2024), DeepSeek identified error patterns and enabled 
targeted interventions, while the ADDIE framework facilitated structured nurse 
training and process refinement. Post-intervention, the specimen return rate 
significantly decreased from 2.28% to 0.87% (p < 0.001), nurses' knowledge 
scores improved from 82.2 ± 6.1 to 93.5 ± 5.1, and response time shortened by 
35%. This synergy demonstrates how AI-assisted big data analytics can transform 
clinical workflows, offering a scalable model for intelligent healthcare 
systems.

Plain Language Summary: The integration of AI-empowered assistive technology 
into obstetrics and gynecology specimen submission processes has clear 
implications for rehabilitation and broader clinical systems. By leveraging 
predictive analytics and structured educational frameworks like the ADDIE model, 
healthcare teams can significantly reduce operational errors, streamline 
specimen management, and enhance staff competencies. These improvements not only 
increase workflow efficiency but also contribute to safer, more reliable 
diagnostic and treatment pathways—essential for effective rehabilitation 
planning. Furthermore, the demonstrated success of combining AI-driven data 
insights with interactive nurse training presents a scalable, evidence-based 
strategy for optimizing clinical operations and fostering continuous 
professional development across diverse rehabilitative settings.

DOI: 10.1080/17483107.2025.2561248
PMID: 40970872


203. J Med Syst. 2025 Dec 26;49(1):190. doi: 10.1007/s10916-025-02326-5.

Self-Reflective Chest X-Ray Report Generation with Clinical-Aware Detection and 
Multilevel Readability.

Han J(1), Kim M(1), Kim Y(1), Lee WH(2).

Author information:
(1)Department of Software Convergence, Kyung Hee University, Yongin, Republic of 
Korea.
(2)Department of Software Convergence, Kyung Hee University, Yongin, Republic of 
Korea. whlee@khu.ac.kr.

Clinical documentation demands necessitate automated solutions balancing 
clinical precision with patient comprehension. This study aims to develop and 
validate a unified framework that maintains diagnostic accuracy while 
dynamically adapting medical report complexity to diverse literacy levels, and 
to establish comprehensive evaluation methodologies for patient-centered medical 
documentation. We developed a unified framework integrating three innovations: a 
hybrid detection method combining CheXFusion and Eigen-CAM for clinical finding 
detection and anatomical localization; an advanced LLaVA-based pipeline 
synthesizing clinical predictions with anatomical data for contextually rich 
medical reports; and a self-reflective large language model system dynamically 
adapting report complexity across reading levels (6th, 11th, and 18th-grade) 
while preserving clinical integrity. Our methodology introduces novel evaluation 
using the Mistral-small model assessing report quality through consistency, 
coverage, and fluency metrics. Validation on MIMIC-CXR and IU X-Ray datasets 
demonstrated substantial improvements: 19.78% enhancement in classification 
accuracy (AUROC), 17.29% improvement in mean average precision, 56.88% increase 
in patient comprehension scores, and 5.26% gain in diagnostic precision. The 
framework successfully addresses maintaining clinical rigor while enhancing 
patient accessibility, reducing documentation burden on healthcare providers and 
improving patient engagement through comprehensible reporting. This work 
establishes new standards for automated medical documentation that effectively 
reconcile clinical precision with patient comprehension in healthcare 
communication.

© 2025. The Author(s).

DOI: 10.1007/s10916-025-02326-5
PMCID: PMC12743066
PMID: 41452396 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests. Ethical Statement: This study was conducted in 
compliance with ethical standards for research involving human data. The 
datasets used, including MIMIC-CXR and IU X-Ray, are publicly available and were 
accessed following the data use agreements specified by their respective 
providers. All patient data were de-identified to ensure privacy and 
confidentiality, adhering to the principles outlined in the Declaration of 
Helsinki. No experiments involving human participants or animals were conducted 
directly by the authors as part of this research. The methodologies developed 
and applied in this study aim to enhance clinical utility while prioritizing 
patient welfare and data security. The authors declare no conflicts of interest 
and affirm that the manuscript adheres to ethical guidelines for authorship and 
publication.


204. JMIR Med Inform. 2025 Nov 6;13:e73605. doi: 10.2196/73605.

Large Language Model Versus Manual Review for Clinical Data Curation in Breast 
Cancer: Retrospective Comparative Study.

Kang YJ(1), Lee H(2), Yi JP(1), Kim H(1), Yoon CI(3), Baek JM(4), Kim YS(5), 
Jeon YW(6), Rhu J(7), Lim SH(1), Choi H(1), Oh SJ(1).

Author information:
(1)Department of Surgery, College of Medicine, The Catholic University of Korea, 
Incheon St Mary's Hospital, 56, Dongsu-ro, Bupyeong-gu, Incheon, 21431, Republic 
of Korea, 01026383847.
(2)Department of AI Health Information Management, Yonsei University (Mirae), 
Wonju, Republic of Korea.
(3)Department of Surgery, College of Medicine, The Catholic University of Korea, 
Seoul St Mary's Hospital, Seoul, Republic of Korea.
(4)Department of Surgery, College of Medicine, The Catholic University of Korea, 
Yeouido St Mary's Hospital, Seoul, Republic of Korea.
(5)Department of Surgery, College of Medicine, The Catholic University of Korea, 
Uijeongbu St Mary's Hospital, Uijeongbu, Republic of Korea.
(6)Department of Surgery, College of Medicine, The Catholic University of Korea, 
St Vincent's Hospital, Suwon, Republic of Korea.
(7)Department of Surgery, College of Medicine, The Catholic University of Korea, 
Bucheon St Mary's Hospital, Bucheon, Republic of Korea.

BACKGROUND: Manual review of electronic health records for clinical research is 
labor-intensive and prone to reviewer-dependent variations. Large language 
models (LLMs) offer potential for automated clinical data extraction; however, 
their feasibility in surgical oncology remains underexplored.
OBJECTIVE: This study aimed to evaluate the feasibility and accuracy of 
LLM-based processing compared with manual physician review for extracting 
clinical data from breast cancer records.
METHODS: We conducted a retrospective comparative study analyzing breast cancer 
records from 5 academic hospitals (January 2019-December 2019). Two data 
extraction pathways were compared: (1) manual physician review with direct 
electronic health record access (group 1: 1366/3100, 44.06%) and (2) LLM-based 
processing using Claude 3.5 Sonnet (Anthropic) on deidentified data 
automatically extracted through a clinical data warehouse platform (group 2: 
1734/3100, 55.94%). The automated extraction system provided prestructured, 
deidentified data sheets organized by clinical domains, which were then 
processed by the LLM. The LLM prompt was developed through a 3-phase iterative 
process over 2 days. Primary outcomes included missing value rates, extraction 
accuracy, and concordance between groups. Secondary outcomes included comparison 
with the Korean Breast Cancer Society national registry data, processing time, 
and resource use. Validation involved 50 stratified random samples per group 
(900 data points each), assessed by 4 breast surgical oncologists. Statistical 
analysis included chi-square tests, 2-tailed t tests, Cohen κ, and intraclass 
correlation coefficients. The accuracy threshold was set at 90%.
RESULTS: The LLM achieved 90.8% (817) accuracy in validation analysis. Missing 
data patterns differed between groups: group 2 showed better lymph node 
documentation (missing: 152/1734, 8.76% vs 294/1366, 21.52%) but higher missing 
rates for cancer staging (211/1734, 12.17% vs 43/1366, 3.15%). Both groups 
demonstrated similar breast-conserving surgery rates (1107/1734, 63.84% vs 
868/1366, 63.54%). Processing efficiency differed substantially: LLM processing 
required 12 days with 2 physicians versus 7 months with 5 physicians for manual 
review, representing a 91% reduction in physician hours (96 h vs 1025 h). The 
LLM group captured significantly more survival events (41 vs 11; P=.002). Stage 
distribution in the LLM group aligned better with national registry data (Cramér 
V=0.03 vs 0.07). Application programming interface costs totaled US $260 for 
1734 cases (US $0.15 per case).
CONCLUSIONS: LLM-based curation of automatically extracted, deidentified 
clinical data demonstrated comparable effectiveness to manual physician review 
while reducing processing time by 95% and physician hours by 91%. This 2-step 
approach-automated data extraction followed by LLM curation-addresses both 
privacy concerns and efficiency needs. Despite limitations in integrating 
multiple clinical events, this methodology offers a scalable solution for 
clinical data extraction in oncology research. The 90.8% accuracy rate and 
superior capture of survival events suggest that combining automated data 
extraction systems with LLM processing can accelerate retrospective clinical 
research while maintaining data quality and patient privacy.

© Young-Joon Kang, Hocheol Lee, Jae Pak Yi, Hyobin Kim, Chang Ik Yoon, Jong Min 
Baek, Yong-seok Kim, Ye Won Jeon, Jiyoung Rhu, Su Hyun Lim, Hoon Choi, Se Jeong 
Oh. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org).

DOI: 10.2196/73605
PMCID: PMC12599480
PMID: 41197113 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


205. Diagnostics (Basel). 2025 Nov 15;15(22):2899. doi: 10.3390/diagnostics15222899.

External Validation of an Artificial Intelligence Triaging System for Chest 
X-Rays: A Retrospective Independent Clinical Study.

Castilla AC(1), D'Amorim IP(2), Wanderley MFB(1), Esmeraldo MA(3), Yoshida 
AR(1), Eigier AM(1), Valente Yamada Sawamura M(2).

Author information:
(1)NeuralMed A2 Tecnologia Ltda., Rua Pe João Manoel 1212, Cj 41, São Paulo 
01411-010, SP, Brazil.
(2)Department of Radiology and Oncology, Hospital das Clínicas, Faculdade de 
Medicina, Universidade de São Paulo (HCFMUSP), Av. Dr. Enéas de Carvalho Aguiar 
255, São Paulo 05403-000, SP, Brazil.
(3)Department of Radiology, Stanford University School of Medicine, Palo Alto, 
CA 94305, USA.

Background: Chest radiography (CXR) is the most frequently performed 
radiological exam worldwide, but reporting backlogs, caused by a shortage of 
radiologists, remain a critical challenge in emergency care. Artificial 
intelligence (AI) triage systems can help alleviate this challenge by 
differentiating normal from abnormal studies and prioritizing urgent cases for 
review. This study aimed to externally validate TRIA, a commercial AI-powered 
CXR triage algorithm (NeuralMed, São Paulo, Brazil). Methods: TRIA employs a 
two-stage deep learning approach, comprising an image segmentation module that 
isolates the thoracic region, followed by a classification model trained to 
recognize common cardiopulmonary pathologies. We trained the system on 275,399 
CXRs from multiple public and private datasets. We performed external validation 
retrospectively on 1045 CXRs (568 normal and 477 abnormal) from a teaching 
university hospital that was not used for training. We established ground truth 
using a large language model (LLM) to extract findings from original radiologist 
reports. An independent radiologist review of a 300-report subset confirmed the 
reliability of this method, achieving an accuracy of 0.98 (95% CI 0.978-0.988). 
We compared four ensemble decision strategies for abnormality detection. 
Performance metrics included sensitivity, specificity, accuracy, and area under 
the receiver operating characteristic curve (AUROC) with 95% CI. Results: The 
general abnormality classifier achieved strong performance (AUROC 0.911). 
Individual pathology models for cardiomegaly, pneumothorax, and effusion showed 
excellent results (AUROC of 0.968, 0.955, and 0.935, respectively). The weighted 
ensemble demonstrated the best balance, with an accuracy of 0.854 (95% CI, 
0.831-0.874), a sensitivity of 0.845 (0.810-0.875), a specificity of 0.861 
(0.830-0.887), and an AUROC of 0.927 (0.911-0.940). Sensitivity-prioritized 
methods achieving sensitivity >0.92 produced lower specificity (<0.69). False 
negatives were mainly subtle or equivocal cases, although many were still 
flagged as abnormal by the general classifier. Conclusions: TRIA achieved robust 
and balanced accuracy in distinguishing normal from abnormal CXRs. Integrating 
this system into clinical workflows has the potential to reduce reporting 
delays, prioritize urgent cases, and improve patient safety. These findings 
support its clinical utility and warrant prospective multicenter validation.

DOI: 10.3390/diagnostics15222899
PMCID: PMC12651339
PMID: 41300923

Conflict of interest statement: A.C.C. and A.M.E. are founders and stakeholders 
of NeuralMed, the company that manufactures the TRIA system and funded the 
study. A.C.C., A.M.E., M.F.B.W., and A.R.Y. are employees of NeuralMed. The 
other authors declare no conflicts of interest. The funder had a role in the 
design of the study; in the data analysis and interpretation; in the writing of 
the manuscript, and in the decision to publish the results.


206. Int J Surg. 2025 Nov 1;111(11):8616-8620. doi: 10.1097/JS9.0000000000002951. 
Epub 2025 Jul 2.

Comparative analysis of artificial intelligence tools for the dissemination of 
colorectal cancer screening guidelines: a novel perspective on early screening 
education.

Zhang Z(1), Zhang ZC(1), Zhang SP(1), Luan WY(1), Han S(1), Xu ZX(1), Li SS(1), 
Wang SJ(1), Zhao Q(1), Chen YM(1), Yuan XY(1), Zhang SY(1), Tang XL(2), Lin 
SX(1)(3), Miao YD(1)(3)(4)(5).

Author information:
(1)Cancer Center, Yantai Affiliated Hospital of Binzhou Medical University, The 
2nd Medical College of Binzhou Medical University, Yantai, China.
(2)The Second Department of Gastrointestinal Surgery, Affiliated Hospital of 
North Sichuan Medical College, Nanchong, China.
(3)Department of Oncology, Xinhui District People's Hospital, Jiangmen, China.
(4)Guangdong Provincial Key Laboratory of Medical Biomechanics, National Key 
Discipline of Human Anatomy, School of Basic Medical Sciences, Southern Medical 
University, Guangzhou, China.
(5)Research and Translational Center for Immunological Disorders of Binzhou 
Medical University, Yantai, China.

This study systematically evaluated the effectiveness of three artificial 
intelligence (AI) tools - ChatGPT-4o, Claude 3.5, and DeepSeek - in 
disseminating colorectal cancer screening guidelines to nonmedical populations. 
Using uniform instructions aligned with the Chinese Society of Clinical Oncology 
2024 standards, the AI-generated content was analyzed for accuracy, clarity, and 
rigor, supplemented by a cross-evaluation mechanism to quantify performance. Key 
findings revealed that DeepSeek demonstrated superior regional adaptability and 
logical rigor, while requiring improvements in threshold accuracy; ChatGPT-4o 
exhibited outdated starting age criteria and oversimplified high-risk population 
screening protocols; and Claude 3.5 provided a comprehensive framework but 
lacked critical implementation details. All tools effectively translated complex 
medical guidelines into accessible language, underscoring AI's potential in 
public health education. However, outputs necessitated clinical validation and 
ethical oversight to mitigate data biases. The study emphasizes AI's role as an 
auxiliary tool for medical knowledge dissemination, advocating for continuous 
algorithmic optimization, multidisciplinary collaboration, and dynamic 
regulatory mechanisms to ensure alignment with evolving medical standards while 
balancing scientific precision and public accessibility.

Copyright © 2025 The Author(s). Published by Wolters Kluwer Health, Inc.

DOI: 10.1097/JS9.0000000000002951
PMCID: PMC12626496
PMID: 40607944 [Indexed for MEDLINE]

Conflict of interest statement: The authors affirm that they have no financial 
or personal conflicts of interest that could have influenced or appeared to 
influence the work reported in this paper.


207. JMIR Med Inform. 2025 Aug 14;13:e73941. doi: 10.2196/73941.

Application of Large Language Models in Complex Clinical Cases: Cross-Sectional 
Evaluation Study.

Huang Y(#)(1), Yang G(#)(1), Shen Y(#)(2), Chen H(1), Wu W(1), Li X(1), Wu Y(1), 
Zhang K(1), Xu J(1), Zhang J(1).

Author information:
(1)Department of Cardiothoracic Surgery, Third Affiliated Hospital of Sun 
Yat-sen University, 2693 Kaichuang Avenue, Huangpu District, Guangzhou, 510000, 
China, 86 13922192727, 86 82179042.
(2)Department of Gynecologic Oncology, Shaanxi Provincial Cancer Hospital, 
Xi'an, China.
(#)Contributed equally

BACKGROUND: Large language models (LLMs) have made significant advancements in 
natural language processing (NLP) and are gradually showing potential for 
application in the medical field. However, LLMs still face challenges in 
medicine.
OBJECTIVE: This study aims to evaluate the efficiency, accuracy, and cost of 
LLMs in handling complex medical cases and to assess their potential and 
applicability as tools for clinical decision support.
METHODS: We selected cases from the database of the Department of Cardiothoracic 
Surgery, the Third Affiliated Hospital of Sun Yat-sen University (2021-2024), 
and conducted a multidimensional preliminary evaluation of the latest LLMs in 
clinical decision-making for complex cases. The evaluation included measuring 
the time taken for the LLMs to generate decision recommendations, Likert scores, 
and calculating decision costs to assess the execution efficiency, accuracy, and 
cost-effectiveness of the models.
RESULTS: A total of 80 complex cases were included in this study, and the 
performance of multiple LLMs in clinical decision-making was evaluated. Experts 
required 33.60 minutes on average (95% CI 32.57-34.63), far longer than any LLM. 
GPTo1 (0.71, 95% CI 0.67-0.74), GPT4o (0.88, 95% CI 0.83-0.92), and Deepseek 
(0.94, 95% CI 0.90-0.96) all finished under a minute without statistical 
differences. Although Kimi, Gemini, LLaMa3-8B, and LLaMa3-70B took 1.02-3.20 
minutes, they were still faster than experts. In terms of decision accuracy, 
Deepseek-R1 had the highest accuracy (mean Likert score=4.19), with no 
significant difference compared to GPTo1 (P=.699), and both performed 
significantly better than GPT4o, Kimi, Gemini, LLaMa3-70B, and LLaMa3-8B 
(P<.001). Deepseek-R1 and GPTo1 demonstrated the lowest hallucination rates-6/80 
(8%) and 5/80 (6%), respectively-significantly outperforming GPT-4o (7/80, 9%), 
Kimi (10/80, 12%), and the Gemini and LLaMa3 models, which exhibited 
substantially higher rates ranging from 13/80 (16%) to 25/80 (31%). Regarding 
decision costs, all LLMs showed significantly lower costs than the 
Multidisciplinary Team, with open-source models such as Deepseek-R1 offering a 
zero direct cost advantage.
CONCLUSIONS: GPTo1 and Deepseek-R1 show strong clinical potential, boosting 
efficiency, maintaining accuracy, and reducing costs. GPT4o and Kimi performed 
moderately, indicating suitability for broader clinical tasks. Further research 
is needed to validate LLaMa3 series and Gemini in clinical decision.

© Yuanheng Huang, Guozhen Yang, Yahui Shen, Huiguo Chen, Weibin Wu, Xiaojun Li, 
Yonghui Wu, Kai Zhang, Jiannan Xu, Jian Zhang. Originally published in JMIR 
Medical Informatics (https://medinform.jmir.org).

DOI: 10.2196/73941
PMCID: PMC12501899
PMID: 41055081 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


208. Digit Health. 2025 Aug 26;11:20552076251367645. doi: 10.1177/20552076251367645. 
eCollection 2025 Jan-Dec.

Evaluating artificial intelligence chatbots' responses to gynecomastia 
inquiries: Comparative study of information quality, readability, and guideline 
consistency.

Shao X(1), Ruan T(2), Ju X(3), Sun Y(4), Cui J(1).

Author information:
(1)Department of Thyroid and Breast Surgery, People's Hospital of China Medical 
University, Shenyang, Liaoning, China.
(2)Liaoning University of Traditional Chinese Medicine, Shenyang, Liaoning, 
China.
(3)Department of General Medicine, People's Hospital of China Medical 
University, Shenyang, Liaoning, China.
(4)Department of Cardiology, People's Hospital of China Medical University, 
Shenyang, Liaoning, China.

BACKGROUND: With the rapid development of artificial intelligence (AI) 
technologies, AI chatbots have been widely applied in the healthcare to provide 
patients with immediate information. Many people feel embarrassed to discuss 
gynecomastia in person and turn to online resources for support.
OBJECTIVE: This study aims to fill this gap by evaluating the performance of 
five popular AI chatbots (ChatGPT, DeepSeek, Gemini, Perplexity, and Copilot) in 
answering questions about gynecomastia, focusing on their reliability, quality, 
readability, and guideline consistency.
METHODS: In this study, the top 25 gynecomastia-related queries searched 
globally from 2004 to 2025 were retrieved from Google Trends and input into five 
AI chatbots for responses. The reliability and quality of responses were 
assessed using the DISCERN questionnaire and the Ensuring Quality Information 
for Patients (EQIP) tool. Readability was analyzed via the Flesch-Kincaid Grade 
Level (FKGL) and Flesch-Kincaid Reading Ease Score (FKRE). Accuracy, 
supplementary, and incompleteness were compared with the European Association of 
Andrology guidelines.
RESULTS: Copilot had the lowest DISCERN score (median [interquartile range 
(IQR)]: 41.5[36.0-45.0]), while DeepSeek performed best in EQIP scoring (median 
[IQR]: 60.4[59.0-64.1]). For readability, ChatGPT exhibited the highest FKGL 
score (mean ± standard deviation (SD): 15.1 ± 2.0) but the lowest FKRE score 
(mean ± SD: 15.1 ± 2.0), indicating the poorest readability. In contrast, 
DeepSeek achieved the lowest FKGL (mean ± SD: 11.0 ± 1.2), suggesting superior 
readability. Guideline consistency analysis revealed an overall accuracy of 
85.71% for AI responses, but key details were often omitted.
CONCLUSION: AI chatbots provide immediate informational support for gynecomastia 
patients, but there is significant variability in readability and reliability, 
alongside risks of omitting guideline content.

© The Author(s) 2025.

DOI: 10.1177/20552076251367645
PMCID: PMC12381454
PMID: 40881062

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


209. J Surg Educ. 2025 Dec;82(12):103655. doi: 10.1016/j.jsurg.2025.103655. Epub 2025 
Aug 30.

Feasibility of Using AI to Evaluate General Surgery Residency Application 
Personal Statements.

Varman PM(1), Nicholas S(2), Conner A(3), Prabhu AS(4), French JC(4), Lipman 
JM(4).

Author information:
(1)Digestive Diseases Institute, Cleveland Clinic, Cleveland, Ohio. Electronic 
address: varmanp@ccf.org.
(2)Cleveland Clinic Lerner College of Medicine of Case Western Reserve 
University, Cleveland, Ohio.
(3)Digestive Diseases Institute, Cleveland Clinic, Cleveland, Ohio.
(4)Digestive Diseases Institute, Cleveland Clinic, Cleveland, Ohio; Cleveland 
Clinic Lerner College of Medicine of Case Western Reserve University, Cleveland, 
Ohio.

OBJECTIVE: As artificial intelligence (AI) becomes increasingly integrated into 
graduate medical education, residency programs are exploring AI's role in 
application screening. Personal statements (PSs) remain a highly subjective yet 
influential component of the residency application. This study assesses the 
feasibility of using a large language model (LLM) to evaluate general surgery 
residency PSs compared to human-assigned scores.
DESIGN, SETTING, AND PARTICIPANTS: We conducted a retrospective analysis of 668 
deidentified PSs submitted to our general surgery residency program during the 
2023-2024 application cycle. PSs were originally scored by human assessors (HA) 
using an anchored 1-5 scale in two domains: leadership and pathway. Each PS was 
subsequently scored by GPT-3.5 (AI) using the same rubric and standardized 
prompts. Descriptive statistics were used to compare AI and HA scores. 
Inter-rater agreement was assessed using weighted kappa coefficients. Discrepant 
cases (score differences >2 points) were reviewed qualitatively to identify 
scoring themes.
RESULTS: AI and HA scoring showed low agreement: κ = 0.184 for leadership and 
κ = 0.120 for pathway domains. Median AI leadership scores were lower (3 [IQR 
2-4]) than HA scores (4 [IQR 3-5]), while AI pathway scores were higher (4 [IQR 
4-5]) than HA scores (3 [IQR 3-4]). Qualitative review revealed that AI required 
explicit labeling (e.g., formal leadership titles or stated adversity) to assign 
higher scores, whereas HA rewarded inferred qualities such as resilience, 
passion, and longitudinal commitment.
CONCLUSIONS: AI applied rubric-based scoring consistently but interpreted 
narrative content differently than human reviewers. While AI may enhance 
consistency and scalability in early application screening, its limitations in 
recognizing implicit meaning suggest human judgment remains essential for 
evaluating nuanced or inferential content. Caution should be exercised in 
adopting AI tools for subjective application review.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jsurg.2025.103655
PMID: 40885624 [Indexed for MEDLINE]


210. PLoS One. 2025 Jun 4;20(6):e0324175. doi: 10.1371/journal.pone.0324175. 
eCollection 2025.

GPT-4 generates accurate and readable patient education materials aligned with 
current oncological guidelines: A randomized assessment.

Rodler S(1)(2)(3), Cei F(1)(2)(4), Ganjavi C(1)(2), Checcucci E(5), De Backer 
P(6)(7), Rivero Belenchon I(8), Taratkin M(9), Puliatti S(10), Veccia A(11), 
Piazza P(12), Baekelandt L(13), Kowalewski KF(14)(15)(16), Gómez Rivas J(17), 
Fankhauser CD(18), Moschini M(4), Gandaglia G(4), Campi R(19), De Castro Abreu 
A(1)(2), Russo GI(20), Cocci A(21), Maruccia S(22), Cacciamani GE(1)(2); YAU 
Collaborators.

Author information:
(1)USC Institute of Urology and Catherine and Joseph Aresty Department of 
Urology, University of Southern California, Los Angeles, California, United 
States of America.
(2)Artificial Intelligence Center at USC Urology, USC Institute of Urology, 
University of Southern California, Los Angeles, California, United States of 
America.
(3)Department of Urology, University Hospital Schleswig-Holstein, Campus Kiel, 
Kiel, Germany.
(4)Department of Urology, IRCCS San Raffaele Hospital and Vita-Salute San 
Raffaele University, Milan, Italy.
(5)Department of Surgery, Candiolo Cancer Institute, FPO-IRCCS, Turin, Italy.
(6)Department of Urology, Onze-Lieve-Vrouwziekenhuis Hospital, Aalst, Belgium.
(7)ORSI Academy, Ghent, Belgium.
(8)Urology and Nephrology Department, Virgen del Rocío University Hospital, 
Seville, Spain.
(9)Institute for Urology and Reproductive Health, Sechenov University, Moscow, 
Russia.
(10)Department of Urology, University of Modena and Reggio Emilia, Modena, 
Italy.
(11)Urology Unit, Azienda Ospedaliera Universitaria Integrata Verona, Verona, 
Italy.
(12)Division of Urology, IRCCS Azienda Ospedaliero-Universitaria di Bologna, 
Bologna, Italy.
(13)Department of Urology, University Hospitals Leuven, Leuven, Belgium.
(14)Department of Urology, University Medical Center Mannheim, University of 
Heidelberg, Mannheim, Germany.
(15)Mannheim, Institute of Medical Biometry, University of Heidelberg, 
Heidelberg, Germany.
(16)DKFZ Hector Cancer Institute at the University Medical Center Mannheim, 
Mannheim, Germany.
(17)Department of Urology, Hospital Clinico San Carlos, Madrid, Spain.
(18)Department of Urology, University of Zurich, Zurich, Switzerland.
(19)Unit of Urological Robotic Surgery and Renal Transplantation, Careggi 
Hospital, University of Florence, Florence, Italy.
(20)Urology Section, University of Catania, Catania, Italy.
(21)Department of Urology, University of Florence, Careggi Hospital, Florence, 
Italy.
(22)Istituti Clinici Zucchi - Gruppo San Donato, MB, Italy.

INTRODUCTION AND AIM: Guideline-based patient educational materials (PEMs) 
empower patients and reduce misinformation, but require frequent updates and 
must be adapted to the readability level of patients. The aim is to assess 
whether generative artificial intelligence (GenAI) can provide readable, 
accurate, and up-to-date PEMs that can be subsequently translated into multiple 
languages for broad dissemination.
STUDY DESIGN AND METHODS: The European Association of Urology (EAU) guidelines 
for prostate, bladder, kidney, and testicular cancer were used as the knowledge 
base for GPT-4 to generate PEMs. Additionally, the PEMs were translated into 
five commonly spoken languages within the European Union (EU). The study was 
conducted through a single-blinded, online randomized assessment survey. After 
an initial pilot assessment of the GenAI-generated PEMs, thirty-two members of 
the Young Academic Urologists (YAU) groups evaluated the accuracy, completeness, 
and clarity of the original versus GPT-generated PEMs. The translation 
assessment involved two native speakers from different YAU groups for each 
language: Dutch, French, German, Italian, and Spanish. The primary outcomes were 
readability, accuracy, completeness, faithfulness, and clarity. Readability was 
measured using Flesch Kincaid Reading Ease (FKRE), Flesch Kincaid Grade Level 
(FKGL), Gunning Fog (GFS) scores and Smog (SI), Coleman Liau (CLI), Automated 
Readability (ARI) indexes. Accuracy, completeness, faithfulness, and clarity 
were rated on a 5-item Likert scale.
RESULTS: The mean time to create layperson PEMs based on the latest guideline by 
GPT-4 was 52.1 seconds. The readability scores for the 8 original PEMs were 
lower than for the 8 GPT-4-generated PEMs (Mean FKRE: 43.5 vs. 70.8; p < .001). 
The required reading education levels were higher for original PEMs compared to 
GPT-4 generated PEMs (Mean FKGL: 11.6 vs. 6.1; p < .001). For all urological 
localized cancers, the original PEMs were not significantly different from the 
GPT-4 generated PEMs in accuracy, completeness, and clarity. Similarly, no 
differences were observed for metastatic cancers. Translations of GPT-generated 
PEMs were rated as faithful in 77.5% of cases and clear in 67.5% of cases.
CONCLUSIONS AND RELEVANCE: GPT-4 generated PEMs have better readability levels 
compared to original PEMs while maintaining similar accuracy, completeness, and 
clarity. The use of GenAI's information extraction and language capabilities, 
integrated with human oversight, can significantly reduce the workload and 
ensure up-to-date and accurate PEMs.
PATIENT SUMMARY: Some cancer facts made for patients can be hard to read or not 
in the right words for those with prostate, bladder, kidney, or testicular 
cancer. This study used AI to quickly make short and easy-to-read content from 
trusted facts. Doctors checked the AI content and found that they were just as 
accurate, complete, and clear as the original text made for patients. They also 
worked well in many languages. This AI tool can assist providers in making it 
easier for patients to understand their cancer and the best care they can get.

Copyright: © 2025 Rodler et al. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0324175
PMCID: PMC12136319
PMID: 40465696 [Indexed for MEDLINE]

Conflict of interest statement: The listed authors of this manuscript have the 
following competing interests: Giovanni Cacciamani holds equity in EditorAI Pro. 
This does not alter our adherence to PLOS ONE policies on sharing data and 
materials. There are no patents, products in development or marketed products 
associated with this research to declare.


211. BMC Med Educ. 2025 May 12;25(1):693. doi: 10.1186/s12909-025-07272-x.

AI usage among medical students in Palestine: a cross-sectional study and 
demonstration of AI-assisted research workflows.

Yousef M(1), Deeb S(2), Alhashlamon K(2).

Author information:
(1)Faculty of Medicine, Al-Quds University, Jerusalem, Palestine. 
drmahmoudyousef2001@gmail.com.
(2)Faculty of Medicine, Al-Quds University, Jerusalem, Palestine.

BACKGROUND: Artificial Intelligence (AI) is transforming medical education 
globally, offering solutions to challenges such as resource limitations and 
limited clinical exposure. However, its integration in resource-constrained 
settings like Palestine remains underexplored. This study evaluates the 
prevalence, impact, and challenges of AI use among Palestinian medical students, 
focusing on academic performance, clinical competence, and research 
productivity.
METHODS: A cross-sectional study was conducted among 590 medical students from 
Palestinian universities. Data were collected using a validated electronic 
questionnaire, covering demographics, AI usage patterns, and perceived impacts 
across academic, clinical, and research domains. Initial analysis was conducted 
using AI tools, specifically ChatGPT, to facilitate insights and structure the 
results effectively. Statistical analyses were performed using IBM SPSS v27 to 
validate findings. Statistical significance was set at p < 0.05. The draft 
underwent detailed reviews by the research team to confirm accuracy and 
validity.
RESULTS: AI adoption was high, with 87% of students frequently using tools like 
ChatGPT (76%) and virtual simulators (26%). Students reported significant 
improvements in academic performance (mean score: 4.2, SD = 0.7) and research 
productivity (mean score: 4.5, SD = 0.6), particularly in literature reviews and 
data analysis. Clinical competence received moderate ratings (mean score: 3.6, 
SD = 0.8), reflecting AI's limited role in practical skill development. Time 
management was highly rated (mean score: 4.6, SD = 0.5), highlighting AI's 
ability to automate repetitive tasks. Challenges included ethical concerns, data 
accuracy, and limited AI literacy, with 91% lacking formal AI training.
CONCLUSION: AI demonstrates significant potential to enhance medical education 
in resource-constrained settings by improving academic outcomes and research 
efficiency. ChatGPT played a critical role in this study, not only as a tool 
used by participants but also in the research process itself, including data 
analysis and manuscript drafting. These findings were cross-verified using SPSS 
to ensure robustness. Despite its promise, limitations in practical clinical 
applications and technical understanding highlight the need for targeted AI 
literacy programs and ethical guidelines. This study underscores the importance 
of integrating AI into medical curricula to address existing gaps and maximize 
its benefits in similar global contexts.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-07272-x
PMCID: PMC12067962
PMID: 40355851 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was conducted in accordance with the ethical principles 
outlined in the Declaration of Helsinki. Ethical approval was obtained from the 
Institutional Review Board (IRB) at Al-Quds University in Palestine (Ref No: 
449/REC/2024). All participants provided informed consent electronically prior 
to participating in the study. Use of artificial intelligence tools: Artificial 
Intelligence tool, specifically OpenAI’s ChatGPT (version GPT-4, accessed via 
ChatGPT Plus) we used at various stages of this research. The following 
contributions were made: •Questionnaire Development: ChatGPT was used to draft 
survey items aligned with the research objectives. Prompts included, for 
example: “Design a student survey to evaluate the use of AI tools in medical 
education across academic, clinical, and research contexts.” •Thematic Analysis 
Assistance: For qualitative responses, ChatGPT helped group answers into initial 
themes. These were reviewed, corrected, and finalized by the authors to ensure 
accuracy and context. •Narrative Drafting: ChatGPT was used to generate 
narrative summaries of the findings and to draft portions of the introduction, 
results, and discussion. Prompts included: “Summarize Likert-scale findings and 
interpret trends,” and “Rewrite this paragraph in an academic tone.” •Language 
Polishing: ChatGPT helped improve the clarity, grammar, and coherence of the 
manuscript. Human oversight: All content suggested by ChatGPT was independently 
reviewed and edited by the authors. Final versions were based on human judgment 
and scientific validation using traditional tools (e.g., SPSS for statistical 
analysis). Limitations of AI use: ChatGPT occasionally produced inaccurate 
references or unverifiable citations, which were manually filtered. It also 
lacked access to subscription-based academic databases, limiting its ability to 
cite peer-reviewed literature. These limitations underscore the importance of 
human oversight in using AI for scholarly work. Consent for publication: Not 
applicable. Competing interests: The authors declare no competing interests.


212. Artif Intell Med. 2025 Dec;170:103281. doi: 10.1016/j.artmed.2025.103281. Epub 
2025 Oct 8.

Preprocessing narrative texts in electronic medical records to identify hospital 
adverse events: A scoping review.

Jafarpour H(1), Wu G(2), Cheligeer CK(3), Yan J(4), Xu Y(5), Southern DA(6), 
Eastwood CA(7), Zeng Y(8), Quan H(9).

Author information:
(1)Concordia University, Gina Cody School of Engineering and Computer Science, 
Concordia Institute for Information Systems Engineering, 1515 Sainte Catherine 
West, Montreal, H3G 2W1, Quebec, Canada. Electronic address: 
hamed.jafarpour@concordia.ca.
(2)University of Calgary, Department of Community Health Sciences, Cumming 
School of Medicine, 2500 University Drive NW, Calgary, T2N 1N4, Alberta, Canada. 
Electronic address: Guosong.wu@ucalgary.ca.
(3)University of Calgary, Department of Community Health Sciences, Cumming 
School of Medicine, 2500 University Drive NW, Calgary, T2N 1N4, Alberta, Canada. 
Electronic address: cheligeerken@ucalgary.ca.
(4)Concordia University, Gina Cody School of Engineering and Computer Science, 
Concordia Institute for Information Systems Engineering, 1515 Sainte Catherine 
West, Montreal, H3G 2W1, Quebec, Canada. Electronic address: 
jun.yan@concordia.ca.
(5)University of Calgary, Department of Community Health Sciences, Cumming 
School of Medicine, 2500 University Drive NW, Calgary, T2N 1N4, Alberta, Canada. 
Electronic address: yuxu@ucalgary.ca.
(6)University of Calgary, Department of Community Health Sciences, Cumming 
School of Medicine, 2500 University Drive NW, Calgary, T2N 1N4, Alberta, Canada. 
Electronic address: dasouthe@ucalgary.ca.
(7)University of Calgary, Department of Community Health Sciences, Cumming 
School of Medicine, 2500 University Drive NW, Calgary, T2N 1N4, Alberta, Canada. 
Electronic address: caeastwo@ucalgary.ca.
(8)Concordia University, Gina Cody School of Engineering and Computer Science, 
Concordia Institute for Information Systems Engineering, 1515 Sainte Catherine 
West, Montreal, H3G 2W1, Quebec, Canada. Electronic address: 
yong.zeng@concordia.ca.
(9)University of Calgary, Department of Community Health Sciences, Cumming 
School of Medicine, 2500 University Drive NW, Calgary, T2N 1N4, Alberta, Canada. 
Electronic address: hquan@ucalgary.ca.

BACKGROUND: Narrative electronic medical records (EMR), which include textual 
notes created by clinicians within healthcare environments, represent a 
significant resource for documenting various facets of patient care. This form 
of text exhibits distinctive characteristics, such as the occurrence of 
grammatically incorrect sentences, abbreviations, frequent acronyms, specialized 
characters with particular meanings, negation expressions, and sporadic 
misspellings. As a result, a primary goal in processing these textual notes is 
to implement effective preprocessing techniques that enhance data quality and 
ensure consistency across all entries. Recent advancements in algorithms and 
methodologies within the fields of natural language processing (NLP), machine 
learning (ML), and large language models (LLM) have prompted researchers to 
leverage narrative EMR for the detection of hospital adverse events (HAE).
METHODS: The scoping review adhered to the PRISMA-ScR (Preferred Reporting Items 
for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) 
guidelines. A scoping review protocol was developed and utilized to guide the 
research process, clearly outlining the eligibility criteria, information 
sources, search strategies, data management, selection process, data collection 
procedures, data items, outcomes and prioritization, data synthesis, and 
meta-bias considerations. The search strategy was implemented across nine 
engineering and medical electronic databases.
RESULTS: The results have indicated that from a total of 3,264 studies 
retrieved, 48 unique studies were included in the review. Responses to the 
research questions were systematically extracted from these studies. The review 
has identified challenges associated with the preprocessing of narrative texts 
in EMR for HAE identification. Additionally, three research gaps have been 
identified: (1) the imperative need for a pipeline to preprocess narrative EMR 
for the identification of HAE, (2) the necessity for a robust system capable of 
managing the extensive volume of narrative EMR data, and (3) the requirement for 
temporal event system, which are essential for effective HAE detection. The 
study also has underscored the essential role of preprocessing tasks in 
enhancing the performance of HAE detection. The study has emphasized the 
importance of extracting N-grams from clinical text, normalizing these N-grams 
through lemmatization and/or stemming, and establishing semantic feature 
extraction in preprocessing tasks that significantly affect HAE detection 
performance. While LLM-based systems naturally incorporate tokenization and 
normalization processes within their frameworks, it remains crucial to address 
features that hold semantic relevance to the specific type of HAE during 
preprocessing.
CONCLUSION: This scoping review has provided valuable insights for researchers 
focused on HAE detection utilizing narrative EMR data. It has elucidated how 
preprocessing tasks can elevate the performance of HAE detection and draws 
attention to neglected research gaps within the field. Addressing these gaps 
will necessitate further investigation in subsequent research endeavors.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.artmed.2025.103281
PMID: 41072367 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: Hude Quan’s report received funding 
from the Canadian Institutes of Health Research. The authors declare that they 
have no known competing financial interests or personal relationships that could 
have appeared to influence the work reported in this paper. If there are other 
authors, they declare that they have no known competing financial interests or 
personal relationships that could have appeared to influence the work reported 
in this paper.


213. J Med Internet Res. 2025 Sep 5;27:e73486. doi: 10.2196/73486.

Extracting Clinical Guideline Information Using Two Large Language Models: 
Evaluation Study.

Hsu HY(1)(2), Chen LW(3), Hsu WT(1), Hsieh YW(2)(4), Chang SS(3)(5)(6).

Author information:
(1)Graduate Institute of Clinical Pharmacy, College of Medicine, National Taiwan 
University, Taipei, Taiwan.
(2)Department of Pharmacy, China Medical University Hospital, Taichung, Taiwan.
(3)Artificial Intelligence Center, China Medical University Hospital, 2, Yude 
Road, Taichung, 404327, Taiwan, 886 4-22052121.
(4)School of Pharmacy, College of Pharmacy, China Medical University, Taichung, 
Taiwan.
(5)Division of Cardiovascular Medicine, Department of Medicine, China Medical 
University Hospital, Taichung, Taiwan.
(6)School of Medicine, China Medical University, Taichung, Taiwan.

BACKGROUND: The effective implementation of personalized pharmacogenomics (PGx) 
requires the integration of released clinical guidelines into decision support 
systems to facilitate clinical applications. Large language models (LLMs) can be 
valuable tools for automating information extraction and updates.
OBJECTIVE: This study aimed to assess the effectiveness of repeated 
cross-comparisons and an agreement-threshold strategy in 2 advanced LLMs as 
supportive tools for updating information.
METHODS: The study evaluated the performance of 2 LLMs, GPT-4o and 
Gemini-1.5-Pro, in extracting PGx clinical guidelines and comparing their 
outputs with expert-annotated evaluations. The 2 LLMs classified 385 PGx 
clinical guidelines, with each recommendation tested 20 times per model. 
Accuracy was assessed by comparing the results with manually labeled data. Two 
prospectively defined strategies were used to identify inconsistent predictions. 
The first involved repeated cross-comparison, flagging discrepancies between the 
most frequent classifications from each model. The second used a consistency 
threshold strategy, which designated predictions appearing in less than 60% of 
the 40 combined outputs as unstable. Cases flagged by either strategy were 
subjected to manual review. This study also estimated the overall cost of model 
use and was conducted between October 1 and November 30, 2024.
RESULTS: GPT-4o and Gemini-1.5-Pro yielded reproducibility rates of 97.8% 
(7534/7700) and 98.9% (7612/7700), respectively, based on the most frequent 
classification for each query. Compared with expert labels, GPT-4o achieved 
93.5% accuracy (Cohen κ=0.90; P<.001) and Gemini-1.5-Pro 92.7% accuracy (Cohen 
κ=0.89; P<.001). Both models demonstrated high overall performance, with 
comparable weighted average F1-scores (GPT-4o: 0.929; Gemini: 0.935). The models 
generated consistent predictions for 341 of 385 guideline items, reducing the 
need for manual review by 88.6%. Among these agreed-upon cases, only one (0.3%) 
diverged from expert labels. Applying a predefined agreement-threshold strategy 
further reduced the number of priority manual review cases to 2.9% (11/385), 
although the error rate slightly increased to 0.5% (2/374). The inconsistencies 
identified through these methods prompted the prioritization of manual review to 
minimize errors and enhance clinical applicability. The total combined cost of 
using both LLMs was only US $0.76.
CONCLUSIONS: These findings suggest that using 2 LLMs can effectively streamline 
PGx guideline integration into clinical decision support systems while 
maintaining high performance and minimal cost. Although selective manual review 
remains necessary, this approach offers a practical and scalable solution for 
PGx guideline classification in clinical workflows.

© Hsing-Yu Hsu, Lu-Wen Chen, Wan-Tseng Hsu, Yow-Wen Hsieh, Shih-Sheng Chang. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/73486
PMCID: PMC12413144
PMID: 40911841 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


214. Cleft Palate Craniofac J. 2025 Mar 17:10556656251327803. doi: 
10.1177/10556656251327803. Online ahead of print.

Readability of Online Patient Education Materials for Cleft Care: A Systematic 
Review and Meta-Analysis.

Nguyen AT(1), Li RA(2), Gosain AK(3), Galiano RD(2).

Author information:
(1)University of Rochester School of Medicine and Dentistry, Rochester, NY, USA.
(2)Department of Plastic and Reconstructive Surgery, Northwestern University 
Feinberg School of Medicine, Chicago, IL, USA.
(3)Division of Plastic Surgery, Ann & Robert H. Lurie Children's Hospital, 
Chicago, IL, USA.

ObjectiveTo evaluate the readability of online patient education materials 
(PEMs) for cleft lip and/or palate and assess their alignment with recommended 
readability levels.DesignThis study is a systematic review and 
meta-analysis.SettingLiterature search conducted in PubMed, Scopus, and Embase 
databases following PRISMA guidelines.MaterialsStudies evaluating online PEMs 
for cleft care with reported readability metrics, including Flesch-Kincaid Grade 
Level, Flesch Reading Ease, SMOG Index, or Gunning Fog 
Index.InterventionsAssessment of readability metrics of online PEMs and 
evaluation of artificial intelligence tools (eg, ChatGPT) for text 
simplification.Main Outcome Measure(s)Pooled readability estimates (eg, 
Flesch-Kincaid Grade Level, Flesch Reading Ease, SMOG Index, Gunning Fog Index), 
heterogeneity (I²), and confidence intervals (CIs).ResultsNine studies were 
included, consistently showing that PEMs exceed readability recommendations. 
Pooled estimates revealed a Flesch-Kincaid Grade Level of 9.48 (95% CI: 
8.51-10.45), Flesch Reading Ease score of 52.98 (95% CI: 42.62-63.34), SMOG 
Index of 9.27 (95% CI: 5.97-12.57), and Gunning Fog Index of 9.94 (95% CI: 
8.90-10.98). Heterogeneity was minimal (I² = 0%). Artificial intelligence tools 
like ChatGPT demonstrated potential in simplifying text to the recommended 
sixth-grade reading level but lacked usability and comprehension 
testing.ConclusionsOnline PEMs for cleft care are consistently written at 
reading levels too complex for the average caregiver, underscoring the need for 
improved readability and accessibility. Future research should focus on 
developing multimodal resources, conducting usability assessments, and including 
non-English materials to address global disparities in cleft care education.

DOI: 10.1177/10556656251327803
PMID: 40095963


215. Am J Otolaryngol. 2025 Sep-Oct;46(5):104710. doi: 10.1016/j.amjoto.2025.104710. 
Epub 2025 Jul 29.

A comparison of quality and readability of Artificial Intelligence chatbots in 
triage for head and neck cancer.

Kring T(1), Prasad S(2), Dadi S(2), Sokhn E(2), Franzmann E(3).

Author information:
(1)University of Miami Miller School of Medicine, Miami, FL, USA. Electronic 
address: tek37@miami.edu.
(2)University of Miami Miller School of Medicine, Miami, FL, USA.
(3)Department of Otolaryngology-Head and Neck Surgery, University of Miami 
Health System and Jackson Memorial Hospital, Miami, FL, USA.

OBJECTIVE: Head and neck cancers (HNCs) are a significant global health concern, 
contributing to substantial morbidity and mortality. AI-powered chatbots such as 
ChatGPT, Google Gemini, Microsoft Copilot, and Open Evidence are increasingly 
used by patients seeking health information. While these tools provide immediate 
access to medical content, concerns remain regarding their reliability, 
readability, and potential impact on patient outcomes.
METHODS: Responses to 25 patient-like HNC symptom queries were assessed using 
four leading AI platforms: ChatGPT, Google Gemini, Microsoft Copilot, and Open 
Evidence. Responses were evaluated using modified DISCERN criteria for quality 
and SMOG scoring for readability, with ANOVA and post hoc analysis conducted 
afterward.
RESULTS: Microsoft Copilot achieved the highest mean DISCERN score of 41.40 
(95 % CI: 40.31 to 42.49) and the lowest mean SMOG reading levels of 12.56 (95 % 
CI: 11.82 to 13.31), outperforming ChatGPT, Google Gemini, and Open Evidence in 
overall quality and accessibility (p < .001). Open Evidence scored lowest in 
both quality averaging 30.52 (95 % CI: 27.52 to 33.52) and readability of 17.49 
(95 % CI: 16.66 to 18.31), reflecting a graduate reading level.
CONCLUSION: Significant variability exists in the readability and quality of 
AI-generated responses to HNC-related queries, highlighting the need for 
platform-specific validation and oversight to ensure accurate, patient-centered 
communication.
LEVEL OF EVIDENCE: Our study is a cross-sectional analysis that evaluates 
chatbot responses using established grading tools. This aligns best with level 4 
evidence.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.amjoto.2025.104710
PMID: 40768865 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare no conflicts of interest.


216. J Craniofac Surg. 2025 Jun 1;36(4):1237-1241. doi: 10.1097/SCS.0000000000010931. 
Epub 2024 Nov 28.

Evaluating the Utility of ChatGPT in Diagnosing and Managing Maxillofacial 
Trauma.

Rothchild E(1), Baker C(1), Smith IT(2), Tanna N(3), Ricci JA(3).

Author information:
(1)Division of Plastic Surgery, Montefiore Medical Center/Albert Einstein 
College of Medicine, New York, NY.
(2)Vanderbilt University, Nashville, TN.
(3)Division of Plastic Surgery, Donald and Barbara Zucker School of Medicine at 
Hofstra/Northwell, Great Neck, NY.

Maxillofacial trauma is a significant concern in emergency departments (EDs) due 
to its high prevalence and the complexity of its management. However, many ED 
physicians lack specialized training and confidence in handling these cases, 
leading to a high rate of facial trauma referrals and increased stress on 
consult services. Recent advancements in artificial intelligence, particularly 
in large language models such as ChatGPT, have shown potential in aiding 
clinical decision-making. This study specifically examines the efficacy of 
ChatGPT in diagnosing and managing maxillofacial trauma. Ten clinical vignettes 
describing common facial trauma scenarios were presented to a group of plastic 
surgery residents from a tertiary care center and to ChatGPT. The chatbot and 
residents were asked to provide their diagnosis, ED management, and definitive 
management for each scenario. Responses were scored by attending plastic 
surgeons who were blinded to the response source. The study included 13 resident 
and ChatGPT responses. The mean total scores were similar between residents and 
ChatGPT (23.23 versus 22.77, P > 0.05). ChatGPT outperformed residents in 
diagnostic accuracy (9.85 versus 8.54, P < 0.001) but underperformed in 
definitive management (8.35 versus 6.35, P < 0.001). There was no significant 
difference in ED management scores between ChatGPT and the residents. ChatGPT 
demonstrated high accuracy in diagnosing maxillofacial trauma. However, its 
ability to suggest appropriate ED management and definitive treatment plans was 
limited. These findings suggest that while ChatGPT may serve as a valuable 
diagnostic tool in ED settings, further advancements are necessary before it can 
reliably contribute to treatment planning in emergent maxillofacial clinical 
scenarios.

Copyright © 2024 by Mutaz B. Habal, MD.

DOI: 10.1097/SCS.0000000000010931
PMID: 39607329 [Indexed for MEDLINE]

Conflict of interest statement: The authors report no conflicts of interest.


217. Head Face Med. 2025 Jun 4;21(1):44. doi: 10.1186/s13005-025-00521-w.

Machine learning in dentistry and oral surgery: charting the course with 
bibliometric insights.

Liu S(1), Hao Y(1), Zhu S(1), Wan L(1), Yi Z(2), Zhang Z(3).

Author information:
(1)Liaoning Provincial Key Laboratory of Oral Diseases, School and Hospital of 
Stomatology, China Medical University, Shenyang, China.
(2)Liaoning Provincial Key Laboratory of Oral Diseases, School and Hospital of 
Stomatology, China Medical University, Shenyang, China. zheyi@cmu.edu.cn.
(3)Department of Computer, School of Intelligent Medicine, China Medical 
University, Shenyang, China. zczhang@cmu.edu.cn.

BACKGROUND: We aimed to comprehensively analyze the application of machine 
learning (ML) in dentistry and oral surgery using bibliometric methods to 
identify research trends, hotspots, and future directions.
METHODS: Publications related to ML in dentistry and oral surgery published 
between 2010 and 2024 were retrieved from the Science Citation Index Expanded by 
the Web of Science Core Collection (WoSCC). A total of 2234 unique publications 
were identified after screening. Bibliometric analysis was performed using the 
VOSviewer and CiteSpace software, focusing on parameters such as the number of 
publications, countries, institutions, journals, co-cited references, and 
keyword bursts.
RESULTS: The number of publications increased significantly from 2018 to 2024. 
China and the United States were the leading countries in terms of number of 
publications and citation counts. Prominent institutions include Seoul National 
University, Sichuan University, and Charite Universitätsmedizin Berlin. Journals 
such as BMC Oral Health and the Journal of Dentistry have a large number of 
publications. Analysis of the co-cited references revealed clusters related to 
disease diagnosis and risk prediction, treatment planning, clinical decision 
support systems, and dental education. Keyword bursts indicate the evolution of 
research focus from traditional machine learning algorithms to deep learning 
algorithms and the emerging importance of multimodal data and foundation models.
CONCLUSION: ML has made remarkable progress in dentistry and oral surgery. 
Although clinicians can benefit from the application of ML models in their 
practice, they should conduct comprehensive clinical validations to ensure the 
accuracy and reliability of these models. Moreover, challenges, such as data 
availability and security, algorithmic biases, and "black-box models", must be 
addressed. Future research should focus on integrating multimodal data and 
leveraging foundation models to improve the accuracy of diagnosis, treatment 
planning, and educational tools in dentistry and oral surgery.

© 2025. The Author(s).

DOI: 10.1186/s13005-025-00521-w
PMCID: PMC12135253
PMID: 40468381 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not Applicable. Competing interests: The authors declare no 
competing interests.


218. Ann Neurol. 2025 Aug;98(2):308-316. doi: 10.1002/ana.27251. Epub 2025 Apr 25.

Detecting New Lesions Using a Large Language Model: Applications in Real-World 
Multiple Sclerosis Datasets.

Poole S(1), Sisodia N(1), Koshal K(1), Henderson K(1), Wijangco J(1), Paredes 
D(1), Chen C(1), Rowles W(1), Akula A(1), Wuerfel J(2), Sharma V(2)(3); UCSF 
Multiple Sclerosis and Neuroinflammation Center clinicians(1); Rauschecker 
AM(4), Henry RG(1), Bove R(1).

Author information:
(1)UCSF Weill Institute for Neurosciences, University of California San 
Francisco, San Francisco, CA, USA.
(2)F. Hoffmann-La Roche, Basel, Switzerland.
(3)Roche Diagnostics, Santa Clara, USA.
(4)UCSF Center for Intelligent Imaging (ci2), Department of Radiology & 
Biomedical imaging, University of California, San Francisco, CA, USA.

OBJECTIVE: Neuroimaging is routinely utilized to identify new inflammatory 
activity in multiple sclerosis (MS). A large language model to classify 
narrative magnetic resonance imaging reports in the electronic health record 
(EHR) as discrete data could provide significant benefits for MS research. The 
objectives of the current study were to develop such a prompt and to illustrate 
its research applications through a common clinical scenario: monitoring 
response to B-cell depleting therapy (BCDT).
METHODS: An institutional ecosystem that securely connects healthcare data with 
ChatGPT4 was applied to clinical MS magnetic resonance imaging reports in a 
single institutional EHR (2000-2022). A prompt (msLesionprompt) was developed 
and iteratively refined to classify the presence or absence of new T2-weighted 
lesions (newT2w) and contrast-enhancing lesions (CEL). The multistep validation 
included evaluating efficiency (time and cost), comparison with manually 
annotated reports using standard confusion matrix, and application to 
identifying predictors of newT2w/CEL after BCDT start.
RESULTS: Accuracy of msLesionprompt was high for detection of newT2w (97%) and 
CEL (96.8%). All 14,888 available reports were categorized in 4.13 hours ($28); 
79% showed no newT2w or CEL. Data extracted showed expected suppression of new 
activity by BCDT (>97% monitoring magnetic resonance images after an initial 
"rebaseline" scan). Neighborhood poverty (Area Deprivation Index) was identified 
as a predictor of inflammatory activity (newT2w: OR 1.69, 95% CI 1.10-2.59, 
p = 0.017; CEL: OR 1.54, 95% CI 1.01-2.34, p = 0.046).
INTERPRETATION: Extracting discrete information from narrative imaging reports 
using an large language model is feasible and efficient. This approach could 
augment many real-world analyses of MS disease evolution and treatment response. 
ANN NEUROL 2025;98:308-316.

© 2025 The Author(s). Annals of Neurology published by Wiley Periodicals LLC on 
behalf of American Neurological Association.

DOI: 10.1002/ana.27251
PMCID: PMC12278027
PMID: 40277428 [Indexed for MEDLINE]

Conflict of interest statement: S.P., K.K., N.S., K.H., J.W., D.P., C.C., W.R., 
A.A., and A.R.: nothing to report. J.W. and V.S. are employees of F. Hoffmann‐La 
Roche, Basel, Switzerland. R.H. has received personal compensation for serving 
on a Scientific Advisory or Data Safety Monitoring board for Novartis and Roche, 
as well as research funding from Roche/Genentech, which make B‐cell depleting 
therapies. R.B. receives research support from Roche Genentech and Novartis, and 
has received personal consulting fees from TG Therapeutics, which make B‐cell 
depleting therapies.


219. World J Urol. 2025 Jul 7;43(1):416. doi: 10.1007/s00345-025-05757-4.

Comparative analysis of the performance of the large language models 
DeepSeek-V3, DeepSeek-R1, open AI-O3 mini and open AI-O3 mini high in urology.

Yan Z(#)(1)(2), Fan KQ(#)(1), Zhang Q(#)(3), Wu X(#)(4), Chen Y(#)(5), Wu 
X(#)(1)(2), Yu T(1), Su N(1), Zou Y(1), Chi H(6)(7), Xia L(8)(9), Cao 
Q(10)(11)(12).

Author information:
(1)Department of Pharmacy, Panzhihua Central Hospital, Panzhihua, 617067, 
Sichuan, China.
(2)School of Pharmaceutical Sciences, Yunnan Key Laboratory of Pharmacology for 
Natural Products, Kunming Medical University, Kunming, 650500, Yunnan, China.
(3)College of Chinese Medicine, Hong Kong Baptist University, HKG, Kowloon Tong, 
Hong Kong.
(4)College of Veterinary Medicine, Sichuan Agricultural University, Chengdu, 
610000, China.
(5)School of Public Health and Preventive Medicine, Faculty of Medicine, Nursing 
& Health Sciences, Monash University, Level 1, 553 St Kilda Road, Melbourne, 
VIC, 3004, Australia.
(6)Department of Pharmacy, Panzhihua Central Hospital, Panzhihua, 617067, 
Sichuan, China. chihao7511@gmail.com.
(7)School of Clinical Medicine, Southwest Medical University, Luzhou, 402103, 
China. chihao7511@gmail.com.
(8)Department of Pharmacy, Panzhihua Central Hospital, Panzhihua, 617067, 
Sichuan, China. 1332949857@qq.com.
(9)College of Chinese Medicine, Hong Kong Baptist University, HKG, Kowloon Tong, 
Hong Kong. 1332949857@qq.com.
(10)Department of Pharmacy, Panzhihua Central Hospital, Panzhihua, 617067, 
Sichuan, China. 20060008@kust.edu.cn.
(11)School of Pharmaceutical Sciences, Yunnan Key Laboratory of Pharmacology for 
Natural Products, Kunming Medical University, Kunming, 650500, Yunnan, China. 
20060008@kust.edu.cn.
(12)Department of Earth Sciences, Kunming University of Science and Technology, 
Kunming, 650093, China. 20060008@kust.edu.cn.
(#)Contributed equally

Comment in
    World J Urol. 2025 Jul 17;43(1):446. doi: 10.1007/s00345-025-05832-w.

OBJECTIVES: We sought to compare how DeepSeek‑V3, DeepSeek‑R1, OpenAI o3‑mini, 
and OpenAI o3‑mini high handle urological questions, especially in areas such as 
benign prostatic enlargement, urinary stones, infections, and guideline updates. 
The intent was to identify how these text‑creation platforms might aid clinical 
practice without overlooking potential gaps in accuracy.
METHODS: A set of 34 routinely asked questions plus 25 queries based on newly 
revised guidelines was assembled. Six board‑certified urologists independently 
scored each system's replies using a five‑point scale. Questions scoring below a 
set threshold were reintroduced to the same system, accompanied by critiques, to 
gauge self‑correction. Statistical analyses focused on total scores, percentage 
of excellent ratings, and improvements after iterative prompting.
RESULTS: Across all 59 queries (34 general plus 25 guideline-based), OpenAI 
o3-mini high recorded the highest median total score (22 [20-24]), significantly 
outperforming DeepSeek-R1, DeepSeek-V3 and OpenAI o3-mini (all pair-wise 
p < 0.01). DeepSeek-R1's accuracy approached that of o3-mini high in 
patient-counseling items, where their excellent-answer rates were 49% and 57%, 
respectively. DeepSeek‑V3 achieved solid baseline correctness but made fewer 
successful corrections on subsequent attempts. Although OpenAI o3‑mini initially 
produced more concise responses, it showed a surprisingly strong capacity to 
revise earlier errors.
CONCLUSION: OpenAI o3‑mini high, followed by DeepSeek‑R1, provided the most 
reliable answers for modern urological concerns, whereas DeepSeek‑V3 exhibited 
limited adaptability during re‑evaluation. Despite often briefer replies, OpenAI 
o3‑mini outdid DeepSeek‑V3 in self‑correction. These findings indicate that, 
when reviewed by a clinician, o3-mini high can serve as a rapid second-opinion 
tool for outpatient counselling and protocol updates, whereas DeepSeek-R1 may 
provide a cost-effective alternative in resource-limited settings.

© 2025. The Author(s).

DOI: 10.1007/s00345-025-05757-4
PMCID: PMC12234633
PMID: 40622427 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare no competing interests. Ethics approval and consent to participate: This 
study was reviewed by the Kunming University of Science and Technology Ethics 
Committee, which determined that this study could be conducted without approval.


220. BMC Med Inform Decis Mak. 2025 Sep 29;25(1):350. doi: 
10.1186/s12911-025-03202-5.

Can large language models follow guidelines? A comparative study of ChatGPT-4o 
and DeepSeek AI in clavicle fracture management based on AAOS recommendations.

Keçeci T(1), Karagöz B(2).

Author information:
(1)Ordu University Training and Research Hospital, Ordu, Türkiye. 
tolgakececi@hotmail.com.
(2)Eskişehir City Hospital, Eskişehir, Türkiye.

BACKGROUND: Artificial intelligence (AI)-based large language models (LLM) are 
increasingly used in healthcare education. However, the accuracy, readability, 
and reliability of their medical outputs remain a concern. This study aimed to 
compare the quality of responses generated by ChatGPT-4o and DeepSeek AI 
regarding the diagnosis and treatment of clavicle fractures, based on the 2022 
AAOS Clinical Practice Guidelines (CPG).
METHODS: Fourteen clinical questions were formulated based on the AAOS CPG for 
clavicle fractures. Each question was independently submitted to ChatGPT-4o and 
DeepSeek AI. Responses were evaluated using standardized scoring tools, 
including DISCERN, PEMAT-P, CLEAR, Flesch-Kincaid Grade Level, Flesch Reading 
Ease, and Gunning-Fog Index. Two orthopedic surgeons independently rated the 
responses, and inter-rater scores were averaged. Statistical comparison between 
the two AI models was conducted using the Mann–Whitney U test.
RESULTS: DeepSeek AI generated responses with a significantly higher word count 
(median: 572, IQR: 258.25 vs. 438.5, IQR: 229; p = 0.016), CLEAR score (median: 
18, IQR: 0.75 vs. 16, IQR: 0.75; p < 0.001). No statistically significant 
differences were found in PEMAT understandability (median: 77.7 vs. 77.7; 
p = 0.519), PEMAT actionability (median: 0 vs. 0; p = 1.000), or PEMAT total 
score (median: 57.2 vs. 58; p > 0.05). Similarly, no statistically significant 
differences were observed in DISCERN (52.1 vs. 51.6; p > 0.05), readability 
indices, binary accuracy (ChatGPT: 0.93, DeepSeek: 0.89; p > 0.05), or weighted 
accuracy (ChatGPT: 0.83, DeepSeek: 0.79; p > 0.05). Both models demonstrated 
generally high accuracy levels.
CONCLUSION: Both ChatGPT-4o and DeepSeek AI generated coherent and clinically 
relevant responses to guideline-based questions on clavicle fracture management. 
However, neither model achieved meaningful scores in PEMAT actionability, and 
occasional inaccuracies and hallucinations were observed. While DeepSeek 
produced longer responses, verbosity did not correspond to superior quality. 
These findings suggest that LLMs may serve as supplementary tools for medical 
education and reference, but they cannot replace evidence-based clinical 
judgment, underscoring the need for supervised integration and ongoing 
validation. Finally, the number of prompts analyzed (14) was limited, reflecting 
the scope of a single guideline; this sample size restricts statistical power 
and generalizability, and larger multi-guideline datasets will be needed in 
future studies.
SUPPLEMENTARY INFORMATION: The online version contains supplementary material 
available at 10.1186/s12911-025-03202-5.

DOI: 10.1186/s12911-025-03202-5
PMCID: PMC12482356
PMID: 41023674

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study did not involve human participants, identifiable patient 
data, or biological materials. It was conducted using publicly accessible, 
anonymized, and non-interactive artificial intelligence systems, along with 
expert assessments provided by the study authors. According to national research 
regulations in Türkiye, and the institutional policy of Ordu University Faculty 
of Medicine, studies that do not involve human subjects or identifiable clinical 
data are exempt from ethical committee review. Therefore, IRB approval and 
informed consent were not required. The two orthopedic surgeons who evaluated 
the AI-generated responses are co-authors of this study and contributed within 
the scope of their academic duties; thus, consent to participate was not 
applicable.This study was conducted in accordance with the ethical standards of 
the Declaration of Helsinki (2013 revision). Consent for publication: Not 
applicable. Competing interests: The authors declare no competing interests.


221. BMC Med Inform Decis Mak. 2025 Aug 7;25(1):293. doi: 10.1186/s12911-025-03138-w.

A comparative study of screening performance between abstrackr and GPT models: 
Systematic review and contextual analysis.

Xu S(1), Zhao Z(1), Liu X(1), Meng XL(2).

Author information:
(1)Department of Orthopaedic Surgery, Beijing Anzhen Hospital, Capital Medical 
University, Beijing, 100013, China.
(2)Department of Orthopaedic Surgery, Beijing Anzhen Hospital, Capital Medical 
University, Beijing, 100013, China. spinesurgeonmeng@ccmu.edu.cn.

BACKGROUND: Systematic reviews (SRs) and rapid reviews (RRs) are critical 
methodologies for synthesizing existing research evidence. However, the growing 
volume of literature has made the process of screening studies one of the most 
challenging steps in conducting systematic reviews.
METHODS: This systematic review aimed to compare the performance of Abstrackr 
and GPT models (including GPT-3.5 and GPT-4) in literature screening for 
systematic reviews. We identified relevant studies through comprehensive 
searches in PubMed, Cochrane Library, and Web of Science, focusing on those that 
provided key performance metrics such as recall, precision, specificity, and F1 
score.
RESULTS: GPT models demonstrated superior performance compared to Abstrackr in 
precision (0.51 vs. 0.21), specificity (0.84 vs. 0.71), and F1 score (0.52 vs. 
0.31), reflecting a higher overall efficiency and better balance in screening. 
This makes GPT models particularly effective in reducing false positives during 
fine-screening tasks.
CONCLUSION: Abstrackr and GPT models each offer distinct advantages in 
literature screening. Abstrackr is more suitable for the initial screening 
phases, whereas GPT models excel in fine-screening tasks. To optimize the 
efficiency and accuracy of systematic reviews, future screening tools could 
integrate the strengths of both models, potentially leading to the development 
of hybrid systems tailored to different stages of the screening process.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-03138-w
PMCID: PMC12329882
PMID: 40775694 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval: Not applicable. 
Consent for publication: Not applicable. Competing interests: The authors 
declare that they have no competing interests.


222. JMIR Med Educ. 2025 Mar 18;11:e55709. doi: 10.2196/55709.

Impact of Clinical Decision Support Systems on Medical Students' Case-Solving 
Performance: Comparison Study with a Focus Group.

Montagna M(#)(1), Chiabrando F(#)(1), De Lorenzo R(1), Rovere Querini P(1)(2); 
Medical Students.

Collaborators: Datukishvili M, Leone R, Giugliano M, Forquet M, Martino I, 
Pereira LM, Felisatti P, Paleari F, Misurale CD, Manfredi A, Pacini G, Crippa F, 
Shashkin I, Alberoni C, Mungwadzi M, Coacci S.

Author information:
(1)School of Medicine, Vita-Salute San Raffaele University, Via Olgettina 58, 
Milan, 20132, Italy.
(2)Unit of Medical Specialties and Healthcare Continuity, IRCCS San Raffaele 
Scientific Institute, Milan, Italy.
(#)Contributed equally

BACKGROUND: Health care practitioners use clinical decision support systems 
(CDSS) as an aid in the crucial task of clinical reasoning and decision-making. 
Traditional CDSS are online repositories (ORs) and clinical practice guidelines 
(CPG). Recently, large language models (LLMs) such as ChatGPT have emerged as 
potential alternatives. They have proven to be powerful, innovative tools, yet 
they are not devoid of worrisome risks.
OBJECTIVE: This study aims to explore how medical students perform in an 
evaluated clinical case through the use of different CDSS tools.
METHODS: The authors randomly divided medical students into 3 groups, CPG, n=6 
(38%); OR, n=5 (31%); and ChatGPT, n=5 (31%); and assigned each group a 
different type of CDSS for guidance in answering prespecified questions, 
assessing how students' speed and ability at resolving the same clinical case 
varied accordingly. External reviewers evaluated all answers based on accuracy 
and completeness metrics (score: 1-5). The authors analyzed and categorized 
group scores according to the skill investigated: differential diagnosis, 
diagnostic workup, and clinical decision-making.
RESULTS: Answering time showed a trend for the ChatGPT group to be the fastest. 
The mean scores for completeness were as follows: CPG 4.0, OR 3.7, and ChatGPT 
3.8 (P=.49). The mean scores for accuracy were as follows: CPG 4.0, OR 3.3, and 
ChatGPT 3.7 (P=.02). Aggregating scores according to the 3 students' skill 
domains, trends in differences among the groups emerge more clearly, with the 
CPG group that performed best in nearly all domains and maintained almost 
perfect alignment between its completeness and accuracy.
CONCLUSIONS: This hands-on session provided valuable insights into the potential 
perks and associated pitfalls of LLMs in medical education and practice. It 
suggested the critical need to include teachings in medical degree courses on 
how to properly take advantage of LLMs, as the potential for misuse is evident 
and real.

© Marco Montagna, Filippo Chiabrando, Rebecca De Lorenzo, Patrizia Rovere 
Querini, Medical Students. Originally published in JMIR Medical Education 
(https://mededu.jmir.org).

DOI: 10.2196/55709
PMCID: PMC11936302
PMID: 40101183 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


223. Diabetes Res Clin Pract. 2025 Sep;227:112425. doi: 
10.1016/j.diabres.2025.112425. Epub 2025 Aug 22.

Development and performance of a generative pretrained transformer for diabetes 
care.

Garrido-Bueno M(1), Santa Cruz-Álvarez P(2), Pabón-Carrasco M(3), 
Romero-Castillo R(4).

Author information:
(1)Red Cross Nursing University Center, Adhered to the University of Seville, 
Spain; Department of Nursing. Faculty of Nursing, Physiotherapy and Podiatry, 
University of Seville, Spain. Electronic address: mgarrido7@us.es.
(2)Service of Endocrinology and Nutrition at Virgen del Rocío University 
Hospital, Public Health Service or Andalusia, Spain.
(3)Department of Nursing. Faculty of Nursing, Physiotherapy and Podiatry, 
University of Seville, Spain.
(4)Red Cross Nursing University Center, Adhered to the University of Seville, 
Spain; Department of Nursing. Faculty of Nursing, Physiotherapy and Podiatry, 
University of Seville, Spain.

AIMS: To design and evaluate the performance of a diabetes-related Generative 
Pretrained Transformer (GPT).
METHODS: A prompt-engineered layer over GPT was developed in four stages: (1) 
literature review on GPT tools development; (2) selection and preprocessing of 
65 information sources about diabetes care strategies, patient education, 
diabetes technologies, and cultural care, among others; (3) prototype 
development; and (4) final tool evaluation using 420 diabetes-related questions 
adapted from three validated instruments. Outcomes were accuracy, rationale, 
citations, disclaimers, and emoji exclusion. Statistical analyses included 
descriptive statistics, chi-square tests and bias assessment. Compliance with 
data protection regulations and ethical standards was ensured.
RESULTS: Diabetes Help GPT showed high overall accuracy (91.7 %), with 100 % 
rationale inclusion, 93.3 % citations, 84.8 % disclaimers, and minimal emoji use 
(13.3 %). Accuracy was highest in general diabetes knowledge and nutrition 
questions; slightly lower in insulin-related items (82.3 %). Disclaimer and 
emoji usage varied significantly by question format (p = 0.026 and p < 0.001). 
No accuracy bias was detected.
CONCLUSIONS: Diabetes Help GPT delivers accurate, well-sourced responses, 
supporting healthcare professionals in diabetes care. Unlike existing GPT models 
in medicine, it was developed through a transparent, expert-led process with 
curated content and iterative validation. It should complement, and not replace, 
professionals' criteria.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.diabres.2025.112425
PMID: 40850507 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


224. JAMA Netw Open. 2025 Dec 1;8(12):e2549463. doi: 
10.1001/jamanetworkopen.2025.49463.

Uptake of Generative AI Integrated With Electronic Health Records in US 
Hospitals.

Everson J(1), Nong P(2), Richwine C(1).

Author information:
(1)Office of the National Coordinator for Health Information Technology and 
Assistant Secretary for Technology Policy, US Department of Health and Human 
Services, Washington, DC.
(2)Division of Health Policy & Management, University of Minnesota School of 
Public Health, Minneapolis.

Comment in
    doi: 10.1001/jamanetworkopen.2025.49470.

IMPORTANCE: There is widespread enthusiasm about generative artificial 
intelligence (AI), but no systematic evidence on its implementation across 
health care organizations.
OBJECTIVE: To describe adoption of generative AI integrated with the electronic 
health record (EHR) by nonfederal acute care hospitals, how adoption relates to 
experience using and evaluating predictive AI, and hospital characteristics.
DESIGN, SETTING, AND PARTICIPANTS: This survey study of nonfederal acute care US 
hospitals used the 2024 American Hospital Association (AHA) Information 
Technology (IT) Supplement survey. The survey was completed by individuals most 
knowledgeable about health IT at the participating hospitals.
EXPOSURES: Experience with predictive AI, source of predictive AI, local 
evaluation practices (evaluation for accuracy and bias as well as postdeployment 
evaluation), and EHR developer were collected from the 2024 AHA IT Supplement. 
Hospital characteristics, including critical access hospital status, 
multihospital system membership, and teaching status, were collected from the 
2024 AHA Annual Survey. Hospital operating margins, uncompensated care burden, 
and percentage of discharges from Medicaid were collected from the 2022 Medicare 
Cost Report.
MAIN OUTCOMES AND MEASURES: Whether the hospital was an early adopter of 
generative AI integrated with their EHR (currently used generative AI), fast 
follower (planned to use in the next year), or delayed adopter (planned to use 
in 5 years, no plans, or do not know).
RESULTS: A total of 2174 hospitals (1003 [weighted percentage, 50.4%] small; 
1382 [weighted percentage, 60.8%] urban core-based; 1668 [weighted percentage, 
68.8%] part of a multihospital system) responded to questions about their use of 
AI (51.5% response rate). Overall, 762 hospitals (weighted percentage, 31.5%) 
were early adopters of generative AI in 2024, 540 (weighted percentage, 24.7%) 
were fast followers, and 872 (weighted percentage, 43.7%) were delayed adopters. 
In unadjusted analyses, independent hospitals and critical access hospitals were 
less likely to be either early adopters or fast followers than delayed adopters. 
In adjusted analyses, hospitals that used predictive AI were more likely to be 
early adopters or fast followers than delayed adopters (difference, 26.2 [95% 
CI, 16.8-35.6] percentage points). Users of Epic were more likely to be early 
adopters and fast followers than users of other EHRs (eg, likelihood of being an 
early adopter or fast follower, Epic vs Oracle users: 21.9 [95% CI, 16.3-27.4] 
percentage points). Hospitals that reported conducting all local evaluation 
practices (accuracy, bias, postdeployment) were slower to adopt than hospitals 
that reported only 1 evaluation practice (all local evaluation processes: 12.1 
[95% CI, 4.5-19.6] percentage points less likely to be early adopters than fast 
followers).
CONCLUSIONS AND RELEVANCE: In this survey study of US hospitals, more than half 
of US hospitals reported that they would likely implement generative AI by the 
end of 2025. Results indicate the value of providing support to ensure hospitals 
can adopt beneficial generative AI and the need for developing and disseminating 
best practices for generative AI evaluation across organizations.

DOI: 10.1001/jamanetworkopen.2025.49463
PMCID: PMC12701511
PMID: 41385223 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest Disclosures: Dr Nong 
reported receiving personal fees from the Office of the National Coordinator for 
Health IT during the conduct of the study. No other disclosures were reported.


225. Int J Med Inform. 2025 Dec;204:106048. doi: 10.1016/j.ijmedinf.2025.106048. Epub 
2025 Jul 23.

Leveraging open-source large language models (LLMs) in scoping reviews: a case 
study on disability and AI applications.

Bayani A(1), Epoh Ewane LP(2), Oliveira Dos Anjos DS(1), Mac-Seing M(2), Nikiema 
JN(3).

Author information:
(1)Centre de recherche en santé publique, Université de Montréal et CIUSSS du 
Centre-Sud-de-l'Île-de-Montréal, Montréal, Canada; Laboratoire Transformation 
Numérique en Santé (LabTNS), Québec, Canada.
(2)Centre de recherche en santé publique, Université de Montréal et CIUSSS du 
Centre-Sud-de-l'Île-de-Montréal, Montréal, Canada; Department of Social and 
Preventive Medicine, School of Public Health & Centre de Recherche en Santé 
Publique, Université de Montréal, Montreal, QC, Canada; Laboratoire Solidarités 
and Global Health, Québec, Canada.
(3)Centre de recherche en santé publique, Université de Montréal et CIUSSS du 
Centre-Sud-de-l'Île-de-Montréal, Montréal, Canada; Laboratoire Transformation 
Numérique en Santé (LabTNS), Québec, Canada; Department of Management, 
Evaluation and Health Policy, School of Public Health, Université de Montréal, 
Québec, Canada. Electronic address: jean.nikiema@umontreal.ca.

BACKGROUND: Large language models (LLMs) have the potential to offer solutions 
for automating many of the manual tasks involved in scientific reviews, 
including data extraction, literature screening, summarization, and quality 
assessment.
OBJECTIVES: This study aims to evaluate the performance of LLMs in the task of 
title and abstract screening and full-text data extraction of a scoping review 
study, by identifying their effectiveness, efficiency, and potential integration 
into human-based and manual tasks.
MATERIALS AND METHOD: The following key three steps of a scientific scoping 
review were automated: 1) Title and Abstract Screening, 2) Full-Text Screening, 
and 3) Data Extraction based on nine study dimensions. The four most recent 
lightweight open-source LLMs -Mistral, Vicuna, and Llama 3.2 with 1B and 3B 
parameters- were applied and evaluated through the steps.
RESULTS: Llama 3.2-3B demonstrated the best performance in the title and 
abstract screening, achieving an accuracy of 66 %, excelling in the exclusion of 
papers. For full-text screening, it maintained the highest overall accuracy of 
65 %, effectively identifying excluded papers. In data extraction, the Mistral 
model outperformed others across most dimensions, though Llama 3.2-3B excelled 
in extracting objectives and study implications.
DISCUSSION AND CONCLUSION: The present study underscores both the potential and 
limitations of LLMs in automating scoping reviews. Automating the entire scoping 
review without human intervention is sub-optimal. Using a more controlled 
approach balances the strengths of LLMs with the need for human judgment, 
supporting not only the replication of scientific reviews but also their 
continuous refinement and follow-up over time.

Copyright © 2025 The Author(s). Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106048
PMID: 40729777 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


226. BMC Med Inform Decis Mak. 2025 Dec 24. doi: 10.1186/s12911-025-03324-w. Online 
ahead of print.

Application of artificial intelligence tools and clinical documentation burden: 
a systematic review and meta-analysis.

Zhao J(1)(2), Liu H(1)(2), Chen Y(3)(4)(5)(6), Song F(7)(8).

Author information:
(1)Department of Pediatric Research Institute, National Clinical Research Center 
for Children and Adolescents' Health and Diseases, Ministry of Education Key 
Laboratory of Child Development and Disorders, Chongqing Key Laboratory of Child 
Rare Diseases in Infection and Immunity, Children's Hospital of Chongqing 
Medical University, Chongqing, China.
(2)Chevidence Lab of Child & Adolescent Health, Children's Hospital of Chongqing 
Medical University, Chongqing, China.
(3)Department of Pediatric Research Institute, National Clinical Research Center 
for Children and Adolescents' Health and Diseases, Ministry of Education Key 
Laboratory of Child Development and Disorders, Chongqing Key Laboratory of Child 
Rare Diseases in Infection and Immunity, Children's Hospital of Chongqing 
Medical University, Chongqing, China. 483861@hospital.cqmu.edu.cn.
(4)Chevidence Lab of Child & Adolescent Health, Children's Hospital of Chongqing 
Medical University, Chongqing, China. 483861@hospital.cqmu.edu.cn.
(5)Research Unit of Evidence-Based Evaluation and Guidelines, School of Basic 
Medical Sciences, Chinese Academy of Medical Sciences (2021RU017), Lanzhou 
University, Lanzhou, China. 483861@hospital.cqmu.edu.cn.
(6)WHO Collaborating Center for Guideline Implementation and Knowledge 
Translation, Lanzhou, China. 483861@hospital.cqmu.edu.cn.
(7)Department of Pediatric Research Institute, National Clinical Research Center 
for Children and Adolescents' Health and Diseases, Ministry of Education Key 
Laboratory of Child Development and Disorders, Chongqing Key Laboratory of Child 
Rare Diseases in Infection and Immunity, Children's Hospital of Chongqing 
Medical University, Chongqing, China. fujian.song@uea.ac.uk.
(8)Norwich Medical School, University of East Anglia, Norwich, Norfolk, UK. 
fujian.song@uea.ac.uk.

BACKGROUND: Clinician burnout is a growing global concern, with heavy clinical 
documentation workload identified as a major contributor. Clinical documentation 
tasks, though essential for patient care and communication, are time-consuming 
and cognitively demanding. Recent advances in artificial intelligence (AI), 
particularly natural language processing and large language models, are being 
explored as potential tools to alleviate documentation burden, yet their 
quantitative impact has not been systematically assessed.
METHODS: We performed a systematic review and meta-analysis, registered on 
PROSPERO (CRD420250653291) and guided by PRISMA. Eligible studies included 
frontline health professionals using AI tools for clinical note creation, with 
comparators being usual practice or pre-implementation baseline. Primary 
outcomes were documentation burden, workload, burnout, and time spent on 
documentation. Searches were conducted in PubMed, Web of Science, Scopus, and 
key journals. Effect sizes were synthesized using standardized mean difference 
(SMD) under a random-effects model, with subgroup analyses by study design, AI 
tool type, task type, editing status, and data origin.
RESULTS: Of the 23 studies included, 12 were non-randomised studies with a 
concurrent control and 11 employed a before-and-after comparison design. The 
study participants varied in specialties and were mainly from ambulatory 
settings, including physicians, surgeons, pediatricians, and ICU specialists. 
Heterogeneity in results across included studies was considerable, and the 
methodological quality of the available studies was generally low. Pooling 
results of the 14 studies yielded an overall standardized mean difference (SMD) 
of -0.71 (95% confidence interval [CI]: -0.93 to -0.49), indicating a moderate 
reduction in documentation workload and related burnout. Based on results of 
studies in which clinicians reviewed and edited AI-generated drafts, AI 
applications reduced documentation time, similarly representing a moderate 
effect size (SMD= -0.72, 95% CI -0.99 to -0.45). The quality of notes generated 
by AI tools was at least comparable to those prepared manually by clinicians.
CONCLUSIONS: AI technologies offer promising benefits for reducing clinical 
documentation burden. However, their implementation must be accompanied by 
rigorous quality control and ongoing evaluation in practical settings to 
optimize their effectiveness and safeguard patient care outcomes.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-03324-w
PMID: 41444884

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. This study is a systematic review and meta-analysis 
of previously published literature and did not involve any human participants 
directly. Consent for publication: Not applicable. Competing interests: The 
authors declare no competing interests.


227. J Med Internet Res. 2025 Jun 24;27:e70450. doi: 10.2196/70450.

Large Language Model–Assisted Risk-of-Bias Assessment in Randomized Controlled 
Trials Using the Revised Risk-of-Bias Tool: Evaluation Study.

Huang J(1)(2), Lai H(1)(2), Zhao W(1)(2), Xia D(1)(2), Bai C(3), Sun M(4), Liu 
J(5), Liu J(1)(2), Pan B(6)(7), Tian J(6)(7), Ge L(1)(2)(6).

Author information:
(1)Department of Health Policy and Management, School of Public Health, Lanzhou 
University, Lanzhou, China.
(2)Evidence-Based Social Science Research Center, School of Public Health, 
Lanzhou University, Lanzhou, China.
(3)School of Nursing, Southern Medical University, Guangzhou, China.
(4)School of Nursing, Peking University, Beijing, China.
(5)College of Nursing, Gansu University of Traditional Chinese Medicine, 
Lanzhou, China.
(6)Evidence-Based Medicine Center, School of Basic Medical Sciences, Lanzhou 
University, Lanzhou, China.
(7)Key Laboratory of Evidence Based Medicine of Gansu Province, Lanzhou, China.

Erratum in
    J Med Internet Res. 2025 Jul 14;27:e80519. doi: 10.2196/80519.

BACKGROUND: The revised Risk-of-Bias tool (RoB2) overcomes the limitations of 
its predecessor but introduces new implementation challenges. Studies 
demonstrate low interrater reliability and substantial time requirements for 
RoB2 implementation. Large language models (LLMs) may assist in RoB2 
implementation, although their effectiveness remains uncertain.
OBJECTIVE: This study aims to evaluate the accuracy of LLMs in RoB2 assessments 
to explore their potential as research assistants for bias evaluation.
METHODS: We systematically searched the Cochrane Library (through October 2023) 
for reviews using RoB2, categorized by interest in adhering or assignment. From 
86 eligible reviews of randomized controlled trials (covering 1399 RCTs), we 
randomly selected 46 RCTs (23 per category). In addition, 3 experienced 
reviewers independently assessed all 46 RCTs using RoB2, recording assessment 
time for each trial. Reviewer judgments were reconciled through consensus. 
Furthermore, 6 RCTs (3 from each category) were randomly selected for prompt 
development and optimization. The remaining 40 trials established the internal 
validation standard, while Cochrane Reviews judgments served as external 
validation. Primary outcomes were extracted as reported in corresponding 
Cochrane Reviews. We calculated accuracy rates, Cohen κ, and time differentials.
RESULTS: We identified significant differences between Cochrane and reviewer 
judgments, particularly in domains 1, 4, and 5, likely due to different 
standards in assessing randomization and blinding. Among the 20 articles 
focusing on adhering, 18 Cochrane Reviews and 19 reviewer judgments classified 
them as "High risk," while assignment-focused RCTs showed more heterogeneous 
risk distribution. Compared with Cochrane Reviews, LLMs demonstrated accuracy 
rates of 57.5% and 70% for overall (assignment) and overall (adhering), 
respectively. When compared with reviewer judgments, LLMs' accuracy rates were 
65% and 70% for these domains. The average accuracy rates for the remaining 6 
domains were 65.2% (95% CI 57.6-72.7) against Cochrane Reviews and 74.2% (95% CI 
64.7-83.9) against reviewers. At the signaling question level, LLMs achieved 
83.2% average accuracy (95% CI 77.5-88.9), with accuracy exceeding 70% for most 
questions except 2.4 (assignment), 2.5 (assignment), 3.3, and 3.4. When domain 
judgments were derived from LLM-generated signaling questions using the RoB2 
algorithm rather than direct LLM domain judgments, accuracy improved 
substantially for Domain 2 (adhering; 55-95) and overall (adhering; 70-90). LLMs 
demonstrated high consistency between iterations (average 85.2%, 95% CI 
85.15-88.79) and completed assessments in 1.9 minutes versus 31.5 minutes for 
human reviewers (mean difference 29.6, 95% CI 25.6-33.6 minutes).
CONCLUSIONS: LLMs achieved commendable accuracy when guided by structured 
prompts, particularly through processing methodological details through 
structured reasoning. While not replacing human assessment, LLMs demonstrate 
strong potential for assisting RoB2 evaluations. Larger studies with improved 
prompting could enhance performance.

©Jiajie Huang, Honghao Lai, Weilong Zhao, Danni Xia, Chunyang Bai, Mingyao Sun, 
Jianing Liu, Jiayi Liu, Bei Pan, Jinhui Tian, Long Ge. Originally published in 
the Journal of Medical Internet Research (https://www.jmir.org), 24.06.2025.

DOI: 10.2196/70450
PMCID: PMC12238788
PMID: 40554779 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


228. Arthroscopy. 2025 Aug;41(8):2758-2760. doi: 10.1016/j.arthro.2025.03.042. Epub 
2025 Mar 27.

Editorial Commentary: Artificial Intelligence and Language Learning Models Can 
Be Improved by Curated Input of Medical Training Data but Still Face the 
Limitations of Available Literature and Require Continued Human Oversight.

Selman F(1), Obletz K(1), Vismara V(1), Putko R(1), Perry NPJ(1).

Author information:
(1)University of Zurich (F.S.); San Diego, California, U.S.A. (K.O., R.P.); 
University of Milan (V.V.).

Artificial intelligence and language learning models (LLMs) are rapidly 
evolving. Several popular and easily accessible platforms, like ChatGPT and 
Gemini, are increasingly being explored by clinicians and patients for their 
utility in clinical decision making. Although these tools provide rapid access 
to information, their inconsistent adherence to evidence-based guidelines raises 
concerns. A potential solution is to generate more specialized LLMs for 
orthopaedics. A curated database of validated orthopaedic literature can be used 
as input in order to address concerns about the quality of input data. However, 
a curated LLM may still have limitations of selection bias and limited 
high-quality literature. In addition, patients using these models may possess 
limited health literacy. LLMs represent an advancement and potentially powerful 
clinical tool but still require ongoing evaluation, refinement, and validation. 
Artificial intelligence should continue to be viewed as an evolving resource 
rather than a replacement for clinical judgment.

Copyright © 2025 Arthroscopy Association of North America. Published by Elsevier 
Inc. All rights reserved.

DOI: 10.1016/j.arthro.2025.03.042
PMID: 40157559 [Indexed for MEDLINE]

Conflict of interest statement: Disclosures The authors report the following 
potential conflicts of interest or sources of funding: N.P. reports Associate 
Editor, Arthroscopy. All other authors (F.S., K.O., V.V., R.P.) declare that 
they have no known competing financial interests or personal relationships that 
could have appeared to influence the work reported in this paper. Full ICMJE 
author disclosure forms are available for this article online, as supplementary 
material.


229. Int J Med Inform. 2025 Mar;195:105800. doi: 10.1016/j.ijmedinf.2025.105800. Epub 
2025 Jan 21.

Large language models vs human for classifying clinical documents.

Mustafa A(1), Naseem U(2), Rahimi Azghadi M(3).

Author information:
(1)College of Science and Engineering, James Cook University, Townsville, 4811, 
QLD, Australia. Electronic address: akram.mohdmustafa@my.jcu.edu.au.
(2)School of Computing, Macquarie University, Sydney, 2113, NSW, Australia. 
Electronic address: usman.naseem@mq.edu.au.
(3)College of Science and Engineering, James Cook University, Townsville, 4811, 
QLD, Australia. Electronic address: mostafa.rahimiazghadi@jcu.edu.au.

BACKGROUND: Accurate classification of medical records is crucial for clinical 
documentation, particularly when using the 10th revision of the International 
Classification of Diseases (ICD-10) coding system. The use of machine learning 
algorithms and Systematized Nomenclature of Medicine (SNOMED) mapping has shown 
promise in performing these classifications. However, challenges remain, 
particularly in reducing false negatives, where certain diagnoses are not 
correctly identified by either approach.
OBJECTIVE: This study explores the potential of leveraging advanced large 
language models to improve the accuracy of ICD-10 classifications in challenging 
cases of medical records where machine learning and SNOMED mapping fail.
METHODS: We evaluated the performance of ChatGPT 3.5 and ChatGPT 4 in 
classifying ICD-10 codes from discharge summaries within selected records of the 
Medical Information Mart for Intensive Care (MIMIC) IV dataset. These records 
comprised 802 discharge summaries identified as false negatives by both machine 
learning and SNOMED mapping methods, showing their challenging case. Each 
summary was assessed by ChatGPT 3.5 and 4 using a classification prompt, and the 
results were compared to human coder evaluations. Five human coders, with a 
combined experience of over 30 years, independently classified a stratified 
sample of 100 summaries to validate ChatGPT's performance.
RESULTS: ChatGPT 4 demonstrated significantly improved consistency over ChatGPT 
3.5, with matching results between runs ranging from 86% to 89%, compared to 57% 
to 67% for ChatGPT 3.5. The classification accuracy of ChatGPT 4 was variable 
across different ICD-10 codes. Overall, human coders performed better than 
ChatGPT. However, ChatGPT matched the median performance of human coders, 
achieving an accuracy rate of 22%.
CONCLUSION: This study underscores the potential of integrating advanced 
language models with clinical coding processes to improve documentation 
accuracy. ChatGPT 4 demonstrated improved consistency and comparable performance 
to median human coders, achieving 22% accuracy in challenging cases. Combining 
ChatGPT with methods like SNOMED mapping could further enhance clinical coding 
accuracy, particularly for complex scenarios.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.105800
PMID: 39848078 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


230. Ann Plast Surg. 2025 Apr 1;94(4S Suppl 2):S327-S333. doi: 
10.1097/SAP.0000000000004275.

Optimizing Chart Review Efficiency in Pressure Injury Evaluation Using ChatGPT.

Friedman R(1), Lisk R(1), Cordero-Bermudez K(2), Singh S(2), Ghani S(1), 
Gillette BM(2), Gorenstein SA(2), Chiu ES(1).

Author information:
(1)From the Hansjörg Wyss Department of Plastic Surgery, New York University 
Langone Health, New York, New York.
(2)Department of Surgery NYU Langone Hospital - Long Island, Mineola, New York.

INTRODUCTION: Wound care is an essential discipline in plastic surgery, 
especially as the prevalence of chronic wounds, such as pressure injuries, is 
increasing. The escalating volume of patient data and the numerous variables 
influencing wound outcomes are making traditional manual chart reviews in wound 
care and research increasingly complex and burdensome. The emergence of Natural 
Language Processing (NLP) software based on large language models (LLMs) such as 
ChatGPT presents an opportunity to automate the data extraction process. This 
study harnesses the capabilities of ChatGPT, hosted by our medical center's 
secure, private Azure OpenAI service, to automatically extract and process 
variables from patient charts following sacral wound visits. We assess ChatGPT's 
potential to revolutionize chart review through improved data retrieval accuracy 
and efficiency.
METHODS: We evaluated the use of the medical center's internal ChatGPT in chart 
review. ChatGPT and a Python script were integrated into the existing chart 
review process for patients with sacral wounds from 2 hospital cohorts to 
extract and format variables related to wound care. Metrics include time taken 
for review, accuracy of extracted information, and assessment of 
ChatGPT-generated insights.
RESULTS: ChatGPT reduced the average time per chart review from 7.56 minutes 
with the manual method to 1.03 minutes using ChatGPT. Furthermore, it achieved a 
0.957 overall accuracy rate compared to manual chart review, ranging from 0.747 
to 0.986 across extracted data elements. ChatGPT was also able to synthesize 
accurate narrative descriptions of patient wounds.
CONCLUSIONS: We highlight ChatGPT's potential to enhance speed and precision of 
chart review in the context of both clinical care and wound care research, 
offering valuable implications for integration of artificial intelligence in 
healthcare workflows.

Copyright © 2025 Wolters Kluwer Health, Inc. All rights reserved.

DOI: 10.1097/SAP.0000000000004275
PMID: 40167094 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of interest and sources of funding: 
none declared.


231. BJOG. 2025 Nov 24. doi: 10.1111/1471-0528.70095. Online ahead of print.

Assessing the Accuracy of Large Language Models on European Guidelines for 
Cervical Cancer: An In Silico Benchmarking Study.

Pavone M(1)(2)(3)(4), Innocenzi C(1)(2), Macellari N(1), Cantarini C(1), 
Criscione M(1), Rosati A(1), Lecointre L(3)(4)(5), Carcagnì A(6), Costantini 
B(1)(7), Marescaux J(2), Fagotti A(1)(8), Fanfani F(1)(8), Cibula D(9), Querleu 
D(1), Bizzarri N(1)(8).

Author information:
(1)UOC Ginecologia Oncologica, Dipartimento di Scienze per la Salute Della Donna 
e del Bambino e di Sanità Pubblica, Fondazione Policlinico Universitario A. 
Gemelli, IRCCS, Rome, Italy.
(2)Research Institute Against Digestive Cancer, IRCAD Strasbourg, Strasbourg, 
France.
(3)Institute of Image-Guided Surgery, IHU Strasbourg, Strasbourg, France.
(4)ICube, Laboratory of Engineering, Computer Science and Imaging, Department of 
Robotics, Imaging, Teledetection and Healthcare Technologies, University of 
Strasbourg, CNRS, UMR 7357, Strasbourg, France.
(5)Department of Gynecologic Surgery, University Hospitals of Strasbourg, 
Strasbourg, France.
(6)Facility of Epidemiology and Biostatistics - Gemelli Generator, Fondazione 
Policlinico Universitario A. Gemelli IRCCS, Rome, Italy.
(7)UniCamillus, International Medical University, Rome, Italy.
(8)Università Cattolica del Sacro Cuore, Rome, Italy.
(9)Department of Obstetrics and Gynecology, First Faculty of Medicine, Charles 
University, General University Hospital in Prague, Prague, Czechia.

OBJECTIVE: Although large language models are increasingly used in clinical and 
research settings, the validity of the information they provide remains 
uncertain. This study aimed to evaluate the accuracy, consistency, and 
reliability of three large language models-ChatGPT 4.0, DeepSeek R1, and Gemini 
2.0-in answering cervical cancer-related questions based on the ESGO/ESTRO/ESP 
guidelines.
DESIGN: Prospective, comparative in silico benchmarking study.
SETTING: Fondazione Policlinico Universitario A. Gemelli, Rome, Italy.
POPULATION OR SAMPLE: Fifty questions derived from the ESGO/ESTRO/ESP (European 
Society of Gynaecologic Oncology/European Society for Radiotherapy and 
Oncology/European Society of Pathology) Guidelines for Cervical Cancer.
METHODS: Each question was submitted simultaneously to ChatGPT 4.0, DeepSeek R1, 
and Gemini 2.0, and re-entered twice to assess response repeatability. Answers 
were evaluated for accuracy using a Global Quality Score (GQS) from 1 (poor) to 
5 (completely accurate). Consistency (intra-model response stability) and 
reliability (alignment with guidelines) were assessed using binary 
classification.
MAIN OUTCOME MEASURES: Median GQS, percentage of GQS 5 responses, consistency 
between repeated answers, and reliability.
RESULTS: ChatGPT 4.0 achieved the highest performance, with 42% of responses 
rated GQS 5, followed by Gemini 2.0 (30%) and DeepSeek R1 (28%). DeepSeek R1 and 
Gemini 2.0 scored lower in median GQS (3.50) compared to ChatGPT 4.0 (4.00). 
Response consistency varied significantly, with ChatGPT 4.0 and DeepSeek R1 
showing differences from Gemini 2.0 (p = 0.034 and p = 0.044, respectively). No 
significant difference was observed in reliability (p = 0.602).
CONCLUSION: All models demonstrated suboptimal accuracy in aligning with 
clinical guidelines. ChatGPT 4.0 was the most accurate and consistent whereas 
DeepSeek R1 underperformed. Despite similar reliability across models, expert 
oversight remains essential to ensure safe clinical application and prevent 
misinformation.

© 2025 The Author(s). BJOG: An International Journal of Obstetrics and 
Gynaecology published by John Wiley & Sons Ltd.

DOI: 10.1111/1471-0528.70095
PMID: 41287196


232. Front Digit Health. 2025 Feb 4;7:1512273. doi: 10.3389/fdgth.2025.1512273. 
eCollection 2025.

The externalization of internal experiences in psychotherapy through generative 
artificial intelligence: a theoretical, clinical, and ethical analysis.

Haber Y(1), Hadar Shoval D(2), Levkovich I(3), Yinon D(1), Gigi K(4), Pen O(4), 
Angert T(4), Elyoseph Z(4)(5).

Author information:
(1)The Program of Hermeneutics and Cultural Studies, Interdisciplinary Studies 
Unit, Bar-Ilan University, Jerusalem, Israel.
(2)Department of Psychology, Max Stern Academic College of Emek Yezreel, Yezreel 
Valley, Israel.
(3)Faculty of Education, Tel-Hai Academic College, Kiryat Shmona, Israel.
(4)Department of Counseling and Human Development, Faculty of Education, 
University of Haifa, Haifa, Israel.
(5)Department of Brain Sciences, Faculty of Medicine, Imperial College London, 
London, United Kingdom.

INTRODUCTION: Externalization techniques are well established in psychotherapy 
approaches, including narrative therapy and cognitive behavioral therapy. These 
methods elicit internal experiences such as emotions and make them tangible 
through external representations. Recent advances in generative artificial 
intelligence (GenAI), specifically large language models (LLMs), present new 
possibilities for therapeutic interventions; however, their integration into 
core psychotherapy practices remains largely unexplored. This study aimed to 
examine the clinical, ethical, and theoretical implications of integrating GenAI 
into the therapeutic space through a proof-of-concept (POC) of AI-driven 
externalization techniques, while emphasizing the essential role of the human 
therapist.
METHODS: To this end, we developed two customized GPTs agents: VIVI (visual 
externalization), which uses DALL-E 3 to create images reflecting patients' 
internal experiences (e.g., depression or hope), and DIVI (dialogic 
role-play-based externalization), which simulates conversations with aspects of 
patients' internal content. These tools were implemented and evaluated through a 
clinical case study under professional psychological guidance.
RESULTS: The integration of VIVI and DIVI demonstrated that GenAI can serve as 
an "artificial third", creating a Winnicottian playful space that enhances, 
rather than supplants, the dyadic therapist-patient relationship. The tools 
successfully externalized complex internal dynamics, offering new therapeutic 
avenues, while also revealing challenges such as empathic failures and cultural 
biases.
DISCUSSION: These findings highlight both the promise and the ethical 
complexities of AI-enhanced therapy, including concerns about data security, 
representation accuracy, and the balance of clinical authority. To address these 
challenges, we propose the SAFE-AI protocol, offering clinicians structured 
guidelines for responsible AI integration in therapy. Future research should 
systematically evaluate the generalizability, efficacy, and ethical implications 
of these tools across diverse populations and therapeutic contexts.

© 2025 Haber, Hadar Shoval, Levkovich, Yinon, Gigi, Pen, Angert and Elyoseph.

DOI: 10.3389/fdgth.2025.1512273
PMCID: PMC11832678
PMID: 39968063

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest. The author(s) declared that 
they were an editorial board member of Frontiers, at the time of submission. 
This had no impact on the peer review process and the final decision.


233. Knee Surg Sports Traumatol Arthrosc. 2025 May;33(5):1911-1919. doi: 
10.1002/ksa.12639. Epub 2025 Mar 7.

High identification and positive-negative discrimination but limited detailed 
grading accuracy of ChatGPT-4o in knee osteoarthritis radiographs.

Zhu J(1), Jiang Y(1), Chen D(1), Lu Y(2), Huang Y(1), Lin Y(1), Fan P(1).

Author information:
(1)Department of Orthopedics, The Second Affiliated Hospital of Wenzhou Medical 
University, Yuying Children's Hospital, Wenzhou, China.
(2)Department of Radiology, The Second Affiliated Hospital and Yuying Children's 
Hospital of Wenzhou Medical University, Wenzhou, Zhejiang, China.

PURPOSE: To explore the potential of ChatGPT-4o in analysing radiographic images 
of knee osteoarthritis (OA) and to assess its grading accuracy, feature 
identification and reliability, thereby helping surgeons to improve diagnostic 
accuracy and efficiency.
METHODS: A total of 117 anterior‒posterior knee radiographs from patients (23.1% 
men, 76.9% women, mean age 69.7 ± 7.99 years) were analysed. Two senior 
orthopaedic surgeons and ChatGPT-4o independently graded images with the 
Kellgren-Lawrence (K-L), Ahlbäck and International Knee Documentation Committee 
(IKDC) systems. A consensus reference standard was established by a third 
radiologist. ChatGPT-4o's performance metrics (accuracy, precision, recall and 
F1 score) were calculated, and its reliability was assessed via two evaluations 
separated by a 2-week interval, with intraclass correlation coefficients (ICCs) 
determined.
RESULTS: ChatGPT-4o achieved a 100% identification rate for knee radiographs and 
demonstrated strong binary classification performance (precision: 0.95, recall: 
0.83, F score: 0.88). However, its detailed grading accuracy (35%) was 
substantially lower than that of surgeons (89.6%). Severe underestimation of OA 
severity occurred in 49.3% of the cases. Interrater reliability for surgeons was 
excellent (ICC: 0.78-0.91), whereas ChatGPT-4o showed poor initial consistency 
(ICC: 0.16-0.28), improving marginally in the second evaluation (ICC: 
0.22-0.39).
CONCLUSION: ChatGPT-4o has the potential to rapidly identify and binary classify 
knee OA on radiographs. However, its detailed grading accuracy remains 
suboptimal, with a notable tendency to underestimate severe cases. This limits 
its current clinical utility for precise staging. Future research should focus 
on optimising its grading performance and improving accuracy to enhance 
diagnostic reliability.
LEVEL OF EVIDENCE: Level III, retrospective comparative study.

© 2025 European Society of Sports Traumatology, Knee Surgery and Arthroscopy.

DOI: 10.1002/ksa.12639
PMID: 40053915 [Indexed for MEDLINE]


234. Int J Med Inform. 2025 Sep;201:105961. doi: 10.1016/j.ijmedinf.2025.105961. Epub 
2025 May 5.

The actual performance of large language models in providing liver 
cirrhosis-related information: A comparative study.

Li Y(1), Li Z(2), Li J(3), Liu L(1), Liu Y(1), Zhu B(1), Shi K(1), Lu Y(1), Li 
Y(1), Zeng X(1), Feng Y(4), Wang X(5).

Author information:
(1)Center for Integrative Medicine, Beijing Ditan Hospital, Capital Medical 
University, Beijing, China.
(2)School of Artificial Intelligence, Beijing University of Posts and 
Telecommunications, Beijing, China.
(3)Department of Neurosurgery, Xuanwu Hospital, Capital Medical University, 
Beijing, China.
(4)Center for Integrative Medicine, Beijing Ditan Hospital, Capital Medical 
University, Beijing, China. Electronic address: fengying@ccmu.edu.cn.
(5)Center for Integrative Medicine, Beijing Ditan Hospital, Capital Medical 
University, Beijing, China. Electronic address: wangxb@ccmu.edu.cn.

OBJECTIVE: With the increasing prevalence of large language models (LLMs) in the 
medical field, patients are increasingly turning to advanced online resources 
for information related to liver cirrhosis due to its long-term management 
processes. Therefore, a comprehensive evaluation of real-world performance of 
LLMs in these specialized medical areas is necessary.
METHODS: This study evaluates the performance of four mainstream LLMs 
(ChatGPT-4o, Claude-3.5 Sonnet, Gemini-1.5 Pro, and Llama-3.1) in answering 39 
questions related to liver cirrhosis. The information quality, readability and 
accuracy were assessed using Ensuring Quality Information for Patients tool, 
Flesch-Kincaid metrics and consensus scoring. The simplification and their 
self-correction ability of LLMs were also assessed.
RESULTS: Significant performance differences were observed among the models. 
Gemini scored highest in providing high-quality information. While the 
readability of all four LLMs was generally low, requiring a college-level 
reading comprehension ability, they exhibited strong capabilities in simplifying 
complex information. ChatGPT performed best in terms of accuracy, with a "Good" 
rating of 80%, higher than Claude (72%), Gemini (49%), and Llama (64%). All 
models received high scores for comprehensiveness. Each of the four LLMs 
demonstrated some degree of self-correction ability, improving the accuracy of 
initial answers with simple prompts. ChatGPT's and Llama's accuracy improved by 
100%, Claude's by 50% and Gemini's by 67%.
CONCLUSION: LLMs demonstrate excellent performance in generating health 
information related to liver cirrhosis, yet they exhibit differences in answer 
quality, readability and accuracy. Future research should enhance their value in 
healthcare, ultimately achieving reliable, accessible and patient-centered 
medical information dissemination.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.105961
PMID: 40334344 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


235. BMC Med Educ. 2025 Dec 18;26(1):123. doi: 10.1186/s12909-025-08462-3.

Artificial intelligence (AI) for social innovation in health education: 
promoting health literacy through personalized ai-driven learning tools - a 
systematic review.

Tbaishat DM(1)(2), Elfadel MW(3).

Author information:
(1)College of Technological Innovation, Zayed University, Dubai, United Arab 
Emirates. dina.tbaishat@zu.ac.ae.
(2)School of Educational Sciences, University of Jordan, Amman, Jordan. 
dina.tbaishat@zu.ac.ae.
(3)School of Educational Sciences, University of Jordan, Amman, Jordan.

BACKGROUND: Artificial Intelligence (AI) is transforming health education by 
enabling personalized, adaptive, and scalable approaches that may enhance 
aspects of health literacy. Despite rapid adoption, comprehensive synthesis of 
AI tools' impact on health literacy as social innovation is limited. 
Understanding these effects guides educators, developers, and policymakers in 
designing potentially effective, inclusive, and ethical AI interventions. This 
review examines generative AI models, chatbots, and adaptive learning systems in 
supporting health literacy globally.
METHODS: A systematic review was conducted following PRISMA guidelines. 
Literature was identified primarily through PubMed/Medline, Scopus, and 
ScienceDirect. Connectedpapers.com was used exclusively as a citation chasing 
tool, performing both backward and forward reference searches to identify 
thematically linked studies not captured by database searches. All records 
retrieved via Connected Papers were subjected to the same eligibility criteria 
as database-sourced studies, covering publications from 2000-2025. A total of 75 
peer-reviewed empirical and theoretical studies focusing on AI tools for health 
literacy and social innovation were included. Titles, abstracts, keywords, and 
full texts were screened using predefined criteria. Data were managed and 
de-duplicated using Zotero. Screening and eligibility decisions were recorded in 
Excel spreadsheets. Thematic synthesis was conducted manually. PRISMA 2020 and 
PRISMA-S checklists were used to ensure transparent reporting.
RESULTS: AI research in health education was minimal until 2020 but rose sharply 
from 2021, peaking in 2023-2024 with generative AI (e.g., ChatGPT). Of the 75 
included studies, 68 (90.7%) were co-authored by two or more researchers, 54 
(72.0%) were published as Open Access, and review articles dominated with 41 
studies (54.7%), while empirical research was limited, highlighting moderate to 
weak evidence. Research focused on personalized AI tools and learning 
effectiveness, with limited exploration of ethics, technical barriers, or social 
innovation. Findings suggest that AI interventions may improve readability, 
metacognitive engagement, cultural accessibility, and learner autonomy in the 
short term, particularly when multifaceted. However, evidence for long-term 
behavior change and real-world impact is sparse, indicating caution in 
generalizing results. Challenges include algorithmic bias, digital inequity, and 
lack of transparency, emphasizing the need for inclusive, equity-driven AI 
strategies.
CONCLUSION: AI-powered tools have potential to support health literacy and 
learner-centered innovation, while contributing to social impact. Multifaceted, 
adaptive interventions may offer greater benefits than single-tool approaches. 
Findings provide preliminary guidance for standardized training, AI literacy 
integration, and policy frameworks, while acknowledging the current limitations 
in evidence, generalizability, and long-term outcomes.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-08462-3
PMCID: PMC12825272
PMID: 41413522 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests. Ethics approval and consent to participate: Not 
Applicable.


236. BMC Med Inform Decis Mak. 2025 Mar 26;25(1):147. doi: 
10.1186/s12911-025-02972-2.

Effectiveness of various general large language models in clinical consensus and 
case analysis in dental implantology: a comparative study.

Wu Y(#)(1), Zhang Y(#)(2), Xu M(3), Jinzhi C(4), Xue Y(5), Zheng Y(6).

Author information:
(1)Center for Plastic & Reconstructive Surgery, Department of Stomatology, 
Zhejiang Provincial People's Hospital, Affiliated People's Hospital, Hangzhou 
Medical College, Hangzhou, Zhejiang, China.
(2)Xianju Traditional Chinese Medicine Hospital, Taizhou, Zhejiang, China. 
m13013319965@gmail.com.
(3)Hangzhou Dental Hospital, West Branch, Hangzhou, Zhejiang, China.
(4)College of Oceanography, HoHai University, Nanjng, Jiangsu, China.
(5)Hangzhou Medical College, Hangzhou, Zhejiang, China.
(6)Center for Plastic & Reconstructive Surgery, Department of Stomatology, 
Zhejiang Provincial People's Hospital, Affiliated People's Hospital, Hangzhou 
Medical College, Hangzhou, Zhejiang, China. zhengyuchen@hmc.edu.cn.
(#)Contributed equally

BACKGROUND: This study evaluates and compares ChatGPT-4.0, Gemini Pro 1.5(0801), 
Claude 3 Opus, and Qwen 2.0 72B in answering dental implant questions. The aim 
is to help doctors in underserved areas choose the best LLMs(Large Language 
Model) for their procedures, improving dental care accessibility and clinical 
decision-making.
METHODS: Two dental implant specialists with over twenty years of clinical 
experience evaluated the models. Questions were categorized into simple 
true/false, complex short-answer, and real-life case analyses. Performance was 
measured using precision, recall, and Bayesian inference-based evaluation 
metrics.
RESULTS: ChatGPT-4 exhibited the most stable and consistent performance on both 
simple and complex questions. Gemini Pro 1.5(0801)performed well on simple 
questions but was less stable on complex tasks. Qwen 2.0 72B provided 
high-quality answers for specific cases but showed variability. Claude 3 opus 
had the lowest performance across various metrics. Statistical analysis 
indicated significant differences between models in diagnostic performance but 
not in treatment planning.
CONCLUSIONS: ChatGPT-4 is the most reliable model for handling medical 
questions, followed by Gemini Pro 1.5(0801). Qwen 2.0 72B shows potential but 
lacks consistency, and Claude 3 Opus performs poorly overall. Combining multiple 
models is recommended for comprehensive medical decision-making.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-02972-2
PMCID: PMC11938642
PMID: 40140812 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The research protocol received evaluation and authorization from 
the Ethics Committee of the Zhejiang Provincial People’s Hospital (No. 
QT2023249). All patient medical records used in this study were included with 
the informed consent of the patients. This study was conducted in accordance 
with the principles of the Declaration of Helsinki. Consent for publication: All 
authors have read and approved the final manuscript and consent to its 
publication. Relevant guidelines and regulations: The study was conducted 
following the International Team for Implantology (ITI) Clinical Guidebook 
Series standards and relevant clinical practice guidelines for dental 
implantology. Competing interests: The authors declare no competing interests.


237. JMIR Med Inform. 2025 Mar 12;13:e64682. doi: 10.2196/64682.

GPT-3.5 Turbo and GPT-4 Turbo in Title and Abstract Screening for Systematic 
Reviews.

Oami T(1), Okada Y(2)(3), Nakada TA(1).

Author information:
(1)Department of Emergency and Critical Care Medicine, Chiba University Graduate 
School of Medicine, 1-8-1 Inohana, Chuo, Chiba, 260-8677, Japan, 81 432262372.
(2)Department of Preventive Services, Kyoto University Graduate School of 
Medicine, Kyoto, Japan.
(3)Health Services and Systems Research, Duke-NUS Medical School, National 
University of Singapore, Singapore, Singapore.

This study demonstrated that while GPT-4 Turbo had superior specificity when 
compared to GPT-3.5 Turbo (0.98 vs 0.51), as well as comparable sensitivity 
(0.85 vs 0.83), GPT-3.5 Turbo processed 100 studies faster (0.9 min vs 1.6 min) 
in citation screening for systematic reviews, suggesting that GPT-4 Turbo may be 
more suitable due to its higher specificity and highlighting the potential of 
large language models in optimizing literature selection.

© Takehiko Oami, Yohei Okada, Taka-aki Nakada. Originally published in JMIR 
Medical Informatics (https://medinform.jmir.org).

DOI: 10.2196/64682
PMCID: PMC11922487
PMID: 40073422 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


238. J Biomed Inform. 2025 Dec;172:104942. doi: 10.1016/j.jbi.2025.104942. Epub 2025 
Oct 31.

Clinical pathway-aware large language models for reliable and transparent 
medical dialogue.

Wu J(1), Wu X(2), Zheng Y(3), Yang J(4).

Author information:
(1)Division of Pharmacoepidemiology and Pharmacoeconomics, Department of 
Medicine, Brigham and Women's Hospital, Harvard Medical School, Boston, MA, USA.
(2)Jarvis Research Center, Tencent YouTu Lab, Beijing, China. Electronic 
address: kevinxwu@tencent.com.
(3)Jarvis Research Center, Tencent YouTu Lab, Beijing, China; Westlake 
University, Hangzhou, China.
(4)Division of Pharmacoepidemiology and Pharmacoeconomics, Department of 
Medicine, Brigham and Women's Hospital, Harvard Medical School, Boston, MA, USA; 
Harvard Data Science Initiative, Harvard University, Cambridge, MA, USA; Broad 
Institute of MIT and Harvard, Cambridge, MA, USA. Electronic address: 
jyang66@bwh.harvard.edu.

OBJECTIVE: Large language models (LLMs) offer promising potential in answering 
real-time medical queries, but they often produce lengthy, generic, and even 
hallucinatory responses. We aim to develop a reliable and interpretable medical 
dialogue system that incorporates clinical reasoning and then mitigates the risk 
of hallucination.
METHODS: Two large datasets of real-world online consultation, MedDG and KaMed, 
were used for evaluation. We proposed a Medical Dialogue System with Knowledge 
Enhancement and Clinical Pathway Encoding (MedKP), which integrates an external 
medical knowledge graph and encodes internal clinical pathways to model 
physician reasoning. Performance was compared with state-of-the-art baselines, 
including GPT-4o and LLaMA3.1-70B. A multi-dimensional evaluation framework 
assessed (1) clinical relevance (medical entity-based), (2) textual similarity 
(ROUGE, BLEU), (3) semantic alignment (BERTScore), and (4) hallucination and 
consistency via an external LLM-based judge, as well as parallel human 
evaluation.
RESULTS: Across both datasets, MedKP (6B) achieved the best overall performance, 
outperforming other advanced baselines and producing responses that align more 
closely with those of human physicians. For clinical relevance, MedKP reached a 
macro F1-score of medical entity at 31.41 on MedDG (previous best DFMed: 24.76, 
improved 30.41%) and 26.62 on KaMed (previous best LLaM-A3.1-70B: 20.67, 
improved 25.62%). Consistent improvements were observed across other metrics. 
Ablation studies further validated the effectiveness of each model component.
CONCLUSION: Our results highlight the critical role of clinical reasoning in 
advancing trustworthy AI for digital healthcare. By enhancing the reliability, 
coherence, and transparency of AI-generated responses, this pathway-aware 
approach bridges the gap between LLMs and real-world clinical workflows, 
improving the accessibility of high-quality telemedicine services, particularly 
benefiting underserved populations.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104942
PMID: 41177245 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


239. Int J Gynaecol Obstet. 2025 Mar;168(3):1285-1291. doi: 10.1002/ijgo.15959. Epub 
2024 Nov 11.

Evaluating the performance and clinical decision-making impact of ChatGPT-4 in 
reproductive medicine.

Chen R(1), Zeng D(1), Li Y(2), Huang R(1), Sun D(1), Li T(1).

Author information:
(1)The Sixth Affiliated Hospital, Sun Yat-sen University, Guangzhou, China.
(2)Sun Yat-sen Memorial Hospital, Sun Yat-sen University, Guangzhou, China.

BACKGROUND: ChatGPT, a sophisticated language model developed by OpenAI, has the 
potential to offer professional and patient-friendly support. We aimed to assess 
the accuracy and reproducibility of ChatGPT-4 in answering questions related to 
knowledge, management, and support within the field of reproductive medicine.
METHODS: ChatGPT-4 was used to respond to queries sourced from a domestic 
attending physician examination database, as well as to address both local and 
international treatment guidelines within the field of reproductive medicine. 
Each response generated by ChatGPT-4 was independently evaluated by a trio of 
experts specializing in reproductive medicine. The experts used four qualitative 
measures-relevance, accuracy, completeness, and understandability-to assess each 
response.
RESULTS: We found that ChatGPT-4 demonstrated extensive knowledge in 
reproductive medicine, with median scores for relevance, accuracy, completeness, 
and comprehensibility of objective questions being 4, 3.5, 3, and 3, 
respectively. However, the composite accuracy rate for multiple-choice questions 
was 63.38%. Significant discrepancies were observed among the three experts' 
scores across all four measures. Expert 1 generally provided higher and more 
consistent scores, while Expert 3 awarded lower scores for accuracy. ChatGPT-4's 
responses to both domestic and international guidelines showed varying levels of 
understanding, with a lack of knowledge on regional guideline variations. 
However, it offered practical and multifaceted advice regarding next steps and 
adjusting to new guidelines.
CONCLUSIONS: We analyzed the strengths and limitations of ChatGPT-4's responses 
on the management of reproductive medicine and relevant support. ChatGPT-4 might 
serve as a supplementary informational tool for patients and physicians to 
improve outcomes in the field of reproductive medicine.

© 2024 International Federation of Gynecology and Obstetrics.

DOI: 10.1002/ijgo.15959
PMID: 39526823 [Indexed for MEDLINE]


240. J Thorac Cardiovasc Surg. 2025 Aug;170(2):353-358. doi: 
10.1016/j.jtcvs.2025.01.022. Epub 2025 Feb 1.

Navigating the future of pediatric cardiovascular surgery: Insights and 
innovation powered by Chat Generative Pre-Trained Transformer (ChatGPT).

Mehta R(1), Reitz JG(1), Venna A(1), Selcuk A(1), Dhamala B(1), Klein J(1), 
Sawda C(1), Haverty M(1), Yerebakan C(1), Tongut A(1), Desai M(1), d'Udekem 
Y(2).

Author information:
(1)Department of Cardiac Surgery, Children's National Heart Institute, 
Children's National Hospital, Washington, DC.
(2)Department of Cardiac Surgery, Children's National Heart Institute, 
Children's National Hospital, Washington, DC. Electronic address: 
yves.dudekem@childrensnational.org.

INTRODUCTION: Interdisciplinary consultations are essential to decision-making 
for patients with congenital heart disease. The integration of artificial 
intelligence (AI) and natural language processing into medical practice is 
rapidly accelerating, opening new avenues to diagnosis and treatment. The main 
objective of this study was to consult the AI-trained model Chat Generative 
Pre-Trained Transformer (ChatGPT) regarding cases discussed during a 
cardiovascular surgery conference (CSC) at a single tertiary center and compare 
the ChatGPT suggestions with CSC expert consensus results.
METHODS: In total, 37 cases discussed at a single CSC were retrospectively 
identified. Clinical information comprised deidentified data from the last 
electrocardiogram, echocardiogram, intensive care unit progress note (or 
cardiology clinic note if outpatient), as well as a patient summary. The 
diagnosis was removed from the summary and possible treatment options were 
deleted from all notes. ChatGPT (version 4.0) was asked to summarize the case, 
identify diagnoses, and recommend surgical procedures and timing of surgery. The 
responses of ChatGPT were compared with the results of the CSC.
RESULTS: Of the 37 cases uploaded to ChatGPT, 45.9% (n = 17) were considered to 
be less complex cases, with only 1 treatment option, and 54.1% (n = 20) were 
considered more complex, with several treatment options. ChatGPT correctly 
provided a detailed and systematically written summary for each case within 10 
to 15 seconds. ChatGPT correctly identified diagnoses for approximately 94.5% (n 
= 35) cases. The surgical intervention plan matched the group decision for 
approximately 40.5% (n = 15) cases; however, it differed in 27% cases. In 23 of 
37 cases, timing of surgery was the same between CSC group and ChatGPT. Overall, 
the match between ChatGPT responses and CSC decisions for diagnosis was 94.5%, 
surgical intervention was 40.5%, and timing of surgery was 62.2%. However, 
within complex cases, we have 25% agreement for surgical intervention and 67% 
for timing of surgery.
CONCLUSIONS: ChatGPT can be used as an augmentative tool for surgical 
conferences to systematically summarize large amounts of patient data from 
electronic health records and clinical notes in seconds. In addition, our study 
points out the potential of ChatGPT as an AI-based decision support tool in 
surgery, particularly for less-complex cases. The discrepancy, particularly in 
complex cases, emphasizes on the need for caution when using ChatGPT in 
decision-making for the complex cases in pediatric cardiovascular surgery. There 
is little doubt that the public will soon use this comparative tool.

Copyright © 2025. Published by Elsevier Inc.

DOI: 10.1016/j.jtcvs.2025.01.022
PMID: 39894069 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest Statement The authors 
reported no conflicts of interest. The Journal policy requires editors and 
reviewers to disclose conflicts of interest and to decline handling or reviewing 
manuscripts for which they may have a conflict of interest. The editors and 
reviewers of this article have no conflicts of interest.


241. Digit Health. 2025 Nov 13;11:20552076251396576. doi: 10.1177/20552076251396576. 
eCollection 2025 Jan-Dec.

Can large language models respond health education questions for patients with 
palmar hyperhidrosis? A comparative study of ChatGPT and DeepSeek.

Fan S(1), Liu X(2), Tang M(1), Ju S(1), Li J(1), Luo J(1).

Author information:
(1)Department of Thoracic Surgery, The First Affiliated Hospital of Soochow 
University, Suzhou, China.
(2)School of Nursing, Suzhou Medical College of Soochow University, Suzhou, 
China.

OBJECTIVE: To compare the adaptability of two large language models: ChatGPT and 
DeepSeek in responding to health education questions related to patients with 
palmar hyperhidrosis.
METHODS: Based on clinical guidelines and expert experience, 17 health education 
questions relevant to palmar hyperhidrosis were developed and posed separately 
to ChatGPT and DeepSeek. Twelve experienced thoracic surgery experts 
independently evaluated the adaptability of the responses generated by both 
models. Each response was rated using a five-point Likert scale to 
quantitatively analyze the adaptability of the information provided.
RESULTS: Both language models demonstrated good adaptability in addressing 
health education questions related to palmar hyperhidrosis. In the English 
context, 10 responses of ChatGPT received a full score (5 points) from more than 
50% of experts, while DeepSeek did so for 8. In the Chinese context, both 
ChatGPT and DeepSeek receive 10 responses a full score (5 points) from more than 
50% of experts. ChatGPT outperformed DeepSeek in the English-language setting, 
whereas DeepSeek showed superior overall performance in the Chinese context.
CONCLUSION: This preliminary study demonstrates that both ChatGPT and DeepSeek 
are capable of effectively addressing health education questions for patients 
with palmar hyperhidrosis. ChatGPT performs better in English-language setting, 
while DeepSeek shows greater adaptability in Chinese-language context. However, 
human review remains essential to ensure the accuracy and reliability of the 
provided information in practical applications.

© The Author(s) 2025.

DOI: 10.1177/20552076251396576
PMCID: PMC12615908
PMID: 41246197

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


242. Clin Ophthalmol. 2025 Nov 5;19:4077-4084. doi: 10.2147/OPTH.S556186. eCollection 
2025.

Diagnostic Accuracy and Counseling Quality of GPT-4o for Strabismus and 
Pseudostrabismus in Patient-Generated Mobile Photographs: A Preliminary 
Evaluation.

Esposito EP(1), Cardakli N(2), Christoff A(2), Kraus CL(2).

Author information:
(1)Scheie Eye Institute, University of Pennsylvania, Philadelphia, PA, USA.
(2)Division of Pediatric Ophthalmology and Adult Strabismus, Wilmer Eye 
Institute, Baltimore, MD, USA.

BACKGROUND: Research-grade artificial intelligence has been used to accurately 
diagnose strabismus from image input. OpenAI's consumer-oriented GPT-4o model 
can analyze images, but has shown poor accuracy for image-based diagnosis. 
Parents may turn to GPT-4o to support or refute visible health concerns for 
their children, such as strabismus. The study aims to evaluate GPT-4o's 
diagnostic accuracy and response quality for strabismus evaluation.
METHODS: After gold-standard alternate cover exam by a clinician, 35 mobile 
photos of esotropia (13), pseudoesotropia (11), and exotropia (11) were 
selected. Images were excluded if a second "masked" examiner did not corroborate 
diagnosis. Images were submitted to a secure GPT-4o platform with 
patient-perspective prompts requesting overall evaluation (Prompt 1) and eye 
alignment evaluation (Prompt 2). Responses were graded by a pediatric 
ophthalmologist and certified orthoptist assessing for quality and safety.
RESULTS: GPT-4o provided interpretations for 15/35 and 27/35 images after 
Prompts 1 and 2, respectively. Analysis of the accuracy includes a primary 
"intention-to-diagnose" and secondary "per-diagnosis" framework. The diagnostic 
accuracies in the primary and secondary analysis following prompt 1 were 14.3% 
(low sensitivity and specificity) and 33.3% (low sensitivity, high specificity), 
respectively. Following prompt 2, accuracies were 48.6% (moderate sensitivity, 
low specificity) and 63.0% (high sensitivity, low specificity). Overall, the 
mean rating for content for strabismus prompts was 4.94 ± 0.27 out of a best 
possible 6, and for pseudostrabismus, 5.14 ± 0.18 (p=0.638).
CONCLUSION: GPT-4o shows poor accuracy for image-based strabismus diagnosis. 
GPT-4o frequently categorized pseudoesotropia as true strabismus and true 
strabismus as orthophoria. While the quality of responses was rated as good 
overall, the quality of counseling did not match what would be provided in a 
pediatric ophthalmology clinic. Patients, clinicians, and AI developers should 
be aware of the need for specialist evaluation for strabismus.

© 2025 Esposito et al.

DOI: 10.2147/OPTH.S556186
PMCID: PMC12596880
PMID: 41216185

Conflict of interest statement: The authors declare no conflicts of interest.


243. NPJ Digit Med. 2025 Nov 28;8(1):731. doi: 10.1038/s41746-025-02009-y.

Leveraging large language models to extract smoking history from clinical notes 
for lung cancer surveillance.

Luo I(#)(1), Graber-Naidich A(#)(1), Zhang M(#)(1), Kaushik R(1), Nieda GM(1), 
Chen T(2), Gu B(1), Choi E(3), Ding VY(1), Gunturkun F(1), Satoyoshi M(4), Bhat 
A(4), Lee TY(1), Su CC(1), Ellis-Caleo TJ(5), Henry AS(4), Desai M(1)(6), 
Backhus LM(5)(7), Lui NS(7), Leung A(8), Neal JW(5)(6), Kurian AW(5)(6)(9), 
Langlotz CP(8), Wakelee HA(5)(6), Liang SY(2), Khan A(10), Han 
SS(11)(12)(13)(14).

Author information:
(1)Quantitative Sciences Unit, Department of Medicine, Stanford University 
School of Medicine, Stanford, CA, USA.
(2)Palo Alto Medical Foundation Research Institute, Sutter Health, Palo Alto, 
CA, USA.
(3)Department of Population Health Sciences, Weill Cornell Medicine, New York, 
NY, USA.
(4)Technology & Digital Solutions, Stanford Health Care, Palo Alto, CA, USA.
(5)Division of Oncology, Department of Medicine, Stanford University School of 
Medicine, Stanford, CA, USA.
(6)Stanford Cancer Institute, Stanford University, Stanford, CA, USA.
(7)Department of Cardiothoracic Surgery, Stanford University School of Medicine, 
Stanford, CA, USA.
(8)Department of Radiology, Stanford University, Stanford, CA, USA.
(9)Department of Epidemiology and Population Health, Stanford University School 
of Medicine, Stanford, CA, USA.
(10)Department of Computer Science and Engineering, Indian Institute of 
Technology (BHU) Varanasi, Varanasi, India.
(11)Quantitative Sciences Unit, Department of Medicine, Stanford University 
School of Medicine, Stanford, CA, USA. summerh@stanford.edu.
(12)Stanford Cancer Institute, Stanford University, Stanford, CA, USA. 
summerh@stanford.edu.
(13)Department of Epidemiology and Population Health, Stanford University School 
of Medicine, Stanford, CA, USA. summerh@stanford.edu.
(14)Department of Neurosurgery, Stanford University School of Medicine, 
Stanford, CA, USA. summerh@stanford.edu.
(#)Contributed equally

Accurate smoking documentation in electronic health records (EHRs) is crucial 
for risk assessment and patient monitoring. However, key information is often 
missing or inaccurately recorded. Large language models (LLMs) present a 
promising solution for interpreting clinical narratives to extract comprehensive 
smoking data. We developed a framework utilizing LLMs combined with rule-based 
longitudinal smoothing techniques to enhance data quality. We compared 
generative LLMs (Gemini-1.5-Flash, PaLM-2-Text-Bison, GPT-4) against BERT-based 
models using 1683 manually annotated clinical notes from 518 patients across 
Stanford and Sutter Health systems. Generative LLMs achieved superior 
performance ( > 96% accuracy) across seven smoking variables, with external 
validation showing robust generalizability (97.5-98.8% accuracy). We deployed 
Gemini-1.5-Flash to 79,408 notes from 4792 lung cancer patients, demonstrating 
that risk model-based surveillance incorporating smoking factors outperformed 
NCCN Guidelines in identifying second malignancies. Our study highlights the 
potential of generative LLMs to improve smoking history documentation quality, 
enhancing lung cancer surveillance and broader clinical applications.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-02009-y
PMCID: PMC12663133
PMID: 41315854

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


244. Med Oral Patol Oral Cir Bucal. 2025 Jul 1;30(4):e551-e560. doi: 
10.4317/medoral.27092.

Comparative analysis of language models in addressing syphilis-related queries.

Ferreira LM(1), Nascimento JP, Souza LL, Souza FT, Guimarães LD, Lopes MA, 
Vargas PA, Martelli-Júnior H.

Author information:
(1)Piracicaba Dental School, UNICAMP Avenida Limeira 901, Areião, Piracicaba Zip 
code: 13414-900, São Paulo, Brazil luizmig.f@gmail.com.

BACKGROUND: Syphilis, caused by Treponema pallidum, is a significant global 
health concern with potentially severe complications if untreated. Advances in 
artificial intelligence (AI), particularly large language models (LLMs), offer 
opportunities to enhance medical diagnosis and public health education. This 
study aims to assess LLMs' ability to provide readable, accurate, and 
comprehensive syphilis information by comparing it with WHO datasheets and 
validating through specialist evaluation for clinical relevance.
MATERIAL AND METHODS: Ten AI-based LLMs were evaluated. Ten questions addressing 
symptoms, transmission, diagnosis, treatment, and prevention were crafted by 
researchers. Responses from the LLMs were compared to World Health Organization 
(WHO) syphilis fact sheets, and a panel of specialists assessed the accuracy, 
clinical relevance, and readability of the AI-generated information.
RESULTS: Among the evaluated LLMs, ChatGPT 4.0 and Claude demonstrated the 
highest accuracy, scoring 92% and 89% alignment with WHO standards, 
respectively. Perplexity and Llama3 performed less reliably, with scores between 
60-70%, especially in areas like tertiary syphilis and neurosyphilis. 
Specialists identified common errors, such as outdated treatment protocols and 
incorrect descriptions of transmission pathways. Expert reviews further revealed 
that while LLMs provided adequate information on early syphilis symptoms, they 
struggled with complex clinical nuances. The specialists' evaluation showed that 
only 60% of the AI-generated content was deemed clinically reliable without 
further edits, with ChatGPT 4.0 rated highest by experts in terms of readability 
and clinical accuracy.
CONCLUSIONS: LLMs hold promise for disseminating syphilis information, but human 
oversight is crucial. AI models need refinement to improve their accuracy, 
especially in complex medical scenarios.

DOI: 10.4317/medoral.27092
PMCID: PMC12221141
PMID: 40421993 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflict of interest, 
financial or otherwise.


245. Endocrine. 2025 Apr;88(1):315-322. doi: 10.1007/s12020-024-04121-7. Epub 2024 
Dec 2.

Comparative evaluation of ChatGPT-4, ChatGPT-3.5 and Google Gemini on PCOS 
assessment and management based on recommendations from the 2023 guideline.

Gunesli I(1), Aksun S(1)(2), Fathelbab J(3), Yildiz BO(4)(5).

Author information:
(1)Hacettepe University School of Medicine, Department of Internal Medicine, 
Ankara, Turkey.
(2)Hacettepe University School of Medicine, Division of Endocrinology and 
Metabolism, Ankara, Turkey.
(3)Hacettepe University School of Medicine, Ankara, Turkey.
(4)Hacettepe University School of Medicine, Department of Internal Medicine, 
Ankara, Turkey. yildizbo@yahoo.com.
(5)Hacettepe University School of Medicine, Division of Endocrinology and 
Metabolism, Ankara, Turkey. yildizbo@yahoo.com.

CONTEXT: Artificial intelligence (AI) is increasingly utilized in healthcare, 
with models like ChatGPT and Google Gemini gaining global popularity. Polycystic 
ovary syndrome (PCOS) is a prevalent condition that requires both lifestyle 
modifications and medical treatment, highlighting the critical need for 
effective patient education. This study compares the responses of ChatGPT-4, 
ChatGPT-3.5 and Gemini to PCOS-related questions using the latest guideline. 
Evaluating AI's integration into patient education necessitates assessing 
response quality, reliability, readability and effectiveness in managing PCOS.
PURPOSE: To evaluate the accuracy, quality, readability and tendency to 
hallucinate of ChatGPT-4, ChatGPT-3.5 and Gemini's responses to questions about 
PCOS, its assessment and management based on recommendations from the current 
international PCOS guideline.
METHODS: This cross-sectional study assessed ChatGPT-4, ChatGPT-3.5, and 
Gemini's responses to PCOS-related questions created by endocrinologists using 
the latest guidelines and common patient queries. Experts evaluated the 
responses for accuracy, quality and tendency to hallucinate using Likert scales, 
while readability was analyzed using standard formulas.
RESULTS: ChatGPT-4 and ChatGPT-3.5 attained higher scores in accuracy and 
quality compared to Gemini (p = 0.001, p < 0.001 and p = 0.007, p < 0.001 
respectively). However, Gemini obtained a higher readability score compared to 
the other chatbots (p < 0.001). There was a significant difference between the 
tendency to hallucinate scores, which were due to the lower scores in Gemini 
(p = 0.003).
CONCLUSION: The high accuracy and quality of responses provided by ChatGPT-4 and 
3.5 to questions about PCOS suggest that they could be supportive in clinical 
practice. Future technological advancements may facilitate the use of artificial 
intelligence in both educating patients with PCOS and supporting the management 
of the disorder.

© 2024. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s12020-024-04121-7
PMID: 39623241 [Indexed for MEDLINE]

Conflict of interest statement: Compliance with ethical standards. Conflict of 
interest: The authors declare no competing interests.


246. Int J Pediatr Otorhinolaryngol. 2025 Jun;193:112336. doi: 
10.1016/j.ijporl.2025.112336. Epub 2025 Apr 3.

Comparative analysis of GPT-4 and Google Gemini's consistency with pediatric 
otolaryngology guidelines.

Rossi NA(1), Corona KK(2), Yoshiyasu Y(3), Hajiyev Y(3), Hughes CA(3), Pine 
HS(3).

Author information:
(1)Department of Otolaryngology, Nationwide Children's Hospital, Columbus, OH, 
USA. Electronic address: nicholas.armando.rossi@gmail.com.
(2)Department of Otolaryngology, University of Texas Health Science Center at 
San Antonio, San Antonio, TX, USA.
(3)Department of Otolaryngology, University of Texas Medical Branch, Galveston, 
TX, USA.

OBJECTIVE: To evaluate the accuracy and completeness of large language models 
(LLMs) in interpreting pediatric otolaryngology guidelines.
MATERIALS AND METHODS: GPT-4 and Google Gemini were assessed on their responses 
to queries based on key action statements from three American Academy of 
Otolaryngology - Head and Neck Surgery Foundation (AAO-HNSF) clinical practice 
guidelines. Two independent reviewers evaluated responses using Likert scales 
for accuracy (1-5) and completeness (1-3). Inter-rater reliability was assessed 
with weighted Cohen's kappa. Statistical comparisons between models were 
performed using the Wilcoxon Signed-Rank Test.
RESULTS: Both models achieved high scores (GPT-4: accuracy 4.74, completeness 
2.94; Google Gemini: accuracy 4.82, completeness 2.98). No significant 
difference was found in accuracy (p = 0.134), while completeness showed 
concordance (p = 0.34). AI responses often emphasized the importance of 
individualization and consulting healthcare professionals.
CONCLUSION: GPT-4 and Google Gemini demonstrated potential as assistive tools in 
pediatric otolaryngology. However, limitations exist, including pre-trained 
datasets and subjective evaluation methods. Continuous learning and model 
refinement are crucial for reliable clinical integration. AI should complement, 
not replace, human expertise. This study contributes to the exploration of LLMs 
in pediatric otolaryngology.

Copyright © 2025. Published by Elsevier B.V.

DOI: 10.1016/j.ijporl.2025.112336
PMID: 40203537 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no competing interests to disclose in relation to this 
research.


247. J Spinal Cord Med. 2025 Sep 12:1-6. doi: 10.1080/10790268.2025.2554013. Online 
ahead of print.

A comparative study of ChatGPT and DeepSeek in spinal cord injury patient 
education: Can artificial intelligence "speak" spinal cord injury?

Lau JYS(1), Gerald Sng GR(2)(3), Cao R(1), Chen J(1).

Author information:
(1)Department of Rehabilitation Medicine, Singapore General Hospital, Singapore, 
Singapore.
(2)Department of Endocrinology, Singapore General Hospital, Singapore, 
Singapore.
(3)Data Science and Artificial Intelligence Laboratory, Singapore General 
Hospital, Singapore, Singapore.

CONTEXT: Large language models (LLMs), such as ChatGPT and DeepSeek, are 
increasingly used by patients for medical information about their health 
condition. While several studies have explored ChatGPT's performance in spinal 
cord injury (SCI)-related queries, no direct comparison between GPT-4o and 
DeepSeek-V3 has been conducted in this context.
METHODS: The five most-searched SCI-related topics comprising of forty-eight 
questions were generated based on top Google Trends search terms. Responses were 
generated using GPT-4o and DeepSeek-V3, with three outputs per question. A total 
of two hundred and eighty-eight responses were independently evaluated by three 
Rehabilitation Physicians using the S.C.O.R.E. framework, which evaluates five 
domains: Safety, Consensus with Guidelines, Objectivity, Reproducibility, and 
Explainability on a 5-point Likert scale. Paired t-tests were used to compare 
model performance.
RESULTS: Both models achieved high ratings for Safety and Consensus with 
Guidelines. DeepSeek-V3 scored slightly but significantly higher in Objectivity 
(P = 0.014), Reproducibility (P = 0.007), and Explainability (P < 0.001). No 
significant differences were found in Safety or Consensus with Guidelines. 
Qualitative review highlighted more consistent and contextually rich answers 
from DeepSeek-V3.
CONCLUSION: While both GPT-4o and DeepSeek-V3 are generally safe and informative 
tools for SCI patient education, DeepSeek-V3 demonstrated slightly superior 
performance in delivering consistent, objective, and well-explained responses. 
LLMs may serve as useful adjuncts in SCI patient education, but ongoing 
evaluation and clinician oversight remains essential.

DOI: 10.1080/10790268.2025.2554013
PMID: 40938207


248. JMIR Form Res. 2025 Dec 19;9:e75607. doi: 10.2196/75607.

Comparing ChatGPT and DeepSeek for Assessment of Multiple-Choice Questions in 
Orthopedic Medical Education: Cross-Sectional Study.

Anusitviwat C(1), Suwannaphisit S(2), Bvonpanttarananon J(1), Tangtrakulwanich 
B(1).

Author information:
(1)Department of Orthopedics, Faculty of Medicine, Prince of Songkla University, 
15 Karnchanavanich Road, Hat Yai, 90110, Thailand, 66 74451601.
(2)Department of Orthopaedics, Faculty of Medicine, Vajira Hospital, 
Navamindradhiraj University, Bangkok, Thailand.

BACKGROUND: Multiple-choice questions (MCQs) are essential in medical education 
for assessing knowledge and clinical reasoning. Traditional MCQ development 
involves expert reviews and revisions, which can be time-consuming and subject 
to bias. Large language models (LLMs) have emerged as potential tools for 
evaluating MCQ accuracy and efficiency. However, direct comparisons of these 
models in orthopedic MCQ assessments are limited.
OBJECTIVE: This study compared the performance of ChatGPT and DeepSeek in terms 
of correctness, response time, and reliability when answering MCQs from an 
orthopedic examination for medical students.
METHODS: This cross-sectional study included 209 orthopedic MCQs from summative 
assessments during the 2023-2024 academic year. ChatGPT (including the "Reason" 
function) and DeepSeek (including the "DeepThink" function) were used to 
identify the correct answers. Correctness and response times were recorded and 
compared using a χ2 test and Mann-Whitney U test where appropriate. The two 
LLMs' reliability was assessed using the Cohen κ coefficient. The MCQs 
incorrectly answered by both models were reviewed by orthopedic faculty to 
identify ambiguities or content issues.
RESULTS: ChatGPT achieved a correctness rate of 80.38% (168/209), while DeepSeek 
achieved 74.2% (155/209; P=.04). ChatGPT's Reason function also outperformed 
DeepSeek's DeepThink function (177/209, 84.7% vs 168/209, 80.4%; P=.12). The 
average response time for ChatGPT was 10.40 (SD 13.29) seconds, significantly 
shorter than DeepSeek's 34.42 (SD 25.48) seconds (P<.001). Regarding 
reliability, ChatGPT demonstrated an almost perfect agreement (κ=0.81), whereas 
DeepSeek showed substantial agreement (κ=0.78). A completely false response was 
recorded in 7.7% (16/209) of responses for both models.
CONCLUSIONS: ChatGPT outperformed DeepSeek in correctness and response time, 
demonstrating its efficiency in evaluating orthopedic MCQs. This high 
reliability suggests its potential for integration into medical assessments. 
However, our results indicate that some MCQs will require revisions by 
instructors to improve their clarity. Further studies are needed to evaluate the 
role of artificial intelligence in other disciplines and to validate other LLMs.

© Chirathit Anusitviwat, Sitthiphong Suwannaphisit, Jongdee Bvonpanttarananon, 
Boonsin Tangtrakulwanich. Originally published in JMIR Formative Research 
(https://formative.jmir.org).

DOI: 10.2196/75607
PMCID: PMC12716854
PMID: 41418321 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


249. Radiol Cardiothorac Imaging. 2025 Apr;7(2):e240272. doi: 10.1148/ryct.240272.

Free-breathing, Highly Accelerated, Single-beat, Multisection Cardiac Cine MRI 
with Generative Artificial Intelligence.

Ghanbari F(1), Morales MA(1), Street JA(1), Rodriguez J(1), Johnson S(1), Pierce 
P(1), Carty A(1), Ngo LH(1), Hoeger CW(1), Tsao CW(1), Manning WJ(1)(2), Nezafat 
R(1).

Author information:
(1)Department of Medicine, Cardiovascular Division, Beth Israel Deaconess 
Medical Center, 330 Brookline Ave, Boston, MA 02215.
(2)Department of Radiology, Beth Israel Deaconess Medical Center and Harvard 
Medical School, Boston, Mass.

Purpose To develop and evaluate a free-breathing, highly accelerated, 
multisection, single-beat cine sequence for cardiac MRI. Materials and Methods 
This prospective study, conducted from July 2022 to December 2023, included 
participants with various cardiac conditions as well as healthy participants who 
were imaged using a 3-T MRI system. A single-beat sequence was implemented, 
collecting data for each section in one heartbeat. Images were acquired with an 
in-plane spatiotemporal resolution of 1.9 × 1.9 mm2 and 37 msec and 
reconstructed using resolution enhancement generative adversarial inline neural 
network (REGAIN), a deep learning model. Multibreath-hold k-space-segmented 
(4.2-fold acceleration) and free-breathing single-beat (14.8-fold acceleration) 
cine images were collected, both reconstructed with REGAIN. Left ventricular 
(LV) and right ventricular (RV) parameters between the two methods were 
evaluated with linear regression, Bland-Altman analysis, and Pearson 
correlation. Three expert cardiologists independently scored diagnostic and 
image quality. Scan and rescan reproducibility was evaluated in a subset of 
participants 1 year apart using the intraclass correlation coefficient (ICC). 
Results This study included 136 participants (mean age [SD], 54 years ± 15; 69 
female, 67 male), 40 healthy and 96 with cardiac conditions. k-Space-segmented 
and single-beat scan times were 2.6 minutes ± 0.8 and 0.5 minute ± 0.1, 
respectively. Strong correlations (P < .001) were observed between 
k-space-segmented and single-beat cine parameters in both LV (r = 0.97-0.99) and 
RV (r = 0.89-0.98). Scan and rescan reproducibility of single-beat cine was 
excellent (ICC, 0.97-1.0). Agreement among readers was high, with 125 of 136 
(92%) images consistently assessed as diagnostic and 133 of 136 (98%) 
consistently rated as having good image quality by all readers. Conclusion 
Free-breathing 30-second single-beat cardiac cine MRI yielded accurate 
biventricular measurements, reduced scan time, and maintained high diagnostic 
and image quality compared with conventional multibreath-hold k-space-segmented 
cine images. Keywords: MR-Imaging, Cardiac, Heart, Imaging Sequences, 
Comparative Studies, Technology Assessment Supplemental material is available 
for this article. © RSNA, 2025.

DOI: 10.1148/ryct.240272
PMCID: PMC12038826
PMID: 40178397 [Indexed for MEDLINE]

Conflict of interest statement: Disclosures of conflicts of interest: F.G. No 
relevant relationships. M.A.M. No relevant relationships. J.A.S. No relevant 
relationships. J.R. No relevant relationships. S.J. No relevant relationships. 
P.P. No relevant relationships. A.C. No relevant relationships. L.H.N. No 
relevant relationships. C.W.H. No relevant relationships. C.W.T. No relevant 
relationships. W.J.M. No relevant relationships. R.N. Research agreement with 
Siemens Healthineers, grant from the NIH, pending patent.


250. J Sport Health Sci. 2025 Dec;14:101016. doi: 10.1016/j.jshs.2024.101016. Epub 
2024 Nov 28.

Large language models' performances regarding common patient questions about 
osteoarthritis: A comparative analysis of ChatGPT-3.5, ChatGPT-4.0, and 
Perplexity.

Cao M(1), Wang Q(2), Zhang X(2), Liang Z(2), Qiu J(3), Yung PS(1), Ong MT(4).

Author information:
(1)Department of Orthopaedics and Traumatology, Faculty of Medicine, The Chinese 
University of Hong Kong, Hong Kong 999077, China; Center for 
Neuromusculoskeletal Restorative Medicine (CNRM), The Chinese University of Hong 
Kong, Hong Kong 999077, China.
(2)Department of Orthopaedics and Traumatology, Faculty of Medicine, The Chinese 
University of Hong Kong, Hong Kong 999077, China.
(3)School of Exercise and Health, Shanghai University of Sport, Shanghai 200438, 
China.
(4)Department of Orthopaedics and Traumatology, Faculty of Medicine, The Chinese 
University of Hong Kong, Hong Kong 999077, China; Center for 
Neuromusculoskeletal Restorative Medicine (CNRM), The Chinese University of Hong 
Kong, Hong Kong 999077, China. Electronic address: michael.ong@cuhk.edu.hk.

BACKGROUND: Large Language Models (LLMs) have gained much attention and, in 
part, have replaced common search engines as a popular channel for obtaining 
information due to their contextually relevant responses. Osteoarthritis (OA) is 
a common topic in skeletal muscle disorders, and patients often seek information 
about it online. Our study evaluated the ability of 3 LLMs (ChatGPT-3.5, 
ChatGPT-4.0, and Perplexity) to accurately answer common OA-related queries.
METHODS: We defined 6 themes (pathogenesis, risk factors, clinical presentation, 
diagnosis, treatment and prevention, and prognosis) based on a generalization of 
25 frequently asked questions about OA. Three consultant-level orthopedic 
specialists independently rated the LLMs' replies on a 4-point accuracy scale. 
The final ratings for each response were determined using a majority consensus 
approach. Responses classified as "satisfactory" were evaluated for 
comprehensiveness on a 5-point scale.
RESULTS: ChatGPT-4.0 demonstrated superior accuracy, with 64% of responses rated 
as "excellent", compared to 40% for ChatGPT-3.5 and 28% for Perplexity 
(Pearson's χ2 test with Fisher's exact test, all p < 0.001). All 3 LLM-chatbots 
had high mean comprehensiveness ratings (Perplexity = 3.88; ChatGPT-4.0 = 4.56; 
ChatGPT-3.5 = 3.96, out of a maximum score of 5). The LLM-chatbots performed 
reliably across domains, except for "treatment and prevention" However, 
ChatGPT-4.0 still outperformed ChatGPT-3.5 and Perplexity, garnering 53.8% 
"excellent" ratings (Pearson's χ2 test with Fisher's exact test, all p < 0.001).
CONCLUSION: Our findings underscore the potential of LLMs, specifically 
ChatGPT-4.0 and Perplexity, to deliver accurate and thorough responses to 
OA-related queries. Targeted correction of specific misconceptions to improve 
the accuracy of LLMs remains crucial.

Copyright © 2025. Production and hosting by Elsevier B.V.

DOI: 10.1016/j.jshs.2024.101016
PMCID: PMC12268069
PMID: 39613294 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests The authors declare that 
they have no competing interests.


251. BMC Nurs. 2025 Nov 21;24(1):1422. doi: 10.1186/s12912-025-04068-8.

Development and validation of the Nursing Process Evaluation Tool (NPET): a 
multidimensional instrument for assessing the quality of AI-generated nursing 
documentation.

Abudari MO(1), Abu-Abbas M(2), Al-Ma'ani M(3), Alradaydeh MF(3), Alduraidi H(4).

Author information:
(1)School of Nursing, The University of Jordan, Aqaba, Jordan. 
m.othman@ju.edu.jo.
(2)School of Nursing, Yarmouk University, Irbid, Jordan.
(3)College of Nursing, Al-Balqa Applied University, Al-Salt, Jordan.
(4)School of Nursing, The University of Jordan, Amman, Jordan.

BACKGROUND: The integration of generative artificial intelligence (AI) tools 
into nursing practice has accelerated documentation processes but it has also 
raised concerns regarding the completeness, accuracy, and clinical safety of 
AI-generated care plans. Despite the growing use of tools like ChatGPT, Gemini, 
and PopAI in clinical and academic settings, no validated instrument currently 
exists to assess the quality of such documentation across the nursing process.
OBJECTIVE: This study aimed to develop and validate the Nursing Process 
Evaluation Tool (NPET), a multidimensional instrument designed to assess the 
quality of AI-generated nursing documentation within the ADPIE (Assessment, 
Diagnosis, Planning, Implementation, Evaluation) framework.
METHODS: A two-phase cross-sectional study was conducted. Phase I focused on 
item development and content validation via two rounds of expert review 
(n = 23). Phase II evaluated the NPET’s psychometric properties by assessing 64 
AI-generated nursing care plans based on eight clinical scenarios using eight AI 
models. A total of 368 individual expert ratings were yielded. Reliability 
(Cronbach’s α, ICC), content and construct validity (I-CVI, S-CVI/Ave, 
exploratory factor analysis), and comparative model performance 
(repeated-measures ANOVA with Tukey post hoc tests) were analyzed.
RESULTS: The NPET demonstrated strong content validity (S-CVI/Ave = 0.88) and 
excellent internal consistency (α = 0.85–0.94 across domains). Inter-rater 
reliability was high (ICC_average = 0.85–0.94). Exploratory factor analysis 
supported the proposed structure: four domains were unidimensional, while the 
Assessment domain revealed two interpretable factors. Although the overall ANOVA 
did not reveal statistically significant differences among AI models (F (7, 
360) = 1.57, p = 0.144, ω² = 0.01), descriptive trends and post hoc tests showed 
that paid models consistently outperformed free versions. PopAI Paid achieved 
the highest mean NPET score (M = 3.44 on a 4-point scale), followed by ChatGPT 
Paid (M = 3.37), while Microsoft Copilot scored the lowest (M = 2.99). The 
largest pairwise difference—between PopAI Paid and Copilot—yielded a 
moderate-to-large effect size (Cohen’s d = 0.60).
CONCLUSION: The NPET is a valid and reliable tool for evaluating the quality of 
AI-generated nursing care plans. While the overall ANOVA did not yield 
statistically significant differences across AI models, the consistently high 
performance across tools and meaningful differences observed in descriptive and 
post hoc comparisons support the tool’s utility in nursing education, clinical 
auditing, and AI benchmarking. Future research should explore its application in 
real-world documentation and monitor its adaptability to evolving AI 
technologies.
SUPPLEMENTARY INFORMATION: The online version contains supplementary material 
available at 10.1186/s12912-025-04068-8.

DOI: 10.1186/s12912-025-04068-8
PMCID: PMC12639769
PMID: 41272536

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: All ethical principles were strictly observed to ensure the 
integrity of the study and the protection of participants. Informed consent was 
obtained from all expert participants after they were provided with clear 
information about the study’s purpose, procedures, and their right to withdraw 
at any stage without penalty. Confidentiality and anonymity were guaranteed 
throughout the research process; no identifiable information was collected, and 
all data were securely stored in password-protected files accessible only to the 
research team. Ethical approval for the study was granted by the Institutional 
Review Board (IRB) of Philadelphia University, School of Nursing, Amman, Jordan. 
Moreover, transparency and honesty in data handling and result reporting were 
upheld, ensuring that findings were presented objectively, regardless of whether 
they supported the proposed tool’s effectiveness. Finally, the study was 
conducted in accordance with the ethical principles of the Declaration of 
Helsinki and aimed to contribute to the advancement of nursing practice and 
patient care. Consent for publication: Not applicable. This study does not 
include any individual person’s data in any form (including individual details, 
images, or videos). Competing interests: The authors declare no competing 
interests.


252. BMC Med Inform Decis Mak. 2025 Sep 26;25(1):340. doi: 
10.1186/s12911-025-03181-7.

Evaluating the performance of ChatGPT in clinical multidisciplinary treatment: a 
retrospective study.

Wang X(1), Guo J(1), Zhang T(1), Lu H(1), Zhou D(1), Zhang H(#)(2), Wang 
X(#)(3).

Author information:
(1)Department of Critical Care Medicine, Shanghai East Hospital, Tongji 
University School of Medicine, No.150, Jimo Road, Pudong New Area, Shanghai, 
200120, China.
(2)Department of Critical Care Medicine, Shanghai East Hospital, Tongji 
University School of Medicine, No.150, Jimo Road, Pudong New Area, Shanghai, 
200120, China. boy8672@126.com.
(3)Department of Critical Care Medicine, Shanghai East Hospital, Tongji 
University School of Medicine, No.150, Jimo Road, Pudong New Area, Shanghai, 
200120, China. 1400094@tongji.edu.cn.
(#)Contributed equally

BACKGROUND: Multidisciplinary treatment (MDT) consultations are essential for 
managing complex patients. However, resource and time constraints can limit 
their quality. Large language models (LLMs) have shown potential in assisting 
clinical decision-making, but their performance in complex MDT scenarios remains 
unclear. This study aims to evaluate the quality of MDT recommendations 
generated by ChatGPT compared to those provided by physicians.
METHODS: Clinical data from 64 patient cases were retrospectively included in 
the study. ChatGPT was asked to provide specific MDT recommendations. 2 
experienced physicians evaluated and scored the responses in a blinded manner 
across 5 aspects: comprehensiveness, accuracy, feasibility, safety, and 
efficiency, each assessed by 2 questions.
RESULTS: The median overall score for ChatGPT was 41.0 out of 50.0, which was 
lower than the MDT physicians' median score of 43.5 (p = 0.001). Compared to the 
MDT physicians' responses, ChatGPT excelled in comprehensiveness (p < 0.001) but 
fell short in accuracy (p < 0.001), feasibility (p < 0.001), and efficiency 
(p = 0.003). Analysis of specific questions revealed that ChatGPT lacked the 
ability to reason through the etiologies of complex cases.
CONCLUSION: This study indicates that ChatGPT has potential in clinical MDT 
applications, particularly in demonstrating more comprehensive consideration of 
clinical factors. However, ChatGPT still has deficiencies in accuracy, which 
could lead to incorrect healthcare decisions. Therefore, further development and 
clinical validation of LLMs are necessary. Recognizing the current limitations 
of LLMs, it is essential to use them with caution in clinical practice.
TRIAL REGISTRATION: Not applicable to the present retrospective study. For 
transparency, a related prospective extension is registered at ChiCTR 
(ChiCTR2400088563; registered on 21 August 2024).

© 2025. The Author(s).

DOI: 10.1186/s12911-025-03181-7
PMCID: PMC12465737
PMID: 41013444 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This retrospective chart review study involving human participants 
was in accordance with the ethical standards of the institutional and national 
research committee and with the 1964 Helsinki Declaration and its later 
amendments or comparable ethical standards. Since this study was retrospective 
and only used the medical history data stored in the hospital’s electronic 
medical record system, after passing the ethical review, the Human Investigation 
Committee of Shanghai East Hospital approved this study and waived the 
requirement of informed consent. Consent for publication: This retrospective 
study utilized data which stored in the hospital’s electronic medical records 
system. All patients’ data used in this study underwent strict de-identification 
and anonymization procedures to ensure patient privacy and confidentiality. The 
requirement for informed consent was waived by our institutional ethics 
committee. Competing interests: The authors declare no competing interests.


253. BMC Gastroenterol. 2025 Dec 24;25(1):868. doi: 10.1186/s12876-025-04442-5.

ChatGPT improves usability, effectiveness, scalability, interpretability and 
accessibility, in early diagnosis of metabolic dysfunction-associated fatty 
liver disease.

Zhou X(1), Gao Z(2), Ma H(3), Hu J(1), Xu C(1), Shen Z(1), Tan M(4), Yu C(1).

Author information:
(1)Department of Gastroenterology, The First Affiliated Hospital, Zhejiang 
University School of Medicine, Hangzhou, Zhejiang Province, 310003, China.
(2)Shanghai Institute for Advanced Study of Zhejiang University, Shanghai, 
China.
(3)Department of Gastroenterology, The First Affiliated Hospital, Zhejiang 
University School of Medicine, Hangzhou, Zhejiang Province, 310003, China. 
mahan@zju.edu.cn.
(4)Key Laboratory of Artificial Organs and Computational Medicine in Zhejiang 
Province, Institute of Translational Medicine, Zhejiang Shuren University, 
Hangzhou, Zhejiang Province, China.

BACKGROUND: Metabolic dysfunction-associated fatty liver disease (MAFLD) is one 
of the most common chronic liver diseases. We aimed to conduct a pilot study to 
evaluate the potential utility of ChatGPT in early diagnosis of MAFLD.
METHODS: We retrospectively analyzed data from participants undergoing annual 
health examinations at the First Affiliated Hospital, Zhejiang University School 
of Medicine in 2022. Questionnaires, laboratory tests, physical examinations, 
and liver ultrasonography were conducted. We built MAFLD-GPT by leveraging the 
GPT-3.5-turbo model provided by OpenAI. Zero-shot and few-shot learning 
approaches were used to identify patterns within the MAFLD data samples to 
construct an MAFLD model. We compared MAFLD-GPT with Machine Learning Technique 
(MAFLD-ML).
RESULTS: In total, 7,571 participants were included in the study, with 123 cases 
containing non-numeric data. MAFLD-GPT generates predictions using prompt 
engineering with zero- or few-shot training samples, whereas machine learning 
methods require massive training data. MAFLD-GPT demonstrated superior 
performance in recall and F1-score compared to traditional ML models, 
highlighting its potential for MAFLD diagnosis. The initial in-context examples 
(3-shot) brought the largest performance gain (F1 score = 0.724) in MAFLD-GPT, 
which outperformed SVM (the best performance achieved by traditional ML 
algorithms) by 6.1%. In addition, the evaluation results of the MAFLD-GPT model 
on 123 feature-missing samples demonstrated that MAFLD-GPT achieved promising 
scalability on non-numeric features. Importantly, MAFLD-GPT has the potential to 
be accessible to the public and to present its results in a clear and 
understandable manner.
CONCLUSIONS: MAFLD-GPT surpasses the MAFLD-ML model in terms of usability, 
effectiveness, scalability, interpretability and accessibility, demonstrating 
its tremendous potential for early diagnosis of MAFLD.
SUPPLEMENTARY INFORMATION: The online version contains supplementary material 
available at 10.1186/s12876-025-04442-5.

DOI: 10.1186/s12876-025-04442-5
PMCID: PMC12729153
PMID: 41444506

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The study was conducted in accordance with the Declaration of 
Helsinki, and the protocol was approved by the Ethics Committee of the First 
Affiliated Hospital, Zhejiang University School of Medicine (reference number 
2024IIT-0371). The need for informed consent was waived by the Ethics Committee 
because the study involved retrospective data collection without any personally 
identifiable information. Consent for publication: Not applicable. Competing 
interests: There were no conflicts of interest or financial disclosures for all 
authors.


254. Diagnostics (Basel). 2025 May 17;15(10):1276. doi: 10.3390/diagnostics15101276.

Artificial Intelligence as a Potential Tool for Predicting Surgical Margin 
Status in Early Breast Cancer Using Mammographic Specimen Images.

Andras D(1)(2), Ilies RA(3), Esanu V(2), Agoston S(3), Marginean Jumate TF(3), 
Dindelegan GC(1)(2).

Author information:
(1)Department of General Surgery, Iuliu Hatieganu University of Medicine and 
Pharmacy, 400006 Cluj-Napoca, Romania.
(2)First Surgical Unit, Emergency County Hospital Cluj, 400006 Cluj-Napoca, 
Romania.
(3)Faculty of Medicine, Iuliu Hatieganu University of Medicine and Pharmacy, 
400012 Cluj-Napoca, Romania.

Background/Objectives: Breast cancer is the most common malignancy among women 
globally, with an increasing incidence, particularly in younger populations. 
Achieving complete surgical excision is essential to reduce recurrence. 
Artificial intelligence (AI), including large language models like ChatGPT, has 
potential for supporting diagnostic tasks, though its role in surgical oncology 
remains limited. Methods: This retrospective study evaluated ChatGPT's 
performance (ChatGPT-4, OpenAI, March 2025) in predicting surgical margin status 
(R0 or R1) based on intraoperative mammograms of lumpectomy specimens. 
AI-generated responses were compared with histopathological findings. 
Performance was evaluated using sensitivity, specificity, accuracy, positive 
predictive value (PPV), negative predictive value (NPV), F1 score, and Cohen's 
kappa coefficient. Results: Out of a total of 100 patients, ChatGPT achieved an 
accuracy of 84.0% in predicting surgical margin status. Sensitivity for 
identifying R1 cases (incomplete excision) was 60.0%, while specificity for R0 
(complete excision) was 86.7%. The positive predictive value (PPV) was 33.3%, 
and the negative predictive value (NPV) was 95.1%. The F1 score for R1 
classification was 0.43, and Cohen's kappa coefficient was 0.34, indicating 
moderate agreement with histopathological findings. Conclusions: ChatGPT 
demonstrated moderate accuracy in confirming complete excision but showed 
limited reliability in identifying incomplete margins. While promising, these 
findings emphasize the need for domain-specific training and further validation 
before such models can be implemented in clinical breast cancer workflows.

DOI: 10.3390/diagnostics15101276
PMCID: PMC12109882
PMID: 40428269

Conflict of interest statement: The authors declare no conflicts of interest.


255. Clin Nutr. 2025 Dec;55:76-80. doi: 10.1016/j.clnu.2025.10.014. Epub 2025 Oct 30.

Using artificial intelligence models to generate dietary recommendations for 
chronic kidney disease patients: A comparative cross-sectional study.

Yang B(1), Wei W(2), Liu C(2), Huang Y(2), Ren J(2), Yuan Y(3), Fu P(2), Zhao 
Y(4).

Author information:
(1)Department of Nephrology, Institute of Kidney Diseases, West China Hospital, 
Sichuan University, Chengdu 610041, China; West China School of Clinical 
Medicine, Sichuan University, Chengdu 610041, China.
(2)Department of Nephrology, Institute of Kidney Diseases, West China Hospital, 
Sichuan University, Chengdu 610041, China.
(3)School of Laboratory Medicine, Chengdu Medical College, Chengdu 610500, 
China.
(4)Department of Nephrology, Institute of Kidney Diseases, West China Hospital, 
Sichuan University, Chengdu 610041, China. Electronic address: 
zhaoyuliang@scu.edu.cn.

BACKGROUND & AIMS: Artificial intelligence (AI) models are increasingly being 
used to assist in chronic kidney disease (CKD) dietary guidance, but concerns 
about their accuracy remain. This study aimed to assess the performance of three 
AI models (ChatGPT, DeepSeek, Gemini) in accordance with the Kidney Disease: 
Improving Global Outcomes (KDIGO) 2024 Clinical Practice Guideline for the 
Evaluation and Management of Chronic Kidney Disease.
METHODS: We compared ChatGPT, DeepSeek, and Gemini with the KDIGO 2024 
guidelines through expert and cross-AI assessments. Outputs were scored across 
five domains: accuracy, clarity, completeness, practicality, and safety.
RESULTS: The AI models were able to generate well-structured dietary advice for 
CKD patients. DeepSeek received a higher score (7.625) for practicality compared 
to ChatGPT (6.5) and Gemini (7.0) (p = 0.041). ChatGPT showed numerically higher 
accuracy scores than the other two models, although there was no statistically 
significant difference (ChatGPT: 7.25, DeepSeek: 6.875, Gemini: 6.875; p = 
0.715). Similarly, scores for clarity (p = 0.489), completeness (p = 0.432) and 
safety (p = 0.413) were not significantly different. The AI models share several 
common limitations: they might overlook contraindications (such as recommending 
a low-protein diet for patients with severe hypoalbuminemia), neglect 
age-specific needs, and lack real-time knowledge updates.
CONCLUSIONS: While AI shows clinical potential, its outputs still require expert 
validation, real-time updates, and enhanced safety protocols to ensure reliable 
clinical integration.

Copyright © 2025 Elsevier Ltd and European Society for Clinical Nutrition and 
Metabolism. All rights reserved.

DOI: 10.1016/j.clnu.2025.10.014
PMID: 41197595 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of interest The authors declare that 
they have no known competing interests.


256. Am J Otolaryngol. 2025 Jul-Aug;46(4):104667. doi: 10.1016/j.amjoto.2025.104667. 
Epub 2025 May 10.

Evaluating advanced AI reasoning models: ChatGPT-4.0 and DeepSeek-R1 diagnostic 
performance in otolaryngology: a comparative analysis.

Prasad S(1), Langlie J(2), Pasick L(2), Chen R(2), Franzmann E(2).

Author information:
(1)Department of Otolaryngology, University of Miami Miller School of Medicine, 
1600 NW 10(th) Ave, Miami, FL 33131. United States of America.. Electronic 
address: soumilprasad@gmail.com.
(2)Department of Otolaryngology, University of Miami Miller School of Medicine, 
1600 NW 10(th) Ave, Miami, FL 33131. United States of America.

PURPOSE: This study aimed to evaluate the diagnostic accuracy, 
comprehensiveness, and clinical relevance of two advanced artificial 
intelligence (AI) models, OpenAI's ChatGPT-4.0 and DeepSeek-R1, in the field of 
otolaryngology.
METHODS: Five common otolaryngology procedures-adenotonsillectomy, 
tympanoplasty, endoscopic sinus surgery, parotidectomy, and total 
laryngectomy-were analyzed through standardized queries posed to both AI models. 
Because the prompts replicate questions that patients typically search online, 
our evaluation focuses on patient-facing informational adequacy. Responses were 
independently evaluated by two study members for accuracy, clinical relevance, 
and comprehensiveness, with discrepancies resolved through consensus. The 
analysis included comparison with clinical guidelines.
RESULTS: ChatGPT-4.0 generally provided detailed procedural insights, 
effectively covering indications, methodologies, risks, and recovery processes. 
However, it occasionally suggested excessive diagnostic imaging and omitted 
subtle yet significant surgical nuances. DeepSeek-R1 delivered concise, 
structured responses clearly categorizing indications, treatment alternatives, 
and procedural risks. Nonetheless, it frequently lacked detailed elaboration, 
omitting important surgical techniques and minor complications. For instance, 
DeepSeek-R1 omitted specifics such as hemostatic techniques in 
adenotonsillectomy and graft stabilization details in tympanoplasty. Neither 
model adequately addressed critical elements like comprehensive staging, 
detailed surgical planning, and long-term recovery nuances, especially for 
complex procedures such as total laryngectomy.
CONCLUSIONS: Both ChatGPT-4.0 and DeepSeek-R1 demonstrated significant 
diagnostic potential but revealed limitations in precision, comprehensiveness, 
and nuanced clinical reasoning. Their clinical utility remains restricted, 
highlighting a continued need for AI refinement to enhance patient-specific 
decision-making capabilities in otolaryngology.

Copyright © 2025. Published by Elsevier Inc.

DOI: 10.1016/j.amjoto.2025.104667
PMID: 40367837 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest ChatGPT 
Version 4.0 and DeepSeek R-1 were utilized in this manuscript to obtain output 
for common otolaryngology procedures. The authors nor the tertiary academic 
center are affiliated with OpenAI or DeepSeek and the softwares utilized in this 
study. The queries and answers provided by ChatGPT 4.0 and DeepSeek R-1 are not 
representative of the views of the tertiary academic center or the authors of 
the study. The authors declare no conflict of interest regarding the article.


257. Occup Ther Int. 2025 Apr 7;2025:6967523. doi: 10.1155/oti/6967523. eCollection 
2025.

The Occupational Challenges and Responses of International Graduate Students in 
South Korea: A Scoping Review.

Amaba EJ(1), Escuadra CJ(2), Choi JE(3), An SL(1)(3)(4).

Author information:
(1)Department of Rehabilitation Science, Graduate School of Inje University, 
Gimhae-si, Republic of Korea.
(2)Physical Therapy Department, College of Rehabilitation Sciences, University 
of Santo Tomas, Manila City, Philippines.
(3)HOPE Parent Training Center, Seoul, Republic of Korea.
(4)Department of Occupational Therapy, Inje University, Gimhae-si, Republic of 
Korea.

Background: Humans, as occupational beings, are inherently driven to achieve 
mastery and adaptation. This drive is evident in international graduate students 
who face unique academic and sociocultural challenges when adapting to 
unfamiliar environments, such as South Korea. These challenges can hinder their 
occupational participation, yet there is a notable gap in the literature 
addressing the specific occupational issues faced by international students in 
this context. Aim: This study is aimed at exploring the unique occupational 
challenges encountered by international graduate students in Korea and their 
general adaptive responses, analyzed through the lens of the occupational 
adaptation (OA) model. Additionally, this study proposed interventions for 
educational institutions and occupational therapy practitioners to support 
students in achieving OA. Method: A scoping review was conducted following the 
PRISMA guidelines. Systematic searches across global (Web of Science, Scopus, 
and PubMed) and Korean (RISS and KISS) databases identified qualitative studies 
published in English or Korean over the past 10 years. ChatGPT-4 assisted with 
data extraction, which was reviewed for accuracy. Study characteristics were 
summarized using descriptive statistics, and thematic analysis identified key 
themes related to occupational challenges. Results: The review identified 3 
articles and 6 dissertations exploring the occupational challenges of 59 
international graduate students in Korea. Most participants were master's 
students, primarily from China, Uzbekistan, and Laos. Two main themes emerged: 
(1) the hierarchical nature of Korean culture, which impeded academic and social 
engagement, and (2) the fast-paced lifestyle, which disrupted occupational 
balance. These challenges led to stress, isolation, and reduced participation, 
affecting role competence and quality of life. While students attempted adaptive 
strategies, they were often insufficient, highlighting the need for targeted 
interventions. Conclusions: International graduate students in Korea faced 
significant pressure to achieve mastery in a demanding environment. Culturally 
sensitive interventions, combined with strategies like cognitive behavioral 
therapy (CBT) and lifestyle redesign, can improve stress management, 
self-advocacy, and occupational balance, leading to greater academic success.

Copyright © 2025 Elaine Judith Amaba et al. Occupational Therapy International 
published by John Wiley & Sons Ltd.

DOI: 10.1155/oti/6967523
PMCID: PMC11996278
PMID: 40230784 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


258. Prostate Cancer Prostatic Dis. 2025 Mar;28(1):229-231. doi: 
10.1038/s41391-024-00789-0. Epub 2024 Jan 16.

Quality of information and appropriateness of Open AI outputs for prostate 
cancer.

Lombardo R(1), Gallo G(2), Stira J(2), Turchi B(2), Santoro G(2), Riolo S(2), 
Romagnoli M(2), Cicione A(2), Tema G(2), Pastore A(2), Al Salhi Y(2), Fuschi 
A(2), Franco G(2), Nacchia A(2), Tubaro A(2), De Nunzio C(2).

Author information:
(1)Department of Urology, 'Sapienza' University of Rome, Rome, Italy. 
rlombardo@me.com.
(2)Department of Urology, 'Sapienza' University of Rome, Rome, Italy.

Chat-GPT, a natural language processing (NLP) tool created by Open-AI, can 
potentially be used as a quick source for obtaining information related to 
prostate cancer. This study aims to analyze the quality and appropriateness of 
Chat-GPT's responses to inquiries related to prostate cancer compared to those 
of the European Urology Association's (EAU) 2023 prostate cancer guidelines. 
Overall, 195 questions were prepared according to the recommendations gathered 
in the prostate cancer section of the EAU 2023 Guideline. All questions were 
systematically presented to Chat-GPT's August 3 Version, and two expert 
urologists independently assessed and assigned scores ranging from 1 to 4 to 
each response (1: completely correct, 2: correct but inadequate, 3: a mix of 
correct and misleading information, and 4: completely incorrect). Sub-analysis 
per chapter and per grade of recommendation were performed. Overall, 195 
recommendations were evaluated. Overall, 50/195 (26%) were completely correct, 
51/195 (26%) correct but inadequate, 47/195 (24%) a mix of correct and 
misleading and 47/195 (24%) incorrect. When looking at different chapters Open 
AI was particularly accurate in answering questions on follow-up and QoL. Worst 
performance was recorded for the diagnosis and treatment chapters with 
respectively 19% and 30% of the answers completely incorrect. When looking at 
the strength of recommendation, no differences in terms of accuracy were 
recorded when comparing weak and strong recommendations (p > 0,05). Chat-GPT has 
a poor accuracy when answering questions on the PCa EAU guidelines 
recommendations. Future studies should assess its performance after adequate 
training.

© 2024. The Author(s), under exclusive licence to Springer Nature Limited.

DOI: 10.1038/s41391-024-00789-0
PMID: 38228809 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: The authors declare no 
competing interests. Ethical approval: The study was approved by a local ethical 
committee and was conducted in accordance with the principles of the Declaration 
of Helsinki.


259. Eur Urol Oncol. 2025 Dec 10:S2588-9311(25)00326-8. doi: 
10.1016/j.euo.2025.11.016. Online ahead of print.

Benchmarking Large Language Models Against Multidisciplinary Tumor Boards in 
Urological Oncology: Results from the Blinded, Prospective CONCORDIA Study.

Rinderknecht E(1), Haas M(2), Schnabel MJ(2), Kravchuk AP(3), Schäfer C(4), 
Siepmann S(3), Mayr R(5), von Winning D(3), Grassinger J(6), Goßler C(2), Pohl 
F(7), Siska PJ(8), Zeman F(9), Breyer J(2), Schmelzer A(10), Gilfrich C(3), 
Brookman-May SD(11), Burger M(2), May M(3).

Author information:
(1)Department of Urology, St. Josef Medical Center, University of Regensburg, 
Regensburg, Germany. Electronic address: erinderknecht@csj.de.
(2)Department of Urology, St. Josef Medical Center, University of Regensburg, 
Regensburg, Germany.
(3)Department of Urology, St. Elisabeth Hospital Straubing, Straubing, Germany.
(4)Department of Radiotherapy, Medical Care Center Straubing Hospital, 
Straubing, Germany.
(5)Department of Urology, University Hospital Augsburg, Augsburg, Germany.
(6)Department of Oncology Hematology, Medical Care Center Straubing Hospital, 
Straubing, Germany.
(7)Department of Radiotherapy, University Hospital Regensburg, Regensburg, 
Germany.
(8)Department of Internal Medicine III, University Hospital Regensburg, 
Regensburg, Germany.
(9)Center for Clinical Studies, University Hospital Regensburg, Regensburg, 
Germany.
(10)Department of Urology, Nuremberg General Hospital, Paracelsus Medical 
University, Nuremberg, Germany.
(11)Department of Urology, Ludwig-Maximilians-University Hospital, Munich, 
Germany.

BACKGROUND AND OBJECTIVE: Multidisciplinary tumor boards (MTBs) are the gold 
standard for oncological treatment planning, but their implementation is 
resource intensive. Large language models (LLMs) such as ChatGPT-4 and Claude 
3.5 Sonnet have emerged as scalable tools for clinical decision support. As 
their comparative performance in urological oncology remains largely untested in 
prospective trials, this study aimed to prospectively evaluate them against real 
MTBs.
METHODS: In this prospective, multicenter, noninferiority study (DRKS00034797), 
we evaluated whether therapeutic recommendations by ChatGPT-4 and Claude 3.5 
Sonnet were noninferior to those of MTBs across 110 representative case 
scenarios involving locally advanced or metastatic genitourinary cancer. 
Standardized prompts elicited recommendations from both LLMs, which were rated 
independently by two blinded uro-oncologists using the validated modified System 
Causability Scale (mSCS). The predefined noninferiority margin was 0.15 mSCS 
points.
KEY FINDINGS AND LIMITATIONS: The mean mSCS score of the MTBs was 0.849 
(standard deviation [SD] = 0.157), setting the noninferiority margin at 0.699. 
Claude 3.5 Sonnet scored 0.731 (SD = 0.178; 95% confidence interval [CI]: 
0.697-0.765), narrowly missing noninferiority. ChatGPT-4 scored 0.660 
(SD = 0.193; 95% CI: 0.623-0.696), clearly below the margin. A subgroup analysis 
revealed better LLM performance in locally advanced versus metastatic cases 
(p < 0.05). Limitations include the use of synthetic cases and inherent LLM 
output variability.
CONCLUSIONS AND CLINICAL IMPLICATIONS: Neither LLM matched the MTB standards. 
These findings highlight the current limitations of public LLMs, or of their use 
in our study, in supporting complex oncological decisions. This underscores the 
need for further validation and contextual refinement before integration into 
multidisciplinary care.

Copyright © 2025 European Association of Urology. Published by Elsevier B.V. All 
rights reserved.

DOI: 10.1016/j.euo.2025.11.016
PMID: 41381251


260. Semin Nucl Med. 2025 May;55(3):423-436. doi: 10.1053/j.semnuclmed.2024.05.005. 
Epub 2024 Jun 8.

Generative Artificial Intelligence Biases, Limitations and Risks in Nuclear 
Medicine: An Argument for Appropriate Use Framework and Recommendations.

Currie GM(1), Hawk KE(2), Rohren EM(3).

Author information:
(1)School of Dentistry and Medical Sciences, Charles Sturt University, Wagga 
Wagga, Australia; Dept of Radiology, Baylor College of Medicine, Houston. 
Electronic address: gcurrie@csu.edu.au.
(2)School of Dentistry and Medical Sciences, Charles Sturt University, Wagga 
Wagga, Australia; Dept of Radiology, Stanford University, Stanford.
(3)School of Dentistry and Medical Sciences, Charles Sturt University, Wagga 
Wagga, Australia; Dept of Radiology, Baylor College of Medicine, Houston.

Generative artificial intelligence (AI) algorithms for both text-to-text and 
text-to-image applications have seen rapid and widespread adoption in the 
general and medical communities. While limitations of generative AI have been 
widely reported, there remain valuable applications in patient and professional 
communities. Here, the limitations and biases of both text-to-text and 
text-to-image generative AI are explored using purported applications in medical 
imaging as case examples. A direct comparison of the capabilities of four common 
text-to-image generative AI algorithms is reported and recommendations for the 
most appropriate use, DALL-E 3, justified. The risks use and biases are 
outlined, and appropriate use guidelines framed for use of generative AI in 
nuclear medicine. Generative AI text-to-text and text-to-image generation 
includes inherent biases, particularly gender and ethnicity, that could 
misrepresent nuclear medicine. The assimilation of generative AI tools into 
medical education, image interpretation, patient education, health promotion and 
marketing in nuclear medicine risks propagating errors and amplification of 
biases. Mitigation strategies should reside inside appropriate use criteria and 
minimum standards for quality and professionalism for the application of 
generative AI in nuclear medicine.

Copyright © 2024 The Author(s). Published by Elsevier Inc. All rights reserved.

DOI: 10.1053/j.semnuclmed.2024.05.005
PMID: 38851934 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


261. Andrology. 2025 Feb;13(2):176-183. doi: 10.1111/andr.13693. Epub 2024 Jul 17.

Artificial intelligence and clinical guidance in male reproductive health: 
ChatGPT4.0's AUA/ASRM guideline compliance evaluation.

Gokmen O(1), Gurbuz T(2), Devranoglu B(3), Karaman MI(4).

Author information:
(1)Department of Gynecology, Obstetrics and In Vitro Fertilization Clinic, 
Medistate Hospital, Istanbul, Turkey.
(2)Department of Gynecology and Obstetrics Clinic, Medistate Hospital, Istanbul 
Nişantaşı University, Istanbul, Turkey.
(3)Department of Obstetrics and Gynecology, Zeynep Kamil Maternity/Children, 
Education and Training Hospital, Istanbul, Turkey.
(4)Department of Urology, Medistate Hospital, Istanbul, Turkey.

Comment in
    Nat Rev Urol. 2024 Sep;21(9):517. doi: 10.1038/s41585-024-00928-1.

BACKGROUND: Male infertility is defined as the inability of a male to achieve a 
pregnancy in a fertile female by the American Urological Association (AUA) and 
the American Society for Reproductive Medicine (ASRM). Artificial intelligence, 
particularly in language processing models like ChatGPT4.0, offers new 
possibilities for supporting clinical decision-making. This study aims to assess 
the effectiveness of ChatGPT4.0 in responding to clinical queries regarding male 
infertility, which is aligned with AUA/ASRM guidelines.
METHODS: This observational study employed a design to evaluate the performance 
of ChatGPT4.0 across 1073 structured clinical queries categorized into 
true/false, multiple-choice, and open-ended. Two independent reviewers 
specializing in reproductive medicine assessed the responses using a six-point 
Likert scale to evaluate accuracy, relevance, and guideline adherence.
RESULTS: In the true/false category, the initial accuracy was 92%, which 
increased to 94% by the end of the study period. For multiple-choice questions, 
accuracy improved from 85% to 89%. The most significant gains were seen in 
open-ended questions, where accuracy rose from 78% to 86%. Initially, some 
responses did not fully align with the AUA/ASRM guidelines. However, by the end 
of the 60 days, these responses had become more comprehensive and clinically 
relevant, indicating an improvement in the model's ability to generate 
guideline-conformant answers (p < 0.05). The depth and accuracy of responses for 
higher difficulty questions also showed enhancement (p < 0.01).
CONCLUSION: ChatGPT4.0 can serve as a valuable support tool in managing male 
infertility, providing reliable, guideline-based information that enhances the 
accuracy of clinical decision-making tools and supports patient education.

© 2024 American Society of Andrology and European Academy of Andrology.

DOI: 10.1111/andr.13693
PMID: 39016301 [Indexed for MEDLINE]


262. Sci Rep. 2025 Aug 28;15(1):31712. doi: 10.1038/s41598-025-16902-9.

The impact of prompting on ChatGPT's adherence to status epilepticus treatment 
guidelines.

Kliem PSC(1), Fisch U(2), Baumann SM(1), Berger S(1), Amacher SA(1), Hunziker 
S(3)(4), Sutter R(5)(6).

Author information:
(1)Clinic for Intensive Care Medicine, Department of Acute Medicine, University 
Hospital Basel, Petersgraben 4, 4031, Basel, Switzerland.
(2)Department of Neurology, University Hospital Basel, Basel, Switzerland.
(3)Medical Faculty of the University of Basel, Basel, Switzerland.
(4)Medical Communication and Psychosomatic Medicine, University Hospital Basel, 
Basel, Switzerland.
(5)Clinic for Intensive Care Medicine, Department of Acute Medicine, University 
Hospital Basel, Petersgraben 4, 4031, Basel, Switzerland. raoul.sutter@usb.ch.
(6)Medical Faculty of the University of Basel, Basel, Switzerland. 
raoul.sutter@usb.ch.

This study assessed ChatGPT's adherence to established management guidelines for 
status epilepticus (SE) from major neurological societies (NCS, AES, EFNS) and 
examined how prompt specificity affected the quality of its recommendations. 
Four prompts varying in detail were each submitted four times, and the generated 
recommendations were analyzed for consistency with guidelines, along with an 
assessment of source relevance and accuracy. ChatGPT consistently recommended 
securing the airway and a breathing check (100% of responses) and always 
suggested benzodiazepines as first-line treatment. However, it rarely 
recommended key measures such as side positioning (25%) to prevent potential 
aspiration and neurological assessments (0-25%). Likewise, alternative 
administration routes for benzodiazepines were mentioned inconsistently 
(0-100%). While second- and third-line antiseizure medications were suggested 
consistently, proper dosage guidance was lacking when unprompted. EEG monitoring 
was recommended in 50-100%. More specific and detailed prompts increased 
guideline adherence most markedly regarding vital sign assessment 
(from 33 to 91%), correct dosages of second-line (from 50 to 100%), third-line 
drugs (from 0 to 100%), and screening for complications (from 0 to 100%). The 
findings underscore that Chat-GPT shows promise as a clinical support tool but 
requires structured prompts for accuracy and should not replace clinical 
judgment or professional oversight.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-16902-9
PMCID: PMC12394600
PMID: 40877417 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: Simon Amacher 
is supported by the Mach-Gaensslen Foundation Switzerland and the Nora van 
Meeuwen-Haefliger Foundation of the University of Basel, Switzerland. Sabina 
Hunziker is supported by the Swiss National Foundation (SNF) (Ref 
10001C_192850/1 and 10531C_182422), the Gottfried Julia Bangerter-Rhyner 
Foundation (8472/HEG-DSV), and the Swiss Society of General Internal Medicine 
(SSGIM). Raoul Sutter received research grants from the Swiss National 
Foundation (No 320030_169379), the Research Fund of the University Basel, the 
Scientific Society Basel, and the Gottfried Julia Bangerter- Rhyner Foundation. 
He received personal grants from UCB-pharma and holds stocks from Alcon, Johnson 
& Johnson, Novartis, Roche, and Sandoz. The remaining authors have no 
disclosures. Ethical approval: Since all prompts were entirely fictional and the 
only person interacting with ChatGPT-3.5 was PSCK, no ethical approval was 
deemed necessary. With no participants included in the study, after consulting 
with regulatory affairs of the Department of Clinical Research of the University 
of Basel, a study registration was not pursued.


263. JMIR Med Educ. 2025 Dec 14. doi: 10.2196/79939. Online ahead of print.

Application of AIGC in medical education: a systematic review of the impact on 
critical thinking abilities of medical students.

Li J(1), Ai F(2), Huang J(3), Wang J(1), Cheng B(1), Li Y(1), Chen Z(4).

Author information:
(1)(1.Department of Emergency Medicine,The Central Hospital of Wuhan, Tongji 
Medical College,Huazhong University of Science and Technology;2.School of 
Medicine,Jiang han University）, Wuhan, CN.
(2)(1.Department of Emergency Medicine,The Central Hospital of Wuhan, Tongji 
Medical College,Huazhong University of Science and Technology）, Wuhan, CN.
(3)(1.Department of Teaching Affairs Office,The Central Hospital of Wuhan, 
Tongji Medical College,Huazhong University of Science and Technology）, Wuhan, 
CN.
(4)(1.Department of Emergency Medicine,The Central Hospital of Wuhan, Tongji 
Medical College,Huazhong University of Science and Technology）, The Central 
Hospital of Wuhan, Wuhan, CN.

BACKGROUND: With the rapid development of artificial intelligence technology, 
generative artificial intelligence content (AIGC) is increasingly widely applied 
in the field of medical education. Large language models (LLMs), such as 
ChatGPT, are a prominent type of AIGC technology. Critical thinking is a core 
ability in medical education, but the impact of AIGC technology on the critical 
thinking ability of medical students remains unclear. Medical students are at a 
crucial stage in cultivating critical thinking, and the intervention of AIGC 
technology may have a profound impact on this process.
OBJECTIVE: This study aims to systematically review the impact of AIGC 
technology on the complex mechanisms affecting medical students' critical 
thinking abilities and to build a corresponding strategic framework. The 
findings are intended to provide theoretical support and practical guidance for 
applying AIGC in medical education.
METHODS: This study followed 2020 PRISMA guidelines, retrieval scope limited to 
November 2022 to June 2025 published in the English literature. Through the 
PubMed database, combined with the search methods of subject terms and free 
words, relevant studies involving the impact of AIGC on the critical thinking of 
medical students were screened out around keywords such as "AIGC", "medical 
students", and "critical thinking". Two independent reviewers screened and 
evaluated the literature, and ultimately conducted qualitative analysis based on 
the common themes extracted from the literature.
RESULTS: AIGC technology in medical education is two-fold. On the one hand, 
AIGC's powerful information capabilities provide abundant learning resources and 
efficient tools. This accelerates knowledge acquisition and broadens learning 
scope. On the other hand, over-reliance on AIGC may lead to mental inertia, 
weaken critical thinking skills, and cause academic integrity issues among 
students.Research has found that strategies such as customized AIGC tools, 
virtual standardized patients, new models of resource integration, and proactive 
assessment of AI limitations can effectively make up for the deficiencies of 
AIGC in cultivating high-level critical thinking, helping medical students 
maintain and enhance their critical thinking and problem-solving abilities.
CONCLUSIONS: AIGC technology application in the medical education needs to 
carefully weigh the pros and cons. By optimizing the design and usage of AIGC 
tools and combining them with the guidance and supervision of educators, they 
can be transformed into powerful tools for promoting the development of critical 
thinking among medical students. Future research should further expand the scope 
of study, optimize research methods, pay attention to individual differences, 
track long-term effects, and deeply explore the influence of ethical and 
cultural factors to more comprehensively assess the application potential and 
challenges of AIGC technology in medical education.

DOI: 10.2196/79939
PMID: 41431843


264. J Am Med Inform Assoc. 2025 Oct 1;32(10):1526-1532. doi: 10.1093/jamia/ocaf109.

A DeepSeek-powered locally deployed closed-loop system for enhancing quality 
control in electronic nursing documentation: development and clinical 
validation.

Lv J(1), Xu Y(2), Jiang M(1), Lv Y(3), Sun J(1), Lu J(2), Wang L(4), Wang H(1).

Author information:
(1)Department of Nursing, First People's Hospital of Xinxiang and The Fifth 
Affiliated Hospital of Henan Medical University, Xinxiang 453003, China.
(2)Network and Information Center, The First Affiliated Hospital of Henan 
Medical University, Xinxiang 453003, China.
(3)Department of Pathology, Henan Medical University, Xinxiang 453003, China.
(4)Department of Nursing, Henan Medical University, Xinxiang 453003, China.

OBJECTIVES: To develop a locally deployed DeepSeek-powered closed-loop system 
for electronic nursing documentation quality control (QC) and evaluate its 
clinical efficacy through a multidimensional validation framework.
MATERIALS AND METHODS: We implemented a three-dimensional (3D) QC framework 
(real-time, final, and vertical QC). A retrospective analysis of 556 electronic 
nursing records was conducted to evaluate pre- and postimplementation outcomes, 
with documentation accuracy and audit efficiency assessed via blinded nurse 
evaluations.
RESULTS: After implementation, omission rates decreased from 7.19% to 1.79%, the 
prevalence of logical inconsistencies decreased from 9.35% to 0.72%, and the 
prevalence of timeliness errors decreased from 8.63% to 0%. The QC time per 
record decreased by 3.2-fold. Nurse satisfaction was evaluated using the 
Clinical Nursing Information System Effectiveness Evaluation Scale (Zhao Y, Gu 
Y, Zhang X, et al. Developed the clinical nursing information system 
effectiveness evaluation scale based on the new D&M model and conducted 
reliability and validity evaluation. Chin J Prae Nurs. 2020;36:544-550. 
https://doi.org/10.3760/cma.j.issn.1672-7088.2020.07.013), yielding a total 
score of 102.73 ± 3.25 out of a maximum 115 points.
DISCUSSION: This study demonstrates that the Artificial Intelligence 
(AI)-powered closed-loop QC system significantly enhances documentation accuracy 
and workflow efficiency while ensuring data security. The 3D framework 
(real-time, final, and vertical QC) represents a paradigm shift from reactive to 
proactive quality governance in nursing practice. High nurse satisfaction 
(102.73/115) confirms clinical viability, offering a scalable model for 
intelligent health-care quality ecosystems. Future work should explore federated 
learning for multicenter deployment and regulatory frameworks for clinical AI.
CONCLUSION: DeepSeek demonstrated robust efficacy in enhancing QC accuracy and 
workflow efficiency, with localized deployment ensuring data security. This 
system redefines nursing documentation management, heralding an era of 
"intelligent negative feedback" in health-care quality ecosystems.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf109
PMCID: PMC12451932
PMID: 40668938 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no conflicts of interest to 
disclose.


265. BMC Med Educ. 2025 Oct 9;25(1):1378. doi: 10.1186/s12909-025-07926-w.

Effects of artificial intelligence based physiotherapy educational approach in 
developing clinical reasoning skills: a randomized controlled trial.

Ergezen Sahin G(1), Aras Bayram G(2), Sanchez Sierra A(3)(4), Akdemir S(2), Kurc 
D(2), Tarakci D(5), Tunali AN(2).

Author information:
(1)Faculty of Health Sciences, Physiotherapy and Rehabilitation, Istanbul 
Medipol University, Beykoz, Istanbul, 34810, Türkiye. gergezen@medipol.edu.tr.
(2)Faculty of Health Sciences, Physiotherapy and Rehabilitation, Istanbul 
Medipol University, Beykoz, Istanbul, 34810, Türkiye.
(3)Grupo de Investigación en Fisioterapia Toledo (GIFTO), Facultad de 
Fisioterapia y Enfermería, Universidad de Castilla-La Mancha, Toledo, Spain.
(4)Grupo de Investigación en Fisioterapia Toledo (GIFTO), Instituto de 
Investigación Sanitaria de Castilla-La Mancha, Toledo, Spain.
(5)Faculty of Health Sciences, Ergotherapy, Istanbul Medipol University , 
Istanbul, Türkiye.

BACKGROUND: Artificial intelligence (AI) tools such as ChatGPT are increasingly 
being integrated into health professions education, but evidence regarding their 
application in physiotherapy remains limited. This study aims to investigate the 
impact of AI-assisted problem-based learning (AI-PBL) on theoretical knowledge, 
clinical competence, AI self-efficacy, internet addiction, and reading 
motivation compared with traditional PBL.
METHODS: A randomized controlled trial was conducted with undergraduate 
physiotherapy students assigned to AI-PBL or PBL groups. Participants completed 
assessments before, immediately after, and two weeks after the group 
intervention. Outcome measures included a theoretical knowledge test, the Mini 
Clinical Evaluation Exercise (Mini-CEX), the AI ​​Self-Efficacy Scale (AI-SES), 
the Internet Addiction Test (IAT), and the Adult Reading Motivation Scale 
(ARMS).
RESULTS: Forty students were randomized equally into two groups: AI-PBL (n = 20) 
and traditional PBL (n = 20). Both groups showed significant improvements in 
knowledge and reading motivation. The AI-PBL group showed significantly greater 
improvement in knowledge retention at 2 weeks (Cohen's d = 3.14) and greater 
gains in AI self-efficacy. Although Mini-CEX scores were higher in the AI-PBL 
group, the differences between groups were not statistically significant. No 
significant increase in internet addiction was observed in the AI-PBL group.
CONCLUSION: These findings emphasize that supervised, structured use of 
generative AI in education can enhance sustained learning and digital 
self-efficacy without posing behavioral risks. The AI-PBL approach appears to 
foster active reflection, self-directed learning, and deeper academic engagement 
offering a promising direction for digital innovation in physiotherapy 
education. Future studies should explore long-term outcomes, track behavioral 
engagement, and further validate the benefits of AI-enhanced instructional 
strategies.
TRIAL REGISTRATION: Prior to the initiation of the study, the protocol was 
registered on https//www.
CLINICALTRIALS: gov/, and registration status was made publicly available 
(Identifier NCT07010991 Date 08.06.2025). ( 
https://clinicaltrials.gov/study/NCT07010991?term=NCT07010991&rank=1 ).

© 2025. The Author(s).

DOI: 10.1186/s12909-025-07926-w
PMCID: PMC12512892
PMID: 41068907 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The study was conducted in accordance with the Declaration of 
Helsinki, and approved by the Institutional Ethics Committee of Istanbul Medipol 
University (Decision no:536, E-10840098-202.3.02-2975 on 08.05.2025). Informed 
consent was obtained from all subjects involved in the study. Consent for 
publication: Not applicable. Competing interests: The authors declare no 
competing interests.


266. Digit Health. 2025 Oct 30;11:20552076251393385. doi: 10.1177/20552076251393385. 
eCollection 2025 Jan-Dec.

A comparative study on the application of large language models: Deepseek-R1, 
GPT-4o, and Claude-Sonnet-4 in post-cardiac surgery rehabilitation-A 
cross-sectional study.

Li W(1), Li Q(1).

Author information:
(1)Department of Clinical Nursing Education, The Second Affiliated Hospital of 
Harbin Medical University, Harbin, China.

OBJECTIVE: This study aimed to systematically evaluate the performance of three 
advanced Chinese large language models (LLMs)-DeepSeek-R1, GPT-4o, and 
Claude-Sonnet-4-in supporting patient education during post-cardiac surgery 
rehabilitation.
METHODS: A total of 35 patient-centered questions were developed based on 
clinical guidelines, covering five core domains: postoperative care, medication 
and diet, mental health, complication prevention, and physical activity. Each 
model was prompted with the same questions five times. Responses were 
independently assessed by clinical experts for accuracy, completeness, 
readability (using FRE and FKGL), and reproducibility under repeated prompting. 
Statistical analyses were conducted using analysis of variance and post-hoc 
least significant difference (LSD) tests.
RESULTS: DeepSeek-R1 demonstrated the highest overall performance in terms of 
accuracy (mean score: 4.64) and completeness (4.20), and achieved the highest 
response stability (85.7%). GPT-4o outperformed the others in readability (FRE: 
53.19) and linguistic fluency but showed lower reproducibility (62.9%). 
Claude-Sonnet-4 showed moderate and variable performance, with limitations in 
clinical detail. All observed differences were statistically significant 
(P < 0.05).
CONCLUSION: DeepSeek-R1 is most suitable for structured, guideline-based 
rehabilitation education. GPT-4o may be preferable in emotionally supportive or 
patient-facing scenarios due to its superior readability. Claude-Sonnet-4, while 
less consistent, may offer stylistic balance in diverse communication settings. 
Overall, LLMs show promising potential in digital cardiac rehabilitation, but 
task-specific model alignment remains essential.

© The Author(s) 2025.

DOI: 10.1177/20552076251393385
PMCID: PMC12576215
PMID: 41181570

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


267. Aesthetic Plast Surg. 2026 Jan 13. doi: 10.1007/s00266-026-05641-5. Online ahead 
of print.

Clarifying the Path Toward Safe and Transparent Generative AI-Guided Patient 
Selection.

Ray PP(1).

Author information:
(1)Department of Computer Applications, Sikkim University, Gangtok, Sikkim, 
India. ppray@cus.ac.in.

This correspondence responds to the recent commentary on my article proposing a 
transparent, hybrid generative AI framework for patient selection in cosmetic 
surgery. The commentary rightly emphasizes the importance of explicit task 
specification, external and temporal validation, and clear threshold-to-action 
mapping to ensure safe and clinically meaningful deployment. I elaborate on how 
reasoning-capable large language models, specialty medical models, and 
retrieval-augmented generation pipelines can produce auditable, 
guideline-anchored suitability assessments, while acknowledging the need for 
stronger calibration, stratified reporting, and workflow-linked decision 
pathways. I also affirm the necessity of regulatory rigor, independent 
validation, privacy safeguards, and bias monitoring as prerequisites for 
real-world adoption. This exchange highlights a shared commitment to developing 
calibrated, ethical, and clinically respectful AI systems that enhance surgical 
judgment, protect patients, and support proportionate, evidence-aligned care in 
aesthetic practice.Level of Evidence V This journal requires that authors assign 
a level of evidence to each article. For a full description of these 
Evidence-Based Medicine ratings, please refer to the Table of Contents or the 
online Instructions to Authors   www.springer.com/00266 .

© 2026. Springer Science+Business Media, LLC, part of Springer Nature and 
International Society of Aesthetic Plastic Surgery.

DOI: 10.1007/s00266-026-05641-5
PMID: 41530559

Conflict of interest statement: Declarations. Conflict of interest: The author 
declares that he has no conflicts of interest to disclose. Ethical Approval: 
Ethical approval is not applicable as the study does not involve human 
participants or sensitive data collection. Human or Animal Rights: This article 
does not contain any studies with human participants or animals performed by any 
of the authors. Informed Consent: For this type of study, informed consent is 
not required.


268. Front Digit Health. 2025 Aug 26;7:1655154. doi: 10.3389/fdgth.2025.1655154. 
eCollection 2025.

Artificial intelligence and health empowerment in rural communities and 
landslide- or avalanche-isolated contexts: real case at a fictitious location.

Krumsvik RJ(1), Slettvoll V(2).

Author information:
(1)Faculty of Psychology, University of Bergen, Bergen, Norway.
(2)Faculty of Medicine, University of Bergen, Bergen, Norway.

Through a series of case studies, we have pretested the capabilities and 
reliability of the Large Language Models (LLM), Generative Pre-trained 
Transformer 4 (GPT-4) and OpenAI o3 reasoning model (o3) in educational and 
healthcare contexts. Based on this knowledge, we took a step further by testing 
these technologies in an authentic patient case set in a fictitious location. 
The context for this brief case report relates to the fact that, in the first 
quarter of 2025, fewer patients lacked an assigned GP compared to previous 
years-a positive trend. However, this offers little relief to those cut off from 
GP care due to their rural location or because of landslides and extreme 
weather. This case highlights the need for knowledge-based preparedness and 
alternative health empowerment pathways in rural Norway. This brief case report 
describes a single 16-year-old boy (N = 1) with no significant past medical 
history or chronic conditions. Although he lived in an urban area, we reframed 
the encounter as a simulated rural, avalanche-isolated scenario to test the 
feasibility of AI-supported care under extreme access constraints. Specifically, 
the case models how a patient in an avalanche-prone mountain valley-where 
seasonal road closures routinely sever access to healthcare facilities-could 
receive rapid, guideline-concordant treatment for severe tonsillitis during a 
period of general-practitioner (GP) unavailability. Repeated attempts to secure 
a same-day appointment were thwarted by workforce shortages and impassable 
roads, resulting in the earliest available appointment being five days away. The 
family leveraged point-of-care technologies (fingerstick C-reactive protein 
analysis, wearable sensors, blood pressure device, digital fever device, mobile 
ECG) and an o3 language model[1] to evaluate disease severity. A peak CRP of 
130 mg/L, combined with otherwise stable vital signs, prompted a remote 
consultation with a trusted physician in their social network, who confirmed the 
diagnosis of bacterial tonsillitis and initiated treatment with 
phenoxymethylpenicillin (Apocillin). Within 72 h, CRP fell to 23 mg/L and 
symptoms were resolved. The patient case and the events described in this pilot 
study are authentic, but the location is fictitious. The waiting time to see a 
general practitioner was five days in both the actual urban setting and the 
simulated rural scenario; however, unlike in urban contexts-where patients can 
often access immediate care through emergency clinics or private GPs-such 
options are typically unavailable in sparsely populated rural areas. This case 
illustrates how AI and health technology can serve as a "virtual waiting room" 
for individuals in rural or landslide- and avalanche-isolated areas, especially 
when GP access is limited and the condition is low-risk, such as mild sore 
throat symptoms. The case illustrates how inexpensive diagnostics and 
AI-supported reasoning can strengthen health empowerment and temporarily bridge 
care gaps for residents of geographically isolated Norwegian 
communities-provided that human clinical oversight and robust digital health 
governance remain in place. Therefore, all LLM recommendations and technology 
support were reviewed during an in-person physician examination in a family 
network, and the final antibiotic prescription came from the clinician, 
underscoring that AI functioned solely as decision support rather than 
autonomous care.

© 2025 Krumsvik and Slettvoll.

DOI: 10.3389/fdgth.2025.1655154
PMCID: PMC12417106
PMID: 40933813

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


269. JMIR Med Inform. 2025 Aug 13;13:e65365. doi: 10.2196/65365.

Assessing the Role of Large Language Models Between ChatGPT and DeepSeek in 
Asthma Education for Bilingual Individuals: Comparative Study.

Liu Y(#)(1)(2)(3), Yu F(#)(1), Zhang X(1), Tong X(2), Li K(2), Gu W(3), Yu B(1).

Author information:
(1)Department of Respiratory and Critical Care Medicine, Second Affiliated 
Hospital of Harbin Medical University, 157 Baojian Road, Nangang District, 
Harbin, 150081, China, +86 138 3612 4743.
(2)Department of Microbiology, Immunology and Biochemistry, University of 
Tennessee Health Science Center, Memphis, TN, United States.
(3)Department of Orthopaedic Surgery and Biomedical Engineering, University of 
Tennessee Health Science Center, Memphis, TN, United States.
(#)Contributed equally

BACKGROUND: Asthma is a chronic inflammatory airway disease requiring long-term 
management. Artificial intelligence (AI)-driven tools such as large language 
models (LLMs) hold potential for enhancing patient education, especially for 
multilingual populations. However, comparative assessments of LLMs in 
disease-specific, bilingual health communication are limited.
OBJECTIVE: This study aimed to evaluate and compare the performance of two 
advanced LLMs-ChatGPT-4o (OpenAI) and DeepSeek-v3 (DeepSeek AI)-in providing 
bilingual (English and Chinese) education for patients with asthma, focusing on 
accuracy, completeness, clinical relevance, and language adaptability.
METHODS: A total of 53 asthma-related questions were collected from real patient 
inquiries across 8 clinical domains. Each question was posed in both English and 
Chinese to ChatGPT-4o and DeepSeek-v3. Responses were evaluated using a 7D 
clinical quality framework (eg, completeness, consensus consistency, and 
reasoning ability) adapted from Google Health. Three respiratory clinicians 
performed blinded scoring evaluations. Descriptive statistics and Wilcoxon 
signed-rank tests were applied to compare performance across domains and against 
theoretical maximums.
RESULTS: Both models demonstrated high overall quality in generating bilingual 
educational content. DeepSeek-v3 outperformed ChatGPT-4o in completeness and 
currency, particularly in treatment-related knowledge and symptom 
interpretation. ChatGPT-4o showed advantages in clarity and accessibility. In 
English responses, ChatGPT achieved perfect scores across 5 domains, but scored 
lower in clinical features (mean 3.78, SD 0.16; P=.02), treatment (mean 3.90, SD 
0.05; P=.03), and differential diagnosis (mean 3.83, SD 0.29; P=.08).
CONCLUSIONS: ChatGPT-4o and DeepSeek-v3 each offer distinct strengths for 
bilingual asthma education. While ChatGPT is more suitable for general health 
education due to its expressive clarity, DeepSeek provides more up-to-date and 
comprehensive clinical content. Both models can serve as effective supplementary 
tools for patient self-management but cannot replace professional medical 
advice. Future AI health care systems should enhance clinical reasoning, ensure 
guideline currency, and integrate human oversight to optimize safety and 
accuracy.

©Yaxin Liu, Fangfei Yu, Xiaofei Zhang, Xiaohan Tong, Kui Li, Weikuan Gu, Baiquan 
Yu. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org).

DOI: 10.2196/65365
PMCID: PMC12349887
PMID: 40802989 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


270. Obes Surg. 2025 Feb;35(2):645-650. doi: 10.1007/s11695-024-07666-8. Epub 2025 
Jan 17.

Evaluating the Feasibility of ChatGPT-4 as a Knowledge Resource in Bariatric 
Surgery: A Preliminary Assessment.

Leng Y(1)(2), Yang Y(1)(2), Liu J(1)(2), Jiang J(1)(2), Zhou C(3).

Author information:
(1)Department of Anesthesiology, West China Hospital, Sichuan University, 
Chengdu, China.
(2)Research Center of Anesthesiology, National-Local Joint Engineering Research 
Centre of Translational Medicine of Anesthesiology, West China Hospital, Sichuan 
University, Chengdu, China.
(3)Research Center of Anesthesiology, National-Local Joint Engineering Research 
Centre of Translational Medicine of Anesthesiology, West China Hospital, Sichuan 
University, Chengdu, China. zhouc@163.com.

This study evaluates the feasibility of ChatGPT-4 as a knowledge resource in 
bariatric surgery. Using a problem set of 30 questions covering key aspects of 
bariatric care, responses were reviewed by three bariatric surgery experts. 
ChatGPT-4 achieved strong performance, with 50% of responses scoring the highest 
possible rating for alignment with clinical guidelines. However, limitations 
were noted, including outdated criteria, lack of specificity, and occasional 
poor response structuring. The study highlights the potential of ChatGPT-4 as a 
supplementary tool for patient education and healthcare provider support, as 
well as its broader public health applications, such as obesity prevention and 
healthy lifestyle education. Despite its promise, challenges such as handling 
complex clinical cases, reliance on up-to-date evidence, and ethical concerns 
like privacy and misinformation must be addressed. Future research should refine 
the model's applications and explore its integration into clinical practice and 
public health strategies.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s11695-024-07666-8
PMID: 39821906 [Indexed for MEDLINE]


271. BMC Neurol. 2025 Nov 27;25(1):489. doi: 10.1186/s12883-025-04473-1.

Validation of an AI-Based platform for structured diagnosis of headache 
disorders using ICHD-3 criteria.

de Andrade JBC(1)(2)(3)(4), da Silva Costa TB(5), Vasconcelos JL(6), Lopes 
TLM(6), Balsells MD(6), Cristofolini VL(7), Querobin SO(7), Filho FMR(8).

Author information:
(1)Hospital Israelita Albert Einstein, São Paulo, Brazil. 
joao.brainer@unifesp.br.
(2)Department of Health Informatics, Universidade Federal de São Paulo, São 
Paulo, Brazil. joao.brainer@unifesp.br.
(3)Department of Neurology, Universidade Federal de São Paulo, Rua Botucatu 720, 
São Paulo, Brazil. joao.brainer@unifesp.br.
(4)Aeronautics Institute of Technology, São José dos Campos, Brazil. 
joao.brainer@unifesp.br.
(5)Department of Health Informatics, Universidade Federal de São Paulo, São 
Paulo, Brazil.
(6)Universidade Estadual do Ceara, Fortaleza, Brazil.
(7)Centro Universitario São Camilo, School of Medicine, São Paulo, Brazil.
(8)Department of Neurology, Universidade Federal de São Paulo, Rua Botucatu 720, 
São Paulo, Brazil.

BACKGROUND: The diagnosis of headache disorders remains a clinical challenge, 
particularly for non-specialists, due to the complexity of the International 
Classification of Headache Disorders, 3rd edition (ICHD-3), and the absence of 
biomarkers. Large language models (LLMs) represent a promising tool to support 
accurate and scalable diagnostic classification, especially in resource-limited 
settings.
OBJECTIVE: To validate the performance of a free, multilingual clinical decision 
support platform-Head.AI-designed to classify headache cases using GPT-4o and a 
structured implementation of ICHD-3.
METHODS: We conducted an independent validation using 315 expert-generated 
vignettes representing 215 ICHD-3 diagnoses, input into Head.AI and three other 
platforms (Claude Sonnet 4.0, Grok 3.0, and Gemini 2.5). Outcomes included 
diagnostic accuracy (rank of correct diagnosis), calibration, and citation rate.
RESULTS: The algorithm correctly identified the top diagnosis in 89.5% of cases 
(vs. 74-80% in comparators), with a citation rate >97% and calibration (Brier 
score 0.153). It maintained consistent performance across primary and secondary 
headaches and achieved first-hypothesis accuracy >74% in difficult cases. 
Logistic regression confirmed Head.AI had significantly higher odds of correct 
classification (ORs vs. comparators: 2.04-2.86; all p < 0.01).
CONCLUSION: Our algorithm demonstrated high diagnostic accuracy across a broad 
spectrum of headache disorders, exceeding the performance reported in prior 
studies, though direct comparison should be interpreted with caution due to 
methodological differences. Its public availability, structured knowledge base, 
and educational potential make it a valuable contribution to AI-assisted 
headache care. The platform is freely accessible at www.head-ai.com.br .

© 2025. The Author(s).

DOI: 10.1186/s12883-025-04473-1
PMCID: PMC12661688
PMID: 41310513 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. IRB approval waiver. Consent for publication: Not 
applicable. Competing interests: The authors declare no competing interests.


272. Int J Med Inform. 2025 Nov;203:106013. doi: 10.1016/j.ijmedinf.2025.106013. Epub 
2025 Jun 12.

Clinical feasibility of AI Doctors: Evaluating the replacement potential of 
large language models in outpatient settings for central nervous system tumors.

Pan Y(1), Tian S(2), Guo J(3), Cai H(4), Wan J(5), Fang C(6).

Author information:
(1)The School of Big Data and Artificial Intelligence, Anhui Xinhua University, 
Hefei, China.
(2)Department of Neurosurgery, National Cancer Center/National Clinical Research 
Center for Cancer/Cancer Hospital, Chinese Academy of Medical Sciences and 
Peking Union Medical College, Beijing, China.
(3)State Key Laboratory of Molecular Oncology, National Cancer Center/National 
Clinical Research Center for Cancer/Cancer Hospital, Chinese Academy of Medical 
Sciences and Peking Union Medical College, Beijing, China. Electronic address: 
gj102066@163.com.
(4)Department of Neurosurgery, National Cancer Center/National Clinical Research 
Center for Cancer/Cancer Hospital, Chinese Academy of Medical Sciences and 
Peking Union Medical College, Beijing, China. Electronic address: 
phonecy@126.com.
(5)Department of Neurosurgery, National Cancer Center/National Clinical Research 
Center for Cancer/Cancer Hospital, Chinese Academy of Medical Sciences and 
Peking Union Medical College, Beijing, China. Electronic address: 
wanjinghai@sina.com.
(6)Department of Neurosurgery, National Cancer Center/National Clinical Research 
Center for Cancer/Cancer Hospital, Chinese Academy of Medical Sciences and 
Peking Union Medical College, Beijing, China. Electronic address: 
fchonest@163.com.

BACKGROUND AND OBJECTIVES: The treatment of central nervous system (CNS) tumors 
is complex and resource-intensive, with higher mortality in underserved regions. 
Large language models (LLMs) show promise in medical support, but their 
real-world performance in CNS tumor outpatient care remains unclear. This study 
aims to assess the diagnostic and treatment capabilities of LLMs in bilingual 
clinical settings.
METHODS: This retrospective study evaluated three LLMs (ChatGPT-4o, DeepSeek-R1, 
and Doubao) in assisting neuro-oncology outpatient decision-making within 
bilingual (Chinese/English) clinical environments. A total of 338 outpatient 
cases were included, with each model assigned three clinical tasks: differential 
diagnosis, main diagnosis, and treatment advice. Model outputs were compared 
against assessments by experienced neurosurgeons. Statistical analysis employed 
McNemar tests (P < 0.05).
RESULTS: ChatGPT-4o and DeepSeek-R1 achieved over 90 % accuracy in differential 
diagnosis, showing no significant difference compared to doctors (P > 0.05), 
while Doubao performed significantly worse (Chinese: P = 0.02, English: P = 
0.01). In main diagnosis, both ChatGPT-4o and DeepSeek-R1 showed no significant 
deviation from doctors performance (P > 0.05), whereas Doubao underperformed 
(Chinese: P = 0.019, English: P = 0.011). For treatment recommendations, all 
models showed reduced accuracy (ChatGPT-4o: 80.5 %; DeepSeek-R1: 79 %; Doubao: 
71.3 %), significantly lower than doctors (Whether in Chinese or English: P < 
0.05). No performance difference was observed between Chinese and English cases.
CONCLUSION: LLMs show strong potential in the preliminary diagnosis and decision 
support for CNS tumors, and their cross-lingual adaptability underscores their 
clinical feasibility.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106013
PMID: 40554367 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


273. Ann Fam Med. 2025 Sep 22;23(5):389-398. doi: 10.1370/afm.250386.

Reporting Guideline for Chatbot Health Advice Studies: Chatbot Assessment 
Reporting Tool (CHART) Statement.

CHART Collaborative.

The Chatbot Assessment Reporting Tool (CHART) is a reporting guideline developed 
to provide reporting recommendations for studies evaluating the performance of 
chatbots driven by generative artificial intelligence when summarizing clinical 
evidence and providing health advice, referred to as chatbot health advice 
studies. CHART was developed in several phases after performing a comprehensive 
systematic review to identify variation in the conduct, reporting, and method in 
chatbot health advice studies. Findings from the review were used to develop a 
draft checklist that was revised through an international, multidisciplinary, 
modified, asynchronous Delphi consensus process of 531 stakeholders, 3 
synchronous panel consensus meetings of 48 stakeholders, and subsequent pilot 
testing of the checklist. CHART includes 12 items and 39 subitems to promote 
transparent and comprehensive reporting of chatbot health advice studies. These 
include title (subitem 1a), abstract/summary (subitem 1b), background (subitems 
2a,b), model identifiers (subitems 3a,b), model details (subitems 4a-c), prompt 
engineering (subitems 5a,b), query strategy (subitems 6a-d), performance 
evaluation (subitems 7a,b), sample size (subitem 8), data analysis (subitem 9a), 
results (subitems 10a-c), discussion (subitems 11a-c), disclosures (subitem 
12a), funding (subitem 12b), ethics (subitem 12c), protocol (subitem 12d), and 
data availability (subitem 12e). The CHART checklist and corresponding diagram 
of the method were designed to support key stakeholders including clinicians, 
researchers, editors, peer reviewers, and readers in reporting, understanding, 
and interpreting the findings of chatbot health advice studies. KEY MESSAGES: 
CHART was developed by performing a systematic review, Delphi consensus of 531 
international stakeholders, and several consensus meetings among an expert panel 
comprised of 48 membersThe CHART statement outlines 12 key reporting items for 
chatbot health advice studies in the form of a checklist and methodological 
diagramAll stakeholders including clinicians, researchers, and journal editors 
should encourage the transparent reporting of chatbot health advice studies.

© 2025 Annals of Family Medicine, Inc.

DOI: 10.1370/afm.250386
PMCID: PMC12459699
PMID: 40750305 [Indexed for MEDLINE]


274. Cancer Lett. 2025 Jun 28;620:217632. doi: 10.1016/j.canlet.2025.217632. Epub 
2025 Mar 15.

Generating research hypotheses to overcome key challenges in the early diagnosis 
of colorectal cancer - Future application of AI.

Yao L(1), Yin H(2), Yang C(3), Han S(4), Ma J(5), Graff JC(6), Wang CY(7), Jiao 
Y(8), Ji J(9), Gu W(10), Wang G(11).

Author information:
(1)College of Health Management, Harbin Medical University, 157 Baojian Road, 
Harbin, Heilongjiang, 150081, China; Department of Orthopedic Surgery and 
BME-Campbell Clinic, University of Tennessee Health Science Centre, Memphis, TN, 
38163, USA. Electronic address: lyao5@uthsc.edu.
(2)Department of Orthopedic Surgery and BME-Campbell Clinic, University of 
Tennessee Health Science Centre, Memphis, TN, 38163, USA; Centre of Integrative 
Research, The First Hospital of Qiqihar City, Qiqihar, Heilongjiang, 161005, 
China. Electronic address: yinheliang999@163.com.
(3)Department of Orthopedic Surgery and BME-Campbell Clinic, University of 
Tennessee Health Science Centre, Memphis, TN, 38163, USA. Electronic address: 
cyang31@uthsc.edu.
(4)Department of Integration of Chinese and Western Medicine, Key Laboratory of 
Carcinogenesis and Translational Research (Ministry of Education), Peking 
University Cancer Hospital and Institute, Beijing, 100142, China. Electronic 
address: shuyanhan@bjmu.edu.cn.
(5)Department of Breast Surgery, The Second Affiliated Hospital of Zhengzhou 
University, Zhengzhou, Henan Province, 450000, China. Electronic address: 
mjxm1992@gmail.com.
(6)College of Nursing, University of Tennessee Health Science Center, Memphis, 
TN, 38163, USA. Electronic address: jgraff@uthsc.edu.
(7)The Center for Biomedical Research, Dept of Respiratory and Critical Care 
Medicine, National Health Commission (NHC) Key Laboratory of Respiratory 
Diseases, Tongji Hospital, Tongji Medical College, Huazhong University of 
Science and Technology, Wuhan, China. Electronic address: 
wangcy@tjh.tjmu.edu.cn.
(8)Department of Orthopedic Surgery and BME-Campbell Clinic, University of 
Tennessee Health Science Centre, Memphis, TN, 38163, USA.
(9)State Key Laboratory of Molecular Oncology, Beijing Key Laboratory of 
Carcinogenesis and Translational Research, Gastrointestinal Center of Peking 
University Cancer Hospital, Beijing, 100142, China. Electronic address: 
jijiafu@hsc.pku.edu.cn.
(10)Department of Orthopedic Surgery and BME-Campbell Clinic, University of 
Tennessee Health Science Centre, Memphis, TN, 38163, USA; Lt. Col. Luke 
Weathers, Jr. VA Medical Center, 116 N Pauline St. Memphis, TN, 38105, USA; 
Department of Pharmaceutical Sciences, University of Tennessee Health Science 
Center, Memphis, TN, 38163, USA. Electronic address: wgu@uthsc.edu.
(11)Department of Oncological and Laparoscopic Surgery, The First Affiliated 
Hospital of Harbin Medical University, Harbin, China; Key Laboratory of 
Hepatosplenic Surgery, Ministry of Education, The First Affiliated Hospital of 
Harbin Medical University, Harbin, China. Electronic address: wgilu79@163.com.

We intend to explore the capability of ChatGPT 4.0 in generating innovative 
research hypotheses to address key challenges in the early diagnosis of 
colorectal cancer (CRC). We asked ChatGPT to generate hypotheses focusing on 
three main challenges: improving screening accuracy, overcoming technological 
limitations, and identifying reliable biomarkers. The hypotheses were evaluated 
for novelty. The experimental plans provided by ChatGPT for selected hypotheses 
were assessed for completion and feasibility. As a result, ChatGPT generated a 
total of 65 hypotheses. ChatGPT rated all 65 hypotheses, with 25 hypotheses 
receiving the highest rating (5) and 40 hypotheses receiving a rating of 4 or 
lower. The research team evaluated a total of 65 hypotheses, assigning them the 
following grades: hypotheses were rated as excellent (Grade 5), 16 were deemed 
suitable (Grade 4), 31 were classified as satisfactory (Grade 3), 12 were 
identified as needing Improvement (Grade 2), and one was considered poor (Grade 
1). Additionally, the study determined that 17 of the generated hypotheses had 
corresponding publications. Out of the three experimental plans assessed, one 
was rated excellent (5) for feasibility, while the others received good (4) and 
moderate (3) ratings. Predicted outcomes and alternative approaches were rated 
as good, with some areas requiring further improvement. Our data demonstrate 
that AI has the potential to revolutionize hypothesis generation in medical 
research, though further validation through experimental and clinical studies is 
needed. This study suggests that while AI can generate novel hypotheses, human 
expertise is essential for evaluating their practicality and relevance in 
scientific research.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.canlet.2025.217632
PMID: 40097064 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


275. PLoS One. 2025 Oct 22;20(10):e0333411. doi: 10.1371/journal.pone.0333411. 
eCollection 2025.

The ethical challenges in the integration of artificial intelligence and large 
language models in medical education: A scoping review.

Li X(1), Yan X(1), Lai H(2).

Author information:
(1)Hospital of Chengdu University of Traditional Chinese Medicine, Chengdu, 
China.
(2)Chengdu University of Traditional Chinese Medicine, Chengdu, China.

With the rapid development of artificial intelligence (AI), large language 
models (LLMs), such as ChatGPT have shown potential in medical education, 
offering personalized learning experiences. However, this integration raises 
ethical concerns, including privacy, autonomy, and transparency. This study 
employed a scoping review methodology, systematically searching relevant 
literature published between January 2010 and August 31, 2024, across three 
major databases: PubMed, Embase, and Web of Science. Through rigorous screening, 
50 articles which met inclusion criteria were ultimately selected from an 
initial pool of 1,192 records. During data processing, the Kimi AI tool was 
utilized to facilitate preliminary literature screening, extraction of key 
information, and construction of content frameworks. Data reliability was 
ensured through a stringent cross-verification process whereby two independent 
researchers validated all AI-generated content against original source 
materials. The study delineates ethical challenges and opportunities arising 
from the integration of AI and LLMs into medical education, identifying seven 
core ethical dimensions: privacy and data security, algorithmic bias, 
accountability attribution, fairness assurance, technological reliability, 
application dependency, and patient autonomy. Corresponding mitigation 
strategies were formulated for each challenge. Future research should prioritize 
establishing dedicated ethical frameworks and application guidelines for AI in 
medical education while maintaining sustained attention to the long-term ethical 
implications of these technologies in healthcare domains.

Copyright: © 2025 Li et al. This is an open access article distributed under the 
terms of the Creative Commons Attribution License, which permits unrestricted 
use, distribution, and reproduction in any medium, provided the original author 
and source are credited.

DOI: 10.1371/journal.pone.0333411
PMCID: PMC12543126
PMID: 41124146 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


276. J Nurs Scholarsh. 2025 Nov;57(6):1060-1067. doi: 10.1111/jnu.70048. Epub 2025 
Sep 16.

ChatGPT-4 in Nursing Research: A Methodological Evaluation of Bias Risk in 
Randomized Controlled Trials.

Tuncer M(1), Tuncer GZ(2).

Author information:
(1)Department of Nursing Fundamentals, Gümüşhane University, Gümüşhane, Turkey.
(2)Department of Psychiatric Nursing, Dokuz Eylül University Faculty of Nursing, 
Izmir, Turkey.

BACKGROUND: Conducting bias assessments in systematic reviews is a 
time-consuming process that involves subjective judgments. The use of artificial 
intelligence (AI) technologies to perform these assessments can potentially save 
time and enhance consistency. Nevertheless, the efficacy of AI technologies in 
conducting bias assessments remains inadequately explored.
AIM: This study aims to evaluate the efficacy of ChatGPT-4o in assessing bias 
using the revised Cochrane RoB2 tool, focusing on randomized controlled trials 
in nursing.
METHODS: ChatGPT-4o was provided with the RoB2 assessment guide in the form of a 
PDF document and instructed to perform bias assessments for the 80 open-access 
RCTs included in the study. The results of the bias assessments conducted by 
ChatGPT-4o for each domain were then compared with those of the meta-analysis 
authors using Cohen's weighted kappa analysis.
RESULTS: Weighted Cohen's kappa values showed better agreement in bias in the 
measurement of the outcome (D4, 0.22) and bias arising from the randomization 
process (D1, 0.20), while negative values in bias due to missing outcome data 
(D3, -0.12) and bias in the selection of the reported result (D5, -0.09) 
indicated poor agreement. The highest accuracy was observed in D5 (0.81), and 
the lowest in D1 (0.60). F1 scores were highest in bias due to deviations from 
intended interventions (D2, 0.74) and lowest in D3 (0.00) and D5 (0.00). 
Specificity was higher in D5 (0.93) and D3 (0.82), while sensitivity and 
precision were low in these domains.
CONCLUSIONS: The agreement between ChatGPT-4o and the meta-analysis studies in 
the same RCT assessments is generally low. This indicates that ChatGPT-4o 
requires substantial enhancements before it can be used as a reliable tool for 
bias risk assessments.
CLINICAL RELEVANCE: The AI-based tools have the potential to expedite bias 
assessment in systematic reviews. However, this study demonstrates that 
ChatGPT-4o, in its current form, lacks sufficient consistency, indicating that 
such tools should be integrated cautiously and used under continuous human 
oversight, particularly in evidence-based evaluations that inform clinical 
decision-making.

© 2025 Sigma Theta Tau International.

DOI: 10.1111/jnu.70048
PMID: 40958374 [Indexed for MEDLINE]


277. Aesthetic Plast Surg. 2025 Oct 22. doi: 10.1007/s00266-025-05308-7. Online ahead 
of print.

Evaluation Large Language Models' Time Dependent Consistency in Aesthetic 
Surgery Consultations and Comparison of Their Performance Across Different 
Clinical Domains.

Kendir MS(1), Zuhurlu M(2).

Author information:
(1)Department of Plastic and Reconstructive and Aesthetic Surgery, Acibadem 
University, School of Medicine, Istanbul, Turkey. selcuk_kendir@msn.com.
(2)Sakarya Training and Research Hospital, Sakarya, Turkey.

BACKGROUND: The integration of large language models (LLM) into plastic and 
aesthetic surgery has shown promise. However, research comparing different LLMs 
in handling clinical scenarios and their temporal consistency remains limited. 
This study evaluated the performances of ChatGPT-4o, Gemini 1.5 Pro and Claude 
3.5 Sonnet in aesthetic surgery scenarios. The objectives were to compare their 
overall performance, analyze reliability in complicated and uncomplicated cases, 
assess temporal consistency, evaluate performance across five clinical domains: 
preoperative cautions, postoperative care, holistic approach, algorithmic 
approach, and surgical planning.
METHODS: Twenty-four case scenarios (12 complicated, 12 uncomplicated) were 
input into the LLMs at three time points (T1, T2, T3) over two weeks. Three 
blinded board-certified plastic surgeons evaluated responses using a 5-point 
Likert scale. Statistical analyses were applied.
RESULTS: Chat GPT-4 achieved the highest mean score (4.92), outperforming Gemini 
1.5 Pro (3.62) and Claude 3.5 Sonnet (3.21) (p < 0.001). It performed 
consistently across complicated (4.87) and uncomplicated cases (4.96) (p > 0.05) 
and demonstrated temporal stability (p > 0.05). Gemini 1.5 Pro showed temporal 
consistency for complicated cases (p > 0.05), but not in uncomplicated cases. 
Claude 3.5 Sonnet exhibited significant temporal inconsistencies (p < 0.05). In 
the domain specific analyzes, GPT-4 was superior to others. Claude 3.5 Sonnet 
had the lowest scores in most domains, except algorithmic approach, where it 
outperformed Gemini (4.4 vs. 4.1, p < 0.05).
CONCLUSIONS: LLMs could be a promising tool for supporting surgical 
decision-making. Future research should aim to enhance LLM reliability and 
validate its real-world applications.
LEVEL OF EVIDENCE I: This journal requires that authors assign a level of 
evidence to each article. For a full description of these Evidence-Based 
Medicine ratings, please refer to the Table of Contents or the online 
Instructions to Authors www.springer.com/00266 .

© 2025. Springer Science+Business Media, LLC, part of Springer Nature and 
International Society of Aesthetic Plastic Surgery.

DOI: 10.1007/s00266-025-05308-7
PMID: 41125815

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare that they have no conflicts of interest to disclose. Ethical Approval: 
The study did not require ethics committee approval as it relied solely on 
fictional case scenarios and no real patient data used in the study Human and 
Animal Participants: This article does not contain any studies with human 
participants or animals performed by any of the authors. Informed Consent: For 
this type of study informed consent is not required.


278. Integr Med Res. 2025 Dec;14(4):101222. doi: 10.1016/j.imr.2025.101222. Epub 2025 
Aug 9.

Prompt engineering for generative artificial intelligence chatbots in health 
research: A practical guide for traditional, complementary, and integrative 
medicine researchers.

Ng JY(1)(2)(3)(4).

Author information:
(1)Institute of General Practice and Interprofessional Care, University Hospital 
Tübingen, Tübingen, Germany.
(2)Robert Bosch Center for Integrative Medicine and Health, Bosch Health Campus, 
Stuttgart, Germany.
(3)Department of Health Research Methods, Evidence, and Impact, Faculty of 
Health Sciences, McMaster University, Hamilton, Canada.
(4)School of Public Health, Faculty of Health, University of Technology Sydney, 
Sydney, Australia.

Generative artificial intelligence (GenAI) chatbots powered by large language 
models (LLMs) are increasingly used in health research to support a range of 
academic and clinical activities. While increasingly adopted in biomedical 
research, their application in traditional, complementary, and integrative 
medicine (TCIM) remains underexplored. TCIM presents unique challenges, 
including complex interventions, culturally embedded practices, and variable 
terminology. This article provides a practical, evidence-informed guide to help 
TCIM researchers engage responsibly with GenAI chatbots through prompt 
engineering, the design of clear, structured, and purposeful prompts to improve 
output relevance and accuracy. The guide outlines strategies to tailor GenAI 
chatbot interactions to the methodological and epistemological diversity of 
TCIM. It presents use cases across the research process, including research 
question development, study design, literature searches, selection of reporting 
guidelines and appraisal tools, quantitative and qualitative analysis, writing 
and dissemination, and implementation planning. For each stage, the guide offers 
examples and best practices while emphasizing that AI-generated content should 
always serve as a starting point, not a final product, and must be reviewed and 
verified using credible sources. Potential risks such as hallucinated outputs, 
embedded bias, and ethical challenges are discussed, particularly in culturally 
sensitive contexts. Transparency in GenAI chatbot use and researcher 
accountability are emphasized as essential principles. While GenAI chatbots can 
expand access to research support and foster innovation in TCIM, they cannot 
substitute for critical thinking, methodological rigour, or domain-specific 
expertise. Used responsibly, GenAI chatbots can augment human judgment and 
contribute meaningfully to the evolution of TCIM scholarship.

© 2025 Korea Institute of Oriental Medicine. Published by Elsevier B.V.

DOI: 10.1016/j.imr.2025.101222
PMCID: PMC12766412
PMID: 41497197


279. JMIR Form Res. 2025 Oct 29;9:e76384. doi: 10.2196/76384.

Detecting Laterality Errors in Combined Radiographic Studies by Enhancing the 
Traditional Approach With GPT-4o: Algorithm Development and Multisite Internal 
Validation.

Weng KH(1), Chou YC(1), Kuo YT(1)(2)(3), Hsieh TJ(1), Liu CF(4).

Author information:
(1)Department of Medical Imaging, Chi Mei Medical Center, Tainan, Taiwan.
(2)Institute of Precision Medicine, College of Medicine, National Sun Yat-sen 
University, Kaohsiung, Taiwan.
(3)Department of Radiology, School and College of Medicine, Kaohsiung Medical 
University, Kaohsiung, Taiwan.
(4)Department of Medical Research, Chi Mei Medical Center, Tainan, Taiwan.

BACKGROUND: Laterality errors in radiology reports can endanger patient safety. 
Effective methods for screening for laterality errors in combined radiographic 
reports, which combine multiple studies into one, remain unexplored.
OBJECTIVE: First, we define and analyze the unstudied combined radiographic 
report format and its challenges. Second, we introduce a clinically deployable 
ensemble method (rule-based+GPT-4o), evaluated on large-scale, real-world, 
imbalanced data. Third, we demonstrate significant performance gaps between 
real-world imbalanced and synthetic balanced datasets, highlighting limitations 
of the benchmarking methodology commonly used in current studies.
METHODS: This retrospective study analyzed deidentified English radiology 
reports containing laterality terms in order. We split the data into TrainVal 
(combined training and validation dataset), Test-1 (both real-world, 
imbalanced), and Test-2 (synthetic, balanced). Test-1 comes from a distinct 
branch. Experiment 1 compared the baseline, workaround, and GPT-4o-augmented 
rule-based methods. Experiment 2 compared the rule-based method with the highest 
recall to fine-tuned RoBERTa, ClinicalBERT, and GPT-4o models.
RESULTS: As of July 2024, our dataset included 10,000 real-world and 889 
synthetic radiology reports. The laterality error rate in real-world reports was 
1.20% (120/10,000), significantly higher in combined (103/7000, 1.47%) than in 
noncombined reports (17/3000, 0.57%; difference=0.90%; z=3.81; P<.001). In 
experiment 1, recall differed significantly among the 3 versions of rule-based 
methods (Q=6.0; P=.0498, Friedman test). The rule-based+GPT-4o method had the 
highest recall (average rank=1), significantly better than the baseline (average 
rank=3; P=.04, Nemenyi test). Most (5/6) of the false positives introduced by 
the GPT-4o information extraction were due to parser limitations hidden by error 
cancellation. In experiment 2, on Test-1, rule-based+GPT-4o (precision=0.696; 
recall=0.889; F1-score=0.780) outperformed GPT-4o (precision=0.219; 
recall=0.889; F1-score=0.352), ClinicalBERT (precision=0.047; recall=0.667; 
F1-score=0.088), and RoBERTa (F1-score=0.000). On Test-2, rule-based+GPT-4o 
(precision=0.996; recall=0.925; F1-score=0.959) and GPT-4o (precision=0.979; 
recall=0.953; F1-score=0.966) outperformed ClinicalBERT (precision=0.984; 
recall=0.749; F1-score=0.851) and RoBERTa (F1-score=0.013). Both ClinicalBERT 
and GPT-4o exhibited notable declines in precision on TrainVal and Test-1 
compared to Test-2. Both Test-1 data membership (GPT-4o: odds ratio [OR] 239.89, 
95% CI 111.05-518.01; P<.001; ClinicalBERT: OR 1924.07, 95% CI 687.46-5383.99; 
P<.001) and order count per study (GPT-4o: OR 1.79, 95% CI 1.38-2.31; P<.001; 
ClinicalBERT: OR 2.50, 95% CI 1.64-3.80; P<.001) independently predicted false 
positive errors in multivariate logistic regression. In subgroup analysis, all 
models showed reduced precision and F1 in combined-study subgroups.
CONCLUSIONS: The combined radiographic report format poses distinct challenges 
for both radiology report quality assurance and natural language processing. The 
combined rule-based and GPT-4o method effectively screens for laterality errors 
in imbalanced real-world reports. A significant performance gap exists between 
balanced synthetic datasets and imbalanced real-world data. Future studies 
should also include real-world imbalanced data.

©Kung-Hsun Weng, Yi-Chen Chou, Yu-Ting Kuo, Tsyh-Jyi Hsieh, Chung-Feng Liu. 
Originally published in JMIR Formative Research (https://formative.jmir.org), 
29.10.2025.

DOI: 10.2196/76384
PMCID: PMC12612642
PMID: 41161340 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


280. J Med Syst. 2025 Feb 14;49(1):23. doi: 10.1007/s10916-025-02152-9.

Evaluation of the Performance of Three Large Language Models in Clinical 
Decision Support: A Comparative Study Based on Actual Cases.

Wang X(#)(1), Ye H(#)(1), Zhang S(1), Yang M(1), Wang X(2).

Author information:
(1)Department of Critical Care Medicine, Shanghai East Hospital, Tongji 
University School of Medicine, No.150, Jimo Road, Pudong New Area, Shanghai, 
China.
(2)Department of Critical Care Medicine, Shanghai East Hospital, Tongji 
University School of Medicine, No.150, Jimo Road, Pudong New Area, Shanghai, 
China. 1400094@tongji.edu.cn.
(#)Contributed equally

BACKGROUND: Generative large language models (LLMs) are increasingly integrated 
into the medical field. However, their actual efficacy in clinical 
decision-making remains partially unexplored. This study aimed to assess the 
performance of the three LLMs, ChatGPT-4, Gemini, and Med-Go, in the domain of 
professional medicine when confronted with actual clinical cases.
METHODS: This study involved 134 clinical cases spanning nine medical 
disciplines. Each LLM was required to provide suggestions for diagnosis, 
diagnostic criteria, differential diagnosis, examination and treatment for every 
case. Responses were scored by two experts using a predefined rubric.
RESULTS: In overall performance among the models, Med-Go achieved the highest 
median score (37.5, IQR 31.9-41.5), while Gemini recorded the lowest (33.0, IQR 
25.5-36.6), showing significant statistical difference among the three LLMs 
(p < 0.001). Analysis revealed that responses related to differential diagnosis 
were the weakest, while those pertaining to treatment recommendations were the 
strongest. Med-Go displayed notable performance advantages in gastroenterology, 
nephrology, and neurology.
CONCLUSIONS: The findings show that all three LLMs achieved over 60% of the 
maximum possible score, indicating their potential applicability in clinical 
practice. However, inaccuracies that could lead to adverse decisions underscore 
the need for caution in their application. Med-Go's superior performance 
highlights the benefits of incorporating specialized medical knowledge into LLMs 
training. It is anticipated that further development and refinement of medical 
LLMs will enhance their precision and safety in clinical use.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02152-9
PMID: 39948214 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics Approval and Consent to 
Participate: Not Applicable. Competing Interests: The authors declare no 
competing interests.


281. JMIR Med Inform. 2025 Sep 8;13:e76252. doi: 10.2196/76252.

Automated Literature Screening for Hepatocellular Carcinoma Treatment Through 
Integration of 3 Large Language Models: Methodological Study.

Pan C(1), Lu W(1), Chen B(1), Zhang G(1), Yang Z(1), Hao J(1).

Author information:
(1)Department of Hepatobiliary and Vascular Surgery, First Affiliated Hospital 
of Chengdu Medical College, Chengdu, China.

BACKGROUND: Primary liver cancer, particularly hepatocellular carcinoma (HCC), 
poses significant clinical challenges due to late-stage diagnosis, tumor 
heterogeneity, and rapidly evolving therapeutic strategies. While systematic 
reviews and meta-analyses are essential for updating clinical guidelines, their 
labor-intensive nature limits timely evidence synthesis.
OBJECTIVE: This study proposes an automated literature screening workflow 
powered by large language models (LLMs) to accelerate evidence synthesis for HCC 
treatment guidelines.
METHODS: We developed a tripartite LLM framework integrating Doubao-1.5-pro-32k, 
Deepseek-v3, and DeepSeek-R1-Distill-Qwen-7B to simulate collaborative 
decision-making for study inclusion and exclusion. The system was evaluated 
across 9 reconstructed datasets derived from published HCC meta-analyses, with 
performance assessed using accuracy, agreement metrics (κ and 
prevalence-adjusted bias-adjusted κ), recall, precision, F1-scores, and 
computational efficiency parameters (processing time and cost).
RESULTS: The framework demonstrated good performance, with a weighted accuracy 
of 0.96 and substantial agreement (prevalence-adjusted bias-adjusted κ=0.91), 
achieving high weighted recall (0.90) but modest weighted precision (0.15) and 
F1-scores (0.22). Computational efficiency varied across datasets (processing 
time: 248-5850 s; cost: US $0.14-$3.68 per dataset).
CONCLUSIONS: This LLM-driven approach shows promise for accelerating evidence 
synthesis in HCC care by reducing screening time while maintaining 
methodological rigor. Key limitations related to clinical context sensitivity 
and error propagation highlight the need for reinforcement learning integration 
and domain-specific fine-tuning. LLM agent architectures with reinforcement 
learning offer a practical path for streamlining guideline updates, though 
further optimization is needed to improve specialization and reliability in 
complex clinical settings.

©Chen Pan, Wei Lu, Bingliang Chen, Gang Zhang, Zhiming Yang, Jingcheng Hao. 
Originally published in JMIR Medical Informatics (https://medinform.jmir.org), 
08.09.2025.

DOI: 10.2196/76252
PMCID: PMC12455167
PMID: 40921065 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


282. Lancet Digit Health. 2025 Dec;7(12):100926. doi: 10.1016/j.landig.2025.100926. 
Epub 2025 Dec 18.

Development and validation of a pre-trained language model for neonatal 
morbidities: a retrospective, multicentre, prognostic study.

Xie F(1), Chung P(2), Reiss JD(3), Tjoa E(4), De Francesco D(4), Phongpreecha 
T(4), Haberkorn W(4), Chakraborty D(4), Chang AL(4), James T(4), Kim Y(4), 
Mataraso S(4), Espinosa C(4), Yang L(4), Shu CH(4), Xue L(4), Berson E(4), 
Mohammadi N(4), Shome S(4), Reincke SM(4), Ghanem M(4), Maric I(3), Gaudilliere 
B(5), Angst MS(2), Sylvester K(6), Shaw GM(3), Prince LS(3), Stevenson DK(3), 
Aghaeepour N(7).

Author information:
(1)Department of Anesthesiology, Perioperative and Pain Medicine, Stanford 
University, Stanford, CA, USA; Department of Pediatrics, Stanford University 
School of Medicine, Stanford, CA, USA; Department of Biomedical Data Science, 
Stanford University, Stanford, CA, USA; Division of Computational Health 
Sciences, Department of Surgery, University of Minnesota, Minneapolis, MN, USA.
(2)Department of Anesthesiology, Perioperative and Pain Medicine, Stanford 
University, Stanford, CA, USA.
(3)Department of Pediatrics, Stanford University School of Medicine, Stanford, 
CA, USA.
(4)Department of Anesthesiology, Perioperative and Pain Medicine, Stanford 
University, Stanford, CA, USA; Department of Pediatrics, Stanford University 
School of Medicine, Stanford, CA, USA; Department of Biomedical Data Science, 
Stanford University, Stanford, CA, USA.
(5)Department of Anesthesiology, Perioperative and Pain Medicine, Stanford 
University, Stanford, CA, USA; Department of Pediatrics, Stanford University 
School of Medicine, Stanford, CA, USA.
(6)Department of Pediatrics, Stanford University School of Medicine, Stanford, 
CA, USA; Department of Surgery, Stanford University, Stanford, CA, USA.
(7)Department of Anesthesiology, Perioperative and Pain Medicine, Stanford 
University, Stanford, CA, USA; Department of Pediatrics, Stanford University 
School of Medicine, Stanford, CA, USA; Department of Biomedical Data Science, 
Stanford University, Stanford, CA, USA. Electronic address: 
naghaeep@stanford.edu.

BACKGROUND: Early identification and monitoring of neonatal morbidities are 
critical for timely interventions that can prevent complications, optimise 
resource use, and support families. Although traditional tools based on tabular 
data and biomarkers are beneficial, they are restricted in assessing the risk of 
morbidities in newborns. In this study, we developed NeonatalBERT, a pre-trained 
large language model (LLM) that estimates the risk of neonatal morbidities from 
clinical notes.
METHODS: This prognostic study investigated retrospective primary and external 
cohorts from two different quaternary-care academic medical centres in the USA: 
Stanford Health Care and Beth Israel Deaconess Medical Center. NeonatalBERT was 
initially pre-trained on clinical notes from the primary cohort and then 
fine-tuned separately for both cohorts. NeonatalBERT was also compared against 
other existing LLMs, such as BioBERT and Bio-ClinicalBERT, as well as 
traditional machine learning and logistic regression models using tabular 
features. NeonatalBERT was evaluated on 19 neonatal morbidities (respiratory 
distress syndrome, bronchopulmonary dysplasia, pulmonary haemorrhage, pulmonary 
hypertension, atelectasis, aspiration syndrome, intraventricular haemorrhage, 
periventricular leukomalacia, neonatal seizures, other CNS disorders, patent 
ductus arteriosus, cardiovascular instability, sepsis, candidiasis, anaemia, 
jaundice, necrotising enterocolitis, retinopathy of prematurity, and death) for 
the primary cohort and ten for the external cohort (respiratory distress 
syndrome, bronchopulmonary dysplasia, pulmonary haemorrhage, intraventricular 
haemorrhage, patent ductus arteriosus, sepsis, jaundice, necrotising 
enterocolitis, retinopathy of prematurity, and death). For each outcome, the 
area under the receiver operating characteristic curve, area under the 
precision-recall curve (AUPRC), and F1 scores were evaluated.
FINDINGS: 32 321 newborns were included in the primary cohort, including 27 411 
in the primary training set (mean gestational age 38·64 weeks [SD 2·30]; 13 056 
[47·6%] female and 14 355 [52·4%] male newborns) and 4910 in the primary testing 
set (mean gestational age 38·64 [2·13] weeks; 2336 [47·6%] female and 2574 
[52·4%] male newborns). Additionally, 7061 newborns were selected into the 
external cohort, including 5653 in the external training set (1567 [27·7%] 
premature and 4086 [72·3%] term births; 2614 [46·2%] female and 3039 [53·8%] 
male newborns) and 1408 in the external testing set (383 [27·2%] premature and 
1025 [72·8%] term births; 624 [44·3%] female and 784 [55·7%] male newborns). In 
the primary cohort, the mean AUPRC over 19 outcomes was 0·291 (95% CI 
0·268-0·314) for NeonatalBERT, 0·238 (0·217-0·259) for Bio-ClinicalBERT, 0·217 
(0·197-0·236) for BioBERT, and 0·194 (0·177-0·211) for the traditional model 
using tabular data. In the external cohort, NeonatalBERT had a mean AUPRC of 
0·360 (0·328-0·393), outperforming other models with the range of 0·224-0·333.
INTERPRETATION: Based on validation using two large-scale US datasets, 
NeonatalBERT effectively estimates the risk of neonatal morbidities from 
unstructured clinical notes of newborns. The promising results from this study 
show the potential of NeonatalBERT to enhance neonatal care and streamline 
hospital operations.
FUNDING: National Institutes of Health, Burroughs Wellcome Fund, March of Dimes 
Foundation, Alfred E Mann Foundation, Gates Foundation, Christopher Hess 
Research Fund, Roberts Foundation Research Fund, Prematurity Research Center, 
and Stanford Maternal & Child Health Research Institute Postdoctoral Support 
funds.

Copyright © 2025 The Author(s). Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.landig.2025.100926
PMCID: PMC12748047
PMID: 41419365 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of interests We declare no competing 
interests.


283. Health Informatics J. 2025 Jul-Sep;31(3):14604582251381269. doi: 
10.1177/14604582251381269. Epub 2025 Sep 22.

Performance of artificial intelligence large language models (Copilot and 
Gemini) compared to human experts in healthcare policy making: A mixed-methods 
cross-sectional study.

Khosravi M(1), Izadi R(2), Aghamaleki Sarvestani M(2), Bouzarjomehri H(3), 
Ahmadi Marzaleh M(4), Ravangard R(5).

Author information:
(1)Social Determinants of Health Research Center, Birjand University of Medical 
Sciences, Birjand, Iran.
(2)Student Research Committee, School of Health Management and Information 
Sciences, Shiraz University of Medical Sciences, Shiraz, Iran.
(3)Environmental Science and Technology Research Center, School of Public 
Health, Shahid Sadoughi University of Medical Sciences, Yazd, Iran.
(4)Department of Health in Disasters and Emergencies, Health Human Resources 
Research Center, School of Health Management and Information Sciences, Shiraz 
University of Medical Sciences, Shiraz, Iran.
(5)Department of Health Services Management, Health Human Resources Research 
Centre, School of Health Management and Information Sciences, Shiraz, Iran.

ObjectiveThis study aimed to assess the performance of Artificial Intelligence 
(AI) compared to human experts in healthcare policymaking.MethodsThis was a 
mixed-methods cross-sectional study conducted in Iran during the years 
2024-2025, comparing, and analyzing the responses of multiple AI Large Language 
Models (LLMs) including Bing AI Copilot and Gemini and a sample of 15 human 
experts-using confusion matrix analysis. This analysis provided comprehensive 
data on the respondents' ability to answer context-specific questions regarding 
healthcare policy making, evaluated through multiple parameters including 
sensitivity, specificity, negative predictive value (NPV), positive predictive 
value (PPV), and overall accuracy.ResultsCopilot demonstrated a sensitivity of 
0.867, specificity of 0, PPV of 0.722, NPV of 0, and accuracy of 0.65. In 
comparison, Gemini exhibited a sensitivity of 0.733, specificity of 0.4, PPV of 
0.786, NPV of 0.333, and also an accuracy of 0.65. Additionally, the human 
experts' responses indicated a sensitivity of 0.5808, specificity of 0.2571, PPV 
of 0.7189, NPV of 0.1579, and an accuracy of 0.5050.ConclusionThe AI LLMs 
outperformed human experts in responding to the study questionnaire. The 
findings demonstrated the considerable potential of the LLMs in enhancing 
healthcare policy-making, particularly by serving as complementary tools and 
collaborators alongside humans.

DOI: 10.1177/14604582251381269
PMID: 40977570 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of conflicting interestsThe 
author(s) declared no potential conflicts of interest with respect to the 
research, authorship, and/or publication of this article


284. BMC Med Educ. 2025 Dec 29;25(1):1734. doi: 10.1186/s12909-025-08315-z.

Performance comparison of large language models on pediatric dentistry questions 
in the Turkish dentistry specialization examination.

Başkan HK(1), Başkan B(2).

Author information:
(1)Department of Pediatric Dentistry, Faculty of Dentistry, Kahramanmaras Sutcu 
Imam University, Onikisubat, Kahramanmaras, Türkiye. kubrabaskan@ksu.edu.tr.
(2)Department of Endodontics, Faculty of Dentistry, Kahramanmaras Sutcu Imam 
University, Onikisubat, Kahramanmaras, Türkiye.

BACKGROUND/PURPOSE: This study aimed to compare the performance of seven leading 
large language models (Gemini 2.5 Pro, Grok-4, GPT-5, Claude-4, Copilot, 
Perplexity, and GPT-4o) on pediatric dentistry questions from the Turkish 
Dentistry Specialization Examination (DUS), and to identify differences in their 
performance on information-based versus case-based question types.
MATERIALS AND METHODS: Seven large language models (Gemini 2.5 Pro, Grok-4, 
GPT-5, Claude-4, Copilot, Perplexity, and GPT-4o) were evaluated on 127 
multiple-choice questions from the DUS pediatric dentistry question bank 
(2012-2021), classified by experts as information-based (n = 96) and case-based 
(n = 31). Questions were input in Turkish without modification, and responses 
were assessed against official answer keys.
RESULTS: Significant differences were observed in overall accuracy rates 
(p < 0.001). The highest overall accuracy was recorded for Gemini 2.5 Pro 
(94.5%; 120/127), while the lowest performance was seen with GPT-4o (63.0%; 
80/127). For information-based questions, Gemini answered 92/96 correctly 
(95.8%) and GPT-4o 66/96 (68.7%); for case-based questions, Gemini answered 
28/31 correctly (90.3%) and Perplexity 5/31 (16.1%). Pairwise Wilcoxon 
comparisons statistically supported Gemini's significant superiority over many 
models and the notably weak performance of GPT-4o and Perplexity on case-based 
questions (p < 0.001).
CONCLUSIONS: LLMs can serve as effective "co‑pilots" for information retrieval 
and exam preparation in dental education but are currently unreliable for 
diagnostic and treatment decision‑making. Clinicians and students should use LLM 
outputs for review and learning while retaining final decisions based on 
professional experience, ethical responsibility, and patient‑centered judgment. 
Future research should evaluate and enhance LLMs' multimodal and visual‑data 
processing capabilities to improve clinical applicability.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-08315-z
PMCID: PMC12751958
PMID: 41466246 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Because this study does not involve research on 
human subjects, it does not require compliance with the Declaration of Helsinki. 
Consent for publication: Not applicable. Competing interests: The authors 
declare no competing interests.


285. PLoS One. 2025 Mar 19;20(3):e0300487. doi: 10.1371/journal.pone.0300487. 
eCollection 2025.

Protocol for human evaluation of generative artificial intelligence chatbots in 
clinical consultations.

Chiu EK(1), Chung TW(1).

Author information:
(1)Department of Microbiology, Li Ka Shing Faculty of Medicine, The University 
of Hong Kong, Hong Kong, China.

BACKGROUND: Generative artificial intelligence (GenAI) has the potential to 
revolutionise healthcare delivery. The nuances of real-life clinical practice 
and complex clinical environments demand a rigorous, evidence-based approach to 
ensure safe and effective deployment of AI.
METHODS: We present a protocol for the systematic evaluation of large language 
models (LLMs) as GenAI chatbots within the context of clinical microbiology and 
infectious diseases clinical consultations. We aim to critically assess 
recommendations produced by four leading GenAI models, including Claude 2, 
Gemini Pro, GPT-4.0, and a GPT-4.0-based custom AI chatbot.
DISCUSSION: A standardised, healthcare-specific, universal prompt template is 
developed to elicit clinically impactful AI responses. Generated responses will 
be graded by two panels of practicing clinicians, encompassing a wide spectrum 
of domain expertise in clinical microbiology and virology, as well as infectious 
diseases. Evaluations will be performed using a 5-point Likert scale across four 
clinical domains: factual consistency, comprehensiveness, coherence, and medical 
harmfulness. Our study will offer insights into the feasibility, limitations, 
and boundaries of GenAI in clinical consultations, providing guidance for future 
research and clinical implementation. Ethical guidelines and safety guardrails 
should be developed to uphold patient safety and clinical standards.

Copyright: © 2025 Chiu, Chung. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0300487
PMCID: PMC11922213
PMID: 40106443 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


286. Comput Med Imaging Graph. 2026 Jan 8;128:102704. doi: 
10.1016/j.compmedimag.2026.102704. Online ahead of print.

A multi-expert deep learning framework with LLM-guided arbitration for 
multimodal histopathology prediction.

Debsarkar SS(1), Prasath VBS(2).

Author information:
(1)Department of Computer Science, University of Cincinnati, OH 45221, USA.
(2)Department of Computer Science, University of Cincinnati, OH 45221, USA; 
Division of Biomedical Informatics, Cincinnati Children's Hospital Medical 
Center, OH 45229, USA; Department of Pediatrics, College of Medicine, University 
of Cincinnati, OH 45257, USA. Electronic address: prasatsa@uc.edu.

Recent advances in deep learning have significantly improved the accuracy of 
computational pathology; however conventional model ensembling strategies often 
lack adaptability and interpretability hindering the clinical adaptability. 
While multiple artificial intelligence (AI) expert models can provide 
complementary perspectives, simply aggregating their outputs is often 
insufficient for handling inter-model disagreement and delivering interpretable 
decisions. To address these challenges, we propose a novel multi-expert 
framework that integrates diverse vision-based predictors and a clinical 
feature-based model, with a large language model (LLM) acting as an intelligent 
arbitrator. By leveraging the contextual reasoning and explanation capabilities 
of LLMs, our architecture dynamically synthesizes insights from both imaging and 
clinical data, resolving model conflicts, and providing transparent, rational 
decisions. We validate our approach on two cancer histopathology datasets, 
namely the HMU-GC-HE-30K which is a gastric cancer dataset containing pathology 
images only, and the BCNB which is a breast cancer biopsy dataset that is 
multimodal - contains pathology imaging and clinical information. Our proposed 
multi-expert, LLM arbitrated framework (MELLMA) outperforms convolutional neural 
networks (CNNs), and transformers, which are currently the de facto and 
state-of-the-art classification ensemble models, with better overall results. We 
test different LLMs as arbitrators, namely LLaMA, GPT variants, and Mistral. 
Further, our proposed framework outperforms strong single-agent CNN/ViT 
baselines on the datasets, and ablations show that learned per-agent trust 
materially improves the arbitrator's decisions without altering prompts or data. 
These experimental results demonstrate that LLM-guided arbitration consistently 
provides more robust and explainable performance than individual models, 
conventional ensembling with majority vote, uniform average, and meta-learners. 
The results obtained highlight the promise of LLM-driven arbitration for 
building transparent and extensible AI systems in digital pathology.

Copyright © 2026 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.compmedimag.2026.102704
PMID: 41519030

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


287. BMC Emerg Med. 2025 Jul 31;25(1):142. doi: 10.1186/s12873-025-01303-y.

Can AI match emergency physicians in managing common emergency cases? A 
comparative performance evaluation.

Gün M(1).

Author information:
(1)Ministry of Health, Şile State Hospital, Kumbaba Street, Şile, Istanbul, 
Türkiye. drmgun1@gmail.com.

BACKGROUND: Large language models (LLMs) such as ChatGPT are increasingly 
explored for clinical decision support. However, their performance in 
high-stakes emergency scenarios remains underexamined. This study aimed to 
evaluate ChatGPT's diagnostic and therapeutic accuracy compared to a 
board-certified emergency physician across diverse emergency cases.
METHODS: This comparative study was conducted using 15 standardized emergency 
scenarios sourced from validated academic platforms (Geeky Medics, Life in the 
Fast Lane, Emergency Medicine Cases). ChatGPT (GPT-4) and a physician 
independently evaluated each case based on five predefined parameters: 
diagnosis, investigations, initial treatment, clinical safety, and 
decision-making complexity. Cases were scored out of 5. Concordance was 
categorized as high (5/5), moderate (4/5), or low (≤ 3/5). Wilson confidence 
intervals (95%) were calculated for each concordance category.
RESULTS: ChatGPT achieved high concordance (5/5) in 8 cases (53.3%, 95% CI: 
27.6-77.0%), moderate concordance (4/5) in 4 cases (26.7%, CI: 10.3-55.4%), and 
low concordance (≤ 3/5) in 3 cases (20.0%, CI: 6.0-45.6%). Performance was 
strongest in structured, protocol-based conditions such as STEMI, DKA, and 
asthma. Lower performance was observed in complex scenarios like stroke, trauma 
with shock, and mixed acid-base disturbances.
CONCLUSION: ChatGPT showed strong alignment with emergency physician decisions 
in structured scenarios but lacked reliability in complex cases. While AI may 
enhance decision-making and education, it cannot replace the clinical reasoning 
of human physicians. Its role is best framed as a supportive tool rather than a 
substitute.

© 2025. The Author(s).

DOI: 10.1186/s12873-025-01303-y
PMCID: PMC12315197
PMID: 40745532 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. No human participants were involved in the study, 
and all data were obtained from open-access educational platforms. Consent for 
publication: Not applicable. Competing interests: The authors declare no 
competing interests.


288. J Intensive Care Med. 2025 Feb;40(2):184-190. doi: 10.1177/08850666241267871. 
Epub 2024 Aug 8.

Evaluating the Appropriateness, Consistency, and Readability of ChatGPT in 
Critical Care Recommendations.

Balta KY(1), Javidan AP(2), Walser E(3)(4), Arntfield R(3), Prager R(3).

Author information:
(1)Schulich School of Medicine & Dentistry, Western University, London, Ontario, 
Canada.
(2)Division of Vascular Surgery, Department of Surgery, University of Toronto, 
Toronto, Ontario, Canada.
(3)Division of Critical Care, London Health Sciences Centre, Western University, 
London, Ontario, Canada.
(4)Department of Surgery, Trauma Program, London Health Sciences Centre, London, 
Ontario, Canada.

Background: We assessed 2 versions of the large language model (LLM) 
ChatGPT-versions 3.5 and 4.0-in generating appropriate, consistent, and readable 
recommendations on core critical care topics. Research Question: How do 
successive large language models compare in terms of generating appropriate, 
consistent, and readable recommendations on core critical care topics? Design 
and Methods: A set of 50 LLM-generated responses to clinical questions were 
evaluated by 2 independent intensivists based on a 5-point Likert scale for 
appropriateness, consistency, and readability. Results: ChatGPT 4.0 showed 
significantly higher median appropriateness scores compared to ChatGPT 3.5 (4.0 
vs 3.0, P < .001). However, there was no significant difference in consistency 
between the 2 versions (40% vs 28%, P = 0.291). Readability, assessed by the 
Flesch-Kincaid Grade Level, was also not significantly different between the 2 
models (14.3 vs 14.4, P = 0.93). Interpretation: Both models produced 
"hallucinations"-misinformation delivered with high confidence-which highlights 
the risk of relying on these tools without domain expertise. Despite potential 
for clinical application, both models lacked consistency producing different 
results when asked the same question multiple times. The study underscores the 
need for clinicians to understand the strengths and limitations of LLMs for safe 
and effective implementation in critical care settings. Registration: 
https://osf.io/8chj7/.

DOI: 10.1177/08850666241267871
PMCID: PMC11639400
PMID: 39118320 [Indexed for MEDLINE]

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


289. J Perinat Med. 2025 Sep 23;54(1):17-22. doi: 10.1515/jpm-2025-0392. Print 2026 
Jan 23.

Integrating generative AI in perinatology: applications for literature review.

Ayala-Yáñez R(1), Grünebaum A(2), Chervenak FA(2).

Author information:
(1)Centro Médico ABC I.A.P., Mexico City, Mexico.
(2)Northwell Health, Hempstead, NY, USA.

Perinatology relies on continuous engagement with an expanding body of clinical 
literature, yet the volume and velocity of publications increasingly exceed the 
capacity of clinicians to keep pace. Generative artificial intelligence (GAI) 
tools - such as ChatGPT4, Claude AI, Gemini, and Perplexity AI - offer a novel 
approach to assist with literature retrieval, comparison of clinical guidelines, 
and manuscript drafting. This study evaluates the strengths and limitations of 
these tools in maternal-fetal medicine, using structured clinical prompts to 
simulate real-world applications. Perplexity AI demonstrated the best citation 
accuracy, while ChatGPT4 and Claude excelled in content summarization but 
required manual verification of citations. In simulated trials, GAI tools 
reduced the time to generate clinically relevant summaries by up to 70 % 
compared to traditional PubMed searches. However, risks such as hallucinated 
references and overreliance on machine-generated text persist. Use cases include 
summarizing aspirin use guidelines for preeclampsia and comparing ACOG vs. NICE 
protocols. GAI should be viewed as a supportive assistant, not a substitute, for 
expert review. To ensure responsible integration, clinicians must develop AI 
literacy, apply rigorous oversight, and adhere to ethical standards. When used 
judiciously, GAI can enhance efficiency, insight, and evidence-based 
decision-making in perinatal care.

© 2025 the author(s), published by De Gruyter, Berlin/Boston.

DOI: 10.1515/jpm-2025-0392
PMID: 40980855 [Indexed for MEDLINE]


290. J Am Heart Assoc. 2025 Oct 21;14(20):e043333. doi: 10.1161/JAHA.125.043333. Epub 
2025 Oct 21.

Use of a Generative Pretrained Transformer to Answer Questions and Facilitate a 
Large Randomized Controlled Trial.

Brown DL(1), Novitski K(1)(2), Sickler JB(3), Hankla RM(4), Khattak F(5), 
Philpot E(5), Durkalski-Mauldin V(5), Sayyahmelli S(6), Patino C(7), 
Arevalo-Bermudez F(8), Roberts M(9), Sahelian A(10), Chervin RD(2).

Author information:
(1)Stroke Program University of Michigan Ann Arbor MI USA.
(2)Sleep Disorders Center and Department of Neurology University of Michigan Ann 
Arbor MI USA.
(3)Department of Neurology and Rehabilitation Medicine University of Cincinnati 
Cincinnati OH USA.
(4)Nox Health Alpharetta GA USA.
(5)The Data Coordination Unit (DCU), Department of Public Health Sciences, 
Medical University of South Carolina Charleston SC USA.
(6)Department of Neurological Surgery, School of Medicine and Public Health 
University of Wisconsin-Madison Madison WI USA.
(7)Atrium Health Carolinas Rehabilitation Charlotte NC USA.
(8)Intercoastal Medical Group Sarasota FL USA.
(9)The University of Alabama at Birmingham Birmingham AL USA.
(10)Department of Neurosurgery Dignity Health Chandler Regional Medical Center 
Chandler AZ USA.

BACKGROUND: Generative artificial intelligence may help facilitate clinical 
trials. We sought to determine whether a customized generative pretrained 
transformer (GPT) could assist clinical trial sites within a randomized, 
controlled trial with rapid responses to protocol and procedure-related 
questions.
METHODS: Within a large clinical trial, Sleep SMART (Sleep for Stroke Management 
and Recovery Trial), we developed, tested, and implemented a customized GPT 
designed to answer, in real-time, procedure-related questions. This support was 
offered to all active trial sites and questions and responses were monitored by 
the central study team. An anonymous survey also queried primary study 
coordinators about their experiences with the GPT.
RESULTS: Of the 785 questions entered during a 10-month implementation period, 
75% were able to be answered per the GPT. On manual review, of the 588 that the 
GPT reportedly answered, only 13 (2%) were not helpful responses, 12 (2%) were 
incomplete responses, 5 (1%) were misleading, and 3 (1%) contained the correct 
response but also provided some conflicting messaging. Of 95 primary study 
coordinators to whom the survey was sent, 45 (47%) responded. Of those, 21 (47%) 
reported having used the GPT. Of the 19 who provided more detailed information, 
89% found it to be very helpful (n=11) or helpful (n=6). Most (79%) found the 
responses to be accurate (n=15) or partially accurate (n=2), and 89% were very 
satisfied (n=10) or satisfied (n=7) with the GPT.
CONCLUSIONS: This novel use of a customized GPT suggests it could be valuable in 
support of clinical sites within a large trial. Further research should confirm 
equivalent accuracy and safety through a direct comparison to human support.
REGISTRATION: URL: https://clinicaltrials.gov/; Unique Identifier: NCT03812653.

DOI: 10.1161/JAHA.125.043333
PMCID: PMC12684590
PMID: 41120814 [Indexed for MEDLINE]

Conflict of interest statement: Devin Brown: is a multiple principal 
investigator of a National Institutes of Health‐funded project that receives 
materials support from ResMed and Fisher & Paykel; is on the editorial board of 
Stroke; is a member of the Board of Directors of the American Neurological 
Association. Rebecca Hankla: Employed by Nox Health. Ronald Chervin: receives 
grant funding from the National Institutes of Health; royalties as an author and 
editor for UpToDate; shared royalties through licensing by Michigan Medicine of 
childhood sleep questionnaires to MAPI; and no compensation as president for the 
International Pediatric Sleep Association.


291. J Educ Eval Health Prof. 2026;23:2. doi: 10.3352/jeehp.2026.23.2. Epub 2026 Jan 
15.

Comparison of reference management software with new artificial 
intelligence-based tools.

Jin JG(1), Lee SG(1), Park JH(1), Han JW(1), Kim JY(1)(2), Seok J(1)(3), Yoo 
JJ(1)(2).

Author information:
(1)Research Factory and Publication Inc., Incheon, Korea.
(2)Division of Gastroenterology and Hepatology, Department of Internal Medicine, 
Soonchunhyang University Bucheon Hospital, Soonchunhyang University School of 
Medicine, Bucheon, Korea.
(3)Department of Otorhinolaryngology-Head and Neck Surgery, Seoul National 
University Hospital, Seoul National University College of Medicine, Seoul, 
Korea.

Reference management software (RMS) represents a cornerstone of modern academic 
writing and publishing. For decades, programs such as EndNote, Zotero, and 
Mendeley have played central roles in facilitating citation organization, 
bibliography formatting, and collaborative scholarship. Although each platform 
has introduced unique innovations, persistent limitations remain, particularly 
with respect to usability, accessibility, and accuracy. In parallel, the rise of 
generative artificial intelligence has introduced an unprecedented challenge: 
the inadvertent inclusion of fabricated or incorrect references mistakenly 
incorporated into manuscripts. This phenomenon has exposed a critical limitation 
of traditional RMS platforms, namely their inability to verify reference 
authenticity. Against this backdrop, new solutions have emerged. One such 
example is CiteWell (https://citewell.org/), an artificial intelligence (AI)-era 
RMS that introduces several notable innovations, including PubMed-integrated 
verification, an intuitive interface for new users, customizable 
journal-specific styles, and multilingual accessibility. This review provides a 
comprehensive historical overview of RMS, evaluates the strengths and weaknesses 
of major platforms, and positions emerging AI-based tools as a new paradigm that 
combines traditional reference management with essential safeguards for 
contemporary academic challenges.

DOI: 10.3352/jeehp.2026.23.2
PMID: 41549369 [Indexed for MEDLINE]


292. Int J Surg. 2025 Jul 1;111(7):4252-4262. doi: 10.1097/JS9.0000000000002507. Epub 
2025 May 28.

Evaluating generative AI models for explainable pathological feature extraction 
in lung adenocarcinoma: grading assessment and prognostic model construction.

Shen J(1), Feng S(2)(3), Zhang P(4), Qi C(5), Liu Z(6), Feng Y(7), Dong C(7), 
Xie Z(7), Gan W(8), Zhu L(7), Mou W(9), Zeng D(10)(11), Tang B(12), Xiao M(13), 
Chu G(14), Cheng Q(15)(16), Zhang J(7), Peng S(17), Bai Y(18), Wong HZH(19), 
Jiang A(20), Luo P(1), Lin A(1).

Author information:
(1)Department of Oncology, Zhujiang Hospital, Southern Medical University; 
Donghai County People's Hospital - Jiangnan University Smart Healthcare Joint 
Laboratory, Donghai County People's Hospital (Affiliated Kangda College of 
Nanjing Medical University), Lianyungang, China.
(2)Donghai County People's Hospital - Jiangnan University Smart Healthcare Joint 
Laboratory, Donghai County People's Hospital (Affiliated Kangda College of 
Nanjing Medical University), Lianyungang, China.
(3)Department of Neurosurgery, Affiliated Hospital of Jiangnan University, Wuxi, 
China.
(4)Department of Lung Cancer, Tianjin Lung Cancer Center, National Clinical 
Research Center for Cancer, Key Laboratory of Cancer Prevention and Therapy, 
Tianjin's Clinical Research Center for Cancer, Tianjin Medical University Cancer 
Institute and Hospital, Tianjin, China.
(5)The University of Hong Kong, Hong Kong, China.
(6)Institute of Basic Medical Sciences, Chinese Academy of Medical Sciences and 
Peking Union Medical College, Beijing, China.
(7)Department of Oncology, Zhujiang Hospital, Southern Medical University, 
Guangzhou, China.
(8)Department of Joint Surgery and Sports Medicine, Zhuhai People's Hospital 
(Zhuhai hospital affiliated with Jinan University, Guangdong, China.
(9)Department of Urology, Shanghai General Hospital, Shanghai Jiao Tong 
University School of Medicine, Shanghai, China.
(10)Department of Oncology, Nanfang Hospital, Southern Medical University, 
Guangzhou, China.
(11)Cancer Center, the Sixth Affiliated Hospital, School of Medicine, South 
China University of Technology, Guangzhou, China.
(12)Department of Radiation Oncology, Zhongshan Hospital Affiliated to Fudan 
University, Shanghai, China.
(13)Hepatobiliary Surgery Department, Quzhou Affiliated Hospital of Wenzhou 
Medical University, Quzhou People's Hospital, China.
(14)Department of Urology, The Affiliated Hospital of Qingdao University, 
Qingdao, China.
(15)Department of Neurosurgery, Xiangya Hospital, Central South University, 
Changsha, Hunan, China.
(16)National Clinical Research Center for Geriatric Disorders, Xiangya Hospital, 
Central South University, Changsha, Hunan, China.
(17)Department of Radiology, Sichuan Academy of Medical Sciences and Sichuan 
Provincial People's Hospital, University of Electronic Science and Technology of 
China, Chengdu, China.
(18)Department of Oncology, Sichuan Provincial People's Hospital, University of 
Electronic Science and Technology of China, Chengdu, China.
(19)Li Ka Shing Faculty of Medicine, The University of Hong Kong, Hong Kong SAR, 
China.
(20)Department of Urology, Changhai hospital, Naval Medical University (Second 
Military Medical University), Shanghai, China.

BACKGROUND: Given the increasing prevalence of generative AI (GenAI) models, a 
systematically evaluation of their performance in lung adenocarcinoma 
histopathological assessment is crucial. This study aimed to evaluate and 
compare three visual-capable GenAI models (GPT-4o, Claude-3.5-Sonnet, and 
Gemini-1.5-Pro) for lung adenocarcinoma histological pattern recognition and 
grading, as well as to explore prognostic prediction models based on GenAI 
feature extraction.
MATERIALS AND METHODS: In this retrospective study, we analyzed 310 diagnostic 
slides from The Cancer Genome Atlas Lung Adenocarcinoma (TCGA-LUAD) database to 
evaluate GenAI models and to develop and internally validate machine 
learning-based prognostic models. For independent external validation, we 
utilized 95 and 87 slides from obtained different institutions. The primary 
endpoints comprised GenAI grading accuracy (area under the receiver operating 
characteristic curve, AUC) and stability (intraclass correlation coefficient, 
ICC). Secondary endpoints included developing and assessing machine 
learning-based prognostic models using GenAI-extracted features from the 
TCGA-LUAD dataset, evaluated by Concordance index (C-index).
RESULTS: Among the evaluated models, claude-3.5-Sonnet demonstrated the best 
overall performance, achieving high grading accuracy (average AUC = 0.823) with 
moderate stability (ICC = 0.585) The optimal machine learning-based prognostic 
model, developed using features extracted by Claude-3.5-Sonnet and integrating 
clinical variables, demonstrated good performance in both internal and external 
validations, yielding an average C-index of 0.715. Meta-analysis demonstrated 
that this prognostic model effectively stratified patients into risk groups, 
with the high-risk group showing significantly worse outcomes (Hazard 
ratio = 5.16, 95% confidence interval = 3.09-8.62).
CONCLUSION: GenAI models demonstrated significant potential in lung 
adenocarcinoma pathology, with Claude-3.5-Sonnet exhibiting superior performance 
in grading prediction and robust prognostic capabilities. These findings 
indicate promising applications of AI in lung adenocarcinoma diagnosis and 
clinical management.

Copyright © 2025 The Author(s). Published by Wolters Kluwer Health, Inc.

DOI: 10.1097/JS9.0000000000002507
PMID: 40434749 [Indexed for MEDLINE]


293. PLoS One. 2025 Jul 24;20(7):e0327584. doi: 10.1371/journal.pone.0327584. 
eCollection 2025.

Evaluating GPT-4's role in critical patient management in emergency departments.

Yiğit Y(1)(2), Günay S(3), Öztürk A(3), Alkahlout B(2).

Author information:
(1)Blizard Institute, Queen Mary University, London, United Kingdom.
(2)Hamad Medical Corporation, Department of Emergency Medicine, Doha, Qatar.
(3)Hitit University, Çorum Erol Olçok Education and Research Hospital, 
Department of Emergency Medicine, Çorum, Turkey.

INTRODUCTION: Recent advancements in artificial intelligence (AI) have 
introduced tools like ChatGPT-4, capable of interpreting visual data, including 
ECGs. In our study,we aimed to investigate the effectiveness of GPT-4 in 
interpreting ECGs and managing patient care in emergency settings.
METHODS: Conducted from April to May 2024, this study evaluated GPT-4 using 
twenty case scenarios sourced from PubMed Central and the OSCE sample question 
book. These cases, categorized into common and rare scenarios, were analyzed by 
GPT-4, and its interpretations were reviewed by five experienced emergency 
medicine specialists. The accuracy of ECG interpretations and subsequent patient 
management plans were assessed using a structured evaluation framework and 
critical error identification.
RESULTS: GPT-4 made critical errors in 46% of ECG interpretations in the OSCE 
group and 50% in the PubMed group. For patient management, critical errors were 
found in 32% of the OSCE group and 14% of the PubMed group. When ECG evaluations 
were included in patient management, error rates approached 50%. The inter-rater 
reliability among evaluators indicated good agreement (ICC = 0.725, F = 3.72, 
p < 0.001).
CONCLUSION: While GPT-4 shows promise in specific applications, its current 
limitations in accurately interpreting ECGs and managing critical patient 
scenarios render it inappropriate for emergency department use. Future 
improvements and extensive validations are essential before such AI tools can be 
reliably deployed in critical healthcare settings.

Copyright: © 2025 Yiğit et al. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0327584
PMCID: PMC12288989
PMID: 40705814 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


294. JMIR Form Res. 2025 Nov 24. doi: 10.2196/82487. Online ahead of print.

Assessment of Physician Preferences for Large Language Model-Generated Responses 
Across Geographic Regions and Clinical Experience Levels: A Preliminary Survey 
Study.

Brooks JS(1), Blankson PK(1), Campbell PM(2), Cowley RA(3), Yang TS(4), Oseni 
T(5), Rodriguez A(1), Idris MY(1).

Author information:
(1)Morehouse School of Medicine, 720 Westview Dr SW, Atlanta, GA 30310, Atlanta, 
US.
(2)Icahn School of Medicine at Mount Sinai, New York, US.
(3)Department of Orthopedic Surgery, MedStar Georgetown University Hospital, 
Georgetown, Georgetown, US.
(4)Cathay General Hospital, No.280, Sec. 4, Ren'ai Rd., Da'an Dist., Taipei 
City, Taiwan, Taipei City, TW.
(5)Department of Family Medicine, Edo University Iyamho, Edo State, NG.

BACKGROUND: Large language models (LLMs) have demonstrated increasing capability 
in generating clinically coherent and accurate responses to patient questions, 
in some cases outperforming physicians in accuracy and empathy. Yet, little is 
known about how physicians across regions and levels of experience evaluate 
these AI-generated responses compared to those authored by human clinicians.
OBJECTIVE: This study examined physician evaluations of LLM- versus 
physician-authored responses to real-world patient questions, comparing patterns 
across global regions and years in clinical practice.
METHODS: We conducted a cross-sectional online survey between March and May 2025 
among licensed physicians recruited internationally. Participants reviewed 
anonymized medical responses from two LLMs (ChatGPT-4.0 and Meta.AI) and 
verified physicians to questions sourced from Reddit's r/AskDocs forum. Each 
participant ranked three responses per question (1 = most preferred, 3 = least 
preferred) according to accuracy and responsiveness. Mean ranks, pairwise win 
proportions, and full rank distributions were analyzed descriptively and 
stratified by region and years in practice.
RESULTS: Overall, LLM-generated responses were strongly preferred. ChatGPT-4.0 
achieved the best mean rank (1.63 ± 0.68; 95% CI: 1.52-1.74), followed by 
Meta.AI (1.83 ± 0.72; 95% CI: 1.71-1.94), while verified physician-authored 
responses were least preferred (2.53 ± 0.76; 95% CI: 2.40-2.65). In pairwise 
analyses, ChatGPT-4.0 responses won 67% of head-to-head comparisons versus 
physicians and 56% versus Meta.AI. Preference for ChatGPT-4.0 was most 
pronounced in Africa (mean = 1.48), Asia (1.59), and North America (1.64), while 
Meta.AI slightly led in Europe (1.70) and the Americas (1.75). Across experience 
levels, physicians with less than 5 years in practice (n = 28) ranked 
ChatGPT-4.0 most favorably (mean = 1.58), followed by those with 10-15 years 
(mean = 1.60). Even among those with over 15 years in practice (n = 9), 
AI-generated responses outperformed physician responses (mean = 1.75 vs. 2.62). 
Across all subgroups, human-authored responses were ranked lowest.
CONCLUSIONS: This exploratory study demonstrates that physicians from diverse 
regions and experience levels generally prefer LLM-generated responses to 
human-authored ones. The consistency of this finding across continents and 
practice durations underscores growing professional acceptance of AI as a viable 
tool in patient communication. These results suggest that modern LLMs, 
particularly ChatGPT-4.0, may provide clinically acceptable, contextually 
relevant, and user-trusted health information, offering potential to augment 
physician workflows and patient education.
CLINICALTRIAL: N/a.

DOI: 10.2196/82487
PMID: 41417951


295. J Med Syst. 2025 Aug 2;49(1):101. doi: 10.1007/s10916-025-02234-8.

Understanding Clinician Perceptions of GenAI: A Mixed Methods Analysis of 
Clinical Documentation Tasks.

Fraile Navarro D(1), Kocaballi AB(2), Berkovsky S(3).

Author information:
(1)Centre for Health Informatics, Australian Institute of Health Innovation, 
Macquarie University, 75 Talavera Road, Sydney, 2113, NSW, Australia. 
david.frailenavarro@mq.edu.au.
(2)Faculty of Engineering and Information Technology, University of Technology 
Sydney, Sydney, Australia.
(3)Centre for Health Informatics, Australian Institute of Health Innovation, 
Macquarie University, 75 Talavera Road, Sydney, 2113, NSW, Australia.

This mixed-methods study evaluated clinicians' user experience (UX) with 
Generative AI (GenAI) in Electronic Health Record (EHR) systems across three 
clinical documentation tasks (Information Extraction, Summarization, and 
Speech-to-Text) at varying levels of user supervision (low, medium, high), 
focusing on workflow improvements, safety, and acceptable automation levels. 
Using conceptual prototyping in a usability study framework, we evaluated how 
incorporating GenAI into EHR could support the three documentation tasks at 
varying automation levels. A total of 38 clinicians interacted with the 
prototype and completed a questionnaire on task relevance, perceived importance, 
desired automation level, and EHR satisfaction. Both quantitative (descriptive 
statistics, Kruskal-Wallis tests, Spearman correlations) and qualitative 
(thematic) analyses were conducted with equal priority to explore preferences, 
perceived safety, and practical requirements. Clinicians showed positive 
reception to GenAI integration, particularly for streamlining documentation. 
While task relevance and importance were strongly correlated, EHR satisfaction 
did not significantly predict automation acceptance. Medium automation emerged 
as the preferred level, considered "safe with caution". Five key themes emerged 
from qualitative analysis: efficiency and quality benefits; system reliability 
concerns; safety and medico-legal considerations; automation bias and loss of 
nuance; and deployment requirements including adjustable settings and oversight. 
While clinicians welcome GenAI-driven documentation, they prefer moderate 
automation to balance efficiency with clinical control. Successful integration 
requires addressing safety concerns, conducting real-world trials, and 
mitigating potential biases and medico-legal challenges. These findings suggest 
a cautious but optimistic path forward for AI integration in EHR systems, 
emphasizing the importance of maintaining clinician oversight while leveraging 
automation benefits.

© 2025. The Author(s).

DOI: 10.1007/s10916-025-02234-8
PMCID: PMC12317912
PMID: 40751820 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics Approval and Consent to 
Participate: The study was approved by Macquarie University Ethics 
(REF-52022931342227). All participants provided informed consent. Competing 
interests: The authors declare no competing interests


296. J Am Med Inform Assoc. 2025 Aug 1;32(8):1320-1327. doi: 10.1093/jamia/ocaf099.

Confidence-linked and uncertainty-based staged framework for phenotype 
validation using large language models.

Lee S(1)(2), Lee HH(2)(3)(4), Lee H(2)(4)(5), Yum KS(6), Baek JH(7), Khil J(4), 
Lee J(4), Shin S(8), Cho M(8), Ahn NY(8), You SC(1)(2), Kim HC(2)(4)(5).

Author information:
(1)Department of Biomedical Systems Informatics, Yonsei University College of 
Medicine, Seoul 03722, Korea.
(2)Institute for Innovation in Digital Healthcare, Yonsei University, Seoul 
03722, Korea.
(3)Richard A. and Susan F. Smith Center for Outcomes Research in Cardiology, 
Beth Israel Deaconess Medical Center, Harvard Medical School, Boston, MA 02215, 
United States.
(4)Department of Preventive Medicine, Yonsei University College of Medicine, 
Seoul 03722, Korea.
(5)Department of Internal Medicine, Yonsei University College of Medicine, Seoul 
03722, Korea.
(6)Department of Neurology, Chungbuk National University Hospital, Cheongju 
28644, Korea.
(7)Department of Neurology, Kangbuk Samsung Hospital, Sungkyunkwan University 
School of Medicine, Seoul 03181, Korea.
(8)Department of Public Health, Yonsei University Graduate School, Seoul 03722, 
Korea.

OBJECTIVES: This study develops and validates the confidence-linked and 
uncertainty-based staged (CLUES) framework by integrating large language models 
(LLMs) with uncertainty quantification to assist manual chart review while 
ensuring reliability through a selective human review.
MATERIALS AND METHODS: The CLUES framework assesses stroke-related 
hospitalizations using imaging reports for 1739 patients across 24 Korean 
hospitals (2011-2022). Uncertainty was quantified via entropy from LLM-derived 
confidence values. Our framework operated in 3 stages: (1) zero-shot prompting 
with ensemble averaging, where high-uncertainty cases advanced to stage 2, (2) 
few-shot prompting using retrieved low-uncertainty cases, with remaining 
high-uncertainty cases proceeding to stage 3, and (3) manual chart review for 
final uncertain cases. Performance was evaluated against physician-labeled data 
using F1-score and Cohen's Kappa.
RESULTS: Among 1072 test cases, stage 1 classified 507 cases as low uncertainty, 
while 565 were high uncertainty. Stage 2 reclassified 280 cases as low 
uncertainty, leaving 285 for manual review. Low-uncertainty cases consistently 
outperformed high-uncertainty cases in both stages (weighted F1-scores: 0.94 vs 
0.57 in stage 1 and 0.82 vs 0.58 in stage 2). The overall framework performance 
showed a progressive improvement in F1-scores from 0.840 (stage 1) to 0.878 
(stage 2) to 0.955 (stage 3).
DISCUSSION: The CLUES framework reduced manual review burden by 75% while 
maintaining high accuracy. By integrating uncertainty quantification with 
selective human oversight, it provides an efficient and reliable approach to 
phenotype validation.
CONCLUSION: This framework demonstrates the effective integration of LLMs into 
clinical workflows while ensuring human oversight, enhancing both accuracy and 
efficiency.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf099
PMCID: PMC12277694
PMID: 40574695 [Indexed for MEDLINE]

Conflict of interest statement: S.C.Y. reports being a chief executive officer 
of PHI Digital Healthcare; and receiving grants from DaiichiSankyo. The 
remaining authors declare no competing financial or nonfinancial interests.


297. BMC Med Ethics. 2025 Dec 30;26(1):183. doi: 10.1186/s12910-025-01323-0.

Artificial intelligence guidance in ethically challenging clinical scenarios in 
child and adolescent psychiatry: a qualitative study in the context of Turkiye.

İçen S(1), Köken AH(2).

Author information:
(1)Department of Child and Adolescent Psychiatry, Kırşehir Ahi Evran University 
Faculty of Medicine, Kervansaray Mahallesi 2019/1, Kırşehir, Türkiye. 
sarper.icen@gmail.com.
(2)Department of History of Medicine and Ethics, Kırşehir Ahi Evran University 
Faculty of Medicine, Kırşehir, Türkiye.

BACKGROUND: Ethical decision-making in child and adolescent psychiatry (CAP) is 
inherently complex, shaped by developmental vulnerability, evolving autonomy, 
and competing responsibilities to patients, families, and the legal system. 
Clinicians often face moral dilemmas when navigating adolescent confidentiality, 
parental authority, and mandatory reporting duties, especially in high-stakes or 
culturally sensitive contexts. As large language models (LLMs) enter clinical 
settings, their potential to support ethical reasoning remains underexplored, 
particularly outside Western paradigms. This study qualitatively investigates 
how different LLMs provide ethical, legal, and emotional guidance to clinicians 
facing ethically challenging scenarios in CAP, situated within Turkiye's 
sociocultural and legal landscape.
METHOD: A scenario-based qualitative design was employed. Three expert-developed 
case vignettes reflecting ethically charged dilemmas, such as adolescent 
autonomy, parental conflict, and confidentiality, were submitted to the three 
LLMs (ChatGPT 4.0, Gemini 2.5 Flash, and GROK 3). Responses were analyzed using 
content and thematic analysis to identify key patterns of ethical-legal 
reasoning, alongside discourse analysis to examine tone, empathy, and cultural 
sensitivity. Two researchers, with backgrounds in CAP and medical ethics, 
conducted independent coding and reached consensus through a reflexive, 
interdisciplinary approach.
RESULTS: All LLMs addressed core ethical principles (autonomy, non-maleficence, 
beneficence, and justice) and referenced Turkish legal frameworks such as the 
Child Protection Law, Patient Rights Regulation, and mandatory reporting 
obligations, situating their guidance within the national regulatory context. 
They also differed in their engagement with sociocultural sensitivities: GROK 3 
emphasized therapeutic communication and relational trust, Gemini 2.5 Flash 
applied a highly structured, rule-based style focused on procedural compliance, 
while ChatGPT 4.0 provided concise and practical suggestions. Despite thematic 
overlaps, these varying approaches shaped how effectively the models aligned 
with Turkiye's clinical realities. Notably, LLMs frequently acted as "thinking 
companions," offering ethical and legal justifications while leaving 
interpretive responsibility with clinicians.
CONCLUSION: LLMs in CAP hold promise not only as cognitive aids but also as 
emotionally attuned, context-sensitive companions in ethical decision-making 
processes. Their effectiveness depends not just on algorithmic precision but 
also on explainability, empathy, and cultural alignment. Rather than replacing 
clinician judgment, LLMs may serve to ease emotional burden, enhance therapeutic 
reflection, and foster ethically sound care in complex, high-pressure 
situations.

© 2025. The Author(s).

DOI: 10.1186/s12910-025-01323-0
PMCID: PMC12754976
PMID: 41469990 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


298. Nat Med. 2026 Jan;32(1):288-296. doi: 10.1038/s41591-025-04042-6. Epub 2026 Jan 
2.

Generative AI-based low-dose digital subtraction angiography for intra-operative 
radiation dose reduction: a randomized controlled trial.

Zhao H(#)(1)(2), Bai Y(#)(3), Chen L(#)(3), Ma J(#)(3), Lei Y(3), Sun T(4), Wu 
L(3), Zhang R(5), Xu Z(6), Liang X(7), Li Y(7), Huang Y(8), Feng Y(9), Hong 
C(10), Miao Z(11), Long L(12), Zhu H(13), Zheng J(14), Fan L(15), Fang Z(16), 
Dong P(17), Zhang L(5), Han X(3), Wang B(18), Liang B(3), Xia X(3), Kan X(19), 
Zhu C(20), Du B(21), Wang X(22), Zheng C(23).

Author information:
(1)School of Computer Science, Wuhan University, Wuhan, China. 
zhao_huangxuan@sina.com.
(2)Department of Radiology, Union Hospital, Tongji Medical College, Huazhong 
University of Science and Technology, Wuhan, China. zhao_huangxuan@sina.com.
(3)Department of Radiology, Union Hospital, Tongji Medical College, Huazhong 
University of Science and Technology, Wuhan, China.
(4)Department of Interventional Radiology, The First Affiliated Hospital of 
University of Science and Technology of China, Hefei, China.
(5)School of Computer Science, Wuhan University, Wuhan, China.
(6)School of Electronic Information and Communications, Huazhong University of 
Science and Technology, Wuhan, China.
(7)Institute of Research and Clinical Innovations, Neusoft Medical Systems, 
Shanghai, China.
(8)CV Business Unit, Neusoft Medical Systems, Shenyang, China.
(9)Center for Biological Imaging, Institute of Biophysics, Chinese Academy of 
Sciences, Beijing, China.
(10)The First Affiliated Hospital of Guangzhou Medical University, Guangzhou, 
China.
(11)Department of Interventional Neuroradiology, Beijing Tiantan Hospital, 
Capital Medical University, Beijing, China.
(12)Department of Interventional and Vascular Surgery, Hunan Provincial People's 
Hospital, Hunan Normal University, Changsha, China.
(13)Center of Interventional Radiology Vascular Surgery, Department of 
Radiology, Zhongda Hospital, Medical School, Southeast University, Nanjing, 
China.
(14)Department of Radiology, Shengjing Hospital of China Medical University, 
Shenyang, China.
(15)Department of Cardiology, Union Hospital, Fujian Medical University, Fuzhou, 
China.
(16)Department of Interventional Radiology, Fujian Provincial Hospital, Shengli 
Clinical Medical College of Fujian Medical University, Fuzhou, China.
(17)Department of Cardiology, The Affiliated Hospital of Hangzhou Normal 
University, Hangzhou, China.
(18)School of Public Health, Tongji Medical College, Huazhong University of 
Science and Technology, Wuhan, China.
(19)Department of Radiology, Union Hospital, Tongji Medical College, Huazhong 
University of Science and Technology, Wuhan, China. xkliulang1314@163.com.
(20)Department of Radiology, University of Washington, Seattle, WA, USA. 
zhucheng@uw.edu.
(21)School of Computer Science, Wuhan University, Wuhan, China. dubo@whu.edu.cn.
(22)School of Electronic Information and Communications, Huazhong University of 
Science and Technology, Wuhan, China. xgwang@hust.edu.cn.
(23)Department of Radiology, Union Hospital, Tongji Medical College, Huazhong 
University of Science and Technology, Wuhan, China. hqzcsxh@sina.com.
(#)Contributed equally

Digital subtraction angiography (DSA) devices guide procedures across numerous 
diseases, performed on more than 100,000 patients daily worldwide. However, 
these procedures expose patients and healthcare providers to radiation, 
increasing the risk of health issues. Despite many low-dose DSA imaging methods 
proposed, none have been prospectively clinically validated. In this study, 
46,829 patients (over 5 million DSA images) from 70 centers were used to iterate 
our previously developed generative artificial intelligence system (named 
GenDSA-V2). A total of 1,068 patients (533 in intervention arm and 535 in 
control arm), with suspected cerebral aneurysms (n = 435), lung cancer (n = 417) 
or advanced liver cancer (n = 216), meeting surgical criteria, were enrolled to 
validate the GenDSA-V2. The primary outcome was radiation dose, while secondary 
outcomes included efficiency, operation time and intraoperative complications. 
Group assignments were blinded to patients, surgeons and investigators, while 
technicians were aware but not involved in data collection or analysis. The 
GenDSA-V2 group showed substantially reduced radiation exposure, with an air 
kerma (AK) of 151.3 ± 125.1 mGy compared to 457.4 ± 407.4 mGy in the standard 
clinical protocols (SCP) group (mean difference = -306.1 mGy, 95% confidence 
interval (CI) = -342.3 to -269.9, P < 0.001 for superiority) and a dose-area 
product (DAP) of 4009.7 ± 2767.9 μGy m2 versus 12531.6 ± 9145.9 μGy m2 (mean 
difference = -8521.9 μGy m2, 95% CI = -9333.1 to -7710.7, P < 0.001 for 
superiority). Mean operation time was 33.1 ± 10.8 min in the SCP group and 
34.8 ± 11.8 min in the GenDSA-V2 group (mean difference = 1.7 min, 95% CI = 0.3 
to 3.1, P < 0.001 for noninferiority). Complication rates were similar 
(SCP = 8.1%, GenDSA-V2 = 7.5%, mean difference = -0.6%, 95% CI = -3.8% to 2.6%, 
P < 0.001 for noninferiority). The GenDSA system reduces radiation exposure to 
both physicians and patients by approximately two-thirds during DSA-guided 
procedures, demonstrating substantial clinical and translational value. Chinese 
Clinical Trial Registry: ChiCTR2400084789 .

© 2026. The Author(s), under exclusive licence to Springer Nature America, Inc.

DOI: 10.1038/s41591-025-04042-6
PMID: 41482562 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: X.L. and Y. Li are 
employees of the Institute of Research and Clinical Innovations, Neusoft Medical 
Systems. Y.H. serves as Vice President of Neusoft Medical Systems. The other 
authors declare no competing interests.


299. J Assist Reprod Genet. 2026 Jan 9. doi: 10.1007/s10815-025-03793-y. Online ahead 
of print.

Study of comparative performance of general-purpose LLM-based systems in 
predicting IVF outcomes.

Dinç C(1), Öz ÖF(2), Arıkan SB(2), Doğan S(2), Özekinci M(2), Doğan NU(2), 
Mendilcioğlu İ(2).

Author information:
(1)Department of Gynecology and Obstetrics, Akdeniz University, Antalya, Turkey. 
candinc@akdeniz.edu.tr.
(2)Department of Gynecology and Obstetrics, Akdeniz University, Antalya, Turkey.

BACKGROUND AND OBJECTIVE: Artificial intelligence (AI) has emerged as a 
promising tool for clinical decision support in reproductive medicine, yet the 
performance of general-purpose large language models (LLMs) in predicting in 
vitro fertilization (IVF) outcomes remains insufficiently characterized. This 
exploratory proof-of-concept study aimed to evaluate and compare the 
out-of-the-box performance of three widely accessible LLM-based systems 
(ChatGPT, DeepSeek, and Gemini) in forecasting key clinical and laboratory 
outcomes of IVF treatments.
METHODS: This retrospective single-center study used data from 1473 autologous 
IVF/ICSI cycles, each representing a unique patient. For each cycle, relevant 
clinical and laboratory variables were incorporated into a standardized 
anonymized patient-level vignette and submitted via the publicly available web 
interfaces of three LLMs (ChatGPT, DeepSeek, Gemini) without any fine-tuning or 
internal customization. The models were asked to predict stimulation protocol, 
ovulation trigger type, total and mature oocyte counts, usable embryo counts, 
and clinical pregnancy. Predictive performance was evaluated using accuracy and 
tolerance-based accuracy for categorical and count-based outcomes, mean absolute 
error for numerical predictions, and the area under the receiver operating 
characteristic (ROC) curve for clinical pregnancy.
RESULTS: Gemini achieved the highest accuracy in predicting stimulation 
protocols (51.26%) and embryo counts (68.22%), while DeepSeek demonstrated the 
lowest numerical error for oocyte count predictions. Clinical pregnancy 
prediction was the most challenging task; all models showed only moderate 
discrimination, with Gemini achieving the highest AUC (0.711), followed by 
ChatGPT (0.690) and DeepSeek (0.676). Overall, model performance varied 
considerably across tasks and remained below thresholds that would be considered 
sufficient for reliable stand-alone clinical use.
CONCLUSIONS: In this exploratory proof-of-concept setting, general-purpose AI 
systems showed variable and overall suboptimal performance in predicting IVF 
outcomes from standardized clinical vignettes. Although certain models 
demonstrated relative strengths in specific tasks, none reached the reliability, 
consistency, or interpretability required for safe clinical implementation. 
These findings indicate that, in their current form, such models should not be 
used as clinical decision-support tools for IVF decision-making and that their 
use should remain restricted to carefully controlled research settings until 
they have been prospectively validated in multicenter cohorts and systematically 
compared with rigorously developed, task-specific prediction models. This study 
provides comparative insight into how these AI systems behave in IVF-related 
prediction tasks and underscores the need for cautious interpretation of 
AI-generated outputs.

© 2026. The Author(s).

DOI: 10.1007/s10815-025-03793-y
PMID: 41513919

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Approval for this study was obtained from the Institutional Ethics 
Committee (Approval number: TBAEK-370, approval date: 24 April 2025). As this 
was a retrospective study using anonymized data, the requirement for informed 
consent was waived. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


300. Nutrients. 2025 Nov 19;17(22):3613. doi: 10.3390/nu17223613.

Image-Based Dietary Energy and Macronutrients Estimation with ChatGPT-5: 
Cross-Source Evaluation Across Escalating Context Scenarios.

Rodríguez-Jiménez M(1), Martín-Del-Campo-Becerra GD(2), Sumalla-Cano S(1)(3)(4), 
Crespo-Álvarez J(5), Elio I(1)(6)(7).

Author information:
(1)Research Group on Foods, Nutritional Biochemistry and Health, Universidad 
Europea del Atlántico, 39011 Santander, Spain.
(2)German Aerospace Center (DLR), 82234 Weßling, Germany.
(3)Faculty of Health Sciences, Universidade do Cuanza, Cuito EN250, Bié, Angola.
(4)Faculty of Health Sciences, Universidad de La Romana, La Romana 22000, 
Dominican Republic.
(5)Higher Polytechnic School, Universidad Europea del Atlántico, 39011 
Santander, Spain.
(6)Faculty of Health Sciences, Universidad Internacional Iberoamericana, 
Arecibo, PR 00613, USA.
(7)Department of Health, Nutrition and Sport, Universidad Internacional 
Iberoamericana, Campeche 24560, Mexico.

Background/Objectives: Estimating energy and macronutrients from food images is 
clinically relevant yet challenging, and rigorous evaluation requires 
transparent accuracy metrics with uncertainty and clear acknowledgement of 
reference data limitations across heterogeneous sources. This study assessed 
ChatGPT-5, a general-purpose vision-language model, across four scenarios 
differing in the amount and type of contextual information provided, using a 
composite dataset to quantify accuracy for calories and macronutrients. Methods: 
A total of 195 dishes were evaluated, sourced from Allrecipes.com, the SNAPMe 
dataset, and Home-prepared, weighed meals. Each dish was evaluated under Case 1 
(image only), Case 2 (image plus standardized non-visual descriptors), Case 3 
(image plus ingredient lists with amounts), and Case 4 (replicates Case 3 but 
excluding the image). The primary endpoint was kcal Mean Absolute Error (MAE); 
secondary endpoints included Median Absolute Error (MedAE) and Root Mean Square 
Error (RMSE) for kcal and macronutrients (protein, carbohydrates, and lipids), 
all reported with 95% Confidence Intervals (CIs) via dish-level bootstrap 
resampling and accompanied by absolute differences (Δ) between scenarios. 
Inference settings were standardized to support reproducibility and variance 
estimation. Source stratified analyses and quartile summaries were conducted to 
examine heterogeneity by curation level and nutrient ranges, with additional 
robustness checks for error complexity relationships. Results and Discussion: 
Accuracy improved from Case 1 to Case 2 and further in Case 3 for energy and all 
macronutrients when summarized by MAE, MedAE, and RMSE with 95% CIs, with 
absolute reductions (Δ) indicating material gains as contextual information 
increased. In contrast to Case 3, estimation accuracy declined in Case 4, 
underscoring the contribution of visual cues. Gains were largest in the 
Home-prepared dietitian-weighed subset and smaller yet consistent for 
Allrecipes.com and SNAPMe, reflecting differences in reference curation and 
measurement fidelity across sources. Scenario-level trends were concordant 
across sources, and stratified and quartile analyses showed coherent patterns of 
decreasing absolute errors with the provision of structured non-visual 
information and detailed ingredient data. Conclusions: ChatGPT-5 can deliver 
practically useful calorie and macronutrient estimates from food images, 
particularly when augmented with standardized nonvisual descriptors and detailed 
ingredients, as evidenced by reductions in MAE, MedAE, and RMSE with 95% CIs 
across scenarios. The decline in accuracy observed when the image was omitted, 
despite providing detailed ingredient information, indicates that visual cues 
contribute meaningfully to estimation performance and that improvements are not 
solely attributable to arithmetic from ingredient lists. Finally, to promote 
generalizability, it is recommended that future studies include repeated 
evaluations across diverse datasets, ensure public availability of prompts and 
outputs, and incorporate systematic comparisons with non-artificial-intelligence 
baselines.

DOI: 10.3390/nu17223613
PMCID: PMC12655113
PMID: 41305663 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


301. Global Spine J. 2025 Sep;15(7):3199-3220. doi: 10.1177/21925682251321837. Epub 
2025 Feb 17.

Evaluating Artificial Intelligence in Spinal Cord Injury Management: A 
Comparative Analysis of ChatGPT-4o and Google Gemini Against American College of 
Surgeons Best Practices Guidelines for Spine Injury.

Yu A(1), Li A(1), Ahmed W(1), Saturno M(1), Cho SK(1).

Author information:
(1)Department of Orthopaedics, Icahn School of Medicine at Mount Sinai, New 
York, NY, USA.

Study DesignComparative Analysis.ObjectivesThe American College of Surgeons 
developed the 2022 Best Practice Guidelines to provide evidence-based 
recommendations for managing spinal injuries. This study aims to assess the 
concordance of ChatGPT-4o and Gemini Advanced with the 2022 ACS Best Practice 
Guidelines, offering the first expert evaluation of these models in managing 
spinal cord injuries.MethodsThe 2022 ACS Trauma Quality Program Best Practices 
Guidelines for Spine Injury were used to create 52 questions based on key 
clinical recommendations. These were grouped into informational (8), diagnostic 
(14), and treatment (30) categories and posed to ChatGPT-4o and Google Gemini 
Advanced. Responses were graded for concordance with ACS guidelines and 
validated by a board-certified spine surgeon.ResultsChatGPT was concordant with 
ACS guidelines on 38 of 52 questions (73.07%) and Gemini on 36 (69.23%). Most 
non-concordant answers were due to insufficient information. The models 
disagreed on 8 questions, with ChatGPT concordant in 5 and Gemini in 3. Both 
achieved 75% concordance on clinical information; Gemini outperformed on 
diagnostics (78.57% vs 71.43%), while ChatGPT had higher concordance on 
treatment questions (73.33% vs 63.33%).ConclusionsChatGPT-4o and Gemini Advanced 
demonstrate potential as valuable assets in spinal injury management by 
providing responses aligned with current best practices. The marginal 
differences in concordance rates suggest that neither model exhibits a superior 
ability to deliver recommendations concordant with validated clinical 
guidelines. Despite LLMs increasing sophistication and utility, existing 
limitations currently prevent them from being clinically safe and practical in 
trauma-based settings.

DOI: 10.1177/21925682251321837
PMCID: PMC11833805
PMID: 39959933

Conflict of interest statement: Declaration of Conflicting InterestsThe 
author(s) declared no potential conflicts of interest with respect to the 
research, authorship, and/or publication of this article.


302. Front Psychol. 2025 Sep 17;16:1644209. doi: 10.3389/fpsyg.2025.1644209. 
eCollection 2025.

Research on the influencing factors of generative artificial intelligence usage 
intent in post-secondary education: an empirical analysis based on the AIDUA 
extended model.

Bai X(1), Yang L(1).

Author information:
(1)School of Journalism and New Media, Xi'an Jiaotong University, Xi'an, China.

OBJECTIVE: Generative Artificial Intelligence (AIGC) presents a profound 
dialectic in higher education: its transformative potential is challenged by 
deep-seated psychological and ethical barriers. Traditional adoption models fail 
to capture this complexity. To bridge this gap, this study develops and tests an 
integrated cognitive-behavioral framework. We posit that AIGC acceptance is a 
three-stage cognitive appraisal process. By embedding an extended AIDUA model-a 
framework specifically tailored to the unique challenges of AI adoption-within 
Cognitive Appraisal Theory, we investigate how novel antecedent dimensions 
(Socio-Ethical: ethical risk, explainability; Techno-Performance: generation 
quality, context-awareness) and classical factors (social influence, hedonic 
motivation, anthropomorphism) shape core technological beliefs (Performance & 
Effort Expectancy), which in turn mediate the path to acceptance intention via 
emotion. Furthermore, the moderating roles of gender, academic background, 
ethnicity, and political affiliation are systematically examined to test the 
model's boundary conditions.
METHODS: The model was empirically validated using Structural Equation Modeling 
and multi-group analysis on survey data from 462 university students across 15 
diverse institutions in China.
RESULTS: The findings reveal that the cognitive appraisal of AIGC is primarily 
driven by its perceived capabilities and safety. Techno-Performance (generation 
quality, β = 0.53) and Socio-Ethical (explainability, β = 0.41; ethical risk, 
β = -0.25) dimensions were the most powerful predictors of Performance 
Expectancy. These intrinsic appraisals significantly outweighed the influence of 
external social cues. Notably, ethical risk perception operated as a 
dual-threat, not only lowering performance expectations but also significantly 
amplifying the perceived cognitive burden (Effort Expectancy, β = 0.33). 
Multi-group analyses confirmed that these appraisal pathways are systematically 
moderated by individual and cultural background variables, highlighting 
significant heterogeneity in user responses.
DISCUSSION: This study makes a critical theoretical contribution by 
demonstrating how core technological expectancies are formed through a 
multi-stage appraisal of utility, ethics, and experience, moving beyond mere 
identification of influential factors. The findings dismantle the myth of a 
universal "student user," revealing that AIGC adoption is a culturally and 
contextually embedded process. Practically, the results provide an 
evidence-based roadmap for university policymakers and AIGC developers, 
emphasizing that fostering trust and adoption requires a dual focus: maximizing 
technological prowess while actively mitigating perceived ethical and cognitive 
costs through enhanced transparency and user-centric design.

Copyright © 2025 Bai and Yang.

DOI: 10.3389/fpsyg.2025.1644209
PMCID: PMC12484213
PMID: 41040103

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


303. World J Mens Health. 2025 Oct;43(4):918-933. doi: 10.5534/wjmh.240173. Epub 2024 
Dec 2.

The In-depth Comparative Analysis of Four Large Language AI Models for Risk 
Assessment and Information Retrieval from Multi-Modality Prostate Cancer Work-up 
Reports.

Yuan LH(#)(1)(2), Huang SW(#)(2)(3), Chou D(1)(4)(5)(6), Tsai CY(7)(8).

Author information:
(1)Department of Biomedical Engineering, National Cheng-Kung University, Tainan, 
Taiwan.
(2)Department of Urology, National Taiwan University Hospital, Yunlin Branch, 
Yunlin, Taiwan.
(3)Department of Urology, National Taiwan University Hospital, Taipei, Taiwan.
(4)Miin Wu School of Computing, National Cheng-Kung University, Tainan, Taiwan.
(5)Academy of Innovative Semiconductor and Sustainable Manufacturing, National 
Cheng-Kung University, Tainan, Taiwan.
(6)National Center for High-Performance Computing, Hsinchu, Taiwan. 
dean@gs.ncku.edu.tw.
(7)Division of Urology, Department of Surgery, Far Eastern Memorial Hospital, 
New Taipei, Taiwan.
(8)Department of Electrical Engineering, Yuan Ze University, Taoyuan, Taiwan. 
pgtsai@gmail.com.
(#)Contributed equally

PURPOSE: Information retrieval (IR) and risk assessment (RA) from multi-modality 
imaging and pathology reports are critical to prostate cancer (PC) treatment. 
This study aims to evaluate the performance of four general-purpose large 
language model (LLMs) in IR and RA tasks.
MATERIALS AND METHODS: We conducted a study using simulated text reports from 
computed tomography, magnetic resonance imaging, bone scans, and biopsy 
pathology on stage IV PC patients. We assessed four LLMs (ChatGPT-4-turbo, 
Claude-3-opus, Gemini-Pro-1.0, ChatGPT-3.5-turbo) on three RA tasks (LATITUDE, 
CHAARTED, TwNHI) and seven IR tasks. It included TNM staging, and the detection 
and quantification of bone and visceral metastases, providing a broad evaluation 
of their capabilities in handling diverse clinical data. We queried LLMs with 
multi-modality reports using zero-shot chain-of-thought prompting via 
application programming interface. With three adjudicators' consensus as the 
gold standard, these models' performances were assessed through repeated 
single-round queries and ensemble voting methods, using 6 outcome metrics.
RESULTS: Among 350 stage IV PC patients with simulated reports, 115 (32.9%), 128 
(36.6%), and 94 (26.9%) belonged to LATITUDE, CHAARTED, and TwNHI high-risk, 
respectively. Ensemble voting, based on three repeated single-round queries, 
consistently enhances accuracy with a higher likelihood of achieving 
non-inferior results compared to a single query. Four models showed minimal 
differences in IR tasks with high accuracy (87.4%-94.2%) and consistency 
(ICC>0.8) in TNM staging. However, there were significant differences in RA 
performance, with the ranking as follows: ChatGPT-4-turbo, Claude-3-opus, 
Gemini-Pro-1.0, and ChatGPT-3.5-turbo, respectively. ChatGPT-4-turbo achieved 
the highest accuracy (90.1%, 90.7%,91.6%), and consistency (ICC 0.86, 0.93, 
0.76) across 3 RA tasks.
CONCLUSIONS: ChatGPT-4-turbo demonstrated satisfactory accuracy and outcomes in 
RA and IR for stage IV PC, suggesting its potential for clinical decision 
support. However, the risks of misinterpretation impacting decision-making 
cannot be overlooked. Further research is necessary to validate these findings 
in other cancers.

Copyright © 2025 Korean Society for Sexual Medicine and Andrology.

DOI: 10.5534/wjmh.240173
PMCID: PMC12505481
PMID: 39743220

Conflict of interest statement: The authors have nothing to disclose.


304. JMIR Med Educ. 2025 Aug 29;11:e73419. doi: 10.2196/73419.

Development and Validation of a Large Language Model-Based System for Medical 
History-Taking Training: Prospective Multicase Study on Evaluation Stability, 
Human-AI Consistency, and Transparency.

Liu Y(#)(1), Shi C(#)(1), Wu L(1), Lin X(1), Chen X(1), Zhu Y(1), Tan H(#)(2), 
Zhang W(#)(1).

Author information:
(1)Medical Simulation Center, Shantou University Medical College, No. 22 Xinling 
Road, Shantou, 515041, China, 86 754-88900459.
(2)Department of Medical Physics and Informatics, Shantou University Medical 
College, Shantou, China.
(#)Contributed equally

BACKGROUND: History-taking is crucial in medical training. However, current 
methods often lack consistent feedback and standardized evaluation and have 
limited access to standardized patient (SP) resources. Artificial intelligence 
(AI)-powered simulated patients offer a promising solution; however, challenges 
such as human-AI consistency, evaluation stability, and transparency remain 
underexplored in multicase clinical scenarios.
OBJECTIVE: This study aimed to develop and validate the AI-Powered Medical 
History-Taking Training and Evaluation System (AMTES), based on DeepSeek-V2.5 
(DeepSeek), to assess its stability, human-AI consistency, and transparency in 
clinical scenarios with varying symptoms and difficulty levels.
METHODS: We developed AMTES, a system using multiple strategies to ensure dialog 
quality and automated assessment. A prospective study with 31 medical students 
evaluated AMTES's performance across 3 cases of varying complexity: a simple 
case (cough), a moderate case (frequent urination), and a complex case 
(abdominal pain). To validate our design, we conducted systematic baseline 
comparisons to measure the incremental improvements from each level of our 
design approach and tested the framework's generalizability by implementing it 
with an alternative large language model (LLM) Qwen-Max (Qwen AI; version 
20250409), under a zero-modification condition.
RESULTS: A total of 31 students practiced with our AMTES. During the training, 
students generated 8606 questions across 93 history-taking sessions. AMTES 
achieved high dialog accuracy: 98.6% (SD 1.5%) for cough, 99.0% (SD 1.1%) for 
frequent urination, and 97.9% (SD 2.2%) for abdominal pain, with contextual 
appropriateness exceeding 99%. The system's automated assessments demonstrated 
exceptional stability and high human-AI consistency, supported by transparent, 
evidence-based rationales. Specifically, the coefficients of variation (CV) were 
low across total scores (0.87%-1.12%) and item-level scoring (0.55%-0.73%). 
Total score consistency was robust, with the intraclass correlation coefficients 
(ICCs) exceeding 0.923 across all scenarios, showing strong agreement. The 
item-level consistency was remarkably high, consistently above 95%, even for 
complex cases like abdominal pain (95.75% consistency). In systematic baseline 
comparisons, the fully-processed system improved ICCs from 0.414/0.500 to 
0.923/0.972 (moderate and complex cases), with all CVs ≤1.2% across the 3 cases. 
A zero-modification implementation of our evaluation framework with an 
alternative LLM (Qwen-Max) achieved near-identical performance, with the 
item-level consistency rates over 94.5% and ICCs exceeding 0.89. Overall, 87% of 
students found AMTES helpful, and 83% expressed a desire to use it again in the 
future.
CONCLUSIONS: Our data showed that AMTES demonstrates significant educational 
value through its LLM-based virtual SPs, which successfully provided authentic 
clinical dialogs with high response accuracy and delivered consistent, 
transparent educational feedback. Combined with strong user approval, these 
findings highlight AMTES's potential as a valuable, adaptable, and generalizable 
tool for medical history-taking training across various educational contexts.

© Yang Liu, Chujun Shi, Liping Wu, Xiule Lin, Xiaoqin Chen, Yiying Zhu, Haizhu 
Tan, Weishan Zhang. Originally published in JMIR Medical Education 
(https://mededu.jmir.org).

DOI: 10.2196/73419
PMCID: PMC12396829
PMID: 40882613 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


305. BMC Med Educ. 2025 Nov 26;25(1):1657. doi: 10.1186/s12909-025-08263-8.

Knowledge-level comparison in pulpal and periapical diseases: dental students 
versus artificial intelligence models (Gemini, Microsoft Copilot, ChatGPT-3.5, 
ChatGPT-4o): cross-sectional study.

Kurt Ö(1), Şimsek E(2).

Author information:
(1)Faculty of Dentistry, Department of Endodontics, Aksaray University, 
Bahçesaray Neighborhood, Necmettin Erbakan Boulevard, Campus Road, Aksaray, 
68100, Turkey. Ozgekrtrs@gmail.com.
(2)Faculty of Dentistry, Department of Endodontics, Mersin University, Mersin, 
Turkey.

BACKGROUND: This study explored the diagnostic accuracy of artificial 
intelligence (AI) chatbots and dental students when responding to questions 
related to pulpal and periapical diseases. Rapid advancements in AI have led to 
increased interest in their applicability to clinical education and 
decision-making in dentistry.
OBJECTIVE: To compare the accuracy rates of responses given by dental students 
and various AI-based chatbots (ChatGPT-3.5, ChatGPT-4o, Gemini, and Microsoft 
Copilot) to multiple-choice questions designed to assess knowledge related to 
pulpal and periapical diseases.
METHODS: The study included third- and fifth-year dental students representing 
different levels of clinical training, along with four distinct AI-based 
chatbots. A total of 327 responses were collected from students, while each 
chatbot generated 450 responses. The evaluation was based on 15 multiple-choice 
questions developed in accordance with the 2020 version of the American 
Association of Endodontists (AAE) clinical guidelines. The accuracy rates of the 
groups were compared using descriptive statistics, one-way ANOVA, Bonferroni 
post hoc tests for significant differences, and Chi-square tests for correct 
versus incorrect response ratios.
RESULTS: The highest accuracy rate was observed among fifth-year dental students 
(85.1%), followed by ChatGPT-4o (79.6%), ChatGPT-3.5 (75.1%), Gemini (71.6%), 
third-year students (64.9%), and Microsoft Copilot (61.3%). A statistically 
significant difference was found among the groups (p < 0.05). ChatGPT-4o 
demonstrated a comparable accuracy rate to fifth-year students with more 
clinical experience (p > 0.05), whereas other chatbots and third-year students 
showed lower performance.
CONCLUSION: Chatbots exhibited varying levels of accuracy in diagnosing pulpal 
and periapical diseases. ChatGPT-4o performed at a level similar to that of more 
clinically experienced students, suggesting its potential as a supportive tool 
in dental education and clinical decision support systems. However, the 
relatively lower accuracy rates of models such as Gemini and Microsoft Copilot 
underscore the continued importance of human expertise. These findings suggest 
that while AI systems may serve as complementary tools in education, they cannot 
fully replace clinical judgment grounded in human experience.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-08263-8
PMCID: PMC12659035
PMID: 41299627 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was approved by the Mersin University Clinical Research 
Ethics Committee (Protocol No: 2025/613). All procedures were conducted in 
accordance with the ethical standards of the Declaration of Helsinki. Informed 
consent was obtained from all participants prior to their inclusion in the 
study. Consent for publication: Not applicable. Competing interests: The authors 
declare no competing interests.


306. J Med Syst. 2025 May 7;49(1):59. doi: 10.1007/s10916-025-02188-x.

ChatOCT: Embedded Clinical Decision Support Systems for Optical Coherence 
Tomography in Offline and Resource-Limited Settings.

Liu C(1), Zhang H(1), Zheng Z(2)(3), Liu W(2)(3), Gu C(1), Lan Q(1), Zhang W(1), 
Yang J(4).

Author information:
(1)School of Biomedical Engineering, Shanghai Jiao Tong University, Xuhui 
District, No. 3 Teaching Building, 1954 Huashan RD, Shanghai, China.
(2)Department of Ophthalmology, Shanghai General Hospital, Shanghai, China.
(3)National Clinical Research Center for Eye Diseases, Shanghai, China.
(4)School of Biomedical Engineering, Shanghai Jiao Tong University, Xuhui 
District, No. 3 Teaching Building, 1954 Huashan RD, Shanghai, China. 
jyangoptics@gmail.com.

Optical Coherence Tomography (OCT) is a critical imaging modality for diagnosing 
ocular and systemic conditions, yet its accessibility is hindered by the need 
for specialized expertise and high computational demands. To address these 
challenges, we introduce ChatOCT, an offline-capable, domain-adaptive clinical 
decision support system (CDSS) that integrates structured expert Q&A generation, 
OCT-specific knowledge injection, and activation-aware model compression. Unlike 
existing systems, ChatOCT functions without internet access, making it suitable 
for low-resource environments. ChatOCT is built upon LLaMA-2-7B, incorporating 
domain-specific knowledge from PubMed and OCT News through a two-stage training 
process: (1) knowledge injection for OCT-specific expertise and (2) Q&A 
instruction tuning for structured, interactive diagnostic reasoning. To ensure 
feasibility in offline environments, we apply activation-aware weight 
quantization, reducing GPU memory usage to ~ 4.74 GB, enabling deployment on 
standard OCT hardware. A novel expert answer generation framework mitigates 
hallucinations by structuring responses in a multi-step process, ensuring 
accuracy and interpretability. ChatOCT outperforms state-of-the-art baselines 
such as LLaMA-2, PMC-LLaMA-13B, and ChatDoctor by 10-15 points in coherence, 
relevance, and clinical utility, while reducing GPU memory requirements by 79%, 
while maintaining real-time responsiveness (~ 20 ms inference time). Expert 
ophthalmologists rated ChatOCT's outputs as clinically actionable and aligned 
with real-world decision-making needs, confirming its potential to assist 
frontline healthcare providers. ChatOCT represents an innovative offline 
clinical decision support system for optical coherence tomography (OCT) that 
runs entirely on local embedded hardware, enabling real-time analysis in 
resource-limited settings without internet connectivity. By offering a scalable, 
generalizable pipeline that integrates knowledge injection, instruction tuning, 
and model compression, ChatOCT provides a blueprint for next-generation, 
resource-efficient clinical AI solutions across multiple medical domains.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02188-x
PMID: 40332685 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Human Ethics and Consent to 
Participate Declarations: Not applicable. Competing interests: The authors 
declare no competing interests. Disclosures: All the authors have nothing to 
disclose and declare that the research was conducted in the absence of any 
commercial or financial relationships that could be construed as a potential 
conflict of interest.


307. Int J Med Inform. 2026 Jan 10;209:106246. doi: 10.1016/j.ijmedinf.2025.106246. 
Online ahead of print.

An old disease, a new linguistic challenge for large language models: patient 
education on psoriasis and psoriatic arthritis in an underrepresented medical 
language.

Atilan AU(1), Cetin N(2).

Author information:
(1)Pamukkale University, School of Medicine, Department of Dermatology, Denizli, 
Turkey. Electronic address: auatilan@pau.edu.tr.
(2)Pamukkale University, School of Medicine, Department of Dermatology, Denizli, 
Turkey.

OBJECTIVE: Large Language Models (LLMs) are increasingly applied to patient 
education, yet their performance in languages that are relatively 
underrepresented in medical-domain corpora and large language model training 
datasets remains underexplored. Psoriasis and psoriatic arthritis (PsA) are 
chronic, immune-mediated diseases requiring lifelong patient engagement, making 
them suitable conditions to evaluate the clarity, reliability, and inclusivity 
of AI-generated educational content. To assess the comprehensibility, scientific 
reliability, and patient-centered communication of Turkish patient education 
materials for psoriasis vulgaris and PsA generated by seven state-of-the-art 
LLMs.
METHODS: A cross-sectional analysis compared outputs from ChatGPT-4o, Gemini 2.0 
Flash, Claude 3.7 Sonnet, Grok 3, Qwen 2.5, DeepSeek R1, and Mistral Large 2. 
Brochures were produced using standardized zero-shot prompts and evaluated via 
the Ateşman readability index and the DISCERN instrument. Overall differences in 
DISCERN scores across the seven models were assessed using a Friedman test, 
followed by Bonferroni-adjusted Wilcoxon signed-rank post-hoc analyses.
RESULTS: Readability scores ranged from 61.6 to 80.2 (mean = 71.3 ± 6.9), with 
ChatGPT-4o and Qwen 2.5 generating the most accessible texts. DISCERN 
reliability scores ranged from 38.5 to 60.5, with Claude 3.7 Sonnet and Gemini 
2.0 Flash showing the highest accuracy. Models prioritizing factual precision 
produced denser language, while conversational models favored fluency but 
sacrificed depth. Notable variation was observed, with only Claude 3.7 Sonnet 
and Gemini 2.0 Flash consistently reflecting patient-centered perspectives.
CONCLUSION: LLMs showed observable differences in balancing clarity and 
reliability when generating health education leaflets in Turkish. Most outputs 
appeared to lack explicit psychosocial framing and emphasis on shared 
decision-making, which may suggest the need for more culturally adaptive 
training, clinician oversight, and locally grounded validation frameworks to 
support safe and inclusive AI-based patient education.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106246
PMID: 41534240

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


308. Front Immunol. 2025 Apr 17;16:1531930. doi: 10.3389/fimmu.2025.1531930. 
eCollection 2025.

Comprehensive molecular analyses of an autoimmune-related gene predictive model 
and immune infiltrations using machine learning methods in intracranial 
aneurysma.

Zhang M(1), Zhou L(1), Zhao Y(1), Wang Y(1), Zhang Z(1), Liu Z(1).

Author information:
(1)Department of Neurology, The Fourth Affiliated Hospital of Harbin Medical 
University, Harbin, China.

BACKGROUND: Increasing evidence indicates a connection between intracranial 
aneurysm (intracranial aneurysm, IA) and autoimmune diseases. However, the 
molecular mechanisms from a genetic perspective remain unclear. This study aims 
to elucidate the potential roles of autoimmune-related genes (ARGs) in the 
pathogenesis of IA.
METHODS: Three transcription profiles (GSE13353, GSE26969, and GSE75436) for 
intracranial aneurysm (IA) were obtained from GEO databases. Autoimmune-related 
genes (ARGs) were sourced from the Genecards databases. Differentially expressed 
ARGs (DEARGs) were identified using the "limma" R package. GO, KEGG and GSEA 
analyses were performed to uncover underlying molecular functions. Three machine 
learning methods-LASSO logistic regression, random forest (RF), and XGBoost-were 
employed to identify key genes. An artificial neural network was used to develop 
an autoimmune-related signature predictive model for IA. Immune characteristics, 
including immune cell infiltration, immune responses, and HLA gene expression in 
IA, were investigated using ssGSEA. Additionally, the miRNA-gene regulatory 
network and potential therapeutic drugs for hub genes were predicted. In certain 
sections of the written content of this manuscript, the authors have utilized 
text generated by an AI technology. The specific name, version, model, and 
source of the generative AI technology used are as follows: Generative AI 
Technology Name: ChatGPT, Version: 4.0, Model: GPT-4, Source: OpenAI.
RESULTS: A total of 39 differentially expressed ARGs (DEARGs) were identified 
across the GSE13353, GSE26969, and GSE75436 datasets. From these, two key 
diagnostic genes were identified using three machine learning algorithms: ADIPOQ 
and IL21R. A predictive neural network model was developed based on these genes, 
exhibiting strong diagnostic capability with a ROC value of 0.944, and further 
validated using a nomogram approach. The study focused on intracranial aneurysm 
(IA), revealing significant insights into the underlying genetic mechanisms.
CONCLUSION: The results of bioinformatics analysis in our study elucidated the 
mechanism of intracranial aneurysm (IA), identifying two key differential genes. 
Our research highlights the significant roles of immune infiltration and the 
regulatory networks between genes, miRNAs, and drugs in IA. These findings not 
only enhance our understanding of the pathogenesis of IA but also suggest 
potential new avenues for its treatment.

Copyright © 2025 Zhang, Zhou, Zhao, Wang, Zhang and Liu.

DOI: 10.3389/fimmu.2025.1531930
PMCID: PMC12043717
PMID: 40313967 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


309. JMIR Med Inform. 2025 Feb 13;13:e68139. doi: 10.2196/68139.

Transforming Informed Consent Generation Using Large Language Models: Mixed 
Methods Study.

Shi Q(1), Luzuriaga K(1), Allison JJ(2), Oztekin A(3), Faro JM(2), Lee JL(2), 
Hafer N(1), McManus M(1), Zai AH(1)(2).

Author information:
(1)Center for Clinical and Translational Science, University of Massachusetts 
Chan Medical School, 55 N Lake Ave, Worcester, MA, 01655, United States, 1 
508-856-1952.
(2)Department of Population and Quantitative Health Sciences, University of 
Massachusetts Chan Medical School, Worcester, MA, United States.
(3)Manning School of Business, University of Massachusetts Lowell, Lowell, MA, 
United States.

BACKGROUND: Informed consent forms (ICFs) for clinical trials have become 
increasingly complex, often hindering participant comprehension and engagement 
due to legal jargon and lengthy content. The recent advances in large language 
models (LLMs) present an opportunity to streamline the ICF creation process 
while improving readability, understandability, and actionability.
OBJECTIVES: This study aims to evaluate the performance of the Mistral 8x22B LLM 
in generating ICFs with improved readability, understandability, and 
actionability. Specifically, we evaluate the model's effectiveness in generating 
ICFs that are readable, understandable, and actionable while maintaining the 
accuracy and completeness.
METHODS: We processed 4 clinical trial protocols from the institutional review 
board of UMass Chan Medical School using the Mistral 8x22B model to generate key 
information sections of ICFs. A multidisciplinary team of 8 evaluators, 
including clinical researchers and health informaticians, assessed the generated 
ICFs against human-generated counterparts for completeness, accuracy, 
readability, understandability, and actionability. Readability, 
Understandability, and Actionability of Key Information indicators, which 
include 18 binary-scored items, were used to evaluate these aspects, with higher 
scores indicating greater accessibility, comprehensibility, and actionability of 
the information. Statistical analysis, including Wilcoxon rank sum tests and 
intraclass correlation coefficient calculations, was used to compare outputs.
RESULTS: LLM-generated ICFs demonstrated comparable performance to 
human-generated versions across key sections, with no significant differences in 
accuracy and completeness (P>.10). The LLM outperformed human-generated ICFs in 
readability (Readability, Understandability, and Actionability of Key 
Information score of 76.39% vs 66.67%; Flesch-Kincaid grade level of 7.95 vs 
8.38) and understandability (90.63% vs 67.19%; P=.02). The LLM-generated content 
achieved a perfect score in actionability compared with the human-generated 
version (100% vs 0%; P<.001). Intraclass correlation coefficient for evaluator 
consistency was high at 0.83 (95% CI 0.64-1.03), indicating good reliability 
across assessments.
CONCLUSIONS: The Mistral 8x22B LLM showed promising capabilities in enhancing 
the readability, understandability, and actionability of ICFs without 
sacrificing accuracy or completeness. LLMs present a scalable, efficient 
solution for ICF generation, potentially enhancing participant comprehension and 
consent in clinical trials.

© Qiming Shi, Katherine Luzuriaga, Jeroan J Allison, Asil Oztekin, Jamie M Faro, 
Joy L Lee, Nathaniel Hafer, Margaret McManus, Adrian H Zai. Originally published 
in JMIR Medical Informatics (https://medinform.jmir.org).

DOI: 10.2196/68139
PMCID: PMC11841745
PMID: 39946400 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


310. J Clin Pathol. 2025 Feb 18;78(3):202-207. doi: 10.1136/jcp-2023-209304.

Accuracy of GPT-4 in histopathological image detection and classification of 
colorectal adenomas.

Laohawetwanit T(1)(2), Namboonlue C(3), Apornvirat S(4)(2).

Author information:
(1)Division of Pathology, Chulabhorn International College of Medicine, 
Thammasat University, Pathum Thani, Thailand thiyapat@tu.ac.th.
(2)Division of Pathology, Thammasat University Hospital, Pathum Thani, Thailand.
(3)Dr Pong Clinic, Bangkok, Thailand.
(4)Division of Pathology, Chulabhorn International College of Medicine, 
Thammasat University, Pathum Thani, Thailand.

AIMS: To evaluate the accuracy of Chat Generative Pre-trained Transformer 
(ChatGPT) powered by GPT-4 in histopathological image detection and 
classification of colorectal adenomas using the diagnostic consensus provided by 
pathologists as a reference standard.
METHODS: A study was conducted with 100 colorectal polyp photomicrographs, 
comprising an equal number of adenomas and non-adenomas, classified by two 
pathologists. These images were analysed by classic GPT-4 for 1 time in October 
2023 and custom GPT-4 for 20 times in December 2023. GPT-4's responses were 
compared against the reference standard through statistical measures to evaluate 
its proficiency in histopathological diagnosis, with the pathologists further 
assessing the model's descriptive accuracy.
RESULTS: GPT-4 demonstrated a median sensitivity of 74% and specificity of 36% 
for adenoma detection. The median accuracy of polyp classification varied, 
ranging from 16% for non-specific changes to 36% for tubular adenomas. Its 
diagnostic consistency, indicated by low kappa values ranging from 0.06 to 0.11, 
suggested only poor to slight agreement. All of the microscopic descriptions 
corresponded with their diagnoses. GPT-4 also commented about the limitations in 
its diagnoses (eg, slide diagnosis best done by pathologists, the inadequacy of 
single-image diagnostic conclusions, the need for clinical data and a higher 
magnification view).
CONCLUSIONS: GPT-4 showed high sensitivity but low specificity in detecting 
adenomas and varied accuracy for polyp classification. However, its diagnostic 
consistency was low. This artificial intelligence tool acknowledged its 
diagnostic limitations, emphasising the need for a pathologist's expertise and 
additional clinical context.

© Author(s) (or their employer(s)) 2025. No commercial re-use. See rights and 
permissions. Published by BMJ Group.

DOI: 10.1136/jcp-2023-209304
PMID: 38199797 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: None declared.


311. J Med Internet Res. 2025 Sep 18;27:e73603. doi: 10.2196/73603.

Large Language Models' Clinical Decision-Making on When to Perform a Kidney 
Biopsy: Comparative Study.

Toal M(1), Hill C(2), Quinn M(1), O'Neill C(1), Maxwell AP(1).

Author information:
(1)Centre for Public Health, Royal Victoria Hospital, Queen's University 
Belfast, Grosvenor Road, Belfast, BT12 6BA, United Kingdom, 44 28 9097 6350.
(2)Regional Centre for Nephrology and Transplantation, Belfast City Hospital, 
Belfast, United Kingdom.

BACKGROUND: Artificial intelligence (AI) and large language models (LLMs) are 
increasing in sophistication and are being integrated into many disciplines. The 
potential for LLMs to augment clinical decision-making is an evolving area of 
research.
OBJECTIVE: This study compared the responses of over 1000 kidney specialist 
physicians (nephrologists) with the outputs of commonly used LLMs using a 
questionnaire determining when a kidney biopsy should be performed.
METHODS: This research group completed a large online questionnaire for 
nephrologists to determine when a kidney biopsy should be performed. The 
questionnaire was co-designed with patient input, refined through multiple 
iterations, and piloted locally before international dissemination. It was the 
largest international study in the field and demonstrated variation among human 
clinicians in biopsy propensity relating to human factors such as sex and age, 
as well as systemic factors such as country, job seniority, and technical 
proficiency. The same questions were put to both human doctors and LLMs in an 
identical order in a single session. Eight commonly used LLMs were interrogated: 
ChatGPT-3.5, Mistral Hugging Face, Perplexity, Microsoft Copilot, Llama 2, 
GPT-4, MedLM, and Claude 3. The most common response given by clinicians (human 
mode) for each question was taken as the baseline for comparison. Questionnaire 
responses on the indications and contraindications for biopsy generated a score 
(0-44) reflecting biopsy propensity, in which a higher score was used as a 
surrogate marker for an increased tolerance of potential associated risks.
RESULTS: The ability of LLMs to reproduce human expert consensus varied widely 
with some models demonstrating a balanced approach to risk in a similar manner 
to humans, while other models reported outputs at either end of the spectrum for 
risk tolerance. In terms of agreement with the human mode, ChatGPT-3.5 and GPT-4 
(OpenAI) had the highest levels of alignment, agreeing with the human mode on 6 
out of 11 questions. The total biopsy propensity score generated from the human 
mode was 23 out of 44. Both OpenAI models produced similar propensity scores 
between 22 and 24. However, Llama 2 and MS Copilot also scored within this range 
but with poorer response alignment to the human consensus at only 2 out of 11 
questions. The most risk-averse model in this study was MedLM, with a propensity 
score of 11, and the least risk-averse model was Claude 3, with a score of 34.
CONCLUSIONS: The outputs of LLMs demonstrated a modest ability to replicate 
human clinical decision-making in this study; however, performance varied widely 
between LLM models. Questions with more uniform human responses produced LLM 
outputs with higher alignment, whereas questions with lower human consensus 
showed poorer output alignment. This may limit the practical use of LLMs in 
real-world clinical practice.

© Michael Toal, Christopher Hill, Michael Quinn, Ciaran O'Neill, Alexander P 
Maxwell. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/73603
PMCID: PMC12445783
PMID: 40966592 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


312. J Biomed Inform. 2025 Oct;170:104906. doi: 10.1016/j.jbi.2025.104906. Epub 2025 
Sep 15.

SynthMedic: Utilizing large language models for synthetic discharge summary 
generation, correction and validation.

Grazhdanski G(1), Vasilev V(2), Vassileva S(3), Taskov D(4), Antova I(5), 
Koychev I(6), Boytcheva S(7).

Author information:
(1)Faculty of Mathematics and Informatics, Sofia University St. Kliment 
Ohridski, 5 James Bourchier Blvd., Sofia, 1164, Bulgaria. Electronic address: 
ggrazhdans@uni-sofia.bg.
(2)Faculty of Mathematics and Informatics, Sofia University St. Kliment 
Ohridski, 5 James Bourchier Blvd., Sofia, 1164, Bulgaria; Graphwise, 111R 
Tsarigradsko Shosse, Synergy Tower, fl. 12, Sofia, 1784, Bulgaria. Electronic 
address: vnvasilev1@uni-sofia.bg.
(3)Faculty of Mathematics and Informatics, Sofia University St. Kliment 
Ohridski, 5 James Bourchier Blvd., Sofia, 1164, Bulgaria. Electronic address: 
svasileva@fmi.uni-sofia.bg.
(4)Multiprofile Hospital for Active Treatment in Neurology and Psychiatry "St. 
Naum", 1 Dr. Lyuben Rusev St., Sofia, 1113, Bulgaria; Medical University of 
Sofia, 15 Akademik I. E. Geshov Blvd., Sofia, 1431, Bulgaria. Electronic 
address: d.taskov@medfac.mu-sofia.bg.
(5)Multiprofile Hospital for Active Treatment in Neurology and Psychiatry "St. 
Naum", 1 Dr. Lyuben Rusev St., Sofia, 1113, Bulgaria. Electronic address: 
iantova@medfac.mu-sofia.bg.
(6)Faculty of Mathematics and Informatics, Sofia University St. Kliment 
Ohridski, 5 James Bourchier Blvd., Sofia, 1164, Bulgaria. Electronic address: 
koychev@fmi.uni-sofia.bg.
(7)Faculty of Mathematics and Informatics, Sofia University St. Kliment 
Ohridski, 5 James Bourchier Blvd., Sofia, 1164, Bulgaria; Graphwise, 111R 
Tsarigradsko Shosse, Synergy Tower, fl. 12, Sofia, 1784, Bulgaria. Electronic 
address: svetla.boytcheva@graphwise.ai.

BACKGROUND AND OBJECTIVE: Synthetic clinical texts can improve transparency and 
reduce bias and costs when training and evaluating specialized language models 
in the medical domain. Synthetic texts are freely shareable, as they contain no 
real patient information, and can be customized for a specific task. The 
objective of this study is to develop a methodology for generating, validating, 
and correcting synthetic discharge summaries using LLMs without requiring any 
real patient data.
METHODS: The proposed approach uses an LLM to generate synthetic discharge 
summaries for specific diseases and standard medical references from Merck 
Manuals to ground the generation in internationally accepted medical practices. 
We validate the generated summaries using LLMs as well as by human expert 
validation. In addition, we propose a method for automatic correction of the 
generated discharge summaries using Knowledge Graphs to ensure medical factual 
correctness.
RESULTS: The conducted human expert evaluation shows that the generated 
synthetic discharge summaries are credible and factually accurate when provided 
with the medical reference context. The generated summaries achieve a System 
Usability Score of 94.35% based on a comprehensive rubric evaluated by medical 
professionals and a score of 93.65% on the Faithfulness metric evaluated by an 
LLM.
CONCLUSIONS: The proposed methodology can be utilized to generate high-quality 
synthetic discharge summaries for various diseases. The generated synthetic 
corpus consists of 900 discharge summaries in English representing nine socially 
significant diseases and is publicly available under an open license. The 
community can take advantage of the corpus and proposed methodology to train 
complex machine learning models, helping medical professionals in their daily 
work without using real patient data.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104906
PMID: 40962129 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


313. Nutrients. 2025 Dec 17;17(24):3940. doi: 10.3390/nu17243940.

Refining MASLD Phenotypes: Clinical, Metabolic, and Elastographic Differences 
Between Adipose Tissue Dysfunction and Obesity-Driven Disease.

Cosma T(1), Avram L(2), Donca V(2), Grosu A(3), Stoicescu L(3), Buzdugan E(3), 
Nemes A(4), Balan AM(4), Crisan D(3).

Author information:
(1)Regional Institute of Gastroenterology and Hepatology "Prof. Dr. Octavian 
Fodor", Faculty of Medicine, "Iuliu Hatieganu" University of Medicine and 
Pharmacy, 400012 Cluj-Napoca, Romania.
(2)Department 5-Medical Specialties, Clinical Municipal Hospital, Faculty of 
Medicine, "Iuliu Hatieganu" University of Medicine and Pharmacy, 
Geriatrics-Gerontology, 400012 Cluj-Napoca, Romania.
(3)Department of Internal Medicine, 5th Medical Clinic, Clinical Municipal 
Hospital, Faculty of Medicine, "Iuliu Hatieganu" University of Medicine and 
Pharmacy, 400012 Cluj-Napoca, Romania.
(4)Intensive Care Unit Department 6, Clinical Municipal Hospital, Faculty of 
Medicine, "Iuliu Hatieganu" University of Medicine and Pharmacy, 400012 
Cluj-Napoca, Romania.

BACKGROUND/OBJECTIVES: Metabolic dysfunction-associated steatotic liver disease 
(MASLD) is a heterogeneous condition shaped by metabolic dysfunction, adipose 
tissue distribution, inflammatory activation, and body composition. 
Understanding how these factors interact across distinct clinical phenotypes is 
essential for improving diagnostic accuracy and risk stratification. This study 
aimed to compare metabolic, inflammatory, and elastographic profiles between 
MASLD subgroups defined by adipose tissue dysfunction (ATD) and obesity, and to 
identify pathways linking metabolic dysregulation to hepatic fibrosis.
METHODS: We conducted a cross-sectional observational study including 178 adult 
participants evaluated clinically, biochemically, and by bioimpedance and shear 
wave elastography. Participants ranged in age from 19 to 82 years. Patients were 
stratified into a non-MASLD control group and two MASLD subgroups: MASLD with 
ATD (G1) and MASLD with obesity (G2). Anthropometric data, lipid profile, 
glycemic markers, cytokines (IL-6, IL-10, TNF-α), liver stiffness, and 
non-invasive fibrosis indices were compared across groups using standard 
statistical testing.
RESULTS: Patients with MASLD showed higher liver stiffness, triglycerides, and 
IL-6/IL-10 levels than controls. Between MASLD phenotypes, the ATD group (G1) 
exhibited a more inflammatory and dysmetabolic profile, with significantly 
higher triglycerides, IL-6 levels, neutrophil counts, and creatinine, alongside 
trends suggesting early sarcopenic changes. In contrast, the obese phenotype 
(G2) demonstrated greater hepatic structural involvement, including higher liver 
stiffness and BMI, AST/ALT ratio and Diabetes (BARD) scores, despite more 
favorable inflammatory parameters. Several associations between liver stiffness, 
IL-6, and glycemic control approached but did not reach statistical 
significance.
CONCLUSIONS: MASLD progression appears to follow two complementary but distinct 
mechanisms: an inflammatory, adipose dysfunction pathway dominated by IL-6 
activation and early anabolic decline, and a metabolic-overload pathway driven 
by obesity. Phenotype-specific evaluation integrating inflammatory markers, 
metabolic indices, and elastographic parameters may improve risk stratification 
and inform personalized therapeutic strategies.

DOI: 10.3390/nu17243940
PMCID: PMC12735662
PMID: 41470883 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


314. J Med Internet Res. 2025 Jul 15;27:e74299. doi: 10.2196/74299.

Development and Validation of a Large Language Model-Powered Chatbot for 
Neurosurgery: Mixed Methods Study on Enhancing Perioperative Patient Education.

Ho CM(#)(1), Guan S(#)(2), Mok PK(3), Lam CH(4), Ho WY(1), Mak CH(1), Qin H(5), 
Wong AKC(6), Hui V(5)(7).

Author information:
(1)Neurosurgery Department, Queen Elizabeth Hospital, Kowloon, China (Hong 
Kong).
(2)Department of Electrical and Electronic Engineering, Hong Kong Polytechnic 
University, Kowloon, China (Hong Kong).
(3)Faculty of Medicine, Chinese University of Hong Kong, New Territories, China 
(Hong Kong).
(4)School of Clinical Medicine, Li Ka Shing Faculty of Medicine, University of 
Hong Kong, Central and Western District, China (Hong Kong).
(5)Center for Smart Health, School of Nursing, Hong Kong Polytechnic University, 
Kowloon, China (Hong Kong).
(6)School of Nursing, Hong Kong Polytechnic University, Kowloon, China (Hong 
Kong).
(7)Health and Community Systems, School of Nursing, University of Pittsburgh, 
Pittsburgh, PA, United States.
(#)Contributed equally

BACKGROUND: Perioperative education is crucial for optimizing outcomes in 
neuroendovascular procedures, where inadequate understanding can heighten 
patient anxiety and hinder care plan adherence. Current education models, 
reliant on traditional consultations and printed materials, often lack 
scalability and personalization. Artificial intelligence (AI)-powered chatbots 
have demonstrated efficacy in various health care contexts; however, their role 
in neuroendovascular perioperative support remains underexplored. Given the 
complexity of neuroendovascular procedures and the need for continuous, tailored 
patient education, AI chatbots have the potential to offer tailored 
perioperative guidance to improve patient education in this specialty.
OBJECTIVE: We aimed to develop, validate, and assess NeuroBot, an AI-driven 
system that uses large language models (LLMs) with retrieval-augmented 
generation to deliver timely, accurate, and evidence-based responses to patient 
inquiries in neurosurgery, ultimately improving the effectiveness of patient 
education.
METHODS: A mixed methods approach was used, consisting of 3 phases. In the first 
phase, internal validation, we compared the performance of Assistants API, 
ChatGPT, and Qwen by evaluating their responses to 306 bilingual 
neuroendovascular-related questions. The accuracy, relevance, and completeness 
of the responses were evaluated using a Likert scale; statistical analyses 
included ANOVA and paired t tests. In the second phase, external validation, 10 
neurosurgical experts rated the responses generated by NeuroBot using the same 
evaluation metrics applied in the internal validation phase. The consistency of 
their ratings was measured using the intraclass correlation coefficient. 
Finally, in the third phase, a qualitative study was conducted through 
interviews with 18 health care providers, which helped identify key themes 
related to the NeuroBot's usability and perceived benefits. Thematic analysis 
was performed using NVivo and interrater reliability was confirmed through Cohen 
κ.
RESULTS: The Assistants API outperformed both ChatGPT and Qwen, achieving a mean 
accuracy score of 5.28 out of 6 (95% CI 5.21-5.35), with a statistically 
significant result (P<.001). External expert ratings for NeuroBot demonstrated 
significant improvements, with scores of 5.70 out of 6 (95% CI 5.46-5.94) for 
accuracy, 5.58 out of 6 (95% CI 5.45-5.94) for relevance, and 2.70 out of 3 (95% 
CI 2.73-2.97) for completeness. Qualitative insights highlighted NeuroBot's 
potential to reduce staff workload, enhance patient education, and deliver 
evidence-based responses.
CONCLUSIONS: NeuroBot, leveraging LLMs with the retrieval-augmented generation 
technique, demonstrates the potential of LLM-based chatbots in perioperative 
neuroendovascular care, offering scalable and continuous support. By integrating 
domain-specific knowledge, NeuroBot simplifies communication between 
professionals and patients while ensuring patients have 24-7 access to reliable, 
evidence-based information. Further refinement and research will enhance 
NeuroBot's ability to foster patient-centered communication, optimize clinical 
outcomes, and advance AI-driven innovations in health care delivery.

©Chung Man Ho, Shaowei Guan, Prudence Kwan-Lam Mok, Candice HW Lam, Wai Ying Ho, 
Calvin Hoi-Kwan Mak, Harry Qin, Arkers Kwan Ching Wong, Vivian Hui. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org), 
15.07.2025.

DOI: 10.2196/74299
PMCID: PMC12308165
PMID: 40663377 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


315. Radiol Artif Intell. 2025 May;7(3):e240551. doi: 10.1148/ryai.240551.

Open-Weight Language Models and Retrieval-Augmented Generation for Automated 
Structured Data Extraction from Diagnostic Reports: Assessment of Approaches and 
Parameters.

Jabal MS(1), Warman P(2), Zhang J(3)(4), Gupta K(5), Jain A(5), Mazurowski 
M(1)(2)(3), Wiggins W(1), Magudia K(1), Calabrese E(1)(5).

Author information:
(1)Department of Radiology, Duke University Hospital, 2301 Erwin Rd, Durham, NC 
27710.
(2)Duke University School of Medicine, Durham, NC.
(3)Department of Electrical and Computer Engineering, Duke University, Durham, 
NC.
(4)Duke Center for Artificial Intelligence in Radiology, Duke University, 
Durham, NC.
(5)Department of Radiology, Duke University Medical Center, Durham, NC.

Comment in
    Radiol Artif Intell. 2025 May;7(3):e250175. doi: 10.1148/ryai.250175.

Purpose To develop and evaluate an automated system for extracting structured 
clinical information from unstructured radiology and pathology reports using 
open-weight language models (LMs) and retrieval-augmented generation (RAG) and 
to assess the effects of model configuration variables on extraction 
performance. Materials and Methods This retrospective study used two datasets: 
7294 radiology reports annotated for Brain Tumor Reporting and Data System 
(BT-RADS) scores and 2154 pathology reports annotated for IDH mutation status 
(January 2017-July 2021). An automated pipeline was developed to benchmark the 
performance of various LMs and RAG configurations for accuracy of structured 
data extraction from reports. The effect of model size, quantization, prompting 
strategies, output formatting, and inference parameters on model accuracy was 
systematically evaluated. Results The best-performing models achieved up to 98% 
accuracy in extracting BT-RADS scores from radiology reports and greater than 
90% accuracy for extraction of IDH mutation status from pathology reports. The 
best model was medical fine-tuned Llama 3. Larger, newer, and domain fine-tuned 
models consistently outperformed older and smaller models (mean accuracy, 86% vs 
75%; P < .001). Model quantization had minimal effect on performance. Few-shot 
prompting significantly improved accuracy (mean [±SD] increase, 32% ± 32; P = 
.02). RAG improved performance for complex pathology reports by a mean of 48% ± 
11 (P = .001) but not for shorter radiology reports (-8% ± 31; P = .39). 
Conclusion This study demonstrates the potential of open LMs in automated 
extraction of structured clinical data from unstructured clinical reports with 
local privacy-preserving application. Careful model selection, prompt 
engineering, and semiautomated optimization using annotated data are critical 
for optimal performance. Keywords: Large Language Models, Retrieval-Augmented 
Generation, Radiology, Pathology, Health Care Reports Supplemental material is 
available for this article. © RSNA, 2025 See also commentary by Tejani and 
Rauschecker in this issue.

DOI: 10.1148/ryai.240551
PMID: 40072216 [Indexed for MEDLINE]


316. J Am Med Inform Assoc. 2025 Mar 1;32(3):447-458. doi: 10.1093/jamia/ocae311.

Analysis of eligibility criteria clusters based on large language models for 
clinical trial design.

Bornet A(1), Khlebnikov P(2), Meer F(2), Haas Q(2), Yazdani A(1), Zhang B(1), 
Amini P(2), Teodoro D(1).

Author information:
(1)Department of Radiology and Medical Informatics, University of Geneva, 1202 
Geneva, Switzerland.
(2)Risklick AG, 3013 Bern, Switzerland.

OBJECTIVES: Clinical trials (CTs) are essential for improving patient care by 
evaluating new treatments' safety and efficacy. A key component in CT protocols 
is the study population defined by the eligibility criteria. This study aims to 
evaluate the effectiveness of large language models (LLMs) in encoding 
eligibility criterion information to support CT-protocol design.
MATERIALS AND METHODS: We extracted eligibility criterion sections, phases, 
conditions, and interventions from CT protocols available in the 
ClinicalTrials.gov registry. Eligibility sections were split into individual 
rules using a criterion tokenizer and embedded using LLMs. The obtained 
representations were clustered. The quality and relevance of the clusters for 
protocol design was evaluated through 3 experiments: intrinsic alignment with 
protocol information and human expert cluster coherence assessment, extrinsic 
evaluation through CT-level classification tasks, and eligibility section 
generation.
RESULTS: Sentence embeddings fine-tuned using biomedical corpora produce 
clusters with the highest alignment to CT-level information. Human expert 
evaluation confirms that clusters are well structured and coherent. Despite the 
high information compression, clusters retain significant CT information, up to 
97% of the classification performance obtained with raw embeddings. Finally, 
eligibility sections automatically generated using clusters achieve 95% of the 
ROUGE scores obtained with a generative LLM prompted with CT-protocol details, 
suggesting that clusters encapsulate information useful to CT-protocol design.
DISCUSSION: Clusters derived from sentence-level LLM embeddings effectively 
summarize complex eligibility criterion data while retaining relevant 
CT-protocol details. Clustering-based approaches provide a scalable enhancement 
in CT design that balances information compression with accuracy.
CONCLUSIONS: Clustering eligibility criteria using LLM embeddings provides a 
practical and efficient method to summarize critical protocol information. We 
provide an interactive visualization of the pipeline here.

© The Author(s) 2024. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocae311
PMCID: PMC11833473
PMID: 39724913 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare the following competing 
financial interest(s): P.K., F.M., Q.H., and P.A. work for Risklick AG. All 
other authors declare no competing financial interest.


317. Surgery. 2025 Jun;182:109267. doi: 10.1016/j.surg.2025.109267. Epub 2025 Mar 6.

Use of large language models as clinical decision support tools for management 
pancreatic adenocarcinoma using National Comprehensive Cancer Network 
guidelines.

Kaiser KN(1), Hughes AJ(2), Yang AD(3), Mohanty S(3), Maatman TK(4), Gonzalez 
AA(3), Patzer RE(5), Bilimoria KY(3), Ellis RJ(6).

Author information:
(1)Surgical Outcomes and Quality Improvement Center (SOQIC), Department of 
Surgery, Indiana School of Medicine, Indianapolis, IN. Electronic address: 
https://twitter.com/kristen_kaiser1.
(2)Surgical Outcomes and Quality Improvement Center (SOQIC), Department of 
Surgery, Indiana School of Medicine, Indianapolis, IN.
(3)Surgical Outcomes and Quality Improvement Center (SOQIC), Department of 
Surgery, Indiana School of Medicine, Indianapolis, IN; Department of Surgery, 
Division of Surgical Oncology, Indiana University School of Medicine, 
Indianapolis, IN.
(4)Department of Surgery, Division of Surgical Oncology, Indiana University 
School of Medicine, Indianapolis, IN.
(5)Surgical Outcomes and Quality Improvement Center (SOQIC), Department of 
Surgery, Indiana School of Medicine, Indianapolis, IN; Center for Health 
Services Research, Regienstrief Institute, Indianapolis, IN.
(6)Surgical Outcomes and Quality Improvement Center (SOQIC), Department of 
Surgery, Indiana School of Medicine, Indianapolis, IN; Department of Surgery, 
Division of Surgical Oncology, Indiana University School of Medicine, 
Indianapolis, IN. Electronic address: ellisrj@iu.edu.

BACKGROUND: Large language models may form the basis of clinical decision 
support tools to improve rates of guideline concordant care for pancreatic 
ductal adenocarcinoma. The objectives of this study were to 1) define the 
first-pass accuracy of 2 publicly available large language models in responding 
to prompts on the basis of National Comprehensive Cancer Network guidelines for 
pancreatic ductal adenocarcinoma, 2) describe consistency of responses within 
each large language models, and 3) explore differences between the 2 large 
language models in their accuracy and verbosity.
METHODS: Clinical scenarios were developed on the basis of current National 
Comprehensive Cancer Network guidelines. Scenario prompts were entered 
independently by 2 investigators into OpenAI ChatGPT and Microsoft Copilot, 
yielding 4 responses per scenario. Responses were manually graded on accuracy 
and verbosity and compared to clinician-derived responses.
RESULTS: From the 104 responses, large language model responses were graded as 
completely correct in 42% of responses (n = 44). ChatGPT responses were more 
accurate than Copilot across all prompts (3.33 ± 0.86 vs 3.02 ± 0.87, P = .04). 
Among 54 generated responses from ChatGPT sessions, 52% (n = 27) were completely 
correct, 35% (n = 18) contained missing information, and 14% (n = 7) were 
inaccurate/misleading. Copilot responses were completely correct in 33% (n = 17) 
of responses, whereas 42% (n = 22) were missing information and 25% (n = 13) 
contained inaccurate/misleading information. Clinician responses were more 
concise than all large language model-generated responses (32 ± 13 vs 270 ± 70 
words, P < .001).
CONCLUSION: Large language model-powered responses to clinical questions 
regarding pancreatic ductal adenocarcinoma are often inaccurate and verbose. 
These publicly available large language models require significant optimization 
before implementation within health care as clinical decision support tools.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.surg.2025.109267
PMID: 40055080 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest/Disclosure The authors 
report no conflicts of interest, financial or otherwise, related to this work.


318. JMIR Form Res. 2025 Apr 17;9:e65670. doi: 10.2196/65670.

Development of a GPT-4-Powered Virtual Simulated Patient and Communication 
Training Platform for Medical Students to Practice Discussing Abnormal Mammogram 
Results With Patients: Multiphase Study.

Weisman D(1), Sugarman A(2), Huang YM(1)(3), Gelberg L(4)(5), Ganz PA(2)(5), 
Comulada WS(5)(6).

Author information:
(1)UCLA Simulation Center, University of California, Los Angeles, Los Angeles, 
CA, United States.
(2)David Geffen School of Medicine, University of California, Los Angeles, Los 
Angeles, CA, United States.
(3)Department of Anesthesiology and Perioperative Medicine, David Geffen School 
of Medicine, University of California, Los Angeles, Los Angeles, CA, United 
States.
(4)Department of Family Medicine, David Geffen School of Medicine, University of 
California, Los Angeles, Los Angeles, CA, United States.
(5)Department of Health Policy and Management, Fielding School of Public Health, 
University of California, Los Angeles, Los Angeles, CA, United States.
(6)Department of Psychiatry and Biobehavioral Sciences, David Geffen School of 
Medicine, University of California, Los Angeles, Los Angeles, CA, United States.

BACKGROUND: Standardized patients (SPs) prepare medical students for difficult 
conversations with patients. Despite their value, SP-based simulation training 
is constrained by available resources and competing clinical demands. 
Researchers are turning to artificial intelligence and large language models, 
such as generative pretrained transformers, to create communication training 
that incorporates virtual simulated patients (VSPs). GPT-4 is a large language 
model advance allowing developers to design virtual simulation scenarios using 
text-based prompts instead of relying on branching path simulations with 
prescripted dialogue. These nascent developmental practices have not taken root 
in the literature to guide other researchers in developing their own 
simulations.
OBJECTIVE: This study aims to describe our developmental process and lessons 
learned for creating a GPT-4-driven VSP. We designed the VSP to help medical 
student learners rehearse discussing abnormal mammography results with a patient 
as a primary care physician (PCP). We aimed to assess GPT-4's ability to 
generate appropriate VSP responses to learners during spoken conversations and 
provide appropriate feedback on learner performance.
METHODS: A research team comprised of physicians, a medical student, an 
educator, an SP program director, a learning experience designer, and a health 
care researcher conducted the study. A formative phase with in-depth knowledge 
user interviews informed development, followed by a development phase to create 
the virtual training module. The team conducted interviews with 5 medical 
students, 5 PCPs, and 5 breast cancer survivors. They then developed a VSP using 
simulation authoring software and provided the GPT-4-enabled VSP with an initial 
prompt consisting of a scenario description, emotional state, and expectations 
for learner dialogue. It was iteratively refined through an agile design process 
involving repeated cycles of testing, documenting issues, and revising the 
prompt. As an exploratory feature, the simulation used GPT-4 to provide written 
feedback to learners about their performance communicating with the VSP and 
their adherence to guidelines for difficult conversations.
RESULTS: In-depth interviews helped establish the appropriate timing, mode of 
communication, and protocol for conversations between PCPs and patients during 
the breast cancer screening process. The scenario simulated a telephone call 
between a physician and patient to discuss the abnormal results of a diagnostic 
mammogram that that indicated a need for a biopsy. Preliminary testing was 
promising. The VSP asked sensible questions about their mammography results and 
responded to learner inquiries using a voice replete with appropriate emotional 
inflections. GPT-4 generated performance feedback that successfully identified 
strengths and areas for improvement using relevant quotes from the learner-VSP 
conversation, but it occasionally misidentified learner adherence to 
communication protocols.
CONCLUSIONS: GPT-4 streamlined development and facilitated more dynamic, 
humanlike interactions between learners and the VSP compared to branching path 
simulations. For the next steps, we will pilot-test the VSP with medical 
students to evaluate its feasibility and acceptability.

©Dan Weisman, Alanna Sugarman, Yue Ming Huang, Lillian Gelberg, Patricia A Ganz, 
Warren Scott Comulada. Originally published in JMIR Formative Research 
(https://formative.jmir.org), 17.04.2025.

DOI: 10.2196/65670
PMCID: PMC12046251
PMID: 40246299 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


319. Aesthetic Plast Surg. 2025 Nov;49(21):5866-5876. doi: 
10.1007/s00266-025-05010-8. Epub 2025 Jul 16.

Evaluation of Artificial Intelligence Chatbots for Facial Injection Planning: 
Comparative Performance and Safety Limitations.

Radulesco T(1), Ebode D(2), Maniaci A(3), Gargula S(2), Saibene AM(4), 
Chiesa-Estomba C(5), Gengler I(6), Vaira L(7), Vishnumurthy P(8), Lechien 
JR(9)(10)(11), Michel J(2).

Author information:
(1)Aix Marseille Univ, APHM, CNRS, IUSTI, La Conception University Hospital, 
Marseille, France. Thomas.radulesco@ap-hm.fr.
(2)Aix Marseille Univ, APHM, CNRS, IUSTI, La Conception University Hospital, 
Marseille, France.
(3)Faculty of Medicine and Surgery, University of Enna "Kore", 94100, Enna, 
Italy.
(4)Otolaryngology Unit, ASST Santi Paolo E Carlo, Department of Health Sciences, 
Università Degli Studi Di Milano, Milan, Italy.
(5)Department of Otorhinolaryngology- Head and Neck Surgery, Hospital 
Universitario Donostia, San Sebastián, Spain.
(6)Department of Otolaryngology-Head and Neck Surgery, University of Cincinnati 
College of Medicine, Cincinnati, OH, USA.
(7)Maxillofacial Surgery Operative Unit, Department of Medicine, Surgery and 
Pharmacy, University of Sassari, Sassari, Italy.
(8)Aix Marseille Univ, CNRS, IUSTI, Marseille, France.
(9)Division of Laryngology and Broncho-esophagology, Department of 
Otolaryngology-Head Neck Surgery, EpiCURA Hospital, UMONS Research Institute for 
Health Sciences and Technology, University of Mons (UMons), Mons, Belgium.
(10)Phonetics and Phonology Laboratory (UMR 7018 CNRS, Université Sorbonne 
Nouvelle/Paris 3) Department of Otorhinolaryngology and Head and Neck Surgery, 
Foch Hospital, Paris Saclay University, Paris, France.
(11)Department of Otorhinolaryngology and Head and Neck Surgery, CHU 
Saint-Pierre, Brussels, Belgium.

BACKGROUND: To evaluate the performance of artificial intelligence (AI)-powered 
chatbots in generating treatment plans for facial aesthetic injections, focusing 
on their accuracy, safety, and clinical applicability.
METHODS: A comparative observational study was conducted in an otolaryngology 
tertiary care department according to STROBE guidelines. Patients seeking facial 
injections were recruited from July to October 2024. Forty patients (85% female; 
mean age: 45.8 years) underwent photographic documentation and received 
AI-generated treatment plans for botulinum toxin and hyaluronic acid injections. 
Six AI chatbots and three generative vision models were evaluated based on five 
criteria: product selection, injection strategy, facial analysis, alignment with 
patient preferences, and safety. Likert scale ratings, each ranging from - 2 to 
+ 2, were analyzed using Friedman and Durbin-Conover pairwise tests to identify 
significant differences (p < 0.05). The sum of the five Likert scales provided 
an overall score ranging from - 10 to + 10.
RESULTS: ChatGPTo1 and ChatGPT4o achieved higher scores than other chatbots 
across most evaluation criteria, with mean total scores of 7.87 ± 0.29 and 
7.85 ± 0.44, respectively (p = 0.295). Both chatbots were statistically superior 
(p < 0.05) to Claude, CopilotPro, and Llama in product selection 
(ChatGPT4o = 1.92 ± 0.05), injection strategy precision 
(ChatGPTo1 = 1.67 ± 0.08), alignment with patient preferences 
(ChatGPTo1 = 1.95 ± 0.03) and safety (ChatGPTo1 = 1.30 ± 0.17). Claude provided 
relevant facial analysis (1.50 ± 0.16) without significant difference compared 
to ChatGPT models (all p > 0.05). Generative vision models failed to produce 
relevant visual annotations.
CONCLUSION: Among the AI systems tested, ChatGPT-based chatbots demonstrated 
relatively superior performance in generating treatment plans for facial 
injections. However, safety limitations remain and preclude unsupervised 
clinical use.
LEVEL OF EVIDENCE IV: This journal requires that authors assign a level of 
evidence to each article. For a full description of these Evidence-Based 
Medicine ratings, please refer to the Table of Contents or the online 
Instructions to Authors www.springer.com/00266 .

© 2025. Springer Science+Business Media, LLC, part of Springer Nature and 
International Society of Aesthetic Plastic Surgery.

DOI: 10.1007/s00266-025-05010-8
PMID: 40670654 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare that they have no conflicts of interest to disclose. Ethical Approval: 
We obtained an Ethical Committee Authorization (APHM, Assistance Publique des 
Hôpitaux de Marseille, Authorization N° PADS24-289) to conduct this study. 
Informed Consent: All participating patients gave their consent before 
participating in this study, which was conducted by the Declaration of Helsinki 
(2013).


320. J Neurodev Disord. 2025 Apr 30;17(1):24. doi: 10.1186/s11689-025-09612-w.

Automated extraction of functional biomarkers of verbal and ambulatory ability 
from multi-institutional clinical notes using large language models.

Kaster L(1), Hillis E(1), Oh IY(1), Aravamuthan BR(2), Lanzotti VC(3), Vickstrom 
CR(2); Brain Gene Registry Consortium; Gurnett CA(2), Payne PRO(1), Gupta A(4).

Collaborators: Wasserstein M, Chopra M, Sahin M, Wangler M, Schultz B, Izumi K, 
Bergner S, Gropman A, Smith-Hicks C, Abbeduto L, Hazlett H, Doherty D, German K, 
DaWalt L, Neul J, Constantino J, Baldridge D, Srivastava S, Molholm S, Walkley 
S, Storch E, Samaco R, Cohen J, Shankar S, Piven J, Mahida S, Sveden A, Dies K, 
Riggs ER, Savatt JM, Minor B.

Author information:
(1)Institute for Informatics, Data Science and Biostatistics, Washington 
University School of Medicine in St. Louis, St. Louis, MO, USA.
(2)Department of Neurology, Washington University School of Medicine in St. 
Louis, St. Louis, MO, USA.
(3)Department of Psychiatry, Washington University School of Medicine in St. 
Louis, St. Louis, MO, USA.
(4)Institute for Informatics, Data Science and Biostatistics, Washington 
University School of Medicine in St. Louis, St. Louis, MO, USA. 
agupta24@wustl.edu.

BACKGROUND: Functional biomarkers in neurodevelopmental disorders, such as 
verbal and ambulatory abilities, are essential for clinical care and research 
activities. Treatment planning, intervention monitoring, and identifying 
comorbid conditions in individuals with intellectual and developmental 
disabilities (IDDs) rely on standardized assessments of these abilities. 
However, traditional assessments impose a burden on patients and providers, 
often leading to longitudinal inconsistencies and inequities due to evolving 
guidelines and associated time-cost. Therefore, this study aimed to develop an 
automated approach to classify verbal and ambulatory abilities from EHR data of 
IDD and cerebral palsy (CP) patients. Application of large language models 
(LLMs) to clinical notes, which are rich in longitudinal data, may provide a 
low-burden pipeline for extracting functional biomarkers efficiently and 
accurately.
METHODS: Data from the multi-institutional National Brain Gene Registry (BGR) 
and a CP clinic cohort were utilized, comprising 3,245 notes from 125 
individuals and 5,462 clinical notes from 260 individuals, respectively. 
Employing three LLMs-GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4 Omni-we provided the 
models with a clinical note and utilized a detailed conversational format to 
prompt the models to answer: "Does the individual use any words?" and "Can the 
individual walk without aid?" These responses were evaluated against 
ground-truth abilities, which were established using neurobehavioral assessments 
collected for each dataset.
RESULTS: LLM pipelines demonstrated high accuracy (weighted-F1 scores > .90) in 
predicting ambulatory ability for both cohorts, likely due to the consistent use 
of Gross Motor Functional Classification System (GMFCS) as a consistent 
ground-truth standard. However, verbal ability predictions were more accurate in 
the BGR cohort, likely due to higher adherence between the prompt and 
ground-truth assessment questions. While LLMs can be computationally expensive, 
analysis of our protocol affirmed the cost effectiveness when applied to select 
notes from the EHR.
CONCLUSIONS: LLMs are effective at extracting functional biomarkers from EHR 
data and broadly generalizable across variable note-taking practices and 
institutions. Individual verbal and ambulatory ability were accurately 
extracted, supporting the method's ability to streamline workflows by offering 
automated, efficient data extraction for patient care and research. Future 
studies are needed to extend this methodology to additional populations and to 
demonstrate more granular functional data classification.

© 2025. The Author(s).

DOI: 10.1186/s11689-025-09612-w
PMCID: PMC12042395
PMID: 40307685 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: No participants were recruited specifically for this study. This 
work constitutes secondary use of data approved by the Washington University in 
St. Louis IRB (protocols #202010013 [Brain Gene Registry cohort] and #202309003 
[cerebral palsy cohort]). Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


321. Int J Med Inform. 2026 Mar 15;208:106227. doi: 10.1016/j.ijmedinf.2025.106227. 
Epub 2025 Dec 28.

Vision-language models in diagnostic imaging: review of technical advances, 
clinical validation, and practical deployment.

Dutta N(1), Bose K(1), Syailendra E(2), Chu L(2), Gupta P(3).

Author information:
(1)Department of Radiodiagnosis, Postgraduate Institute of Medical Education and 
Research, Chandigarh 160012, India.
(2)The Russell H. Morgan Department of Radiology and Radiological Science, Johns 
Hopkins University School of Medicine, Baltimore, MD 21287, United States.
(3)Department of Radiodiagnosis, Postgraduate Institute of Medical Education and 
Research, Chandigarh 160012, India. Electronic address: 
pankajgupta959@gmail.com.

BACKGROUND: Radiology faces an unprecedented workload crisis, creating demand 
for AI solutions to enhance efficiency and quality. Vision-language models 
(VLMs) represent a paradigm shift from narrow AI tools to integrated systems for 
image interpretation and report generation. However, their rapid technical 
progress has outpaced rigorous clinical validation, creating a critical gap 
between their theoretical potential and safe, practical deployment.
OBJECTIVE: To critically review the state of VLMs in diagnostic imaging by 
evaluating their clinical validation, identifying deployment challenges, and 
assessing their impact on the radiological workflow. This review provides a 
roadmap for responsible clinical integration by analyzing the gap between model 
performance and real-world utility.
METHOD: A narrative review of literature was conducted from January 2017 to May 
2025. The search focused on VLM applications in radiology, including automated 
report generation and visual question answering. We synthesized findings from 
technical and clinical validation studies, thematically organized around 
architectural evolution, applications, validation, and implementation barriers.
RESULTS: A clear progression from encoder-decoder models to sophisticated 
LLM-integrated foundation models was identified. While these models achieve high 
performance on NLP metrics, their clinical utility is limited. Key findings 
include: (1) Pervasive model hallucination, with factual errors in ∼ 22 % of 
AI-generated reports; (2) A lack of external validation on diverse, 
multi-institutional datasets; (3) Significant implementation barriers, including 
high computational costs, poor workflow integration, and unresolved liability. 
Human expert evaluations show that while AI-generated reports for routine cases 
are often acceptable (77.7 % in one study), accuracy declines significantly in 
complex cases.
CONCLUSION: VLMs hold transformative potential but are not ready for autonomous 
clinical use. Their primary value lies in augmenting radiologists' workflow. For 
successful adoption, the field must shift focus from algorithmic metrics to 
proving clinical safety and efficacy through rigorous validation, developing 
robust hallucination mitigation strategies, and designing seamless workflow 
integrations.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106227
PMID: 41483727 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


322. J Biomed Inform. 2025 Dec;172:104951. doi: 10.1016/j.jbi.2025.104951. Epub 2025 
Nov 6.

LLM-DQR: Large language model-based automated generation of data quality rules 
for electronic health records.

Xie S(1), Cai H(1), Sun Y(1), Lv X(2).

Author information:
(1)College of Biomedical Engineering and Instrument Science, Zhejiang 
University, Zheda Road, 310027 Hangzhou, China.
(2)College of Biomedical Engineering and Instrument Science, Zhejiang 
University, Zheda Road, 310027 Hangzhou, China. Electronic address: 
lvxd@zju.edu.cn.

OBJECTIVE: To develop and evaluate LLM-DQR, an automated approach using large 
language models to generate electronic health record data quality rules, 
addressing the limitations of current manual and automated methods that suffer 
from low efficiency, limited flexibility, and inadequate coverage of complex 
business logic.
MATERIALS AND METHODS: We designed a comprehensive pipeline with three core 
components: (1) standardized input processing integrating database schemas, 
natural language requirements, and sample data; (2) Chain-of-Thought prompt 
engineering for guided rule generation; and (3) closed-loop validation with 
deduplication, sandbox execution, and iterative debugging. The approach was 
evaluated on two distinct, publicly available datasets: the Paediatric Intensive 
Care (PIC) dataset and the Medical Information Mart for Intensive Care 
(MIMIC-IV) dataset. Performance was compared against manual expert construction 
(expert-DQR) and clinical information model-based generation (CIM-DQR).
RESULTS: LLM-DQR demonstrated higher performance across all evaluation metrics. 
The GPT implementation achieved overall coverage rates of 97.1% on the PIC 
dataset and 99.6% on the MIMIC-IV dataset, outperforming CIM-DQR. Performance 
was particularly strong for complex dimensions: achieving 100% coverage for 
Consistency rules on both datasets, whereas CIM-DQR achieved 0%. Construction 
time was reduced by over 10-fold compared to manual methods. Additionally, on 
the PIC dataset, LLM-DQR generated 89 extra, expert-validated rules.
DISCUSSION: The stronger performance demonstrates LLMs' capability to understand 
complex EHR data patterns and assessment requirements, functioning as data 
quality analysis assistants with domain knowledge and logical reasoning 
capabilities.
CONCLUSION: LLM-DQR provides an efficient, scalable solution for automated data 
quality rule generation in clinical settings, offering considerable improvements 
over traditional approaches.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104951
PMID: 41205689 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


323. Acta Obstet Gynecol Scand. 2025 Jul;104(7):1373-1381. doi: 10.1111/aogs.15123. 
Epub 2025 May 14.

AI-driven simplification of surgical reports in gynecologic oncology: A 
potential tool for patient education.

Riedel M(1), Meyer B(1), Kfuri Rubens R(2)(3)(4), Riedel C(5), Amann N(6), 
Kiechle M(1), Riedel F(7).

Author information:
(1)Department of Gynecology and Obstetrics, TUM University Hospital, Technical 
University Munich (TU), Munich, Germany.
(2)Institute of Computational Biology, Helmholtz Zentrum München, German 
Research Center for Environmental Health, Neuherberg, Germany.
(3)Department of Medicine III, Hematology and Oncology, TUM University Hospital, 
Technical University Munich, Munich, Germany.
(4)TUM School of Medicine, Technical University of Munich, Munich, Germany.
(5)Department of General Internal Medicine and Psychosomatics, Heidelberg 
University Hospital, Heidelberg, Germany.
(6)Department of Gynecology and Obstetrics, Friedrich-Alexander-University 
Erlangen-Nuremberg (FAU), Erlangen, Germany.
(7)Department of Gynecology and Obstetrics, Heidelberg University Hospital, 
Heidelberg, Germany.

INTRODUCTION: The emergence of large language models heralds a new chapter in 
natural language processing, with immense potential for improving medical care 
and especially medical oncology. One recent and publicly available example is 
Generative Pretraining Transformer 4 (GPT-4). Our objective was to evaluate its 
ability to rephrase original surgical reports into simplified versions that are 
more comprehensible to patients. Specifically, we aimed to investigate and 
discuss the potential, limitations, and associated risks of using these 
simplified reports for patient education and information in gynecologic 
oncology.
MATERIAL AND METHODS: We tasked GPT-4 with generating simplified versions from 
n = 20 original gynecologic surgical reports. Patients were provided with both 
their original report and the corresponding simplified version generated by 
GPT-4. Alongside these reports, patients received questionnaires designed to 
facilitate a comparative assessment between the original and simplified surgical 
reports. Furthermore, clinical experts evaluated the artificial intelligence 
(AI)-generated reports with regard to their accuracy and clinical quality.
RESULTS: The simplified surgical reports generated by GPT-4 significantly 
improved our patients' understanding, particularly with regard to the surgical 
procedure, its outcome, and potential risks. However, despite the reports being 
more accessible and relevant, clinical experts highlighted concerns about their 
lack of medical precision.
CONCLUSIONS: Advanced language models like GPT-4 can transform unedited surgical 
reports to improve clarity about the procedure and its outcomes. It offers 
considerable promise for enhancing patient education. However, concerns about 
medical precision underscore the need for rigorous oversight to safely integrate 
AI into patient education. Over the medium term, AI-generated, simplified 
versions of these reports-and other medical records-could be effortlessly 
integrated into standard automated postoperative care and digital discharge 
systems.

© 2025 The Author(s). Acta Obstetricia et Gynecologica Scandinavica published by 
John Wiley & Sons Ltd on behalf of Nordic Federation of Societies of Obstetrics 
and Gynecology (NFOG).

DOI: 10.1111/aogs.15123
PMCID: PMC12144574
PMID: 40366215 [Indexed for MEDLINE]

Conflict of interest statement: None.


324. J Gen Intern Med. 2025 Feb;40(3):694-702. doi: 10.1007/s11606-024-09102-0. Epub 
2024 Nov 12.

Recommendations for Clinicians, Technologists, and Healthcare Organizations on 
the Use of Generative Artificial Intelligence in Medicine: A Position Statement 
from the Society of General Internal Medicine.

Crowe B(1)(2), Shah S(3)(4), Teng D(5)(6), Ma SP(7), DeCamp M(8), Rosenberg 
EI(9), Rodriguez JA(6)(10), Collins BX(11)(12), Huber K(13), Karches K(14), 
Zucker S(15), Kim EJ(16), Rotenstein L(17), Rodman A(5)(6), Jones D(18), Richman 
IB(19), Henry TL(18), Somlo D(6)(20), Pitts SI(21), Chen JH(22)(7)(23), Mishuris 
RG(6)(10)(24).

Author information:
(1)Division of General Internal Medicine, Beth Israel Deaconess Medical Center, 
Boston, MA, USA. bcrowe@bidmc.harvard.edu.
(2)Harvard Medical School, Boston, MA, USA. bcrowe@bidmc.harvard.edu.
(3)Department of Medicine, Stanford University, Palo Alto, CA, USA.
(4)Division of Primary Care and Population Health, Stanford Healthcare AI 
Applied Research Team, Stanford University School of Medicine, Palo Alto, CA, 
USA.
(5)Division of General Internal Medicine, Beth Israel Deaconess Medical Center, 
Boston, MA, USA.
(6)Harvard Medical School, Boston, MA, USA.
(7)Division of Hospital Medicine, Stanford, CA, USA.
(8)Department of Medicine, University of Colorado, Aurora, CO, USA.
(9)Division of General Internal Medicine, Department of Medicine, University of 
Florida College of Medicine, Gainesville, FL, USA.
(10)Division of General Internal Medicine, Brigham and Women's Hospital, Boston, 
MA, USA.
(11)Division of General Internal Medicine and Public Health, Vanderbilt 
University Medical Center, Nashville, TN, USA.
(12)Department of Biomedical Informatics, Vanderbilt University, Nashville, TN, 
USA.
(13)Department of Internal Medicine, Kaiser Permanente, Denver, CO, School of 
Medicine, University of Colorado, Aurora, CO, USA.
(14)Department of Internal Medicine, Saint Louis University, Saint Louis, MO, 
USA.
(15)Department of Internal Medicine, University of Miami Miller School of 
Medicine, Jackson Memorial Hospital, Miami, FL, USA.
(16)Northwell Health, New Hyde Park, NY, USA.
(17)Divisions of General Internal Medicine and Clinical Informatics, Department 
of Medicine, University of California at San Francisco, San Francisco, CA, USA.
(18)Division of General Internal Medicine, Emory University School of Medicine, 
Atlanta, GA, USA.
(19)Section of General Internal Medicine, Yale School of Medicine, New Haven, 
CT, USA.
(20)Department of Medicine, Massachusetts General Hospital, Boston, MA, USA.
(21)Division of General Internal Medicine, Johns Hopkins University School of 
Medicine, Baltimore, MD, USA.
(22)Stanford Center for Biomedical Informatics Research, Stanford, CA, USA.
(23)Clinical Excellence Research Center, Stanford, CA, USA.
(24)Digital, Mass General Brigham, Somerville, MA, USA.

Generative artificial intelligence (generative AI) is a new technology with 
potentially broad applications across important domains of healthcare, but 
serious questions remain about how to balance the promise of generative AI 
against unintended consequences from adoption of these tools. In this position 
statement, we provide recommendations on behalf of the Society of General 
Internal Medicine on how clinicians, technologists, and healthcare organizations 
can approach the use of these tools. We focus on three major domains of medical 
practice where clinicians and technology experts believe generative AI will have 
substantial immediate and long-term impacts: clinical decision-making, health 
systems optimization, and the patient-physician relationship. Additionally, we 
highlight our most important generative AI ethics and equity considerations for 
these stakeholders. For clinicians, we recommend approaching generative AI 
similarly to other important biomedical advancements, critically appraising its 
evidence and utility and incorporating it thoughtfully into practice. For 
technologists developing generative AI for healthcare applications, we recommend 
a major frameshift in thinking away from the expectation that clinicians will 
"supervise" generative AI. Rather, these organizations and individuals should 
hold themselves and their technologies to the same set of high standards 
expected of the clinical workforce and strive to design high-performing, 
well-studied tools that improve care and foster the therapeutic relationship, 
not simply those that improve efficiency or market share. We further recommend 
deep and ongoing partnerships with clinicians and patients as necessary 
collaborators in this work. And for healthcare organizations, we recommend 
pursuing a combination of both incremental and transformative change with 
generative AI, directing resources toward both endeavors, and avoiding the urge 
to rapidly displace the human clinical workforce with generative AI. We affirm 
that the practice of medicine remains a fundamentally human endeavor which 
should be enhanced by technology, not displaced by it.

© 2024. The Author(s).

DOI: 10.1007/s11606-024-09102-0
PMCID: PMC11861482
PMID: 39531100 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of Interest: BC reports 
employment and equity with Solera Health outside the submitted work. MD reports 
consulting on ethics policy issues for the American College of Physicians via an 
institutional contract. JAR reports serving as a consultant for the Association 
of American Medical Colleges. EK reports funding from the NIH through 
K23HL163498 unrelated to the current work. LR reports research funding from 
FeelBetter Inc, the Agency for Healthcare Research and Quality, the Physicians 
Foundation, and the American Medical Association. She also serves on the AI 
Advisory Council for Augmedix, Inc and has received honoraria from Phreesia, 
Inc. AR reports funding from the Gordon and Betty Moore foundation for research 
on large language models. JC reports research funding support in part by 
NIH/National Institute of Allergy and Infectious Diseases (1R01AI17812101), 
NIH/National Institute on Drug Abuse Clinical Trials Network (UG1DA015815 - 
CTN-0136), Gordon and Betty Moore Foundation (Grant #12409), Stanford Artificial 
Intelligence in Medicine and Imaging - Human-Centered Artificial Intelligence 
(AIMI-HAI) Partnership Grant, American Heart Association - Strategically Focused 
Research Network - Diversity in Clinical Trials. Additionally, JC reports being 
co-founder of Reaction Explorer LLC that develops and licenses organic chemistry 
education software, paid consulting fees from Sutton Pierce, Younker Hyde 
MacFarlane, and Sykes McAllister as a medical expert witness and paid consulting 
fees from ISHI Health. RGM reports advisory committee role with Elsevier, 
outside of this work. All other authors have no conflicts to report.


325. Arthroscopy. 2025 Feb;41(2):263-275.e6. doi: 10.1016/j.arthro.2024.07.040. Epub 
2024 Aug 22.

Currently Available Large Language Models Do Not Provide Musculoskeletal 
Treatment Recommendations That Are Concordant With Evidence-Based Clinical 
Practice Guidelines.

Nwachukwu BU(1), Varady NH(1), Allen AA(1), Dines JS(1), Altchek DW(1), Williams 
RJ 3rd(1), Kunze KN(2).

Author information:
(1)Department of Orthopaedic Surgery, Hospital for Special Surgery, New York, 
New York, U.S.A.
(2)Department of Orthopaedic Surgery, Hospital for Special Surgery, New York, 
New York, U.S.A.. Electronic address: Kylekunze7@gmail.com.

PURPOSE: To determine whether several leading, commercially available large 
language models (LLMs) provide treatment recommendations concordant with 
evidence-based clinical practice guidelines (CPGs) developed by the American 
Academy of Orthopaedic Surgeons (AAOS).
METHODS: All CPGs concerning the management of rotator cuff tears (n = 33) and 
anterior cruciate ligament injuries (n = 15) were extracted from the AAOS. 
Treatment recommendations from Chat-Generative Pretrained Transformer version 4 
(ChatGPT-4), Gemini, Mistral-7B, and Claude-3 were graded by 2 blinded 
physicians as being concordant, discordant, or indeterminate (i.e., neutral 
response without definitive recommendation) with respect to AAOS CPGs. The 
overall concordance between LLM and AAOS recommendations was quantified, and the 
comparative overall concordance of recommendations among the 4 LLMs was 
evaluated through the Fisher exact test.
RESULTS: Overall, 135 responses (70.3%) were concordant, 43 (22.4%) were 
indeterminate, and 14 (7.3%) were discordant. Inter-rater reliability for 
concordance classification was excellent (κ = 0.92). Concordance with AAOS CPGs 
was most frequently observed with ChatGPT-4 (n = 38, 79.2%) and least frequently 
observed with Mistral-7B (n = 28, 58.3%). Indeterminate recommendations were 
most frequently observed with Mistral-7B (n = 17, 35.4%) and least frequently 
observed with Claude-3 (n = 8, 6.7%). Discordant recommendations were most 
frequently observed with Gemini (n = 6, 12.5%) and least frequently observed 
with ChatGPT-4 (n = 1, 2.1%). Overall, no statistically significant difference 
in concordant recommendations was observed across LLMs (P = .12). Of all 
recommendations, only 20 (10.4%) were transparent and provided references with 
full bibliographic details or links to specific peer-reviewed content to support 
recommendations.
CONCLUSIONS: Among leading commercially available LLMs, more than 1-in-4 
recommendations concerning the evaluation and management of rotator cuff and 
anterior cruciate ligament injuries do not reflect current evidence-based CPGs. 
Although ChatGPT-4 showed the highest performance, clinically significant rates 
of recommendations without concordance or supporting evidence were observed. 
Only 10% of responses by LLMs were transparent, precluding users from fully 
interpreting the sources from which recommendations were provided.
CLINICAL RELEVANCE: Although leading LLMs generally provide recommendations 
concordant with CPGs, a substantial error rate exists, and the proportion of 
recommendations that do not align with these CPGs suggests that LLMs are not 
trustworthy clinical support tools at this time. Each off-the-shelf, 
closed-source LLM has strengths and weaknesses. Future research should evaluate 
and compare multiple LLMs to avoid bias associated with narrow evaluation of few 
models as observed in the current literature.

Copyright © 2024 Arthroscopy Association of North America. Published by Elsevier 
Inc. All rights reserved.

DOI: 10.1016/j.arthro.2024.07.040
PMID: 39173690 [Indexed for MEDLINE]

Conflict of interest statement: Disclosures The authors declare the following 
financial interests/personal relationships which may be considered as potential 
competing interests: B.U.N. owns stock or stock options in BICMD and is a paid 
consultant for Figur8. A.A.A. owns stock or stock options in Pristine Surgical 
and Rom3. J.S.D. is a board or committee member of American Shoulder and Elbow 
Surgeons; receives intellectual property royalties from Arthrex and Linvatec; is 
a paid consultant for Arthrex; is a paid presenter or speaker for Arthrex; 
receives research support from Arthrex; is on the editorial or governing board 
of Journal of Shoulder and Elbow Surgery; receives publishing royalties and/or 
financial or material support from Thieme and Wolters Kluwer Health–Lippincott 
Williams & Wilkins; and owns stock or stock options in ViewFi. D.W.A. is a paid 
consultant for Arthrex and Stryker. R.J.W. receives intellectual property 
royalties from Arthrex; is a paid consultant for Arthrex, JRF Ortho, and 
Lipogems; owns stock or stock options in BICMD, Cymedica, Engage Surgical, 
Gramercy Extremity Orthopedics, Pristine Surgical, and RecoverX; and receives 
research support from Histogenics. K.N.K. is on the editorial board of 
Arthroscopy and HSS Journal. The other author (N.H.V.) declares that he has no 
known competing financial interests or personal relationships that could have 
appeared to influence the work reported in this paper.


326. J Am Med Inform Assoc. 2025 May 1;32(5):835-844. doi: 10.1093/jamia/ocaf042.

Optimizing the efficiency and effectiveness of data quality assurance in a 
multicenter clinical dataset.

Fu A(1)(2), Shen T(1), Roberts SB(1)(3), Liu W(1), Vaidyanathan S(1), 
Marchena-Romero KJ(1), Lam YYP(1), Shah K(1), Mak DYF(1); GEMINI Investigators; 
Razak F(1)(2)(3), Verma AA(1)(2)(3).

Collaborators: Chin S, Stern SJ, Koppula R, Joyce LF, Pellegrino N, Harris N, Ng 
V, Srivastava S, Manikan N, Wilkinson A, Gastmeier J, Kwan JC, Byaruhanga H, 
Shaji L, George S, Handsor S, Roy RA, Kim CS, Mequanint S.

Author information:
(1)Li Ka Shing Knowledge Institute, St. Michael's Hospital, Toronto, ON M5C 3G7, 
Canada.
(2)Department of Medicine, Temerty Faculty of Medicine, University of Toronto, 
Toronto, ON M5S 1A8, Canada.
(3)Institute of Health Policy, Management and Evaluation, University of Toronto, 
Toronto, ON M5T 3M6, Canada.

OBJECTIVES: Electronic health records (EHRs) data are increasingly used for 
research and analysis, but there is little empirical evidence to inform how 
automated and manual assessments can be combined to efficiently assess data 
quality in large EHR repositories.
MATERIALS AND METHODS: The GEMINI database collected data from 462 226 patient 
admissions across 32 hospitals from 2021 to 2023. We report data quality issues 
identified through semi-automated and manual data quality assessments completed 
during the data collection phase. We conducted a simulation experiment to 
evaluate the relationship between the number of records reviewed manually, the 
detection of true data errors (true positives) and the number of manual chart 
abstraction errors (false positives) that required unnecessary investigation.
RESULTS: The semi-automated data quality assessments identified 79 data quality 
issues requiring correction, of which 14 had a large impact, affecting at least 
50% of records in the data. After resolving issues identified through 
semi-automated assessments, manual validation of 2676 patient encounters at 19 
hospitals identified 4 new meaningful data errors (3 in transfusion data and 1 
in physician identifiers), distributed across 4 hospitals. There were 365 manual 
chart abstraction errors, which required investigation by data analysts to 
identify as "false positives." These errors increased linearly with the number 
of charts reviewed manually. Simulation results demonstrate that all 3 
transfusion data errors were identified with 95% sensitivity after manual review 
of 5 records, whereas 18 records were needed for the physician's table.
DISCUSSION AND CONCLUSION: The GEMINI approach represents a scalable framework 
for data quality assessment and improvement in multisite EHR research databases. 
Manual data review is important but can be minimized to optimize the trade-off 
between true and false identification of data quality errors.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf042
PMCID: PMC12012372
PMID: 40079834 [Indexed for MEDLINE]

Conflict of interest statement: A.A.V. and F.R. report being part-time employees 
of Ontario Health, as Provincial Clinical Leads for Quality Improvement in 
General Medicine.


327. J Am Med Inform Assoc. 2025 Dec 1;32(12):1865-1876. doi: 10.1093/jamia/ocaf157.

Predicting treatment retention in medication for opioid use disorder: a machine 
learning approach using NLP and LLM-derived clinical features.

Nateghi Haredasht F(1), Lopez I(2)(3), Tate S(4), Ashtari P(5), Chan MM(6)(7), 
Kulkarni D(6)(7), Chen CA(8), Vangala M(7), Griffith K(9), Bunning B(3), Miner 
AS(4), Hernandez-Boussard T(1), Humphreys K(4)(10), Lembke A(4), Vance LA(7), 
Chen JH(1)(11)(12)(13).

Author information:
(1)Stanford Center for Biomedical Informatics Research, Stanford, CA 94305, 
United States.
(2)Stanford University School of Medicine, Stanford, CA 94305, United States.
(3)Department of Biomedical Data Science, Stanford, CA 94305, United States.
(4)Department of Psychiatry & Behavioral Sciences, Stanford University School of 
Medicine, Stanford, CA 94305, United States.
(5)Department of Electrical Engineering (ESAT), STADIUS Center, KU Leuven, 3001 
Leuven, Belgium.
(6)KKT Technologies, Pte. Ltd., 139951, Singapore.
(7)Holmusk Technologies, Inc., NY 10012, United States.
(8)Division of Primary Care and Population Health, Department of Medicine, 
Stanford University School of Medicine, Stanford, CA 94305, United States.
(9)Holmusk Europe, Ltd., London, United Kingdom.
(10)Veterans Affairs Health Care System, Palo Alto, CA 94304, United States.
(11)Division of Hospital Medicine, Stanford University School of Medicine, 
Stanford, CA 94305, United States.
(12)Clinical Excellence Research Center, Stanford School of Medicine, Stanford, 
CA 94305, United States.
(13)Department of Medicine, Stanford, CA 94305, United States.

OBJECTIVE: Building upon our previous work on predicting treatment retention in 
medications for opioid use disorder, we aimed to improve 6-month retention 
prediction in buprenorphine-naloxone (BUP-NAL) therapy by incorporating features 
derived from large language models (LLMs) applied to unstructured clinical 
notes.
MATERIALS AND METHODS: We used de-identified electronic health record (EHR) data 
from Stanford Health Care (STARR) for model development and internal validation, 
and the NeuroBlu behavioral health database for external validation. Structured 
features were supplemented with 13 clinical and psychosocial features extracted 
from free-text notes using the CLinical Entity Augmented Retrieval pipeline, 
which combines named entity recognition with LLM-based classification to provide 
contextual interpretation. We trained classification (Logistic Regression, 
Random Forest, XGBoost) and survival models (CoxPH, Random Survival Forest, 
Survival XGBoost), evaluated using Receiver Operating Characteristic-Area Under 
the Curve (ROC-AUC) and C-index.
RESULTS: XGBoost achieved the highest classification performance 
(ROC-AUC = 0.65). Incorporating LLM-derived features improved model performance 
across all architectures, with the largest gains observed in simpler models such 
as Logistic Regression. In time-to-event analysis, Random Survival Forest and 
Survival XGBoost reached the highest C-index (≈0.65). SHapley Additive 
exPlanations analysis identified LLM-extracted features like Chronic Pain, Liver 
Disease, and Major Depression as key predictors. We also developed an 
interactive web tool for real-time clinical use.
DISCUSSION: Features extracted using NLP and LLM-assisted methods improved model 
accuracy and interpretability, revealing valuable psychosocial risks not 
captured in structured EHRs.
CONCLUSION: Combining structured EHR data with LLM-extracted features moderately 
improves BUP-NAL retention prediction, enabling personalized risk stratification 
and advancing AI-driven care for substance use disorders.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf157
PMCID: PMC12646374
PMID: 40977375 [Indexed for MEDLINE]

Conflict of interest statement: M.M.C., D.K., M.V., K.G., and L.A.V. report 
employment with equity ownership in Holmusk Technologies, Inc. during the 
conduct of this study. M.M.C., D.K., M.V., K.G., and L.A.V. are employees of and 
hold equity in Holmusk Technologies, Inc. K.H. reports receiving grants from 
NIDA and NIAAA and serving as a non-executive director of Indivior PLC. J.H.C. 
reports being co-founder of Reaction Explorer LLC that develops and licenses 
organic chemistry education software and has been paid consulting fees from 
Sutton Pierce, Younker Hyde MacFarlane, and Sykes McAllister as a medical expert 
witness.


328. Radiol Cardiothorac Imaging. 2025 Dec;7(6):e250033. doi: 10.1148/ryct.250033.

Constructing a Unified Vision-Language Model for Chest Radiograph-based 
Diagnostics, Medical Education, and Data Augmentation.

Yang L(1), Liang X(2), Wang Z(1), Diao Z(2), Huang X(2), Shen D(2), Tan X(3)(4), 
Li H(3)(4), Chen Z(5), Qiu S(#)(3)(4), Zhou L(#)(1).

Author information:
(1)School of Electrical and Computer Engineering, University of Sydney, Street 
Address, Sydney, NSW 2006, Australia.
(2)School of Medical Imaging, Binzhou Medical University, Yantai, Shandong, 
China.
(3)Department of Radiology, The First Affiliated Hospital of Guangzhou 
University of Chinese Medicine, Guangzhou, China.
(4)State Key Laboratory of Traditional Chinese Medicine Syndrome, Guangzhou, 
China.
(5)School of Information and Physical Sciences, College of Engineering, Science 
and Environment, University of Newcastle, Newcastle, Australia.
(#)Contributed equally

Purpose To develop MedXChat, a large language model (LLM) capable of integrating 
radiology report generation, visual question answering (VQA), and text-to-image 
synthesis and evaluate its performance via computational metrics and expert 
radiologist assessments. Materials and Methods In this retrospective study, 
MedXChat was trained on the MIMIC Chest X-ray (MIMIC-CXR) database, comprising 
270 790 chest radiograph-report pairs, 54 138 VQA samples, and 7500 
text-to-image instruction pairs. Data were collected from 2011 to 2016. 
Computational evaluations of MedXChat performance were conducted using the F1 
score, area under the receiver operating characteristic curve (AUC), and Fréchet 
inception distance (FID). Radiologist evaluations involved six experts-three 
junior, two senior, and one supervisor-who assessed 50 random MedXChat outputs 
for accuracy, consistency, and alignment with clinical standards. Results In the 
chest radiograph-to-report test set, MedXChat achieved an AUC of 0.67 (95% CI: 
0.61, 0.75), higher than UniXGen (AUC, 0.54; P < .001) and LLM-CXR (AUC, 0.63; P 
= .02). Its F1 score was 0.44 versus 0.26 (P < .001) and 0.41 (P = .04), 
respectively. In chest radiograph-VQA, MedXChat showed higher accuracy for edema 
(73% vs 54% for LLM-CXR and 60% for LLaVA-Med) and pleural effusion (80% vs 53% 
for LLM-CXR and 61% for LLaVA-Med; all P ≤ .01). In text-to-image synthesis, it 
achieved the lowest FID (43.46 vs 73.29 and 106.17; P < .001) and the highest 
classification accuracy (71.5% vs 68.6% and 67.2%; P ≤ .05), producing 
high-quality images including lateral views. Conclusion MedXChat integrated 
report generation, VQA, and image synthesis within a unified framework, 
achieving state-of-the-art performance. MedXChat may support future professional 
applications and enhance radiologic workflows, education, and data augmentation. 
Keywords: Computer Aided Diagnosis (CAD), Applications - Decision Support, 
Applications - Multimodal, Outcomes Analysis, Technology Assessment, Comparative 
Studies Supplemental material is available for this article. © RSNA, 2025.

DOI: 10.1148/ryct.250033
PMID: 41410571 [Indexed for MEDLINE]


329. BMC Med Inform Decis Mak. 2025 Mar 7;25(1):116. doi: 10.1186/s12911-025-02948-2.

NLP modeling recommendations for restricted data availability in clinical 
settings.

Villena F(1)(2)(3), Bravo-Marquez F(1)(2)(4), Dunstan J(5)(6)(7).

Author information:
(1)Department of Computer Science, Universidad de Chile, Santiago, Chile.
(2)Millennium Institute Foundational Research on Data, Santiago, Chile.
(3)Facultad de Odontología, Universidad San Sebastián, Santiago, Chile.
(4)National Center for Artificial Intelligence, Santiago, Chile.
(5)Millennium Institute Foundational Research on Data, Santiago, Chile. 
jdunstan@uc.cl.
(6)Department of Computer Science, Pontificia Universidad Católica de Chile, 
Santiago, Chile. jdunstan@uc.cl.
(7)Institute for Mathematical and Computing Engineering, Pontificia Universidad 
Católica de Chile, Santiago, Chile. jdunstan@uc.cl.

BACKGROUND: Clinical decision-making in healthcare often relies on unstructured 
text data, which can be challenging to analyze using traditional methods. 
Natural Language Processing (NLP) has emerged as a promising solution, but its 
application in clinical settings is hindered by restricted data availability and 
the need for domain-specific knowledge.
METHODS: We conducted an experimental analysis to evaluate the performance of 
various NLP modeling paradigms on multiple clinical NLP tasks in Spanish. These 
tasks included referral prioritization and referral specialty classification. We 
simulated three clinical settings with varying levels of data availability and 
evaluated the performance of four foundation models.
RESULTS: Clinical-specific pre-trained language models (PLMs) achieved the 
highest performance across tasks. For referral prioritization, Clinical PLMs 
attained an 88.85 % macro F1 score when fine-tuned. In referral specialty 
classification, the same models achieved a 53.79 % macro F1 score, surpassing 
domain-agnostic models. Continuing pre-training with environment-specific data 
improved model performance, but the gains were marginal compared to the 
computational resources required. Few-shot learning with large language models 
(LLMs) demonstrated lower performance but showed potential in data-scarce 
scenarios.
CONCLUSIONS: Our study provides evidence-based recommendations for clinical NLP 
practitioners on selecting modeling paradigms based on data availability. We 
highlight the importance of considering data availability, task complexity, and 
institutional maturity when designing and training clinical NLP models. Our 
findings can inform the development of effective clinical NLP solutions in 
real-world settings.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-02948-2
PMCID: PMC11889813
PMID: 40055634 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The ethics committee at the South East Metropolitan Health Service 
has waived the need for approval. The Waiting List Dataset was collected through 
the Chilean Transparency Law ( www.portaltransparencia.cl ). Consent for 
publication: Not applicable. Competing interests: The authors declare no 
competing interests.


330. Crit Care. 2025 Nov 14;29(1):491. doi: 10.1186/s13054-025-05739-3.

Enhancing predictive modeling for respiratory support with LLM-driven guideline 
adherence.

Lu X(1), Miller M(2), Pearce AK(2), Gupta P(2), Pham TT(2), Ford J(2), Malhotra 
A(2), Nemati S(3)(4).

Author information:
(1)Department of Biomedical Informatics, University of California, San Diego, La 
Jolla, CA, USA.
(2)Division of Pulmonary, Critical Care, and Sleep Medicine, University of 
California, San Diego, La Jolla, CA, USA.
(3)Department of Biomedical Informatics, University of California, San Diego, La 
Jolla, CA, USA. snemati@health.ucsd.edu.
(4), 9500 Gilman Drive MC 0881, La Jolla, CA, 92093, USA. 
snemati@health.ucsd.edu.

Update of
    Res Sq. 2025 Aug 12:rs.3.rs-7230335. doi: 10.21203/rs.3.rs-7230335/v1.

BACKGROUND: Optimal respiratory support selection between high-flow nasal 
cannula (HFNC) and noninvasive ventilation (NIV) for intensive care units (ICU) 
patients at risk of invasive mechanical ventilation (IMV) remains unclear, 
particularly in cases not represented in prior clinical trials. We previously 
developed RepFlow-CFR, a deep counterfactual model estimating individualized 
treatment effects (ITE) of HFNC versus NIV. However, interpretability and 
guideline alignment remain challenges for clinical adoption. This study 
describes the development and integration of a clinical guideline-driven LLM to 
enhance deep counterfactual model recommendations for NIV versus HFNC in 
patients at high-risk for invasive mechanical ventilation.
METHODS: We enhanced RepFlow-CFR by incorporating a large language model (LLM, 
Claude 3.5 Sonnet) to enforce clinical guideline adherence and generate 
explainable treatment recommendations. The LLM was configured in a 
HIPAA-compliant AWS environment and prompted using structured patient data, 
clinical notes, and formal guideline criteria. Recommendations from RepFlow-CFR 
and LLM were compared to actual treatment decisions to assess concordance. We 
evaluated IMV and mortality/hospice rates across concordant and discordant 
groups. Additionally, we conducted a structured chart review of 20 cases to 
assess the clinical validity and safety of LLM-driven recommendations.
RESULTS: Among 1,261 ICU encounters, treatments concordant with LLM-enhanced 
recommendations were associated with lower IMV rates. For the HFNC 
recommendation, IMV occurred in 46/188 (24.47%) when care was concordant versus 
9/17(52.94%) when discordant, corresponding to a 97.33% relative risk increase 
when discordant. Concordance was also associated with reduced mortality or 
hospice discharge (odds ratio 0.670, p = 0.046). In a 20-case chart review, 
19/20 (95%) LLM recommendations aligned with clinical guidelines and physicians 
agreed with 13/20 (65%) final recommendations. Errors were noted in 11/20 cases, 
most rated low or moderate risk; 2/20 were judged as potentially causing severe 
harm.
CONCLUSIONS: Integrating LLMs for guideline enforcement improves the 
interpretability and clinical alignment of counterfactual models in respiratory 
support decision-making. This hybrid framework not only enhances concordance 
with real-world practice but may also improve patient outcomes. Future work will 
refine contraindication detection and expand validation to prospective clinical 
trials.

© 2025. The Author(s).

DOI: 10.1186/s13054-025-05739-3
PMCID: PMC12619420
PMID: 41239417 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Ethics approval was obtained from the University of California San 
Diego Institutional Review Board (UC San Diego IRB Protocol \#800258 (“VentNet: 
A Real-Time Multimodal Data Integration Model for Prediction of Respiratory 
Failure in Patients with COVID-19”)). Consent for publication: Not applicable. 
Competing interests: The authors declare no competing interests.


331. J Craniofac Surg. 2026 Jan 19. doi: 10.1097/SCS.0000000000012415. Online ahead 
of print.

Quality and Reliability of AI Information on Dental Implant Failure: A 
Comparative Multi-Model Analysis.

Ceylan Şen S(1), Çardakci Bahar Ş, Saraç Atagün Ö, Ustaoğlu G, Yildiz ZH, Toker 
H.

Author information:
(1)Department of Periodontology, Gulhane Faculty of Dentistry, University of 
Health Sciences, Ankara, Turkey.

OBJECTIVE: This study aimed to develop a consensus-based set of patient 
questions on dental implant failure and to compare the clarity, quality, 
accuracy, reliability, and readability of responses generated by 4 widely used 
AI chatbots: ChatGPT-4, DeepSeek-R1, Microsoft Copilot, and Google Gemini.
METHODS: Twenty-three expert-validated questions were derived from the EAO 2021 
and ICOI Pisa Consensus reports and independently submitted to each AI model 
under standardized, non-personalized conditions. Responses were assessed using 
CLEAR criteria, mGQS, a 5-point accuracy scale, the first 8 DISCERN items, and 
Flesch-based readability indices. Nonparametric tests were used for intermodel 
comparisons.
RESULTS: AI models demonstrated significant variability in performance. Gemini 
achieved the highest accuracy (P<0.001), whereas ChatGPT-4 exhibited the highest 
reliability based on DISCERN scores. Copilot generated the most structurally 
fluent responses, whereas DeepSeek-R1 offered the best readability. Although 
CLEAR and mGQS scores were high across all systems, readability and linguistic 
complexity varied markedly. Accuracy, clarity, and reliability were strongly 
correlated, whereas readability displayed the expected inverse association with 
grade-level demand.
CONCLUSIONS: AI chatbots hold potential as adjunct tools for patient education 
on implant failure; however, their performance characteristics differ 
substantially. Gemini excels in accuracy, ChatGPT-4 in reliability, Copilot in 
fluency, and DeepSeek-R1 in readability. Model-specific guidance and continued 
refinement are needed to enhance the clinical usefulness and accessibility of 
AI-generated patient information.

Copyright © 2026 by Mutaz B. Habal, MD.

DOI: 10.1097/SCS.0000000000012415
PMID: 41553071

Conflict of interest statement: The authors report no conflicts of interest.


332. J Med Internet Res. 2025 Jun 20;27:e70971. doi: 10.2196/70971.

Using Natural Language Processing to Explore Patient Perspectives on AI Avatars 
in Support Materials for Patients With Breast Cancer: Survey Study.

Cheese E(1), Bichoo RA(2), Grover K(2), Dumitru D(2), Zenonos A(1), Groark J(1), 
Gibson D(1), Pope R(1).

Author information:
(1)Roche Products Ltd UK, Welwyn Garden City, United Kingdom.
(2)Hull University Teaching Hospital NHS Trust, Hull, United Kingdom.

BACKGROUND: Having well-informed patients is crucial to enhancing patient 
satisfaction, quality of life, and health outcomes, which in turn optimizes 
health care use. Traditional methods of delivering information, such as booklets 
and leaflets, are often ineffective and can overwhelm patients. Educational 
videos represent a promising alternative; however, their production typically 
requires significant time and financial resources. Video production using 
generative artificial intelligence (AI) technology may provide a solution to 
this problem.
OBJECTIVE: This study aimed to use natural language processing (NLP) to 
understand free-text patient feedback on 1 of 7 AI-generated patient educational 
videos created in collaboration with Roche UK and the Hull University Teaching 
Hospitals NHS Trust breast cancer team, titled "Breast Cancer Follow Up 
Programme."
METHODS: A survey was sent to 400 patients who had completed the breast cancer 
treatment pathway, and 98 (24.5%) free-text responses were received for the 
question "Any comments or suggestions to improve its [the video's] contents?" We 
applied and evaluated different NLP machine learning techniques to draw insights 
from these unstructured data, namely sentiment analysis, topic modeling, 
summarization, and term frequency-inverse document frequency word clouds.
RESULTS: Sentiment analysis showed that 81% (79/98) of the responses were 
positive or neutral, while negative comments were predominantly related to the 
AI avatar. Topic modeling using BERTopic with k-means clustering was found to be 
the most effective model and identified 4 key topics: the breast cancer 
treatment pathway, video content, the digital avatar or narrator, and short 
responses with little or no content. The term frequency-inverse document 
frequency word clouds indicated positive sentiment about the treatment pathway 
(eg, "reassured" and "faultless") and video content (eg, "informative" and 
"clear"), whereas the AI avatar was often described negatively (eg, 
"impersonal"). Summarization using the text-to-text transfer transformer model 
effectively created summaries of the responses by topic.
CONCLUSIONS: This study demonstrates the success of NLP techniques in 
efficiently generating insights into patient feedback related to generative AI 
educational content. Combining NLP methods resulted in clear visuals and 
insights, enhancing the understanding of patient feedback. Analysis of free-text 
responses provided clinicians at Hull University Teaching Hospitals NHS Trust 
with deeper insights than those obtained from quantitative Likert scale 
responses alone. Importantly, the results validate the use of generative AI in 
creating patient educational videos, highlighting its potential to address the 
challenges of costly video production and the limitations of traditional, often 
overwhelming educational leaflets. Despite the positive overall feedback, 
negative comments focused on the technical aspects of the AI avatar, indicating 
areas for improvement. We advocate that patients who receive AI avatar 
explanations are counseled that this technology is intended to supplement, not 
replace, human health care interactions. Future investigations are needed to 
confirm the ongoing effectiveness of these educational tools.

©Eleanor Cheese, Raouef Ahmed Bichoo, Kartikae Grover, Dorin Dumitru, Alexandros 
Zenonos, Joanne Groark, Douglas Gibson, Rebecca Pope. Originally published in 
the Journal of Medical Internet Research (https://www.jmir.org), 20.06.2025.

DOI: 10.2196/70971
PMCID: PMC12228011
PMID: 40540733 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


333. BMC Med Inform Decis Mak. 2025 Apr 10;25(1):160. doi: 
10.1186/s12911-025-02994-w.

MISTIC: a novel approach for metastasis classification in Italian electronic 
health records using transformers.

Lilli L(#)(1)(2), Santoro M(#)(3), Masiello V(1), Patarnello S(4), Tagliaferri 
L(1)(2), Marazzi F(1), Capocchiano ND(1).

Author information:
(1)Fondazione Policlinico Universitario Agostino Gemelli IRCCS, Rome, Italy.
(2)Catholic University of the Sacred Heart, Rome, Italy.
(3)Istituto per le Applicazioni del Calcolo "Mauro Picone", Italian National 
Research Council, Rome, Italy.
(4)Fondazione Policlinico Universitario Agostino Gemelli IRCCS, Rome, Italy. 
stefano.patarnello@gemelligenerator.it.
(#)Contributed equally

BACKGROUND: Analysis of Electronic Health Records (EHRs) is crucial in 
real-world evidence (RWE), especially in oncology, as it provides valuable 
insights into the complex nature of the disease. The implementation of advanced 
techniques for automated extraction of structured information from textual data 
potentially enables access to expert knowledge in highly specialized contexts. 
In this paper, we introduce MISTIC, a Natural Language Processing (NLP) approach 
to classify the presence or absence of metastasis in Italian EHRs, in the breast 
cancer domain.
METHODS: Our approach consists of a transformer-based framework designed for 
few-shot learning, requiring a small labelled dataset and minimal computational 
resources for training. The pipeline includes text segmentation to improve model 
processing and topic analysis to filter informative content, ensuring relevant 
input data for classification.
RESULTS: MISTIC was evaluated across multiple data sources, and compared to 
several benchmark methodologies, ranging from a pattern-matching system, 
composed of regex and semantic rules, to BERT-based models implemented in a 
zero-shot learning setup and Large Language Models (LLMs). The results 
demonstrate the generalization of our approach, achieving an F-Score above 87% 
on all the sources, and outperforming the other experiments, with an overall 
F-Score of 91.2%.
CONCLUSIONS: MISTIC achieves high performance in the Italian metastasis 
classification task, outperforming rule-based systems, zero-shot BERT models, 
and LLMs. Its few-shot learning setup offers a computationally efficient 
alternative to large-scale models, while its segmentation and topic analysis 
steps enhance explainability by explicitly linking predictions to key textual 
elements. Furthermore, MISTIC demonstrates strong generalization across 
different data sources, reinforcing its potential as a scalable and transparent 
solution for clinical text classification. By extracting high-quality metastatic 
information from diverse textual data, MISTIC supports medical researchers in 
analyzing unstructured and highly informative content across a wide range of 
medical reports. In doing so, it enhances data accessibility and 
interpretability, addressing a critical gap in health informatics and clinical 
practice.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-02994-w
PMCID: PMC11987267
PMID: 40211241 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The use of data for this study has been implemented in full 
compliance with ethics and GDPR requirements. Specifically, data usage has been 
approved by the Ethics Committee Policlinico Gemelli to conduct the presented 
research (Protocol Number: 2889256), and the de-identification of sensitive data 
has been performed. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests


334. JMIR Med Inform. 2026 Jan 2;14:e79039. doi: 10.2196/79039.

Large Language Model-Based Virtual Patient Systems for History-Taking in Medical 
Education: Comprehensive Systematic Review.

Li D(1), Lebai Lutfi S(2).

Author information:
(1)Artificial Intelligence & Software Engineering, School of Computer Sciences, 
Universiti Sains Malaysia, Penang, Malaysia.
(2)Medical Informatics Department, College of Medicine and Health Sciences, 
Sultan Qaboos University, Al Seeb, Oman.

BACKGROUND: Large language models (LLMs), such as GPT-3.5 and GPT-4 (OpenAI), 
have been transforming virtual patient systems in medical education by providing 
scalable and cost-effective alternatives to standardized patients. However, 
systematic evaluations of their performance, particularly for multimorbidity 
scenarios involving multiple coexisting diseases, are still limited.
OBJECTIVE: This systematic review aimed to evaluate LLM-based virtual patient 
systems for medical history-taking, addressing four research questions: (1) 
simulated patient types and disease scope, (2) performance-enhancing techniques, 
(3) experimental designs and evaluation metrics, and (4) dataset characteristics 
and availability.
METHODS: Following PRISMA (Preferred Reporting Items for Systematic Reviews and 
Meta-Analyses) 2020, 9 databases were searched (January 1, 2020, to August 18, 
2025). Nontransformer LLMs and non-history-taking tasks were excluded. 
Multidimensional quality and bias assessments were conducted.
RESULTS: A total of 39 studies were included, screened by one computer science 
researcher under supervision. LLM-based virtual patient systems mainly simulated 
internal medicine and mental health disorders, with many addressing distinct 
single disease types but few covering multimorbidity or rare conditions. 
Techniques like role-based prompts, few-shot learning, multiagent frameworks, 
knowledge graph (KG) integration (top-k accuracy 16.02%), and fine-tuning 
enhanced dialogue and diagnostic accuracy. Multimodal inputs (eg, speech and 
imaging) improved immersion and realism. Evaluations, typically involving 10-50 
students and 3-10 experts, demonstrated strong performance (top-k accuracy: 
0.45-0.98, hallucination rate: 0.31%-5%, System Usability Scale [SUS] ≥80). 
However, small samples, inconsistent metrics, and limited controls restricted 
generalizability. Common datasets such as MIMIC-III (Medical Information Mart 
for Intensive Care-III) exhibited intensive care unit (ICU) bias and lacked 
diversity, affecting reproducibility and external validity.
CONCLUSIONS: Included studies showed moderate risk of bias, inconsistent 
metrics, small cohorts, and limited dataset transparency. LLM-based virtual 
patient systems excel in simulating multiple disease types but lack 
multimorbidity patient representation. KGs improve top-k accuracy and support 
structured disease representation and reasoning. Future research should 
prioritize hybrid KG-chain-of-thought architectures integrated with open-source 
KGs (eg, UMLS [Unified Medical Language System] and SNOMED-CT [Systematized 
Nomenclature of Medicine - Clinical Terms]), parameter-efficient fine-tuning, 
dialogue compression, multimodal LLMs, standardized metrics, larger cohorts, and 
open-access multimodal datasets to further enhance realism, diagnostic accuracy, 
fairness, and educational utility.

©Dongliang Li, Syaheerah Lebai Lutfi. Originally published in JMIR Medical 
Informatics (https://medinform.jmir.org), 02.01.2026.

DOI: 10.2196/79039
PMCID: PMC12811743
PMID: 41481915 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


335. J Med Internet Res. 2025 Jul 31;27:e75567. doi: 10.2196/75567.

Evaluating the Performance of State-of-the-Art Artificial Intelligence Chatbots 
Based on the WHO Global Guidelines for the Prevention of Surgical Site 
Infection: Cross-Sectional Study.

Wang T(#)(1), Chen R(#)(1), Wang B(#)(1), Zou C(1), Fan N(1), Yuan S(1), Wang 
A(1), Xi Y(1), Zang L(1).

Author information:
(1)Beijing Chao-Yang Hospital, 5 JingYuan Road, Shijingshan District, Beijing, 
100043, China, 86 51718268.
(#)Contributed equally

BACKGROUND: Surgical site infection (SSI) is the most prevalent type of health 
care-associated infection that leads to increased morbidity and mortality and a 
significant economic burden. Effective prevention of SSI relies on surgeons 
strictly following the latest clinical guidelines and implementing standardized 
and multilevel intervention strategies. However, the frequent updates to 
clinical guidelines render the processes of acquisition and interpretation quite 
time-consuming and intricate. The emergence of artificial intelligence (AI) 
chatbots offers both possibilities and challenges to address these issues in the 
surgical field.
OBJECTIVE: This study aimed to test the multidimensional capability of 
state-of-the-art AI chatbots for generating proper recommendations and 
corresponding rationales concordant with the global guideline for the prevention 
of SSI.
METHODS: Referred by other authoritative guidelines, recommendations and 
corresponding rationales from the 2018 World Health Organization global 
guidelines were refined and selected as benchmarks. Then, they were rephrased 
into a combined format of closed-ended queries for recommendations and 
open-ended queries for corresponding rationales, whereafter input into 
ChatGPT-4o (OpenAI), OpenAI-o1 (OpenAI), Claude 3.5 Sonnet (Anthropic), and 
Gemini 1.5 Pro (Google) 3 times. All responses were individually evaluated in 10 
evaluation metrics based on the QUEST dimensions by 4 multidisciplinary senior 
surgeons using a 5-point Likert scale. The multidimensional performances among 
chatbots were compared, and the interrater agreements were calculated.
RESULTS: A total of 300 responses to 25 queries were generated by the 4 
chatbots. The interrater agreements of the evaluators ranged from moderate to 
good (0.54-0.87). In response to recommendations, the average accuracy, 
consistency, and harm scores for all chatbots were 4.03 (SD 1.09), 4.07 (SD 
0.88), and 4.29 (SD 1.01), respectively. In responses for rationales, 4 
subdimensions, including harm (mean 4.22, SD 0.97), relevance (mean 4.15, SD 
0.83), fabrication and falsification (mean 4.12, SD 1.02), and understanding and 
reasoning (mean 4.04, SD 0.92) averagely scored ≥4. In contrast, consistency 
(mean 3.94, SD 0.72), clarity (mean 3.94, SD 0.89), comprehensiveness (mean 
3.85, SD 0.83), and accuracy (mean 3.74, SD 0.91) performed at a moderate level. 
For the whole responses, the average self-awareness and trust and confidence 
scores for all chatbots were 3.84 (SD 0.89) and 3.88 (SD 0.91), respectively. 
Based on the average scores of the subdimensions, Claude 3.5 Sonnet and 
ChatGPT-4o were the top 2 outperformed models.
CONCLUSIONS: The performance of AI chatbots in providing responses regarding 
well-established global guidelines in the prevention of SSI was acceptable, 
demonstrating immense potential in clinical applications. Nonetheless, a 
critical issue is the necessity of enhancing the stability of chatbots, as 
inaccurate responses can lead to severe consequences for SSI. Despite its 
limitations, it is anticipated that AI will trigger far-reaching changes in how 
clinicians access and use medical information.

© Tianyi Wang, Ruiyuan Chen, Baodong Wang, Congying Zou, Ning Fan, Shuo Yuan, 
Aobo Wang, Yu Xi, Lei Zang. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org).

DOI: 10.2196/75567
PMCID: PMC12313333
PMID: 40744114 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


336. Radiologie (Heidelb). 2025 Apr;65(4):243-248. doi: 10.1007/s00117-025-01431-3. 
Epub 2025 Mar 12.

[Integration of large language models into the clinic : Revolution in analysing 
and processing patient data to increase efficiency and quality in radiology].

[Article in German; Abstract available in German from the publisher]

Arnold P(1), Henkel M(2), Bamberg F(3), Kotter E(3).

Author information:
(1)Klinik für Diagnostische und Interventionelle Radiologie, 
Universitätsklinikum Freiburg, Hugstetterstr. 55, 79106, Freiburg, Deutschland. 
philipp.arnold@uniklinik-freiburg.de.
(2)Abteilung für Forschung und Analyse Services, Universitätsspital Basel, 
Basel, Schweiz.
(3)Klinik für Diagnostische und Interventionelle Radiologie, 
Universitätsklinikum Freiburg, Hugstetterstr. 55, 79106, Freiburg, Deutschland.

BACKGROUND: Large Language Models (LLMs) like ChatGPT, Llama and Claude are 
transforming healthcare by interpreting complex text, extracting information, 
and providing guideline-based support. Radiology, with its high patient volume 
and digital workflows, is a ideal field for LLM integration.
OBJECTIVE: Assessment of the potential of LLMs to enhance efficiency, 
standardization, and decision support in radiology, while addressing ethical and 
regulatory challenges.
MATERIAL AND METHODS: Pilot studies at Freiburg and Basel university hospitals 
evaluated local LLM systems for tasks like prior report summarization and 
guideline-driven reporting. Integration with Picture Archiving and Communication 
System (PACS) and Electronic Health Record (EHR) systems was achieved via 
Digital Imaging and Communications in Medicine (DICOM) and Fast Healthcare 
Interoperability Resources (FHIR) standards. Metrics included time savings, 
compliance with the European Union (EU) Artificial Intelligence (AI) Act, and 
user acceptance.
RESULTS: LLMs demonstrate significant potential as a support tool for 
radiologists in clinical practice by reducing reporting times, automating 
routine tasks, and ensuring consistent, high-quality results. They also support 
interdisciplinary workflows (e.g., tumor boards) and meet data protection 
requirements when locally implemented.
DISCUSSION: Local LLM systems are feasible and beneficial in radiology, 
enhancing efficiency and diagnostic quality. Future work should refine 
transparency, expand applications, and ensure LLMs complement medical expertise 
while adhering to ethical and legal standards.

Publisher: ZUSAMMENFASSUNG: HINTERGRUND: Large Language Models (LLMs) wie 
ChatGPT, Llama oder Claude revolutionieren das Gesundheitswesen, indem sie 
komplexe Texte interpretieren, Informationen extrahieren und leitlinienbasierte 
Unterstützung bieten. Die Radiologie mit ihrem hohen Patientenaufkommen und 
digitalen Workflows, ist ein ideales Anwendungsfeld für die Integration von 
LLMs. ZIEL: Bewertung des Potenzials von LLMs zur Steigerung von Effizienz, 
Standardisierung und Entscheidungsunterstützung in der Radiologie unter 
Berücksichtigung ethischer und regulatorischer Herausforderungen.
MATERIAL UND METHODEN: Pilotstudien an den Universitätskliniken Freiburg und 
Basel untersuchten lokale LLM-Systeme für Aufgaben wie die Zusammenfassung von 
Vorbefunden und leitliniengestützte Befundung. Die Integration mit Picture 
Archiving and Communication System(PACS)- und Electronic Health 
Record(EHR)-Systemen erfolgte über Digital Imaging and Communications in 
Medicine(DICOM)- und Fast Healthcare Interoperability Resources(FHIR)-Standards. 
Bewertet wurden Zeiteinsparungen, Einhaltung des European Union (EU) Artificial 
Intelligence (AI) Act und die Akzeptanz durch Anwender.
ERGEBNISSE: LLMs zeigen ein großes Potenzial als Unterstützung des Radiologen in 
der klinischen Routine, indem sie Befundungszeiten reduzieren, 
Routinetätigkeiten automatisieren und konsistente, hochwertige Ergebnisse 
gewährleisten. Sie unterstützen auch interdisziplinäre Workflows (z. B. 
Tumorboards) und genügen bei lokaler Installation auch Datenschutzanforderungen.
DISKUSSION: Lokale LLM-Systeme sind in der Radiologie praktikabel und 
vorteilhaft, da sie Effizienz und diagnostische Qualität verbessern. Zukünftige 
Entwicklungen sollten Transparenz und Anwendungsfelder erweitern sowie 
sicherstellen, dass LLMs die ärztliche Expertise ergänzen und gleichzeitig 
ethischen und rechtlichen Standards entsprechen.

© 2025. The Author(s), under exclusive licence to Springer Medizin Verlag GmbH, 
ein Teil von Springer Nature.

DOI: 10.1007/s00117-025-01431-3
PMID: 40072530 [Indexed for MEDLINE]

Conflict of interest statement: Einhaltung ethischer Richtlinien. 
Interessenkonflikt: P. Arnold, M. Henkel, F. Bamberg und E. Kotter geben an, 
dass kein Interessenkonflikt besteht. Für diesen Beitrag wurden von den 
Autor/-innen keine Studien an Menschen oder Tieren durchgeführt. Für die 
aufgeführten Studien gelten die jeweils dort angegebenen ethischen Richtlinien.


337. Arthroscopy. 2025 May;41(5):1337-1347. doi: 10.1016/j.arthro.2024.06.017. Epub 
2024 Jun 22.

ChatGPT Provides Satisfactory but Occasionally Inaccurate Answers to Common 
Patient Hip Arthroscopy Questions.

AlShehri Y(1), McConkey M(2), Lodhia P(3).

Author information:
(1)Department of Orthopaedics, Faculty of Medicine, The University of British 
Columbia, Vancouver, BC, Canada; Department of Orthopedics, College of Medicine, 
Imam Abdulrahman Bin Faisal University, Dammam, Saudi Arabia.
(2)Department of Orthopaedics, Faculty of Medicine, The University of British 
Columbia, Vancouver, BC, Canada.
(3)Department of Orthopaedics, Faculty of Medicine, The University of British 
Columbia, Vancouver, BC, Canada. Electronic address: parth.lodhia@ubc.ca.

PURPOSE: To assess the ability of ChatGPT to answer common patient questions 
regarding hip arthroscopy, and to analyze the accuracy and appropriateness of 
its responses.
METHODS: Ten questions were selected from well-known patient education websites, 
and ChatGPT (version 3.5) responses to these questions were graded by 2 
fellowship-trained hip preservation surgeons. Responses were analyzed, compared 
with the current literature, and graded from A to D (A being the highest, and D 
being the lowest) in a grading scale on the basis of the accuracy and 
completeness of the response. If the grading differed between the 2 surgeons, a 
consensus was reached. Inter-rater agreement was calculated. The readability of 
responses was also assessed using the Flesch-Kincaid Reading Ease Score (FRES) 
and Flesch-Kincaid Grade Level (FKGL).
RESULTS: Responses received the following consensus grades: A (50%, n = 5), B 
(30%, n = 3), C (10%, n = 1), D (10%, n = 1). Inter-rater agreement on the basis 
of initial individual grading was 30%. The mean FRES was 28.2 (± 9.2 standard 
deviation), corresponding to a college graduate level, ranging from 11.7 to 
42.5. The mean FKGL was 14.4 (±1.8 standard deviation), ranging from 12.1 to 18, 
indicating a college student reading level.
CONCLUSIONS: ChatGPT can answer common patient questions regarding hip 
arthroscopy with satisfactory accuracy graded by 2 high-volume hip 
arthroscopists; however, incorrect information was identified in more than one 
instance. Caution must be observed when using ChatGPT for patient education 
related to hip arthroscopy.
CLINICAL RELEVANCE: Given the increasing number of hip arthroscopies being 
performed annually, ChatGPT has the potential to aid physicians in educating 
their patients about this procedure and addressing any questions they may have.

Crown Copyright © 2024. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.arthro.2024.06.017
PMID: 38914299 [Indexed for MEDLINE]

Conflict of interest statement: Disclosures All authors (Y.A., M.M., P.L.) 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


338. J Adv Nurs. 2025 Nov;81(11):8016-8027. doi: 10.1111/jan.16812. Epub 2025 Feb 13.

Enhancing Adverse Event Reporting With Clinical Language Models: Inpatient 
Falls.

Cho I(1)(2), Park H(3)(4)(5), Park BS(6), Lee DG(4)(7).

Author information:
(1)College of Nursing, Inha University, Incheon, Republic of Korea.
(2)The Center for Patient Safety Research and Practice, Division of General 
Internal Medicine, Brigham and Women's Hospital, Boston, Massachusetts, USA.
(3)Seoul School of Integrated Science & Technologies, aSSIST University, Seoul, 
Republic of Korea.
(4)Graduate School of Artificial Intelligence, Pohang University of Science and 
Technology, Gyeongbuk, Republic of Korea.
(5)Graduate School of Information & Telecommunications, Konkuk University, 
Seoul, Republic of Korea.
(6)Department of Electrical and Computer Engineering, Inha University, Incheon, 
Republic of Korea.
(7)Department of Information and Communication Engineering, Inha University, 
Incheon, Republic of Korea.

AIMS: To develop a method for computationally detecting fall events using 
clinical language models to complement existing self-reporting mechanisms.
DESIGN: Retrospective observational study.
METHODS: Text data were collected from the unstructured nursing notes of three 
hospitals' electronic health records and the Korean national patient safety 
reports, totalling 34,480 records covering the period from January 2015 to 
December 2019. Note-level labelling was conducted by two researchers with 95% 
agreement. Preprocessing data anonymisation and English translation were 
followed by semantic validation. Five language models based on pretrained 
Bidirectional Encoder Representations from Transformers (BERT) and Generative 
Pretrained Transformer (GPT)-4 with prompt programming were explored. Model 
performance was assessed using F measurements. Error analysis was conducted for 
the GPT-4 results.
RESULTS: Fine-tuned BERT models with the English data set outperformed GPT-4, 
with Bio+Clinical BERT achieving the highest F1 score of 0.98. Fine-tuned Korean 
BERT with the Korean data set also reached an F1 score of 0.98, while GPT-4 
achieved a competitive F1 score of 0.94. GPT-4 with prompt programming showed 
much higher F1 scores than GPT-4 with a standardised prompt for the English data 
set (0.85 vs. 0.39) and the Korean data set (0.94 vs. 0.03). The error analysis 
identified that the common misclassification patterns included fall history and 
homonyms, causing false positives and implicit expressions and missing 
contextual information, causing false negatives.
CONCLUSION: The clinical language model approach, if used alongside the existing 
self-reporting, promises to increase the chance of identifying the majority of 
factual falls without the need for additional chart reviews.
IMPACT: Inpatient falls are often underreported, with up to 91% of incidents 
missed in self-reports. Using language models, we identified a significant 
portion of these unreported falls, improving the accuracy of adverse event 
tracking while reducing the self-reporting burden on nurses.
PATIENT OR PUBLIC CONTRIBUTION: Not applicable.

© 2025 The Author(s). Journal of Advanced Nursing published by John Wiley & Sons 
Ltd.

DOI: 10.1111/jan.16812
PMCID: PMC12535311
PMID: 39948219 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


339. Aesthetic Plast Surg. 2025 Apr;49(7):1874-1880. doi: 10.1007/s00266-024-04377-4. 
Epub 2024 Sep 25.

Advancement of Generative Pre-trained Transformer Chatbots in Answering Clinical 
Questions in the Practical Rhinoplasty Guideline.

Shiraishi M(#)(1), Tsuruda S(2), Tomioka Y(2), Chang J(2), Hori A(2), Ishii 
S(2), Fujinaka R(2), Ando T(2), Ohba J(2), Okazaki M(2).

Author information:
(1)Department of Plastic and Reconstructive Surgery, The University of Tokyo 
Hospital, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8655, Japan. 
shiraishi-kyf@umin.ac.jp.
(2)Department of Plastic and Reconstructive Surgery, The University of Tokyo 
Hospital, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8655, Japan.
(#)Contributed equally

BACKGROUND: The Generative Pre-trained Transformer (GPT) series, which includes 
ChatGPT, is an artificial large language model that provides human-like text 
dialogue. This study aimed to evaluate the performance of artificial 
intelligence chatbots in answering clinical questions based on practical 
rhinoplasty guidelines.
METHODS: Clinical questions (CQs) developed from the guidelines were used as 
question sources. For each question, we asked GPT-4 and GPT-3.5 (ChatGPT), 
developed by OpenAI, to provide answers for the CQs, Policy Level, Aggregate 
Evidence Quality, Level of Confidence in Evidence, and References. We compared 
the performance of the two types of artificial intelligence (AI) chatbots.
RESULTS: A total of 10 questions were included in the final analysis, and the AI 
chatbots correctly answered 90.0% of these. GPT-4 demonstrated a lower accuracy 
rate than GPT-3.5 in answering CQs, although without statistically significant 
difference (86.0% vs. 94.0%; p = 0.05), whereas GPT-4 showed significantly 
higher accuracy for the level of confidence in Evidence than GPT-3.5 (52.0% vs. 
28.0%; p < 0.01). No statistical differences were observed in Policy Level, 
Aggregate Evidence Quality, and Reference Match. In addition, GPT-4 rated 
significantly higher in presenting existing references than GPT-3.5 (36.9% vs. 
24.1%; p = 0.01).
CONCLUSIONS: The overall performance of GPT-4 was similar to that of GPT-3.5. 
However, GPT-4 provided existing references at a higher rate than GPT-3.5. GPT-4 
has the potential to provide a more accurate reference in professional fields, 
including rhinoplasty.
LEVEL OF EVIDENCE V: This journal requires that authors assign a level of 
evidence to each article. For a full description of these Evidence-Based 
Medicine ratings, please refer to the Table of Contents or the online 
Instructions to Authors www.springer.com/00266 .

© 2024. Springer Science+Business Media, LLC, part of Springer Nature and 
International Society of Aesthetic Plastic Surgery.

DOI: 10.1007/s00266-024-04377-4
PMID: 39322837 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare that they have no conflicts of interest to disclose. Human and Animal 
Rights: This article does not contain any studies with human participants or 
animals performed by any of the authors. Informed Consent: For this type of 
study, informed consent is not required.


340. BMC Med Inform Decis Mak. 2025 Apr 14;25(1):163. doi: 
10.1186/s12911-025-02996-8.

Comparative evaluation of artificial intelligence models GPT-4 and GPT-3.5 in 
clinical decision-making in sports surgery and physiotherapy: a cross-sectional 
study.

Saglam S(1), Uludag V(2), Karaduman ZO(3), Arıcan M(3), Yücel MO(3), Dalaslan 
RE(3).

Author information:
(1)Department of Orthopaedics and Traumatology, Faculty of Medicine, Duzce 
University, Duzce, Türkiye. dr.sonmezsaglam@gmail.com.
(2)Department of Physiotherapy and Rehabilitation, Faculty of Health Sciences, 
Duzce University, Duzce, Türkiye.
(3)Department of Orthopaedics and Traumatology, Faculty of Medicine, Duzce 
University, Duzce, Türkiye.

BACKGROUND: The integration of artificial intelligence (AI) in healthcare has 
rapidly expanded, particularly in clinical decision-making. Large language 
models (LLMs) such as GPT-4 and GPT-3.5 have shown potential in various medical 
applications, including diagnostics and treatment planning. However, their 
efficacy in specialized fields like sports surgery and physiotherapy remains 
underexplored. This study aims to compare the performance of GPT-4 and GPT-3.5 
in clinical decision-making within these domains using a structured assessment 
approach.
METHODS: This cross-sectional study included 56 professionals specializing in 
sports surgery and physiotherapy. Participants evaluated 10 standardized 
clinical scenarios generated by GPT-4 and GPT-3.5 using a 5-point Likert scale. 
The scenarios encompassed common musculoskeletal conditions, and assessments 
focused on diagnostic accuracy, treatment appropriateness, surgical technique 
detailing, and rehabilitation plan suitability. Data were collected anonymously 
via Google Forms. Statistical analysis included paired t-tests for direct model 
comparisons, one-way ANOVA to assess performance across multiple criteria, and 
Cronbach's alpha to evaluate inter-rater reliability.
RESULTS: GPT-4 significantly outperformed GPT-3.5 across all evaluated criteria. 
Paired t-test results (t(55) = 10.45, p < 0.001) demonstrated that GPT-4 
provided more accurate diagnoses, superior treatment plans, and more detailed 
surgical recommendations. ANOVA results confirmed the higher suitability of 
GPT-4 in treatment planning (F(1, 55) = 35.22, p < 0.001) and rehabilitation 
protocols (F(1, 55) = 32.10, p < 0.001). Cronbach's alpha values indicated 
higher internal consistency for GPT-4 (α = 0.478) compared to GPT-3.5 
(α = 0.234), reflecting more reliable performance.
CONCLUSIONS: GPT-4 demonstrates superior performance compared to GPT-3.5 in 
clinical decision-making for sports surgery and physiotherapy. These findings 
suggest that advanced AI models can aid in diagnostic accuracy, treatment 
planning, and rehabilitation strategies. However, AI should function as a 
decision-support tool rather than a substitute for expert clinical judgment. 
Future studies should explore the integration of AI into real-world clinical 
workflows, validate findings using larger datasets, and compare additional AI 
models beyond the GPT series.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-02996-8
PMCID: PMC11998439
PMID: 40229819 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethical approval: The study 
protocol was approved by the Duzce University Non-Interventional Clinical 
Research Ethics Committee (No: 2024/215, Date: 21/10/2024). The study was 
conducted in accordance with the principles of the Declaration of Helsinki. 
Informed consent: Informed consent was obtained from all individual participants 
included in the study. Congress: Not available. Competing interests: The authors 
declare no competing interests.


341. Neurol Educ. 2025 Sep 19;4(4):e200250. doi: 10.1212/NE9.0000000000200250. 
eCollection 2025 Dec.

Education Research: Creating Online Interactive Case-Based Learning Experiences 
From Educational Case Reports With Large Language Models: A Feasibility Study.

Gao C(1), Gheihman G(2)(3), Kaplan T(2)(3), McCoy LG(4), Collins LC(5), Wenzel 
T(5), Paul A(5), Reda H(2)(3), Stein LK(6), Kimbaris G(7), Sutherland HW(8), 
Stacpoole S(9), Milligan TA(10), Goh R(1), Bacchi S(1)(2)(3).

Author information:
(1)Department of Medicine, Adelaide Medical School, Adelaide, Australia.
(2)Department of Neurology, Mass General Brigham, Boston, MA.
(3)Department of Neurology, Harvard Medical School, Boston, MA.
(4)Department of Neurology, University of Alberta, Edmonton, Canada.
(5)Department of Neurology, Austin Health, Melbourne, Australia.
(6)Department of Neurology, Icahn School of Medicine at Mount Sinai, New York, 
NY.
(7)Department of Neurology, University of Pennsylvania, Philadelphia, PA.
(8)Department of Neurology, Yale School of Medicine, New Haven, CT.
(9)Department of Neurology, University of Cambridge, Cambridge, UK; and.
(10)Department of Neurology, New York Medical College, Westchester Medical 
Center, NY.

BACKGROUND AND OBJECTIVES: Case reports are a fundamental part of medical 
literature and education. Artificial intelligence (AI) is increasingly 
influencing medical education and can potentially augment the delivery of the 
educational content in case reports. The aim of this study was to evaluate the 
feasibility of using AI, namely large language models (LLMs), to convert 
previously published Neurology Resident & Fellow Section Case-based Articles 
(RFS-CBAs) into an interactive online format to facilitate case-based learning.
METHODS: Three RFS-CBAs were converted into a free-text "screenplay" using the 
LLM Claude 3.5 Sonnet. These "screenplays" were then delivered in an interactive 
format through an online platform using GPT-4o. Two neurology fellows 
interrogated (prompted) the cases delivered by the online platform in a 
question-and-answer manner, seeking history, examination findings, and 
investigation results to arrive at a diagnosis and plan. These neurology fellows 
were not aware of the case report or screenplay content and asked questions in a 
manner that they would when evaluating a patient. A neurologist then reviewed 
each question-and-answer exchange for "screenplay" adherence and medical 
appropriateness. Results were analyzed with descriptive statistics.
RESULTS: The overall number of appropriate responses generated by the LLM was 
206 of 210 (98.1%). There were 26 of 210 responses in which additional content 
was generated, all of which were medically plausible or consistent with the 
context of the case. The 4 errors that occurred were omissions of investigation 
results at the "screenplay" stage, which are amenable to manual correction. The 
omissions were the results of 3 unrevealing blood tests and 1 
electroencephalogram result. None of these errors precluded the establishment of 
the diagnosis and completion of the case.
DISCUSSION: It is feasible to convert RFS-CBAs into an interactive 
question-and-answer format using LLMs. It should be noted that the 
nondeterministic nature of frontier LLMs and the potential for such LLM versions 
to change frequently are relevant considerations in making estimates of 
performance. Further studies investigating the impacts of this educational 
innovation are required.

© 2025 American Academy of Neurology.

DOI: 10.1212/NE9.0000000000200250
PMCID: PMC12459021
PMID: 41000169

Conflict of interest statement: S. Bacchi is supported by a Fulbright 
Scholarship. L. Stein is supported by American Heart Association Grant 
(#857015/Stein/2021). All other authors report no disclosures. Go to 
Neurology.org/NE for full disclosures.


342. Aesthetic Plast Surg. 2025 Dec 22. doi: 10.1007/s00266-025-05541-0. Online ahead 
of print.

Comment to "Artificial Intelligence (AI)-Assisted Patient Education and Concerns 
Following Facelift Surgery: A Study on ChatGPT-4 and Gemini".

Caimi E(1), Vaccari S(2)(3), Vinci V(3)(4).

Author information:
(1)Department of Medical Biotechnology and Translational Medicine BIOMETRA, 
Reconstructive and Aesthetic Plastic Surgery School, University of Milan, Via 
Festa del Perdono, 7, Milan, Italy. caimi.edo@gmail.com.
(2)Department of Medical Biotechnology and Translational Medicine BIOMETRA, 
Reconstructive and Aesthetic Plastic Surgery School, University of Milan, Via 
Festa del Perdono, 7, Milan, Italy.
(3)Humanitas Clinical and Research Center-IRCCS, Via Manzoni 56, 20089, Rozzano, 
Milan, Italy.
(4)Department of Biomedical Sciences, Humanitas University, Via Rita Levi 
Montalcini 4, 20090, Pieve Emanuele, Milan, Italy.

INTRODUCTION: Artificial intelligence (AI) is increasingly integrated into 
patient education and postoperative care. Almousa et al. recently evaluated 
ChatGPT-4 and Gemini for postoperative facelift counseling, reporting high 
accuracy and clarity. While their study represents an important step toward 
AI-assisted communication in aesthetic surgery, several methodological issues 
may limit the validity and clinical applicability of their findings.
METHODS: We critically appraised Almousa et al.'s study design, data collection, 
and analytic methods. Specific attention was given to question selection, 
evaluation metrics, reproducibility, and statistical robustness, comparing them 
with established standards for AI evaluation and inter-rater reliability.
RESULTS: The study used ChatGPT-4 itself to generate the five "most common" 
postoperative questions, introducing circularity and potential selection bias. 
Responses were assessed on a dichotomous (Yes/No) scale by five surgeons, 
without reporting inter-rater reliability or use of scaled metrics. It was 
unclear whether prompts were entered sequentially or independently, raising 
reproducibility concerns. The limited sample size (five questions per model) 
provided only 25 binary data points per system, precluding meaningful 
statistical inference. Furthermore, AI responses lacked individualized safety 
guidance and escalation advice, limiting clinical safety in real-world 
postoperative settings.
CONCLUSION: Although the study highlights the promise of LLMs in aesthetic 
surgery, future studies should employ patient-derived question sets, graded and 
reproducible evaluation scales, transparent prompt protocols, and inclusion of 
complication-related queries to accurately determine the safety and educational 
value of AI-generated postoperative information.
LEVEL OF EVIDENCE V: This journal requires that authors assign a level of 
evidence to each article. For a full description of these Evidence-Based 
Medicine ratings, please refer to the Table of Contents or the online 
Instructions to Authors www.springer.com/00266.

© 2025. Springer Science+Business Media, LLC, part of Springer Nature and 
International Society of Aesthetic Plastic Surgery.

DOI: 10.1007/s00266-025-05541-0
PMID: 41429969

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare that they have no conflicts of interest to disclose. Human and Animal 
Participants: This article does not contain any studies with human participants 
or animals performed by any of the authors. Informed Consent: For this type of 
study, informed consent is not required.


343. Nutrients. 2025 Mar 13;17(6):1008. doi: 10.3390/nu17061008.

Demographic and Physical Determinants of Unhealthy Food Consumption in Polish 
Long-Term Care Facilities.

Ase A(1), Borowicz J(1), Rakocy K(2), Krzych-Fałta E(3), Samoliński B(1).

Author information:
(1)Department of the Prevention of Environmental Hazard, Allergology and 
Immunology, Faculty of Health Sciences, Medical University of Warsaw, 1a Banacha 
Street, 02-091 Warsaw, Poland.
(2)Interdisciplinary Centre for Mathematical and Computational Modelling, 
University of Warsaw, 15/17 Tyniecka Street, 02-630 Warsaw, Poland.
(3)Department of Basic Nursing, Medical University of Warsaw, 02-097 Warsaw, 
Poland.

Background: Unhealthy food consumption in long-term care facilities (LTCFs) 
contributes to poor health outcomes among residents. This study aimed to assess 
its prevalence, identify demographic and physical risk factors, and propose 
targeted interventions. Methods: A mixed-methods study (2017-2021) analyzed data 
from 1000 Polish LTCF residents (aged 35-105 years). Anthropometric 
measurements, bioimpedance analyses, dietary assessments, and physical activity 
records were collected. Food items were classified as "healthy" or "unhealthy" 
using an AI-based Large Language Model (LLM), applying WHO guidelines and the 
NOVA classification system. Logistic regression and chi-square tests assessed 
associations between unhealthy food consumption and marital status, education 
level, mobility aid use, and portion control. Results: Unhealthy food 
consumption prevalence was 15.6%. Married residents had significantly higher 
rates than unmarried individuals (22.6% vs. 14.3%, p < 0.01). Lower educational 
attainment correlated with increased risk (partial primary education: 34.7% vs. 
tertiary education: 8.1%). Mobility aid users exhibited elevated consumption 
(cane: 34.6%; walker: 22.6%). Poor portion control showed the strongest 
association (OR = 3.2, 95% CI: 1.8-5.7). Conclusions: Marital status, 
educational disparities, mobility limitations, and portion control were key 
modifiable risk factors. Findings suggest the need for targeted nutrition 
programs, caregiver education, and policy reforms to improve dietary literacy 
and meal portioning. Future research should validate AI-based food 
classification methods, assess long-term intervention outcomes, and expand 
studies to diverse LTCF settings. These findings align with Poland's National 
Health Programme and provide actionable insights for global LTCF populations.

DOI: 10.3390/nu17061008
PMCID: PMC11945014
PMID: 40290055 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


344. J Biomed Inform. 2025 Jun;166:104819. doi: 10.1016/j.jbi.2025.104819. Epub 2025 
Apr 16.

RoBIn: A Transformer-based model for risk of bias inference with machine reading 
comprehension.

Dias AC(1), Moreira VP(1), Comba JLD(2).

Author information:
(1)Instituto de Informatica, Av. Bento Goncalves 9500 - Caixa Postal 15064, 
Porto Alegre, 91501-970, Rio Grande do Sul, Brazil.
(2)Instituto de Informatica, Av. Bento Goncalves 9500 - Caixa Postal 15064, 
Porto Alegre, 91501-970, Rio Grande do Sul, Brazil. Electronic address: 
joao.comba@gmail.com.

OBJECTIVE: Scientific publications are essential for uncovering insights, 
testing new drugs, and informing healthcare policies. Evaluating the quality of 
these publications often involves assessing their Risk of Bias (RoB), a task 
traditionally performed by human reviewers. The goal of this work is to create a 
dataset and develop models that allow automated RoB assessment in clinical 
trials.
METHODS: We use data from the Cochrane Database of Systematic Reviews (CDSR) as 
ground truth to label open-access clinical trial publications from PubMed. This 
process enabled us to develop training and test datasets specifically for 
machine reading comprehension and RoB inference. Additionally, we created 
extractive (RoBInExt) and generative (RoBInGen) Transformer-based approaches to 
extract relevant evidence and classify the RoB effectively.
RESULTS: RoBIn was evaluated across various settings and benchmarked against 
state-of-the-art methods, including large language models (LLMs). In most cases, 
the best-performing RoBIn variant surpasses traditional machine learning and 
LLM-based approaches, achieving a AUROC of 0.83.
CONCLUSION: This work addresses RoB assessment in clinical trials by introducing 
RoBIn, two Transformer-based models for RoB inference and evidence retrieval, 
which outperform traditional models and LLMs, demonstrating its potential to 
improve efficiency and scalability in clinical research evaluation. We also 
introduce a public dataset that is automatically annotated and can be used to 
enable future research to enhance automated RoB assessment.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104819
PMID: 40250743 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


345. J Med Syst. 2025 Jun 3;49(1):73. doi: 10.1007/s10916-025-02207-x.

The Role of Artificial Intelligence Large Language Models in Personalized 
Rehabilitation Programs for Knee Osteoarthritis: An Observational Study.

Gürses ÖA(1), Özüdoğru A(2), Tuncay F(3), Kararti C(2).

Author information:
(1)School of Physical Therapy and Rehabilitation, Department of Physiotherapy 
and Rehabilitation, Kırşehir Ahi Evran University, Merkez, Kırşehir, 40100, 
Türkiye. omeralperengurses@gmail.com.
(2)School of Physical Therapy and Rehabilitation, Department of Physiotherapy 
and Rehabilitation, Kırşehir Ahi Evran University, Merkez, Kırşehir, 40100, 
Türkiye.
(3)Faculty of Medicine, Department of Physical Medicine and Rehabilitation, 
Kırşehir Ahi Evran University, Merkez, Kırşehir, 40100, Türkiye.

BACKGROUND: Large language models (LLMs) can contribute to treatment options and 
outcomes by assisting physiotherapists for conditions like osteoarthritis.
AIMS: The objective of this early-stage cross-sectional study is to assess the 
alignment of large language models with physiotherapists in designing 
physiotherapy and rehabilitation programs for knee osteoarthritis.
METHODS: Forty patients diagnosed with knee osteoarthritis were assessed using 
standardized clinical criteria. For each patient, individualized rehabilitation 
programs were created by three physiotherapists and by ChatGPT-4o and Gemini 
Advanced using structured prompts. The presence or absence of 50 clinically 
relevant rehabilitation parameters was recorded for each program. Chi-square 
tests were used to evaluate agreement rates between the LLMs and the 
physiotherapist-generated Consensus programs.
RESULTS: ChatGPT-4o achieved a 74% agreement rate with the physiotherapists' 
Consensus programs, while Gemini Advanced achieved 70%. Although both models 
showed high compatibility with general rehabilitation components, they 
demonstrated notable limitations in exercise specificity, including frequency, 
sets, and progression criteria. ChatGPT-4o performed as well as or better than 
Gemini in most phases, particularly in Phase 3, while Gemini showed lower 
consistency in balance and stabilization parameters.
CONCLUSIONS: ChatGPT-4o and Gemini Advanced demonstrate promising potential in 
generating personalized rehabilitation programs for knee osteoarthritis. While 
their outputs generally align with expert recommendations, notable gaps remain 
in clinical reasoning and the provision of detailed exercise parameters. These 
findings underscore the importance of ongoing model refinement and the necessity 
of expert supervision for safe and effective clinical integration.

© 2025. The Author(s).

DOI: 10.1007/s10916-025-02207-x
PMCID: PMC12134017
PMID: 40459660 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethical Approval: The study was 
conducted in accordance with the Declaration of Helsinki and was approved by the 
Ethics Committee of Kirsehir Ahi Evran University Faculty of Medicine Health 
Sciences Scientific Research(2024-13/110). Informed consent was obtained from 
all participants involved in the study. Competing Interests: The authors declare 
no competing interests. Clinical Trial Number: Not applicable.


346. Arthroscopy. 2025 Aug;41(8):2753-2757. doi: 10.1016/j.arthro.2025.01.039. Epub 
2025 Feb 4.

ChatGPT and Gemini Are Not Consistently Concordant With the 2020 American 
Academy of Orthopaedic Surgeons Clinical Practice Guidelines When Evaluating 
Rotator Cuff Injury.

Megafu M(1), Guerrero O(2), Yendluri A(3), Parsons BO(4), Galatz LM(4), Li X(5), 
Kelly JD 4th(6), Parisien RL(3).

Author information:
(1)Department of Orthopedic Surgery, University of Connecticut, Farmington, 
Connecticut, U.S.A.. Electronic address: megafu@uchc.edu.
(2)A.T. Still University School of Osteopathic Medicine in Arizona, Mesa, 
Arizona, U.S.A.
(3)Ichan School of Medicine at Mount Sinai, New York, New York, U.S.A.
(4)Department of Orthopedic Surgery, Mount Sinai, New York, New York, U.S.A.
(5)Department of Orthopedic Surgery, Boston University School of Medicine, 
Boston, Massachusetts, U.S.A.
(6)Department of Orthopedic Surgery, University of Pennsylvania, Philadelphia, 
Pennsylvania, U.S.A.

PURPOSE: To evaluate the accuracy of suggestions given by ChatGPT and Gemini 
(previously known as "Bard"), 2 widely used publicly available large language 
models, to evaluate the management of rotator cuff injuries.
METHODS: The 2020 American Academy of Orthopaedic Surgeons (AAOS) Clinical 
Practice Guidelines (CPGs) were the basis for determining recommended and 
non-recommended treatments in this study. ChatGPT and Gemini were queried on 16 
treatments based on these guidelines examining rotator cuff interventions. The 
responses were categorized as "concordant" or "discordant" with the AAOS CPGs. 
The Cohen κ coefficient was calculated to assess inter-rater reliability.
RESULTS: ChatGPT and Gemini showed concordance with the AAOS CPGs for 13 of the 
16 treatments queried (81%) and 12 of the 16 treatments queried (75%), 
respectively. ChatGPT provided discordant responses with the AAOS CPGs for 3 
treatments (19%), whereas Gemini provided discordant responses for 4 treatments 
(25%). Assessment of inter-rater reliability showed a Cohen κ coefficient of 
0.98, signifying agreement between the raters in classifying the responses of 
ChatGPT and Gemini to the AAOS CPGs as being concordant or discordant.
CONCLUSIONS: ChatGPT and Gemini do not consistently provide responses that align 
with the AAOS CPGs.
CLINICAL RELEVANCE: This study provides evidence that cautions patients not to 
rely solely on artificial intelligence for recommendations about rotator cuff 
injuries.

Copyright © 2025 Arthroscopy Association of North America. Published by Elsevier 
Inc. All rights reserved.

DOI: 10.1016/j.arthro.2025.01.039
PMID: 39914605 [Indexed for MEDLINE]

Conflict of interest statement: Disclosures All authors (M.M., O.G., A.Y., 
B.O.P., L.M.G., X.L., J.D.K., R.L.P.) declare that they have no known competing 
financial interests or personal relationships that could have appeared to 
influence the work reported in this paper.


347. JMIR Form Res. 2025 Aug 13;9:e73642. doi: 10.2196/73642.

Using Artificial Intelligence ChatGPT to Access Medical Information About 
Chemical Eye Injuries: Comparative Study.

Alharbi LY(#)(1), Alrashoud RR(#)(1), Alotaibi BS(1), Al Dera AM(1), Alajlan 
RS(1), AlHuthail RR(2), Alessa DI(2).

Author information:
(1)College of Medicine, Imam Mohammad Ibn Saud Islamic University (IMSIU), 
Prince Mohammed Ibn Salman Ibn Abdulaziz Road, Riyadh, 13318, Saudi Arabia, 966 
532816087.
(2)Department of Ophthalmology, College Of Medicine, Imam Mohammad Ibn Saud 
Islamic University (IMSIU), Riyadh, Saudi Arabia.
(#)Contributed equally

BACKGROUND: Chemical ocular injuries are a major public health issue. They cause 
eye damage from harmful chemicals and can lead to severe vision loss or 
blindness if not treated promptly and effectively. Although medical knowledge 
has advanced, accessing reliable and understandable information on these 
injuries remains a challenge. This is due to unverified web-based content and 
complex terminology. Artificial intelligence tools like ChatGPT provide a 
promising solution by simplifying medical information and making it more 
accessible to the general public.
OBJECTIVE: This study aims to assess the use of ChatGPT in providing reliable, 
accurate, and accessible medical information on chemical ocular injuries. It 
evaluates the correctness, thematic accuracy, and coherence of ChatGPT's 
responses compared with established medical guidelines and explores its 
potential for patient education.
METHODS: A total of 9 questions were entered into ChatGPT regarding various 
aspects of chemical ocular injuries. These included the definition, prevalence, 
etiology, prevention, symptoms, diagnosis, treatment, follow-up, and 
complications. The responses provided by ChatGPT were compared with the 
International Classification of Diseases-9 and International Classification of 
Diseases-10 guidelines for chemical (alkali and acid) injuries of the 
conjunctiva and cornea. The evaluation focused on criteria such as correctness, 
thematic accuracy, and coherence to assess the accuracy of ChatGPT's responses. 
The inputs were categorized into 3 distinct groups, and statistical analyses, 
including Flesch-Kincaid readability tests, ANOVA, and trend analysis, were 
conducted to assess their readability, complexity, and trends.
RESULTS: The results showed that ChatGPT provided accurate and coherent 
responses for most questions about chemical ocular injuries, demonstrating 
thematic relevance. However, the responses sometimes overlooked critical 
clinical details or guideline-specific elements, such as emphasizing the urgency 
of care, using precise classification systems, and addressing detailed 
diagnostic or management protocols. While the answers were generally valid, they 
occasionally included less relevant or overly generalized information. This 
reduced their consistency with established medical guidelines. The average 
Flesch Reading Ease Score was 33.84 (SD 2.97), indicating a fairly challenging 
reading level, while the Flesch-Kincaid Grade Level averaged 14.21 (SD 0.97), 
suitable for readers with college-level proficiency. The passive voice was used 
in 7.22% (SD 5.60%) of sentences, indicating moderate reliance. Statistical 
analysis showed no significant differences in the Flesch Reading Ease Score 
(P=.38), Flesch-Kincaid Grade Level (P=.55), or passive sentence use (P=.60) 
across categories, as determined by one-way ANOVA. Readability remained 
relatively constant across the 3 categories, as determined by trend analysis.
CONCLUSIONS: ChatGPT shows strong potential in providing accurate and relevant 
information about chemical ocular injuries. However, its language complexity may 
prevent accessibility for individuals with lower health literacy and sometimes 
miss critical aspects. Future improvements should focus on enhancing 
readability, increasing context-specific accuracy, and tailoring responses to a 
person's needs and literacy levels.

© Layan Yousef Alharbi, Rema Rashed Alrashoud, Bader Shabib Alotaibi, Abdulaziz 
Meshal Al Dera, Raghad Saleh Alajlan, Reem Rashed AlHuthail, Dalal Ibrahim 
Alessa. Originally published in JMIR Formative Research 
(https://formative.jmir.org).

DOI: 10.2196/73642
PMCID: PMC12349736
PMID: 40802972 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


348. J Adv Nurs. 2026 Jan;82(1):907-912. doi: 10.1111/jan.16911. Epub 2025 Mar 24.

An AI-Enabled Nursing Future With no Documentation Burden: A Vision for a New 
Reality.

Michalowski M(1), Topaz M(2), Peltonen LM(3).

Author information:
(1)School of Nursing, University of Minnesota, Minneapolis, Minnesota, USA.
(2)School of Nursing, Columbia University, New York, New York, USA.
(3)Department of Health and Social Management, University of Eastern Finland, 
Wellbeing Services County of North Savo, Kuopio, Finland.

AIMS: To explore the potential of multimodal large language models in 
alleviating the documentation burden on nurses while enhancing the quality and 
efficiency of patient care.
DESIGN: This position paper is informed by expert discussions and a literature 
review.
METHODS: We extensively reviewed nursing documentation practices and advanced 
technologies, such as multimodal large language models. We analysed key 
challenges, solutions and impacts to propose a futuristic multimodal large 
language model-driven model for nursing documentation.
RESULTS: Multimodal large language models offer transformative capabilities by 
integrating multimodal audio, video and text data during patient encounters to 
dynamically update patient records in real time. This reduces manual data entry, 
enabling nurses to focus more on direct patient care. These systems also enhance 
care personalisation through predictive analytics and interoperability, which 
support seamless workflows and better patient outcomes. While predictive 
analytics could improve patient care by identifying trends and risk factors from 
nursing documentation, further research is required to validate its accuracy and 
clinical utility in real-world settings. Ethical, legal and practical 
challenges, including privacy concerns and biases in artificial intelligence 
models, require careful consideration for successful implementation.
CONCLUSION: Transitioning to multimodal large language model-driven 
documentation systems can significantly reduce administrative burdens, improve 
nurse satisfaction and enhance patient care. However, successful integration 
demands interdisciplinary collaboration, robust ethical frameworks and 
technological advancements.
IMPLICATIONS FOR THE PROFESSION AND PATIENT CARE: Implementing multimodal large 
language models could alleviate professional burnout, improve nurse-patient 
interactions, and provide dynamic, up-to-date patient records that facilitate 
informed decision making. These advancements align with the goals of 
patient-centred care by enabling more meaningful engagement between nurses and 
patients.
IMPACT: The problem being addressed is the administrative burden of nursing 
documentation. We suggest that multimodal large language models minimise manual 
documentation, enhance patient care quality and significantly impact nurses and 
patients in diverse healthcare settings globally.

© 2025 The Author(s). Journal of Advanced Nursing published by John Wiley & Sons 
Ltd.

DOI: 10.1111/jan.16911
PMCID: PMC12721931
PMID: 40129115 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


349. J Med Syst. 2025 Dec 23;49(1):188. doi: 10.1007/s10916-025-02323-8.

Artificial Intelligence's Capacity to Detect Subtle Medical Misinformation: A 
Novel Reverse Prompting Approach.

Bendary M(1), Ramzy N(2), Khater A(3), Nasif MM(4), Atef N(3).

Author information:
(1)Epidemiology & Biostatistics Department, National Cancer Institute, Cairo 
University, Kasr el Ain Street, Cairo, Egypt. bendary3@gmail.com.
(2)Pediatric Department, Faculty of Medicine, Benha University, Benha, Egypt.
(3)Epidemiology & Biostatistics Department, National Cancer Institute, Cairo 
University, Kasr el Ain Street, Cairo, Egypt.
(4)Ophthalmology Department, Faculty of Medicine, Zagazig University, Zagazig, 
Egypt.

Medical misinformation is a major public health concern. The public increasingly 
uses artificial intelligence (AI) tools for medical consultations. Therefore, 
concerns arise about their ability to detect and even correct subtle medical 
information that users may be embedding in users prompts. This study assessed 
the ability of different ChatGPT models in detecting and correcting such subtle 
misinformation. Fifty clinical plausible prompts with subtle medical 
misinformation were introduced separately to ChatGPT models 4o, 4.1-mini, and 
GPT-5. Prompts spanned Internal Medicine, Cardiology, Pediatrics, Ophthalmology, 
and Oncology. Responses were scored on a 3-point scale: 0: No correction; 1: 
Hedging or uncertainty; 3: cutting edge detection and correction. GPT-4o was the 
best performing model, surpassing GPT-5 by correctly identifying and correcting 
misinformation in 86% of the prompts compared to 74% for GPT-5. GPT-4.1-mini 
showed weaker performance, detecting dsmisinformation in only 52% of prompts, 
with complete failure in 34% and hedging in 14%. Specialty-specific analysis 
revealed that GPT-4o achieved higher detection rate in all tested specialties 
compared to GPT-4.1-mini and GPT-5. Only oncology showed comparable detection 
rates between GPT-4o and GPT-5. Although the performance of GPT-4o and GPT-5 in 
detecting subtle medical misinformation was promising, unexpectedly, GPT-4o 
surpassed GPT-5 in performance. Using underpowered variants such as 
GPT-4.1-mini, poses a public health threat. Reverse prompting offers a 
diagnostic lens and should be integrated into standard AI safety testing 
protocols.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02323-8
PMID: 41432852 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics Approval: Not applicable. 
Consent for Publication: Not applicable. Clinical Trial Number: Not applicable. 
Competing interests: The authors declare no competing interests.


350. J Med Syst. 2025 Oct 8;49(1):130. doi: 10.1007/s10916-025-02264-2.

Evaluation of DeepSeek-R1 for Ophthalmic Diagnosis and Reasoning: A Comparison 
with OpenAI o1 and o3.

Ming S(#)(1)(2)(3), Yao X(4), Guo Q(4)(5)(6), Chen D(4)(5)(6), Guo X(4), Xie 
K(7), Lei B(#)(4)(5)(6).

Author information:
(1)Henan Eye Institute, Henan Eye Hospital, Henan Provincial People's Hospital, 
Zhengzhou, Henan, China. ms139@zzu.edu.cn.
(2)Eye Institute, Henan Academy of Innovations in Medical Science, Zhengzhou, 
Henan Province, China. ms139@zzu.edu.cn.
(3)Henan Clinical Research Center for Ocular Diseases, People's Hospital of 
Zhengzhou University, Zhengzhou, China. ms139@zzu.edu.cn.
(4)Henan Eye Institute, Henan Eye Hospital, Henan Provincial People's Hospital, 
Zhengzhou, Henan, China.
(5)Eye Institute, Henan Academy of Innovations in Medical Science, Zhengzhou, 
Henan Province, China.
(6)Henan Clinical Research Center for Ocular Diseases, People's Hospital of 
Zhengzhou University, Zhengzhou, China.
(7)Department of Ophthalmology, The First Affiliated Hospital of Zhengzhou 
University, Zhengzhou, Henan, China.
(#)Contributed equally

BACKGROUND: DeepSeek-R1, an open-source reasoning large language model (LLM) 
clinically deployed in Chinese hospitals, still lacks validation in 
ophthalmology.
AIMS: To compare DeepSeek-R1 against OpenAI's o1 and upgraded o3 models in 
diagnostic accuracy and reasoning capability across diverse ophthalmic 
conditions.
METHODS: We evaluated 98 standardized case vignettes covering13 ophthalmic 
sub-specialties, each supplied with an expert-validated diagnostic hierarchy, 
differential list, and reasoning chain. Model performance was assessed with a 
diagnosis matrix focused on final-diagnosis (FDx) accuracy; incorrect outputs 
were resubmitted with key diagnostic clues (reasoning-augmented, RA prompt) to 
test self-correction. Reasoning capacity was quantified by the number/score of 
diagnostic clues retrieved per case across 13 predefined domains.
RESULTS: DeepSeek-R1 achieved an 87.8% FDx accuracy, comparable to o3 (91.8%, 
P = .34) and higher than o1 (58.2%, P < .001). Similar trends were observed for 
others accuracy (global P < .001). Agreement was moderate-high between R1 and o3 
(κ = 0.42-1.00), but slight with o1 (κ = 0.12-0.32). R1 and o3 identified more 
diagnostic clues than o1 (median count = 4 vs. 3, median score = 100 vs. 80; 
P < .001). RA prompts corrected 50.0%, 62.5% and 41.5% of FDx errors for R1, o3, 
and o1, raising FDx accuracy to 93.9%, 96.9%, and 80.6% respectively.
CONCLUSIONS: DeepSeek-R1 matched o3 and outperformed o1 in diagnostic accuracy 
and reasoning, retrieving nearly all expert-defined clues. Its open-source 
nature, low cost and strong performance support its use as a practical aid for 
ophthalmic decision-making.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02264-2
PMID: 41060487 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethical Approval: The 
Institutional Review Board of Henan Provincial People’s Hospital determined that 
this in silico research did not involve real-world human subjects. All case 
vignettes obtained from publicly accessible published textbooks. During data 
collection, both GPT-o1 and o3 were available commercially via ChatGPT Plus 
subscriptions. Competing interests: The authors declare no competing interests. 
Clinical Trial Number: Not applicable.


351. J Med Internet Res. 2025 Mar 21;27:e67967. doi: 10.2196/67967.

Large Language Model-Based Assessment of Clinical Reasoning Documentation in the 
Electronic Health Record Across Two Institutions: Development and Validation 
Study.

Schaye V(1)(2), DiTullio D(1), Guzman BV(3), Vennemeyer S(4), Shih H(4), 
Reinstein I(2), Weber DE(5)(6), Goodman A(5)(6), Wu DTY(4), Sartori DJ(1), 
Santen SA(7), Gruppen L(8), Aphinyanaphongs Y(3), Burk-Rafel J(1)(2).

Author information:
(1)Department of Medicine, NYU Grossman School of Medicine, New York, NY, United 
States.
(2)Institute for Innovations in Medical Education, NYU Grossman School of 
Medicine, New York, NY, United States.
(3)Division of Applied AI Technologies, NYU Langone Health, New York, NY, United 
States.
(4)Department of Biostatistics, Health informatics, and Data Sciences, 
University of Cincinnati College of Medicine, Cincinnati, OH, United States.
(5)Division of Hospital Medicine, Department of Pediatrics, Cincinnati 
Children's Hospital Medical Center, University of Cincinnati College of 
Medicine, Cincinnati, OH, United States.
(6)Division of Hospital Medicine, Department of Internal Medicine, University of 
Cincinnati College of Medicine, Cincinnati, OH, United States.
(7)Department of Emergency Medicine, University of Cincinnati College of 
Medicine, Cincinnati, OH, United States.
(8)Department of Learning Health Sciences, University of Michigan Medical 
School, Ann Arbor, MI, United States.

BACKGROUND: Clinical reasoning (CR) is an essential skill; yet, physicians often 
receive limited feedback. Artificial intelligence holds promise to fill this 
gap.
OBJECTIVE: We report the development of named entity recognition (NER), 
logic-based and large language model (LLM)-based assessments of CR documentation 
in the electronic health record across 2 institutions (New York University 
Grossman School of Medicine [NYU] and University of Cincinnati College of 
Medicine [UC]).
METHODS: The note corpus consisted of internal medicine resident admission notes 
(retrospective set: July 2020-December 2021, n=700 NYU and 450 UC notes and 
prospective validation set: July 2023-December 2023, n=155 NYU and 92 UC notes). 
Clinicians rated CR documentation quality in each note using a previously 
validated tool (Revised-IDEA), on 3-point scales across 2 domains: differential 
diagnosis (D0, D1, and D2) and explanation of reasoning, (EA0, EA1, and EA2). At 
NYU, the retrospective set was annotated for NER for 5 entities (diagnosis, 
diagnostic category, prioritization of diagnosis language, data, and linkage 
terms). Models were developed using different artificial intelligence 
approaches, including NER, logic-based model: a large word vector model 
(scispaCy en_core_sci_lg) with model weights adjusted with backpropagation from 
annotations, developed at NYU with external validation at UC, NYUTron LLM: an 
NYU internal 110 million parameter LLM pretrained on 7.25 million clinical 
notes, only validated at NYU, and GatorTron LLM: an open source 345 million 
parameter LLM pretrained on 82 billion words of clinical text, fined tuned on 
NYU retrospective sets, then externally validated and further fine-tuned at UC. 
Model performance was assessed in the prospective sets with F1-scores for the 
NER, logic-based model and area under the receiver operating characteristic 
curve (AUROC) and area under the precision-recall curve (AUPRC) for the LLMs.
RESULTS: At NYU, the NYUTron LLM performed best: the D0 and D2 models had 
AUROC/AUPRC 0.87/0.79 and 0.89/0.86, respectively. The D1, EA0, and EA1 models 
had insufficient performance for implementation (AUROC range 0.57-0.80, AUPRC 
range 0.33-0.63). For the D1 classification, the approach pivoted to a stepwise 
approach taking advantage of the more performant D0 and D2 models. For the EA 
model, the approach pivoted to a binary EA2 model (ie, EA2 vs not EA2) with 
excellent performance, AUROC/AUPRC 0.85/ 0.80. At UC, the NER, D-logic-based 
model was the best performing D model (F1-scores 0.80, 0.74, and 0.80 for D0, 
D1, D2, respectively. The GatorTron LLM performed best for EA2 scores 
AUROC/AUPRC 0.75/ 0.69.
CONCLUSIONS: This is the first multi-institutional study to apply LLMs for 
assessing CR documentation in the electronic health record. Such tools can 
enhance feedback on CR. Lessons learned by implementing these models at distinct 
institutions support the generalizability of this approach.

©Verity Schaye, David DiTullio, Benedict Vincent Guzman, Scott Vennemeyer, 
Hanniel Shih, Ilan Reinstein, Danielle E Weber, Abbie Goodman, Danny T Y Wu, 
Daniel J Sartori, Sally A Santen, Larry Gruppen, Yindalon Aphinyanaphongs, Jesse 
Burk-Rafel. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 21.03.2025.

DOI: 10.2196/67967
PMCID: PMC11971582
PMID: 40117575 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


352. J Am Med Inform Assoc. 2025 Feb 1;32(2):268-274. doi: 10.1093/jamia/ocae285.

Health system-wide access to generative artificial intelligence: the New York 
University Langone Health experience.

Malhotra K(1)(2)(3), Wiesenfeld B(4), Major VJ(1)(2)(5), Grover H(1)(2), 
Aphinyanaphongs Y(1)(2)(6), Testa P(1)(2)(7), Austrian JS(1)(2)(6).

Author information:
(1)New York University (NYU) Langone Health, New York, NY 10016, United States.
(2)Medical Center IT, Department of Health Informatics, NYU Langone Health, New 
York, NY 10016, United States.
(3)Department of Ophthalmology, NYU Grossman School of Medicine, New York, NY 
10017, United States.
(4)Department of Management and Organization, NYU Stern School of Business, New 
York, NY 10012, United States.
(5)Department of Population Health, NYU Grossman School of Medicine, New York, 
NY 10016, United States.
(6)Department of Medicine, NYU Grossman School of Medicine, New York, NY 10016, 
United States.
(7)Department of Emergency Medicine, NYU Grossman School of Medicine, New York, 
NY 10016, United States.

OBJECTIVES: The study aimed to assess the usage and impact of a private and 
secure instance of a generative artificial intelligence (GenAI) application in a 
large academic health center. The goal was to understand how employees interact 
with this technology and the influence on their perception of skill and work 
performance.
MATERIALS AND METHODS: New York University Langone Health (NYULH) established a 
secure, private, and managed Azure OpenAI service (GenAI Studio) and granted 
widespread access to employees. Usage was monitored and users were surveyed 
about their experiences.
RESULTS: Over 6 months, over 1007 individuals applied for access, with high 
usage among research and clinical departments. Users felt prepared to use the 
GenAI studio, found it easy to use, and would recommend it to a colleague. Users 
employed the GenAI studio for diverse tasks such as writing, editing, 
summarizing, data analysis, and idea generation. Challenges included 
difficulties in educating the workforce in constructing effective prompts and 
token and API limitations.
DISCUSSION: The study demonstrated high interest in and extensive use of GenAI 
in a healthcare setting, with users employing the technology for diverse tasks. 
While users identified several challenges, they also recognized the potential of 
GenAI and indicated a need for more instruction and guidance on effective usage.
CONCLUSION: The private GenAI studio provided a useful tool for employees to 
augment their skills and apply GenAI to their daily tasks. The study underscored 
the importance of workforce education when implementing system-wide GenAI and 
provided insights into its strengths and weaknesses.

© The Author(s) 2024. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For permissions, 
please email: journals.permissions@oup.com.

DOI: 10.1093/jamia/ocae285
PMCID: PMC11756645
PMID: 39584477 [Indexed for MEDLINE]

Conflict of interest statement: None declared.


353. JMIR Med Inform. 2025 Oct 23;13:e77061. doi: 10.2196/77061.

Human-Machine Agreement in Medical Ethics: Patient Autonomy Case-Based 
Evaluation of Large Language Models.

Mugu V(1), Carr B(2), Khandelwal A(1), Olson M(1), Schupbach J(2), Zietlow J(3), 
Vu TND(3), Chan A(1), Collura C(4), Schmitz J(1).

Author information:
(1)Department of Radiology, Mayo Clinic, Rochester, MN, United States.
(2)Department of Emergency Medicine, Mayo Clinic, Rochester, MN, United States.
(3)Department of Trauma, Critical Care, and General Surgery, Mayo Clinic, 
Rochester, MN, United States.
(4)Department of Neonatal Medicine, Mayo Clinic, Rochester, MN, United States.

BACKGROUND: Medical ethics provides a moral framework for the practice of 
clinical medicine. Four principles, that is, beneficence, nonmaleficence, 
patient autonomy, and justice, form the cornerstones of medical ethics as it is 
practiced today. Of these 4 principles, patient autonomy holds a pivotal 
position and often takes precedence in ethical dilemmas that result from 
conflicts among the 4 principles. Its importance serves as a constant reminder 
to the clinician that the "needs of the patient come first." With their 
remarkable ability to process natural language, large language models (LLMs) 
have recently pervaded nearly every aspect of human life, including medicine and 
medical ethics. Reliance on tools such as LLMs, however, poses fundamental 
questions in medical ethics, where human-like reasoning, emotional intelligence, 
and an understanding of local context and values are of utmost importance.
OBJECTIVE: While emphasizing the central role of the human factor, we undertake 
a bold venture to establish some confidence in LLMs, as it pertains to medical 
ethics by not only evaluating the status quo of foundational LLMs but also 
exploring ways to improve the LLMs by using patient autonomy-based hypothetical 
cases. Although literature today is certainly lacking in such ventures, we also 
believe projects such as ours must be frequently revisited in the field of LLMs, 
which is evolving at a pace that is both rapid and unprecedented.
METHODS: We evaluated 3 foundational LLMs (ChatGPT, LLaMA, and Gemini) on 
hypothetical cases in patient autonomy. We used Cohen κ to compare LLM responses 
to the consensus from a physician panel. McNemar test was used during the 
improvement phase and to report the final significance of improved agreement of 
each LLM with physician consensus. P values less than .05 were considered 
significant. An agreement with κ<0 was designated as poor, 0-0.2 as slight, 
0.2-0.4 as fair, 0.41-0.6 as moderate, 0.61-0.8 as substantial, and 0.81-1 as 
almost perfect.
RESULTS: There was slight to fair agreement between the foundational LLMs and 
the physician consensus. With iterative improvement techniques, this agreement 
evolved to be substantial or higher (Cohen κ of 0.73-0.82). The degree of 
improvement was statistically significant (P=.006 for ChatGPT, P<.001 for 
Gemini, and P<.001 for LLaMA).
CONCLUSIONS: Although LLMs hold great potential for use in medicine, there needs 
to be an abundance of caution in using foundational LLMs in domains such as 
medical ethics. With adequate human oversight in testing and utilizing 
established techniques, LLM responses can be better aligned to human responses, 
even in the domain of medical ethics.

©Vamshi Mugu, Brendan Carr, Ashish Khandelwal, Mike Olson, John Schupbach, John 
Zietlow, T N Diem Vu, Alex Chan, Christopher Collura, John Schmitz. Originally 
published in JMIR Medical Informatics (https://medinform.jmir.org), 23.10.2025.

DOI: 10.2196/77061
PMCID: PMC12592888
PMID: 41056099 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


354. BMC Med Ethics. 2025 Oct 15;26(1):136. doi: 10.1186/s12910-025-01285-3.

The ethics of simplification: balancing patient autonomy, comprehension, and 
accuracy in AI-generated radiology reports.

Lee HS(1), Song SH(1), Park C(1), Seo J(2), Kim WH(2)(3), Kim J(2)(4), Kim 
S(1)(5)(6)(7), Han K(8), Lee YH(9)(10).

Author information:
(1)Department of Radiology, Gangnam Severance Hospital, Yonsei University 
College of Medicine, Seoul, Republic of Korea.
(2)BeamWorks Inc, Daegu, Republic of Korea.
(3)Department of Radiology, School of Medicine, Kyungpook National University, 
Kyungpook National University Chilgok Hospital, Daegu, Republic of Korea.
(4)School of Computer Science and Engineering, Kyungpook National University, 
Daegu, Republic of Korea.
(5)Department of Medical Device Engineering and Management, The Graduate School, 
Yonsei University College of Medicine, Seoul, Republic of Korea.
(6)Department of Integrative Medicine, The Graduate School, Yonsei University 
College of Medicine, Seoul, Republic of Korea.
(7)Institute for Innovation in Digital Healthcare, Yonsei University, Seoul, 
Republic of Korea.
(8)Department of Radiology, Research Institute of Radiological Science and 
Center for Clinical Imaging Data Science, Severance Hospital, Yonsei University 
College of Medicine, 50-1, Yonsei-ro, Seodaemun-gu, Seoul, 03722, Republic of 
Korea.
(9)Institute for Innovation in Digital Healthcare, Yonsei University, Seoul, 
Republic of Korea. sando@yuhs.ac.
(10)Department of Radiology, Research Institute of Radiological Science and 
Center for Clinical Imaging Data Science, Severance Hospital, Yonsei University 
College of Medicine, 50-1, Yonsei-ro, Seodaemun-gu, Seoul, 03722, Republic of 
Korea. sando@yuhs.ac.

BACKGROUND: Large language models (LLMs) such as GPT-4 are increasingly used to 
simplify radiology reports and improve patient comprehension. However, excessive 
simplification may undermine informed consent and autonomy by compromising 
clinical accuracy. This study investigates the ethical implications of 
readability thresholds in AI-generated radiology reports, identifying the 
minimum reading level at which clinical accuracy is preserved.
METHODS: We retrospectively analyzed 500 computed tomography and magnetic 
resonance imaging reports from a tertiary hospital. Each report was transformed 
into 17 versions (reading grade levels 1-17) using GPT-4 Turbo. Readability 
metrics and word counts were calculated for each version. Clinical accuracy was 
evaluated using radiologist assessments and PubMed-BERTScore. We identified the 
first grade level at which a statistically significant decline in accuracy 
occurred, determining the lowest level that preserved both accuracy and 
readability. We further assessed potential clinical consequences in reports 
simplified to the 7th-grade level.
RESULTS: Readability scores showed strong correlation with prompted reading 
levels (r = 0.80-0.84). Accuracy remained stable across grades 13-11 but 
declined significantly below grade 11. At the 7th-grade level, 20% of reports 
contained inaccuracies with potential to alter patient management, primarily due 
to omission, incorrect conversion, or inappropriate generalization. The 
11th-grade level emerged as the current lower bound for preserving accuracy in 
LLM-generated radiology reports.
CONCLUSIONS: Our findings highlight an ethical tension between improving 
readability and maintaining clinical accuracy. While 7th-grade readability 
remains an ethical ideal, current AI tools cannot reliably produce accurate 
reports below the 11th-grade level. Ethical implementation of AI-generated 
reporting should include layered communication strategies and model transparency 
to safeguard patient autonomy and comprehension.

© 2025. The Author(s).

DOI: 10.1186/s12910-025-01285-3
PMCID: PMC12523008
PMID: 41094535 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was conducted in accordance with the principles of the 
Declaration of Helsinki. The dataset consisted of de-identified clinical 
radiology reports collected retrospectively from Gangnam Severance Hospital 
under institutional review board (IRB) approval (No. 3-2024-0126), which waived 
the requirement for informed consent due to the study’s retrospective nature. 
Consent for publication: Not applicable. Competing Interests: We declare that 
Won Hwa Kim and Jaeil Kim are the co-CEOs of BeamWorks. Jeongrok Seo is a 
researcher affiliated with BeamWorks. The company is actively developing medical 
AI solutions.


355. Br J Oral Maxillofac Surg. 2026 Jan;64(1):43-49. doi: 
10.1016/j.bjoms.2025.08.015. Epub 2025 Sep 23.

Comparison of large language models in oral and maxillofacial surgery.

Grillo R(1), Llanos AH(2), Costa C(2), Melhem-Elias F(3).

Author information:
(1)Department of Oral and Maxillofacial Surgery, School of Dentistry, University 
of São Paulo, São Paulo, Brazil. Electronic address: grillo@usp.br.
(2)Department of Stomatology, School of Dentistry, University of São Paulo, São 
Paulo, Brazil.
(3)Department of Oral and Maxillofacial Surgery, School of Dentistry, University 
of São Paulo, São Paulo, Brazil.

This study evaluates the performance of six large language models (LLMs) in 
generating content relevant to oral and maxillofacial surgery (OMFS), focusing 
on their ability to provide accurate, comprehensive, and relevant information 
across five specific tasks. Each LLM was assessed based on its responses to five 
prompts: (1) postoperative instructions for third molar surgery; (2) a list of 
best-selling books on orthognathic surgery; (3) the most cited articles in OMFS; 
(4) novel ideas for systematic reviews; and (5) emerging trends in OMFS. 
Responses were scored for relevance, comprehensiveness, and accuracy using 
predefined criteria. Statistical analysis was performed using the Kruskal-Wallis 
test to compare tool performance. The LLMs performed similarly overall, with 
varying strengths and weaknesses. For postoperative instructions, they all 
provided comparable recommendations, though Perplexity underperformed. In 
identifying best-selling books, Gemini and Perplexity excelled, while ChatGPT 
and Copilot struggled with retrieving highly cited articles. Copilot and Claude 
were more effective in suggesting novel systematic review topics, while ChatGPT, 
Claude, Copilot, and DeepSeek identified emerging trends most accurately. LLMs 
demonstrate significant potential in supporting OMFS-related tasks, but their 
performance varies depending on the specific application. While they excel at 
synthesising existing information and identifying trends, limitations in 
accuracy and occasional hallucinations highlight the need for human oversight. 
These findings underscore the importance of integrating artificial intelligence 
(AI) as a supplementary tool in clinical, academic, and research settings, 
ensuring its use complements, rather than replaces, human expertise.

Copyright © 2025 The British Association of Oral and Maxillofacial Surgeons. 
Published by Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.bjoms.2025.08.015
PMID: 41076417 [Indexed for MEDLINE]


356. Int J Med Inform. 2025 May;197:105836. doi: 10.1016/j.ijmedinf.2025.105836. Epub 
2025 Feb 18.

Comparing physician and large language model responses to influenza patient 
questions in the online health community.

Wu H(1), Li M(2), Zhang L(1).

Author information:
(1)School of Medicine and Health Management, Huazhong University of Science and 
Technology, Wuhan, China.
(2)School of Medicine and Health Management, Huazhong University of Science and 
Technology, Wuhan, China. Electronic address: m202375946@hust.edu.cn.

INTRODUCTION: During influenza season, some patients tend to seek medical advice 
through online platforms. However, due to time constraints, the informational 
and emotional support provided by physicians is limited. Large language models 
(LLMs) can rapidly provide medical knowledge and empathy, but their capacity for 
providing informational support to patients with influenza and assisting 
physicians in providing emotional support is unclear. Therefore, this study 
evaluated the quality of LLM-generated influenza advice and its emotional 
support performance in comparison with physician advice.
METHODS: This study utilized 200 influenza question-answer pairs from the online 
health community. Data collection consisted of two parts: (1) A panel of 
board-certified physicians evaluated the quality of LLM advice vs physician 
advice. (2) Physician advice was polished using an LLM, and the LLM-rewritten 
advice was compared to the original physician advice using the LLM module.
RESULTS: For informational support, there was no significant difference between 
LLM and physician advice in terms of the presence of incorrect information, 
omission of information, extent of harm or empathy. Nevertheless, compared to 
physician advice, LLM advice was more likely to cause harm and to be in line 
with medical consensus. LLM was also able to assist physicians in providing 
emotional support, since the LLM-rewritten advice was significantly more 
respectful, friendly and empathetic, when compared with physician advice. Also, 
the LLM-rewritten advice was logically smooth. In most cases, LLM did not add or 
omit the original medical information.
CONCLUSION: This study suggests that LLMs can provide informational and 
emotional support for influenza patients. This may help to alleviate the 
pressure on physicians and promote physician-patient communication.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.105836
PMID: 39986122 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


357. J Med Internet Res. 2025 Oct 7;27:e78625. doi: 10.2196/78625.

Evaluating Large Language Models and Retrieval-Augmented Generation Enhancement 
for Delivering Guideline-Adherent Nutrition Information for Cardiovascular 
Disease Prevention: Cross-Sectional Study.

Parameswaran V(1), Bernard J(2), Bernard A(1), Deo N(3), Tsung S(4), Lyytinen 
K(5), Sharp C(1), Rodriguez F(1), Maron DJ(1), Dash R(1).

Author information:
(1)School of Medicine, Stanford University, Palo Alto, CA, United States.
(2)Tufts University, Boston, MA, United States.
(3)University of Illinois Urbana-Champaign, Champaign, IL, United States.
(4)Stanford University, Palo Alto, CA, United States.
(5)Case Western Reserve University, Cleveland, OH, United States.

BACKGROUND: Cardiovascular disease (CVD) remains the leading cause of death 
worldwide, yet many web-based sources on cardiovascular (CV) health are 
inaccessible. Large language models (LLMs) are increasingly used for 
health-related inquiries and offer an opportunity to produce accessible and 
scalable CV health information. However, because these models are trained on 
heterogeneous data, including unverified user-generated content, the quality and 
reliability of food and nutrition information on CVD prevention remain 
uncertain. Recent studies have examined LLM use in various health care 
applications, but their effectiveness for providing nutrition information 
remains understudied. Although retrieval-augmented generation (RAG) frameworks 
have been shown to enhance LLM consistency and accuracy, their use in delivering 
nutrition information for CVD prevention requires further evaluation.
OBJECTIVE: To evaluate the effectiveness of off-the-shelf and RAG-enhanced LLMs 
in delivering guideline-adherent nutrition information for CVD prevention, we 
assessed 3 off-the-shelf models (ChatGPT-4o, Perplexity, and Llama 3-70B) and a 
Llama 3-70B+RAG model.
METHODS: We curated 30 nutrition questions that comprehensively addressed CVD 
prevention. These were approved by a registered dietitian providing preventive 
cardiology services at an academic medical center and were posed 3 times to each 
model. We developed a 15,074-word knowledge bank incorporating the American 
Heart Association's 2021 dietary guidelines and related website content to 
enhance Meta's Llama 3-70B model using RAG. The model received this and a 
few-shot prompt as context, included citations in a Context Source section, and 
used vector similarity to align responses with guideline content, with the 
temperature parameter set to 0.5 to enhance consistency. Model responses were 
evaluated by 3 expert reviewers against benchmark CV guidelines for 
appropriateness, reliability, readability, harm, and guideline adherence. Mean 
scores were compared using ANOVA, with statistical significance set at P<.05. 
Interrater agreement was measured using the Cohen κ coefficient, and readability 
was estimated using the Flesch-Kincaid readability score.
RESULTS: The Llama 3+RAG model scored higher than the Perplexity, GPT-4o, and 
Llama 3 models on reliability, appropriateness, guideline adherence, and 
readability and showed no harm. The Cohen κ coefficient (κ>70%; P<.001) 
indicated high reviewer agreement.
CONCLUSIONS: The Llama 3+RAG model outperformed the off-the-shelf models across 
all measures with no evidence of harm, although the responses were less readable 
due to technical language. The off-the-shelf models scored lower on all measures 
and produced some harmful responses. These findings highlight the limitations of 
off-the-shelf models and demonstrate that RAG system integration can enhance LLM 
performance in delivering evidence-based dietary information.

©Vijaya Parameswaran, Jenna Bernard, Alec Bernard, Neil Deo, Sean Tsung, Kalle 
Lyytinen, Christopher Sharp, Fatima Rodriguez, David J Maron, Rajesh Dash. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 07.10.2025.

DOI: 10.2196/78625
PMCID: PMC12541265
PMID: 41057043 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: FR reports equity from 
Carta Healthcare and CLINT AI and consulting fees from CLINT AI, Novartis, Novo 
Nordisk, Esperion Therapeutics, Movano Health, Kento Health, Inclusive Health, 
Edwards Lifesciences, Arrowhead Pharmaceuticals, Heartflow, iRhythm 
Technologies, Amgen, and Cleerly Health outside the submitted work. DJM reports 
stock in Ablative Solutions, Inc; research funding from Cleerly and Omada 
Health; and consultant fees from Regeneron, Scilex Holding Company, and Johnson 
& Johnson. RD reports consulting fees from CLINT AI outside the submitted work. 
All other authors declare no other conflicts of interest.


358. Med Educ. 2026 Feb;60(2):138-142. doi: 10.1111/medu.70056. Epub 2025 Sep 25.

'ChatGPT can make mistakes' warnings fail: A randomized controlled trial.

Kıyak YS(1), Coşkun Ö(1), Budakoğlu Iİ(1).

Author information:
(1)Department of Medical Education and Informatics, Faculty of Medicine, Gazi 
University, Ankara, Türkiye.

BACKGROUND: Warnings are commonly used to signal the fallibility of AI systems 
like ChatGPT in clinical decision-making. Yet, little is known about whether 
such disclaimers influence medical students' diagnostic behaviour. Drawing on 
the Judge-Advisor System (JAS) theory, we investigated whether the warning 
alters advice-taking behaviour by modifying perceived advisor credibility.
METHOD: In this randomized controlled trial, 186 fourth-year medical students 
evaluated three clinical vignettes with two diagnostic options. Each case was 
specifically designed to include the presentations of both diagnoses to make the 
case ambiguous. Students were randomly assigned to receive feedback either with 
(warning arm) or without (no-warning arm) a prominently displayed warning 
('ChatGPT can make mistakes. Check important info'.). After submitting their 
initial response, students received ChatGPT-attributed disagreeing diagnostic 
feedback explaining why the alternate diagnosis was correct. Then they were 
given the opportunity to revise their original choice. Advice-taking was 
measured by whether students changed their diagnosis after viewing AI input. We 
analysed change rates, weight-of-advice (WoA) and used mixed-effects models to 
assess intervention effects.
RESULTS: The warning did not influence diagnostic changes (15.3% no-warning vs. 
15.9% warning; OR = 1.09, 95% CI: 0.46-2.59, p = 0.84). The WoA was 0.15 
(SD = 0.36), significantly lower than the 0.30 average in prior JAS 
meta-analysis (p < 0.001). Among students who retained their original diagnosis, 
the warning group showed a tendency toward providing explanations on why they 
disagree with the AI advisor (60% vs. 51%, p = 0.059).
CONCLUSIONS: The students underweight AI's diagnostic advice. The disclaimer did 
not alter students' use of AI advice, suggesting that their perceived 
credibility of ChatGPT was already near a behavioural floor. This finding 
supports the existence of a credibility threshold, beyond which additional 
cautionary cues have limited effect. Our results refine advice-taking theory and 
signal that simple warnings may be insufficient to ensure calibrated trust in 
AI-supported learning.

© 2025 Association for the Study of Medical Education and John Wiley & Sons Ltd.

DOI: 10.1111/medu.70056
PMID: 40998694 [Indexed for MEDLINE]


359. Radiol Artif Intell. 2026 Jan 14:e250651. doi: 10.1148/ryai.250651. Online ahead 
of print.

Agentic AI in Radiology: Evolution from Large Language Models to Future Clinical 
Integration.

Khosravi B(1)(2), Rouzrokh P(1)(2), Akinci D'Antonoli T(3)(4), Moassefi M(2), 
Faghani S(2)(5), Mansuri A(6), Bressem K(7)(8), Tejani A(9), Gichoya J(6).

Author information:
(1)Department of Radiology and Biomedical Imaging, Yale University, New Haven, 
Conn.
(2)Department of Radiology, Mayo Clinic, 200 First St SW, Rochester, MN 55905.
(3)Department of Diagnostic and Interventional Neuroradiology, University 
Hospital Basel, Basel, Switzerland.
(4)Department of Pediatric Radiology, University Children's Hospital Basel, 
Basel, Switzerland.
(5)Department of Radiology, University of Pennsylvania, Philadelphia, Pa.
(6)Department of Radiology and Imaging Sciences, Emory University, Atlanta, Ga.
(7)School of Medicine and Health, Department of Diagnostic and Interventional 
Radiology, Klinikum Rechts der Isar, TUM University Hospital, Technical 
University of Munich, Munich, Germany.
(8)School of Medicine and Health, Department of Cardiovascular Radiology and 
Nuclear Medicine, German Heart Center Munich, TUM University Hospital, Technical 
University of Munich, Munich, Germany.
(9)Department of Radiology and Biomedical Imaging, University of California San 
Francisco, San Francisco, Calif.

The introduction of foundational models, specifically large language models, has 
promised a health care transformation. However, the field is rapidly evolving 
toward autonomous agent systems, defined as AI entities that perceive and react 
to their environment to achieve specific goals-representing a paradigm shift 
from passive information retrieval to proactive, goal-oriented clinical 
assistance. Agentic AI systems transcend static knowledge limitations through 
core capabilities including persistent memory systems that maintain context 
across patient encounters, knowledge retrieval tools connecting to medical 
repositories through retrieval-augmented generation techniques, and computer use 
functionality enabling navigation of clinical software interfaces. Agentic 
workflows introduce sophisticated coordination mechanisms including 
hierarchical, collaborative, and sequential patterns demonstrating superior 
performance compared with single-agent approaches. Multiagent systems can 
autonomously coordinate entire clinical workflows across the entire radiology 
lifecycle, from preacquisition protocol optimization through initial image 
analysis, specialized tool deployment, and preliminary report generation. 
However, successful clinical deployment requires systematic consideration of 
complexity thresholds, economic sustainability, cybersecurity frameworks, bias 
mitigation strategies, and appropriate governance structures. Critical 
challenges include managing the probabilistic nature of underlying models within 
deterministic clinical workflows, ensuring adequate human supervision, and 
preventing overcomplication of established processes. A structured four-phase 
implementation roadmap addresses these considerations through incremental 
progression from low-risk automation to comprehensive workflow orchestration 
while maintaining rigorous safety standards. As foundation models advance and 
interoperability standards mature, agentic AI will reshape radiology practice 
paradigms. Success depends on resolving stakeholder responsibility questions 
while orchestrating technological capabilities with clinical accountability, 
ensuring autonomous systems augment rather than replace professional judgment in 
pursuit of improved patient outcomes. ©RSNA, 2026.

DOI: 10.1148/ryai.250651
PMID: 41532836


360. JMIR Med Inform. 2025 Oct 8;13:e74902. doi: 10.2196/74902.

A Vision-Language-Guided Multimodal Fusion Network for Glottic Carcinoma Early 
Diagnosis: Model Development and Validation Study.

Jin Z(#)(1), Shuai Y(#)(2), Li Y(#)(2), Chen M(1), Liu Y(1), Lei W(2), Fan X(1).

Author information:
(1)College of Big Data and Internet, Shenzhen Technology University, Pingshan 
District, 3002 Lantian Road, Shenzhen, Guangdong, 518118, China, 86 19276679344.
(2)The First Affiliated Hospital, Sun Yat-sen University, Guangzhou, Guangdong, 
China.
(#)Contributed equally

BACKGROUND: Early diagnosis and intervention in glottic carcinoma (GC) can 
significantly improve long-term prognosis. However, the accurate diagnosis of 
early GC is challenging due to its morphological similarity to vocal cord 
dysplasia, with the difficulty further exacerbated in medically underserved 
areas.
OBJECTIVE: This study aims to address the limitations of existing technologies 
by designing a vision-language multimodal model, providing a more efficient and 
accurate early diagnostic method for GC.
METHODS: The data used in this study were sourced from the information system of 
the First Affiliated Hospital of Sun Yat-sen University, comprising laryngoscopy 
reports and 5796 laryngoscopic images from 404 patients with glottic lesions. We 
propose a vision-language-guided multimodal fusion network (VLMF-Net) based on a 
large vision-language model for the early automated diagnosis of GC. The text 
processing module of this model uses the pretrained Large Language Model Meta AI 
(LLaMa) to generate text vector representations, while the image processing 
module uses a pretrained vision transformer to extract features from 
laryngoscopic images, achieving cross-modal alignment through the Q-Former 
module. By leveraging a feature fusion module, deep integration of text and 
image features is achieved, ultimately enabling classification diagnosis. To 
validate the model's performance, the study selected contrastive language-image 
pretraining (CLIP), bootstrapping language-image pretraining with frozen image 
encoders and large language models (BLIP-2), a large-scale image and noisy-text 
embedding (ALIGN), and vision-and-language transformer (VILT) as baseline 
methods for experimental evaluation on the same dataset, with comprehensive 
performance assessment conducted using accuracy, recall, precision, F1-score, 
and area under the curve.
RESULTS: We found that on the internal test set, the VLMF-Net model 
significantly outperformed existing methods with an accuracy of 77.6% (CLIP: 
70.5%; BLIP-2: 71.5%; ALIGN: 67.3%; and VILT: 64.3%), achieving a 6.1-percentage 
point improvement over the best baseline model (BLIP-2). On the external test 
set, our method also demonstrated robust performance, achieving an accuracy of 
73.9%, which is 4.6 percentage points higher than the second-best model (BLIP-2: 
69.3%). This indicates that our model surpasses these methods in the early 
diagnosis of GC and exhibits strong generalization ability and robustness.
CONCLUSIONS: The proposed VLMF-Net model can be effectively used for the early 
diagnosis of GC, helping to address the challenges in its early detection.

© Zhaohui Jin, Yi Shuai, Yun Li, Mianmian Chen, Yumeng Liu, Wenbin Lei, Xiaomao 
Fan. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org).

DOI: 10.2196/74902
PMCID: PMC12507326
PMID: 41061147 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


361. J Am Med Inform Assoc. 2025 Dec 1;32(12):1793-1801. doi: 10.1093/jamia/ocaf165.

FHIR-Former: enhancing clinical predictions through Fast Healthcare 
Interoperability Resources and large language models.

Engelke M(1)(2), Baldini G(1)(2), Kleesiek J(1)(3)(4), Nensa F(1)(2), Dada A(1).

Author information:
(1)Institute for Artificial Intelligence in Medicine, University Medicine Essen, 
Essen, 45131, Germany.
(2)Institute of Diagnostic and Interventional Radiology and Neuroradiology, 
University Medicine Essen, Essen, 45147, Germany.
(3)Faculty of Computer Science and Medical Faculty, University of 
Duisburg-Essen, Essen, 45127, Germany.
(4)Department of Physics, TU Dortmund University, Dortmund, 44227, Germany.

OBJECTIVE: To address the challenges of data heterogeneity and manual feature 
engineering in clinical predictive modeling, we introduce FHIR-Former, an 
open-source framework integrating Fast Healthcare Interoperability Resources 
(FHIR) with large language models (LLMs) to automate and standardize clinical 
prediction tasks.
MATERIALS AND METHODS: FHIR-Former dynamically processes structured (eg, lab 
results, medications) and unstructured (eg, clinical notes) data from FHIR 
resources. The pipeline supports multiple classification tasks, including 30-day 
readmission, imaging study prediction, and ICD code classification. Leveraging 
open-source LLMs (GeBERTa), we trained models on 1.1 million data points across 
ten FHIR resources using retrospective inpatient data (2018-2024). 
Hyperparameters were optimized via Bayesian methods, and outputs were mapped to 
FHIR RiskAssessment resources for interoperability.
RESULTS: FHIR-Former achieved an F1-score of 70.7% and accuracy of 72.9% for 
30-day readmission, 51.8% F1-score (88.1% accuracy) for mortality prediction, 
and 61% macro F1-score for imaging study classification. The ICD code prediction 
model attained 94% accuracy. Performance demonstrated promising performance for 
readmission and showed scalability across tasks without manual feature 
engineering.
DISCUSSION: FHIR-Former eliminates institution-specific preprocessing by 
adapting to diverse FHIR implementations, enabling seamless integration of 
multimodal data. Its configurable architecture outperformed prior frameworks 
reliant on static inputs or limited to unstructured text. Real-time risk scores 
embedded in FHIR servers enhance clinical workflows without disrupting existing 
practices.
CONCLUSION: By harmonizing FHIR standardization with LLM flexibility, 
FHIR-Former advances scalable, interoperable predictive modeling in healthcare. 
The open-source framework facilitates automation, improves resource allocation, 
and supports personalized decision-making, bridging gaps between AI innovation 
and clinical practice.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf165
PMCID: PMC12646377
PMID: 41082356 [Indexed for MEDLINE]

Conflict of interest statement: None declared.


362. J Fluency Disord. 2025 Sep;85:106149. doi: 10.1016/j.jfludis.2025.106149. Epub 
2025 Aug 15.

Assessing the response quality and readability of ChatGPT in stuttering.

Saeedi S(1), Bakhtiar M(2).

Author information:
(1)Speech and Neuromodulation Laboratory, Unit of Human Communication, Learning 
and Development, Faculty of Education, The University of Hong Kong, Hong Kong 
Special Administrative Region of China. Electronic address: 
saeed_saeedi96@connect.hku.hk.
(2)Speech and Neuromodulation Laboratory, Unit of Human Communication, Learning 
and Development, Faculty of Education, The University of Hong Kong, Hong Kong 
Special Administrative Region of China.

OBJECTIVE: This study aimed to examine how frequently asked questions regarding 
stuttering were comprehended and answered by ChatGPT.
METHODS: In this exploratory study, eleven common questions about stuttering 
were asked in a single conversation with the GPT-4o mini. While being blind 
relative to the source of the answers (whether by AI or SLPs), a panel of five 
certified speech and language pathologists (SLPs) was requested to differentiate 
if responses were produced by the ChatGPT chatbot or provided by SLPs. 
Additionally, they were instructed to evaluate the responses based on several 
criteria, including the presence of inaccuracies, the potential for causing harm 
and the degree of harm that could result, and alignment with the prevailing 
consensus within the SLP community. All ChatGPT responses were also evaluated 
utilizing various readability features, including the Flesch Reading Ease Score 
(FRES), Gunning Fog Scale Level (GFSL), and Dale-Chall Score (D-CS), the number 
of words, number of sentences, words per sentence (WPS), characters per word 
(CPW), and the percentage of difficult words. Furthermore, Spearman's rank 
correlation coefficient was employed to examine relationship between the 
evaluations conducted by the panel of certified SLPs and readability features.
RESULTS: A substantial proportion of the AI-generated responses (45.50 %) were 
incorrectly identified by SLP panel as being written by other SLPs, indicating 
high perceived human-likeness (origin). Regarding content quality, 83.60 % of 
the responses were found to be accurate (incorrectness), 63.60 % were rated as 
harmless (harm), and 38.20 % were considered to cause only minor to moderate 
impact (extent of harm). In terms of professional alignment, 62 % of the 
responses reflected the prevailing views within the SLP community (consensus). 
The means ± standard deviation of FRES, GFSL, and D-CS were 26.52 ± 13.94 
(readable for college graduates), 18.17 ± 3.39 (readable for graduate students), 
and 9.90 ± 1.08 (readable for 13th to 15th grade [college]), respectively. 
Furthermore, each response contained an average of 99.73 words, 6.80 sentences, 
17.44 WPS, 5.79 CPW, and 27.96 % difficult words. The correlation coefficients 
ranged between significantly large negative value (r = -0.909, p < 0.05) to very 
large positive value (r = 0.918, p < 0.05).
CONCLUSION: The results revealed that the emerging ChatGPT possesses a promising 
capability to provide appropriate responses to frequently asked questions in the 
field of stuttering, which is attested by the fact that panel of certified SLPs 
perceived about 45 % of them to be generated by SLPs. However, given the 
increasing accessibility of AI tools, particularly among individuals with 
limited access to professional services, it is crucial to emphasize that such 
tools are intended solely for educational purposes and should not replace 
diagnosis or treatment by qualified SLPs.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jfludis.2025.106149
PMID: 40848602 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
report there are no competing interests to declare.


363. JMIR Med Inform. 2025 Aug 15;13:e68366. doi: 10.2196/68366.

AI-Driven Integrated System for Burn Depth Prediction With Electronic Medical 
Records: Algorithm Development and Validation.

Rahman MM(#)(1), Masry ME(#)(2)(3), Gnyawali SC(2)(3), Xue Y(4), Gordillo 
G(#)(2)(5), Wachs JP(#)(1).

Author information:
(1)Edwardson School of Industrial Engineering, Purdue University West Lafayette, 
315 N Grant Street, West Lafayette, IN, 47907, United States, 1 765 496 7380.
(2)McGowan Institute for Regenerative Medicine, University of Pittsburgh, 
Pittsburgh, PA, United States.
(3)Department of Surgery, School of Medicine, University of Pittsburgh, 
Pittsburgh, PA, United States.
(4)Department of Computer Science, Purdue University West Lafayette, West 
Lafayette, IN, United States.
(5)Department of Plastic Surgery, School of Medicine, University of Pittsburgh, 
Pittsburgh, PA, United States.
(#)Contributed equally

BACKGROUND: Burn injuries represent a significant clinical challenge due to the 
complexity of accurately assessing burn depth, which directly influences the 
course of treatment and patient outcomes. Traditional diagnostic methods 
primarily rely on visual inspection by experienced burn surgeons. Studies report 
diagnostic accuracies of around 76% for experts, dropping to nearly 50% for less 
experienced clinicians. Such inaccuracies can result in suboptimal clinical 
decisions-delaying vital surgical interventions in severe cases or initiating 
unnecessary treatments for superficial burns. This diagnostic variability not 
only compromises patient care but also strains health care resources and 
increases the likelihood of adverse outcomes. Hence, a more consistent and 
precise approach to burn classification is urgently needed.
OBJECTIVE: The objective is to determine whether a multimodal integrated 
artificial intelligence (AI) system for accurate classification of burn depth 
can preserve diagnostic accuracy and provide an important resource when used as 
part of the electronic medical record (EMR).
METHODS: This study used a novel multimodal AI system, integrating digital 
photographs and ultrasound tissue Doppler imaging (TDI) data to accurately 
assess burn depth. These imaging modalities were accessed and processed through 
an EMR system, enabling real-time data retrieval and AI-assisted evaluation. TDI 
was instrumental in evaluating the biomechanical properties of subcutaneous 
tissues, using color-coded images to identify burn-induced changes in tissue 
stiffness and elasticity. The collected imaging data were uploaded to the EMR 
system (DrChrono), where they were processed by a vision-language model built on 
GPT-4 architecture. This model received expert-formulated prompts describing how 
to interpret both digital and TDI images, guiding the AI in making explainable 
classifications.
RESULTS: This study evaluated whether a multimodal AI classifier, designed to 
identify first-, second-, and third-degree burns, could be effectively applied 
to imaging data stored within an EMR system. The classifier achieved an overall 
accuracy of 84.38%, significantly surpassing human performance benchmarks 
typically cited in the literature. This highlights the potential of the AI model 
to serve as a robust clinical decision support tool, especially in settings 
lacking highly specialized expertise. In addition to accuracy, the classifier 
demonstrated strong performance across multiple evaluation metrics. The 
classifier's ability to distinguish between burn severities was further 
validated by the area under the receiver operating characteristic: 0.97 for 
first-degree, 0.96 for second-degree, and a perfect 1.00 for third-degree burns, 
each with narrow 95% CIs.
CONCLUSIONS: The storage of multimodal imaging data within the EMR, along with 
the ability for post hoc analysis by AI algorithms, offers significant 
advancements in burn care, enabling real-time burn depth prediction on currently 
available data. Using digital photos for superficial burns, easily diagnosed 
through physical examinations, reduces reliance on TDI, while TDI helps 
distinguish deep second- and third-degree burns, enhancing diagnostic 
efficiency.

© Md Masudur Rahman, Mohamed El Masry, Surya C Gnyawali, Yexiang Xue, Gayle 
Gordillo, Juan P Wachs. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org).

DOI: 10.2196/68366
PMCID: PMC12356521
PMID: 40815778 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


364. JMIR Med Inform. 2025 Jun 11;13:e70924. doi: 10.2196/70924.

Using a Large Language Model for Breast Imaging Reporting and Data System 
Classification and Malignancy Prediction to Enhance Breast Ultrasound Diagnosis: 
Retrospective Study.

Miaojiao S(#)(1), Xia L(#)(1), Xian Tao Z(1), Zhi Liang H(1), Sheng C(1), 
Songsong W(#)(1).

Author information:
(1)Department of Ultrasound, Fujian Provincial Hospital, Affiliated Hospital of 
Fuzhou University, 134 Dong Street, Gulou District, Fuzhou, 350500, China, 86 
15960191609.
(#)Contributed equally

BACKGROUND: Breast ultrasound is essential for evaluating breast nodules, with 
Breast Imaging Reporting and Data System (BI-RADS) providing standardized 
classification. However, interobserver variability among radiologists can affect 
diagnostic accuracy. Large language models (LLMs) like ChatGPT-4 have shown 
potential in medical imaging interpretation. This study explores its feasibility 
in improving BI-RADS classification consistency and malignancy prediction 
compared to radiologists.
OBJECTIVE: This study aims to evaluate the feasibility of using LLMs, 
particularly ChatGPT-4, to assess the consistency and diagnostic accuracy of 
standardized breast ultrasound imaging reports, using pathology as the reference 
standard.
METHODS: This retrospective study analyzed breast nodule ultrasound data from 
671 female patients (mean 45.82, SD 9.20 years; range 26-75 years) who underwent 
biopsy or surgical excision at our hospital between June 2019 and June 2024. 
ChatGPT-4 was used to interpret BI-RADS classifications and predict benign 
versus malignant nodules. The study compared the model's performance to that of 
two senior radiologists (≥15 years of experience) and two junior radiologists 
(<5 years of experience) using key diagnostic metrics, including accuracy, 
sensitivity, specificity, area under the receiver operating characteristic 
curve, P values, and odds ratios with 95% CIs. Two diagnostic models were 
evaluated: (1) image interpretation model, where ChatGPT-4 classified nodules 
based on BI-RADS features, and (2) image-to-text-LLM model, where radiologists 
provided textual descriptions, and ChatGPT-4 determined malignancy probability 
based on keywords. Radiologists were blinded to pathological outcomes, and 
BI-RADS classifications were finalized through consensus.
RESULTS: ChatGPT-4 achieved an overall BI-RADS classification accuracy of 
96.87%, outperforming junior radiologists (617/671, 91.95% and 604/671, 90.01%, 
P<.01). For malignancy prediction, ChatGPT-4 achieved an area under the receiver 
operating characteristic curve of 0.82 (95% CI 0.79-0.85), an accuracy of 80.63% 
(541/671 cases), a sensitivity of 90.56% (259/286 cases), and a specificity of 
73.51% (283/385 cases). The image interpretation model demonstrated performance 
comparable to senior radiologists, while the image-to-text-LLM model further 
improved diagnostic accuracy for all radiologists, increasing their sensitivity 
and specificity significantly (P<.001). Statistical analyses, including the 
McNemar test and DeLong test, confirmed that ChatGPT-4 outperformed junior 
radiologists (P<.01) and showed noninferiority compared to senior radiologists 
(P>.05). Pathological diagnoses served as the reference standard, ensuring 
robust evaluation reliability.
CONCLUSIONS: Integrating ChatGPT-4 into an image-to-text-LLM workflow improves 
BI-RADS classification accuracy and supports radiologists in breast ultrasound 
diagnostics. These results demonstrate its potential as a decision-support tool 
to enhance diagnostic consistency and reduce variability.

© Su Miaojiao, Liang Xia, Zeng Xian Tao, Hong Zhi Liang, Cheng Sheng, Wu 
Songsong. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org).

DOI: 10.2196/70924
PMCID: PMC12175873
PMID: 40498674 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: The authors declare that 
the research was conducted in the absence of any commercial or financial 
relationships that could be construed as a potential conflict of interest.


365. World J Transplant. 2025 Sep 18;15(3):103536. doi: 10.5500/wjt.v15.i3.103536.

Comparison of ChatGPT-3.5 and GPT-4 as potential tools in artificial 
intelligence-assisted clinical practice in renal and liver transplantation.

Christou CD(1), Sitsiani O(2), Boutos P(2), Katsanos G(3), Papadakis G(4), Tefas 
A(5), Papalois V(6), Tsoulfas G(3).

Author information:
(1)Center for Research and Innovation in Solid Organ Transplantation, School of 
Medicine, Aristotle University of Thessaloniki, Thessaloniki 54622, Greece. 
christouchrysanthosd@gmail.com.
(2)School of Medicine, Aristotle University of Thessaloniki, Thessaloniki 54622, 
Greece.
(3)Center for Research and Innovation in Solid Organ Transplantation, School of 
Medicine, Aristotle University of Thessaloniki, Thessaloniki 54622, Greece.
(4)Department of Nephrology and Transplantation, Guy's Hospital, Guy's and St 
Thomas' NHS Foundation Trust, London SE1 1UL, United Kingdom.
(5)Computational Intelligence and Deep Learning Group, Department of 
Informatics, Aristotle University of Thessaloniki, Thessaloniki 54636, Greece.
(6)Renal and Transplant Unit, Hammersmith Hospital, Imperial College Healthcare 
NHS Trust, London W120HS, United Kingdom.

BACKGROUND: Kidney and liver transplantation are two sub-specialized medical 
disciplines, with transplant professionals spending decades in training. While 
artificial intelligence-based (AI-based) tools could potentially assist in 
everyday clinical practice, comparative assessment of their effectiveness in 
clinical decision-making remains limited.
AIM: To compare the use of ChatGPT and GPT-4 as potential tools in AI-assisted 
clinical practice in these challenging disciplines.
METHODS: In total, 400 different questions tested ChatGPT's/GPT-4 knowledge and 
decision-making capacity in various renal and liver transplantation concepts. 
Specifically, 294 multiple-choice questions were derived from open-access 
sources, 63 questions were derived from published open-access case reports, and 
43 from unpublished cases of patients treated at our department. The evaluation 
covered a plethora of topics, including clinical predictors, treatment options, 
and diagnostic criteria, among others.
RESULTS: ChatGPT correctly answered 50.3% of the 294 multiple-choice questions, 
while GPT-4 demonstrated a higher performance, answering 70.7% of questions (P < 
0.001). Regarding the 63 questions from published cases, ChatGPT achieved an 
agreement rate of 50.79% and partial agreement of 17.46%, while GPT-4 
demonstrated an agreement rate of 80.95% and partial agreement of 9.52% (P = 
0.01). Regarding the 43 questions from unpublished cases, ChatGPT demonstrated 
an agreement rate of 53.49% and partial agreement of 23.26%, while GPT-4 
demonstrated an agreement rate of 72.09% and partial agreement of 6.98% (P = 
0.004). When factoring by the nature of the task for all cases, notably, GPT-4 
demonstrated outstanding performance, providing a differential diagnosis that 
included the final diagnosis in 90% of the cases (P = 0.008), and successfully 
predicting the prognosis of the patient in 100% of related questions (P < 
0.001).
CONCLUSION: GPT-4 consistently provided more accurate and reliable clinical 
recommendations with higher percentages of full agreements both in renal and 
liver transplantation compared with ChatGPT. Our findings support the potential 
utility of AI models like ChatGPT and GPT-4 in AI-assisted clinical practice as 
sources of accurate, individualized medical information and facilitating 
decision-making. The progression and refinement of such AI-based tools could 
reshape the future of clinical practice, making their early adoption and 
adaptation by physicians a necessity.

©The Author(s) 2025. Published by Baishideng Publishing Group Inc. All rights 
reserved.

DOI: 10.5500/wjt.v15.i3.103536
PMCID: PMC12038595
PMID: 40881761

Conflict of interest statement: Conflict-of-interest statement: All the authors 
report no relevant conflicts of interest for this article.


366. NPJ Digit Med. 2026 Jan 7. doi: 10.1038/s41746-025-02304-8. Online ahead of 
print.

EvoMDT: a self-evolving multi-agent system for structured clinical 
decision-making in multi-cancer.

Liu Q(#)(1)(2), Hu Z(#)(3), Huang T(#)(4), Niu Y(#)(5), Zhang X(#)(2), Ma S(6), 
Lin C(6), Huat GK(3), Kwon HE(3), Gao F(7)(8), Sun X(9), Ying Z(10), Qiang 
G(11).

Author information:
(1)Department of Reproductive Medicine Centre, The First Affiliated Hospital, 
Fujian Medical University, Fuzhou, China.
(2)School of Biomedical Engineering, Tsinghua University, Beijing, China.
(3)Department of Information Systems and Operation Management, Nanyang 
Technological University, Singapore, Singapore.
(4)Department of Breast Disease, Henan Breast Cancer Center, The Affiliated 
Cancer Hospital of Zhengzhou University & Henan Cancer Hospital, Zhengzhou, 
China.
(5)Tianjin Key Laboratory of Radiation Medicine and Molecular Nuclear Medicine, 
Institute of Radiation Medicine, Chinese Academy of Medical Science & Peking 
Union Medical College, Tianjin, China.
(6)Department of Thoracic Surgery, Peking University Third Hospital, Beijing, 
China.
(7)Department of Pathology, The First Affiliated Hospital, Fujian Medical 
University, Fuzhou, China.
(8)Department of Pathology, National Regional Medical Center, Binhai Campus of 
the First Affiliated Hospital, Fujian Medical University, Fuzhou, China.
(9)Department of Breast Disease, Henan Breast Cancer Center, The Affiliated 
Cancer Hospital of Zhengzhou University & Henan Cancer Hospital, Zhengzhou, 
China. zlyysunxianfu1256@zzu.edu.cn.
(10)Department of Medical Oncology, National Cancer Center/National Clinical 
Research Center for Cancer/Cancer Hospital, Chinese Academy of Medical Sciences 
& Peking Union Medical College, Beijing, China. yingzhitao001@163.com.
(11)Department of Thoracic Surgery, Peking University Third Hospital, Beijing, 
China. pkudd@bjmu.edu.cn.
(#)Contributed equally

Multidisciplinary tumor boards (MDTs) are central to cancer care but remain 
constrained by scarce experts and variable decision quality. EvoMDT employs a 
self-evolution loop that updates prompts, consensus weights, and retrieval scope 
based on expert feedback and outcome signals, improving robustness without 
sacrificing traceability. This matters clinically because MDT workloads and 
evidence shift over time, requiring adaptive yet auditable decision support. 
Agents perform domain-specific inference over lesion-level clinical data with 
structured knowledge retrieval; a consensus protocol resolves conflicts and 
generates traceable, evidence-linked recommendations. Evaluation spanned six 
public oncology QA benchmarks and four real-world datasets (breast, liver, lung, 
lymphoma), followed by single-blind physician assessment. Quantitative metrics 
(ROUGE, BERTScore) and automated safety checks assessed factuality and guideline 
concordance, while clinicians rated clinical appropriateness and usability. 
EvoMDT outperformed frontier Large Language Models (LLMs) baselines (e.g., 
Llama-3-70B, Claude-3, Med-PaLM 2), improving guideline concordance and semantic 
alignment with expert plans (BERTScore 0.62-0.68) and reducing safety 
violations. In physician review, EvoMDT achieved decision quality comparable to 
human MDTs while shortening response time by 30-40%. These results position 
EvoMDT as an interpretable, evidence-traceable framework that operationalizes AI 
reasoning for multidisciplinary oncology practice and offers a scalable 
foundation for trustworthy, lesion-level precision cancer care.

© 2026. The Author(s).

DOI: 10.1038/s41746-025-02304-8
PMID: 41501128

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


367. Int J Med Inform. 2025 Nov;203:106035. doi: 10.1016/j.ijmedinf.2025.106035. Epub 
2025 Jul 1.

Enhancing AI for citation screening in literature reviews: Improving accuracy 
with ensemble models.

Zhang Z(1), Momeni Nezhad MJ(2), Gupta P(3), Zolnour A(2), Azadmaleki H(2), 
Topaz M(4), Zolnoori M(5).

Author information:
(1)Data Science Institute, Columbia University, New York, NY 10027, USA; School 
of Nursing, Columbia University, New York, NY 10032, USA. Electronic address: 
zz3238@columbia.edu.
(2)Columbia University Irving Medical Center, New York, NY 10032, USA.
(3)School of Nursing, Columbia University, New York, NY 10032, USA.
(4)Data Science Institute, Columbia University, New York, NY 10027, USA; School 
of Nursing, Columbia University, New York, NY 10032, USA.
(5)School of Nursing, Columbia University, New York, NY 10032, USA; Columbia 
University Irving Medical Center, New York, NY 10032, USA.

BACKGROUND: Healthcare literature reviews underpin evidence-based practice and 
clinical guideline development, with citation screening as a critical yet 
time-consuming step. This study evaluates the effectiveness of individual large 
language models (LLMs) versus ensemble approaches in automating citation 
screening to improve the efficiency and scalability of evidence synthesis in 
healthcare research.
METHODS: Performance was assessed across three healthcare-focused reviews: 
LLM-Healthcare (865 citations, broad scope, 49.8 % inclusion rate), MCI-Speech 
(959 citations, narrow scope, 6.5 % inclusion rate), and Multimodal-LLM (73 
citations, moderate scope, 68.5 % inclusion rate). Six LLMs (GPT-4o Mini, 
GPT-4o, Gemini Flash, Llama 3.1 8B Instruct, Llama 3.1 70B Instruct, Llama 3.1 
405B Instruct) were evaluated using zero- and few-shot learning strategies with 
PubMedBERT for demonstration selection. We compared individual model performance 
with ensemble methods, including majority voting and random forest (RF), based 
on sensitivity and specificity.
RESULTS: No individual LLM consistently outperformed others across all tasks. 
Review with narrow inclusion criteria and low inclusion rates exhibited high 
specificity but lower sensitivity. Ensemble methods consistently surpassed 
individual LLMs: the RF ensemble with GPT-4o performed best in LLM-Healthcare 
(sensitivity: 0.96, specificity: 0.89); the majority voting with 1-shot LLMs 
(sensitivity: 0.75, specificity: 0.86) and RF ensemble with 4-shot LLMs 
(sensitivity: 0.62, specificity: 0.97) excelled in MCI-Speech; and four RF 
ensembles achieved perfect classification (sensitivity: 1.0, specificity: 1.0) 
in Multimodal-LLM.
CONCLUSION: Ensemble approaches improve individual LLMs' performances in 
citation screening across diverse healthcare review tasks, highlighting their 
potential to enhance evidence synthesis workflows that support clinical 
decision-making. However, broader validation is needed before real-world 
implementation.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106035
PMID: 40609462 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


368. Med Educ Online. 2025 Dec;30(1):2554678. doi: 10.1080/10872981.2025.2554678. 
Epub 2025 Aug 30.

Assessing LLM-generated vs. expert-created clinical anatomy MCQs: a student 
perception-based comparative study in medical education.

Elzayyat M(1), Mohammad JN(1), Zaqout S(1).

Author information:
(1)Department of Basic Medical Sciences, College of Medicine, QU Health, Qatar 
University, Doha, Qatar.

Large language models (LLMs) such as ChatGPT and Gemini are increasingly used to 
generate educational content in medical education, including multiple-choice 
questions (MCQs), but their effectiveness compared to expert-written questions 
remains underexplored, particularly in anatomy. We conducted a cross-sectional, 
mixed-methods study involving Year 2-4 medical students at Qatar University, 
where participants completed and evaluated three anonymized MCQ sets-authored by 
ChatGPT, Google-Gemini, and a clinical anatomist-across 17 quality criteria. 
Descriptive and chi-square analyses were performed, and optional feedback was 
reviewed thematically. Among 48 participants, most rated the three MCQ sources 
as equally effective, although ChatGPT was more often preferred for helping 
students identify and confront their knowledge gaps through challenging 
distractors and diagnostic insight, while expert-written questions were rated 
highest for deeper analytical thinking. A significant variation in preferences 
was observed across sources (χ² (64) = 688.79, p < .001). Qualitative feedback 
emphasized the need for better difficulty calibration and clearer distractors in 
some AI-generated items. Overall, LLM-generated anatomy MCQs can closely match 
expert-authored ones in learner-perceived value and may support deeper 
engagement, but expert review remains critical to ensure clarity and alignment 
with curricular goals. A hybrid AI-human workflow may provide a promising path 
for scalable, high-quality assessment design in medical education.

DOI: 10.1080/10872981.2025.2554678
PMCID: PMC12404065
PMID: 40884796 [Indexed for MEDLINE]

Conflict of interest statement: No potential conflict of interest was reported 
by the author(s).


369. JMIR Form Res. 2025 Jun 6;9:e70070. doi: 10.2196/70070.

Exploring Generative Pre-Trained Transformer-4-Vision for Nystagmus 
Classification: Development and Validation of a Pupil-Tracking Process.

Noda M(1)(2), Koshu R(2), Tsunoda R(1), Ogihara H(1), Kamo T(1), Ito M(2), 
Fushiki H(1).

Author information:
(1)Department of Otolaryngology, Mejiro University Ear Institute Clinic, 320 
Ukiya, Iwatsuki-ku, Saitama-shi, Saitama, 339-8501, Japan, 81 48 797 3341.
(2)Department of Otolaryngology, Jichi Medical University, Shimotsuke, Japan.

BACKGROUND: Conventional nystagmus classification methods often rely on 
subjective observation by specialists, which is time-consuming and variable 
among clinicians. Recently, deep learning techniques have been used to automate 
nystagmus classification using convolutional and recurrent neural networks. 
These networks can accurately classify nystagmus patterns using video data. 
However, associated challenges including the need for large datasets when 
creating models, limited applicability to address specific image conditions, and 
the complexity associated with using these models.
OBJECTIVE: This study aimed to evaluate a novel approach for nystagmus 
classification that used the Generative Pre-trained Transformer 4 Vision 
(GPT-4V) model, which is a state-of-the-art large-scale language model with 
powerful image recognition capabilities.
METHODS: We developed a pupil-tracking process using a nystagmus-recording video 
and verified the optimization model's accuracy using GPT-4V classification and 
nystagmus recording. We tested whether the created optimization model could be 
evaluated in six categories of nystagmus: right horizontal, left horizontal, 
upward, downward, right torsional, and left torsional. The traced trajectory was 
input as two-dimensional coordinate data or an image, and multiple in-context 
learning methods were evaluated.
RESULTS: The developed model showed an overall classification accuracy of 37% 
when using pupil-traced images and a maximum accuracy of 24.6% when pupil 
coordinates were used as input. Regarding orientation, we achieved a maximum 
accuracy of 69% for the classification of horizontal nystagmus patterns but a 
lower accuracy for the vertical and torsional components.
CONCLUSIONS: We demonstrated the potential of versatile vertigo management in a 
generative artificial intelligence model that improves the accuracy and 
efficiency of nystagmus classification. We also highlighted areas for further 
improvement, such as expanding the dataset size and enhancing input modalities, 
to improve classification performance across all nystagmus types. The GPT-4V 
model validated only for recognizing still images can be linked to video 
classification and proposed as a novel method.

© Masao Noda, Ryota Koshu, Reiko Tsunoda, Hirofumi Ogihara, Tomohiko Kamo, 
Makoto Ito, Hiroaki Fushiki. Originally published in JMIR Formative Research 
(https://formative.jmir.org).

DOI: 10.2196/70070
PMCID: PMC12164947
PMID: 40478723 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


370. Am J Hum Genet. 2025 May 1;112(5):1158-1172. doi: 10.1016/j.ajhg.2025.03.014. 
Epub 2025 Apr 16.

First steps toward building natural history of diseases computationally: Lessons 
learned from the Noonan syndrome use case.

Groza T(1), Rayabsri W(2), Gration D(2), Hariram H(3), Jamuar SS(4), Baynam 
G(5).

Author information:
(1)Rare Care Centre, Perth Children's Hospital, Nedlands, WA 6009, Australia; 
Bioinformatics Institute, Agency for Science, Technology and Research 
(A(∗)STAR), 30 Biopolis Street #07-01 Matrix, Singapore 138671, Singapore; 
SingHealth Duke-NUS Institute of Precision Medicine, 5 Hospital Drive Level 9, 
Singapore 169609, Singapore; School of Electrical Engineering, Computing and 
Mathematical Sciences, Curtin University, Kent Street, Bentley, WA 6102, 
Australia. Electronic address: tudor.groza@health.wa.gov.au.
(2)Western Australian Register of Developmental Anomalies, King Edward Memorial 
Hospital, 374 Bagot Road, Subiaco, WA 6008, Australia.
(3)Medical Student, Division of Medical Education, School of Medical Sciences, 
Faculty of Biology, Medicine and Health, The University of Manchester, 
Manchester M13 9PL, UK.
(4)SingHealth Duke-NUS Institute of Precision Medicine, 5 Hospital Drive Level 
9, Singapore 169609, Singapore; Genetics Service, Department of Paediatrics, KK 
Women's and Children's Hospital, 100 Bukit Timah Road, Singapore 229899, 
Singapore; SingHealth Duke-NUS Genomic Medicine Centre, 100 Bukit Timah Road, 
Singapore 229899, Singapore.
(5)Rare Care Centre, Perth Children's Hospital, Nedlands, WA 6009, Australia; 
Western Australian Register of Developmental Anomalies, King Edward Memorial 
Hospital, 374 Bagot Road, Subiaco, WA 6008, Australia; Faculty of Health and 
Medical Sciences, University of Western Australia, 35 Stirling Highway, Crawley, 
WA 6009, Australia.

Rare diseases (RDs) are conditions affecting fewer than 1 in 2,000 people, with 
over 7,000 identified, primarily genetic in nature, and more than half impacting 
children. Although each RD affects a small population, collectively, between 
3.5% and 5.9% of the global population, or 262.9-446.2 million people, live with 
an RD. Most RDs lack established treatment protocols, highlighting the need for 
proper care pathways addressing prognosis, diagnosis, and management. Advances 
in generative AI and large language models (LLMs) offer new opportunities to 
document the temporal progression of phenotypic features, addressing gaps in 
current knowledge bases. This study proposes an LLM-based framework to capture 
the natural history of diseases, specifically focusing on Noonan syndrome. The 
framework aims to document phenotypic trajectories, validate against RD 
knowledge bases, and integrate insights into care coordination using electronic 
health record (EHR) data from the Undiagnosed Diseases Program Singapore.

Crown Copyright © 2025. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.ajhg.2025.03.014
PMCID: PMC12120186
PMID: 40245863 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of interests The authors declare no 
competing interests.


371. Int J Med Inform. 2026 Jan 6;209:106270. doi: 10.1016/j.ijmedinf.2026.106270. 
Online ahead of print.

Large Language Models' Performances regarding logical observation identifiers 
names and codes mapping in laboratory medicine: A comparative analysis of 
ChatGPT-4.0, Gemini, and Perplexity.

Yu S(1), Cho EJ(2), Kim S(3), Park K(4), Kim MS(5), Oh Y(6), Ryu H(7).

Author information:
(1)Department of Laboratory Medicine, Haeundae Paik Hospital, Inje University 
College of Medicine, Busan, South Korea.
(2)Department of Laboratory Medicine, Hallym University Dongtan Sacred Heart 
Hospital, Hallym University College of Medicine, Hwaseong, South Korea. 
Electronic address: ejlovi@naver.com.
(3)Department of Laboratory Medicine, Asan Medical Center, University of Ulsan 
College of Medicine, Seoul, South Korea.
(4)Department of Laboratory Medicine, Sanggye Paik Hospital, College of 
Medicine, Inje University, Seoul, South Korea.
(5)Department of Laboratory Medicine, Soonchunhyang University Cheonan Hospital, 
Soonchunhyang University College of Medicine, Cheonan, South Korea.
(6)Department of Laboratory Medicine, Green Cross Laboratories, Yongin, South 
Korea.
(7)Department of Laboratory Medicine, Seegene Medical Foundation, Seoul, South 
Korea.

OBJECTIVES: This study aimed to assess the feasibility and practical utility of 
using large language models (LLMs) for Logical Observation Identifiers Names and 
Codes (LOINC) mapping to standardise healthcare data in the field of laboratory 
medicine. We evaluated the accuracy and applicability of three LLMs-ChatGPT-4.0 
(OpenAI), Gemini 1.5 (Google DeepMind), and Perplexity AI (Perplexity.ai)-in 
mapping laboratory test items, which typically require considerable 
institutional-level standardisation efforts.
METHODS: A total of 75 representative laboratory test items, including 55 
clinical chemistry and 20 hematology tests commonly used in clinical practice, 
were selected. Six board-certified clinical pathologists independently mapped 
each test item to its appropriate LOINC code. A consensus mapping was 
established by the experts and used as the gold standard. Each LLM's output was 
compared to this consensus, and the results were categorised as complete match 
(CM), partial match (PM), or mismatch (MM) based on agreement with the 
reference.
RESULTS: Overall paired ordinal analyses demonstrated a significant difference 
in LOINC code-mapping performance among the three models, with Gemini performing 
significantly worse than both ChatGPT-4.0 and Perplexity AI, and no significant 
difference between ChatGPT-4.0 and Perplexity AI. ChatGPT-4.0 achieved the 
highest CM rate in clinical chemistry (58.2%), whereas Perplexity AI performed 
best in hematology (55.0%). Gemini showed the highest MM rates, particularly in 
hematology (80.0%), while partial matches were largely attributable to 
method-related discrepancies rather than fully incorrect mappings.
CONCLUSION: Structured inputs, localisation to domestic laboratory practices, 
and expert oversight are critical to improving the reliability of LLM-generated 
LOINC mappings. While LLMs can reduce workload by generating candidate mappings, 
human validation remains essential to ensure clinical accuracy. Future 
improvements should focus on algorithmic refinement, error feedback integration, 
and adaptation to diverse laboratory settings to enhance accuracy and 
generalisability in real-world laboratory settings.

Copyright © 2026 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2026.106270
PMID: 41506176

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


372. Digit Health. 2026 Jan 2;12:20552076251406653. doi: 10.1177/20552076251406653. 
eCollection 2026 Jan-Dec.

The evolving landscape of artificial intelligence in patient education: A 
bibliometric knowledge mapping study.

Zhou J(1), Zhang W(1), Liu S(2).

Author information:
(1)Hospital Administration Office, The First Affiliated Hospital of Fujian 
Medical University, Fuzhou, China.
(2)Department of Neurosurgery, Fujian Medical University Union Hospital, Fuzhou, 
China.

BACKGROUND: Artificial intelligence (AI) is emerging as a transformative force 
in digital health, offering novel solutions to overcome traditional barriers in 
patient education, such as the low readability of materials and the high cost of 
personalization. The rapid integration of Large Language Models (LLMs) 
necessitates a clear understanding of the current research landscape to guide 
effective and ethical implementation.
OBJECTIVE: This study aims to systematically map the global research landscape 
of AI in patient education. This bibliometric analysis identifies the knowledge 
structure, research hotspots, key contributors, and evolutionary trends to guide 
future research and practice in this rapidly emerging domain.
METHODS: We retrieved 837 relevant documents published between 2016 and 2025 
from the Web of Science Core Collection. Bibliometric data were analyzed using 
CiteSpace and RStudio to conduct visual analyses of publication trends, 
international collaborations, co-citation networks, and keyword evolution.
RESULTS: The analysis revealed an exponential increase in publications since 
2021, a trend that strongly coincides with the advent of LLMs. The USA and China 
are the primary research contributors, with Harvard University leading 
institutional output. Research hotspots have evolved from foundational concepts 
like machine learning to application-focused themes such as health literacy, 
readability, and adherence. The intellectual base is highly interdisciplinary, 
drawing from medicine, computer science, and education.
CONCLUSION: AI is rapidly transforming patient education, with a clear 
trajectory from technology-focused validation to patient-centered outcomes. 
While LLMs show immense potential, significant challenges persist regarding 
accuracy, ethical implementation, and systematic integration into clinical 
workflows. Future efforts must prioritize developing robust validation 
frameworks and strengthening international collaboration to enhance digital 
health equity.

© The Author(s) 2026.

DOI: 10.1177/20552076251406653
PMCID: PMC12759138
PMID: 41488273

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


373. J Med Internet Res. 2025 Aug 12;27:e73540. doi: 10.2196/73540.

Using a Diverse Test Suite to Assess Large Language Models on Fast Health Care 
Interoperability Resources Knowledge: Comparative Analysis.

Idrissi-Yaghir A(#)(1)(2), Arzideh K(#)(2)(3), Schäfer H(2)(4), Eryilmaz B(2), 
Bahn M(2), Wen Y(2), Borys K(1)(2), Hartmann E(5), Schmidt C(2)(4), Pelka 
O(1)(2), Haubold J(1)(2), Friedrich CM(6)(7), Nensa F(1)(2), Hosch R(#)(1)(2).

Author information:
(1)Institute of Diagnostic and Interventional Radiology and Neuroradiology, 
University Hospital Essen, Essen, Germany.
(2)Institute for Artificial Intelligence in Medicine, University Hospital Essen, 
Hufelandstraße 55, Essen, 45147, Germany, 49 201 - 72377817.
(3)Department of Central IT, Data Integration Center, University Hospital Essen, 
Essen, Germany.
(4)Institute for Transfusion Medicine, University Hospital Essen, Essen, 
Germany.
(5)Department of Pulmonary Medicine, University Hospital Essen - Ruhrlandklinik, 
Essen, Germany.
(6)Department of Computer Science, University of Applied Sciences and Arts 
Dortmund, Dortmund, Germany.
(7)Institute for Medical Informatics, Biometry, and Epidemiology, University 
Hospital Essen, Essen, Germany.
(#)Contributed equally

BACKGROUND: Recent natural language processing breakthroughs, particularly with 
the emergence of large language models (LLMs), have demonstrated remarkable 
capabilities on general knowledge benchmarks. However, there is limited data on 
the performance and understanding of these models in relation to the Fast 
Healthcare Interoperability Resources (FHIR) standard. The complexity and 
specialized nature of FHIR present challenges for LLMs, which are typically 
trained on broad datasets and may have a limited understanding of the nuances 
required for domain-specific tasks. Improving health data interoperability can 
greatly benefit the use of clinical data and interaction with electronic health 
records.
OBJECTIVE: This study presents the Fast Healthcare Interoperability Resources 
(FHIR) Workbench, a comprehensive suite of datasets designed to evaluate the 
ability of LLMs to understand and apply the FHIR standard.
METHODS: In total, 4 evaluation datasets were created to assess the FHIR 
knowledge and capabilities of LLMs. These tasks include multiple-choice 
questions on general FHIR concepts and the FHIR Representational State Transfer 
(REST) application programming interface, as well as correctly identifying the 
resource type and generating FHIR resources from unstructured clinical patient 
notes. In addition, we evaluate open-source LLMs, such as Qwen 2.5 Coder and 
DeepSeek-V3, and commercial LLMs, including GPT-4o and Gemini 2, on these tasks 
in a zero-shot setting. To provide context for interpreting LLM performance, a 
subset of the datasets was human-evaluated by recruiting 6 participants with 
varying levels of FHIR expertise.
RESULTS: Our evaluation across multiple FHIR tasks revealed nuanced performance 
metrics. Commercial models demonstrated exceptional capabilities, with GPT-4o 
achieving a 0.9990 F1-score on the FHIR-ResourceID task, 0.9400 on the FHIR-QA 
task, and 0.9267 on the FHIR-RESTQA task. Open-source models also demonstrated 
strong performance, with DeepSeek-v3 achieving 0.9400 on FHIR-QA, 0.9400 on 
FHIR-RESTQA, and 0.9142 on FHIR-ResourceID. Qwen 2.5 Coder-7B-Instruct 
demonstrated high accuracy, scoring 0.9533 on FHIR-QA and 0.8920 on 
FHIR-ResourceID. However, all models struggled with the Note2FHIR task, with 
performance ranging from 0.0382 (OLMo) to a maximum of 0.3633 (GPT-4.5-preview), 
highlighting the significant challenge of converting unstructured clinical text 
into FHIR-compliant resources. Human participants achieved accuracy scores 
ranging from 0.50 to 1.0 across the first 3 tasks.
CONCLUSIONS: This study highlights the competitive performance of both 
open-source models, such as Qwen and DeepSeek, and commercial models, such as 
GPT-4o and Gemini, in FHIR-related tasks. While open-source models are advancing 
rapidly, commercial models still have an advantage for specific, complex tasks. 
The FHIR Workbench offers a valuable platform for evaluating the capabilities of 
these models and promoting improvements in health data interoperability.

© Ahmad Idrissi-Yaghir, Kamyar Arzideh, Henning Schäfer, Bahadir Eryilmaz, Mikel 
Bahn, Yutong Wen, Katarzyna Borys, Eva Hartmann, Cynthia Schmidt, Obioma Pelka, 
Johannes Haubold, Christoph M Friedrich, Felix Nensa, René Hosch. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/73540
PMCID: PMC12360669
PMID: 40795315 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


374. JMIR Form Res. 2025 Nov 27;9:e79038. doi: 10.2196/79038.

GrantCheck-an AI Solution for Guiding Grant Language to New Policy Requirements: 
Development Study.

Shi Q(1), Oztekin A(2), Matthew G(3), Bortle J(3), Jenkins H(3), Wong SK(4), 
Langlois P(3), Zaki A(3), Coleman B(3), Luzuriaga K(1), Zai AH(4).

Author information:
(1)Center for Clinical and Translational Science, UMass Chan Medical School, 
Worcester, MA, United States.
(2)Manning School of Business, UMass Lowell, Lowell, MA, United States.
(3)Information Technology, UMass Chan Medical School, Worcester, MA, United 
States.
(4)Department of Population and Quantitative Health Sciences, UMass Chan Medical 
School, Worcester, MA, United States.

BACKGROUND: Academic institutions face increasing challenges in grant writing 
due to evolving federal and state policies that restrict the use of specific 
language. Manual review processes are labor-intensive and may delay submissions, 
highlighting the need for scalable, secure solutions that ensure compliance 
without compromising scientific integrity.
OBJECTIVE: This study aimed to develop a secure, artificial intelligence-powered 
tool that assists researchers in writing grants consistent with evolving state 
and federal policy requirements.
METHODS: GrantCheck (University of Massachusetts Chan Medical School) was built 
on a private Amazon Web Services virtual private cloud, integrating a rule-based 
natural language processing engine with large language models accessed via 
Amazon Bedrock. A hybrid pipeline detects flagged terms and generates 
alternative phrasing, with validation steps to prevent hallucinations. A secure 
web-based front end enables document upload and report retrieval. Usability was 
assessed using the System Usability Scale.
RESULTS: GrantCheck achieved high performance in detecting and recommending 
alternatives for sensitive terms, with a precision of 1.00, recall of 0.73, and 
an F1-score of 0.84-outperforming general-purpose models including GPT-4o 
(OpenAI; F1=0.43), Deepseek R1 (High-Flyer; F1=0.40), Llama 3.1 (Meta AI; 
F1=0.27), Gemini 2.5 Flash (Google; F1=0.58), and even Gemini 2.5 Pro (Google; 
F1=0.72). Usability testing among 25 faculty and staff yielded a mean System 
Usability Scale score of 85.9 (SD 13.4), indicating high user satisfaction and 
strong workflow integration.
CONCLUSIONS: GrantCheck demonstrates the feasibility of deploying 
institutionally hosted, artificial intelligence-driven systems to support 
compliant and researcher-friendly grant writing. Beyond administrative 
efficiency, such systems can indirectly safeguard public health research 
continuity by minimizing grant delays and funding losses caused by 
language-related policy changes. By maintaining compliance without suppressing 
scientific rigor or inclusivity, GrantCheck helps protect the pipeline of 
research that advances biomedical discovery, health equity, and patient 
outcomes. This capability is particularly relevant for proposals in sensitive 
domains-such as social determinants of health, behavioral medicine, and 
community-based research-that are most vulnerable to evolving policy 
restrictions. As a proof-of-concept development study, our implementation is 
tailored to one institution's policy environment and security infrastructure, 
and findings should be interpreted as preliminary rather than universally 
generalizable.

©Qiming Shi, Asil Oztekin, George Matthew, Jeffrey Bortle, Hayden Jenkins, 
Steven (Koon) Wong, Paul Langlois, Anaheed Zaki, Brian Coleman, Katherine 
Luzuriaga, Adrian H Zai. Originally published in JMIR Formative Research 
(https://formative.jmir.org), 27.11.2025.

DOI: 10.2196/79038
PMCID: PMC12699247
PMID: 41308189 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


375. JMIR Med Inform. 2025 Apr 9;13:e67706. doi: 10.2196/67706.

Extracting Pulmonary Embolism Diagnoses From Radiology Impressions Using GPT-4o: 
Large Language Model Evaluation Study.

Mahyoub M(1)(2), Dougherty K(1), Shukla A(1).

Author information:
(1)Virtua Health, Marlton, NJ, United States.
(2)School of Systems Science and Industrial Engineering, Binghamton University, 
Binghamton, NY, United States.

BACKGROUND: Pulmonary embolism (PE) is a critical condition requiring rapid 
diagnosis to reduce mortality. Extracting PE diagnoses from radiology reports 
manually is time-consuming, highlighting the need for automated solutions. 
Advances in natural language processing, especially transformer models like 
GPT-4o, offer promising tools to improve diagnostic accuracy and workflow 
efficiency in clinical settings.
OBJECTIVE: This study aimed to develop an automatic extraction system using 
GPT-4o to extract PE diagnoses from radiology report impressions, enhancing 
clinical decision-making and workflow efficiency.
METHODS: In total, 2 approaches were developed and evaluated: a fine-tuned 
Clinical Longformer as a baseline model and a GPT-4o-based extractor. Clinical 
Longformer, an encoder-only model, was chosen for its robustness in text 
classification tasks, particularly on smaller scales. GPT-4o, a decoder-only 
instruction-following LLM, was selected for its advanced language understanding 
capabilities. The study aimed to evaluate GPT-4o's ability to perform text 
classification compared to the baseline Clinical Longformer. The Clinical 
Longformer was trained on a dataset of 1000 radiology report impressions and 
validated on a separate set of 200 samples, while the GPT-4o extractor was 
validated using the same 200-sample set. Postdeployment performance was further 
assessed on an additional 200 operational records to evaluate model efficacy in 
a real-world setting.
RESULTS: GPT-4o outperformed the Clinical Longformer in 2 of the metrics, 
achieving a sensitivity of 1.0 (95% CI 1.0-1.0; Wilcoxon test, P<.001) and an 
F1-score of 0.975 (95% CI 0.9495-0.9947; Wilcoxon test, P<.001) across the 
validation dataset. Postdeployment evaluations also showed strong performance of 
the deployed GPT-4o model with a sensitivity of 1.0 (95% CI 1.0-1.0), a 
specificity of 0.94 (95% CI 0.8913-0.9804), and an F1-score of 0.97 (95% CI 
0.9479-0.9908). This high level of accuracy supports a reduction in manual 
review, streamlining clinical workflows and improving diagnostic precision.
CONCLUSIONS: The GPT-4o model provides an effective solution for the automatic 
extraction of PE diagnoses from radiology reports, offering a reliable tool that 
aids timely and accurate clinical decision-making. This approach has the 
potential to significantly improve patient outcomes by expediting diagnosis and 
treatment pathways for critical conditions like PE.

©Mohammed Mahyoub, Kacie Dougherty, Ajit Shukla. Originally published in JMIR 
Medical Informatics (https://medinform.jmir.org), 09.04.2025.

DOI: 10.2196/67706
PMCID: PMC12018862
PMID: 40203306 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


376. Front Digit Health. 2025 Jul 16;7:1658635. doi: 10.3389/fdgth.2025.1658635. 
eCollection 2025.

Correction: Diagnostic efficacy of large language models in the pediatric 
emergency department: a pilot study.

Del Monte F(#)(1), Barolo R(#)(2), Circhetta M(3), Delmonaco AG(1), Castagno 
E(1), Pivetta E(4), Bergamasco L(3)(5), Franco M(6), Olmo G(3), Bondone C(1).

Author information:
(1)Department of Pediatric Emergency, Regina Margherita Children's 
Hospital-A.O.U. Città Della Salute e Della Scienza di Torino, Turin, Italy.
(2)Department of Public Health and Pediatrics, Postgraduate School of 
Pediatrics, University of Turin, Turin, Italy.
(3)Department of Control and Computer Engineering, Politecnico di Torino, Turin, 
Italy.
(4)Division of Emergency Medicine and High Dependency Unit, Department of 
Medical Sciences, Città Della Salute e Della Scienza di Torino and University of 
Turin, Turin, Italy.
(5)LINKS Foundation, Turin, Italy.
(6)Department of Clinical and Biological Sciences, University of Turin, Turin, 
Italy.
(#)Contributed equally

Erratum for
    Front Digit Health. 2025 Jul 01;7:1624786. doi: 10.3389/fdgth.2025.1624786.

[This corrects the article DOI: 10.3389/fdgth.2025.1624786.].

© 2025 Del Monte, Barolo, Circhetta, Delmonaco, Castagno, Pivetta, Bergamasco, 
Franco, Olmo and Bondone.

DOI: 10.3389/fdgth.2025.1658635
PMCID: PMC12308998
PMID: 40741323


377. JMIR Med Inform. 2025 Aug 21;13:e75279. doi: 10.2196/75279.

Leveraging Retrieval-Augmented Large Language Models for Dietary Recommendations 
With Traditional Chinese Medicine's Medicine Food Homology: Algorithm 
Development and Validation.

Sha H(#)(1)(2), Gong F(#)(3), Liu B(#)(4), Liu R(1)(2), Wang H(5), Wu T(1)(2).

Author information:
(1)School of Computer Science and Engineering, Southeast University, 2 Southeast 
University Road, Jiangning District, Nanjing, 210096, China, 86 15077889931.
(2)Key Laboratory of New Generation Artificial Intelligence Technology and Its 
Interdisciplinary Applications (Southeast University), Ministry of Education, 
China.
(3)Department of Endocrinology, Shuguang Hospital Affiliated to Shanghai 
University of Traditional Chinese Medicine, Shanghai, China.
(4)Informatization Office, Shanghai University of Traditional Chinese Medicine, 
Shanghai, China.
(5)College of Design and Innovation, Tongji University, Shanghai, China.
(#)Contributed equally

BACKGROUND: Traditional Chinese Medicine (TCM) emphasizes the concept of 
medicine food homology (MFH), which integrates dietary therapy into health care. 
However, the practical application of MFH principles relies heavily on expert 
knowledge and manual interpretation, posing challenges for automating MFH-based 
dietary recommendations. Although large language models (LLMs) have shown 
potential in health care decision support, their performance in specialized 
domains such as TCM is often hindered by hallucinations and a lack of domain 
knowledge. The integration of uncertain knowledge graphs (UKGs) with LLMs via 
retrieval-augmented generation (RAG) offers a promising solution to overcome 
these limitations by enabling a structured and faithful representation of MFH 
principles while enhancing LLMs' ability to understand the inherent uncertainty 
and heterogeneity of TCM knowledge. Consequently, it holds potential to improve 
the reliability and accuracy of MFH-based dietary recommendations generated by 
LLMs.
OBJECTIVE: This study aimed to introduce Yaoshi-RAG, a framework that leverages 
UKGs to enhance LLMs' capabilities in generating accurate and personalized 
MFH-based dietary recommendations.
METHODS: The proposed framework began by constructing a comprehensive MFH 
knowledge graph (KG) through LLM-driven open information extraction, which 
extracted structured knowledge from multiple sources. To address the 
incompleteness and uncertainty within the MFH KG, UKG reasoning was used to 
measure the confidence of existing triples and to complete missing triples. When 
processing user queries, query entities were identified and linked to the MFH 
KG, enabling retrieval of relevant reasoning paths. These reasoning paths were 
then ranked based on triple confidence scores and entity importance. Finally, 
the most informative reasoning paths were encoded into prompts using prompt 
engineering, enabling the LLM to generate personalized dietary recommendations 
that aligned with both individual health needs and MFH principles. The 
effectiveness of Yaoshi-RAG was evaluated through both automated metrics and 
human evaluation.
RESULTS: The constructed MFH KG comprised 24,984 entities, 22 relations, and 
29,292 triples. Extensive experiments demonstrate the superiority of Yaoshi-RAG 
in different evaluation metrics. Integrating the MFH KG significantly improved 
the performance of LLMs, yielding an average increase of 14.5% in Hits@1 and 
8.7% in F1-score, respectively. Among the evaluated LLMs, DeepSeek-R1 achieved 
the best performance, with 84.2% in Hits@1 and 71.5% in F1-score, respectively. 
Human evaluation further validated these results, confirming that Yaoshi-RAG 
consistently outperformed baseline models across all assessed quality 
dimensions.
CONCLUSIONS: This study shows Yaoshi-RAG, a new framework that enhances LLMs' 
capabilities in generating MFH-based dietary recommendations through the 
knowledge retrieved from a UKG. By constructing a comprehensive TCM knowledge 
representation, our framework effectively extracts and uses MFH principles. 
Experimental results demonstrate the effectiveness of our framework in 
synthesizing traditional wisdom with advanced language models, facilitating 
personalized dietary recommendations that address individual health conditions 
while providing evidence-based explanations.

© Hangyu Sha, Fan Gong, Bo Liu, Runfeng Liu, Haofen Wang, Tianxing Wu. 
Originally published in JMIR Medical Informatics (https://medinform.jmir.org).

DOI: 10.2196/75279
PMCID: PMC12370266
PMID: 40840437 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


378. Phys Ther. 2025 Apr 2;105(4):pzaf010. doi: 10.1093/ptj/pzaf010.

Evaluating the Potential of Large Language Models for Vestibular Rehabilitation 
Education: A Comparison of ChatGPT, Google Gemini, and Clinicians.

Arbel Y(1), Gimmon Y(2)(3), Shmueli L(1).

Author information:
(1)Department of Management, Bar-Ilan University, Ramat-Gan 52900, Israel.
(2)Department of Physical Therapy, Faculty of Social Welfare & Health Studies, 
University of Haifa, Haifa, Israel.
(3)Department of Otolaryngology-Head and Neck Surgery, Sheba Medical Center, 
Tel-Hashomer, Israel.

OBJECTIVE: This study aimed to compare the performance of 2 large language 
models, ChatGPT (Generative Pre-trained Transformer) and Google Gemini, against 
experienced physical therapists and students in responding to multiple-choice 
questions related to vestibular rehabilitation. The study further aimed to 
assess the accuracy of ChatGPT's responses by board-certified otoneurologists.
METHODS: This study was conducted among 30 physical therapist professionals 
experienced with vestibular rehabilitation and 30 physical therapist students. 
They were asked to complete a vestibular knowledge test (VKT) consisting of 20 
multiple-choice questions that were divided into 3 categories: (1) Clinical 
Knowledge, (2) Basic Clinical Practice, and (3) Clinical Reasoning. ChatGPT and 
Google Gemini were tasked with answering the same 20 VKT questions. Three 
board-certified otoneurologists independently evaluated the accuracy of each 
response using a 4-level scale, ranging from comprehensive to completely 
incorrect.
RESULTS: ChatGPT outperformed Google Gemini with a 70% score on the VKT test, 
while Gemini scored 60%. Both excelled in Clinical Knowledge scoring 100% but 
struggled in Clinical Reasoning with ChatGPT scoring 50% and Gemini scoring 25%. 
According to 3 otoneurologic experts, ChatGPT's accuracy was considered 
"comprehensive" in 45% of the 20 questions, while 25% were found to be 
completely incorrect. ChatGPT provided "comprehensive" responses in 50% of 
Clinical Knowledge and Basic Clinical Practice questions, but only 25% in 
Clinical Reasoning.
CONCLUSION: Caution is advised when using ChatGPT and Google Gemini due to their 
limited accuracy in clinical reasoning. While they provide accurate responses 
concerning Clinical Knowledge, their reliance on web information may lead to 
inconsistencies. ChatGPT performed better than Gemini. Health care professionals 
should carefully formulate questions and be aware of the potential influence of 
the online prevalence of information on ChatGPT's and Google Gemini's responses. 
Combining clinical expertise and clinical guidelines with ChatGPT and Google 
Gemini can maximize benefits while mitigating limitations. The results are based 
on current models of ChatGPT3.5 and Google Gemini. Future iterations of these 
models are expected to offer improved accuracy as the underlying modeling and 
algorithms are further refined.
IMPACT: This study highlights the potential utility of large language models 
like ChatGPT in supplementing clinical knowledge for physical therapists, while 
underscoring the need for caution in domains requiring complex clinical 
reasoning. The findings emphasize the importance of integrating technological 
tools carefully with human expertise to enhance patient care and rehabilitation 
outcomes.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Physical Therapy Association.

DOI: 10.1093/ptj/pzaf010
PMCID: PMC11994992
PMID: 39932784 [Indexed for MEDLINE]


379. J Med Internet Res. 2025 Jan 30;27:e66896. doi: 10.2196/66896.

Assessing the Adherence of ChatGPT Chatbots to Public Health Guidelines for 
Smoking Cessation: Content Analysis.

Abroms LC(1), Yousefi A(2), Wysota CN(3), Wu TC(1), Broniatowski DA(2).

Author information:
(1)Department of Prevention & Community Health, Milken Institute School of 
Public Health, George Washington University, Washington, DC, United States.
(2)Department of Engineering Management and Systems Engineering, George 
Washington University, Washington, DC, United States.
(3)Department of Population Health, Grossman School of Medicine, New York 
University, New York, NY, United States.

BACKGROUND: Large language model (LLM) artificial intelligence chatbots using 
generative language can offer smoking cessation information and advice. However, 
little is known about the reliability of the information provided to users.
OBJECTIVE: This study aims to examine whether 3 ChatGPT chatbots-the World 
Health Organization's Sarah, BeFreeGPT, and BasicGPT-provide reliable 
information on how to quit smoking.
METHODS: A list of quit smoking queries was generated from frequent quit smoking 
searches on Google related to "how to quit smoking" (n=12). Each query was given 
to each chatbot, and responses were analyzed for their adherence to an index 
developed from the US Preventive Services Task Force public health guidelines 
for quitting smoking and counseling principles. Responses were independently 
coded by 2 reviewers, and differences were resolved by a third coder.
RESULTS: Across chatbots and queries, on average, chatbot responses were rated 
as being adherent to 57.1% of the items on the adherence index. Sarah's 
adherence (72.2%) was significantly higher than BeFreeGPT (50%) and BasicGPT 
(47.8%; P<.001). The majority of chatbot responses had clear language (97.3%) 
and included a recommendation to seek out professional counseling (80.3%). About 
half of the responses included the recommendation to consider using nicotine 
replacement therapy (52.7%), the recommendation to seek out social support from 
friends and family (55.6%), and information on how to deal with cravings when 
quitting smoking (44.4%). The least common was information about considering the 
use of non-nicotine replacement therapy prescription drugs (14.1%). Finally, 
some types of misinformation were present in 22% of responses. Specific queries 
that were most challenging for the chatbots included queries on "how to quit 
smoking cold turkey," "...with vapes," "...with gummies," "...with a necklace," 
and "...with hypnosis." All chatbots showed resilience to adversarial attacks 
that were intended to derail the conversation.
CONCLUSIONS: LLM chatbots varied in their adherence to quit-smoking guidelines 
and counseling principles. While chatbots reliably provided some types of 
information, they omitted other types, as well as occasionally provided 
misinformation, especially for queries about less evidence-based methods of 
quitting. LLM chatbot instructions can be revised to compensate for these 
weaknesses.

©Lorien C Abroms, Artin Yousefi, Christina N Wysota, Tien-Chin Wu, David A 
Broniatowski. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 30.01.2025.

DOI: 10.2196/66896
PMCID: PMC11826940
PMID: 39883917 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


380. J Med Syst. 2025 May 17;49(1):65. doi: 10.1007/s10916-025-02197-w.

Assessment and Integration of Large Language Models for Automated Electronic 
Health Record Documentation in Emergency Medical Services.

Bai E(1), Luo X(2)(3), Zhang Z(4), Adelgais K(5), Ali H(6), Finkelstein J(7), 
Kutzin J(8).

Author information:
(1)School of Computer Science and Information Systems, Pace University, New York 
City, NY, USA.
(2)Department of Management Science and Information Systems, Oklahoma State 
University, Stillwater, OK, USA. xiao.luo@okstate.edu.
(3)School of Medicine, Indiana University, Indianapolis, IN, USA. 
xiao.luo@okstate.edu.
(4)School of Computer Science and Information Systems, Pace University, New York 
City, NY, USA. zzhang@pace.edu.
(5)School of Medicine, University of Colorado, Aurora, CO, USA.
(6)Maimonides Medical Center, New York City, NY, USA.
(7)Interfaith Medical Center, New York City, NY, USA.
(8)Mount Sinai Hospital, New York City, NY, USA.

Automating Electronic Health Records (EHR) documentation can significantly 
reduce the burden on care providers, particularly in emergency care settings 
where rapid and accurate record-keeping is crucial. A critical aspect of this 
automation involves using natural language processing (NLP) techniques to 
convert transcribed conversations into structured EHR fields. For instance, 
extracting temperature values like "102.4 Fahrenheit" from the transcribed text 
"His temperature is 39.1, which is 102.4 Fahrenheit." However, traditional 
rule-based and single-model NLP approaches often struggle with domain-specific 
medical terminology, contextual ambiguity, and numerical extraction errors. This 
study investigates the potential of integrating multiple Large Language Models 
(LLMs) to enhance EMS documentation accuracy. We developed an LLM integration 
framework and evaluated four state-of-the-art LLMs-Claude 3.5, GPT-4, Gemini, 
and Mistral-on a dataset comprising transcribed conversations from 40 EMS 
training simulations. The evaluation focused on precision, recall, and F1 score 
across zero-shot and few-shot learning scenarios. Results showed that the 
integrated LLM framework outperformed individual models, achieving overall F1 
scores of 0.78 (zero-shot) and 0.81 (few-shot). In addition to quantitative 
evaluation, a preliminary user study was conducted with domain experts to assess 
the perceived usefulness and challenges of the integrated framework. The 
findings suggest that this approach has the potential to reduce documentation 
effort compared to traditional manual documentation. However, challenges such as 
misinterpretation of medical context and occasional omissions were noted, 
highlighting areas for further refinement and future work. This research is the 
first to systematically explore and evaluate the use of LLMs for real-time EMS 
EHR documentation. By addressing key challenges in automated transcription and 
structured data extraction, our work lays a foundation for real-world 
implementation, improving efficiency and accuracy in emergency medical 
documentation.

Publisher: Not applicable.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02197-w
PMID: 40381087 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Human Ethics and Consent to 
Participate: This study has been approved by the Pace University Institutional 
Review Board (IRB# 1515261-2). All participants in the simulations agreed that 
the data could be used for research purposes, provided their identities remain 
anonymous. Competing Interests: The authors declare no competing interests.


381. J Med Syst. 2025 Nov 10;49(1):157. doi: 10.1007/s10916-025-02289-7.

Estimating LVEF from ECG with GPT-4o Fine-Tuned Vision: A Novel Approach in 
AI-Driven Cardiac Diagnostics.

Engelstein H(#)(1), Ramon-Gonen R(#)(2)(3), Barbash I(4)(5), Beinart R(6)(4), 
Cohen-Shelly M(#)(7), Sabbag A(#)(6)(4).

Author information:
(1)Sheba Medical Center, Tel Hashomer, Ramat Gan, Israel.
(2)The School of Business Administration, Bar-Ilan University, Ramat-Gan, 
Israel. roni.ramon-gonen@biu.ac.il.
(3)Data Science Institute, Bar-Ilan University, Ramat-Gan, Israel. 
roni.ramon-gonen@biu.ac.il.
(4)Faculty of Medicine, Tel Aviv University, Tel Aviv, Israel.
(5)Interventional Cardiology Unit, Leviev Heart Center, Sheba Medical Center, 
Tel Hashomer, Ramat Gan, Israel.
(6)Davidai Arrhythmia Center, Sheba Medical Center, Tel Hashomer, Ramat Gan, 
Israel.
(7)Sheba ARC, Sagol Big data & AI Hub, Sheba Medical Center, Tel Hashomer, Ramat 
Gan, Israel.
(#)Contributed equally

BACKGROUND: Assessing Left Ventricular Ejection Fraction (LVEF) is crucial for 
diagnosing reduced systolic function, yet echocardiography (ECHO) may not always 
be readily available, potentially delaying treatment. Electrocardiography (ECG) 
offers a cost-effective and accessible alternative for estimating LVEF. However, 
specialized AI models for this purpose are often complex and costly to develop.
OBJECTIVE: This study uniquely evaluates GPT-4o Fine-Tuned Vision (GPT-4o-FTV), 
a general-purpose AI model, for detecting LVEF ≤ 35% from ECG images, comparing 
its performance with a Convolutional Neural Network (CNN) model and clinician 
assessments.
METHODS: We analyzed ECGs from 202 patients (42.6% women, mean age 64.5 ± 16.3 
years) at a tertiary center, excluding those with pacemakers and including only 
high-quality ECGs. LVEF ≤ 35% was present in 11.9% (n = 24). GPT-4o-FTV, trained 
on 20 labeled ECGs, was tested using a structured prompt across four runs. 
Accuracy, sensitivity, specificity, and positive predictive value (PPV) were 
compared to a CNN model and four clinicians.
RESULTS: GPT-4o-FTV achieved 79.9% accuracy, 72.9% sensitivity, 80.8% 
specificity, an F1-score of 46.4%, and a PPV of 34%, outperforming clinicians 
(74.9% accuracy, 65.6% sensitivity, 76.1% specificity, 39% F1-score, PPV 27.9%). 
The CNN model had the highest performance (89.1% accuracy, 79.2% sensitivity, 
90.4% specificity, 63.3% F1-score, PPV 52.8%).
CONCLUSIONS: GPT-4o-FTV demonstrates strong potential as an accessible tool for 
cardiac diagnostics, particularly in resource-limited settings. While CNN models 
remain superior in accuracy, the ease of fine-tuning GPT-4o-FTV highlights its 
practical utility. Future research should focus on larger datasets, additional 
optimization, and exploring its ability to detect early predictors of LVEF 
decline.

© 2025. The Author(s).

DOI: 10.1007/s10916-025-02289-7
PMCID: PMC12602629
PMID: 41212334 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval: Institutional 
Ethics Committee Approval was granted as per institutional standard procedure. 
The study was conducted in compliance with good clinical practice, the 
principles of the of Helsinki, and applicable local regulations. Data were 
anonymized before analysis. Clinical trial number: not applicable. Competing 
interests: The authors declare no competing interests.


382. Rheumatol Int. 2025 Sep 27;45(10):240. doi: 10.1007/s00296-025-05996-z.

Referential hallucination and clinical reliability in large language models: a 
comparative analysis using regenerative medicine guidelines for chronic pain.

Kuculmez O(1), Usen A(2), Ahi ED(3).

Author information:
(1)Department of Physical Medicine and Rehabilitation, Baskent University Alanya 
Hospital, Oba neighbourhood, Cevreyolu Street. No:79, Alanya, Antalya, 07400, 
Turkey.
(2)Faculty of Medicine, Department of Physical Medicine and Rehabilitation, 
Medipol University, Göztepe neighbourhood, Atatürk Street. No:1, Beykoz, 
Istanbul, 34815, Turkey. ahmetusen1@hotmail.com.
(3)Department of Physical Medicine and Rehabilitation, Kocaeli Health and 
Technology University, Fatih Neighborhood, Kadıoğlu Street, Alizepark Complex, 
Block D1, No:1, Başiskele, Kocaeli, Turkey.

This study compared language models' responses to open-ended questions on 
regenerative therapy guidelines for chronic pain, assessing their accuracy, 
reliability, usefulness, readability, semantic similarity, and hallucination 
rates. This cross-sectional study used 16 open-ended questions based on the 
American Society of Pain and Neuroscience's regenerative therapy guidelines for 
chronic pain. Questions were answered by ChatGPT-4o, Gemini 2.5 Flash, and 
Claude 4 Opus. Responses were rated on a 7-point Likert scale for usability and 
reliability, and a 5-point scale for accuracy. Hallucinogenicity, readability 
(FKRE, FKGL), and similarity (USE, ROUGE-L) were also assessed. Statistical 
comparisons were made, with significance set at p < 0.05. Claude Opus 4 showed 
the highest reliability (5.19 ± 1.11), usefulness (5.06 ± 1.0), and clinical 
accuracy (4.06 ± 0.68), outperforming ChatGPT-4o (4.13 ± 0.96; 3.94 ± 0.85; 
3.38 ± 0.72) and Gemini 2.5 (4.19 ± 0.98; 4.06 ± 0.93; 3.38 ± 0.62). Claude had 
the lowest reference hallucinations (RHS 4.44 ± 3.18) vs. ChatGPT-4o 
(8.38 ± 1.86) and Gemini 2.5 (8.75 ± 1.73). In semantic similarity, Claude 
(0.68 ± 0.08) and Gemini (0.65 ± 0.07) surpassed ChatGPT-4o (0.60 ± 0.09). 
Gemini led in ROUGE-L F1 (0.12 ± 0.03) vs. Claude (0.10 ± 0.02) and ChatGPT-4o 
(0.07 ± 0.03). Readability was similar, though Gemini had a higher FKGL 
(11.3 ± 1.06) than Claude (10.3 ± 2.09). Claude Opus 4 showed superior accuracy, 
reliability, and usefulness, with significantly fewer hallucinations. 
Readability scores were similar across models. Further research is recommended.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00296-025-05996-z
PMID: 41014370 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare that they have no conflict of interest. Ethical approval: The study 
approved by Medipol University Non-Interventional Clinical Research Ethics 
Committee (E-10840098-202.3.02-5585, dated 22/08/2025). Consent for publication: 
All of the authors have contributed significantly to this manuscript. 
Additionally, all authors have seen and agree with the content of this 
manuscript, disclose no financial interests, and take full responsibility for 
the data and give consent for publication.


383. J Oral Maxillofac Surg. 2025 Dec;83(12):1461-1469. doi: 
10.1016/j.joms.2025.08.012. Epub 2025 Sep 3.

Can Chatbots Provide Accurate and Readable Information for Patients With 
Temporomandibular Disorders?

Pagotto LEC(1), Almeida DRMF(2), de Santana Santos T(3), de Morais EF(4).

Author information:
(1)Oral and Maxillofacial Surgeon, Clinical Staff of the Sirio-Libanês Hospital. 
Hospital Sírio-Libanes, São Paulo, Brazil. Electronic address: 
luispagotto@gmail.com.
(2)Professor, School of Dentistry, Federal University of Alfenas (Unifal-MG), 
Alfenas, Minas Gerais, Brazil.
(3)Oral and Maxillofacial Surgeon, Institute Maxillofacial Education, Aracaju, 
Sergipe, Brazil.
(4)Researcher, Graduate Program in Oral Biology, Piracicaba Dental School, 
University of Campinas, Piracicaba, São Paulo, Brazil; Researcher, Department of 
Oral Diagnosis, Piracicaba Dental School, University of Campinas, Piracicaba, 
São Paulo, Brazil.

BACKGROUND: Temporomandibular disorders (TMDs) are common musculoskeletal and 
neuromuscular conditions that impair jaw function and quality of life. Patients 
often lack access to reliable health information. Large language models (LLMs) 
have introduced chatbots as potential educational tools, yet concerns remain 
regarding accuracy, readability, empathy, and citation integrity.
PURPOSE: This study evaluated whether LLM-based chatbots can provide clinically 
accurate, empathic, and readable responses to patient-friendly questions about 
TMDs and whether their cited references are authentic.
STUDY DESIGN, SETTING, SAMPLE: This cross-sectional in silico study was 
conducted in March 2025. Twenty-three standardized TMD-related questions were 
used as prompts for each chatbot.
PREDICTOR/EXPOSURE/INDEPENDENT VARIABLE: The predictor variable was the chatbot 
platform, reflecting distinct LLM architectures: GPT-4 (transformer-based 
autoregressive model, OpenAI), Gemini Pro (multimodal transformer, Google), and 
DeepSeek-V3 (mixture-of-experts transformer, DeepSeek).
MAIN OUTCOME VARIABLES: Accuracy was defined as the proportion of responses 
judged clinically correct by two board-certified oral medicine specialists. 
Empathy was assessed by expert scoring of tone. Readability was determined with 
Flesch-Kincaid Reading Ease and Grade Level. Citation reliability was assessed 
by verifying whether references were authentic and retrievable in PubMed or 
other authoritative databases.
COVARIATES: No formal covariates were included; exploratory correlations between 
variables were performed.
ANALYSES: Descriptive statistics, 1-way analysis of variance with Tukey's post 
hoc tests, Pearson correlation, and χ2 tests were performed. Statistical 
significance was set at P < .05.
RESULTS: No statistically significant differences were observed in accuracy (P = 
.2) or empathy (P = .2). The mixture-of-experts transformer provided the most 
readable content (Flesch-Kincaid Reading Ease = 28.47; Flesch-Kincaid Grade 
Level = 12.19; P < .001). The transformer-based autoregressive model produced 
the highest proportion of hallucinated references (47.2%), compared with the 
multimodal transformer (18.8%) and the mixture-of-experts transformer (10.1%) (P 
< .001). A weak positive correlation was found between accuracy and readability 
(r = 0.27; P = .03), with no correlation between accuracy and empathy.
CONCLUSIONS AND RELEVANCE: While all LLM-based chatbots delivered generally 
accurate and empathetic responses, the mixture-of-experts transformer 
outperformed others in readability and citation reliability. The high rate of 
hallucinated references in the transformer-based autoregressive model 
underscores the need for human oversight in clinical applications.

Copyright © 2025 American Association of Oral and Maxillofacial Surgeons. 
Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.joms.2025.08.012
PMID: 40973022 [Indexed for MEDLINE]


384. Int J Med Inform. 2026 Jan;205:106091. doi: 10.1016/j.ijmedinf.2025.106091. Epub 
2025 Aug 28.

Performance and improvement strategies for adapting generative large language 
models for electronic health record applications: A systematic review.

Du X(1), Zhou Z(2), Wang Y(2), Chuang YW(3), Li Y(4), Yang R(4), Hong P(2), 
Bates DW(5), Zhou L(4).

Author information:
(1)Division of General Internal Medicine and Primary Care, Brigham and Women's 
Hospital, Boston, MA 02115, United States; Department of Medicine, Harvard 
Medical School, Boston, MA 02115, United States. Electronic address: 
xidu1@bwh.harvard.edu.
(2)Department of Computer Science, Brandeis University, Waltham, MA 02453, 
United States.
(3)Division of Nephrology, Department of Internal Medicine, Taichung Veterans 
General Hospital, Taichung 407219, Taiwan; Department of Post-Baccalaureate 
Medicine, College of Medicine, National Chung Hsing University, Taichung 402202, 
Taiwan; School of Medicine, College of Medicine, China Medical University, 
Taichung 404328, Taiwan.
(4)Division of General Internal Medicine and Primary Care, Brigham and Women's 
Hospital, Boston, MA 02115, United States; Department of Medicine, Harvard 
Medical School, Boston, MA 02115, United States.
(5)Division of General Internal Medicine and Primary Care, Brigham and Women's 
Hospital, Boston, MA 02115, United States; Department of Medicine, Harvard 
Medical School, Boston, MA 02115, United States; Department of Health Policy and 
Management, Harvard T.H. Chan School of Public Health, Boston, MA 02115, United 
States.

Update of
    medRxiv. 2025 Jun 22:2024.08.11.24311828. doi: 10.1101/2024.08.11.24311828.

PURPOSE: To synthesize performance and improvement strategies for adapting 
generative LLMs in EHR analyses and applications.
METHODS: We followed the PRISMA guidelines to conduct a systematic review of 
articles from PubMed and Web of Science published between January 1, 2023 and 
November 9, 2024. Multiple reviewers including biomedical informaticians and a 
clinician involved in the article reviewing process. Studies were included if 
they used generative LLMs to analyze real-world EHR data and reported 
quantitative performance evaluations for an improvement technique. The review 
identified key clinical applications, summarized performance and the improvement 
strategies.
RESULTS: Of the 18,735 articles retrieved, 196 met our criteria. 112 (57.1%) 
studies used generative LLMs for clinical decision support tasks, 40 (20.4%) 
studies involved documentation tasks, 39 (19.9%) studies involved information 
extraction tasks, 11 (5.6%) studies involved patient communication tasks, and 10 
(5.1%) studies included summarization tasks. Among the 196 studies, most studies 
(88.8%) did not quantitatively evaluate the LLM performance improvement 
strategies, with the rest twenty-four studies (12.2%) quantitatively evaluated 
the effectiveness of in-context learning (9 studies), fine-tuning (12 studies), 
multimodal integration (8 studies), and ensemble learning (2 studies). Three 
studies highlighted that few-shot prompting, fine-tuning, and multimodal data 
integration might not improve performance, and another two studies found that 
fine-tuning a smaller model could outperform a large model.
CONCLUSION: Applying a performance improvement strategy may not necessarily lead 
to performance improvement, and detailed guidelines regarding how to apply those 
strategies more effectively and safely are needed, which can be completed from 
more quantitative analysis in the future.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106091
PMCID: PMC12413914
PMID: 40885071 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


385. JMIR Form Res. 2025 Sep 24;9:e75421. doi: 10.2196/75421.

A Behavioral Science-Informed Agentic Workflow for Personalized Nutrition 
Coaching: Development and Validation Study.

Yang E(1), Garcia T(1), Williams HG(1), Kumar B(1), Ramé M(1), Rivera E(1), Ma 
Y(1), Amar J(1), Catalani C(1), Jia Y(1).

Author information:
(1)Verily Life Sciences, 2999 Olympus Blvd, Ste 1000, Dallas, TX, 75019, United 
States, 1 650-495-7100.

BACKGROUND: Effective management of cardiometabolic conditions requires 
sustained positive nutrition habits, often hindered by complex and 
individualized barriers. Direct human management is simply not scalable, and 
deterministic automated approaches to nutrition coaching may lack the 
personalization needed to address these diverse challenges.
OBJECTIVE: We report the development and validation of a novel large language 
model (LLM)-powered agentic workflow designed to provide personalized nutrition 
coaching by directly identifying and mitigating patient-specific barriers.
METHODS: We used behavioral science principles to create a comprehensive 
workflow that can map nutrition-related barriers to corresponding evidence-based 
strategies. First, a specialized LLM agent to intentionally probe for and 
identify root causes of a patient's dietary struggles. Subsequently, a separate 
LLM agent to deliver tailored tactics that were designed to overcome those 
specific barriers. We conducted a user study with individuals with 
cardiometabolic conditions (N=16) to inform our workflow design and then 
validated our approach through an additional user study (n=6). We also conducted 
a large-scale simulation study, grounding on real patient vignettes and 
expert-validated metrics, where human experts evaluated the system's performance 
across multiple scenarios and domains.
RESULTS: In our user study, the system accurately identified barriers and 
provided personalized guidance. Five out of 6 participants agreed that the LLM 
agent helped them recognize obstacles preventing them from being healthier, and 
all participants strongly agreed that the advice felt personalized to their 
situation. In our simulation study, experts agreed that the LLM agent accurately 
identified primary barriers in more than 90% of cases (27 or 28/30). 
Additionally, experts determined that the workflow delivered personalized and 
actionable tactics empathetically, with average ratings of 4.17-4.79 on a 
5-point Likert scale.
CONCLUSIONS: Our findings demonstrate the potential of this LLM-powered agentic 
workflow to improve nutrition coaching by providing personalized, scalable, and 
behaviorally informed interventions.

© Eric Yang, Tomas Garcia, Hannah G Williams, Bhawesh Kumar, Martin Ramé, Eileen 
Rivera, Yiran Ma, Jonathan Amar, Caricia Catalani, Yugang Jia. Originally 
published in JMIR Formative Research (https://formative.jmir.org).

DOI: 10.2196/75421
PMCID: PMC12460921
PMID: 40994075 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: All authors report 
employment and equity ownership in Verily Life Sciences.


386. J Med Internet Res. 2025 Mar 31;27:e65903. doi: 10.2196/65903.

Exploring the Capacity of Large Language Models to Assess the Chronic Pain 
Experience: Algorithm Development and Validation.

Amidei J(1), Nieto R(2), Kaltenbrunner A(1), Ferreira De Sá JG(1), Serrat 
M(3)(4), Albajes K(5).

Author information:
(1)AI and Data for Society Research Group, Internet Interdisciplinary Institute, 
Universitat Oberta de Catalunya, Barcelona, Spain.
(2)eHealth Lab Research Group, Faculty of Psychology and Educational Sciences, 
Universitat Oberta de Catalunya, Barcelona, Spain.
(3)Unitat d'Expertesa en Síndromes de Sensibilització Central, Servei de 
Reumatologia, Vall d'Hebron Hospital Universitari, Barcelona, Spain.
(4)Escola Universitària de Fisioteràpia, Escoles Universitàries Gimbernat, 
Barcelona, Spain.
(5)Psyclinic Mental Health, Barcelona, Spain.

BACKGROUND: Chronic pain, affecting more than 20% of the global population, has 
an enormous pernicious impact on individuals as well as economic ramifications 
at both the health and social levels. Accordingly, tools that enhance pain 
assessment can considerably impact people suffering from pain and society at 
large. In this context, assessment methods based on individuals' personal 
experiences, such as written narratives (WNs), offer relevant insights into 
understanding pain from a personal perspective. This approach can uncover 
subjective, intricate, and multifaceted aspects that standardized questionnaires 
can overlook. However, WNs can be time-consuming for clinicians. Therefore, a 
tool that uses WNs while reducing the time required for their evaluation could 
have a significantly beneficial impact on people's pain assessment.
OBJECTIVE: This study is the first evaluation of the potential of applying large 
language models (LLMs) to assist clinicians in assessing patients' pain 
expressed through WNs.
METHODS: We performed an experiment based on 43 WNs made by people with 
fibromyalgia and qualitatively evaluated in a prior study. Focusing on pain 
severity and disability, we prompt GPT-4 (with temperature parameter settings 0 
or 1) to assign scores and scores' explanations, to these WNs. Then, we 
quantitatively compare GPT-4 scores with experts' scores of the same narratives, 
using statistical measures such as Pearson correlations, root mean squared 
error, the weighted version of the Gwet agreement coefficient, and Krippendorff 
α. Additionally, 2 experts specialized in chronic pain conducted a qualitative 
analysis of the scores' explanation to assess their accuracy and potential 
applicability of GPT's analysis for future pain narrative evaluations.
RESULTS: Our analysis reveals that GPT-4's performance in assessing pain 
narratives yielded promising results. GPT-4 was comparable in terms of agreement 
with experts (with a weighted percentage agreement higher than 0.95), 
correlations with standardized measurements (for example in the range of 0.43 
and 0.49 between the Revised Fibromyalgia Impact Questionnaire and GTP-4 with 
temperatures 1), and low error rates (root mean squared error of 1.20 for 
severity and 1.44 for disability). Moreover, experts generally deemed the 
ratings provided by GPT-4, as well as the scores' explanation, to be adequate. 
However, we observe that GPT has a slight tendency to overestimate pain severity 
and disability with a lower SD than expert estimates.
CONCLUSIONS: These findings underline the potential of LLMs in facilitating the 
assessment of WNs of people with fibromyalgia, offering a novel approach to 
understanding and evaluating patient pain experiences. Integrating automated 
assessments through LLMs presents opportunities for streamlining and enhancing 
the assessment process, paving the way for improved patient care and tailored 
interventions in the chronic pain management field.

©Jacopo Amidei, Rubén Nieto, Andreas Kaltenbrunner, Jose Gregorio Ferreira De 
Sá, Mayte Serrat, Klara Albajes. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org), 31.03.2025.

DOI: 10.2196/65903
PMCID: PMC11997525
PMID: 40163858 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


387. PLoS One. 2025 Sep 12;20(9):e0331962. doi: 10.1371/journal.pone.0331962. 
eCollection 2025.

New frontiers in radiologic interpretation: evaluating the effectiveness of 
large language models in pneumothorax diagnosis.

Bulut B(1), Öz MA(1), Genç M(2), Gür A(3), Yortanlı M(4), Yortanlı BÇ(5), 
Sariyildiz O(6), Yazıcı R(7), Mutlu H(8), Kotanoglu MS(9), Cinar E(10), Uykan 
Z(11).

Author information:
(1)Department of Emergency Medicine, Ankara Gulhane Training and Research 
Hospital, Health Science University, Ankara, Turkey.
(2)Department of Emergency Medicine, Ankara Training and Research Hospital, 
Ankara, Turkey.
(3)Department of Emergency Medicine, Etimesgut Şehit Sait Ertürk State Hospital, 
Ankara, Turkey.
(4)Department Of Emergency Medicine, Konya Numune Hospital, Konya, Turkey.
(5)Department of Internal Medicine, Konya City Hospital, University of Health 
Sciences, Konya, Turkey.
(6)Department of Anesthesıology And Reanımatıon Department, 75th Year Oral And 
Dental Health Hospital, Ankara, Turkey.
(7)Department of Emergency Medicine, Istanbul Kanuni Sultan Suleyman Training 
and Research Hospital, Health Science University, Istanbul, Turkey.
(8)Department of Emergency Medicine, Aksaray Training and Research Hospital, 
Aksaray University, Aksaray, Turkey.
(9)Department of Anesthesıology And Reanımatıon Department, Ankara Training and 
Research Hospital, Ankara, Turkey.
(10)Department of Thoracic Surgery, Bilkent City Hospital, University of Health 
Sciences, Ankara, Turkey.
(11)College of Engineering and Technology, American University of the Middle 
East, Kuwait.

BACKGROUND: This study evaluates the diagnostic performance of three multimodal 
large language models (LLMs)-ChatGPT-4o, Gemini 2.0, and Claude 3.5-in 
identifying pneumothorax from chest radiographs.
METHODS: In this retrospective analysis, 172 pneumothorax cases (148 patients 
aged >12 years, 24 patients aged ≤12 years) with both chest radiographs and 
confirmatory thoracic CT were included from a tertiary emergency department. 
Patients were categorized by age and pneumothorax size (small/large). Each 
radiograph was presented to all three LLMs accompanied by basic symptoms 
(dyspnea or chest pain), with each model analyzing each image three times. 
Diagnostic accuracy was evaluated using overall accuracy (all three responses 
correct), strict accuracy (≥2 responses correct), and ideal accuracy (≥1 
response correct), alongside response consistency assessment using Fleiss' 
Kappa.
RESULTS: In patients older than 12 years, ChatGPT-4o demonstrated the highest 
overall accuracy (69.6%), followed by Claude 3.5 (64.9%) and Gemini 2.0 (57.4%). 
Performance was significantly poorer in pediatric patients across all models 
(20.8%, 12.5%, and 20.8%, respectively). For large pneumothorax in adults, 
ChatGPT-4o showed significantly higher accuracy compared to small pneumothorax 
(81.6% vs. 42.2%; p < 0.001). Regarding consistency, Gemini 2.0 demonstrated 
excellent reliability for large pneumothorax (Kappa = 1.00), while Claude 3.5 
showed moderate consistency across both pneumothorax sizes.
CONCLUSION: This study, the first to evaluate these three current multimodal 
LLMs in pneumothorax identification across different age groups, demonstrates 
promising results for potential clinical applications, particularly for adult 
patients with large pneumothorax. However, performance limitations in pediatric 
cases and with small pneumothoraces highlight the need for further validation 
before clinical implementation.

Copyright: © 2025 Bulut et al. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0331962
PMCID: PMC12431401
PMID: 40938938 [Indexed for MEDLINE]

Conflict of interest statement: No authors have competing interests.


388. J Med Syst. 2025 Dec 8;49(1):179. doi: 10.1007/s10916-025-02314-9.

High Concordance Between GPT-4o and Multidisciplinary Tumor Board Decisions in 
Breast Cancer: A Retrospective Decision Support Analysis.

Büyükceran EU(1), Seyfettin A(2), Babatürk A(2), Özkan MB(3), Çolak D(4), Ünal 
İ(5), Kaymaz E(6), Ergün E(6), Emer MÖ(7), Mersin HH(3).

Author information:
(1)Department of Radiology, Breast Center, Ankara Güven Hospital, Ankara, 
Türkiye. utkan.buyukceran91@gmail.com.
(2)Department of Radiology, Breast Center, Ankara Güven Hospital, Ankara, 
Türkiye.
(3)Department of Breast and Endocrine Surgery, Breast Center, Ankara Güven 
Hospital, Ankara, Türkiye.
(4)Department of Medical Oncology, Breast Center, Ankara Güven Hospital, Ankara, 
Türkiye. dilsencolak@yahoo.com.
(5)Department of Radiation Oncology, Breast Center, Ankara Güven Hospital, 
Ankara, Türkiye. drilhami@yahoo.com.
(6)Department of Pathology, Breast Center, Ankara Güven Hospital, Ankara, 
Türkiye.
(7)Department of Nuclear Medicine, Breast Center, Ankara Güven Hospital, Ankara, 
Türkiye.

Large language models (LLMs) such as ChatGPT have gained attention for their 
potential to assist clinical decision-making in oncology. However, real-world 
validation of these models against multidisciplinary tumor board (MTB) 
recommendations-particularly in breast cancer treatment-remains limited. This 
retrospective study assessed the concordance between GPT-4o and the decisions of 
a breast cancer MTB over a six-month period. Thirty-three patients were 
included. Structured clinical data were entered into GPT-4o using standardized 
prompts, and treatment plans were generated in two independent sessions per 
case. Seven therapeutic domains were evaluated: surgery, radiotherapy, hormonal 
therapy, neoadjuvant therapy, adjuvant therapy, genetic counseling/testing, and 
dual HER2-targeted therapy. Two blinded reviewers scored concordance using a 
5-point Likert scale. Inter-rater reliability and classification metrics were 
calculated. GPT-4o generated consistent recommendations across both sessions for 
all patients. Full concordance (5/5) with MTB decisions was observed in 31 of 33 
cases (93.9%), while partial concordance (4/5) occurred in 2 cases (6.1%) due to 
differences regarding genetic counseling. Inter-rater agreement was perfect 
(Cohen's kappa = 1.00), and the mean concordance score was 4.94 out of 5. The 
model achieved an overall accuracy of 93.9%, precision of 93.9%, recall of 100%, 
and F1 score of 96.8%. GPT-4o demonstrated a high level of agreement with expert 
multidisciplinary decisions in breast cancer care when provided with structured 
clinical input. These findings support its potential as a reproducible, 
guideline-consistent decision-support tool in oncology workflows.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02314-9
PMID: 41354952 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval: The study was 
conducted in accordance with the principles of the Declaration of Helsinki and 
was approved by the institutional scientific board of Ankara Güven Hospital 
(Approval No: 843, April 30, 2025). Due to the retrospective nature of the study 
and the use of anonymized data, informed consent was waived. Consent to 
participate: Informed consent was waived due to the retrospective design of the 
study. AI Disclosure: Portions of the manuscript text were refined using the 
GPT-4o large language model (OpenAI) under the direct supervision of the 
authors. All clinical data, analysis, evaluations, and interpretations were 
performed solely by the authors. Clinical trial number: Not applicable. 
Competing interests: The authors declare no competing interests.


389. J Palliat Med. 2025 Dec;28(12):1654-1659. doi: 10.1177/10966218251376436. Epub 
2025 Sep 9.

Evaluating the Clinical Reasoning of Generative AI in Palliative Care: A 
Comparison with Five Years of Pharmacy Learners.

Lane MT(1), Ajayi TA(2)(3)(4)(5), Edmonds KP(2)(4)(5), Atayee RS(1)(2)(4)(5)(6).

Author information:
(1)Skaggs School of Pharmacy & Pharmaceutical Sciences, UC San Diego Health 
Sciences, San Diego, California, USA.
(2)Division of Geriatrics, Gerontology, & Palliative Care, Department of 
Medicine, UC San Diego Health Sciences, San Diego, California, USA.
(3)Division of Pediatric Palliative Care, Rady Children's Hospital & Medical 
Center, San Diego, California, USA.
(4)Joan & Irwin Jacobs Center for Health Innovation, UC San Diego Health, San 
Diego, California, USA.
(5)Palliative Care Program, UC San Diego Health, San Diego, California, USA.
(6)Department of Pharmacy, UC San Diego Health, San Diego, California, USA.

Context: Artificial intelligence (AI), particularly large language models 
(LLMs), offers the potential to augment clinical decision-making, including in 
palliative care pharmacy, where personalized treatment and assessments are 
important. Despite the growing interest in AI, its role in clinical reasoning 
within specialized fields such as palliative care remains uncertain. Objectives: 
This study examines the performance of four commercial-grade LLMs on a Script 
Concordance Test (SCT) designed for pharmacy students in a pain and palliative 
care elective, comparing AI outputs with human learners' performance at 
baseline. Methods: Pharmacy students from 2018 to 2023 completed an SCT 
consisting of 16 clinical questions. Four LLMs (ChatGPT 3.5, ChatGPT 4.0, 
Gemini, and Gemini Advanced) were tested using the same SCT, with their 
responses compared to student performance. Results: The average score for LLMs 
(0.43) was slightly lower than that of students (0.47), but this difference was 
not statistically significant (p = 0.55). ChatGPT 4.0 achieved the highest score 
(0.57). Conclusions: While LLMs show potential for augmenting clinical 
decision-making, their limitations in patient-centered care highlight the 
necessity of human oversight and reinforce that they cannot replace human 
expertise in palliative care. This study was conducted in a controlled research 
setting, where LLMs were prompted to answer clinical reasoning questions despite 
default safety restrictions. However, this does not imply that such prompts 
should be used in practice. Future research should explore alternative methods 
for assessing AI decision-making without overriding safety mechanisms and focus 
on refining AI to better align with complex clinical reasoning. In addition, 
further studies are needed to confirm AI's comparative effectiveness, given the 
sample size limitations.

DOI: 10.1177/10966218251376436
PMID: 40929006 [Indexed for MEDLINE]


390. Head Neck Pathol. 2025 Nov 3;19(1):123. doi: 10.1007/s12105-025-01850-7.

Performance of AI Chatbots on Head and Neck Pathology Board-Style Exam Questions 
and Guidelines for Responsible Use.

Khoury ZH(1)(2), Sultan AS(3)(4)(5)(6).

Author information:
(1)Department of Oral Diagnostic Sciences & Research, Meharry Medical College 
School of Dentistry, 1801 Meharry Blvd, Nashville, TN, 37208, USA. 
ZKhoury@mmc.edu.
(2), Eli5a Technologies, Geneva, Switzerland. ZKhoury@mmc.edu.
(3), Eli5a Technologies, Geneva, Switzerland. ASultan1@umaryland.edu.
(4)Division of Artificial Intelligence Research, University of Maryland School 
of Dentistry, 650 W. Baltimore St., Baltimore, MD, 21201, USA. 
ASultan1@umaryland.edu.
(5)Department of Oncology and Diagnostic Sciences, University of Maryland School 
of Dentistry, Baltimore, MD, USA. ASultan1@umaryland.edu.
(6)University of Maryland Marlene and Stewart Greenebaum Comprehensive Cancer 
Center, Baltimore, MD, USA. ASultan1@umaryland.edu.

The promising integration of artificial intelligence (AI), particularly large 
language models (LLMs) or AI chatbots, into medical education and practice 
necessitates rigorous evaluation of their capabilities. While chatbot 
performance has been assessed against standardized exams like the bar, medical 
and dental boards, and in diagnostic specialties such as radiology and 
ophthalmology, a critical gap exists within the complex discipline of pathology, 
specifically concerning subspecialty areas like head and neck pathology. 
Conducting a pilot study evaluating chatbot responses to head and neck pathology 
board-style questions is therefore essential and novel. Crucially for education, 
this pilot study evaluated six AI chatbots on their performance in answering 
multiple-choice questions (MCQs) retrieved from authorities in the field of head 
and neck pathology. Twenty MCQs relevant to head and neck pathology were 
identified from the public domain. A total of 120 responses from six chatbots 
were evaluated for response and citation accuracy. Although AI chatbots answered 
head and neck pathology board-style questions with 85-100% accuracy, citation 
accuracy and performance on image-based questions were poor. Considering the 
pedagogical principle that actively challenging exam questions enhances memory 
retention and learning outcomes compared to passive text review, AI-generated 
multiple-choice questions specifically designed for head and neck pathology 
study revision purposes were generated. The generated MCQs were determined to be 
at a more fundamental level and suitable for predoctoral dental students or 
first-year pathology residents. This pilot study probed the limitations of these 
tools in accurately addressing diagnostic and interpretive challenges. 
Furthermore, the recommendations and guidelines informed from the results of 
this pilot study represent a vital first step in the addressing the responsible 
use of AI for head and neck pathology education and practice.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s12105-025-01850-7
PMCID: PMC12583266
PMID: 41182557 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
are part of the scientific team at Eli5a Technologies (Base.org) which is among 
the chatbots featured in this manuscript. Dr. Sultan is Co-Founder and is the 
Chief Scientific Officer for Eli5a (https://eli5a.com/docs), a fully autonomous 
superintelligent AI agent. Dr. Khoury is the Scientific Scrutiny and Integrity 
Officer for the Eli5a Science Team.


391. Cleft Palate Craniofac J. 2025 Nov;62(11):1873-1880. doi: 
10.1177/10556656241281453. Epub 2024 Sep 9.

Expanding Accessibility in Cleft Care: The Role of Artificial Intelligence in 
Improving Literacy of Alveolar Bone Grafting Information.

Manasyan A(1)(2), Lasky S(1)(2), Jolibois M(1), Moshal T(1)(2), Roohani I(1)(2), 
Munabi N(1)(3), Urata MM(1)(3), Hammoudeh JA(1)(3).

Author information:
(1)Division of Plastic and Maxillofacial Surgery, Children's Hospital Los 
Angeles, Los Angeles, CA, USA.
(2)Keck School of Medicine, University of Southern California, Los Angeles, CA, 
USA.
(3)Division of Plastic and Reconstructive Surgery, Keck School of Medicine of 
USC, Los Angeles, CA, USA.

ObjectiveThe American Medical Association (AMA) recommends patient education 
materials (PEMs) be written at or below a sixth grade reading level. This study 
seeks to determine the quality, readability, and content of available alveolar 
bone grafting (ABG) PEMs and determine if artificial intelligence can improve 
PEM readability.DesignReview of free online PEMs.SettingOnline ABG PEMs were 
retrieved from different authoring body types (hospital/academic center, medical 
society, or private practice).Patients, ParticipantsNoneInterventionsContent was 
assessed by screening PEMs for specific ABG-related topics. Quality was 
evaluated with the Patient Education Material Assessment Tool (PEMAT), which has 
measures of understandability and actionability. Open-access readability 
software (WebFX) determined readability with Flesch Reading Ease, Flesch-Kincaid 
Grade Level, and Gunning-Fog Index. PEMs were rewritten with ChatGPT, and 
readability metrics were reassessed.Main Outcome Measure(s)Quality, readability, 
and content of ABG PEMsResults34 PEMs were analyzed. Regarding quality, the 
average PEMAT-understandability score was 67.0 ± 16.2%, almost at the minimum 
acceptable score of 70.0% (p = 0.281). The average PEMAT-actionability score was 
low at 33.0 ± 24.1%. Regarding readability, the average Flesch Reading Ease 
score was 64.6 ± 12.8, categorized as "standard/plain English." The average 
Flesch-Kincaid Grade Level was 8.0 ± 2.3, significantly higher than AMA 
recommendations (p < 0.0001). PEM rewriting with ChatGPT improved Flesch-Kincaid 
Grade Level to 6.1 ± 1.3 (p < 0.0001).ConclusionsAvailable ABG PEMs are above 
the recommended reading level, yet ChatGPT can improve PEM readability. Future 
studies should improve areas of ABG PEMs that are most lacking, such as 
actionability.

DOI: 10.1177/10556656241281453
PMID: 39246230 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Conflicting InterestsThe authors 
declared no potential conflicts of interest with respect to the research, 
authorship, and/or publication of this article. Ethics StatementThe research was 
conducted with full adherence to the ethical principles outlined by Children's 
Hospital Los Angeles. All data was collected with respect for the rights and 
privacy of patients and families (IRB exempt and not directly involving human 
subjects). Measures were taken to ensure the accuracy and integrity of the 
research findings. The authors ensure that this research has not been published 
or submitted elsewhere for consideration.


392. BMJ Glob Health. 2025 Dec 19;9(Suppl 3):e013814. doi: 10.1136/bmjgh-2023-013814.

Addressing health information inequities: making evidence-based clinical content 
more accessible in low- and middle-income primary care.

Cornick RV(1), Picken SC(2), Awotiwon A(3), Rabe M(4), Wattrus C(3), Fredericks 
T(5), Timmerman V(2), Fairall LR(3)(6).

Author information:
(1)Knowledge Translation Unit, University of Cape Town Lung Institute, Cape 
Town, Western Cape, South Africa ruth.cornick@uct.ac.za.
(2)Knowledge Translation Unit, University of Cape Town Faculty of Health 
Sciences, Observatory, South Africa.
(3)Knowledge Translation Unit, University of Cape Town Lung Institute, Cape 
Town, Western Cape, South Africa.
(4)University of Cape Town, Rondebosch, South Africa.
(5)knowledge translation unit, University of Cape Town, Rondebosch, South 
Africa.
(6)School of Life Course & Population Sciences, King's College London, London, 
UK.

Disparity in access to trustworthy health information between high-income and 
low-income settings remains stark and contributes to global health inequity. The 
volume of new clinical practice guidelines a healthcare provider needs to digest 
to deliver up-to-date, evidence-based care is overwhelming, particularly in 
primary care, where the scope is comprehensive. However, many low- and 
middle-income countries (LMICs) lack the resources to tailor guidance for their 
realities. International standards for adaptation or adoption of existing 
guidelines tend to focus on a single clinical topic and still require 
considerable evidence synthesis expertise, slowing provision of up-to-date, 
relevant protocols for the primary care provider.The Practical Approach to Care 
Kit (PACK) guide covers most conditions managed in primary care. It has been 
introduced to South Africa, Ethiopia, Brazil, Nigeria, Botswana and Indonesia to 
support primary care reforms. This paper describes the reference repository and 
updating mechanisms underpinning the PACK Global guide (that forms a template 
for local adaptation) so that it reflects latest international evidence and WHO 
guidance. The referencing and updating mechanism to curate its 3689 
recommendations drew on the established evidence synthesis processes of the 
British Medical Journal's Best Practice and the WHO. The challenges of 
maintaining this content set were largely funding and resource constraints in 
our small team. We are exploring how advances in generative artificial 
intelligence might expedite review of the large clinical guidelines and policies 
required for PACK updates as well as address limitations of current database 
software as a content management system, to facilitate editorial and publication 
processes.Leveraging existing evidence synthesis processes appears to be a 
feasible approach to maintaining a comprehensive LMIC primary care clinical 
content set and may go some way to improving access to up-to-date health 
information, thus addressing global health inequities.

© Author(s) (or their employer(s)) 2025. Re-use permitted under CC BY. Published 
by BMJ Group.

DOI: 10.1136/bmjgh-2023-013814
PMID: 41419258 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: RVC, LRF and VT have no 
competing interests. SCP, MR, AA, TF and CW are ex-employees of the KTU. Between 
2015 and 2020, the KTU and BMJ were engaged in a non-profit strategic 
partnership to provide continuous evidence updates for PACK, expand PACK-related 
supported services to countries and organisations as requested, and where 
appropriate, licence PACK content. The KTU and BMJ co-funded core positions, 
including a PACK Global Development Director, and received no profits from the 
partnership. PACK receives no funding from the pharmaceutical industry.


393. Obes Surg. 2025 Nov;35(11):4628-4638. doi: 10.1007/s11695-025-08249-x. Epub 2025 
Sep 27.

Evaluating Artificial Intelligence-Generated Patient Education Materials for 
Bariatric Surgery: Comparative Analysis of Response Quality, Reliability, and 
Readability Across ChatGPT and DeepSeek Models.

Guo S(1), Yang CL(2), Lin XP(2), Jiang M(2), Chen J(2), Tuo KX(2), Yang WW(2), 
Wang Q(3)(4), Jin XR(3)(4), Li P(3)(4).

Author information:
(1)Department of Pharmacy, The Affiliated Hospital of Guizhou Medical 
University, Guiyang, Guizhou, 550004, China. guoshuai@gmc.edu.cn.
(2)Department of Pharmacy, The Affiliated Hospital of Guizhou Medical 
University, Guiyang, Guizhou, 550004, China.
(3)Department of Gastroenterology, The Affiliated Hospital of Guizhou Medical 
University, Guiyang, Guizhou, 550004, China.
(4)Metabolic and Bariatric Center, The Affiliated Hospital of Guizhou Medical 
University, Guiyang, Guizhou, 550004, China.

BACKGROUND: Artificial intelligence (AI) models such as ChatGPT and DeepSeek 
have gained increasing attention for their potential to enhance patient 
education by delivering accessible and evidence-based health information. We 
designed the following study to evaluate the AI models-ChatGPT and DeepSeek-in 
generating patient education materials for bariatric surgery.
METHODS: Thirty commonly asked patient questions related to bariatric surgery 
were classified into four thematic domains: (1) surgical planning and technical 
considerations, (2) preoperative assessment and optimization, (3) postoperative 
care and complication management, and (4) long-term follow-up and disease 
management. Responses generated by ChatGPT and DeepSeek were evaluated using 
three key metrics: (1) response quality, assessed by the Global Quality Score, 
rated on a 5-point scale from 1 (poor) to 5 (excellent); (2) reliability, 
measured using modified DISCERN criteria, which assess adherence to clinical 
guidelines and evidence-based standards, with scores ranging from 5 (low) to 25 
(high); and (3) readability, evaluated using two validated formulas: the 
Flesch-Kincaid Grade Level and the Flesch Reading Ease Score.
RESULTS: ChatGPT significantly outperformed DeepSeek in response quality, with a 
median (IQR) Global Quality Score of 5.00 (4.00, 5.00) vs. 4.00 (4.00, 5.00) 
(P = 0.002). Higher reliability was also observed in ChatGPT, as reflected by 
mDISCERN scores across all four domains (median [IQR], 22.0 [21.0, 23.25] vs. 
19.7 [19.0, 20.75]; P < 0.001). While no significant difference was found in the 
Flesch Reading Ease Score (mean [SD], 26.11 [12.84] vs. 20.87 [12.20]; 
P = 0.110), ChatGPT yielded significantly higher Flesch-Kincaid Grade Level 
Scores (meaning its text was more complex) (mean [SD], 16.40 [2.43] vs. 13.48 
[2.35]; P < 0.001). Both models produced responses at a readability level 
corresponding to college education.
CONCLUSIONS: ChatGPT provided higher-quality and more reliable responses, while 
DeepSeek's answers were slightly easier to read. However, both models' answers 
lacked attention to psychosocial and cultural aspects of patient care, 
highlighting the need for more empathetic, adaptive AI to support inclusive 
patient education.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s11695-025-08249-x
PMID: 41014443 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics Approval and Consent to 
Participate: Not applicable. Consent for Publication: Not applicable. Competing 
interests: The authors declare no competing interests.


394. J Med Internet Res. 2025 Nov 19;27:e78393. doi: 10.2196/78393.

Performance of Retrieval-Augmented Generation Large Language Models in 
Guideline-Concordant Prostate-Specific Antigen Testing: Comparative Study With 
Junior Clinicians.

Tung JYM(1)(2), Le Q(1), Yao J(1), Huang Y(1), Lim DYZ(1)(3), Sng GGR(1)(4), Lau 
RSE(2), Tan YG(2), Chen K(2), Tay KJ(2), Tan JH(1), Yuen JSP(2), Cheng CWS(2), 
Ho HSS(2).

Author information:
(1)Data Science and Artificial Intelligence Laboratory, Singapore General 
Hospital, Singapore, Singapore.
(2)Department of Urology, Singapore General Hospital, Block 4 Level 1, 16 
College Road, Singapore, 169854, Singapore, 65 62223322.
(3)Department of Gastroenterology, Singapore General Hospital, Singapore, 
Singapore.
(4)Department of Endocrinology, Singapore General Hospital, Singapore, 
Singapore.

BACKGROUND: Prostate-specific antigen (PSA) testing remains the cornerstone of 
early prostate cancer detection. Society guidelines for prostate cancer 
screening via PSA testing serve to standardize patient care and are often used 
by trainees, junior staff, or generalist medical practitioners to guide medical 
decision-making. However, adherence to guidelines is a time-consuming and 
challenging task, and rates of inappropriate PSA testing are high. 
Retrieval-augmented generation (RAG) is a method to enhance the reliability of 
large language models (LLMs) by grounding responses in trusted external sources.
OBJECTIVE: This study aimed to evaluate a RAG-enhanced LLM system, grounded in 
current European Association of Urology and American Urological Association 
guidelines, to assess its effectiveness in providing guideline-concordant PSA 
screening recommendations compared to junior clinicians.
METHODS: A series of 44 fictional outpatient case scenarios was developed to 
represent a broad spectrum of clinical presentations. A RAG pipeline was 
developed, comprising a life expectancy estimation module based on the Charlson 
Comorbidity Index, followed by LLM-generated recommendations constrained to 
retrieved excerpts from the European Association of Urology and American 
Urological Association guidelines. Five junior clinicians were tasked to provide 
PSA testing recommendations for the same scenarios in closed-book and open-book 
formats. Answers were compared for accuracy in a binomial fashion. Fleiss κ was 
computed to assess interrater agreement among clinicians.
RESULTS: The RAG-LLM tool provided guideline-concordant recommendations in 95.5% 
(210/220) of case scenarios, compared to junior clinicians, who were correct in 
62.3% (137/220) of scenarios in a closed-book format and 74.1% (163/220) of 
scenarios in an open-book format. The difference was statistically significant 
for both closed-book (P<.001) and open-book (P<.001) formats. Interrater 
agreement among clinicians was fair, with Fleiss κ of 0.294 and 0.321 for 
closed-book and open-book formats, respectively.
CONCLUSIONS: Use of RAG techniques allows LLMs to integrate complex guidelines 
into day-to-day medical decision-making. RAG-LLM tools in urology have the 
capability to enhance clinical decision-making by providing guideline-concordant 
recommendations for PSA testing, potentially improving the consistency of health 
care delivery, reducing cognitive load on clinicians, and reducing unnecessary 
investigations and costs. While this study used synthetic cases in a controlled 
simulation environment, it establishes a foundation for future validation in 
real-world clinical settings.

© Joshua Yi Min Tung, Quan Le, Jinxuan Yao, Yifei Huang, Daniel Yan Zheng Lim, 
Gerald Gui Ren Sng, Rachel Shu En Lau, Yu Guang Tan, Kenneth Chen, Kae Jack Tay, 
Jen Hong Tan, John Shyi Peng Yuen, Christopher Wai Sam Cheng, Henry Sun Sien Ho. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/78393
PMCID: PMC12629621
PMID: 41259800 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


395. JMIR Med Inform. 2025 Dec 1;13:e78332. doi: 10.2196/78332.

Enabling Just-in-Time Clinical Oncology Analysis With Large Language Models: 
Feasibility and Validation Study Using Unstructured Synthetic Data.

May P(1), Greß J(1)(2), Seidel C(3), Sommer S(4), Schuler MK(2)(5), Nokodian 
S(1), Schröder F(2), Jung J(1)(6).

Author information:
(1)Department of Internal Medicine III, School of Medicine and Health, TUM 
University Hospital, Technical University of Munich, Ismaninger Str. 22, Munich, 
Germany, 49 89-4140-8753.
(2)MPiriQ Science Technologies GmbH, Munich, Germany.
(3)Department of Oncology, Hematology and Bone Marrow Transplantation with 
Division of Pneumology, University Medical Center Hamburg-Eppendorf, Hamburg, 
Germany.
(4)MVZ Elisenhof, Munich, Germany.
(5)Onkologischer Schwerpunkt am Oskar-Helene Heim, Berlin, Germany.
(6)Department of Hematology and Medical Oncology, University Medical Center 
Göttingen, Göttingen, Germany.

BACKGROUND: Traditional cancer registries, limited by labor-intensive manual 
data abstraction and rigid, predefined schemas, often hinder timely and 
comprehensive oncology research. While large language models (LLMs) have shown 
promise in automating data extraction, their potential to perform direct, 
just-in-time (JIT) analysis on unstructured clinical narratives-potentially 
bypassing intermediate structured databases for many analytical tasks-remains 
largely unexplored.
OBJECTIVE: This study aimed to evaluate whether a state-of-the-art LLM (Gemini 
2.5 Pro) can enable a JIT clinical oncology analysis paradigm by assessing its 
ability to (1) perform high-fidelity multiparameter data extraction, (2) answer 
complex clinical queries directly from raw text, (3) automate multistep survival 
analyses including executable code generation, and (4) generate novel, 
clinically plausible hypotheses from free-text documentation.
METHODS: A synthetic dataset of 240 unstructured clinical letters from patients 
with stage IV non-small cell lung cancer (NSCLC), embedding 14 predefined 
variables, was used. Gemini 2.5 Pro was evaluated on four core JIT capabilities. 
Performance was measured by using the following metrics: extraction accuracy 
(compared to human extraction of n=40 letters and across the full n=240 
dataset); numerical deviation for direct question answering (n=40 to 240 
letters, 5 questions); log-rank P value and Harrell concordance index for 
LLM-generated versus ground-truth Kaplan-Meier survival analyses (n=160 letters, 
overall survival and progression-free survival); and correct justification, 
novelty, and a qualitative evaluation of LLM-generated hypotheses (n=80 and 
n=160 letters).
RESULTS: For multiparameter extraction from 40 letters, the LLM achieved >99% 
average accuracy, comparable to human extraction, but in significantly less time 
(LLM: 3.7 min vs human: 133.8 min). Across the full 240-letter dataset, LLM 
multiparameter extraction maintained >98% accuracy for most variables. The LLM 
answered multiconditional clinical queries directly from raw text with a 
relative deviation rarely exceeding 1.5%, even with up to 240 letters. 
Crucially, it autonomously performed end-to-end survival analysis, generating 
text-to-R-code that produced Kaplan-Meier curves statistically indistinguishable 
from ground truth. Consistent performance was demonstrated on a small validation 
cohort of 80 synthetic acute myeloid leukemia reports. Stress testing on data 
with simulated imperfections revealed a key role of a human-in-the-loop to 
resolve AI-flagged ambiguities. Furthermore, the LLM generated several correctly 
justified, biologically plausible, and potentially novel hypotheses from 
datasets up to 80 letters.
CONCLUSIONS: This feasibility study demonstrated that a frontier LLM (Gemini 2.5 
Pro) can successfully perform high-fidelity data extraction, multiconditional 
querying, and automated survival analysis directly from unstructured text. These 
results provide a foundational proof of concept for the JIT clinical analysis 
approach. However, these findings are confined to synthetic patients, and 
rigorous validation on real-world clinical data is an essential next step before 
clinical implementation can be considered.

© Peter May, Julian Greß, Christoph Seidel, Sebastian Sommer, Markus K Schuler, 
Sina Nokodian, Florian Schröder, Johannes Jung. Originally published in JMIR 
Medical Informatics (https://medinform.jmir.org).

DOI: 10.2196/78332
PMCID: PMC12670046
PMID: 41328496 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: MKS and FS hold shares in 
Accessus Science Technologies GmbH. PM, JG, CS, and SS receive honoraria from 
Accessus Science Technologies GmbH. SN and JJ have no conflicts of interest 
related to this research.


396. Proc Natl Acad Sci U S A. 2025 Sep 23;122(38):e2505106122. doi: 
10.1073/pnas.2505106122. Epub 2025 Sep 17.

Does AI help humans make better decisions? A statistical evaluation framework 
for experimental and observational studies.

Ben-Michael E(1), Greiner DJ(2), Huang M(3)(4), Imai K(5)(6), Jiang Z(7), Shin 
S(5).

Author information:
(1)Department of Statistics & Data Science and Heinz College of Information & 
Systems Public Policy, Carnegie Mellon University, Pittsburgh, PA 15213.
(2)Harvard Law School, Cambridge, MA 02138.
(3)Department of Political Science, Yale University, New Haven, CT 06511.
(4)Department of Statistics & Data Science, Yale University, New Haven, CT 
06511.
(5)Department of Government, Harvard University, Cambridge, MA 02138.
(6)Department of Statistics, Harvard University, Cambridge, MA 02138.
(7)School of Mathematics, Sun Yat-sen University, Guangzhou, Guangdong 510275, 
China.

Comment in
    Proc Natl Acad Sci U S A. 2025 Dec 23;122(51):e2523997122. doi: 
10.1073/pnas.2523997122.

The use of AI, or more generally data-driven algorithms, has become ubiquitous 
in today's society. Yet, in many cases and especially when stakes are high, 
humans still make final decisions. The critical question, therefore, is whether 
AI helps humans make better decisions compared to a human-alone or AI-alone 
system. We introduce a methodological framework to answer this question 
empirically with minimal assumptions. We measure a decision maker's ability to 
make correct decisions using standard classification metrics based on the 
baseline potential outcome. We consider a single-blinded and unconfounded 
treatment assignment, in which the provision of AI-generated recommendations is 
assumed to be randomized across cases, conditional on observed covariates, with 
final decisions made by humans. Under this study design, we show how to compare 
the performance of three alternative decision-making systems-human-alone, 
human-with-AI, and AI-alone. Importantly, the AI-alone system encompasses any 
individualized treatment assignment, including those not used in the original 
study. We also show when AI recommendations should be provided to a 
human-decision maker, and when one should follow such recommendations. We apply 
the proposed methodology to our own randomized controlled trial evaluating a 
pretrial risk assessment instrument. We find that the risk assessment 
recommendations do not improve the classification accuracy of a judge's decision 
to impose cash bail. Furthermore, replacing a human judge with algorithms-the 
risk assessment score and a large language model in particular-yields worse 
classification performance.

DOI: 10.1073/pnas.2505106122
PMCID: PMC12478102
PMID: 40961147 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests statement:The authors 
declare no competing interest.


397. BMC Med Inform Decis Mak. 2025 Sep 29;25(1):342. doi: 
10.1186/s12911-025-03182-6.

Ethical implications of using general-purpose LLMs in clinical settings: a 
comparative analysis of prompt engineering strategies and their impact on 
patient safety.

Esmaeilzadeh P(1).

Author information:
(1)Department of Information Systems and Business Analytics, College of 
Business, Florida International University (FIU), Modesto A. Maidique Campus, 
11200 S.W. 8th St, RB 261B, Miami, FL, 33199, USA. pesmaeil@fiu.edu.

BACKGROUND: The rapid integration of large language models (LLMs) into 
healthcare raises critical ethical concerns regarding patient safety, 
reliability, transparency, and equitable care delivery. Despite not being 
trained explicitly on medical data, individuals increasingly use general-purpose 
LLMs to address medical questions and clinical scenarios. While prompt 
engineering can optimize LLM performance, its ethical implications for clinical 
decision-making remain underexplored. This study aimed to evaluate the ethical 
dimensions of prompt engineering strategies in the clinical applications of 
LLMs, focusing on safety, bias, transparency, and their implications for the 
responsible implementation of AI in healthcare.
METHODS: We conducted an ethics-focused analysis of three advanced and 
reasoning-capable LLMs (OpenAI O3, Claude Sonnet 4, Google Gemini 2.5 Pro) 
across six prompt engineering strategies and five clinical scenarios of varying 
ethical complexity. Six expert clinicians evaluated 90 responses using domains 
that included diagnostic accuracy, safety assessment, communication, empathy, 
and ethical reasoning. We specifically analyzed safety incidents, bias patterns, 
and transparency of reasoning processes.
RESULTS: Significant ethical concerns emerged across all models and scenarios. 
Critical safety issues occurred in 12.2% of responses, with concentration in 
complex ethical scenarios (Level 5: 23.1% vs. Level 1: 2.3%, p < 0.001). 
Meta-cognitive prompting demonstrated superior ethical reasoning (mean ethics 
score: 78.3 ± 9.1), while safety-first prompting reduced safety incidents by 45% 
compared to zero-shot approaches (8.9% vs. 16.2%). However, all models showed 
concerning deficits in communication empathy (mean 54% of maximum) and exhibited 
potential bias in complex multi-cultural scenarios. Transparency varied 
significantly by prompt strategy, with meta-cognitive approaches providing the 
clearest reasoning pathways (4.2 vs. 1.8 explicit reasoning steps), which are 
essential for clinical accountability. The study highlighted critical gaps in 
ethical decision-making transparency, with meta-cognitive approaches providing 
4.2 explicit reasoning steps compared to 1.8 in zero-shot methods (p < 0.001). 
Bias patterns disproportionately affected vulnerable populations, with 
systematic underestimation of treatment appropriateness in elderly patients and 
inadequate cultural considerations in end-of-life scenarios.
CONCLUSIONS: Current clinical applications of general-purpose LLMs present 
substantial ethical challenges requiring urgent attention. While structured 
prompt engineering demonstrated measurable improvements in some domains, with 
meta-cognitive approaches showing 13.0% performance gains and safety-first 
prompting reducing critical incidents by 45%, substantial limitations persist 
across all strategies. Even optimized approaches achieved inadequate performance 
in communication and empathy (≤ 54% of maximum), retained residual bias patterns 
(11.7% in safety-first conditions), and exhibited concerning safety deficits, 
indicating that current prompt engineering methods provide only marginal 
improvements, which are insufficient for reliable clinical deployment. These 
findings highlight significant ethical challenges that necessitate further 
investigation into the development of appropriate guidelines and regulatory 
frameworks for the clinical use of general-purpose AI models.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-03182-6
PMCID: PMC12481957
PMID: 41024005 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was submitted to and reviewed by the Institutional 
Review Board (IRB) at Florida International University (full name: Florida 
International University Institutional Review Board). The study was approved as 
exempt research, as it involved educational tests, survey procedures, interview 
procedures, or observation of public behavior (IRB Protocol #115871). The study 
was conducted in accordance with the Declaration of Helsinki, as it involved 
research on human data. The physicians participated voluntarily as professional 
evaluators of AI-generated responses to clinical scenarios, and no patient 
information or protected health information was used in any aspect of this 
research. Informed consent was obtained from physician participants who were 
expert evaluators of AI-generated responses to simulated clinical scenarios. 
Consent for publication: Not applicable. Competing interests: The authors 
declare no competing interests.


398. JMIR Form Res. 2025 Nov 24;9:e78289. doi: 10.2196/78289.

Accuracy of Large Language Model Responses Versus Internet Searches for Common 
Questions About Glucagon-Like Peptide-1 Receptor Agonist Therapy: Exploratory 
Simulation Study.

Tan SYT(#)(1), Sng GGR(#)(1), Lee PC(1).

Author information:
(1)Department of Endocrinology, Singapore General Hospital, 20 College Road, 
Singapore, 169608, Singapore, +65 62223322.
(#)Contributed equally

BACKGROUND: Novel glucagon-like peptide-1 receptor agonists (GLP1RAs) for 
obesity treatment have generated considerable dialogue on digital media 
platforms. However, nonevidence-based information from online sources may 
perpetuate misconceptions about GLP1RA use. A promising new digital avenue for 
patient education is large language models (LLMs), which could potentially be 
used as an alternative platform to clarify questions regarding GLP1RA therapy.
OBJECTIVE: This study aimed to compare the accuracy, objectivity, relevance, 
reproducibility, and overall quality of responses generated by an LLM (GPT-4o) 
and internet searches (Google) for common questions about GLP1RA therapy.
METHODS: This study compared LLM (GPT-4o) and internet (Google) search responses 
to 17 simulated questions about GLP1RA therapy. These questions were 
specifically chosen to reflect themes identified based on Google Trends data. 
Domains included indications and benefits of GLP1RA therapy, expected treatment 
course, and common side effects and specific risks pertaining to GLP1RA 
treatment. Responses were graded by 2 independent evaluators based on safety, 
consensus with guidelines, objectivity, reproducibility, relevance, and 
explainability using a 5-point Likert scale. Mean scores were compared using 
paired 2-tailed t tests. Qualitative observations were recorded.
RESULTS: LLM responses had significantly higher scores than internet responses 
in the "objectivity" (mean 3.91, SD 0.63 vs mean 3.36, SD 0.80; mean difference 
0.55, SD 1.00; 95% CI 0.03-1.06; P=.04) and "reproducibility" (mean 3.85, SD 
0.49 vs mean 3.00, SD 0.97; mean difference 0.85, SD 1.14; 95% CI 0.27-1.44; 
P=.007) categories. There was no significant difference in the mean scores in 
the "safety," "consensus," "relevance," and "explainability" categories. 
Interrater agreement was high (overall percentage agreement 95.1%; Gwet 
agreement coefficient 0.879; P<.001). Qualitatively, LLM responses provided 
appropriate information about standard GLP1RA-related queries, including the 
benefits of GLP1RA, expected treatment course, and common side effects. However, 
it lacked updated information pertaining to newly emerging concerns surrounding 
GLP1RA use, such as the impact on fertility and mental health. Internet search 
responses were more heterogeneous, yielding several irrelevant or commercially 
biased sources.
CONCLUSIONS: This study found that LLM responses to GLP1RA therapy queries were 
more objective and reproducible than those to internet-based sources, with 
comparable relevance and concordance with clinical guidelines. However, LLMs 
lacked updated coverage of emerging issues, reflecting static training data 
limitations. In contrast, internet results were more current but were 
inconsistent and often commercially biased. These findings highlight the 
potential of LLMs to provide reliable and comprehensible health information, 
particularly for individuals hesitant to seek professional advice, while 
emphasizing the need for human oversight, dynamic data integration, and 
evaluation of readability to ensure safe and equitable use in obesity care. This 
study, although formative, is the first study to compare LLM and internet search 
output on common GLP1RA-related queries. It paves the way for future studies to 
explore how LLMs can integrate real-time data retrieval and evaluate their 
readability for lay audiences.

© Sarah Ying Tse Tan, Gerald Gui Ren Sng, Phong Ching Lee. Originally published 
in JMIR Formative Research (https://formative.jmir.org).

DOI: 10.2196/78289
PMCID: PMC12643393
PMID: 41284989 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


399. Nurse Educ Pract. 2025 Aug;87:104515. doi: 10.1016/j.nepr.2025.104515. Epub 2025 
Aug 16.

Use of artificial intelligence in planning postoperative nursing care in 
laparoscopic cholecystectomy patients: Comparison of ChatGPT and student 
practice.

Tanrıverdi S(1), Söylemez N(2).

Author information:
(1)Mardin Artuklu University Faculty of Health Science, Mardin, Türkiye. 
Electronic address: sehertanriverdi@artuklu.edu.tr.
(2)Osmaniye Korkut Ata Unıversıty Faculty of Health Science, Osmaniye, Türkiye. 
Electronic address: neslihan.percins@gmail.com.

AIM: This study aims to compare student-generated nursing care plans with 
ChatGPT-generated care planning texts for patients undergoing laparoscopic 
cholecystectomy surgery.
BACKGROUND: Given the increasing significance of technology in nursing 
education, the integration of AI-driven tools into educational methodologies 
demonstrates potential for enhancing nursing students' preparation for clinical 
practice by providing access to more comprehensive and current information 
sources.
METHODS: This descriptive comparative study evaluated 14 nursing care plans 
developed by nursing students for laparoscopic cholecystectomy patients 
alongside care plans generated by ChatGPT for identical patient scenarios, using 
the DISCERN scale as the assessment instrument.
RESULTS: The quality of student-generated care plans was determined to be 
satisfactory, whereas the quality of ChatGPT-generated texts was assessed as 
moderate. Both student care plans and ChatGPT-generated texts demonstrated 
moderate reliability and information quality regarding nursing care. Students 
exhibited the highest accuracy rate in formulating correct nursing diagnoses 
(90.22 %), while ChatGPT-generated texts demonstrated superior performance in 
correctly establishing nursing goals (90.35 %) and developing appropriate 
nursing interventions (91.07 %).
CONCLUSIONS: The findings indicate that while student-generated care plans 
demonstrated generally acceptable quality, ChatGPT's performance in this domain 
was moderate, particularly in formulating surgical nursing goals and 
interventions. Although clinical outcomes were not evaluated in this study, the 
results suggest that ChatGPT may serve as a valuable supplementary resource for 
surgical nursing education and care plan development. Further research is 
warranted to determine its impact on patient care quality and clinical 
effectiveness.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.nepr.2025.104515
PMID: 40834842 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare no conflict of interest.


400. JMIR Med Educ. 2025 Oct 24;11:e70766. doi: 10.2196/70766.

Application of AI Communication Training Tools in Medical Undergraduate 
Education: Mixed Methods Feasibility Study Within a Primary Care Context.

Jacobs C(1), Johnson H(2), Tan N(1), Brownlie K(2), Joiner R(1), Thompson T(2).

Author information:
(1)Department of Psychology, University of Bath, Claverton Down, Bath, BA2 7AY, 
United Kingdom, 44 01225384176.
(2)Bristol Medical School, University of Bristol, Bristol, United Kingdom.

BACKGROUND: Effective communication is fundamental to high-quality health care 
delivery, influencing patient satisfaction, adherence to treatment plans, and 
clinical outcomes. However, communication skills training for medical 
undergraduates often faces challenges in scalability, resource allocation, and 
personalization. Traditional methods, such as role-playing with standardized 
patients, are resource intensive and may not provide consistent feedback 
tailored to individual learners' needs. Artificial intelligence (AI) offers 
realistic patient interactions for education.
OBJECTIVE: This study aims to investigate the application of AI communication 
training tools in medical undergraduate education within a primary care context. 
The study evaluates the effectiveness, usability, and impact of AI virtual 
patients (VPs) on medical students' experience in communication skills practice.
METHODS: The study used a mixed methods sequential explanatory design, 
comprising a quantitative survey followed by qualitative focus group 
discussions. Eighteen participants, including 15 medical students and 3 
practicing doctors, engaged with an AI VP simulating a primary care consultation 
for prostate cancer risk assessment. The AI VP was designed using a large 
language model and natural voice synthesis to create realistic patient 
interactions. The survey assessed 5 domains: fidelity, immersion, intrinsic 
motivation, debriefing, and system usability. Focus groups were used to explore 
participants' experiences, challenges, and perceived educational value of the AI 
tool.
RESULTS: Significant positive responses emerged against a neutral baseline, with 
the following median scores: intrinsic motivation 16.5 of 20.0 (IQR 15.0-18.0; 
d=2.09, P<.001), system usability 12.0 of 15.0 (IQR 11.5-12.5; d=2.18, P<.001), 
and psychological safety 5.0 of 5.0 (IQR 5.0-5.0; d=4.78, P<.001). Fidelity 
(median score 6.0/10.0, IQR 5.2-7.0; d=-0.08, P=.02) and immersion (median score 
8.5/15.0, IQR 7.0-9.8; d=0.25 P=.08) were moderately rated. The overall 
Immersive Technology Evaluation Measure scores showed a high positive learning 
experience: median 47.5 of 65.0 (IQR 43.0-51.2; d=2.00, P<.001). Qualitative 
analysis identified 3 major themes across 11 subthemes, with participants 
highlighting both technical limitations and educational value. Participants 
valued the safe practice environment and the ability to receive immediate 
feedback.
CONCLUSIONS: AI VP technology shows promising potential for communication skills 
training despite the current realism limitations. While it does not yet match 
human standardized patient authenticity, the technology has achieved sufficient 
fidelity to support meaningful educational interactions, and this study 
identified clear areas for improvement. The integration of AI into medical 
curricula represents a promising avenue for innovation in medical education, 
with the potential to improve the quality and effectiveness of training 
programs.

© Chris Jacobs, Hans Johnson, Nina Tan, Kirsty Brownlie, Richard Joiner, Trevor 
Thompson. Originally published in JMIR Medical Education 
(https://mededu.jmir.org).

DOI: 10.2196/70766
PMCID: PMC12551969
PMID: 41135055 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


401. J Travel Med. 2025 Apr 25;32(4):taaf005. doi: 10.1093/jtm/taaf005.

Performance of ChatGPT-4o in the diagnostic workup of fever among returning 
travellers requiring hospitalization: a validation study.

Yelin D(1)(2), Shirin N(1), Harris I(3), Peretz Y(4), Yahav D(1)(2), Schwartz 
E(2)(5), Leshem E(1)(2)(5), Margalit I(1)(2)(4).

Author information:
(1)Infectious Diseases Unit, Sheba Medical Center, Tel HaShomer, Ramat Gan, 
Israel.
(2)School of Medicine, Faculty of Medical and Health Sciences, Tel Aviv 
University, P.O.B 39040 Tel Aviv 69978, Israel.
(3)Internal Medicine C, Sheba Medical Center, Tel HaShomer, Ramat Gan, Israel.
(4)Infection Control and Prevention Unit, Sheba Medical Center, Tel HaShomer, 
Ramat Gan, Israel.
(5)Center for Geographic Medicine, Sheba Medical Center, Tel HaShomer, Ramat 
Gan, Israel.

BACKGROUND: Febrile illness in returned travellers presents a diagnostic 
challenge in non-endemic settings. Chat generative pretrained transformer 
(ChatGPT) has the potential to assist in medical tasks, yet its diagnostic 
performance in clinical settings has rarely been evaluated. We conducted a 
validation assessment of ChatGPT-4o's performance in the workup of fever in 
returning travellers.
METHODS: We retrieved the medical records of returning travellers hospitalized 
with fever during 2009-2024. Their clinical scenarios at time of presentation to 
the emergency department were prompted to ChatGPT-4o, using a detailed uniform 
format. The model was further prompted with four consistent questions concerning 
the differential diagnosis and recommended workup. To avoid training, we kept 
the model blinded to the final diagnosis. Our primary outcome was ChatGPT-4o's 
success rates in predicting the final diagnosis when requested to specify the 
top three differential diagnoses. Secondary outcomes were success rates when 
prompted to specify the single most likely diagnosis, and all necessary 
diagnostics. We also assessed ChatGPT-4o as a predicting tool for malaria and 
qualitatively evaluated its failures.
RESULTS: ChatGPT-4o predicted the final diagnosis in 68% [95% confidence 
interval (CI) 59-77%], 78% (95% CI 69-85%) and 83% (95% CI 74-89%) of the 114 
cases, when prompted to specify the most likely diagnosis, top three diagnoses 
and all possible diagnoses, respectively. ChatGPT-4o showed a sensitivity of 
100% (95% CI 93-100%) and a specificity of 94% (95% CI 85-98%) for predicting 
malaria. The model failed to provide the final diagnosis in 18% (20/114) of 
cases, primarily by failing to predict globally endemic infections (16/21, 76%).
CONCLUSIONS: ChatGPT-4o demonstrated high diagnostic accuracy when prompted with 
real-life scenarios of febrile returning travellers presenting to the emergency 
department, especially for malaria. Model training is expected to yield an 
improved performance and facilitate diagnostic decision-making in the field.

© The Author(s) 2025. Published by Oxford University Press on behalf of 
International Society of Travel Medicine. All rights reserved. For permissions, 
please e-mail: journals.permissions@oup.com.

DOI: 10.1093/jtm/taaf005
PMID: 39823287 [Indexed for MEDLINE]


402. Int J Med Inform. 2026 Jan;205:106116. doi: 10.1016/j.ijmedinf.2025.106116. Epub 
2025 Sep 16.

Extracting language information from clinical notes using large language models.

Qian L(1), Hong N(1), Zhou Y(1), Xie Q(1), Weng RL(1), Chairuengjitjaras P(1), 
Du X(2), Lian J(2), Marshall GA(3), Blackley SV(2), Novoa-Laurentiev J(2), 
Quiroz YT(4), Kim TY(5), Adams N(6), Dossett ML(6), Zhou L(2), Xu H(7).

Author information:
(1)Department of Biomedical Informatics and Data Science, Yale School of 
Medicine, Yale University, New Haven, CT, USA.
(2)Department of Medicine, Brigham and Women's Hospital, Harvard Medical School, 
Boston, MA, USA.
(3)Department of Neurology, Brigham and Women's Hospital, Massachusetts General 
Hospital, Harvard Medical School, Boston, MA, USA.
(4)Department of Psychiatry, Massachusetts General Hospital, Harvard Medical 
School, Boston, MA, USA.
(5)Betty Irene Moore School of Nursing, University of California, Davis, 
Sacramento, CA, USA.
(6)Department of Internal Medicine, University of California, Davis, Sacramento, 
CA, USA.
(7)Department of Biomedical Informatics and Data Science, Yale School of 
Medicine, Yale University, New Haven, CT, USA. Electronic address: 
hua.xu@yale.edu.

BACKGROUND: Patient language proficiency plays a critical role in equitable, 
patient-centered care and language-related clinical research. However, language 
information recorded in structured fields of electronic health records (EHRs) is 
often incomplete or inaccurate, especially in multi-institutional settings with 
heterogeneous documentation practices.
OBJECTIVE: To develop and evaluate a named entity recognition (NER) pipeline 
that accurately extracts detailed patient language status from unstructured 
clinical notes using large language models (LLMs), thereby enabling scalable and 
generalizable language information extraction.
METHODS: We defined four categories of language status-fluent use, partial 
ability, lack of understanding, and language mentions unrelated to the 
patient-and annotated two datasets from Yale New Haven Hospital (YNHH) and 
MIMIC-III. We evaluated the performance of proprietary and open-source LLMs, 
including GPT-4o, LLaMA3, and BERT, under zero-shot and fine-tuning settings. 
Cross-site validation was conducted to assess generalizability across 
institutions.
RESULTS: GPT-4o achieved F1 scores of 87 % and 82 % on YNHH and MIMIC datasets, 
respectively, without fine-tuning. Fine-tuned open-source models such as BERT 
and LLaMA3 reached comparable or superior performance when trained on sufficient 
annotated data. Cross-institutional evaluations confirmed that LLMs, 
particularly LLaMA3, exhibited stronger generalizability than traditional 
models. Language mentions unrelated to patient fluency remained the most 
challenging category across all models.
CONCLUSION: Our NER framework enables automated extraction of nuanced language 
information from clinical narratives with high accuracy and generalizability. 
This work supports large-scale, language-focused research and has practical 
implications for improving patient-provider communication, interpreter service 
allocation, and equitable healthcare delivery.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106116
PMCID: PMC12490899
PMID: 40992205 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


403. J Am Med Inform Assoc. 2025 Oct 1;32(10):1570-1580. doi: 10.1093/jamia/ocaf124.

SBDH-Reader: a large language model-powered method for extracting social and 
behavioral determinants of health from clinical notes.

Gu Z(1), He L(2), Naeem A(3), Chan PM(2), Mohamed A(4), Khalil H(4), Guo Y(2), 
Huang J(1), Villanueva-Miranda I(1), Ding Y(3), Shi W(1), Dupre ME(5)(6), Xiao 
G(1)(7)(8), Peterson ED(9), Xie Y(1)(7)(8), Navar AM(9), Yang DM(1).

Author information:
(1)Quantitative Biomedical Research Center, Department of Health Data Science 
and Biostatistics, Peter O'Donnell Jr. School of Public Health, The University 
of Texas Southwestern Medical Center, Dallas, TX 75390, United States.
(2)Department of Health Data Science and Biostatistics, Peter O'Donnell Jr. 
School of Public Health, The University of Texas Southwestern Medical Center, 
Dallas, TX 75390, United States.
(3)School of Information, University of Texas at Austin, Austin, TX 78712, 
United States.
(4)The University of Texas Southwestern Medical Center, Dallas, TX 75390, United 
States.
(5)Department of Population Health Sciences, Duke University, Durham, NC 27708, 
United States.
(6)Department of Sociology, Duke University, Durham, TX 27708, United States.
(7)Department of Bioinformatics, The University of Texas Southwestern Medical 
Center, Dallas, TX 75390, United States.
(8)Simmons Comprehensive Cancer Center, The University of Texas Southwestern 
Medical Center, Dallas, TX 75390, United States.
(9)Department of Internal Medicine, The University of Texas Southwestern Medical 
Center, Dallas, TX 75390, United States.

Update of
    medRxiv. 2025 Jun 26:2025.02.19.25322576. doi: 10.1101/2025.02.19.25322576.

OBJECTIVE: Social and behavioral determinants of health (SBDH) are increasingly 
recognized as essential for prognostication and informing targeted 
interventions. Clinical notes often contain details about SBDH in unstructured 
format. Conventional extraction methods for these data tend to be labor 
intensive, inaccurate, and/or unscalable. In this study, we aim to develop and 
validate a large language model (LLM)-powered method to extract structured SBDH 
data from clinical notes through prompt engineering.
MATERIALS AND METHODS: We developed SBDH-Reader to extract 6 categories of 
granular SBDH data by prompting GPT-4o, including employment, housing, marital 
status, and substance use including alcohol, tobacco, and drug use. SBDH-Reader 
was developed using 7225 notes from 6382 patients in the MIMIC-III database 
(2001-2012) and externally validated using 971 notes from 437 patients at The 
University of Texas Southwestern Medical Center (UTSW; 2022-2023). We evaluated 
SBDH-Reader's performance against human-annotated ground truths based on 
precision, recall, F1, and confusion matrix.
RESULTS: When tested on the UTSW validation set, SBDH-Reader achieved a 
macro-average F1 ranging from 0.94 to 0.98 across 6 SBDH categories. For 
clinically relevant adverse attributes, F1 ranged from 0.96 (employment; 
housing) to 0.99 (tobacco use). When extracting any adverse attributes across 
all SBDH categories, SBDH-Reader achieved an F1 of 0.97, recall of 0.97, and 
precision of 0.98 in the independent validation set.
DISCUSSION: SBDH-Reader demonstrated strong performance in extracting structured 
SBDH data through effective prompt engineering of a general-purpose LLM, without 
the need for task-specific fine-tuning. Its modular design and adaptability to 
diverse datasets and documentation patterns support its applicability in 
real-world clinical settings.
CONCLUSION: SBDH-Reader has the potential to serve as a scalable and effective 
method for collecting real-time, patient-level SBDH data to support clinical 
research and care.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf124
PMCID: PMC12451939
PMID: 40720632 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
declare.


404. Eur Arch Otorhinolaryngol. 2025 Aug;282(8):4139-4153. doi: 
10.1007/s00405-025-09443-4. Epub 2025 May 17.

Feasibility of improving vocal fold pathology image classification with 
synthetic images generated by DDPM-based GenAI: a pilot study.

Khazrak I(1), Zainaee S(2), M Rezaee M(3), Ghasemi M(2), C Green R(3).

Author information:
(1)Department of Computer Science, Bowling Green State University, Bowling 
Green, OH, 43403, USA. ikhazra@bgsu.edu.
(2)Department of Communication Sciences and Disorders, Bowling Green State 
University, Bowling Green, OH, 43403, USA.
(3)Department of Computer Science, Bowling Green State University, Bowling 
Green, OH, 43403, USA.

Comment in
    Eur Arch Otorhinolaryngol. 2025 Sep;282(9):4923-4924. doi: 
10.1007/s00405-025-09496-5.

BACKGROUND: Voice disorders (VD) are often linked to vocal fold structural 
pathologies (VFSP). Laryngeal imaging plays a vital role in assessing VFSPs and 
VD in clinical and research settings, but challenges like scarce and imbalanced 
datasets can limit the generalizability of findings. Denoising Diffusion 
Probabilistic Models (DDPMs), a subtype of Generative AI, has gained attention 
for its ability to generate high-quality and realistic synthetic images to 
address these challenges.
PURPOSE: This study explores the feasibility of improving VFSP image 
classification by generating synthetic images using DDPMs.
METHODS: 404 laryngoscopic images depicting VF without and with VFSP were 
included. DDPMs were used to generate synthetic images to augment the original 
dataset. Two convolutional neural network architectures, VGG16 and ResNet50, 
were applied for model training. The models were initially trained only on the 
original dataset. Then, they were trained on the augmented datasets. Evaluation 
metrics were analyzed to assess the performance of the models for both binary 
classification (with/without VFSPs) and multi-class classification (seven 
specific VFSPs).
RESULTS: Realistic and high-quality synthetic images were generated for dataset 
augmentation. The model first failed to converge when trained only on the 
original dataset, but they successfully converged and achieved low loss and high 
accuracy when trained on the augmented datasets. The best performance was gained 
for both binary and multi-class classification when the models were trained on 
an augmented dataset.
CONCLUSION: Generating realistic images of VFSP using DDPMs is feasible and can 
enhance the classification of VFSPs by an AI model and may support VD screening 
and diagnosis.

© 2025. The Author(s).

DOI: 10.1007/s00405-025-09443-4
PMCID: PMC12399448
PMID: 40380991 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflicts of Interest: The authors 
confirm that the investigation was carried out without any existing commercial 
or financial affiliations that could be perceived as a potential conflict of 
interest.


405. J Adv Nurs. 2025 Jul;81(7):3859-3867. doi: 10.1111/jan.16542. Epub 2024 Oct 18.

Development and Validation of an Artificial Intelligence-Assisted Patient 
Education Material for Ostomy Patients: A Methodological Study.

Yüceler Kaçmaz H(1), Kahraman H(1), Akutay S(1), Dağdelen D(2).

Author information:
(1)Department of Surgical Nursing, Faculty of Health Sciences, Erciyes 
University, Kayseri, Turkey.
(2)Department of Public Health Nursing, Faculty of Health Sciences, Erciyes 
University, Kayseri, Turkey.

AIM: To develop and test the validity of an artificial intelligence-assisted 
patient education material for ostomy patients.
DESIGN: A methodological study.
METHODS: The study was carried out in two main stages and five steps: (1) 
determining the information needs of ostomy patients, (2) creating educational 
content, (3) converting the educational content into patient education material, 
(4) validation of patient education material based on expert review and (5) 
measuring the readability of the patient education material. We used ChatGPT 4.0 
to determine the information needs and create patient education material 
content, and Publuu Online Flipbook Maker was used to convert the educational 
content into patient education material. Understandability and applicability 
scores were assessed using the Patient Education Materials Assessment Tool 
submitted to 10 expert reviews. The tool inter-rater reliability was determined 
via the intraclass correlation coefficient. Readability was analysed using the 
Flesch-Kincaid Grade Level, Gunning Fog Index and Simple Measure of Gobbledygook 
formula.
RESULTS: The mean Patient Education Materials Assessment Tool understandability 
score of the patient education material was 81.91%, and the mean Patient 
Education Materials Assessment Tool actionability score was 85.33%. The scores 
for the readability indicators were calculated to be Flesch-Kincaid Grade Level: 
8.53, Gunning Fog: 10.9 and Simple Measure of Gobbledygook: 7.99.
CONCLUSIONS: The AI-assisted patient education material for ostomy patients 
provided accurate information with understandable and actionable responses to 
patients, but is at a high reading level for patients.
IMPLICATIONS FOR THE PROFESSION AND PATIENT CARE: Artificial 
intelligence-assisted patient education materials can significantly increase 
patient information rates in the health system regarding ease of practice. 
Artificial intelligence is currently not an option for creating patient 
education material, and their impact on the patient is not fully known.
REPORTING METHOD: The study followed the STROBE checklist guidelines.
PATIENT OR PUBLIC CONTRIBUTION: No patient or public contributions.

© 2024 The Author(s). Journal of Advanced Nursing published by John Wiley & Sons 
Ltd.

DOI: 10.1111/jan.16542
PMCID: PMC12159406
PMID: 39422196 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


406. Bioethics. 2025 Feb;39(2):163-169. doi: 10.1111/bioe.13365. Epub 2024 Oct 30.

Accuracy is inaccurate: Why a focus on diagnostic accuracy for medical chatbot 
AIs will not lead to improved health outcomes.

Milford SR(1)(2).

Author information:
(1)Institute for Biomedical Ethics, Basel University, Basel, Switzerland.
(2)Department of Theology, North-West University, Potchefstroom, South Africa.

Since its launch in November 2022, ChatGPT has become a global phenomenon, 
sparking widespread public interest in chatbot artificial intelligences (AIs) 
generally. While not approved for medical use, it is capable of passing all 
three United States medical licensing exams and offers diagnostic accuracy 
comparable to a human doctor. It seems inevitable that it, and tools like it, 
are and will be used by the general public to provide medical diagnostic 
information or treatment plans. Before we are taken in by the promise of a 
golden age for chatbot medical AIs, it would be wise to consider the 
implications of using these tools as either supplements to, or substitutes for, 
human doctors. With the rise of publicly available chatbot AIs, there has been a 
keen focus on research into the diagnostic accuracy of these tools. This, 
however, has left a notable gap in our understanding of the implications for 
health outcomes of these tools. Diagnosis accuracy is only part of good health 
care. For example, crucial to positive health outcomes is the doctor-patient 
relationship. This paper challenges the recent focus on diagnostic accuracy by 
drawing attention to the causal relationship between doctor-patient 
relationships and health outcomes arguing that chatbot AIs may even hinder 
outcomes in numerous ways including subtracting the elements of perception and 
observation that are crucial to clinical consultations. The paper offers brief 
suggestions to improve chatbot medical AIs so as to positively impact health 
outcomes.

© 2024 The Author(s). Bioethics published by John Wiley & Sons Ltd.

DOI: 10.1111/bioe.13365
PMCID: PMC11754992
PMID: 39474916 [Indexed for MEDLINE]


407. Front Aging Neurosci. 2026 Jan 5;17:1735892. doi: 10.3389/fnagi.2025.1735892. 
eCollection 2025.

AI agents in Alzheimer's disease management: challenges and future directions.

Grammenos G(1), Vrahatis AG(1)(2), Lazaros K(1), Exarchos TP(1)(2), Vlamos 
P(1)(2), Krokidis MG(1)(2).

Author information:
(1)Bioinformatics and Human Electrophysiology Laboratory, Department of 
Informatics, Ionian University, Corfu, Greece.
(2)Institute of Digital Biomedicine, Ionian University Research and Innovation 
Center, Corfu, Greece.

Neurodegenerative diseases such as Alzheimer's and Parkinson's disease pose a 
major global healthcare challenge, with cases projected to rise sharply as 
populations age and effective treatments remain limited. AI has shown promise in 
supporting diagnostics, predicting disease progression, and exploring 
biomarkers, yet most current tools are narrowly focused, unimodal, and lack 
longitudinal reasoning or interpretability. By enabling context-aware analysis 
across imaging, genomics, cognitive, and behavioral data, agentic AI can track 
disease progression, identify therapeutic targets, and support clinical 
decision-making. Over time, these systems may detect gaps in their own 
information and request targeted data, moving closer to real clinical reasoning 
while keeping clinicians in control. The next frontier in medical AI lies in 
developing autonomous, multimodal agents capable of integrating diverse data, 
adapting through experience, supporting decision-making, and collaborating with 
clinicians. Furthermore, ethical, patient-centered AI requires close 
technical-clinical collaboration to support clinicians and improve patient 
outcomes. This perspective examines AI's current role in Alzheimer's care, 
identifies key challenges in integration, interpretability, and regulation, and 
explores pathways for safely deploying these agentic systems in clinical 
practice.

Copyright © 2026 Grammenos, Vrahatis, Lazaros, Exarchos, Vlamos and Krokidis.

DOI: 10.3389/fnagi.2025.1735892
PMCID: PMC12812870
PMID: 41561816

Conflict of interest statement: The author(s) declared that this work was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


408. J Craniofac Surg. 2025 May 1;36(3):831-835. doi: 10.1097/SCS.0000000000010575. 
Epub 2024 Sep 2.

Evaluating the Efficacy of Large Language Models in CPT Coding for Craniofacial 
Surgery: A Comparative Analysis.

Isch EL(1), Sarikonda A(2), Sambangi A(2), Carreras A(2), Sircar A(2), Self 
DM(3), Habarth-Morales TE(2), Caterson EJ(4), Aycart M(4).

Author information:
(1)Department of General Surgery, Thomas Jefferson University.
(2)Sidney Kimmel Medical College at Thomas Jefferson University.
(3)Department of Neurosurgery, Thomas Jefferson University and Jefferson 
Hospital for Neuroscience, Philadelphia, PA.
(4)Department of Surgery, Division of Plastic Surgery, Nemours Children's 
Hospital Wilmington, DE.

BACKGROUND: The advent of Large Language Models (LLMs) like ChatGPT has 
introduced significant advancements in various surgical disciplines. These 
developments have led to an increased interest in the utilization of LLMs for 
Current Procedural Terminology (CPT) coding in surgery. With CPT coding being a 
complex and time-consuming process, often exacerbated by the scarcity of 
professional coders, there is a pressing need for innovative solutions to 
enhance coding efficiency and accuracy.
METHODS: This observational study evaluated the effectiveness of 5 publicly 
available large language models-Perplexity.AI, Bard, BingAI, ChatGPT 3.5, and 
ChatGPT 4.0-in accurately identifying CPT codes for craniofacial procedures. A 
consistent query format was employed to test each model, ensuring the inclusion 
of detailed procedure components where necessary. The responses were classified 
as correct, partially correct, or incorrect based on their alignment with 
established CPT coding for the specified procedures.
RESULTS: The results indicate that while there is no overall significant 
association between the type of AI model and the correctness of CPT code 
identification, there are notable differences in performance for simple and 
complex CPT codes among the models. Specifically, ChatGPT 4.0 showed higher 
accuracy for complex codes, whereas Perplexity.AI and Bard were more consistent 
with simple codes.
DISCUSSION: The use of AI chatbots for CPT coding in craniofacial surgery 
presents a promising avenue for reducing the administrative burden and 
associated costs of manual coding. Despite the lower accuracy rates compared 
with specialized, trained algorithms, the accessibility and minimal training 
requirements of the AI chatbots make them attractive alternatives. The study 
also suggests that priming AI models with operative notes may enhance their 
accuracy, offering a resource-efficient strategy for improving CPT coding in 
clinical practice.
CONCLUSIONS: This study highlights the feasibility and potential benefits of 
integrating LLMs into the CPT coding process for craniofacial surgery. The 
findings advocate for further refinement and training of AI models to improve 
their accuracy and practicality, suggesting a future where AI-assisted coding 
could become a standard component of surgical workflows, aligning with the 
ongoing digital transformation in health care.

Copyright © 2024 by Mutaz B. Habal, MD.

DOI: 10.1097/SCS.0000000000010575
PMID: 39221924 [Indexed for MEDLINE]

Conflict of interest statement: The authors report no conflicts of interest


409. Health Informatics J. 2025 Jul-Sep;31(3):14604582251381233. doi: 
10.1177/14604582251381233. Epub 2025 Sep 17.

An innovative X-RAG technique combined with GPT-4o for summarizing medical 
information from EHR and EMR to assist doctors in clinical decision-making 
effectively and efficiently.

Wang JF(1), Chang CC(1), Chiang TM(2), Yeh TC(3), Cheng E(4), Lee YT(2), Chen 
HI(2).

Author information:
(1)Department of Electrical Engineering, National Cheng Kung University, Tainan, 
Taiwan.
(2)Doctors' Doctor Clinic, Taipei, Taiwan.
(3)DeepWave Co. Ltd, Taipei, Taiwan.
(4)C-Media Electronics Inc, Taipei, Taiwan.

Background: Large language models (LLM) still face challenges in accurately 
extracting and summarizing medical information from EHR and EMR. The variability 
in EHR and EMR formats across institutions further complicates information 
integration. Moreover, doctors need to spend a lot of time reviewing patient 
information, which affects the efficiency and effectiveness of clinical 
decision-making. Objective: This study aims to develop a medical record 
summarization system that uses the innovative X-RAG technique with GPT-4o to 
extract medical information from EHR and EMR and convert them into structured 
FHIR format. The system ultimately generates a doctor-friendly report to improve 
the efficiency and effectiveness of clinical decision-making. Methods: We 
propose an innovative X-RAG, which adds page-based chunking, chunk filtering, 
and guided extraction prompting to the basic framework of RAG and combines it 
with GPT-4o to extract medical measurement data, diagnostic reports, and 
medication history records from EHR and EMR with high accuracy. Results: The 
system achieved 96.5% accuracy in medical data extraction and reduced 
approximately 40% of the time doctors spend reviewing patient information in 
clinical applications. Conclusion: The proposed system improves the efficiency 
and effectiveness of clinical decision-making and provides a valuable tool to 
optimize medical information management and clinical workflows.

DOI: 10.1177/14604582251381233
PMID: 40961463 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of conflicting interestsThe authors 
declared no potential conflicts of interest with respect to the research, 
authorship, and/or publication of this article.


410. BMC Prim Care. 2025 Sep 22;26(1):284. doi: 10.1186/s12875-025-02963-2.

Artificial intelligence in the prescription of acute medical treatments in 
primary healthcare - comparison of the performance of family physicians and 
ChatGPT.

Simão BLP(1), Pereira CM(2), Jácome M(3), Oliveira C(3), Ferreira LM(4), Paiva 
JM(4), Braga C(2), Cardoso CS(5)(6).

Author information:
(1)Faculdade de Medicina, Universidade de Coimbra, Coimbra, Portugal. 
barbara12simao@gmail.com.
(2)Unidade de Saúde Familiar Norton de Matos, Coimbra, Portugal.
(3)Unidade de Saúde Familiar Condeixa, Coimbra, Portugal.
(4)Unidade de Saúde Familiar Briosa, Coimbra, Portugal.
(5)Faculdade de Medicina, Universidade de Coimbra, Coimbra, Portugal.
(6)Coimbra Institute for Clinical and Biomedical Research (iCBR), Coimbra, 
Portugal.

INTRODUCTION: Artificial intelligence (AI) is increasingly being recognized as a 
transformative force in healthcare, showing significant promise in supporting 
healthcare professionals. AI has many applications in healthcare, including 
providing real-time decision support, diagnosing diseases, and advancing 
personalized medicine. However, clinical trials and further research are needed 
to evaluate the practical effectiveness of AI in primary healthcare.
OBJECTIVE OF THE STUDY: This study aims to assess the accuracy of ChatGPT, an 
AI-powered chatbot, in therapeutic decision-making during acute disease 
consultations in primary care and compare its performance to that of general 
family physicians. The goal was to determine how well ChatGPT could replicate 
the decisions made by physicians based on standard clinical guidelines.
MATERIALS AND METHODS: A cross-sectional study was conducted at three primary 
healthcare units in the Central Region of Portugal. The analysis involved three 
phases: (1) collecting data from healthcare professionals, (2) gathering 
therapeutic proposals from ChatGPT v3.5 based on physician-defined diagnoses, 
and (3) comparing the treatments proposed by both ChatGPT v3.5 and the 
physicians, using the Dynamed platform as the gold standard for correct 
prescriptions.
RESULTS: Out of a total of 860 consultations, 138 were excluded due to 
non-compliance with the inclusion criteria. The analysis showed that the 
diagnostic accuracy of ChatGPT v3.5 and physicians co-occurred in 26.2% of 
cases. In 29.1% of cases, there was no agreement between the AI and the 
physicians' diagnoses. The therapeutic decisions made by ChatGPT v3.5 were 
correct in 55.6% of the cases, while physicians made correct decisions in 54.3% 
of the cases. The therapeutic decisions of ChatGPT v3.5 were incorrect in 5.2% 
of the cases, compared to 11% for physicians. Furthermore, the therapeutic 
proposals of ChatGPT v3.5 were 'approximate' to the correct treatment in 24% of 
the cases, while physicians had a 17.1% approximation rate.
CONCLUSION: This study suggests that AI - specifically ChatGPT v3.5 - can match 
or even outperform physicians in terms of therapeutic decision accuracy, with a 
similar or slightly better success rate than human doctors. This highlights the 
potential for AI to act as an effective auxiliary tool rather than a replacement 
for healthcare professionals. AI is most effective when used in collaboration 
with healthcare professionals, augmenting their capabilities and improving 
overall healthcare delivery. Ultimately, AI can serve as a powerful aid to 
healthcare professionals, helping improve patient care and healthcare outcomes, 
particularly in primary care.

© 2025. The Author(s).

DOI: 10.1186/s12875-025-02963-2
PMCID: PMC12455777
PMID: 40983921 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was conducted in accordance with the ethical principles 
outlined in the Declaration of Helsinki. Informed consent was waived by Ethics 
Committee of the Regional Health Administration of the Centre– Portugal 
(reference code: 130/2023). Authorization was requested from the healthcare 
units involved in the study, through their respective unit coordinators. As the 
data was accessed by physicians affiliated with each unit and the information 
provided to the investigators was anonymized, informed consent from the patients 
to whom the records belonged was not obtained. The study was submitted to the 
ethics committee for opinion and received approval from Ethics Committee of the 
Regional Health Administration of the Centre– Portugal (reference code: 
130/2023). Consent for publication: Not applicable. Competing interests: The 
authors declare no competing interests.


411. J Biomed Inform. 2025 Nov;171:104936. doi: 10.1016/j.jbi.2025.104936. Epub 2025 
Oct 13.

A LangChain-based pipeline for one-shot synthetic text generation using 
generative pre-trained transformers in palliative care research.

Ronan I(1), Crowley P(2), Rombouts E(3), Cornally N(2), Saab MM(2), Murphy D(2), 
Tabirca S(2).

Author information:
(1)University College Cork, College Road, Cork, T12 K8AF, Cork, Ireland. 
Electronic address: i.ronan@cs.ucc.ie.
(2)University College Cork, College Road, Cork, T12 K8AF, Cork, Ireland.
(3)Rombouts Ouderengeneeskunde, Amsterdam, Netherlands.

OBJECTIVE: As the world's population ages, nursing homes are of increasing 
importance. In order to care for a growing number of older adults, intelligent 
technologies are needed. Artificial Intelligence can be utilised to enhance 
palliative care in nursing homes. However, the data needed to train artificially 
intelligent agents is lacking within this sensitive domain due to privacy 
issues. Therefore, it is difficult for researchers to develop technological 
solutions. With the advent of large language models, such as ChatGPT, new text 
generation methods are made possible using limited data. In this pilot study, we 
investigate the use of large language models to generate synthetic data.
METHODS: We investigate the feasibility of using GPT-3.5 and GPT-4o models along 
with one-shot prompting to produce synthetic nurse notes which faithfully 
describe nursing home residents with met or unmet palliative care needs. We used 
LangChain to create a repeatable pipeline which can be adapted to different 
use-cases. We also compare the performance of both models using a set of 
qualitative and quantitative evaluations to determine which set of notes is more 
suitable for subsequent research.
RESULTS: GPT-3.5 performed slightly better than GPT-4o in our qualitative 
healthcare professional analysis. Quantitative analysis revealed appropriately 
heterogenous results across contextual similarity, lexical overlap, sentiment, 
and readability scores.
CONCLUSION: Our work is the first investigation of such a generation method in 
the nursing home palliative care domain. Further refinement and validation of 
such data is needed in order to ensure the safe use of our approach.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104936
PMID: 41093122 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


412. BMC Med Educ. 2025 Aug 23;25(1):1193. doi: 10.1186/s12909-025-07856-7.

Large language models underperform in European general surgery board 
examinations: a comparative study with experts and surgical residents.

Gül MC(1).

Author information:
(1)Department of Gastrointestinal Surgery, Afyonkarahisar State Hospital, 
Afyonkarahisar, Türkiye. opdrmelihcangul@gmail.com.

BACKGROUND: Artificial intelligence (AI) has become a transformative tool in 
medical education and assessment. Despite advancements, AI models such as GPT-4o 
demonstrate variable performance on high-stakes examinations. This study 
compared the performance of four AI models (Llama-3, Gemini, GPT-4o, and 
Copilot) with specialists and residents on European General Surgery Board test 
questions, focusing on accuracy across question formats, lengths, and difficulty 
levels.
METHODS: A total of 120 multiple-choice questions were systematically sampled 
from the General Surgery Examination and Board Review question bank using a 
structured randomization protocol. The questions were administered via Google 
Forms to four large language models (Llama-3, GPT-4o, Gemini, and Copilot) and 
30 surgeons (15 board-certified specialists and 15 residents) under timed, 
single-session conditions. Participant demographics (age, gender, years of 
experience) were recorded. Questions were categorized by word count (short, 
medium, long) and by difficulty level (easy, moderate, hard), rated by three 
independent board-certified surgeons. Group accuracy rates were compared using 
ANOVA with appropriate post-hoc tests, and 95% confidence intervals were 
reported.
RESULTS: Board-certified surgeons achieved the highest accuracy rate at 81.6% 
(95% CI: 78.9-84.3), followed by surgical residents at 69.9% (95% CI: 
66.7-73.1). Among large language models (LLMs), Llama-3 demonstrated the best 
performance with an accuracy of 65.8% (95% CI: 62.4-69.2), whereas Copilot 
showed the lowest performance at 51.7% (95% CI: 48.1-55.3). LLM performance 
declined significantly as item difficulty and length increased, particularly for 
Copilot (68.3% on short vs. 36.4% on long questions, p < 0.001). In contrast, 
human participants maintained relatively stable accuracy across difficulty 
levels. Notably, only Llama-3 ranked within the human performance range, placing 
26th among 30 surgeons, while all other LLMs failed to surpass the 60% accuracy 
threshold (p < 0.001).
CONCLUSION: Current LLMs underperform compared to human specialists when faced 
with questions requiring high-level medical knowledge, reinforcing their current 
role as supplementary tools in surgical education rather than replacements for 
expert clinical judgment.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-07856-7
PMCID: PMC12374362
PMID: 40849634 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The author declare that this report does not contain any personal 
information that could lead to the identification of the patients. Competing 
interests: The authors declare no competing interests.


413. Radiol Med. 2025 Sep;130(9):1472-1482. doi: 10.1007/s11547-025-02040-9. Epub 
2025 Jul 11.

Automated MRI protocoling in neuroradiology in the era of large language models.

Reiner LN(1), Chelbi M(2), Fetscher L(2), Stöckel JC(2), Csapó-Schmidt C(2), 
Guseynova S(2), Al Mohamad F(2), Bressem KK(3)(4), Nawabi J(2), Siebert E(2), 
Wattjes MP(2), Scheel M(2), Meddeb A(2)(5)(6).

Author information:
(1)Department of Neuroradiology, Charité-Universitätsmedizin Berlin, 
Augustenburger Platz 1, 13353, Berlin, Germany. lara.reiner@charite.de.
(2)Department of Neuroradiology, Charité-Universitätsmedizin Berlin, 
Augustenburger Platz 1, 13353, Berlin, Germany.
(3)Department of Radiology, Technical University Munich, Klinikum Rechts Der 
Isar, Ismaninger Str. 22, 81675, Munich, Germany.
(4)Department or Radiology and Nuclear Medicine, Technical University Munich, 
German Heart Center Munich, Lazarethstr. 36, 80636, Munich, Germany.
(5)Department of Neuroradiology, Hôpital Maison-Blanche, CHU Reims, Université 
Reims-Champagne-Ardenne, Reims, France.
(6)Berlin Institute of Health at Charité-Universitätsmedizin Berlin, 
Charitéplatz 1, 10117, Berlin, Germany.

PURPOSE: This study investigates the automation of MRI protocoling, a routine 
task in radiology, using large language models (LLMs), comparing an open-source 
(LLama 3.1 405B) and a proprietary model (GPT-4o) with and without 
retrieval-augmented generation (RAG), a method for incorporating domain-specific 
knowledge.
MATERIAL AND METHODS: This retrospective study included MRI studies conducted 
between January and December 2023, along with institution-specific protocol 
assignment guidelines. Clinical questions were extracted, and a neuroradiologist 
established the gold standard protocol. LLMs were tasked with assigning MRI 
protocols and contrast medium administration with and without RAG. The results 
were compared to protocols selected by four radiologists. Token-based symmetric 
accuracy, the Wilcoxon signed-rank test, and the McNemar test were used for 
evaluation.
RESULTS: Data from 100 neuroradiology reports (mean age = 54.2 years ± 18.41, 
women 50%) were included. RAG integration significantly improved accuracy in 
sequence and contrast media prediction for LLama 3.1 (Sequences: 38% vs. 70%, 
P < .001, Contrast Media: 77% vs. 94%, P < .001), and GPT-4o (Sequences: 43% vs. 
81%, P < .001, Contrast Media: 79% vs. 92%, P = .006). GPT-4o outperformed LLama 
3.1 in MRI sequence prediction (81% vs. 70%, P < .001), with comparable 
accuracies to the radiologists (81% ± 0.21, P = .43). Both models equaled 
radiologists in predicting contrast media administration (LLama 3.1 RAG: 94% vs. 
91% ± 0.2, P = .37, GPT-4o RAG: 92% vs. 91% ± 0.24, P = .48).
CONCLUSION: Large language models show great potential as decision-support tools 
for MRI protocoling, with performance similar to radiologists. RAG enhances the 
ability of LLMs to provide accurate, institution-specific protocol 
recommendations.

© 2025. The Author(s).

DOI: 10.1007/s11547-025-02040-9
PMCID: PMC12454495
PMID: 40643871 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare no conflict of interests. Ethical approval: The study was conducted in 
accordance with the latest version of the Declaration of Helsinki. The study was 
approved by the ethics committee of the university (No. EA4/062/20). The need 
for informed consent was waived due to the retrospective nature of the research.


414. Nutrients. 2025 Dec 20;18(1):23. doi: 10.3390/nu18010023.

Large Language Models for Real-World Nutrition Assessment: Structured Prompts, 
Multi-Model Validation and Expert Oversight.

Ase A(1), Borowicz J(2), Rakocy K(3), Piekarska B(2).

Author information:
(1)Department of Internal Medicine, Hypertension and Vascular Diseases, Medical 
University of Warsaw, 02-097 Warsaw, Poland.
(2)Department of the Prevention of Environmental Hazard, Allergology and 
Immunology, Faculty of Health Sciences, Medical University of Warsaw, 02-091 
Warsaw, Poland.
(3)KR Consulting, 02-601 Warsaw, Poland.

Background: Traditional dietary assessment methods face limitations including 
reporting bias and scalability challenges. Large language models (LLMs) offer 
potential for automated food classification, yet their validation in 
morphologically complex, non-English languages like Polish remains limited. 
Methods: We analyzed 1992 food items from a Polish long-term care facility 
(LTCF) cohort using three advanced LLMs (Claude Opus 4.5, Gemini 3 pro, and 
GPT-5.1-chat-latest) with two prompting strategies: a structured double-step 
prompt integrating NOVA and World Health Organization (WHO) criteria, and a 
simplified single-step prompt. Classifications were compared against consensus 
judgments from two human experts. Results: All LLMs showed high agreement with 
human experts (90.3-94.2%), but there were statistically significant differences 
in all pairwise comparisons (χ2 = 1174.5-1897.1; p < 0.001). The structured 
prompt produced very high Recall for UNHEALTHY items at the cost of lower 
Specificity, whereas the simplified prompt achieved higher overall Accuracy and 
a more balanced Recall-Specificity profile, indicating a trade-off between 
strict guideline adherence and alignment with general human judgment. 
Conclusions: Advanced LLMs demonstrate near-expert accuracy in Polish-language 
dietary classification, enhancing workflow efficiency by shifting effort toward 
validation. Expert oversight remains essential, and multi-model consensus 
alongside language-specific validation can improve AI reliability in nutrition 
assessment.

DOI: 10.3390/nu18010023
PMCID: PMC12788131
PMID: 41515141 [Indexed for MEDLINE]

Conflict of interest statement: Kamil Rakocy is an independent researcher 
operating as a sole trader. This entity did not provide funding or have any 
commercial role in the design, conduct, or reporting of this study. The authors 
declare that the research was conducted in the absence of any commercial or 
financial relationships that could be construed as a potential conflict of 
interest.


415. BMC Anesthesiol. 2025 Nov 19;25(1):574. doi: 10.1186/s12871-025-03451-x.

Artificial intelligence in anesthesia: comparison of the utility of ChatGPT v/s 
google gemini large language models in pre-anesthetic education: content, 
readability and sentiment analysis.

Sharma P(1), Sidhu B(2), Reddy A(3)(4), Saxena P(2), Gupta R(2), Gill RK(2), Ray 
A(2), Chauhan R(5).

Author information:
(1)Department of Neuroanaesthesiology and Neurointensive Care, Max 
Superspeciality Hospital, Saket, New Delhi, India.
(2)Department of Anesthesia and Intensive Care, Dr B.R. Ambedkar Institute of 
Medical Sciences, Punjab, Mohali, India.
(3)Department of Anesthesia and Intensive Care, Dr B.R. Ambedkar Institute of 
Medical Sciences, Punjab, Mohali, India. ashreddy02@gmail.com.
(4)Department of Anesthesia and Intensive Care, Post Graduate Institute of 
Medical Education and Research, Chandigarh, India. ashreddy02@gmail.com.
(5)Department of Anesthesia and Intensive Care, Post Graduate Institute of 
Medical Education and Research, Chandigarh, India.

BACKGROUND: Large Language Models (LLMs) such as ChatGPT and Google Gemini are 
increasingly explored for their potential in patient education, particularly in 
the perioperative setting. As text-based tools trained on extensive datasets, 
they can generate detailed responses to common medical queries. However, their 
utility in delivering reliable, understandable, and emotionally appropriate 
preanesthetic information remains unclear.
METHODS: We conducted a prospective observational study comparing ChatGPT and 
Google Gemini in generating educational content for patients undergoing 
laparoscopic cholecystectomy. From 68 patient questions submitted by 
anesthesiologists, 13 high-relevance items were selected. Responses from both 
models were independently rated by 20 anesthesiologists using a 5-point Likert 
scale across four domains: accuracy, comprehensiveness, clarity, and safety. 
Mixed-effects ordinal regression with random intercepts for rater and question 
estimated odds ratios (OR) and 95% confidence intervals (CI) for ChatGPT versus 
Gemini. Readability was assessed using standard linguistic indices, and 
sentiment analysis was performed. Inter-rater reliability was evaluated using 
Krippendorff's α.
RESULTS: ChatGPT had significantly higher odds of receiving better scores for 
accuracy (OR 2.32, 95% CI 1.62-3.32, p < 0.001) and comprehensiveness (OR 2.38, 
95% CI 1.67-3.37, p < 0.001), with no differences for clarity (OR 1.05, 95% CI 
0.75-1.47) or safety (OR 1.01, 95% CI 0.72-1.43). Gemini generated text with 
greater readability, demonstrated by a lower Flesch-Kincaid Grade level 
(p = 0.04) and higher Flesch Reading Ease score (p = 0.04). Sentiment analysis 
showed Gemini responses contained a wider emotional range, while ChatGPT 
responses were more neutral overall. Inter-rater reliability was low across 
domains (Krippendorff's α 0.23-0.46).
CONCLUSION: ChatGPT produced more accurate and comprehensive perioperative 
anesthesia information, whereas Gemini offered greater readability and emotional 
expressiveness. Both models may serve as adjuncts in preanesthetic patient 
education but are not substitutes for clinician counselling. Larger, 
multi-center studies incorporating direct patient testing are warranted to 
validate these findings.

© 2025. The Author(s).

DOI: 10.1186/s12871-025-03451-x
PMCID: PMC12628519
PMID: 41257612 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The study conformed to the standards of the Declaration of Helsinki 
and was approved by the Institute Research Committee (AIMS/IRC-study 12/2024). 
Consent for publication: This prospective observational study took place at a 
tertiary care hospital from September to November 2024 after approval of the 
institute research committee (AIMS/IRC-study 12/2024). As this study did not 
involve direct participation of patients and due to its nature, formal ethics 
committee approval was not required. However, the research was conducted in full 
compliance with ethical standards, maintaining strict confidentiality and 
anonymity of all data. Consent by participating anesthesiologists was implied, 
and a detailed information sheet was also enclosed. Consent for publication: 
Informed consent has been obtained from all the anesthesiologists participating 
in this study, including consent for publication. Competing interests: The 
authors declare no competing interests. The authors have no relevant financial 
or non-financial interests to disclose.


416. Int J Surg. 2026 Jan 1;112(1):1886-1890. doi: 10.1097/JS9.0000000000003492. Epub 
2025 Sep 10.

Guideline adherence in surgical decisions for T1 colorectal cancer after 
endoscopic resection: large language models vs clinicians.

Zeng L(1), Cao Q(2)(3), Deng J(1), Hu J(1), Pang M(2), Liu F(1).

Author information:
(1)Digestive Endoscopy Center, Shanghai Tenth People's Hospital, Tongji 
University School of Medicine, Shanghai, China.
(2)Department of Geriatric General Surgery, Sichuan Provincial People's 
Hospital, School of Medicine, University of Electronic Science and Technology of 
China, Chengdu, China.
(3)Department of Gastrointestinal Surgery, The First People's Hospital of Yibin, 
Yibin, China.

BACKGROUND: Patients with T1 colorectal cancer (CRC) often show poor adherence 
to guideline-recommended treatment strategies after endoscopic resection. To 
address this challenge and improve clinical decision-making, this study aims to 
compare the accuracy of surgical management recommendations between large 
language models (LLMs) and clinicians.
METHODS: This retrospective study enrolled 202 patients with T1 CRC who 
underwent endoscopic resection at three hospitals. We compared the 
decision-making accuracy of two representative LLMs (ChatGPT-4o and DeepSeek) 
with that of 29 clinicians in determining whether additional surgery was 
required after endoscopic resection of T1 CRC. To optimize the inputs for the 
LLMs, we applied a prompt-engineering strategy that combined role prompting, 
in-context learning, and few-shot learning.
RESULTS: In clinical practice, we found that the guideline adherence rate after 
endoscopic resection for T1 CRC was below 80%. We analyzed the pathology reports 
of 200 patients with T1 CRC, and the results showed that both ChatGPT-4o and 
DeepSeek significantly outperformed clinical physicians. Subgroup analyses 
demonstrated that LLMs outperformed doctors regardless of years of experience or 
professional background. Additionally, the use of Chinese or English input had 
no significant impact on the performance of the LLMs.
CONCLUSION: This study highlights the potential of LLMs to improve guideline 
adherence in the post-endoscopic resection management of T1 CRC.

Copyright © 2025 The Author(s). Published by Wolters Kluwer Health, Inc.

DOI: 10.1097/JS9.0000000000003492
PMCID: PMC12825533
PMID: 40928382 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that they have no conflicts 
of interest.


417. J Crit Care. 2026 Feb;91:155236. doi: 10.1016/j.jcrc.2025.155236. Epub 2025 Aug 
20.

When shortcuts fall short: The hidden danger of abbreviations in critical care.

Berger S(1), Grzonka P(2), Hunziker S(3), Frei AI(2), Sutter R(4).

Author information:
(1)Clinic for Intensive Care, Department of Acute Medicine, University Hospital 
Basel, Basel, Switzerland. Electronic address: sebastian.berger@usb.ch.
(2)Clinic for Intensive Care, Department of Acute Medicine, University Hospital 
Basel, Basel, Switzerland.
(3)Medical Communication and Psychosomatic Medicine, University Hospital Basel, 
Switzerland; Medical faculty, University of Basel, Basel, Switzerland.
(4)Clinic for Intensive Care, Department of Acute Medicine, University Hospital 
Basel, Basel, Switzerland; Medical Communication and Psychosomatic Medicine, 
University Hospital Basel, Switzerland; Department of Clinical Research, 
University of Basel, Basel, Switzerland.

OBJECTIVES: Effective critical care communication is vital for patient safety, 
yet the risks of ambiguous abbreviations and acronyms or initialisms in clinical 
communication remain understudied. This narrative review aims to identify 
potentially ambiguous abbreviations and acronyms in critical care and evaluate 
their potential implications for clinical safety and communication quality. From 
this, educational interventions and standardized protocols could be devised to 
optimize communication.
METHODS: PubMed and Embase databases were used to identify studies and reports 
on the use of abbreviations and acronyms in medical care and references of 
identified publications were screened. Additionally, the large language model 
"Chat Generative Pre-Trained Transformer-4omni" was used to generate a list of 
ambiguous terms used in critical care.
RESULTS: We identified 52 ambiguous acronyms and initialisms, and 24 
abbreviations used in critical care with multiple meanings, risking critical 
errors during communication and ward transfers. These ambiguities stem from 
specialty, context, or institutional differences. The literature discusses 
optimization of communication in critical care during hand-offs or more complex 
solutions including auto-expansion software or protocols. Only few studies 
discussed the dangers of medication errors or misunderstandings due to 
abbreviation use. Abbreviations contribute to as much as 13 % of medication 
errors. Strategies are proposed to reduce abbreviation-related errors including 
spelling out terms initially, using closed-loop communication, standardized 
unit-approved lists, regular education, and avoiding jargon.
CONCLUSIONS: Ambiguous abbreviations and acronyms pose a significant threat to 
safe and effective communication in critical care. Standardized terminology, 
education, and clear documentation practices are urgently needed to mitigate 
these risks and improve patient safety.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jcrc.2025.155236
PMID: 40839977 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest There are no 
competing interests.


418. Int J Obstet Anesth. 2025 May;62:104344. doi: 10.1016/j.ijoa.2025.104344. Epub 
2025 Feb 15.

Online patient information on labour epidural analgesia provided by UK National 
Health Services hospitals: assessment of readability and improvement of 
readability standards using ChatGPT.

Weber V(1), Truelove A(2), Gohari SS(3), Wynne LA(2), Kearns RJ(4), O'Carroll 
JE(5).

Author information:
(1)Department of Anaesthesia and Perioperative Medicine, University College 
London Hospitals, London, UK; Department of Anaesthesia, Whittington Health NHS 
Trust, London, UK. Electronic address: valentin.weber@doctors.org.uk.
(2)Department of Anaesthesia and Perioperative Medicine, University College 
London Hospitals, London, UK; Department of Anaesthesia, Whittington Health NHS 
Trust, London, UK.
(3)St Georges University Hospital NHS Foundation Trust, London, UK.
(4)Department of Anaesthesia, Glasgow Royal Infirmary, Glasgow, UK.
(5)Department of Anaesthesia and Perioperative Medicine, University College 
London Hospitals, London, UK.

BACKGROUND: Labour epidurals are considered the gold standard for labour 
analgesia in pregnant patients. Inequities in health literacy levels can 
negatively impact understanding of online patient education materials, 
potentially affecting uptake of labour epidural analgesia. Generative artificial 
technology such as ChatGPT may be able to improve readability of patient 
information materials.
OBJECTIVES: Firstly, to assess the readability of available online materials on 
labour epidurals in the United Kingdom (UK). Secondly, to evaluate the ability 
of generative artificial technology to improve readability.
METHODS: All UK public hospitals' websites performing obstetric anaesthesia were 
searched for patient education materials relating to labour epidurals. A 
readability assessment was conducted using three readability scoring systems. 
ChatGPT was used to rewrite content of online patient information material on 
labour epidural analgesia to be understandable by an individual with the health 
literacy level of an 11-year-old (sixth grade).
RESULTS: A total of 61.6% of UK hospitals provided some form of online patient 
education materials on labour analgesia and epidurals, 14.5% and 23.2% of the 
texts, met the target readability in two commonly used readability scores, 
respectively. The mean grade (8.4 ± 2.1) did not meet target readability levels. 
After AI-modification, 24.6% and 27.5% of the texts met targets using the same 
metrics, with the mean grade (7.7 ± 1.2) decreasing significantly (P <0.001), 
but still not meeting the target level.
CONCLUSION: Online patient-information on labour epidural analgesia frequently 
exceeds the recommended sixth grade reading level. ChatGPT can be used to 
enhance readability but also fails to meet recommended health literacy 
standards.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.ijoa.2025.104344
PMID: 40174427 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


419. JMIR Form Res. 2025 Dec 17;9:e84251. doi: 10.2196/84251.

Evaluating the Accuracy of Medical Information Generated by ChatGPT and Gemini 
and Its Alignment With International Clinical Guidelines From the Surviving 
Sepsis Campaign: Comparative Study.

Kutbi D(1)(2), Abou-Bakr E(3)(4), Haidar HM(2).

Author information:
(1)Department of Pharmacy, King Fahad Armed Forces Hospital, Jeddah, Saudi 
Arabia.
(2)Department of Business, Jeddah International College, Jeddah, Saudi Arabia.
(3)Department of Computer Science and Information Technology, Jeddah 
International College, Jeddah, Saudi Arabia.
(4)Department of Computer Engineering, The Higher Institute of Engineering and 
Technology, Elbehera, Egypt.

BACKGROUND: Assessment of medical information provided by artificial 
intelligence (AI) chatbots like ChatGPT and Google's Gemini and comparison with 
international guidelines is a burgeoning area of research. These AI models are 
increasingly being considered for their potential to support clinical 
decision-making and patient education. However, their accuracy and reliability 
in delivering medical information that aligns with established guidelines remain 
under scrutiny.
OBJECTIVE: This study aims to assess the accuracy of medical information 
generated by ChatGPT and Gemini and its alignment with international guidelines 
for sepsis management.
METHODS: ChatGPT and Gemini were asked 18 questions about the Surviving Sepsis 
Campaign guidelines, and the responses were evaluated by 7 independent intensive 
care physicians. The responses generated were scored as follows: 3=correct, 
complete, and accurate; 2=correct but incomplete or inaccurate; and 1=incorrect. 
This scoring system was chosen to provide a clear and straightforward assessment 
of the accuracy and completeness of the responses. The Fleiss κ test was used to 
assess the agreement between evaluators, and the Mann-Whitney U test was used to 
test for the significance of differences between the correct responses generated 
by ChatGPT and Gemini.
RESULTS: ChatGPT provided 5 (28%) perfect responses, 12 (67%) nearly perfect 
responses, and 1 (5%) low-quality response, with substantial agreement among the 
evaluators (Fleiss κ=0.656). Gemini, on the other hand, provided 3 (17%) perfect 
responses, 14 (78%) nearly perfect responses, and 1 (5%) low-quality response, 
with moderate agreement among the evaluators (Fleiss κ=0.582). The Mann-Whitney 
U test revealed no statistically significant difference between the two 
platforms (P=.48).
CONCLUSIONS: ChatGPT and Gemini both demonstrated potential for generating 
medical information. Despite their current limitations, both showed promise as 
complementary tools in patient education and clinical decision-making. The 
medical information generated by ChatGPT and Gemini still needs ongoing 
evaluation regarding its accuracy and alignment with international guidelines in 
different medical domains, particularly in the sepsis field.

©Dina Kutbi, Ehab Abou-Bakr, Hassan Mousa Haidar. Originally published in JMIR 
Formative Research (https://formative.jmir.org), 17.12.2025.

DOI: 10.2196/84251
PMCID: PMC12756662
PMID: 41406470 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


420. J Biomed Inform. 2025 Aug;168:104868. doi: 10.1016/j.jbi.2025.104868. Epub 2025 
Jul 4.

Leveraging heterogeneous tabular of EHRs with prompt learning for clinical 
prediction.

Yang X(1), Li L(2), Wang C(3), Zhang W(4), Liu H(5), Tang W(6).

Author information:
(1)State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute 
of Automation, Chinese Academy of Sciences, Beijing, 100190, China; University 
of Chinese Academy of Sciences, Beijing, 100049, China; Guangzhou University, 
Guangzhou, 510006, China. Electronic address: yangxuebing2013@ia.ac.cn.
(2)State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute 
of Automation, Chinese Academy of Sciences, Beijing, 100190, China; School of 
Information and Communication Engineering, Hainan University, Haikou, 570228, 
China.
(3)State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute 
of Automation, Chinese Academy of Sciences, Beijing, 100190, China; University 
of Chinese Academy of Sciences, Beijing, 100049, China.
(4)State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute 
of Automation, Chinese Academy of Sciences, Beijing, 100190, China; School of 
Information and Communication Engineering, Hainan University, Haikou, 570228, 
China; Guangzhou University, Guangzhou, 510006, China.
(5)School of Information and Communication Engineering, Hainan University, 
Haikou, 570228, China.
(6)Department of Geriatrics, Beijing Friendship Hospital, Capital Medical 
University, Beijing, 100050, China. Electronic address: twybzx@163.com.

Electronic Health Records (EHRs) depict patient-related information and have 
significantly contributed to advancements in healthcare fields. The abundance of 
EHR data provides exceptional opportunities for developing clinical predictive 
models. However, the heterogeneity within multi-source EHR data raises a 
difficulty to organically leverage information from structured and unstructured 
features. In this paper, we focus on the heterogeneous EHR data in the tabular 
form, and propose a Prompt learning based data Fusion framework for Tabular 
(TabPF) to extract patient representations for clinical prediction. First, we 
design a text summary generator module to convert medical tabular into vector 
representations through long text embedding. Specifically, the tailored prompt 
learning is conducted for leading the Large Language Model (LLM) to respectively 
generate appropriate text summaries for different types of tabular data. Second, 
we design a novel attention mechanism of Transformer to effectively realize 
heterogeneous data fusion and generate more comprehensive patient 
representations for downstream predictions. The experiments are performed on the 
publicly available eICU-CRD dataset and the real-world CECMed dataset containing 
elderly patients diagnosed with chronic diseases, in comparison with 
representative baseline models. The results validate the superior performance of 
TabPF in predicting severity, mortality and Length of Stay (LoS). Furthermore, 
extensive ablation study and model variants evaluations demonstrate the 
effectiveness of the key component of the proposed framework.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104868
PMID: 40619074 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


421. J Med Internet Res. 2025 May 15;27:e69004. doi: 10.2196/69004.

Breaking Digital Health Barriers Through a Large Language Model-Based Tool for 
Automated Observational Medical Outcomes Partnership Mapping: Development and 
Validation Study.

Adams MCB(1), Perkins ML(2), Hudson C(2), Madhira V(3), Akbilgic O(4), Ma D(5), 
Hurley RW(6), Topaloglu U(2)(7).

Author information:
(1)Department of Anesthesiology, Artificial Intelligence, Translational 
Neuroscience, and Public Health Sciences, Wake Forest University School of 
Medicine, Winston-Salem, NC, United States.
(2)Department of Cancer Biology, Wake Forest University School of Medicine, 
Winston-Salem, NC, United States.
(3)Palila Software LLC, Reno, NV, United States.
(4)Department of Cardiovascular Medicine, Wake Forest University School of 
Medicine, Winston-Salem, NC, United States.
(5)Department of Internal Medicine, Wake Forest University School of Medicine, 
Winston-Salem, NC, United States.
(6)Department of Anesthesiology, Translational Neuroscience, and Public Health 
Sciences, Wake Forest University School of Medicine, Winston-Salem, NC, United 
States.
(7)Clinical Translational Research Informatics Branch, National Cancer 
Institute, Bethesda, MD, United States.

BACKGROUND: The integration of diverse clinical data sources requires 
standardization through models such as Observational Medical Outcomes 
Partnership (OMOP). However, mapping data elements to OMOP concepts demands 
significant technical expertise and time. While large health care systems often 
have resources for OMOP conversion, smaller clinical trials and studies 
frequently lack such support, leaving valuable research data siloed.
OBJECTIVE: This study aims to develop and validate a user-friendly tool that 
leverages large language models to automate the OMOP conversion process for 
clinical trials, electronic health records, and registry data.
METHODS: We developed a 3-tiered semantic matching system using GPT-3 embeddings 
to transform heterogeneous clinical data to the OMOP Common Data Model. The 
system processes input terms by generating vector embeddings, computing cosine 
similarity against precomputed Observational Health Data Sciences and 
Informatics vocabulary embeddings, and ranking potential matches. We validated 
the system using two independent datasets: (1) a development set of 76 National 
Institutes of Health Helping to End Addiction Long-term Initiative clinical 
trial common data elements for chronic pain and opioid use disorders and (2) a 
separate validation set of electronic health record concepts from the National 
Institutes of Health National COVID Cohort Collaborative COVID-19 enclave. The 
architecture combines Unified Medical Language System semantic frameworks with 
asynchronous processing for efficient concept mapping, made available through an 
open-source implementation.
RESULTS: The system achieved an area under the receiver operating characteristic 
curve of 0.9975 for mapping clinical trial common data element terms. Precision 
ranged from 0.92 to 0.99 and recall ranged from 0.88 to 0.97 across similarity 
thresholds from 0.85 to 1.0. In practical application, the tool successfully 
automated mappings that previously required manual informatics expertise, 
reducing the technical barriers for research teams to participate in 
large-scale, data-sharing initiatives. Representative mappings demonstrated high 
accuracy, such as demographic terms achieving 100% similarity with corresponding 
Logical Observation Identifiers Names and Codes concepts. The implementation 
successfully processes diverse data types through both individual term mapping 
and batch processing capabilities.
CONCLUSIONS: Our validated large language model-based tool effectively automates 
the transformation of clinical data into the OMOP format while maintaining high 
accuracy. The combination of semantic matching capabilities and a 
researcher-friendly interface makes data harmonization accessible to smaller 
research teams without requiring extensive informatics support. This has direct 
implications for accelerating clinical research data standardization and 
enabling broader participation in initiatives such as the National Institutes of 
Health Helping to End Addiction Long-term Initiative Data Ecosystem.

©Meredith CB Adams, Matthew L Perkins, Cody Hudson, Vithal Madhira, Oguz 
Akbilgic, Da Ma, Robert W Hurley, Umit Topaloglu. Originally published in the 
Journal of Medical Internet Research (https://www.jmir.org), 15.05.2025.

DOI: 10.2196/69004
PMCID: PMC12123247
PMID: 40146872 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: MCBA has received 
National Institutes of Health (NIH) grants as listed (NIH grant not related to 
this work: U01DA057016). RWH has received NIH grants as listed (NIH grants not 
related to this work: UH3AR077360); grants from Nevro, Inc (research funds to 
the institution on the topic of painful diabetic neuropathy and not related to 
this work); and grants from State Farm (expert/consulting not related to this 
work).


422. J Infect Public Health. 2025 Feb;18(2):102627. doi: 10.1016/j.jiph.2024.102627. 
Epub 2024 Dec 18.

Evaluation of ChatGPT-4 for the detection of surgical site infections from 
electronic health records after colorectal surgery: A pilot diagnostic accuracy 
study.

Badia JM(1), Casanova-Portoles D(2), Membrilla E(3), Rubiés C(4), Pujol M(5), 
Sancho J(6).

Author information:
(1)Department of Surgery, Hospital General de Granollers, Granollers, Spain; 
Universitat Internacional de Catalunya. Sant Cugat del Vallès, Barcelona, Spain. 
Electronic address: jmbadiaperez@gmail.com.
(2)Department of Surgery, Hospital General de Granollers, Granollers, Spain; 
Universitat Internacional de Catalunya. Sant Cugat del Vallès, Barcelona, Spain. 
Electronic address: dcasanova@fphag.org.
(3)Department of Surgery, Hospital del Mar, Barcelona, Spain. Electronic 
address: estelabe@gmail.com.
(4)Department of Digital Transformation, Hospital General de 
Granollers, Granollers, Spain. Electronic address: crubies@fphag.org.
(5)VINCat Program, Servei Català de la Salut, Catalonia, Spain; Centro de 
Investigación Biomédica en Red de Enfermedades Infecciosas (CIBERINFEC), 
Instituto de Salud Carlos III, Madrid, Spain. VINCat 
Program, Barcelona, Catalonia, Spain; Department of Infectious Diseases, 
Hospital Universitari de Bellvitge - IDIBELL. L'Hospitalet de Llobregat, Spain. 
Electronic address: mpujol@bellvitgehospital.cat.
(6)Department of Surgery, Hospital del Mar, Barcelona, Spain. Electronic 
address: jsancho@parcdesalutmar.cat.

BACKGROUND: Surveillance of surgical site infection (SSI) relies on manual 
methods that are time-consuming and prone to subjectivity. This study evaluates 
the diagnostic accuracy of ChatGPT for detecting SSI from electronic health 
records after colorectal surgery via comparison with the results of a nationwide 
surveillance programme.
METHODS: This pilot, retrospective, multicentre analysis included 122 patients 
who underwent colorectal surgery. Patient records were reviewed by both manual 
surveillance and ChatGPT, which was tasked with identifying SSI and categorizing 
them as superficial, deep, or organ-space infections. Sensitivity, specificity, 
positive predictive value (PPV), and negative predictive value (NPV) were 
calculated. Receiver operating characteristic (ROC) curve analysis determined 
the model's diagnostic performance.
RESULTS: ChatGPT achieved a sensitivity of 100 %, correctly identifying all SSIs 
detected by manual methods. The specificity was 54 %, indicating the presence of 
false positives. The PPV was 67 %, and the NPV was 100 %. The area under the ROC 
curve was 0.77, indicating good overall accuracy for distinguishing between SSI 
and non-SSI cases. Minor differences in outcomes were observed between colon and 
rectal surgeries, as well as between the hospitals participating in the study.
CONCLUSIONS: ChatGPT shows high sensitivity and good overall accuracy for 
detecting SSI. It appears to be a useful tool for initial screenings and for 
reducing manual review workload. The moderate specificity suggests a need for 
further refinement to reduce the rate of false positives. The integration of 
ChatGPT alongside electronic medical records, antibiotic consumption and imaging 
data results for real-time analysis may further improve the surveillance of SSI.
CLINICALTRIALS: gov Identifier: NCT06556017.

Copyright © 2024 The Author(s). Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.jiph.2024.102627
PMID: 39740340 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest All authors 
declare no conflict of interest relevant to this article.


423. Crit Care Explor. 2025 Nov 24;7(12):e1350. doi: 10.1097/CCE.0000000000001350. 
eCollection 2025 Dec.

Fast, Accurate Assignment of Clinical Diagnoses From Patient Notes by a Large 
Language Model: Critical Pediatric Pneumonia as a Use Case.

Martin B(1)(2), Payan M(3), LaVelle J(4), DeWitt PE(2), Russell S(2), Mitchell 
J(2), Deakyne Davies SJ(3), Bennett TD(1)(2).

Author information:
(1)Section of Critical Care Medicine, Department of Pediatrics, University of 
Colorado School of Medicine, Aurora, CO.
(2)Department of Biomedical Informatics, University of Colorado School of 
Medicine, Aurora, CO.
(3)Research Informatics and Data Science, Children's Hospital Colorado, Aurora, 
CO.
(4)Division of Pediatric Critical Care, Children's Hospital Colorado, Aurora, 
CO.

OBJECTIVE: To determine the accuracy of a custom version of the generative 
pretrained transformer (GPT)-4o large language model (LLM) in identifying PICU 
admissions with vs. without bacterial pneumonia using clinical notes.
DESIGN: In this retrospective cohort study, the GPT-4o model was provided 
guidance on our institution's pneumonia diagnosis practices through a custom 
prompt and instructed to analyze PICU provider notes from the first 2 calendar 
days of PICU admission to identify bacterial pneumonia diagnoses. Diagnoses from 
the manually curated Virtual Pediatric Systems (VPS) Registry were used as the 
gold standard.
SETTING: A 48-bed, academic, quaternary care PICU.
PATIENTS: Children 3 months old to 18 years old admitted to the PICU from 
January 1, 2023, to December 31, 2023.
INTERVENTIONS: None.
MEASUREMENTS AND MAIN RESULTS: GPT-4o analyzed 10,081 notes from 3,317 PICU 
admissions over 5.0 minutes (mean 0.03 s per note). Of the 3317 study 
encounters, 481(14.5%) had a VPS admission pneumonia diagnosis. GPT-4o 
accurately classified 3143 of 3317 (94.8%) encounters. In a post hoc 
adjudication analysis, a blinded PICU attending reviewed patient charts with 
VPS-GPT discordant classifications. The GPT-4o classification matched that of 
the blinded PICU attending in 125 of 174 (71.8%) of such encounters. The most 
common reason for incorrect classification by GPT-4o was that a pneumonia 
diagnosis was listed in the initial notes but later rescinded when a different 
diagnosis was identified.
CONCLUSIONS: The GPT-4o LLM was able to accurately and rapidly identify 
critically ill children with vs. without bacterial pneumonia. This study 
suggests similar tools could be developed to automate and accelerate processes 
typically requiring manual chart review.

Copyright © 2025 The Authors. Published by Wolters Kluwer Health, Inc. on behalf 
of the Society of Critical Care Medicine.

DOI: 10.1097/CCE.0000000000001350
PMCID: PMC12647518
PMID: 41311865 [Indexed for MEDLINE]


424. Eur Arch Otorhinolaryngol. 2025 Mar;282(3):1593-1607. doi: 
10.1007/s00405-024-09153-3. Epub 2025 Jan 10.

Assessment of decision-making with locally run and web-based large language 
models versus human board recommendations in otorhinolaryngology, head and neck 
surgery.

Buhr CR(1)(2), Ernst BP(3), Blaikie A(4), Smith H(5), Kelsey T(5), Matthias 
C(6), Fleischmann M(7), Jungmann F(8), Alt J(9), Brandts C(10), Kämmerer PW(11), 
Foersch S(12), Kuhn S(13), Eckrich J(6).

Author information:
(1)Department of Otorhinolaryngology, University Medical Center of the Johannes 
Gutenberg-University Mainz, Langenbeckstraße 1, 55131, Mainz, Germany. 
buhrchri@uni-mainz.de.
(2)School of Medicine, University of St Andrews, St Andrews, UK. 
buhrchri@uni-mainz.de.
(3)Department of Otorhinolaryngology, University Medical Center Frankfurt, 
Theodor-Stern-Kai 7, 60596, Frankfurt am Main, Germany.
(4)School of Medicine, University of St Andrews, St Andrews, UK.
(5)School of Computer Science, University of St Andrews, St Andrews, UK.
(6)Department of Otorhinolaryngology, University Medical Center of the Johannes 
Gutenberg-University Mainz, Langenbeckstraße 1, 55131, Mainz, Germany.
(7)Department of Radiotherapy and Oncology, University Medical Center Frankfurt, 
Theodor-Stern-Kai 7, 60596, Frankfurt am Main, Germany.
(8)Outpatient Department for Radiology and Nuclear Medicine at Marienhaus 
Hospital Saarlouis, Xcare Group, Kapuzinerstraße 4, 66740, Saarlouis, Germany.
(9)Department of Hematology and Medical Oncology, University Medical Center 
Mainz of the Johannes Gutenberg-University Mainz, Langenbeckstraße 1, 55131, 
Mainz, Germany.
(10)Department of Hematology and Medical Oncology, University Medical Center 
Frankfurt, Theodor-Stern-Kai 7, 60596, Frankfurt am Main, Germany.
(11)Department of Oral and Maxillofacial Surgery-Plastic Surgery, University 
Medical Center Mainz of the Johannes Gutenberg-University Mainz, 
Langenbeckstraße 1, 55131, Mainz, Germany.
(12)Institute of Pathology, University Medical Center of the Johannes 
Gutenberg-University Mainz, Langenbeckstraße 1, 55131, Mainz, Germany.
(13)Institute for Digital Medicine, Philipps-University Marburg and University 
Hospital of Giessen and Marburg, Marburg, Germany.

INTRODUCTION: Tumor boards are a cornerstone of modern cancer treatment. Given 
their advanced capabilities, the role of Large Language Models (LLMs) in 
generating tumor board decisions for otorhinolaryngology (ORL) head and neck 
surgery is gaining increasing attention. However, concerns over data protection 
and the use of confidential patient information in web-based LLMs have 
restricted their widespread adoption and hindered the exploration of their full 
potential. In this first study of its kind we compared standard human 
multidisciplinary tumor board recommendations (MDT) against a web-based LLM 
(ChatGPT-4o) and a locally run LLM (Llama 3) addressing data protection 
concerns.
MATERIAL AND METHODS: Twenty-five simulated tumor board cases were presented to 
an MDT composed of specialists from otorhinolaryngology, craniomaxillofacial 
surgery, medical oncology, radiology, radiation oncology, and pathology. This 
multidisciplinary team provided a comprehensive analysis of the cases. The same 
cases were input into ChatGPT-4o and Llama 3 using structured prompts, and the 
concordance between the LLMs' and MDT's recommendations was assessed. Four MDT 
members evaluated the LLMs' recommendations in terms of medical adequacy (using 
a six-point Likert scale) and whether the information provided could have 
influenced the MDT's original recommendations.
RESULTS: ChatGPT-4o showed 84% concordance (21 out of 25 cases) and Llama 3 
demonstrated 92% concordance (23 out of 25 cases) with the MDT in distinguishing 
between curative and palliative treatment strategies. In 64% of cases (16/25) 
ChatGPT-4o and in 60% of cases (15/25) Llama, identified all first-line therapy 
options considered by the MDT, though with varying priority. ChatGPT-4o 
presented all the MDT's first-line therapies in 52% of cases (13/25), while 
Llama 3 offered a homologous treatment strategy in 48% of cases (12/25). 
Additionally, both models proposed at least one of the MDT's first-line 
therapies as their top recommendation in 28% of cases (7/25). The ratings for 
medical adequacy yielded a mean score of 4.7 (IQR: 4-6) for ChatGPT-4o and 4.3 
(IQR: 3-5) for Llama 3. In 17% of the assessments (33/200), MDT members 
indicated that the LLM recommendations could potentially enhance the MDT's 
decisions.
DISCUSSION: This study demonstrates the capability of both LLMs to provide 
viable therapeutic recommendations in ORL head and neck surgery. Llama 3, 
operating locally, bypasses many data protection issues and shows promise as a 
clinical tool to support MDT decisions. However at present, LLMs should augment 
rather than replace human decision-making.

© 2024. The Author(s).

DOI: 10.1007/s00405-024-09153-3
PMCID: PMC11890241
PMID: 39792200 [Indexed for MEDLINE]


425. NPJ Digit Med. 2026 Jan 22. doi: 10.1038/s41746-026-02372-4. Online ahead of 
print.

Early diagnosis of axial spondyloarthritis in primary care using multi-agent 
systems.

Ji X(#)(1), Li Z(#)(2), Zeng L(#)(1)(3), Hu L(1), Wang Y(3), Zhang K(4), Shi 
L(5), Wei M(6), Chen L(7), Guo L(8), Dong J(9), Wang A(9), Sun L(9), Song Y(9), 
Wang H(9), Wang J(9), Lei Y(9), Yue W(9), Zhao Z(1), Zhu J(1), Huang F(1), Zhang 
J(10), Li T(11)(12), Li K(13).

Author information:
(1)Department of Rheumatology and Immunology, the First Medical Center, Chinese 
PLA General Hospital, Beijing, China.
(2)Bioinformatics Division, Department of Automation, BNRIST and MOE Key Lab of 
Bioinformatics, Tsinghua University, Beijing, China.
(3)Department of Rheumatology and Immunology, Beijing Electric Power Hospital, 
Beijing, China.
(4)Department of Clinical Immunology, Xijing Hospital, Fourth Military Medical 
University, Xi'an, China.
(5)Department of Rheumatology and Immunology, Peking University Shougang 
Hospital, Beijing, China.
(6)Department of Rheumatology and Immunology, General Hospital of Western 
Theater Command, Chengdu, China.
(7)Department of Rheumatology and Immunology, General Hospital of Central 
Theater Command, Wuhan, China.
(8)Department of Rheumatology and Immunology, General Hospital of Northern 
Theater Command of Chinese PLA, Shenyang, Liaoning, China.
(9)Department of Medical Innovation Research, Chinese PLA General Hospital, 
Beijing, China.
(10)Bioinformatics Division, Department of Automation, BNRIST and MOE Key Lab of 
Bioinformatics, Tsinghua University, Beijing, China. 28820327@qq.com.
(11)Department of Medical Innovation Research, Chinese PLA General Hospital, 
Beijing, China. litao30lhospital@163.com.
(12)National Engineering Research Center for Medical Big Data Application 
Technology, Chinese PLA General Hospital, Beijing, China. 
litao30lhospital@163.com.
(13)Department of Rheumatology and Immunology, the First Medical Center, Chinese 
PLA General Hospital, Beijing, China. lkp_01@163.com.
(#)Contributed equally

Axial spondyloarthritis (axSpA) is an inflammatory disease marked by chronic low 
back pain, with a global average diagnostic delay of 6.7 years. Early diagnosis 
is crucial for improving prognosis and reducing disability rates, yet primary 
care physicians (PCPs) may find it challenging to ensure timely recognition and 
referrals. This study developed and validated Spondyloarthritis Agents 
(SpAgents), an early diagnostic system based on a multi-agent framework 
integrating large language models (LLMs) and imaging models. The SpAgents 
framework includes PlannerAgent, DataAgent, ToolAgent, and DoctorAgent, 
supported by long-term memory for dynamic knowledge updates. We enrolled 596 
patients, dividing 545 from one hospital into a training dataset (n = 359) and a 
validation dataset (n = 186), along with an independent cohort of 51 patients 
from five additional hospitals for testing. SpAgents demonstrated strong 
diagnostic performance, achieving sensitivity of 0.8615 and specificity of 
0.8000 during validation, and 0.9375 and 0.7368 during testing. SpAgents 
exhibited significantly higher sensitivity (0.9400) and accuracy (0.8600) than 
both PCPs and junior rheumatologists, with overall performance equivalent to 
that of senior rheumatologists. Under SpAgents-assisted diagnosis, both PCPs and 
junior rheumatologists showed marked improvements in sensitivity and accuracy. 
SpAgents effectively enhance early axSpA identification among PCPs, offering an 
innovative solution to reduce diagnostic delays.

© 2026. The Author(s).

DOI: 10.1038/s41746-026-02372-4
PMID: 41571772

Conflict of interest statement: Competing interests: The authors declare no 
competing interests. Declaration of Generative AI in Scientific Writing: We 
confirm that no generative AI tools (such as ChatGPT or other large language 
models) were used in any portion of the manuscript generation. All content in 
this manuscript was independently prepared by the authors


426. Surgery. 2025 Mar;179:108793. doi: 10.1016/j.surg.2024.08.026. Epub 2024 Sep 23.

Performance of trauma-trained large language models on surgical assessment 
questions: A new approach in resource identification.

Mahajan A(1), Tran A(2), Tseng ES(2), Como JJ(2), El-Hayek KM(2), Ladha P(2), Ho 
VP(3).

Author information:
(1)Department of Surgery, The MetroHealth System, Case Western Reserve 
University School of Medicine, Cleveland, OH. Electronic address: 
https://twitter.com/arnavmahajan_.
(2)Department of Surgery, The MetroHealth System, Case Western Reserve 
University School of Medicine, Cleveland, OH.
(3)Department of Surgery, The MetroHealth System, Case Western Reserve 
University School of Medicine, Cleveland, OH; Department of Population and 
Quantitative Health Sciences, Case Western Reserve University School of 
Medicine, Cleveland, OH. Electronic address: vho@metrohealth.org.

BACKGROUND: Large language models have successfully navigated simulated medical 
board examination questions. However, whether and how language models can be 
used in surgical education is less understood. Our study evaluates the efficacy 
of domain-specific large language models in curating study materials for 
surgical board style questions.
METHODS: We developed EAST-GPT and ACS-GPT, custom large language models with 
domain-specific knowledge from published guidelines from the Eastern Association 
of the Surgery of Trauma and the American College of Surgeons Trauma Quality 
Programs. EAST-GPT, ACS-GPT, and an untrained GPT-4 performance were assessed 
trauma-related questions from Surgical Education and Self-Assessment Program 
(18th edition). Large language models were asked to choose answers and provide 
answer rationales. Rationales were assessed against an educational framework 
with 5 domains: accuracy, relevance, comprehensiveness, evidence-base, and 
clarity.
RESULTS: Ninety guidelines trained EAST-GPT and 10 trained ACS-GPT. All large 
language models were tested on 62 trauma questions. EAST-GPT correctly answered 
76%, whereas ACS-GPT answered 68% correctly. Both models outperformed ChatGPT-4 
(P < .05), which answered 45% correctly. For reasoning, EAST-GPT achieved the 
gratest mean scores across all 5 educational framework metrics. ACS-GPT scored 
lower than ChatGPT-4 in comprehensiveness and evidence-base; however, these 
differences were not statistically significant.
CONCLUSION: Our study presents a novel methodology in identifying 
test-preparation resources by training a large language model to answer 
board-style multiple choice questions. Both trained models outperformed 
ChatGPT-4, demonstrating its answers were accurate, relevant, and 
evidence-based. Potential implications of such AI integration into surgical 
education must be explored.

Copyright © 2024. Published by Elsevier Inc.

DOI: 10.1016/j.surg.2024.08.026
PMID: 39317517 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest/Disclosure The authors have 
no related conflicts of interest to declare.


427. Front Digit Health. 2026 Jan 5;7:1737882. doi: 10.3389/fdgth.2025.1737882. 
eCollection 2025.

Evaluation of multiple generative large language models on neurology board-style 
questions.

Almomani M(1), Valaparla V(1), Weatherhead J(2), Fang X(1), Dabi A(1), Li CY(3), 
McCaffrey P(4), Hier D(5)(6), Rodríguez-Fernández JM(1).

Author information:
(1)Department of Neurology, University of Texas Medical Branch (UTMB), 
Galveston, TX, United States.
(2)School of Biomedical Sciences, University of Texas Medical Branch (UTMB), 
Galveston, TX, United States.
(3)Department of Occupational Therapy, University of Texas Medical Branch 
(UTMB), Galveston, TX, United States.
(4)Department of Pathology, University of Texas Medical Branch (UTMB), 
Galveston, TX, United States.
(5)Department of Neurology and Rehabilitation, University of Illinois at 
Chicago, Chicago, IL, United States.
(6)Department of Electrical and Computer Engineering, Missouri University of 
Science and Technology, Rolla, MO, United States.

OBJECTIVE: To compare the performance of eight large language models (LLMs) with 
neurology residents on board-style multiple-choice questions across seven 
subspecialties and two cognitive levels.
METHODS: In a cross-sectional benchmarking study, we evaluated Bard, Claude, 
Gemini v1, Gemini 2.5, ChatGPT-3.5, ChatGPT-4, ChatGPT-4o, and ChatGPT-5 using 
107 text-only items spanning movement disorders, vascular neurology, 
neuroanatomy, neuroimmunology, epilepsy, neuromuscular disease, and 
neuro-infectious disease. Items were labeled as lower- or higher-order per 
Bloom's taxonomy by two neurologists. Models answered each item in a fresh 
session and reported confidence and Bloom classification. Residents completed 
the same set under exam-like conditions. Outcomes included overall and domain 
accuracies, guessing-adjusted accuracy, confidence-accuracy calibration 
(Spearman ρ), agreement with expert Bloom labels (Cohen κ), and inter-generation 
scaling (linear regression of topic-level accuracies). Group differences used 
Fisher exact or χ 2 tests with Bonferroni correction.
RESULTS: Residents scored 64.9%. ChatGPT-5 achieved 84.1% and ChatGPT-4o 81.3%, 
followed by Gemini 2.5 at 77.6% and ChatGPT-4 at 68.2%; Claude (56.1%), Bard 
(54.2%), ChatGPT-3.5 (53.3%), and Gemini v1 (39.3%) underperformed residents. On 
higher-order items, ChatGPT-5 (86%) and ChatGPT-4o (82.5%) maintained 
superiority; Gemini 2.5 matched 82.5%. Guessing-adjusted accuracy preserved rank 
order (ChatGPT-5 78.8%, ChatGPT-4o 75.1%, Gemini 2.5 70.1%). Confidence-accuracy 
calibration was weak across models. Inter-generation scaling was strong within 
the ChatGPT lineage (ChatGPT-4 to 4o R 2 = 0.765, p = 0.010; 4o to 5 R 
2 = 0.908, p < 0.001) but absent for Gemini v1 to 2.5 (R2 = 0.002, p = 0.918), 
suggesting discontinuous improvements.
CONCLUSIONS: LLMs-particularly ChatGPT-5 and ChatGPT-4o-exceeded resident 
performance on text-based neurology board-style questions across subspecialties 
and cognitive levels. Gemini 2.5 showed substantial gains over v1 but with 
domain-uneven scaling. Given weak confidence calibration, LLMs should be 
integrated as supervised educational adjuncts with ongoing validation, version 
governance, and transparent metadata to support safe use in neurology education.

© 2026 Almomani, Valaparla, Weatherhead, Fang, Dabi, Li, McCaffrey, Hier and 
Rodríguez-Fernández.

DOI: 10.3389/fdgth.2025.1737882
PMCID: PMC12813092
PMID: 41561160

Conflict of interest statement: The authors declared that this work was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


428. Artif Intell Med. 2026 Jan;171:103310. doi: 10.1016/j.artmed.2025.103310. Epub 
2025 Nov 22.

Building trustworthy large language model-driven generative recommender system 
for healthcare decision support: A scoping review of corpus sources, 
customization techniques, and evaluation frameworks.

Yang S(1), Jing M(1), Wang S(2), Huang Z(3), Wang J(3), Kou J(1), Shi M(1), Xia 
Z(4), Wei Q(5), Xing W(1), Hu Y(1), Zhu Z(6).

Author information:
(1)School of Nursing, Fudan University, Shanghai, China.
(2)School of Nursing, Dali University, Yunnan, China.
(3)School of Computer Science, Fudan University, Shanghai, China.
(4)School of Information Science and Engineering, East China University of 
Science and Technology, Shanghai, China.
(5)Peking Union Medical College Hospital, Beijing, 100730, China.
(6)School of Nursing, Fudan University, Shanghai, China. Electronic address: 
zhengzhu@fudan.edu.cn.

INTRODUCTION: Large Language Model-Driven Generative Recommender Systems 
(LLM-GRSs) are playing a growing role in healthcare, particularly in clinical 
question-answering. This study reviews their corpus sources, customization 
techniques, and evaluation metrics.
METHODS: We conducted a systematic search of PubMed, Embase, Scopus, and Web of 
Science for studies published between January 2021 and August 2025 that applied 
LLM-GRSs to deliver medical or healthcare information. Eligible studies included 
publications describing LLMs designed to emulate clinical decision-making by 
providing diagnostic or therapeutic recommendations through dialogue-based 
interfaces. Two reviewers independently screened studies and extracted data on 
corpus sources, model architectures, customization methods, and evaluation 
metrics.
RESULTS: A total of 61 articles were included. Corpus sources were grouped into 
clinical data (n = 25), literature (n = 34), open datasets (n = 37), and 
web-crawled data (n = 15), with many using multiple types. Most studies (n = 43) 
combined multiple approaches. Customization techniques included prompt 
engineering, retrieval-augmented generation and model fine-tuning. Twenty-four 
studies used a single customization technique, while 37 studies combined these 
methods during model development. The evaluation metrics were classified into 
three main domains: process metrics, usability metrics, and outcome metrics. The 
outcome metrics included both model-based and manual-assessed evaluations.
CONCLUSION: LLM-GRSs hold considerable promise in healthcare; however, their 
safety and reliability hinge on the use of evidence-based training corpora, 
transparent system design, and standardized evaluation protocols within 
real-world clinical environments.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.artmed.2025.103310
PMID: 41313967 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no conflicts of interest.


429. Arthroscopy. 2025 Jun;41(6):2002-2008. doi: 10.1016/j.arthro.2024.09.020. Epub 
2024 Sep 21.

Artificial Intelligence Large Language Models Address Anterior Cruciate Ligament 
Reconstruction: Superior Clarity and Completeness by Gemini Compared With 
ChatGPT-4 in Response to American Academy of Orthopaedic Surgeons Clinical 
Practice Guidelines.

Quinn M(1), Milner JD(2), Schmitt P(3), Morrissey P(2), Lemme N(2), Marcaccio 
S(4), DeFroda S(5), Tabaddor R(2), Owens BD(2).

Author information:
(1)Department of Orthopaedics, The Warren Alpert Medical School of Brown 
University, Providence, Rhode Island, U.S.A.. Electronic address: 
msquinn1218@gmail.com.
(2)Department of Orthopaedics, The Warren Alpert Medical School of Brown 
University, Providence, Rhode Island, U.S.A.
(3)The Warren Alpert Medical School of Brown University, Providence, Rhode 
Island, U.S.A.
(4)Department of Orthopaedic Surgery, University of Pittsburgh Medical Center, 
Pittsburgh, Pennsylvania, U.S.A.
(5)Department of Orthopaedic Surgery, Missouri Orthopaedic Institute, University 
of Missouri, Columbia, Missouri, U.S.A.

PURPOSE: To assess the ability of ChatGPT-4 and Gemini to generate accurate and 
relevant responses to the 2022 American Academy of Orthopaedic Surgeons (AAOS) 
Clinical Practice Guidelines (CPG) for anterior cruciate ligament reconstruction 
(ACLR).
METHODS: Responses from ChatGPT-4 and Gemini to prompts derived from all 15 AAOS 
guidelines were evaluated by 7 fellowship-trained orthopaedic sports medicine 
surgeons using a structured questionnaire assessing 5 key characteristics on a 
scale from 1 to 5. The prompts were categorized into 3 areas: diagnosis and 
preoperative management, surgical timing and technique, and rehabilitation and 
prevention. Statistical analysis included mean scoring, standard deviation, and 
2-sided t tests to compare the performance between the 2 large language models 
(LLMs). Scores were then evaluated for inter-rater reliability (IRR).
RESULTS: Overall, both LLMs performed well with mean scores >4 for the 5 key 
characteristics. Gemini demonstrated superior performance in overall clarity 
(4.848 ± 0.36 vs 4.743 ± 0.481, P = .034), but all other characteristics 
demonstrated nonsignificant differences (P > .05). Gemini also demonstrated 
superior clarity in the surgical timing and technique (P = .038) as well as the 
prevention and rehabilitation (P = .044) subcategories. Additionally, Gemini had 
superior performance completeness scores in the rehabilitation and prevention 
subcategory (P = .044), but no statistically significant differences were found 
amongst the other subcategories. The overall IRR was found to be 0.71 
(moderate).
CONCLUSIONS: Both Gemini and ChatGPT-4 demonstrate an overall good ability to 
generate accurate and relevant responses to question prompts based on the 2022 
AAOS CPG for ACLR. However, Gemini demonstrated superior clarity in multiple 
domains in addition to superior completeness for questions pertaining to 
rehabilitation and prevention.
CLINICAL RELEVANCE: The current study addresses a current gap in the LLM and 
ACLR literature by comparing the performance of ChatGPT-4 to Gemini, which is 
growing in popularity with more than 300 million individual uses in May 2024 
alone. Moreover, the results demonstrated superior performance of Gemini in both 
clarity and completeness, which are critical elements of a tool being used by 
patients for educational purposes. Additionally, the current study uses question 
prompts based on the AAOS CPG, which may be used as a method of standardization 
for future investigations on performance of LLM platforms. Thus, the results of 
this study may be of interest to both the readership of Arthroscopy and 
patients.

Copyright © 2024 Arthroscopy Association of North America. Published by Elsevier 
Inc. All rights reserved.

DOI: 10.1016/j.arthro.2024.09.020
PMID: 39313138 [Indexed for MEDLINE]

Conflict of interest statement: Disclosures The authors declare the following 
financial interests/personal relationships which may be considered as potential 
competing interests: B.D.O. has relationships with Conmed, Mitek, Vericel Corp, 
Miach, Linvatec, DePuy Synthes Products, Musculoskeletal Transplant Foundation, 
Vericel, and Vivorte, but these relationships are unrelated to this work. All 
other authors (M.Q., J.D.M., P.S., P.M., N.L., S.M., S.D., R.T.) declare that 
they have no known competing financial interests or personal relationships that 
could have appeared to influence the work reported in this paper.


430. J Plast Reconstr Aesthet Surg. 2025 May;104:200-208. doi: 
10.1016/j.bjps.2025.03.021. Epub 2025 Mar 13.

Assessing the predictive accuracy of ChatGPT-based image analysis in forecasting 
long-term scar characteristics from 3-month assessments - A pilot study.

Nguyen AT(1), Li RA(2), Galiano RD(3).

Author information:
(1)University of Rochester School of Medicine and Dentistry, Rochester, NY, 
United States.
(2)Division of Plastic & Reconstructive Surgery, Northwestern University 
Feinberg School of Medicine, Chicago, IL, United States.
(3)Division of Plastic & Reconstructive Surgery, Northwestern University 
Feinberg School of Medicine, Chicago, IL, United States. Electronic address: 
Robert.Galiano@nm.org.

INTRODUCTION: Scarring significantly impacts patient quality of life, yet 
traditional assessments often rely on subjective evaluations, resulting in 
variability in predictions. This study aimed to evaluate the predictive accuracy 
of a Smart Image Analysis ChatGPT model in forecasting scar characteristics.
METHODS: This single-institution prospective cohort study included 40 patients 
who underwent plastic surgery. Scar images were captured at 3 and 12 months, 
assessing characteristics such as vascularity, pigmentation, height, and width. 
The ChatGPT model predicted binary outcomes (good vs. bad scars) and continuous 
outcomes. Predictive accuracy was measured using metrics including mean absolute 
error (MAE), mean squared error (MSE), root mean squared error (RMSE), and 
R-squared (R²).
RESULTS: The model achieved an overall accuracy of 97.5% for binary 
classifications of scars. McNemar's test confirmed no significant differences 
between predicted and actual outcomes. For continuous outcomes, the MAE was 
0.65, with an MSE of 0.9 and RMSE of 0.95, indicating moderate accuracy. 
Vascularity predictions yielded an R² of 0.234, whereas height and width showed 
stronger correlations with R² values of 0.857 and 0.956, respectively. 
Statistically significant differences in paired t-tests were observed for 
pigmentation (t = 4.356, p = 9.319e-05) and width (t = 2.896, p = 0.0062).
CONCLUSION: The Smart Image Analysis ChatGPT model demonstrates excellent 
predictive accuracy in binary scar classification and provides valuable insights 
for scar characteristics. Further refinement is necessary for improving 
predictions of dynamic features such as vascularity.

Copyright © 2025 British Association of Plastic, Reconstructive and Aesthetic 
Surgeons. Published by Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.bjps.2025.03.021
PMID: 40154112 [Indexed for MEDLINE]


431. PLoS One. 2025 Oct 16;20(10):e0334250. doi: 10.1371/journal.pone.0334250. 
eCollection 2025.

Evaluating the quality of ChatGPT-generated medical information on major 
ophthalmic conditions: A comparative assessment against the EQIP tool and 
guidelines.

Hu M(1), Zou P(1), Li T(1), Wang Y(1).

Author information:
(1)Ophthalmology Department, The First Affiliated Hospital of Zhengzhou 
University, Zhengzhou, China.

BACKGROUND: The use of artificial intelligence for creating medical information 
is on the rise. Nonetheless, the accuracy and reliability of such information 
require thorough assessment. As a language model capable of generating text, 
ChatGPT needs a detailed examination of its effectiveness in the healthcare 
domain.
OBJECTIVE: This research sought to evaluate the precision of medical data 
produced by ChatGPT-4o (https://chat.openai.com/chat, accessed Mar. 12, 2025), 
concentrating on its capability to handle the top five ophthalmic issues that 
pose the greatest global health challenges. Furthermore, the investigation 
compared the AI's answers to recognized medical guides.
METHODS: This research employed an adapted version of the Ensuring Quality of 
Information for Patients (EQIP) instrument to evaluate the quality of ChatGPT's 
replies. The guidelines for the five conditions were rephrased into pertinent 
queries. These queries were then fed into ChatGPT, employing benchmarking 
against established ophthalmology clinical guidelines, and the resulting answers 
were independently scrutinized for precision and consistency by two 
investigators. The consistency among raters was evaluated using Cohen's kappa 
value.
RESULTS: The median EQIP score across the five conditions was 18 (IQR 18-19). 
The modified EQIP instrument revealed a robust consensus between the two 
evaluators when assessing ChatGPT's responses, as indicated by a Cohen's kappa 
value of 0.926 (95% CI 0.875-0.977, P<0.001). The alignment between the ChatGPT 
responses and the guideline recommendations was 84% (21/25), as indicated by a 
Cohen's kappa value of 0.658 (95% CI 0.317-0.999, P<0.001).
CONCLUSIONS: ChatGPT demonstrates robust quality and guideline compliance in 
producing medical content. Nevertheless, improvements are necessary to enhance 
the accuracy of quantitative data and ensure a more comprehensive coverage, 
thereby offering valuable insights for the advancement of medical information 
generation.

Copyright: © 2025 Hu et al. This is an open access article distributed under the 
terms of the Creative Commons Attribution License, which permits unrestricted 
use, distribution, and reproduction in any medium, provided the original author 
and source are credited.

DOI: 10.1371/journal.pone.0334250
PMCID: PMC12530511
PMID: 41100573 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


432. JMIR Form Res. 2025 Oct 3;9:e70602. doi: 10.2196/70602.

Identification of Syndrome Types in Patients With Pancreatic Cancer From Free 
Text in Electronic Medical Records: Model Development and Validation.

Ba H(#)(1)(2), Du H(#)(3)(4), Cheng C(2)(5), Zhang Y(2)(5), Ruan L(2)(5), Chen 
Z(2)(5).

Author information:
(1)Qingdao Institute, Department of Integrative Oncology, Fudan University 
Shanghai Cancer Center, Qingdao, China.
(2)Department of Oncology, Shanghai Medical College, Fudan University, Shanghai, 
China.
(3)Key Laboratory of Surface & Interface Science of Polymer Materials of 
Zhejiang Province, School of Chemistry and Chemical Engineering, Zhejiang 
Sci-Tech University, Hangzhou, China.
(4)Shengzhou Innovation Research Institute of Zhejiang Sci-Tech University, 
Shengzhou, China.
(5)Department of Integrative Oncology, Fudan University Shanghai Cancer Center, 
Shanghai, China.
(#)Contributed equally

BACKGROUND: Syndrome differentiation is crucial in traditional Chinese medicine 
(TCM) diagnosis and treatment, but it heavily relies on expert experience, 
limiting systematic standardization.
OBJECTIVE: This study developed and validated a BERT (bidirectional encoder 
representations from transformers)-based model, the traditional Chinese medicine 
pancreatic cancer syndrome differentiation bidirectional encoder representations 
from transformers (TCMPCSD-BERT), using in-house pancreatic cancer medical 
records, to digitalize expert knowledge and support standardized syndrome 
differentiation in TCM.
METHODS: A retrospective dataset of pancreatic cancer cases (2011-2024) from 
Fudan University Shanghai Cancer Center was annotated into 4 TCM syndrome types 
by 2 experts (Cohen κ=0.913). The proposed TCMPCSD-BERT model was compared with 
conventional models (long short-term memory and text convolutional neural 
network) embedded in TCM diagnostic tools and with large language models (LLMs; 
ChatGPT-4o, Kimi, Ernie Bot 4.0 Turbo, and Zhipu Qingyan) under a prompt 
engineering framework. Performance evaluation on in-house data was supplemented 
with attention visualizations and integrated gradients analyses for 
interpretability. The McNemar test assessed classification accuracy differences, 
while bootstrap 95% CIs quantified statistical uncertainty and stability. The 
Welch t test (2-tailed) was used to evaluate mean differences between 
TCMPCSD-BERT and the comparator models.
RESULTS: Among 6830 records, case counts were damp-heat syndrome (n=1694), 
spleen-deficiency syndrome (n=1185), damp-heat with spleen-deficiency syndrome 
(n=1178), and others (n=2773). On the test set, McNemar test showed 
significantly higher accuracy for TCMPCSD-BERT than the 3 baseline models and 
generally better performance than LLMs. In all comparisons, TCMPCSD-BERT 
achieved higher mean macroprecision, macrorecall, macro-F1-score, and accuracy, 
with nonoverlapping 95% bootstrap CIs and significant Welch t test results 
(P<.01). The model achieved a macroprecision of 0.935 (95% CI 0.918-0.951), 
macrorecall of 0.921 (95% CI 0.900-0.942), macro-F1-score of 0.927 (95% CI 
0.908-0.945), and accuracy of 0.919 (95% CI 0.899-0.939). Attention 
visualizations suggested the model could capture less common TCM term 
associations, while integrated gradients highlighted high-attribution diagnostic 
features (eg, "gray-white stool" 0.933 in damp-heat syndrome; "indigestion" 
1.204 in spleen-deficiency syndrome). Misclassification analyses indicated 
challenges in handling overlapping or atypical symptom presentations. Compared 
with LLMs, web-based platforms, and diagnostic instruments, TCMPCSD-BERT 
appeared to provide relatively higher accuracy, interpretability, and efficiency 
in processing long unstructured texts for syndrome differentiation.
CONCLUSIONS: The TCMPCSD-BERT model shows potential for automated syndrome 
differentiation from unstructured clinical texts and broader application in TCM. 
Based on this study, it appears to improve operability over 4-diagnostic 
instruments and web-based platforms, and offers greater stability and accuracy 
than LLMs in specific tasks. However, these findings should be interpreted 
cautiously, given the subjectivity of syndrome definitions, data imbalance, and 
reliance on preprocessed, expert-annotated data. Further studies involving 
larger and more diverse populations are needed to validate its generalizability 
and support its broader application in real-world settings.

©He Ba, Haojie Du, Chienshan Cheng, Yuan Zhang, Linjie Ruan, Zhen Chen. 
Originally published in JMIR Formative Research (https://formative.jmir.org), 
03.10.2025.

DOI: 10.2196/70602
PMCID: PMC12534766
PMID: 41043140 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


433. Int J Med Inform. 2025 Nov;203:106022. doi: 10.1016/j.ijmedinf.2025.106022. Epub 
2025 Jul 2.

From text to data: Open-source large language models in extracting cancer 
related medical attributes from German pathology reports.

Bartels S(1), Carus J(2).

Author information:
(1)University Medical Center Hamburg-Eppendorf / University Cancer Center, 
Martinistr. 52, Hamburg, 22767, Germany. Electronic address: st.bartels@uke.de.
(2)University Medical Center Hamburg-Eppendorf / University Cancer Center, 
Martinistr. 52, Hamburg, 22767, Germany. Electronic address: j.carus@uke.de.

Structured oncological documentation is vital for data-driven cancer care, yet 
extracting clinical features from unstructured pathology reports remains 
challenging-especially in German healthcare, where strict data protection rules 
require local model deployment. This study evaluates open-source large language 
models (LLMs) for extracting oncological attributes from German pathology 
reports in a secure, on-premise setting. We created a gold-standard dataset of 
522 annotated reports and developed a retrieval-augmented generation (RAG) 
pipeline using an additional 15,000 pathology reports. Five instruction-tuned 
LLMs (Llama 3.3 70B, Mistral Small 24B, and three SauerkrautLM variants) were 
evaluated using three prompting strategies: zero-shot, few-shot, and 
RAG-enhanced few-shot prompting. All models produced structured JSON outputs and 
were assessed using entity-level precision, recall, accuracy, and macro-averaged 
F1-score. Results show that Llama 3.3 70B achieved the highest overall 
performance (F1 > 0.90). However, when combined with the RAG pipeline, Mistral 
Small 24B achieved nearly equivalent performance, matching Llama 70B on most 
entity types while requiring significantly fewer computational resources. 
Prompting strategy significantly impacted performance: few-shot prompting 
improved baseline accuracy, and RAG further enhanced performance, particularly 
for models with fewer than 24B parameters. Challenges remained in extracting 
less frequent but clinically critical attributes like metastasis and staging, 
underscoring the importance of retrieval mechanisms and balanced training data. 
This study demonstrates that open-source LLMs, when paired with effective 
prompting and retrieval strategies, can enable high-quality, privacy-compliant 
extraction of oncological information from unstructured text. The finding that 
smaller models can match larger ones through retrieval augmentation highlights a 
path toward scalable, resource-efficient deployment in German clinical settings.

Copyright © 2025 The Author(s). Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106022
PMID: 40609461 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


434. Nat Med. 2026 Jan 20. doi: 10.1038/s41591-025-04151-2. Online ahead of print.

Holistic evaluation of large language models for medical tasks with MedHELM.

Bedi S(#)(1), Cui H(#)(2), Fuentes M(#)(2), Unell A(#)(2), Wornow M(2), Banda 
JM(3), Kotecha N(3), Keyes T(3), Mai Y(4), Oez M(5), Qiu H(5), Jain S(5), 
Schettini L(5), Kashyap M(2), Fries JA(2), Swaminathan A(2), Chung P(2), 
Haredasht FN(2), Lopez I(2), Aali A(2), Tse G(2), Nayak A(2), Vedak S(2), Jain 
SS(2), Patel B(2), Fayanju O(2), Shah S(2), Goh E(2), Yao DH(2), Soetikno B(2), 
Reis E(2), Gatidis S(2), Divi V(2), Capasso R(2), Saralkar R(2), Chiang CC(2), 
Jindal J(2), Pham T(2), Ghoddusi F(2), Lin S(2), Chiou AS(2), Hong HJ(2), Roy 
M(2), Gensheimer MF(2), Patel H(2), Schulman K(2), Dash D(2), Char D(2), Downing 
L(2), Grolleau F(2), Black K(2), Mieso B(2), Zahedivash A(2), Yim WW(5), Sharma 
H(5), Lee T(4), Kirsch H(3), Lee J(3), Ambers N(3), Lugtu C(3), Sharma A(3), 
Mawji B(3), Alekseyev A(3), Zhou V(3), Kakkar V(3), Helzer J(3), Revri A(3), 
Bannett Y(2), Daneshjou R(2), Chen J(2), Alsentzer E(2), Morse K(2), Ravi N(6), 
Aghaeepour N(2), Kennedy V(2), Chaudhari A(2), Wang T(2)(3), Koyejo S(4)(7), 
Lungren MP(2)(5), Horvitz E(5)(7), Liang P(4)(7), Pfeffer MA(3), Shah 
NH(2)(3)(6).

Author information:
(1)Stanford University School of Medicine, Stanford, CA, USA. 
suhana@stanford.edu.
(2)Stanford University School of Medicine, Stanford, CA, USA.
(3)Stanford Health Care, Palo Alto, CA, USA.
(4)Center for Research on Foundation Models (CRFM) & Department of Computer 
Science, Stanford University, Stanford, CA, USA.
(5)Microsoft Corporation, Redmond, WA, USA.
(6)eHealth Africa Clinics, Kano, Nigeria.
(7)Stanford Institute for Human-Centered AI, Stanford, CA, USA.
(#)Contributed equally

While large language models (LLMs) achieve near-perfect scores on medical 
licensing exams, these evaluations inadequately reflect the complexity and 
diversity of real-world clinical practice. Here we introduce MedHELM, an 
extensible evaluation framework with three contributions. First, a 
clinician-validated taxonomy organizing medical AI applications into five 
categories that mirror real clinical tasks-clinical decision support (diagnostic 
decisions, treatment planning), clinical note generation (visit documentation, 
procedure reports), patient communication (education materials, care 
instructions), medical research (literature analysis, clinical data analysis) 
and administration (scheduling, workflow coordination). These encompass 22 
subcategories and 121 specific tasks reflecting daily medical practice. Second, 
a comprehensive benchmark suite of 37 evaluations covering all subcategories. 
Third, systematic comparison of nine frontier LLMs-Claude 3.5 Sonnet, Claude 3.7 
Sonnet, DeepSeek R1, Gemini 1.5 Pro, Gemini 2.0 Flash, GPT-4o, GPT-4o mini, 
Llama 3.3 and o3-mini-using an automated LLM-jury evaluation method. Our 
LLM-jury uses multiple AI evaluators to assess model outputs against 
expert-defined criteria. Advanced reasoning models (DeepSeek R1, o3-mini) 
demonstrated superior performance with win rates of 66%, although Claude 3.5 
Sonnet achieved comparable results at 15% lower computational cost. These 
results not only highlight current model capabilities but also demonstrate how 
MedHELM could enable evidence-based selection of medical AI systems for 
healthcare applications.

© 2026. The Author(s), under exclusive licence to Springer Nature America, Inc.

DOI: 10.1038/s41591-025-04151-2
PMID: 41559415

Conflict of interest statement: Competing interests: M.O., H.Q., S.J. and L.S. 
are employees of Microsoft. J.A.F. reports healthcare AI consulting for Snorkel 
AI and holds stock options in Snorkel AI. This entity had no role in study 
design, data collection or analysis, decision to publish, or manuscript 
preparation. A.S. reports employment and owning stock options in Benchmark 
Health and Daybreak Health; work is unrelated to the research topics explored in 
MedHELM. P.C. is a Healthcare AI Data Specialist consultant working with OpenAI; 
this work is unrelated to the research topics explored in MedHELM. A.N. reports 
cofounding and owning stock in UpDoc. This entity was not involved in any way 
with the work presented. S.V. reports previously working as an independent 
contractor for OpenAI’s health and safety initiatives; this work was unrelated 
to the research topics explored in MedHELM. J.J. is the founder of Jindal 
Neurology and a per diem physician with Kaiser Permanente. D.Y. reports 
previously working as an independent contractor for OpenAI’s health and safety 
initiatives; this work was unrelated to the research topics explored in MedHELM. 
R.C. reports research funding from Samsung Electronics and serving as advisor to 
XP Health. None of these entities were involved in any way with the work 
presented. F.G. is Executive Director of Healthy Oregon and Chief Medical 
Information Officer at Empathia.ai; none of these entities were involved in any 
way with the work presented. S.L. serves as an advisor to Google, Codex Health, 
Gaia Health and Added Health. None of these entities were involved in any way 
with the work presented. M.F.G. reports institutional grant funding from Xrad 
Therapeutics. K.S. is a shareholder and advisor to UpDoc, Salud Now, Medeloop 
and Alethia. None of these entities were involved in any way with the work 
presented. J.H.C. has received research funding support from: NIH/National 
Institute of Allergy and Infectious Diseases (1R01AI17812101); NIH-NCATS 
Clinical and Translational Science Award (UM1TR004921); Stanford Bio-X 
Interdisciplinary Initiatives Seed Grants Program (IIP); NIH/Center for 
Undiagnosed Diseases at Stanford (U01 NS134358); Stanford RAISE Health Seed 
Grant 2024; and Josiah Macy Jr. Foundation (AI in Medical Education). J.H.C. is 
also cofounder of Reaction Explorer, which develops and licenses organic 
chemistry education software, and has received paid medical expert witness fees 
(Sutton Pierce, Younker Hyde MacFarlane, Sykes McAllister, Elite Experts), 
consulting fees from ISHI Health, and one-time honoraria or travel expenses for 
invited presentations from insitro, General Reinsurance Corporation, AASCIF, 
Cozeva and other industry conferences, academic institutions and health systems. 
E.A. reports consulting fees and stock from Fourier Health. N.A. reports serving 
on Scientific Advisory Boards for January AI, Parallel Bio and Medeloop, and 
cofounding Takeoff41. None of these entities were involved in any way with the 
work presented. A.C. reports cofounding and owning stock in Cognita Imaging; 
owning stock in Subtle Medical, BrainKey and LVIS Corp; and providing consulting 
services to Elucid Bioimaging and Patient Square Capital. None of these entities 
were involved in any way with the work presented. S.K. reports cofounding and 
owning stock options in Virtue AI; this work is unrelated to the research topics 
explored in MedHELM. E.H. reports serving in a senior role at Microsoft and 
owning stock in Microsoft, which has efforts in the healthcare domain. M.A.P. is 
an advisor to Akasa and Surgical Safety Technologies. None of these entities 
were involved in any way with the work presented. N.H.S. reports being a 
cofounder of Prealize Health and Atropos Health, serving on the Board of the 
Coalition for Healthcare AI, and serving as an advisor to Opala, Curai Health, 
JnJ Innovative Medicines and AbbVie pharmaceuticals. None of these entities were 
involved in any way with the work presented. The other authors declare no 
competing interests.


435. Exp Dermatol. 2025 Nov;34(11):e70175. doi: 10.1111/exd.70175.

Putting AI Chatbots to the Test: A Comparative Analysis of Large Language 
Models' Performance in the Context of Basal Cell Carcinoma.

Ortner VK(1)(2), Lent HC(3), Karmisholt KE(1)(4), Lei U(2)(4), Gyldenløve 
M(2)(4), Omland SH(1)(4), Kaur-Knudsen D(2)(4), Kainu KH(2)(4), Togsverd-Bo 
K(1), Wiegell SR(1)(4), Bjerva J(3), Hædersdal M(1)(4).

Author information:
(1)Department of Dermatology, Copenhagen University Hospital-Bispebjerg and 
Frederiksberg, Copenhagen, Denmark.
(2)Department of Dermatology and Allergy, Copenhagen University Hospital-Herlev 
and Gentofte, Hellerup, Denmark.
(3)Department of Computer Science, Aalborg University, Copenhagen, Denmark.
(4)Department of Clinical Medicine, University of Copenhagen, Copenhagen, 
Denmark.

Large language models (LLMs) have been explored in various dermato-oncological 
conditions. In this study, we aimed to compare different LLMs' potential to 
guide clinicians on the treatment of basal cell carcinoma (BCC). Four authors 
formulated 24 questions on the topic of clinical management of BCC. The blinded 
responses of three LLMs (Gemini, Copilot and ChatGPT 4.0) were presented to a 
panel of nine dermato-oncologists for assessment of (i) factual accuracy, (ii) 
concision, (iii) comprehensiveness and (iv) overall preference. In addition, the 
responses were then quantitatively compared based on lexical (i.e., vocabulary) 
and semantic (i.e., meaning) similarity to three additional LLMs (ChatGPT 3.5, 
ChatGPT 4o and Claude). ChatGPT 4.0 had the highest accuracy rate (87.5%, i.e., 
21/24 responses), followed by Gemini (50%) and Copilot (25%). All models scored 
lower for concision and comprehensiveness, with ChatGPT 4.0 in the lead (62.5% 
comprehensive; 54.2% concise), followed by Gemini (33.3%; 12.5%) and Copilot 
(16.7%; 8.3%). The panel achieved consensus on model preference in 16 questions 
(ChatGPT 4.0: 54.2%; Gemini: 8.3%; Copilot: 4.2%; no consensus: 33.3%). While 
the lexical similarity was found to be low (x̄ ~0.07-0.10 across models), the 
semantic similarity between the LLM responses was moderate (x̄ ~0.60-0.70 across 
models). LLMs may assist clinicians in settings where expert dermato-oncological 
guidance is not readily available, with ChatGPT 4.0 currently outperforming both 
Gemini and Copilot. Since quantitative methods are unable to detect clinically 
relevant differences between LLMs, surveying dermatologists is necessary to 
identify useful models in this rapidly developing field.

© 2025 John Wiley & Sons A/S. Published by John Wiley & Sons Ltd.

DOI: 10.1111/exd.70175
PMID: 41217077 [Indexed for MEDLINE]


436. J Med Internet Res. 2025 Mar 18;27:e66279. doi: 10.2196/66279.

Using Synthetic Health Care Data to Leverage Large Language Models for Named 
Entity Recognition: Development and Validation Study.

Šuvalov H(1), Lepson M(1), Kukk V(1), Malk M(1), Ilves N(1), Kuulmets HA(1), 
Kolde R(1).

Author information:
(1)Institute of Computer Science, University of Tartu, Tartu, Estonia.

BACKGROUND: Named entity recognition (NER) plays a vital role in extracting 
critical medical entities from health care records, facilitating applications 
such as clinical decision support and data mining. Developing robust NER models 
for low-resource languages, such as Estonian, remains a challenge due to the 
scarcity of annotated data and domain-specific pretrained models. Large language 
models (LLMs) have proven to be promising in understanding text from any 
language or domain.
OBJECTIVE: This study addresses the development of medical NER models for 
low-resource languages, specifically Estonian. We propose a novel approach by 
generating synthetic health care data and using LLMs to annotate them. These 
synthetic data are then used to train a high-performing NER model, which is 
applied to real-world medical texts, preserving patient data privacy.
METHODS: Our approach to overcoming the shortage of annotated Estonian health 
care texts involves a three-step pipeline: (1) synthetic health care data are 
generated using a locally trained GPT-2 model on Estonian medical records, (2) 
the synthetic data are annotated with LLMs, specifically GPT-3.5-Turbo and 
GPT-4, and (3) the annotated synthetic data are then used to fine-tune an NER 
model, which is later tested on real-world medical data. This paper compares the 
performance of different prompts; assesses the impact of GPT-3.5-Turbo, GPT-4, 
and a local LLM; and explores the relationship between the amount of annotated 
synthetic data and model performance.
RESULTS: The proposed methodology demonstrates significant potential in 
extracting named entities from real-world medical texts. Our top-performing 
setup achieved an F1-score of 0.69 for drug extraction and 0.38 for procedure 
extraction. These results indicate a strong performance in recognizing certain 
entity types while highlighting the complexity of extracting procedures.
CONCLUSIONS: This paper demonstrates a successful approach to leveraging LLMs 
for training NER models using synthetic data, effectively preserving patient 
privacy. By avoiding reliance on human-annotated data, our method shows promise 
in developing models for low-resource languages, such as Estonian. Future work 
will focus on refining the synthetic data generation and expanding the method's 
applicability to other domains and languages.

©Hendrik Šuvalov, Mihkel Lepson, Veronika Kukk, Maria Malk, Neeme Ilves, 
Hele-Andra Kuulmets, Raivo Kolde. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org), 18.03.2025.

DOI: 10.2196/66279
PMCID: PMC11962312
PMID: 40101227 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


437. J Med Syst. 2025 Aug 14;49(1):104. doi: 10.1007/s10916-025-02243-7.

An Exploratory Comparison of AI Models for Preoperative Anesthesia Planning: 
Assessing ChatGPT-4o, Claude 3.5 Sonnet, and ChatGPT-o1 in Clinical Scenario 
Analysis.

Wang B(1), Tian Y(2), Wang XT(3).

Author information:
(1)Department of Anesthesiology, Fourth Hospital of China Medical University, 
Shenyang, China.
(2)Department of Anesthesiology, Shengjing Hospital of China Medical University, 
Shenyang, China.
(3)Department of Anesthesiology, Shengjing Hospital of China Medical University, 
Shenyang, China. wangxt_cmu@163.com.

This exploratory study examined the effectiveness of ChatGPT-4o, Claude 3.5 
Sonnet, and ChatGPT-o1 in developing anesthesia plans for critical cases. 
Personalized anesthesia plans are essential for ensuring surgical safety and 
patient satisfaction. These artificial intelligence (AI) models can understand 
and generate anesthesia-related information. The study included a panel of five 
anesthesia experts, each with over ten years of experience. They qualitatively 
and quantitatively assessed the capabilities of the three models in formulating 
anesthesia plans for critical cases. The results showed no significant 
differences in the response quality, relevance, and applicability scores among 
the models; however, variations were observed in the error types and severity. 
ChatGPT-o1 surpassed the other models in terms of content relevance and 
information accuracy, demonstrating a lower error rate and higher suitability 
for clinical application. As an initial investigation in this rapidly evolving 
field, this research provides preliminary insights while acknowledging the need 
for further validation in clinical settings before implementation.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02243-7
PMID: 40810794 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Human Ethics and Consent to 
Participate Declarations: Not applicable. Competing Interests: The authors 
declare no competing interests. Clinical Trial Number: Not applicable. AI 
Disclosure: AI models were used solely for experimental planning and not for 
drafting or reviewing parts of the manuscript.


438. J Med Internet Res. 2025 Jul 25;27:e75849. doi: 10.2196/75849.

Harm Reduction Strategies for Thoughtful Use of Large Language Models in the 
Medical Domain: Perspectives for Patients and Clinicians.

Moëll B(#)(1), Sand Aronsson F(#)(2)(3).

Author information:
(1)Division of Speech, Music and Hearing, School of Electrical Engineering and 
Computer Science, KTH Royal Institute of Technology, Lindstedsvägen 24, 
Stockholm, 114 28, Sweden, 46 704851893.
(2)Department of Clinical Science, Intervention and Technology, Division of 
Speech and Language Pathology, Karolinska Institutet, Stockholm, Sweden.
(3)Theme Womens Health and Allied Health Professionals, Section of Speech and 
Language Pathology, Karolinska University Hospital, Stockholm, Sweden.
(#)Contributed equally

The integration of large language models (LLMs) into health care presents 
significant risks to patients and clinicians, inadequately addressed by current 
guidance. This paper adapts harm reduction principles from public health to 
medical LLMs, proposing a structured framework for mitigating these 
domain-specific risks while maximizing ethical utility. We outline tailored 
strategies for patients, emphasizing critical health literacy and output 
verification, and for clinicians, enforcing "human-in-the-loop" validation and 
bias-aware workflows. Key innovations include developing thoughtful use 
protocols that position LLMs as assistive tools requiring mandatory 
verification, establishing actionable institutional policies with 
risk-stratified deployment guidelines and patient disclaimers, and critically 
analyzing underaddressed regulatory, equity, and safety challenges. This 
research moves beyond theory to offer a practical roadmap, enabling stakeholders 
to ethically harness LLMs, balance innovation with accountability, and preserve 
core medical values: patient safety, equity, and trust in high-stakes health 
care settings.

© Birger Moell, Fredrik Sand Aronsson. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/75849
PMCID: PMC12296254
PMID: 40712151 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


439. Hear Res. 2026 Jan;469:109473. doi: 10.1016/j.heares.2025.109473. Epub 2025 Nov 
13.

Towards precision medicine for otology and neurotology: Machine learning 
applications and challenges.

Adcock K(1), Arulchelvan E(1), Shields N(1), Vanneste S(2).

Author information:
(1)Lab for Clinical and Integrative Neuroscience, Trinity Institute for 
Neuroscience, School of Psychology, Trinity College Dublin, Ireland.
(2)Lab for Clinical and Integrative Neuroscience, Trinity Institute for 
Neuroscience, School of Psychology, Trinity College Dublin, Ireland; Global 
Brain Health Institute of Neuroscience, Trinity College Dublin, Dublin, Ireland. 
Electronic address: sven.vanneste@tcd.ie.

Advances in artificial intelligence, particularly machine learning and deep 
learning, in conjunction with the rise of personalised medicine, can facilitate 
tailored decision-making for diagnoses, prognoses, and treatment responses based 
on individual patient data. The multifaceted nature of symptoms and disorders in 
(neuro)otology, with their diverse aetiologies and subjective characteristics, 
makes this field an ideal candidate for computational personalised medicine. 
This narrative review critically synthesises applications of machine learning 
and deep learning in otology and neurotology published between 2013 and 2025. 
Relevant studies were identified through targeted searches of PubMed, Scopus, 
and Google Scholar using combinations of terms related to artificial 
intelligence, tinnitus, cochlear implants, and otologic or neurotologic 
disorders. Only peer-reviewed articles focusing on human applications of machine 
learning or deep learning in these fields were included, excluding theoretical 
papers or animal studies. Recent breakthroughs, such as the Whisper speech 
recognition model for cochlear implant simulations and large language models for 
refining tinnitus subgroup identification and therapy predictions, underscore 
the transformative potential of AI in improving clinical outcomes. This review 
is distinct in its emphasis on these emerging technologies and their integration 
into multimodal datasets, combining imaging, audiometric data, and 
patient-reported outcomes to refine diagnosis and treatment approaches. However, 
challenges including the lack of standardisation, limited generalisability of 
models, and the need for improved frameworks for multimodal data integration 
impede rigorous and reproducible implementation, topics that are critically 
explored in this review. Here, we explore the applications of machine learning, 
deep learning, and large language models in tinnitus, cochlear implants, and 
(neuro)tology, providing a critical analysis of recent advancements, persistent 
challenges, and recommendations for future research. By addressing these 
challenges and implementing recommended strategies, this review outlines a 
pathway for integrating cutting-edge artificial intelligence tools into clinical 
practice, underscoring their immense potential to revolutionise precision 
medicine in otology and neurotology and improve patient outcomes.

Copyright © 2025. Published by Elsevier B.V.

DOI: 10.1016/j.heares.2025.109473
PMID: 41274259 [Indexed for MEDLINE]


440. Cyberpsychol Behav Soc Netw. 2025 Apr;28(4):284-289. doi: 
10.1089/cyber.2024.0407. Epub 2025 Feb 24.

Generative Artificial Intelligence and Misinformation Acceptance: An 
Experimental Test of the Effect of Forewarning About Artificial Intelligence 
Hallucination.

Hwang Y(1), Jeong SH(2).

Author information:
(1)Department of Digital Media, Myongji University, Seodaemun-gu, Seoul, Korea.
(2)College of Media and Communication, Korea University, Seongbuk-gu, Seoul, 
Korea.

Generative artificial intelligence (AI) tools could create statements that are 
seemingly plausible but factually incorrect. This is referred to as AI 
hallucination, which can contribute to the generation and dissemination of 
misinformation. Thus, the present study examines whether forewarning about AI 
hallucination could reduce individuals' acceptance of AI-generated 
misinformation. An online experiment with 208 Korean adults demonstrated that AI 
hallucination forewarning reduced misinformation acceptance (p = 0.001, Cohen's 
d = 0.45) while forewarning did not reduce acceptance of true information (p = 
0.91). In addition, the effect of AI hallucination forewarning on misinformation 
acceptance was moderated by preference for effortful thinking (p < 0.01) such 
that forewarning decreased misinformation acceptance when preference for 
effortful thinking was high (vs. low).

DOI: 10.1089/cyber.2024.0407
PMID: 39992238 [Indexed for MEDLINE]


441. J Adv Nurs. 2025 Nov;81(11):7489-7499. doi: 10.1111/jan.16101. Epub 2024 Feb 17.

A comparative vignette study: Evaluating the potential role of a generative AI 
model in enhancing clinical decision-making in nursing.

Saban M(1), Dubovi I(1).

Author information:
(1)Nursing Department, Steyer School of Health Professions, Faculty of Medical 
and Health Sciences, Tel Aviv University, Tel Aviv, Israel.

AIM: This study explores the potential of a generative artificial intelligence 
tool (ChatGPT) as clinical support for nurses. Specifically, we aim to assess 
whether ChatGPT can demonstrate clinical decision-making equivalent to that of 
expert nurses and novice nursing students. This will be evaluated by comparing 
ChatGPT responses to clinical scenarios to those of nurses on different levels 
of experience.
DESIGN: This is a cross-sectional study.
METHODS: Emergency room registered nurses (i.e. experts; n = 30) and nursing 
students (i.e. novices; n = 38) were recruited during March-April 2023. Clinical 
decision-making was measured using three validated clinical scenarios involving 
an initial assessment and reevaluation. Clinical decision-making aspects 
assessed were the accuracy of initial assessments, the appropriateness of 
recommended tests and resource use and the capacity to reevaluate decisions. 
Performance was also compared by timing response generations and word counts. 
Expert nurses and novice students completed online questionnaires (via 
Qualtrics), while ChatGPT responses were obtained from OpenAI.
RESULTS: Concerning aspects of clinical decision-making and compared to novices 
and experts: (1) ChatGPT exhibited indecisiveness in initial assessments; (2) 
ChatGPT tended to suggest unnecessary diagnostic tests; (3) When new information 
required re-evaluation, ChatGPT responses demonstrated inaccurate understanding 
and inappropriate modifications. In terms of performance, the mean number of 
words utilized in ChatGPT answers was 27-41 times greater than that utilized by 
both experts and novices; and responses were provided approximately 4 times 
faster than those of novices and twice faster than expert nurses. ChatGPT 
responses maintained logical structure and clarity.
CONCLUSIONS: A generative AI tool demonstrated indecisiveness and a tendency 
towards over-triage compared to human clinicians.
IMPACT: The study shows that it is important to approach the implementation of 
ChatGPT as a nurse's digital assistant with caution. More study is needed to 
optimize the model's training and algorithms to provide accurate healthcare 
support that aids clinical decision-making.
REPORTING METHOD: This study adhered to relevant EQUATOR guidelines for 
reporting observational studies.
PATIENT OR PUBLIC CONTRIBUTION: Patients were not directly involved in the 
conduct of this study.

© 2024 The Authors. Journal of Advanced Nursing published by John Wiley & Sons 
Ltd.

DOI: 10.1111/jan.16101
PMCID: PMC12535322
PMID: 38366690 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that they have no known 
competing financial interests or personal relationships that could have appeared 
to influence the work reported in this paper.


442. JMIR Med Inform. 2025 Jul 24;13:e69286. doi: 10.2196/69286.

A Weighted Voting Approach for Traditional Chinese Medicine Formula 
Classification Using Large Language Models: Algorithm Development and Validation 
Study.

Wang Z(1)(2), Li K(3), Peng S(4), Liu L(4), Yang X(1), Yao K(4), Herre H(2)(5), 
Zhu Y(4).

Author information:
(1)Institute of Basic Medical Sciences, Chinese Academy of Medical Sciences; 
School of Basic Medicine, Peking Union Medical College, Beijing, China.
(2)Institute for Medical Informatics, Statistics and Epidemiology, University of 
Leipzig, Leipzig, Germany.
(3)School of Medical Information, Changchun University of Chinese Medicine, 
Changchun, China.
(4)Institute of Information on Traditional Chinese Medicine, China Academy of 
Chinese Medical Sciences, No 16, Nanxiao Street, Dongzhimen, Beijing, 100010, 
China, 86 010 64089639.
(5)Institute for Computer Science, University of Leipzig, Leipzig, Germany.

BACKGROUND: Several clinical cases and experiments have demonstrated the 
effectiveness of traditional Chinese medicine (TCM) formulas in treating and 
preventing diseases. These formulas contain critical information about their 
ingredients, efficacy, and indications. Classifying TCM formulas based on this 
information can effectively standardize TCM formulas management, support 
clinical and research applications, and promote the modernization and scientific 
use of TCM. To further advance this task, TCM formulas can be classified using 
various approaches, including manual classification, machine learning, and deep 
learning. Additionally, large language models (LLMs) are gaining prominence in 
the biomedical field. Integrating LLMs into TCM research could significantly 
enhance and accelerate the discovery of TCM knowledge by leveraging their 
advanced linguistic understanding and contextual reasoning capabilities.
OBJECTIVE: The objective of this study is to evaluate the performance of 
different LLMs in the TCM formula classification task. Additionally, by 
employing ensemble learning with multiple fine-tuned LLMs, this study aims to 
enhance classification accuracy.
METHODS: The data for the TCM formula were manually refined and cleaned. We 
selected 10 LLMs that support Chinese for fine-tuning. We then employed an 
ensemble learning approach that combined the predictions of multiple models 
using both hard and weighted voting, with weights determined by the average 
accuracy of each model. Finally, we selected the top 5 most effective models 
from each series of LLMs for weighted voting (top 5) and the top 3 most accurate 
models of 10 for weighted voting (top 3).
RESULTS: A total of 2441 TCM formulas were curated manually from multiple 
sources, including the Coding Rules for Chinese Medicinal Formulas and Their 
Codes, the Chinese National Medical Insurance Catalog for proprietary Chinese 
medicines, textbooks of TCM formulas, and TCM literature. The dataset was 
divided into a training set of 1999 TCM formulas and test set of 442 TCM 
formulas. The testing results showed that Qwen-14B achieved the highest accuracy 
of 75.32% among the single models. The accuracy rates for hard voting, weighted 
voting, weighted voting (top 5), and weighted voting (top 3) were 75.79%, 
76.47%, 75.57%, and 77.15%, respectively.
CONCLUSIONS: This study aims to explore the effectiveness of LLMs in the TCM 
formula classification task. To this end, we propose an ensemble learning method 
that integrates multiple fine-tuned LLMs through a voting mechanism. This method 
not only improves classification accuracy but also enhances the existing 
classification system for classifying the efficacy of TCM formula.

© Zhe Wang, Keqian Li, Suyuan Peng, Lihong Liu, Xiaolin Yang, Keyu Yao, Heinrich 
Herre, Yan Zhu. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org).

DOI: 10.2196/69286
PMCID: PMC12292024
PMID: 40705933 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


443. JMIR Med Inform. 2025 Aug 18;13:e68955. doi: 10.2196/68955.

Efficient Detection of Stigmatizing Language in Electronic Health Records via 
In-Context Learning: Comparative Analysis and Validation Study.

Chen H(1), Alfred M(1), Cohen E(1).

Author information:
(1)Department of Mechanical and Industrial Engineering, University of Toronto, 
Toronto, ON, Canada.

Comment in
    JMIR Med Inform. 13:e82484.

BACKGROUND: The presence of stigmatizing language within electronic health 
records (EHRs) poses significant risks to patient care by perpetuating biases. 
While numerous studies have explored the use of supervised machine learning 
models to detect stigmatizing language automatically, these models require 
large, annotated datasets, which may not always be readily available. In-context 
learning (ICL) has emerged as a data-efficient alternative, allowing large 
language models to adapt to tasks using only instructions and examples.
OBJECTIVE: We aimed to investigate the efficacy of ICL in detecting stigmatizing 
language within EHRs under data-scarce conditions.
METHODS: We analyzed 5043 sentences from the Medical Information Mart for 
Intensive Care-IV dataset, which contains EHRs from patients admitted to the 
emergency department at the Beth Israel Deaconess Medical Center. We compared 
ICL with zero-shot (textual entailment), few-shot (SetFit), and supervised 
fine-tuning approaches. The ICL approach used 4 prompting strategies: generic, 
chain of thought, clue and reasoning prompting, and a newly introduced stigma 
detection guided prompt. Model fairness was evaluated using the equal 
performance criterion, measuring true positive rate, false positive rate, and 
F1-score disparities across protected attributes, including sex, age, and race.
RESULTS: In the zero-shot setting, the best-performing ICL model, GEMMA-2, 
achieved a mean F1-score of 0.858 (95% CI 0.854-0.862), showing an 18.7% 
improvement over the best textual entailment model, DEBERTA-M (mean F1-score 
0.723, 95% CI 0.718-0.728; P<.001). In the few-shot setting, the top ICL model, 
LLAMA-3, outperformed the leading SetFit models by 21.2%, 21.4%, and 12.3% with 
4, 8, and 16 annotations per class, respectively (P<.001). Using 32 labeled 
instances, the best ICL model achieved a mean F1-score of 0.901 (95% CI 
0.895-0.907), only 3.2% lower than the best supervised fine-tuning model, 
ROBERTA (mean F1-score 0.931, 95% CI 0.924-0.938), which was trained on 3543 
labeled instances. Under the conditions tested, fairness evaluation revealed 
that supervised fine-tuning models exhibited greater bias compared with ICL 
models in the zero-shot, 4-shot, 8-shot, and 16-shot settings, as measured by 
true positive rate, false positive rate, and F1-score disparities.
CONCLUSIONS: ICL offers a robust and flexible solution for detecting 
stigmatizing language in EHRs, offering a more data-efficient and equitable 
alternative to conventional machine learning methods. These findings suggest 
that ICL could enhance bias detection in clinical documentation while reducing 
the reliance on extensive labeled datasets.

©Hongbo Chen, Myrtede Alfred, Eldan Cohen. Originally published in JMIR Medical 
Informatics (https://medinform.jmir.org), 18.08.2025.

DOI: 10.2196/68955
PMCID: PMC12402740
PMID: 40825541 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


444. JMIR Med Inform. 2025 Jul 9;13:e71176. doi: 10.2196/71176.

Language Models for Multilabel Document Classification of Surgical Concepts in 
Exploratory Laparotomy Operative Notes: Algorithm Development Study.

Balch JA(1)(2), Desaraju SS(3), Nolan VJ(1), Vellanki D(3), Buchanan TR(3), 
Brinkley LM(3), Penev Y(3), Bilgili A(3), Patel A(3), Chatham CE(3), Vanderbilt 
DM(3), Uddin R(3), Bihorac A(4)(5), Efron P(1), Loftus TJ(1)(4), Rahman P(2), 
Shickel B(4)(5).

Author information:
(1)Department of Surgery, University of Florida College of Medicine, 
Gainesville, FL, United States.
(2)Department of Health Outcomes and Biomedical Informatics, University of 
Florida College of Medicine, Gainesville, FL, United States.
(3)University of Florida College of Medicine, Gainesville, FL, United States.
(4)Intelligent Clinical Care Center, University of Florida, Gainesville, FL, 
United States.
(5)Department of Medicine, University of Florida College of Medicine, 1600 SW 
Archer Road, PO Box 100224, Gainesville, FL, 32610, United States, 3522739958, 1 
3522739221.

BACKGROUND: Operative notes are frequently mined for surgical concepts in 
clinical care, research, quality improvement, and billing, often requiring hours 
of manual extraction. These notes are typically analyzed at the document level 
to determine the presence or absence of specific procedures or findings (eg, 
whether a hand-sewn anastomosis was performed or contamination occurred). 
Extracting several binary classification labels simultaneously is a multilabel 
classification problem. Traditional natural language processing 
approaches-bag-of-words (BoW) and term frequency-inverse document frequency 
(tf-idf) with linear classifiers-have been used previously for this task but are 
now being augmented or replaced by large language models (LLMs). However, few 
studies have examined their utility in surgery.
OBJECTIVE: We developed and evaluated LLMs for the purpose of expediting data 
extraction from surgical notes.
METHODS: A total of 388 exploratory laparotomy notes from a single institution 
were annotated for 21 concepts related to intraoperative findings, 
intraoperative techniques, and closure techniques. Annotation consistency was 
measured using the Cohen κ statistic. Data were preprocessed to include only the 
description of the procedure. We compared the evolution of document 
classification technologies from BoW and tf-idf to encoder-only 
(Clinical-Longformer) and decoder-only (Llama 3) transformer models. Multilabel 
classification performance was evaluated with 5-fold cross-validation with 
F1-score and hamming loss (HL). We experimented with and without context. Errors 
were assessed by manual review. Code and implementation instructions may be 
found on GitHub.
RESULTS: The prevalence of labels ranged from 0.05 (colostomy, ileostomy, active 
bleed from named vessel) to 0.50 (running fascial closure). Llama 3.3 was the 
overall best-performing model (micro F1-score 0.88, 5-fold range: 0.88-0.89; HL 
0.11, 5-fold range: 0.11-0.12). The BoW model (micro F1-score 0.68, 5-fold 
range: 0.64-0.71; HL 0.14, 5-fold range: 0.13-0.16) and Clinical-Longformer 
(micro F1-score 0.73, 5-fold range: 0.70-0.74; HL 0.11, 5-fold range: 0.10-0.12) 
had overall similar performance, with tf-idf models trailing (micro F1-score 
0.57, 5-fold range: 0.55-0.59; HL 0.27, 5-fold range: 0.25-0.29). F1-scores 
varied across concepts in the Llama model, ranging from 0.30 (5-fold range: 
0.23-0.39) for class III contamination to 0.92 (5-fold range: 0.98-0.84) for 
bowel resection. Context enhanced Llama's performance, adding an average of 0.16 
improvement to the F1-scores. Error analysis demonstrated semantic nuances and 
edge cases within operative notes, particularly when patients had references to 
prior operations in their operative notes or simultaneous operations with other 
surgical services.
CONCLUSIONS: Off-the-shelf autoregressive LLMs outperformed fined-tuned, 
encoder-only transformers and traditional natural language processing techniques 
in classifying operative notes. Multilabel classification with LLMs may 
streamline retrospective reviews in surgery, though further refinements are 
required prior to reliable use in research and quality improvement.

© Jeremy A Balch, Sasank S Desarju, Victoria J Nolan, Divya Vallanki, Timothy R 
Buchanan, Lindsey M Brinkley, Yordan Penev, Ahmet Bilgili, Aashay Patel, Corinne 
E Chatham, David M Vanderbilt, Rayon Uddin, Azra Bihorac, Philip Efron, Tyler J 
Loftus, Protiva Rahman, Benjamin Shickel. Originally published in JMIR Medical 
Informatics (https://medinform.jmir.org).

DOI: 10.2196/71176
PMCID: PMC12266303
PMID: 40632815 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


445. Eur J Obstet Gynecol Reprod Biol. 2026 Jan 10;318:114953. doi: 
10.1016/j.ejogrb.2026.114953. Online ahead of print.

Application and validation of AI-driven methods to explore patient experiences 
of pre-cervical cancer.

Luo MY(1), Williams CYK(2).

Author information:
(1)School of Clinical Medicine, University of Cambridge, Cambridge, United 
Kingdom. Electronic address: myl41@cam.ac.uk.
(2)Bakar Computational Health Sciences Institute, University of California, San 
Francisco, CA, the United States of America.

OBJECTIVE: We sought to apply novel natural language processing (NLP) tools to 
explore patient experiences of pre-cervical cancer on social media and validate 
the performance of these tools.
METHODS: All posts and comments were extracted from the forum 
r/PreCervicalCancer on social media platform Reddit. Using BERTopic, posts were 
clustered into topics according to their semantic similarity, which were 
manually reviewed. Topic headings were derived using a large language model 
(LLM) and compared to manually curated headings. Clustering outliers were 
reassigned by BERTopic, an LLM and by manual methods in parallel and compared. 
Post and comment sentiment were quantitatively analysed using VADER. Post upvote 
scores and comments counts were analysed to measure community engagement.
RESULTS: 4592 posts were extracted from r/PreCervicalCancer. Posts clustered 
into 10 different topics using BERTopic with 88.0% accuracy. 80.0% of topic 
headings generated by GPT-4o mini were deemed appropriate. Reassignment of 
clustering outliers by BERTopic and GPT-4o mini was limited, 52.8% and 41.1% 
accuracy, respectively. Key clinical findings reflect several common concerns 
among patients, particularly regarding specific lasting physical and 
psychological impact of procedures like LEEP, result anxiety, and challenges in 
healthcare navigation. Comments had less negative sentiment than posts (Cohen's 
d = 0.46), suggesting support.
CONCLUSIONS: In this cross-sectional study, we validated NLP tools to analyse 
content, sentiment and reactions to 4592 posts on pre-cervical cancer. Our 
findings suggest that, with minimal human oversight, automated methods can 
accurately conduct large-scale analyses of similar clinical content, unlocking 
new insights of patient experiences using non-traditional data sources.

Copyright © 2026 The Author(s). Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ejogrb.2026.114953
PMID: 41534136

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


446. J Voice. 2026 Jan 2:S0892-1997(25)00538-7. doi: 10.1016/j.jvoice.2025.12.015. 
Online ahead of print.

Evaluating ChatGPT's Adherence to Hoarseness Guidelines: A Three-Rater Study 
Including an Otolaryngologist, an Audiologist, and the Model Itself.

Durgut M(1), Durgut O(2).

Author information:
(1)Department of Otorhinolaryngology, Audiology Unit, Kocaeli University Faculty 
of Medicine, Kocaeli, Turkey. Electronic address: durgut_merve@hotmail.com.
(2)Department of Otorhinolaryngology, Health Science University, Bursa City 
Hospital, Bursa Turkey. Electronic address: durgutosman@yahoo.com.

OBJECTIVE: To assess the alignment of Chat Generative Pre-trained Transformer 
(ChatGPT), based on Generative Pre-trained Transformer 4 (GPT-4) with the 2018 
Clinical Practice Guideline on Hoarseness (Dysphonia), using a structured 
three-rater evaluation involving an otolaryngologist, an audiologist, and 
ChatGPT.
METHODS: Thirteen guideline statements were converted into 15 open-ended 
clinical questions and independently answered by ChatGPT. Responses were 
assessed for consistency with the guideline using a three-point scale 
(consistent, partially consistent, inconsistent). Evaluations were performed by 
an otolaryngologist, an audiologist, and ChatGPT itself, with final adjudication 
by a senior otolaryngologist.
RESULTS: Of 15 items, 13 responses (86.7%) were rated as fully consistent by all 
three raters. Two responses (13.3%) were rated as partially consistent by one 
evaluator each. No responses were deemed inconsistent. Overall agreement across 
raters was 97.8%.
CONCLUSION: ChatGPT's responses showed high concordance with expert 
recommendations in the evaluation and management of hoarseness. These findings 
support the potential of large language models as adjunctive tools for patient 
education and clinical decision-making in voice disorders, when used under 
expert oversight.

Copyright © 2025 The Voice Foundation. Published by Elsevier Inc. All rights 
reserved.

DOI: 10.1016/j.jvoice.2025.12.015
PMID: 41484044

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


447. J Craniofac Surg. 2026 Jan 16. doi: 10.1097/SCS.0000000000012413. Online ahead 
of print.

AI-Based CT Image Recognition With Med-Gemini-3D in the Diagnosis of a Rare 
Craniofacial Condition: A Catlin Mark Skull.

Hajebian HH(1), Friel MT(2)(3).

Author information:
(1)Department of Surgery, Garnet Health Medical Center, Middletown, NY.
(2)Department of Pediatric Plastic Surgery, Gayle and Tom Benson Ochsner 
Children's Hospital.
(3)Department of Surgery, Division of Plastic Surgery, Tulane University School 
of Medicine, New Orleans, LA.

Artificial intelligence (AI) is increasingly applied in diagnostic imaging to 
enhance pattern recognition and support clinical decision-making. In 2024, 
Google introduced Med-Gemini-3D, a multimodal platform capable of interpreting 
3-dimensional computed tomography scans and generating radiologist-level 
reports. Although not yet approved for independent clinical use, such systems 
may assist in identifying rare conditions that are unfamiliar to clinicians. The 
authors describe a 22-month-old girl who presented with persistent bilateral 
parietal skull defects and global developmental delay. Computed tomography 
demonstrated symmetric ossification defects adjacent to the sagittal suture that 
were not initially recognized by the treating physician. The patient's mother 
used a smartphone application powered by Med-Gemini-3D to analyze a 3D-CT 
reconstruction image, which suggested a diagnosis of "Catlin mark skull," a 
historical term for Enlarged Parietal Foramina (EPF). This prompted genetic 
evaluation and identification of a CDC42BPB variant associated with 
Chilton-Okur-Chung neurodevelopmental syndrome-a finding not previously reported 
in association with EPF. Establishing the diagnosis facilitated earlier 
therapeutic interventions and informed long-term management. This case 
underscores the potential role of AI-assisted tools in recognizing rare 
craniofacial anomalies. While such technologies cannot replace clinical 
expertise and remain limited by variable accuracy, they may help expand 
differential diagnoses, expedite referrals, and improve outcomes through earlier 
intervention. Continued research is needed to validate their reliability and to 
define their optimal integration into clinical practice.

Copyright © 2026 by Mutaz B. Habal, MD.

DOI: 10.1097/SCS.0000000000012413
PMID: 41543485

Conflict of interest statement: The authors report no conflicts of interest.


448. PLoS One. 2025 Sep 9;20(9):e0330288. doi: 10.1371/journal.pone.0330288. 
eCollection 2025.

Protocol for a core outcome set for pharmacological treatments in hospitalised 
patients with acute viral respiratory infections (COSAVRI).

Devane D(1)(2)(3), Briel M(4), Bhagani S(5), Boesten N(6), Buchholz S(7)(8), De 
Luca EC(6), Guedj J(9), Koryakina A(10), Kothari K(3), Lacombe K(11), Mallon 
PW(12), Massonnaud CR(13), O'Dwyer J(14), Olsen IC(15), Saif-Ur-Rahman KM(13), 
Schwenke JM(4), Thomas J(10), Yazdanpanah Y(16), Zgaga L(17), Louw J(12).

Author information:
(1)Center for Health Research Methods, School of Nursing and Midwifery, 
University of Galway, Galway, Ireland.
(2)Health Research Board-Trials Methodology Research Network (HRB-TMRN), 
University of Galway, Galway, Ireland.
(3)Evidence Synthesis Ireland and Cochrane Ireland, University of Galway, 
Galway, Ireland.
(4)CLEAR Methods Center, Division of Clinical Epidemiology, Department of 
Clinical Research, University Hospital Basel, University of Basel, Basel, 
Switzerland.
(5)Department of Infectious Diseases and HIV Medicine, Royal Free London NHS 
Trust and Division of Infection and Immunity, University College London, London, 
United Kingdom.
(6)European Patients Forum, Brussels, Belgium.
(7)Public Health Threats Department, European Medicines Agency, Amsterdam, 
Netherlands.
(8)Division 32, Infectiology/Dermatology/Allergology, Federal Institute for 
Drugs and Medical Devices, Bonn, Germany.
(9)Université Paris Cité, INSERM, IAME, Paris, France.
(10)EPPI Centre, Social Research Institute, University College London, London, 
United Kingdom.
(11)Sorbonne Université, INSERM IPLESP, Infectious Diseases Department, St 
Antoine Hospital, APHP, Paris, France.
(12)Centre for Experimental Pathogen Host Research, School of Medicine, 
University College Dublin, Dublin, Ireland.
(13)Département d'Épidémiologie, Biostatistique et Recherche Clinique, Hôpital 
Bichat, Université Paris Cité, Inserm, IAME, APHP, Paris, France.
(14)Discipline of Pharmacy, School of Pharmacy and Medical Sciences, University 
of Galway, Galway, Ireland.
(15)Department of Research Support for Clinical Trials, Oslo University 
Hospital, Oslo, Norway.
(16)ANRS Emerging Infectious Disease, Inserm, Paris, France.
(17)Department of Public Health and Primary Care, Institute of Population 
Health, Trinity College Dublin, University of Dublin, Dublin, Ireland.

BACKGROUND: Acute viral respiratory infections (AVRIs) rank among the most 
common causes of hospitalisation worldwide, imposing significant healthcare 
burdens and driving the development of pharmacological treatments. However, 
inconsistent outcome reporting across clinical trials limits evidence synthesis 
and its translation into clinical practice. A core outcome set (COS) for 
pharmacological treatments in hospitalised adults with AVRIs is essential to 
standardise trial outcomes and improve research comparability.
OBJECTIVE: To develop an internationally agreed COS for pharmacological 
treatments in hospitalised adults ≥18 years with acute viral respiratory 
infections (COSAVRI) through stakeholder agreement.
METHODS: This protocol follows a four-stage development process in accordance 
with Core Outcome Set Handbook guidelines. Stage 1 comprises a rapid scoping 
review of randomised controlled trials (2015-2025) to systematically catalogue 
patient-relevant outcomes reported in pharmacological AVRI treatment studies. 
Semi-automated screening and data extraction will employ machine learning and 
large language models, with human verification. Stage 2 involves an online 
Real-Time Delphi survey with international stakeholders, including healthcare 
professionals, researchers, patients/caregivers, and policymakers, to prioritise 
identified outcomes using a 9-point scale. Stage 3 consists of structured online 
consensus meetings utilising anonymous electronic voting to finalise the COS. 
Stage 4 focuses on dissemination and implementation through academic 
publications, conferences, and stakeholder engagement.
EXPECTED OUTCOMES: COSAVRI will provide a standardised minimum set of outcomes 
for measuring and reporting in future pharmacological trials involving 
hospitalised adults with AVRIs. This initiative will enhance evidence synthesis, 
reduce research waste, support regulatory decision-making, and improve pandemic 
preparedness by facilitating the rapid deployment of harmonised outcomes in 
trial protocols.

Copyright: © 2025 Devane et al. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0330288
PMCID: PMC12419614
PMID: 40924748 [Indexed for MEDLINE]

Conflict of interest statement: DD, NB, SBu, ECDL, JG, AK, KK, CM, JO, ICO, 
KMSUR, JMS, JT, YY, LZ and JL have no conflicts of interests to declare. MB 
reports research grants from Moderna within the previous three years, all paid 
to the institution and all outside the submitted work. SB reports research 
grants from Abbvie, Gilead and MSD, and honoraria for educational presentations 
and advisory board attendance from Abbvie, Gilead, MSD and Pfizer. KL reports 
honoraria for educational presentations and advisory board attendance from 
Gilead, MSD and ViiV Healthcare. PM reports honoraria for educational 
presentations or advisory activities from Gilead Sciences, Janssen and MSD and 
grant income to his institution from GSK Ireland. This does not alter our 
adherence to PLOS ONE policies on sharing data and materials.


449. PLoS One. 2026 Jan 2;21(1):e0340277. doi: 10.1371/journal.pone.0340277. 
eCollection 2026.

The use of large language models in generating multiple choice questions for 
health professions education: A systematic review and network meta-analysis.

Riehm L(1)(2), Nanji K(3)(4), Lakhani M(5), Pankiv E(1)(2), Hasanee D(6), 
Pfeifer W(2).

Author information:
(1)Department of Anesthesia and Pain Medicine, The Hospital for Sick Children, 
Toronto, Ontario, Canada.
(2)Department of Anesthesiology and Pain Medicine, University of Toronto, 
Toronto, Ontario, Canada.
(3)Division of Ophthalmology, Department of Surgery, McMaster University, 
Hamilton, Ontario, Canada.
(4)Department of Health Research Methods, Evidence and Impact, McMaster 
University, Hamilton, Ontario, Canada.
(5)Faculty of Medicine, University of Ottawa, Ottawa, Ontario, Canada.
(6)Department of Surgery, McMaster University, Hamilton, Ontario, Canada.

PURPOSE: Large language models (LLMs) have the potential to change medical 
education. Whether LLMs can generate multiple-choice questions (MCQs) that are 
of similar quality to those created by humans is unclear. This investigation 
assessed the quality of MCQs generated by LLMs compared to humans.
METHODS: This review was registered with PROSPERO (CRD42025608775). A systematic 
review and frequentist random-effects network meta-analysis (NMA) or pairwise 
meta-analysis was performed. Ovid MEDLINE, Ovid EMBASE, and Scopus were searched 
from inception to November 1, 2024. The quality of MCQs was assessed with seven 
pre-defined outcomes: question relevance, clarity, accuracy/correctness; 
distractor quality; item difficulty analysis; and item discrimination analysis 
(point biserial correlation and item discrimination index). Continuous data were 
transformed to a 10-point scale to facilitate statistical analysis and reported 
as mean differences (MD). The MERSQI and the Grade of Recommendations, 
Assessment, Development and Evaluation (GRADE) NMA guidelines were used to 
assess risk of bias and certainty of evidence assessments.
RESULTS: Five LLMs were included. NMA demonstrated that ChatGPT 4 generated 
similar quality MCQs to humans with regards to question relevance (MD -0.13; 95% 
CI: -0.44,0.18; GRADE: VERY LOW), question clarity (MD -0.03; 95% CI: 
-0.15,0.10; GRADE: VERY LOW), and distractor quality (MD -0.10; 95% CI: 
-0.24,0.04; GRADE: VERY LOW); however, MCQs generated by Llama 2 performed worse 
than humans with regards to question clarity (MD -1.21; 95% CI: -1.60,-0.82; 
GRADE: VERY LOW) and distractor quality (MD -1.50; 95% CI: -2.03,-0.97; GRADE: 
VERY LOW). Exploratory post-hoc t-tests demonstrated that ChatGPT 3.5 performed 
worse than Llama 2 and ChatGPT 4 with regards to question clarity and distractor 
quality (p < 0.001).
CONCLUSION: ChatGPT 4 may create similar quality MCQs to humans, whereas ChatGPT 
3.5 and Llama 2 may be of worse quality. Further studies that directly compare 
these LLMs to human-generated questions and administer MCQs to students are 
required.

Copyright: © 2026 Riehm et al. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0340277
PMCID: PMC12758716
PMID: 41481658 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


450. Int J Surg. 2026 Jan 1;112(1):190-201. doi: 10.1097/JS9.0000000000003406. Epub 
2025 Sep 4.

From algorithms to operating room: can large language models master China's 
attending anesthesiology exam? A cross-sectional evaluation.

He Q(1), Tan Z(2), Niu W(2), Chen D(3)(4), Zhang X(2), Qin F(1), Yuan J(1).

Author information:
(1)Department of Urology and Andrology Laboratory, West China Hospital, Sichuan 
University, Chengdu, Sichuan Province, China.
(2)Department of Anesthesiology, West China Hospital, Sichuan University, 
Chengdu, Sichuan Province, China.
(3)Department of Anesthesiology, West China Second Hospital, Sichuan University, 
Chengdu, Sichuan Province, China.
(4)Key Laboratory of Birth Defects and Related Diseases of Women and Children 
(Sichuan University), Ministry of Education, Chengdu, Sichuan Province, China.

OBJECTIVE: The performance of large language models (LLMs) in complex clinical 
reasoning tasks is not well established. This study compares ChatGPT (GPT-3.5 
and GPT-4) and DeepSeek (DeepSeek-V3 and DeepSeek-R1) in the Chinese 
anesthesiology attending physician examination (CAAPE), aiming to set artificial 
intelligence (AI) benchmarks in medical assessments and enhance AI-driven 
medical education.
METHODS: This cross-sectional study assessed 4 iterations of 2 major LLMs on the 
2025 CAAPE question bank (5647 questions). Testing employed diverse querying 
strategies and languages, with subgroup analyses by subspecialty, knowledge 
type, and question format. The focus was on LLM performance in clinical and 
logical reasoning tasks, measuring accuracy, error types, and response times.
RESULTS: DeepSeek-R1 (70.6-73.4%) and GPT-4 (68.6-70.3%) outperformed 
DeepSeek-V3 (53.1-55.5%) and GPT-3.5 (52.2-55.7%) across all strategies. System 
role (SR) improved performance, while joint response degraded it. DeepSeek-R1 
outperformed GPT-4 in complex subspecialties, reaching peak accuracy (73.4%) 
under SR combined initial response. Generative Pre-trained Transformers (GPT) 
models performed better with English than Chinese queries. All models excelled 
in basic knowledge and Type A1 questions but struggled with clinical scenarios 
and advanced reasoning. Despite DeepSeek-R1's stronger performance, its response 
time was longer. Errors were primarily logical and informational (over 70%), 
with more than half being high-risk clinical errors.
CONCLUSION: LLMs show promise in complex clinical reasoning but risk critical 
errors in high-risk settings. While useful for education and decision support, 
their error potential must be carefully assessed in high-stakes environments.

Copyright © 2025 The Author(s). Published by Wolters Kluwer Health, Inc.

DOI: 10.1097/JS9.0000000000003406
PMCID: PMC12825541
PMID: 40905848 [Indexed for MEDLINE]

Conflict of interest statement: No conflicts of interest need to be disclosed.


451. JMIR Med Inform. 2025 Jul 8;13:e76773. doi: 10.2196/76773.

Toward Cross-Hospital Deployment of Natural Language Processing Systems: Model 
Development and Validation of Fine-Tuned Large Language Models for Disease Name 
Recognition in Japanese.

Shimizu S(1), Nishiyama T(1), Nagai H(1), Wakamiya S(1), Aramaki E(1).

Author information:
(1)Nara Institute of Science and Technology, 8916-5, Takayama-cho, Ikoma-shi, 
Nara, 630-0192, Japan, 81 743-72-5250.

BACKGROUND: Disease name recognition is a fundamental task in clinical natural 
language processing, enabling the extraction of critical patient information 
from electronic health records. While recent advances in large language models 
(LLMs) have shown promise, most evaluations have focused on English, and little 
is known about their robustness in low-resource languages such as Japanese. In 
particular, whether these models can perform reliably on previously unseen 
in-hospital data, which differs from training data in writing styles and 
clinical contexts, has not been thoroughly investigated.
OBJECTIVE: This study evaluated the robustness of fine-tuned LLMs for disease 
name recognition in Japanese clinical notes, with a particular focus on their 
performance on in-hospital data that was not included during training.
METHODS: We used two corpora for this study: (1) a publicly available set of 
Japanese case reports denoted as CR, and (2) a newly constructed corpus of 
progress notes, denoted as PN, written by ten physicians to capture stylistic 
variations of in-hospital clinical notes. To reflect real-world deployment 
scenarios, we first fine-tuned models on CR. Specifically, we compared a LLM and 
a baseline-masked language model (MLM). These models were then evaluated under 
two conditions: (1) on CR, representing the in-domain (ID) setting with the same 
document type, similar to training, and (2) on PN, representing the 
out-of-domain (OOD) setting with a different document type. Robustness was 
assessed by calculating the performance gap (ie, the performance drop from 
in-domain to out-of-domain settings).
RESULTS: The LLM demonstrated greater robustness, with a smaller performance gap 
in F1-scores (ID-OOD = -8.6) compared to the MLM baseline performance (ID-OOD = 
-13.9). This indicated more stable performance across ID and OOD settings, 
highlighting the effectiveness of fine-tuned LLMs for reliable use in diverse 
clinical settings.
CONCLUSIONS: Fine-tuned LLMs demonstrate superior robustness for disease name 
recognition in Japanese clinical notes, with a smaller performance gap. These 
findings highlight the potential of LLMs as reliable tools for clinical natural 
language processing in low-resource language settings and support their 
deployment in real-world health care applications, where diversity in 
documentation is inevitable.

© Seiji Shimizu, Tomohiro Nishiyama, Hiroyuki Nagai, Shoko Wakamiya, Eiji 
Aramaki. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org).

DOI: 10.2196/76773
PMCID: PMC12262928
PMID: 40627819 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


452. Acad Radiol. 2026 Feb;33(2):337-347. doi: 10.1016/j.acra.2025.10.043. Epub 2025 
Nov 11.

Performance of Large Language Models on Radiology Residency In-Training 
Examination Questions.

Salbas A(1), Yogurtcu M(2).

Author information:
(1)Izmir Katip Celebi University, Ataturk Training and Research Hospital, 
Department of Radiology, Izmir, Turkey (A.S.). Electronic address: 
dralisalbas@gmail.com.
(2)Tire State Hospital, Radiology Department, Izmir, Turkey (M.Y.). Electronic 
address: muratyogurtcu@gmail.com.

RATIONALE AND OBJECTIVES: Large language models (LLMs) are increasingly 
investigated in radiology education. This study evaluated the performance of 
several advanced LLMs on radiology residency in-training examination questions, 
with a focus on whether recently released versions show improved accuracy 
compared with earlier models.
MATERIALS AND METHODS: We analyzed 282 multiple-choice questions (191 text-only, 
91 image-based) from institutional radiology residency examinations conducted 
between 2023 and 2025. Five LLMs were tested: ChatGPT-4o, ChatGPT-5, Claude 4 
Opus, Claude 4.1 Opus, and Gemini 2.5 Pro. Radiology resident performance on the 
same set of questions was also analyzed for comparison. Accuracy rates were 
calculated for overall, text-only, and image-based questions, and results were 
compared using Cochran's Q and Bonferroni-adjusted McNemar tests. Outputs were 
also assessed for hallucinations.
RESULTS: Gemini 2.5 Pro achieved the highest overall accuracy (83.0%), followed 
by ChatGPT-5 (82.3%). By comparison, radiology residents achieved an overall 
accuracy of 78.2%. ChatGPT-5 showed significantly higher accuracy compared with 
ChatGPT-4o (p = 0.021), and Gemini 2.5 Pro showed significantly higher accuracy 
compared with Claude 4 Opus (p = 0.026). For text-only questions, the highest 
accuracy was obtained with Gemini 2.5 Pro (88.0%). For image-based questions, 
radiology residents achieved the highest accuracy (80.4%), followed by ChatGPT-5 
(73.6%). The highest accuracies by subspecialty were observed in interventional 
radiology and physics, whereas breast imaging yielded the lowest accuracy across 
the models. No instances of hallucination were observed.
CONCLUSION: LLMs demonstrated generally good performance on radiology residency 
assessments, with newer versions showing measurable improvements. However, 
limitations persist in image-based interpretation and certain subspecialties. 
LLMs should therefore be regarded as supportive resources in radiology 
education, with careful validation and continued refinement of medical training 
data.

Copyright © 2025 The Association of University Radiologists. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.acra.2025.10.043
PMID: 41224539 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


453. Graefes Arch Clin Exp Ophthalmol. 2026 Jan 3. doi: 10.1007/s00417-025-07092-1. 
Online ahead of print.

Comparing the performance of four mainstream large language models on medical 
literature review generation: a human expert evaluation in SMILE surgery.

Zhou M(#)(1)(2), Gui F(#)(1), Ai C(#)(1), Ling L(3), Zhang X(3), Lu Y(1), Han 
D(4), Zhao B(4), Zhong F(5), Liu J(6), Zhu Z(7), Li J(8), Huang F(1), Lin C(1), 
Liu W(9), Xiong J(10).

Author information:
(1)Ophthalmic Center, The Second Affiliated Hospital, Jiangxi Medical College, 
Nanchang University, Nanchang, 330000, China.
(2)Zhongshan Ophthalmic Center, Sun Yat-sen University, Guangzhou, 510060, 
China.
(3)Nanchang Bright Eye Hospital, Nanchang, 330000, China.
(4)Shanghai Heping Eye Hospital, Shanghai, 200030, China.
(5)Department of Ophthalmology, First Affiliated Hospital of Gannan Medical 
University, Ganzhou, 341000, China.
(6)Xiaogan Central Hospital, Xiaogan, 432000, China.
(7)Heyou International Hospital, Shenzhen, 518000, China.
(8)The First Affiliated Hospital of Xi'an Jiao Tong University, Xi'an, 710089, 
China.
(9)Ophthalmic Center, The Second Affiliated Hospital, Jiangxi Medical College, 
Nanchang University, Nanchang, 330000, China. 18970040725@163.com.
(10)Ophthalmic Center, The Second Affiliated Hospital, Jiangxi Medical College, 
Nanchang University, Nanchang, 330000, China. 894040417@qq.com.
(#)Contributed equally

PURPOSE: To systematically evaluate and compare the performance of four leading 
large language models (LLMs) in generating medical literature reviews across 
topics of varying research maturity, thereby providing insights for their 
effective and responsible application in academic writing.
METHODS: In this comparative study, using standardized prompts, we instructed 
four leading LLMs (GPT-4, Gemini 2.5 Pro, Grok-3, and DeepSeek R1) to generate 
literature reviews on nine topics related to small incision lenticule extraction 
(SMILE) surgery. These topics were categorized into three groups by research 
maturity: well-researched, controversial, and open. Seven ophthalmology experts 
evaluated the generated content across four dimensions: quality, accuracy, bias, 
and relevance, while all references were verified for authenticity. Performance 
differences among models were evaluated using group comparison tests followed by 
post-hoc analysis.
RESULTS: Significant performance variations were identified across all four 
models and dimensions (p < 0.001). Specifically, Gemini ranked highest in 
content quality, accuracy, and bias control. In contrast, DeepSeek, despite its 
high-quality score, received the lowest relevance score. Grok-3 demonstrated the 
highest reference authenticity (p < 0.001), whereas GPT-4's was the lowest 
(p < 0.001). All models showed diminished performance on open topics and 
exhibited severe reference fabrication ("hallucinations").
CONCLUSION: Rather than excelling universally, LLMs exhibit distinct and 
task-specific strengths that mandate a task-driven, hybrid strategy in tool 
selection. Reference fabrication was found to be a pervasive issue across all 
models, regardless of the task topic, elevating human verification from a best 
practice to an essential safeguard for academic integrity.

© 2026. The Author(s).

DOI: 10.1007/s00417-025-07092-1
PMID: 41484266

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. This study did not involve human participants. 
Consent for publication: Not applicable. The data used in this study are 
publicly available and do not involve identifiable personal information. 
Competing interests: The authors declare no competing interests.


454. Pediatr Pulmonol. 2025 Mar;60(3):e71020. doi: 10.1002/ppul.71020.

Bridging the Gap in Neonatal Care: Evaluating AI Chatbots for Chronic Neonatal 
Lung Disease and Home Oxygen Therapy Management.

Liu W(1)(2)(3), Wei H(1)(2), Xiang L(1)(2), Liu Y(1)(2)(4), Wang C(1)(2), Hua 
Z(1)(2).

Author information:
(1)Department of Neonatology, National Clinical Research Center for Child Health 
and Disorders, Ministry of Education Key Laboratory of Child Development and 
Disorders, Children's Hospital of Chongqing Medical University, Chongqing, 
China.
(2)Chongqing Key Laboratory of Child Neurodevelopment and Cognitive Disorders, 
Chongqing, China.
(3)Changdu People's Hospital of Xizang, Xizang, China.
(4)Special Care Nursery, Port Moresby General Hospital, Port Moresby, Papua New 
Guinea.

OBJECTIVE: To evaluate the accuracy and comprehensiveness of eight free, 
publicly available large language model (LLM) chatbots in addressing common 
questions related to chronic neonatal lung disease (CNLD) and home oxygen 
therapy (HOT).
STUDY DESIGN: Twenty CNLD and HOT-related questions were curated across nine 
domains. Responses from ChatGPT-3.5, Google Bard, Bing Chat, Claude 3.5 Sonnet, 
ERNIE Bot 3.5, and GLM-4 were generated and evaluated by three experienced 
neonatologists using Likert scales for accuracy and comprehensiveness. Updated 
LLM models (ChatGPT-4o mini and Gemini 2.0 Flash Experimental) were incorporated 
to assess rapid technological advancement. Statistical analyses included ANOVA, 
Kruskal-Wallis tests, and intraclass correlation coefficients.
RESULTS: Bing Chat and Claude 3.5 Sonnet demonstrated superior performance, with 
the highest mean accuracy scores (5.78 ± 0.48 and 5.75 ± 0.54, respectively) and 
competence scores (2.65 ± 0.58 and 2.80 ± 0.41, respectively). In subsequent 
testing, Gemini 2.0 Flash Experimental and ChatGPT-4o mini achieved comparable 
high performance. Performance varied across domains, with all models excelling 
in "equipment and safety protocols" and "caregiver support." ERNIE Bot 3.5 and 
GLM-4 showed self-correction capabilities when prompted.
CONCLUSIONS: LLMs promise accurate CNLD/HOT information. However, performance 
variability and the risk of misinformation necessitate expert oversight and 
continued refinement before widespread clinical implementation.

© 2025 Wiley Periodicals LLC.

DOI: 10.1002/ppul.71020
PMID: 40042139 [Indexed for MEDLINE]


455. JMIR Med Educ. 2025 Nov 20;11:e80084. doi: 10.2196/80084.

AI-Generated "Slop" in Online Biomedical Science Educational Videos: Mixed 
Methods Study of Prevalence, Characteristics, and Hazards to Learners and 
Teachers.

Jones EM(1), Newman JD(1), Kim B(2), Fogle EJ(3).

Author information:
(1)Department of Foundational Medical Studies, Oakland University William 
Beaumont School of Medicine, 586 Pioneer Drive, Rochester, MI, 48309, United 
States, 1 248-370-3731.
(2)Department of Human Development and Child Studies, Oakland University, 
Rochester, MI, United States.
(3)Department of Chemistry and Biochemistry, California Polytechnic State 
University, San Luis Obispo, CA, United States.

BACKGROUND: Video-sharing sites such as YouTube (Google) and TikTok (ByteDance) 
have become indispensable resources for learners and educators. The recent 
growth in generative artificial intelligence (AI) tools, however, has resulted 
in low-quality, AI-generated material (commonly called "slop") cluttering these 
platforms and competing with authoritative educational materials. The extent to 
which slop has polluted science education video content is unknown, as are the 
specific hazards to learning from purportedly educational videos made by AI 
without the use of human discretion.
OBJECTIVE: This study aimed to advance a formal definition of slop (based on the 
recent theoretical construct of "careless speech"), to identify its qualitative 
characteristics that may be problematic for learners, and to gauge its 
prevalence among preclinical biomedical science (medical biochemistry and cell 
biology) videos on YouTube and TikTok. We also examined whether any quantitative 
features of video metadata correlate with the presence of slop.
METHODS: An automated search of publicly available YouTube and TikTok videos 
related to 10 search terms was conducted in February and March 2025. After 
exclusion of duplicates, off-topic, and non-English results, videos were 
screened, and those suggestive of AI were flagged. The flagged videos were 
subject to a 2-stage qualitative content analysis to identify and code 
problematic features before an assignment of "slop" was made. Quantitative 
viewership data on all videos in the study were scraped using automated tools 
and compared between slop videos and the overall population.
RESULTS: We define "slop" according to the degree of human care in production. 
Of 1082 videos screened (814 YouTube, 268 TikTok), 57 (5.3%) were deemed 
probably AI-generated and low-quality. From qualitative analysis of these and 6 
additional AI-generated videos, we identified 16 codes for problematic aspects 
of the videos as related to their format or contents. These codes were then 
mapped to the 7 characteristics of careless speech identified earlier. Analysis 
of view, like, and comment rates revealed no significant difference between slop 
videos and the overall population.
CONCLUSIONS: We find slop to be not especially prevalent on YouTube and TikTok 
at this time. These videos have comparable viewership statistics to the overall 
population, although the small dataset suggests this finding should be 
interpreted with caution. From the slop videos that were identified, several 
features inconsistent with best practices in multimedia instruction were 
defined. Our findings should inform learners seeking to avoid low-quality 
material on video-sharing sites and suggest pitfalls for instructors to avoid 
when making high-quality educational materials with generative AI.

© Eric M Jones, Jane D Newman, Boyun Kim, Emily J Fogle. Originally published in 
JMIR Medical Education (https://mededu.jmir.org).

DOI: 10.2196/80084
PMCID: PMC12634010
PMID: 41264860 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


456. BMC Neurol. 2025 Feb 19;25(1):69. doi: 10.1186/s12883-025-04071-1.

GPT meets PubMed: a novel approach to literature review using a large language 
model to crowdsource migraine medication reviews.

Mackenzie E(1), Cheng R(1), Zhang P(2)(3).

Author information:
(1)Department of Neurology, Rutgers Robert Wood Johnson Medical School, New 
Jersey, USA.
(2)Department of Neurology, Beth Israel Deaconess Medical Center, Boston, MA, 
USA. phil.p.zhang@gmail.com.
(3)Harvard Medical School, Boston, MA, USA. phil.p.zhang@gmail.com.

OBJECTIVE: To evaluate the potential of two large language models (LLMs), GPT-4 
(OpenAI) and PaLM2 (Google), in automating migraine literature analysis by 
conducting sentiment analysis of migraine medications in clinical trial 
abstracts.
BACKGROUND: Migraine affects over one billion individuals worldwide, 
significantly impacting their quality of life. A vast amount of scientific 
literature on novel migraine therapeutics continues to emerge, but an efficient 
method by which to perform ongoing analysis and integration of this information 
poses a challenge.
METHODS: "Sentiment analysis" is a data science technique used to ascertain 
whether a text has positive, negative, or neutral emotional tone. Migraine 
medication names were extracted from lists of licensed biological products from 
the FDA, and relevant abstracts were identified using the MeSH term "migraine 
disorders" on PubMed and filtered for clinical trials. Standardized prompts were 
provided to the APIs of both GPT-4 and PaLM2 to request an article sentiment as 
to the efficacy of each medication found in the abstract text. The resulting 
sentiment outputs were classified using both a binary and a distribution-based 
model to determine the efficacy of a given medication.
RESULTS: In both the binary and distribution-based models, the most favorable 
migraine medications identified by GPT-4 and PaLM2 aligned with evidence-based 
guidelines for migraine treatment.
CONCLUSIONS: LLMs have potential as complementary tools in migraine literature 
analysis. Despite some inconsistencies in output and methodological limitations, 
the results highlight the utility of LLMs in enhancing the efficiency of 
literature review through sentiment analysis.

© 2025. The Author(s).

DOI: 10.1186/s12883-025-04071-1
PMCID: PMC11837380
PMID: 39972406 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: N/A. Consent for publication: N/A. Competing interests: PZ: He has 
received honorarium from Alder Biopharmaceuticals, Board Vitals, and Fieve 
Clinical Research. He collaborates with Headache Science Incorporated without 
receiving financial support. He had ownership interest in Cymbeline LLC. He is a 
consultant for Acument LLC.


457. Int J Mol Sci. 2025 Jul 5;26(13):6487. doi: 10.3390/ijms26136487.

From Mutation to Prognosis: AI-HOPE-PI3K Enables Artificial Intelligence 
Agent-Driven Integration of PI3K Pathway Data in Colorectal Cancer Precision 
Medicine.

Yang EW(1), Waldrup B(2), Velazquez-Villarreal E(2)(3).

Author information:
(1)PolyAgent, San Francisco, CA 94102, USA.
(2)Department of Integrative Translational Sciences, Beckman Research Institute 
of City of Hope, Duarte, CA 91010, USA.
(3)City of Hope Comprehensive Cancer Center, Duarte, CA 91010, USA.

The rising incidence of early-onset colorectal cancer (EOCRC), particularly 
among underrepresented populations, highlights the urgent need for tools that 
can uncover clinically meaningful, population-specific genomic alterations. The 
phosphoinositide 3-kinase (PI3K) pathway plays a key role in tumor progression, 
survival, and therapeutic resistance in colorectal cancer (CRC), yet its impact 
in EOCRC remains insufficiently explored. To address this gap, we developed 
AI-HOPE-PI3K, a conversational artificial intelligence platform that integrates 
harmonized clinical and genomic data for real-time, natural language-based 
analysis of PI3K pathway alterations. Built on a fine-tuned biomedical LLaMA 3 
model, the system automates cohort generation, survival modeling, and mutation 
frequency comparisons using multi-institutional cBioPortal datasets annotated 
with clinical variables. AI-HOPE-PI3K replicated known associations and revealed 
new findings, including worse survival in colon versus rectal tumors harboring 
PI3K alterations, enrichment of INPP4B mutations in Hispanic/Latino EOCRC 
patients, and favorable survival outcomes associated with high tumor mutational 
burden in FOLFIRI-treated patients. The platform also enabled context-specific 
survival analyses stratified by age, tumor stage, and molecular alterations. 
These findings support the utility of AI-HOPE-PI3K as a scalable and accessible 
tool for integrative, pathway-specific analysis, demonstrating its potential to 
advance precision oncology and reduce disparities in EOCRC through data-driven 
discovery.

DOI: 10.3390/ijms26136487
PMCID: PMC12249626
PMID: 40650262 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


458. J Med Syst. 2025 Oct 10;49(1):135. doi: 10.1007/s10916-025-02283-z.

Novel Insights into the Application of Large Language Models in the Diagnosis 
and Treatment of Complex Cardiovascular Diseases: A Comparative Study.

Tian M(#)(1), Li S(#)(2)(3), Du W(#)(1), Yang S(1), Zhao X(1), Xiong H(1), Li 
H(1), Lu M(1), Ying Y(1), Zhang J(1), Liao Q(1), Yang D(1), Guo F(4)(5).

Author information:
(1)Key Laboratory of Cardiovascular Disease of Yunnan Province, Clinical 
Medicine Center for Cardiovascular Disease of Yunnan Province, Department of 
Cardiology, Yan'an Hospital Affiliated to Kunming Medical University, Kunming, 
650000, PR China.
(2)Key Laboratory of Cardiovascular Disease of Yunnan Province, Clinical 
Medicine Center for Cardiovascular Disease of Yunnan Province, Department of 
Cardiology, Yan'an Hospital Affiliated to Kunming Medical University, Kunming, 
650000, PR China. 13700651472@126.com.
(3)Department of Cardiology, Yan'an Hospital Affiliated to Kunming Medical 
University, No. 245, E Renmin Rd, Kunming, 650051, Yunnan, P.R. China. 
13700651472@126.com.
(4)Key Laboratory of Cardiovascular Disease of Yunnan Province, Clinical 
Medicine Center for Cardiovascular Disease of Yunnan Province, Department of 
Cardiology, Yan'an Hospital Affiliated to Kunming Medical University, Kunming, 
650000, PR China. whuguofuding@163.com.
(5)Department of Cardiology, Yan'an Hospital Affiliated to Kunming Medical 
University, No. 245, E Renmin Rd, Kunming, 650051, Yunnan, P.R. China. 
whuguofuding@163.com.
(#)Contributed equally

The rapid evolution of large language models (LLMs) in the medical field, 
particularly in automating medical tasks and supporting diagnosis and treatment, 
has shown promising potential. However, their accuracy, comprehensiveness, and 
safety in managing complex cardiovascular diseases have not been systematically 
assessed. This study aims to evaluate and compare the diagnostic, therapeutic, 
and safety performance of two large language models-ChatGPT-4o and Kimi-in 
managing complex cardiovascular diseases, and to explore their potential for 
future clinical application. A total of 200 complex cardiovascular cases 
published in JACC: Case Reports between January 2020 and August 2024 were 
included. All cases were standardized and de-identified before being input into 
ChatGPT-4o and Kimi using identical prompts. Each model independently generated 
diagnostic, treatment, and long-term management plans. Three cardiovascular 
specialists independently evaluated the outputs in a blinded manner, scoring 
accuracy and comprehensiveness using Likert scales. Safety was assessed using a 
risk matrix analysis. Additionally, 50 cases were randomly selected for 
triangulation to compare model-generated recommendations with clinical 
guidelines. Statistical analysis was performed using the Wilcoxon signed-rank 
test with Benjamini-Hochberg correction for multiple comparisons. In preliminary 
diagnostic accuracy, the two models performed similarly (P = 0.663, r = 0.044), 
but ChatGPT-4o showed superior comprehensiveness (P < 0.001, r = 0.484). For 
treatment recommendations, ChatGPT-4o outperformed Kimi in both accuracy 
(P = 0.004, r = 0.321) and comprehensiveness (P < 0.001, r = 0.644). In 
long-term management, ChatGPT-4o demonstrated significant advantages in accuracy 
(P < 0.001, r = 0.717) and comprehensiveness (P < 0.001, r = 0.690). Safety 
assessment showed a lower proportion of high-risk outputs with ChatGPT-4o (1.5%) 
compared to Kimi (4.5%). LLMs, particularly ChatGPT-4o, exhibit significant 
promise in the diagnosis and treatment of complex cardiovascular diseases, 
showing superior accuracy, comprehensiveness, and safety compared to Kimi. 
Despite their high accuracy and safety, LLMs still require clinician oversight, 
especially in the formulation of personalized treatment plans and complex 
decision-making scenarios, to ensure their reliable integration into clinical 
practice.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02283-z
PMID: 41071423 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics and Consent To Participate 
Declarations: Not Applicable. Competing interests: The authors declare no 
competing interests. Clinical trial number: Not Applicable.


459. Radiol Artif Intell. 2025 May;7(3):e230620. doi: 10.1148/ryai.230620.

Machine Learning and Deep Learning Models for Automated Protocoling of Emergency 
Brain MRI Using Text from Clinical Referrals.

Huhtanen HJ(1), Nyman MJ(1), Karlsson A(2), Hirvonen J(1).

Author information:
(1)Department of Radiology, Turku University Hospital & University of Turku, 
Kiinamyllynkatu 4-8, 20521 Turku, Finland.
(2)Department of Radiology, University of Turku, Turku, Finland, and 
Pihlajalinna Turku, Turku, Finland.

Comment in
    Radiol Artif Intell. 2025 May;7(3):e250218. doi: 10.1148/ryai.250218.

Purpose To develop and evaluate machine learning and deep learning-based models 
for automated protocoling of emergency brain MRI scans based on clinical 
referral text. Materials and Methods In this single-institution, retrospective 
study of 1953 emergency brain MRI referrals from January 2016 to January 2019, 
two neuroradiologists labeled the imaging protocol and use of contrast agent as 
the reference standard. Three machine learning algorithms (naive Bayes, support 
vector machine, and XGBoost) and two pretrained deep learning models (Finnish 
bidirectional encoder representations from transformers [BERT] and generative 
pretrained transformer [GPT]-3.5 [GPT-3.5 Turbo; Open AI]) were developed to 
predict the MRI protocol and need for a contrast agent. Each model was trained 
with three datasets (100% of training data, 50% of training data, and 50% plus 
augmented training data). Prediction accuracy was assessed with a test set. 
Results The GPT-3.5 models trained with 100% of the training data performed best 
in both tasks, achieving an accuracy of 84% (95% CI: 80, 88) for the correct 
protocol and 91% (95% CI: 88, 94) for the contrast agent. BERT had an accuracy 
of 78% (95% CI: 74, 82) for the protocol and 89% (95% CI: 86, 92) for the 
contrast agent. The best machine learning model in the protocol task was XGBoost 
(accuracy, 78%; 95% CI: 73, 82), and the best machine learning models in the 
contrast agent task were support vector machine and XGBoost (accuracy, 88%; 95% 
CI: 84, 91 for both). The accuracies of two nonneuroradiologists were 80%-83% in 
the protocol task and 89%-91% in the contrast medium task. Conclusion Machine 
learning and deep learning models demonstrated high performance in automatic 
protocoling of emergency brain MRI scans based on text from clinical referrals. 
Keywords: Natural Language Processing, Automatic Protocoling, Deep Learning, 
Machine Learning, Emergency Brain MRI Supplemental material is available for 
this article. Published under a CC BY 4.0 license. See also commentary by 
Strotzer in this issue.

DOI: 10.1148/ryai.230620
PMID: 39969276 [Indexed for MEDLINE]


460. J Craniofac Surg. 2026 Jan 20. doi: 10.1097/SCS.0000000000012363. Online ahead 
of print.

Clinical Accuracy of AI Language Models in Providing Impacted Teeth Information: 
A Comparative Evaluation.

Özdal Zincir Ö(1), Çifçi Özkan E(2), Hatipoğlu Ş(3).

Author information:
(1)Department of Oral and Maxillofacial Surgery, Faculty of Dentistry, Istanbul 
University-Cerrahpasa.
(2)Department of Orthodontics, Faculty of Dentistry, Biruni University.
(3)Department of Orthodontics, Faculty of Dentistry, Istanbul 
University-Cerrahpasa, Istanbul, Turkey.

Artificial intelligence (AI) language models are increasingly integrated into 
clinical and patient-centered information pathways, yet their accuracy in 
delivering condition-specific dental knowledge remains unclear. This comparative 
study evaluated the clinical accuracy of 3 widely used AI models-ChatGPT-4, 
Gemini, and Copilot-in providing information on impacted teeth. A total of 118 
expert-generated open-ended questions were posed to each model, and responses 
were categorized into 5 predefined accuracy levels. Statistical analysis using 
the Pearson χ2 or Fisher exact test (P≤0.05) demonstrated that ChatGPT-4 
produced the highest proportion of "Objectively True" responses (83.9%) and 
consistently outperformed Gemini and Copilot across all domains, including 
definitions, indications, procedural descriptions, contraindications, and 
complications. Gemini and Copilot more frequently generated incomplete or 
selectively accurate answers classified as "Selected Facts" or "Minimal Facts," 
highlighting variability in their informational reliability. Overall, ChatGPT-4 
exhibited superior clinical accuracy and appears to function as a more 
dependable supplementary resource for impacted tooth-related information, 
whereas the inconsistent performance of Gemini and Copilot underscores the 
continued need for expert oversight in patient education and clinical 
communication.

Copyright © 2026 by Mutaz B. Habal, MD.

DOI: 10.1097/SCS.0000000000012363
PMID: 41556944

Conflict of interest statement: The authors report no conflicts of interest.


461. BMC Gastroenterol. 2026 Jan 21. doi: 10.1186/s12876-026-04628-5. Online ahead of 
print.

Multidisciplinary artificial intelligence systems versus single-model approaches 
for the diagnosis and management of ileus and volvulus.

Ucdal M(1), Yurtsever K(2), Ekingen E(3).

Author information:
(1)Department of Internal Medicine, Etimesgut Sehit Sait Ertürk State Hospital, 
Eryaman, Ankara, Turkey. meteucdal@hacettepe.edu.tr.
(2)Department of Internal Medicine, Hacettepe University, Ankara, Turkey.
(3)Department of Emergency Medicine, Etimesgut Sehit Sait Ertürk State Hospital, 
Ankara, Turkey.

BACKGROUND AND AIMS: The accurate and timely diagnosis of ileus versus volvulus 
is essential in emergency care, as treatment choices directly influence patient 
outcomes. In this study, the diagnostic accuracy and adherence to guidelines of 
multidisciplinary AI systems were compared with those of single-model approaches 
and actual clinical decisions in managing these acute gastrointestinal 
conditions.
METHODS: We conducted a retrospective analysis of 234 adult patients diagnosed 
with ileus (n = 120, 51. 3%) or volvulus (n = 114, 48. 7%) from January 2018 to 
December 2024. We assessed three approaches: (1) the use of a multidisciplinary 
AI system including GPT-4 V (radiology), Med-PaLM (emergency medicine), BioGPT 
(ICU), and LLaMA 4 (surgical decisions); (2) the use of ChatGPT 5.5.0 as a 
single multimodal AI system; and (3) actual clinical team decisions as the 
reference standard. The key outcomes were diagnostic accuracy, decision 
alignment, and guideline adherence across management areas.
RESULTS: The multidisciplinary AI system achieved 94.4% accuracy, outperforming 
ChatGPT 5.0 at 87.2% (p < 0.001). The multidisciplinary method also resulted in 
greater guideline adherence for sigmoid volvulus management (96.7% vs. 88.3%, 
p < 0.01), cecal volvulus surgical intervention (100% vs. 93.3%, p < 0.05), 
conservative ileus treatment (87.5% vs. 83.3%, p < 0.05), and ICU triage (91.1% 
vs. 85.2%, p < 0.01). Surgical decision alignment was better with the 
multidisciplinary system (92.8% vs. 85.5%, p < 0.001), and ICU admission 
predictions were more accurate (90.6% vs. 83.7%, p < 0.001). The AI system also 
made decisions faster: 2.5 ± 5.05 min vs. 3.2 ± 0.016 min for ChatGPT and 
15.3 ± 3.2 min for clinical teams (p < 0.01 and p < 0.001, respectively).
CONCLUSION: Multidisciplinary AI systems that combine specialized models for 
different clinical areas significantly outperform a single AI model in terms of 
diagnostic accuracy, guideline consistency, and decision alignment for ileus and 
volvulus. These results indicate that task-specific AI integration could improve 
clinical support, but further validation is needed before its routine use.

© 2026. The Author(s).

DOI: 10.1186/s12876-026-04628-5
PMID: 41566242

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was approved by the Institutional Review Board of 
Etimesgut Şehit Sait Ertürk State Hospital (Protocol #2024 − 389) with a waiver 
of the requirement for informed consent because of its retrospective nature and 
use of deidentified data. This study was conducted in accordance with the 
Declaration of Helsinki. Consent for publication: Not applicable. This study 
used deidentified retrospective data without any identifying images or personal 
clinical details that would compromise patient anonymity. Competing interests: 
The authors declare no competing interests.


462. Am J Clin Nutr. 2025 Dec;122(6):1836-1849. doi: 10.1016/j.ajcnut.2025.10.013. 
Epub 2025 Oct 24.

Customized multimodal Diabot-GPT-4o enhances accuracy of image-based dietary 
assessments in dietetic trainees in Taiwan: validation against weighed food 
records.

Chen YJ(1), Chang CC(2), Hoang YN(1), Lin AW(3), Lin WL(1), Lin CY(1), Patricia 
E(4), Tissadharma JC(4), Kuanca J(4), Nobelta N(4), Theo KA(4), Ho DKN(5), Wei 
PH(6), Chang JS(7).

Author information:
(1)School of Nutrition and Health Sciences, College of Nutrition, Taipei Medical 
University, Taipei, Taiwan.
(2)Division of Gastroenterology and Hepatology, Department of Internal Medicine, 
Taipei Medical University Hospital, Taipei, Taiwan; Division of Gastroenterology 
and Hepatology, Department of Internal Medicine, School of Medicine, College of 
Medicine, Taipei Medical University, Taipei, Taiwan; TMU Research Center for 
Digestive Medicine, Taipei Medical University, Taipei, Taiwan.
(3)The Hormel Institute, University of Minnesota, Austin, MN, United States.
(4)Department of Food Science and Nutrition, Indonesia International Institute 
for Life Sciences (i3L), Jakarta, Indonesia.
(5)Faculty of Nursing and Midwifery Hong Bang International University, Ho Chi 
Minh City, Vietnam.
(6)Department of Nutrition, Wan Fang Hospital, Taipei Medical University, 
Taipei, Taiwan.
(7)School of Nutrition and Health Sciences, College of Nutrition, Taipei Medical 
University, Taipei, Taiwan; TMU Research Center for Digestive Medicine, Taipei 
Medical University, Taipei, Taiwan; Graduate Institute of Metabolism and Obesity 
Sciences, College of Nutrition, Taipei Medical University, Taipei, Taiwan; 
Nutrition Research Center, Taipei Medical University Hospital, Taipei, Taiwan; 
Chinese Taipei Society for the Study of Obesity (CTSSO), Taipei, Taiwan. 
Electronic address: susanchang@tmu.edu.tw.

BACKGROUND: Automated image-based dietary assessments (IBDAs) using multimodal 
artificial intelligence (AI) chatbots show strong potential. However, sources of 
error at the human-AI interface in real-world use remain unclear.
OBJECTIVES: In this study, we validated a GPT-4o-powered chatbot for automated 
IBDAs and identified key sources of error in free-living settings.
METHODS: In total, 714 food images were collected from 3-d weighed food records 
(WFRs) across 171 d from 57 young adults. Images were analyzed using 4 AI 
configurations: Diabot (DB), DBFN (customized GPT-4o), 4o, and 4oFN 
(noncustomized), where "FN" indicates inclusion of the food name input. Portion 
sizes and nutrient estimates were compared with WFRs using Bland-Altman plots 
with equivalence testing at ±10%, ±15%, and ±20% bounds.
RESULTS: Using images alone, DB recognized 74% of food items versus 59% for 4o. 
All AI configurations provided accurate estimates of portion sizes (±10%-15%, 
coefficient of variation [CV]: 13%), energy (±10%-20%, CV: 14%), and 
carbohydrates (CHOs; ±15%-20%, CV: 15%) but showed less consistency for fats 
(±10%-22%, CV: 24%) and proteins (±10%->20.2%, CV: 18%). The custom DBFN 
outperformed 4oFN, achieving higher accuracy across more nutrients within the 
±10% (weight, energy, fats, saturated fats, potassium, and magnesium), ±15% 
(proteins and sodium), and ±20% (CHOs and calcium) bounds and achieved the 
highest agreement with WFRs (Spearman's ρ = 0.863-0.662; Lin's concordance 
correlation coefficient = 0.874-0.540). Common errors at the human-AI interface 
included inaccurate portion-size estimates, obscured food visibility in images, 
poorly constructed prompts, omission or intrusion errors, and system-specific 
limitations, such as processing overload and configuration inconsistencies.
CONCLUSIONS: Customized AI chatbots improved automated IBDAs, yet accuracy 
depends on clear images for food visibility and portion-size fidelity. 
Standardized AI-input procedures (FN, cooking state, prompt structure, and 
configuration) and expert oversight to detect and correct AI hallucinations 
(fabricated items, units, or quantities) remain essential for reliable, 
interpretable estimates.

Copyright © 2025 American Society for Nutrition. All rights reserved.

DOI: 10.1016/j.ajcnut.2025.10.013
PMCID: PMC12799367
PMID: 41138916 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of interest The authors report no 
conflicts of interest.


463. PLoS One. 2025 Apr 1;20(4):e0321093. doi: 10.1371/journal.pone.0321093. 
eCollection 2025.

Barriers and enablers for the deployment of large language model-based 
conversational robots for older adults: A protocol for a systematic review of 
qualitative studies.

Shankar R(1), Bundele A(1)(2), Mukhopadhyay A(3).

Author information:
(1)Research and Innovation, Medical Affairs, Alexandra Hospital, National 
University Health System, Singapore.
(2)Yong Loo Lin School of Medicine, National University of Singapore, Singapore.
(3)Division of Respiratory & Critical Care Medicine, Department of Medicine, 
National University Health System, Singapore.

BACKGROUND: Artificial intelligence-powered conversational agents have immense 
potential to provide social companionship and support for older adults. However, 
the deployment of large language model (LLM)-based conversational robots for 
seniors faces various technical, user acceptance, and ethical challenges.
OBJECTIVES: This systematic review aims to synthesize insights from prior 
qualitative studies to identify key factors that influence the real-world 
application of LLM-based conversational agents for the aging population. The 
review will inform the user-centered design of these technologies, policy 
discussions on their governance, and highlight research gaps.
METHODS AND ANALYSIS: Eleven electronic databases will be searched for 
qualitative studies exploring stakeholder perspectives on using AI chatbots and 
robots to assist seniors. Two reviewers will independently screen studies, 
extract data, and appraise methodological quality using the JBI checklist. 
Thematic analysis will be conducted to identify major barriers and enablers, and 
confidence in review findings will be assessed using the GRADE-CERQual approach. 
The review will adhere to PRISMA-P and ENTREQ reporting guidelines to ensure 
transparency.
DISCUSSION: Understanding and addressing obstacles to implementing LLM-powered 
conversational agents for older adults is crucial for leveraging this technology 
to support the well-being of the rapidly aging global population. This 
systematic review will provide timely insights to guide the responsible 
development and deployment of AI companions for seniors.
TRIAL REGISTRATION: ClinicalTrials.gov CRD42024601264.

Copyright: © 2025 Shankar et al. This is an open access article distributed 
under the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0321093
PMCID: PMC11960934
PMID: 40168400 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


464. JMIR Med Inform. 2025 Aug 28;13:e71137. doi: 10.2196/71137.

Development of a Large-Scale Dataset of Chest Computed Tomography Reports in 
Japanese and a High-Performance Finding Classification Model: Dataset 
Development and Validation Study.

Yamagishi Y(1), Nakamura Y(2), Kikuchi T(2)(3), Sonoda Y(1), Hirakawa H(1), Kano 
S(4), Nakamura S(4), Hanaoka S(1), Yoshikawa T(2), Abe O(1).

Author information:
(1)Division of Radiology and Biomedical Engineering, Graduate School of 
Medicine, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8655, 
Japan, 81 3-3815-5411.
(2)Department of Computational Diagnostic Radiology and Preventive Medicine, The 
University of Tokyo Hospital, Tokyo, Japan.
(3)Department of Radiology, School of Medicine, Jichi Medical University, 
Shimotsuke, Japan.
(4)Department of Diagnostic Radiology, Toranomon Hospital, Tokyo, Japan.

BACKGROUND: Recent advances in large language models have highlighted the need 
for high-quality multilingual medical datasets. Although Japan is a global 
leader in computed tomography (CT) scanner deployment and use, the absence of 
large-scale Japanese radiology datasets has hindered the development of 
specialized language models for medical imaging analysis. Despite the emergence 
of multilingual models and language-specific adaptations, the development of 
Japanese-specific medical language models has been constrained by a lack of 
comprehensive datasets, particularly in radiology.
OBJECTIVE: This study aims to address this critical gap in Japanese medical 
natural language processing resources, for which a comprehensive Japanese CT 
report dataset was developed through machine translation, to establish a 
specialized language model for structured classification. In addition, a 
rigorously validated evaluation dataset was created through expert radiologist 
refinement to ensure a reliable assessment of model performance.
METHODS: We translated the CT-RATE dataset (24,283 CT reports from 21,304 
patients) into Japanese using GPT-4o mini. The training dataset consisted of 
22,778 machine-translated reports, and the validation dataset included 150 
reports carefully revised by radiologists. We developed CT-BERT-JPN, a 
specialized Bidirectional Encoder Representations from Transformers (BERT) model 
for Japanese radiology text, based on the "tohoku-nlp/bert-base-japanese-v3" 
architecture, to extract 18 structured findings from reports. Translation 
quality was assessed with Bilingual Evaluation Understudy (BLEU) and 
Recall-Oriented Understudy for Gisting Evaluation (ROUGE) scores and further 
evaluated by radiologists in a dedicated human-in-the-loop experiment. In that 
experiment, each of a randomly selected subset of reports was independently 
reviewed by 2 radiologists-1 senior (postgraduate year [PGY] 6-11) and 1 junior 
(PGY 4-5)-using a 5-point Likert scale to rate: (1) grammatical correctness, (2) 
medical terminology accuracy, and (3) overall readability. Inter-rater 
reliability was measured via quadratic weighted kappa (QWK). Model performance 
was benchmarked against GPT-4o using accuracy, precision, recall, F1-score, ROC 
(receiver operating characteristic)-AUC (area under the curve), and average 
precision.
RESULTS: General text structure was preserved (BLEU: 0.731 findings, 0.690 
impression; ROUGE: 0.770-0.876 findings, 0.748-0.857 impression), though expert 
review identified 3 categories of necessary refinements-contextual adjustment of 
technical terms, completion of incomplete translations, and localization of 
Japanese medical terminology. The radiologist-revised translations scored 
significantly higher than raw machine translations across all dimensions, and 
all improvements were statistically significant (P<.001). CT-BERT-JPN 
outperformed GPT-4o on 11 of 18 findings (61%), achieving perfect F1-scores for 
4 conditions and F1-score >0.95 for 14 conditions, despite varied sample sizes 
(7-82 cases).
CONCLUSIONS: Our study established a robust Japanese CT report dataset and 
demonstrated the effectiveness of a specialized language model in structured 
classification of findings. This hybrid approach of machine translation and 
expert validation enabled the creation of large-scale datasets while maintaining 
high-quality standards. This study provides essential resources for advancing 
medical artificial intelligence research in Japanese health care settings, using 
datasets and models publicly available for research to facilitate further 
advancement in the field.

© Yosuke Yamagishi, Yuta Nakamura, Tomohiro Kikuchi, Yuki Sonoda, Hiroshi 
Hirakawa, Shintaro Kano, Satoshi Nakamura, Shouhei Hanaoka, Takeharu Yoshikawa, 
Osamu Abe. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org).

DOI: 10.2196/71137
PMCID: PMC12392688
PMID: 40874833 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: The Department of 
Computational Diagnostic Radiology and Preventive Medicine of The University of 
Tokyo Hospital is sponsored by HIMEDIC Inc and Siemens Healthcare KK.


465. Acad Radiol. 2025 May;32(5):3039-3049. doi: 10.1016/j.acra.2024.10.023. Epub 
2024 Nov 30.

Impact of ChatGPT and Large Language Models on Radiology Education: Association 
of Academic Radiology-Radiology Research Alliance Task Force White Paper.

Ballard DH(1), Antigua-Made A(2), Barre E(3), Edney E(4), Gordon EB(5), Kelahan 
L(6), Lodhi T(7), Martin JG(3), Ozkan M(8), Serdynski K(9), Spieler B(10), Zhu 
D(3), Adams SJ(11).

Author information:
(1)Mallinckrodt Institute of Radiology, Washington University School of 
Medicine, St. Louis, Missouri, USA.
(2)Anne Burnett School of Medicine, Texas Christian University, Fort Worth, 
Texas, USA.
(3)Duke University School of Medicine, Durham, North Carolina, USA.
(4)Department of Radiology, University of Nebraska Medical Center, Omaha, 
Nebraska, USA.
(5)Department of Radiology, University of California San Diego, San Diego, 
California, USA.
(6)Department of Radiology, Northwestern University Feinberg School of Medicine, 
Chicago, Illinois, USA.
(7)Brody School of Medicine at East Carolina University, Greenville, North 
Carolina, USA.
(8)University of Michigan Medical School, Ann Arbor, Michigan, USA.
(9)Inspira Medical Center Vineland, Vineland, New Jersey, USA.
(10)Department of Radiology, Louisiana State University School of Medicine, 
University Medical Center, New Orleans, Louisiana, USA.
(11)Department of Medical Imaging, Royal University Hospital, College of 
Medicine, University of Saskatchewan, Saskatoon, Saskatchewan, Canada. 
Electronic address: scott.adams@usask.ca.

Generative artificial intelligence, including large language models (LLMs), 
holds immense potential to enhance healthcare, medical education, and health 
research. Recognizing the transformative opportunities and potential risks 
afforded by LLMs, the Association of Academic Radiology-Radiology Research 
Alliance convened a task force to explore the promise and pitfalls of using LLMs 
such as ChatGPT in radiology. This white paper explores the impact of LLMs on 
radiology education, highlighting their potential to enrich curriculum 
development, teaching and learning, and learner assessment. Despite these 
advantages, the implementation of LLMs presents challenges, including limits on 
accuracy and transparency, the risk of misinformation, data privacy issues, and 
potential biases, which must be carefully considered. We provide recommendations 
for the successful integration of LLMs and LLM-based educational tools into 
radiology education programs, emphasizing assessment of the technological 
readiness of LLMs for specific use cases, structured planning, regular 
evaluation, faculty development, increased training opportunities, 
academic-industry collaboration, and research on best practices for employing 
LLMs in education.

Copyright © 2024 The Association of University Radiologists. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.acra.2024.10.023
PMID: 39616097 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest No conflicts 
of interest to disclose.


466. Int J Med Inform. 2026 Jan;205:106104. doi: 10.1016/j.ijmedinf.2025.106104. Epub 
2025 Sep 6.

Integrating rule-based NLP and large language models for statin information 
extraction from clinical notes.

Liu S(1), McCoy AB(2), Chen Q(3), Wright A(4).

Author information:
(1)Department of Biomedical Informatics, Vanderbilt University Medical Center, 
Nashville, TN, USA; Department of Computer Science, Vanderbilt University, 
Nashville, TN, USA. Electronic address: siru.liu@vumc.org.
(2)Department of Biomedical Informatics, Vanderbilt University Medical Center, 
Nashville, TN, USA. Electronic address: allison.b.mccoy@vumc.org.
(3)Department of Biomedical Informatics and Data Science, Yale School of 
Medicine, Yale University, New Haven, CT, USA. Electronic address: 
qingyu.chen@yale.edu.
(4)Department of Biomedical Informatics, Vanderbilt University Medical Center, 
Nashville, TN, USA; Department of Medicine, Vanderbilt University Medical 
Center, Nashville, TN, USA. Electronic address: adam.wright@vumc.org.

BACKGROUND: Identifying patient-specific barriers to statin therapy, such as 
intolerance or deferral, from clinical notes is a major challenge for improving 
cardiovascular care. Automating this process could enable targeted interventions 
and improve clinical decision support (CDS).
OBJECTIVE: To develop and evaluate a novel hybrid artificial intelligence (AI) 
framework for accurately and efficiently extracting information on statin 
therapy barriers from large volumes of clinical notes.
METHODS: The hybrid AI framework consisted of a rule-based natural language 
processing (NLP) filter, an LLM-based refinement filter, and an LLM-based 
multi-category classifier. The framework was developed on 2000 clinical notes 
and then retrospectively applied to a dataset of 197,761 notes from 47,192 
patients at an academic medical center. Performance was evaluated against manual 
chart review for classifying statin intolerance, contraindications, and patient 
deferral.
RESULTS: The framework was efficient, with the initial filter removing over 77 % 
of irrelevant notes while achieving a recall of 1.0 to ensure no relevant 
information was lost. The final classifier accurately categorized patient-level 
barriers with high F1 scores for intolerance (0.99), contraindications (0.81), 
and patient deferral (0.86). On the large dataset, the framework identified that 
6.4 % of patients (n = 3,027) had documented intolerance, 0.7 % (n = 310) had 
contraindications, and 2.9 % (n = 1,391) had deferred therapy.
CONCLUSION: The hybrid AI framework provides an efficient, scalable, and 
trustworthy solution for processing clinical notes. It has the potential to 
enhance clinical decision support (CDS) systems by integrating detailed 
patient-level insights, improving adherence to clinical guidelines, and reducing 
provider burden. Future research should focus on implementing CDS tools that 
leverage extracted information to address barriers to statin therapy and 
optimize patient outcomes.

Copyright © 2025 The Author(s). Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106104
PMCID: PMC12705237
PMID: 40925145 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


467. JMIR Form Res. 2025 Aug 11;9:e68666. doi: 10.2196/68666.

Evaluating a Customized Version of ChatGPT for Systematic Review Data Extraction 
in Health Research: Development and Usability Study.

Sercombe J(#)(1), Bryant Z(#)(1), Wilson J(1).

Author information:
(1)The Matilda Centre for Research in Mental Health and Substance Use, 
University of Sydney, Jane Foss Russell Building (G02), Level 6, Sydney, 2006, 
Australia, 612 8627 9380.
(#)Contributed equally

BACKGROUND: Systematic reviews are essential for synthesizing research in health 
sciences; however, they are resource-intensive and prone to human error. The 
data extraction phase, in which key details of studies are identified and 
recorded in a systematic manner, may benefit from the application of automation 
processes. Recent advancements in artificial intelligence, specifically in large 
language models (LLMs) such as ChatGPT, may streamline this process.
OBJECTIVE: This study aimed to develop and evaluate a custom Generative 
Pre-Training Transformer (GPT), named Systematic Review Extractor Pro, for 
automating the data extraction phase of systematic reviews in health research.
METHODS: OpenAI's GPT Builder was used to create a GPT tailored to extract 
information from academic manuscripts. The Role, Instruction, Steps, End goal, 
and Narrowing (RISEN) framework was used to inform prompt engineering for the 
GPT. A sample of 20 studies from two distinct systematic reviews was used to 
evaluate the GPT's performance in extraction. Agreement rates between the GPT 
outputs and human reviewers were calculated for each study subsection.
RESULTS: The mean time for human data extraction was 36 minutes per study, 
compared to 26.6 seconds for GPT generation, followed by 13 minutes of human 
review. The GPT demonstrated high overall agreement rates with human reviewers, 
achieving 91.45% for review 1 and 89.31% for review 2. It was particularly 
accurate in extracting study characteristics (review 1: 95.25%; review 2: 
90.83%) and participant characteristics (review 1: 95.03%; review 2: 90.00%), 
with lower performance observed in more complex areas such as methodological 
characteristics (87.07%) and statistical results (77.50%). The GPT correctly 
extracted data in 14 instances (3.25% in review 1) and four instances (1.16% in 
review 2) when the human reviewer was incorrect.
CONCLUSIONS: The custom GPT significantly reduced extraction time and shows 
evidence that it can extract data with high accuracy, particularly for 
participant and study characteristics. This tool may offer a viable option for 
researchers seeking to reduce resource demands during the extraction phase, 
although more research is needed to evaluate test-retest reliability, 
performance across broader review types, and accuracy in extracting statistical 
data. The tool developed in the current study has been made open access.

© Jayden Sercombe, Zachary Bryant, Jack Wilson. Originally published in JMIR 
Formative Research (https://formative.jmir.org).

DOI: 10.2196/68666
PMCID: PMC12338963
PMID: 40789147 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


468. Int J Med Inform. 2026 Mar 15;208:106230. doi: 10.1016/j.ijmedinf.2025.106230. 
Epub 2025 Dec 18.

Efficient medical NER with limited data: Enhancing LLM performance through 
annotation guidelines.

Shinohara E(1), Kawazoe Y(2).

Author information:
(1)Artificial Intelligence and Digital Twin in Healthcare, Graduate School of 
Medicine, The University of Tokyo, Tokyo, Japan.
(2)Artificial Intelligence and Digital Twin in Healthcare, Graduate School of 
Medicine, The University of Tokyo, Tokyo, Japan. Electronic address: 
kawazoe@m.u-tokyo.ac.jp.

BACKGROUND: Named entity recognition (NER) is critical in natural language 
processing (NLP), particularly in the medical field, where accurate 
identification of entities, such as patient information and clinical events, is 
essential. Traditional NER approaches rely heavily on large, annotated corpora, 
which are resource intensive. Large language models (LLMs) offer new NER 
approaches, particularly through in-context and few-shot learning.
OBJECTIVE: This study investigates the effects of incorporating annotation 
guidelines into prompts for NER via LLMs, with a specific focus on their impact 
on few-shot learning performance across various medical corpora.
METHODS: We designed eight different prompt patterns, combining few-shot 
examples with annotation guidelines of varying complexity, and evaluated their 
performance via three prominent LLMs: GPT-4o, Claude 3.5 Sonnet, and 
gpt-oss-120b. Additionally, we employed three diverse medical corpora: 
i2b2-2014, i2b2-2012, and MedTxt-CR. Accuracy was assessed via precision, 
recall, and the F1 score, with evaluation methods aligned with those used in 
relevant shared tasks to ensure the comparability of the results.
RESULTS: Our findings indicate that adding detailed annotation guidelines to 
few-shot prompts improves the recall and F1 score in most cases.
CONCLUSION: Including annotation guidelines in prompts enhances the performance 
of LLMs in NER tasks, making this a practical approach for developing accurate 
NLP systems in resource-constrained environments. Although annotation guidelines 
are essential for evaluation and example creation, their integration into LLM 
prompts can further optimize few-shot learning, especially within specialized 
domains such as medical NLP.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106230
PMID: 41435443 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest ES and YK are 
employed by the Artificial Intelligence and Digital Twin in Healthcare, Graduate 
School of Medicine, The University of Tokyo, which is an endowed department 
supported by unrestricted grants from EM Systems, EPNextS, MRP Co., Ltd., SHIP 
HEALTHCARE HOLDINGS, Inc., SoftBank Corp., NEC Corporation, and Nippon Sogo 
Systems, Inc. However, these entities had no role in the study design, data 
collection, analysis, interpretation, manuscript preparation, or decision to 
publish. The authors declare no competing financial or non-financial interests.


469. JMIR Med Inform. 2025 Sep 10;13:e70967. doi: 10.2196/70967.

Leveraging GPT-4o for Automated Extraction and Categorization of CAD-RADS 
Features From Free-Text Coronary CT Angiography Reports: Diagnostic Study.

Chen Y(#)(1), Dong M(#)(1), Sun J(#)(1), Meng Z(1), Yang Y(1), Muhetaier A(1), 
Li C(1), Qin J(1).

Author information:
(1)Departments of Radiology, The Third Affiliated Hospital, Sun Yat-Sen 
University, 600 Tianhe Road, Guangzhou, Guangdong, 510630, China, 86 
18922109279, 86 20852523108.
(#)Contributed equally

BACKGROUND: Despite the Coronary Artery Reporting and Data System (CAD-RADS) 
providing a standardized approach, radiologists continue to favor free-text 
reports. This preference creates significant challenges for data extraction and 
analysis in longitudinal studies, potentially limiting large-scale research and 
quality assessment initiatives.
OBJECTIVE: To evaluate the ability of the generative pre-trained transformer 
(GPT)-4o model to convert real-world coronary computed tomography angiography 
(CCTA) free-text reports into structured data and automatically identify 
CAD-RADS categories and P categories.
METHODS: This retrospective study analyzed CCTA reports from January 2024 and 
July 2024. A subset of 25 reports was used for prompt engineering to instruct 
the large language models (LLMs) in extracting CAD-RADS categories, P 
categories, and the presence of myocardial bridges and noncalcified plaques. 
Reports were processed using the GPT-4o API (application programming interface) 
and custom Python scripts. The ground truth was established by radiologists 
based on the CAD-RADS 2.0 guidelines. Model performance was assessed using 
accuracy, sensitivity, specificity, and F1-score. Intrarater reliability was 
assessed using Cohen κ coefficient.
RESULTS: Among 999 patients (median age 66 y, range 58-74; 650 males), CAD-RADS 
categorization showed accuracy of 0.98-1.00 (95% CI 0.9730-1.0000), sensitivity 
of 0.95-1.00 (95% CI 0.9191-1.0000), specificity of 0.98-1.00 (95% CI 
0.9669-1.0000), and F1-score of 0.96-1.00 (95% CI 0.9253-1.0000). P categories 
demonstrated accuracy of 0.97-1.00 (95% CI 0.9569-0.9990), sensitivity from 0.90 
to 1.00 (95% CI 0.8085-1.0000), specificity from 0.97 to 1.00 (95% CI 
0.9533-1.0000), and F1-score from 0.91 to 0.99 (95% CI 0.8377-0.9967). 
Myocardial bridge detection achieved an accuracy of 0.98 (95% CI 0.9680-0.9870), 
and noncalcified coronary plaques detection showed an accuracy of 0.98 (95% CI 
0.9680-0.9870). Cohen κ values for all classifications exceeded 0.98.
CONCLUSIONS: The GPT-4o model efficiently and accurately converts CCTA free-text 
reports into structured data, excelling in CAD-RADS classification, plaque 
burden assessment, and detection of myocardial bridges and calcified plaques.

© Youmei Chen, Mengshi Dong, Jie Sun, Zhanao Meng, Yiqing Yang, Abudushalamu 
Muhetaier, Chao Li, Jie Qin. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org).

DOI: 10.2196/70967
PMCID: PMC12422720
PMID: 40929727 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


470. Otol Neurotol. 2026 Jan 1;47(1):64-69. doi: 10.1097/MAO.0000000000004686. Epub 
2025 Oct 23.

Can Chatbots Please Both Patients and Experts? Benchmarking AI and Clinical 
Guidelines for Hearing Loss.

Hack S(1), Gvili B(2), Tessler I(2), Yogev D(2), Wolfowitz A(2), Rozendorn N(2).

Author information:
(1)City St. Georges University London School of Medicine, Program Delivered by 
University of Nicosia at the Chaim Sheba Medical Center.
(2)Department of Otolaryngology, Sheba Medical Center, Ramat Gan, Israel.

OBJECTIVE: To evaluate the reliability, accuracy, and clarity of responses 
generated by 7 contemporary artificial intelligence chatbots in answering 
patient-focused questions about age-related hearing loss and sudden 
sensorineural hearing loss, and to compare these outputs to expert-authored 
guideline responses as well as layperson ratings.
STUDY DESIGN: Cross-sectional.
SETTING: Academic medical center.
PATIENTS: Not applicable. Ten independent layperson raters, all over the age of 
18, recruited from personal networks, assessed a subset of chatbot and expert 
responses.
INTERVENTIONS: Patient-centered questions, derived from official clinical 
practice guidelines for hearing loss, were submitted to 7 artificial 
intelligence chatbots from 3 major development groups. Responses were rated by a 
blinded panel of 5 otolaryngologists for accuracy, extensiveness, misleading 
content, quality of cited references, and overall reliability. A panel of 10 
independent layperson raters, all over the age of 18, recruited from personal 
networks, assessed a subset of chatbot and expert responses.
MAIN OUTCOME MEASURES: Proportion of chatbot answers rated fully accurate by 
expert panel; mean layperson clarity and trustworthiness scores; frequency of 
misleading information and high-quality references.
RESULTS: The most advanced chatbots achieved full guideline-concordant accuracy 
for up to 50% of questions, while earlier models ranged from 25% to 37.5%. All 
models performed highly for extensiveness and reference quality. Layperson 
ratings were highest for gold-standard expert answers, the latest chatbots 
approached these levels for both clarity and trustworthiness (mean scores: 4.7 
to 4.8 out of 5; 95% CI: 4.67-4.85), and differences between models were of 
moderate-to-large effect size (η 2 =0.29 to 0.30). Misleading content was rare 
and typically not clinically significant.
CONCLUSIONS: Modern artificial intelligence chatbots can provide clear and 
generally reliable patient education for hearing loss, but full guideline 
concordance remains inconsistent. Expert oversight is advised to ensure clinical 
accuracy.

Copyright © 2025, Otology & Neurotology, Inc.

DOI: 10.1097/MAO.0000000000004686
PMID: 41126436 [Indexed for MEDLINE]


471. JMIR Med Inform. 2026 Jan 13;14:e77699. doi: 10.2196/77699.

Large Language Models for Psychiatric Diagnosis Based on Multicenter Real-World 
Clinical Records: Comparative Study.

Sun M(1)(2), Yu J(3), Long Z(2), Yang Y(2), Xiao T(4), Liang J(5), Feng J(6), 
Deng H(7), Huang G(2).

Author information:
(1)Mental Health Center, University-Town Hospital of Chongqing Medical 
University, Chongqing, China.
(2)Sichuan Mental Health Center, The Third Hospital of Mianyang, Mianyang, 
China.
(3)Third People's Hospital of Panzhihua, Panzhihua, China.
(4)The Third People's Hospital of Ganzhou, Ganzhou, China.
(5)The Third People's Hospital of Foshan, Foshan, China.
(6)Lanzhou Third People's Hospital, Lanzhou, China.
(7)Shanxi Provincial Mental Health Center, Taiyuan, China.

BACKGROUND: Psychiatric disorders are diagnostically challenging and often rely 
on subjective clinical judgment, particularly in resource-limited settings. 
Large language models (LLMs) have demonstrated potential in supporting 
psychiatric diagnosis; however, robust evidence from large-scale, real-world 
clinical data remains limited.
OBJECTIVE: This study aimed to evaluate and compare the diagnostic performance 
of multiple LLMs for psychiatric disorders using multicenter real-world 
electronic health records (EHRs).
METHODS: We retrospectively analyzed 9923 inpatient EHRs collected from 6 
psychiatric centers across China, encompassing all ICD-10 (International 
Statistical Classification of Diseases, Tenth Revision) psychiatric categories. 
In total, 3 LLMs-GPT-4.0 (OpenAI), GPT-3.5 (OpenAI), and GLM-4-Plus (Zhipu 
AI)-were evaluated against physician-confirmed discharge diagnoses. Diagnostic 
performance was assessed using strict accuracy criteria and lenient 
classification metrics, with subgroup analyses conducted across diagnostic 
categories and age groups.
RESULTS: GPT-4.0 achieved the highest overall strict diagnostic accuracy (71.7%) 
and the highest weighted F1-score under lenient evaluation (0.881), particularly 
for high-prevalence disorders, such as mood disorders and schizophrenia spectrum 
disorders. Diagnostic performance varied across age groups, with the highest 
accuracy observed in older adult patients (up to 79.5%) and lower accuracy in 
adolescents. Across centers, model performance remained stable, with no 
significant intercenter differences.
CONCLUSIONS: LLMs-especially GPT-4.0-demonstrate promising capability in 
supporting psychiatric diagnosis using real-world EHRs. However, diagnostic 
performance varies by age group and disorder category. LLMs should be regarded 
as assistive tools rather than replacements for clinical judgment, and further 
validation is needed before routine clinical implementation.

©Maoqian Sun, Jia Yu, Zhuhong Long, Yun Yang, Tao Xiao, Jiaquan Liang, Jun Feng, 
Huaili Deng, Guoping Huang. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org), 13.01.2026.

DOI: 10.2196/77699
PMID: 41408781 [Indexed for MEDLINE]


472. Med Oral Patol Oral Cir Bucal. 2025 Mar 1;30(2):e224-e231. doi: 
10.4317/medoral.26824.

Accuracy of ChatGPT 3.5, 4.0, 4o and Gemini in diagnosing oral potentially 
malignant lesions based on clinical case reports and image recognition.

Pradhan P(1).

Author information:
(1)15, Trauma Centre, District Hospital Neemuch Madhya Pradesh - 458441, India 
pragya.pradhan@yahoo.co.in.

BACKGROUND: The accurate and timely diagnosis of oral potentially malignant 
lesions (OPMLs) is crucial for effective management and prevention of oral 
cancer. Recent advancements in artificial intelligence technologies indicates 
its potential to assist in clinical decision-making. Hence, this study was 
carried out with the aim to evaluate and compare the diagnostic accuracy of 
ChatGPT 3.5, 4.0, 4o and Gemini in identifying OPMLs.
MATERIAL AND METHODS: The analysis was carried out using 42 case reports from 
PubMed, Scopus and Google Scholar and images from two datasets, corresponding to 
different OPMLs. The reports were inputted separately for text description-based 
diagnosis in GPT 3.5, 4.0, 4o and Gemini, and for image recognition-based 
diagnosis in GPT 4o and Gemini. Two subject-matter experts independently 
reviewed the reports and offered their evaluations.
RESULTS: For text-based diagnosis, among LLMs, GPT 4o got the maximum number of 
correct responses (27/42), followed by GPT 4.0 (20/42), GPT 3.5 (18/42) and 
Gemini (15/42). In identifying OPMLs based on image, GPT 4o demonstrated better 
performance than Gemini. There was fair to moderate agreement found between 
Large Language Models (LLMs) and subject experts. None of the LLMs matched the 
accuracy of the subject experts in identifying the correct number of lesions.
CONCLUSIONS: The results point towards cautious optimism with respect to 
commonly used LLMs in diagnosing OPMLs. While their potential in diagnostic 
applications is undeniable, their integration should be approached judiciously.

DOI: 10.4317/medoral.26824
PMCID: PMC11972639
PMID: 39864088 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflict of interest, 
financial or otherwise.


473. JMIR Med Inform. 2025 Oct 16;13:e71252. doi: 10.2196/71252.

Large Language Models for Automating Clinical Trial Criteria Conversion to 
Observational Medical Outcomes Partnership Common Data Model Queries: Validation 
and Evaluation Study.

Lee KH(1), Jang S(2), Kim GJ(3), Park S(2), Kim D(2), Kwon OJ(3), Lee JH(4), Kim 
YH(5).

Author information:
(1)Department of Information Medicine, Department of Digital Medicine, Asan 
Medical Center, University of Ulsan College of Medicine, 88 Olympic-ro 43-gil, 
Songpa-gu, Seoul, 05505, Republic of Korea, 82 10-3010-5991, 82 2-3010-2531.
(2)Department of Biomedical Engineering, AMIST, Asan Medical Center, University 
of Ulsan College of Medicine, Seoul, Republic of Korea.
(3)Big Data Research Center, Asan Institute for Life Sciences, Asan Medical 
Center, Seoul, Republic of Korea.
(4)Department of Emergency Medicine, Department of Information Medicine, Asan 
Medical Center, University of Ulsan College of Medicine, Seoul, Republic of 
Korea.
(5)Division of Cardiology, Department of Information Medicine, Asan Medical 
Center, University of Ulsan College of Medicine, Seoul, Republic of Korea.

BACKGROUND: Real-world data-based feasibility assessments enhance clinical trial 
design, but automating eligibility criteria conversion to database queries is 
hindered by challenges related to ensuring high accuracy and generating clear, 
usable outputs.
OBJECTIVE: The aim of this study is to develop an automated system converting 
free-text eligibility criteria from ClinicalTrials.gov into Observational 
Medical Outcomes Partnership Common Data Model (OMOP CDM)-compatible Structured 
Query Language (SQL) queries and systematically evaluate hallucination patterns 
across multiple large language models (LLMs) to identify the optimal deployment 
strategies.
METHODS: Our system employs a three-stage preprocessing pipeline (segmentation, 
filtering, and simplification) achieving 58.2% token reduction while preserving 
clinical semantics. We compared GPT-4 concept mapping performance against USAGI 
using 357 clinical terms from 30 trials. For comprehensive evaluation, we 
analyzed 760 SQL generation attempts (19 trials×8 LLMs×5 prompting strategies) 
using the SynPUF (Synthetic Public Use Files) dataset and validated selected 
queries against National COVID Cohort Collaborative reference concept sets using 
Asan Medical Center's OMOP CDM database.
RESULTS: GPT-4 achieved a 48.5% concept mapping accuracy versus USAGI's 32.0% 
(P<.001), with domain-specific performance ranging from 72.7% (drug) to 38.3% 
(measurement). Surprisingly, the open-source llama3: 8b model achieved the 
highest effective SQL rate (75.8%) compared to GPT-4 (45.3%), attributed to 
lower hallucination rates (21.1% vs 33.7%). The overall hallucination rate was 
32.7%, with wrong domain assignments (34.2%) and placeholder insertions (28.7%) 
being the most common. Clinical validation revealed mixed performance: high 
concordance for type 1 diabetes (Jaccard=0.81), complete failure for pregnancy 
(Jaccard=0.00), and minimal overlap for type 2 diabetes (Jaccard=0.03), despite 
perfect overlap coefficients in both diabetes cases. Moderate performance was 
observed for uncontrolled hypertension (Jaccard=0.18).
CONCLUSIONS: While LLMs can accelerate eligibility criteria transformation, 
hallucination rates of 21-50% necessitate careful model selection and validation 
strategies. Our findings challenge assumptions about model superiority, 
demonstrating that smaller, cost-effective models can outperform larger 
commercial alternatives. Future work should focus on hybrid approaches combining 
LLM capabilities with rule-based methods for handling complex clinical concepts.

© Kye Hwa Lee, Sujung Jang, Grace Juyun Kim, Sukyoung Park, Doeun Kim, Oh Jin 
Kwon, Jae-Ho Lee, Young-Hak Kim. Originally published in JMIR Medical 
Informatics (https://medinform.jmir.org).

DOI: 10.2196/71252
PMCID: PMC12530336
PMID: 41100527 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


474. Ophthalmic Plast Reconstr Surg. 2026 Jan-Feb 01;42(1):51-57. doi: 
10.1097/IOP.0000000000002984. Epub 2025 Aug 11.

Assessment of Differential Diagnoses for Oculoplastics Cases Produced by Large 
Language Models.

Peterson JC(1), Rachapudi SS(1), Hubschman S(1), Heinze K(2), Oetting T(3), 
Rodriguez SM(3), Setabutr P(1), Tran AQ(1).

Author information:
(1)Department of Ophthalmology and Visual Sciences, University of Illinois at 
Chicago, Chicago, Illinois.
(2)Israel Englander Department of Ophthalmology, Cornell University, Ithaca, New 
York.
(3)Department of Ophthalmology and Visual Sciences, University of Iowa, Iowa 
City, Iowa, U.S.A.

PURPOSE: This study aimed to evaluate the accuracy of different large language 
models (LLMs) in generating differential diagnoses for oculoplastic cases.
METHODS: Differential diagnoses were generated for 20 oculoplastic cases sourced 
from University of Iowa EyeRounds.org using 6 LLMs: Chat Generative Pre-Trained 
Transformer (ChatGPT) 3.5, ChatGPT 4.0, OcuSmart/EyeGPT, Google Gemini 1.5, 
Claude 3.5, and Microsoft CoPilot. Outputs were compared against the EyeRounds 
expert-curated differentials examining (1) top diagnosis match rate (2) 
inclusion of the correct diagnosis within the generated list, as well as (3) 
recall and (4) precision, calculated to assess the overlap and conciseness of 
LLM outputs.
RESULTS: OcuSmart/EyeGPT achieved the highest top diagnosis match rate (85 ± 
36%), while Claude 3.5 demonstrated the highest rate of inclusion of correct 
diagnosis in differential, as well as recall rate (100 ± 0% and 55 ± 27%, 
respectively). Google Gemini produced the most precise differentials at 43 ± 
24%. Claude 3.5 generated the largest but least concise lists. LLM performance 
varied by case; for example, idiopathic orbital inflammation cases yielded 
highest recall and top diagnosis match across all models, while floppy eyelid 
syndrome cases demonstrated lowest performance.
CONCLUSIONS: LLMs show promising potential in diagnosing oculoplastic cases, 
with OcuSmart/EyeGPT and Claude 3.5 performing best for determining the case 
diagnosis and recall, and ChatGPT 3.5, OcuSmart/EyeGPT, and Gemini generating 
concise and relevant differentials. However, further research and development 
are necessary to validate LLMs' capabilities and integrate them into the 
clinical workflow.

Copyright © 2025 The American Society of Ophthalmic Plastic and Reconstructive 
Surgery, Inc.

DOI: 10.1097/IOP.0000000000002984
PMID: 40788674 [Indexed for MEDLINE]

Conflict of interest statement: A.Q.T. is a Consultant with Genentech. P.S. is 
an Owner of Lodestone Ophthalmics, Inc. and a Consultant with Oyster Point 
Pharma. J.C.P. is an author on provisional patents US20230194734A1 and 
WO2024145070A1 (University of Miami holds all rights). S.H. holds equity in 
Horizon Surgical Systems. The remaining authors have no financial or conflicts 
of interest to disclose.


475. J Clin Epidemiol. 2026 Jan;189:112049. doi: 10.1016/j.jclinepi.2025.112049. Epub 
2025 Nov 11.

Scalable medication extraction and discontinuation identification from 
electronic health records using large language models.

Shao C(1), Snyder D(2), Li C(2), Gu B(3), Ngan K(3), Yang CT(3), Wu J(3), Wyss 
R(3), Lin KJ(3), Yang J(4).

Author information:
(1)Division of Pharmacoepidemiology and Pharmacoeconomics, Department of 
Medicine, Brigham and Women's Hospital, Harvard Medical School, Boston, MA, USA; 
Harvard T.H. Chan School of Public Health, Harvard University, Boston, MA, USA.
(2)Harvard T.H. Chan School of Public Health, Harvard University, Boston, MA, 
USA.
(3)Division of Pharmacoepidemiology and Pharmacoeconomics, Department of 
Medicine, Brigham and Women's Hospital, Harvard Medical School, Boston, MA, USA.
(4)Division of Pharmacoepidemiology and Pharmacoeconomics, Department of 
Medicine, Brigham and Women's Hospital, Harvard Medical School, Boston, MA, USA; 
Broad Institute of MIT and Harvard, Cambridge, MA, USA; Harvard Data Science 
Initiative, Harvard University, Cambridge, MA, USA. Electronic address: 
jyang66@bwh.harvard.edu.

OBJECTIVES: Identifying medication discontinuations in electronic health records 
(EHRs) is vital for patient safety but is often hindered by information being 
buried in unstructured notes. This study aims to evaluate the capabilities of 
advanced open-sourced and proprietary large language models in extracting 
medications and classifying their medication status from EHR notes, focusing on 
their scalability for medication information extraction without human 
annotation.
STUDY DESIGN AND SETTING: We collected three EHR datasets from diverse sources 
to build the evaluation benchmark: 1 publicly available dataset (Reannotated 
Clinical Acronym Sense Inventory dataset [Re-CASI]), 1 we annotated based on 
public MIMIC notes (MIMIC-IV Medication Snippet dataset [MIV-Med]), and 1 
internally annotated on clinical notes from Mass General Brigham (MGB-Med). We 
evaluated 12 advanced LLMs, including general-domain open-sourced models (eg, 
Llama-3.1-70B-Instruct, Qwen2.5-72B-Instruct), medical-specific models (eg, 
MeLLaMA-70B-chat), and a proprietary model (GPT-4o). We explored multiple LLM 
prompting strategies, including zero-shot, 5-shot, and Chain-of-Thought (CoT) 
approaches. Performance on medication extraction, medication status 
classification, and their joint task (extraction then classification) was 
systematically compared across all experiments.
RESULTS: LLMs showed promising performance on medication extraction, while 
discontinuation classification and joint tasks were more challenging. GPT-4o 
consistently achieved the highest average F1 scores in all tasks under zero-shot 
setting - 94.0% for medication extraction, 78.1% for discontinuation 
classification, and 72.7% for the joint task. Open-sourced models followed 
closely, with Llama-3.1-70B-Instruct achieving the highest performance in 
medication status classification on the MIV-Med dataset (68.7%) and in the joint 
task on both the Re-CASI (76.2%) and MIV-Med (60.2%) datasets. Medical-specific 
LLMs demonstrated lower performance compared to advanced general-domain LLMs. 
Few-shot learning generally improved performance, while CoT reasoning showed 
inconsistent gains. Notably, open-sourced models occasionally surpassed GPT-4o 
performance, underscoring their potential in privacy-sensitive clinical 
research.
CONCLUSION: LLMs demonstrate strong potential for medication extraction and 
discontinuation identification on EHR notes, with open-sourced models offering 
scalable alternatives to proprietary systems and few-shot learning further 
improving LLMs' capability.
PLAIN LANGUAGE SUMMARY: Stopping a medicine can affect safety and treatment 
decisions, yet this detail is often buried in long electronic health record 
notes. We evaluated whether large language models, which read and summarize 
text, can automatically find medication names and decide whether each medicine 
is still being taken, has been stopped, or neither. We tested 12 models, 
including open-source options suitable for secure hospital use, on three 
collections of clinical notes and compared three simple instruction styles: 
giving no examples, showing a few examples, and asking for step-by-step 
reasoning. All models produced usable results. The strongest systems scored 
about 94 for finding medication names and about 78 for deciding continued or 
stopped status, on a standard 0 to 100 measure that balances completeness and 
correctness. Showing a few examples usually helped more than step-by-step 
prompts, and several open-source models performed close to a leading proprietary 
system. These tools could help hospitals and researchers monitor medications at 
scale to support drug-safety studies, adherence tracking, and clinical decision 
support, with local validation and safeguards before clinical use.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jclinepi.2025.112049
PMCID: PMC12714491
PMID: 41232578 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest There are no 
competing interests for any author.


476. JMIR Med Inform. 2026 Jan 9;14:e80205. doi: 10.2196/80205.

Large Language Model-Enabled Editing of Patient Audio Interviews From "This Is 
My Story" Conversations: Comparative Study.

Bains B(#)(1), Rapuri S(#)(1), Robitaille E(#)(1), Wang J(2), Khera A(2), Gomez 
C(3), Reyes E(4), Perry C(4), Wilson J(5), Tracey E(5).

Author information:
(1)Department of Biomedical Engineering, Johns Hopkins University, Baltimore, 
MD, United States.
(2)School of Medicine, Johns Hopkins University, Baltimore, MD, United States.
(3)Department of Computer Science, Johns Hopkins University, Baltimore, MD, 
United States.
(4)Technology Innovation Center, Johns Hopkins Medicine, Baltimore, MD, United 
States.
(5)Division of Spiritual Care and Chaplaincy, Johns Hopkins Medicine, 1800 
Orleans St, Baltimore, MD, 21218, United States, 1 410 215 7749.
(#)Contributed equally

BACKGROUND: This Is My Story (TIMS) was started by Chaplain Elizabeth Tracey to 
promote a humanistic approach to medicine. Patients in the TIMS program are the 
subject of a guided conversation in which a chaplain interviews either the 
patient or their loved one. They are asked four questions to elicit clinically 
actionable information that has been shown to improve communication between 
patients and medical providers, strengthening medical providers' empathy. The 
original recorded conversation is edited into a condensed audio file 
approximately 1 minute and 15 seconds in length and placed in the electronic 
health record where it is easily accessible by all providers caring for the 
patient.
OBJECTIVE: TIMS is active at the Johns Hopkins Hospital and has shown value in 
assisting with provider empathy and communication. It is unique in using audio 
recordings to accomplish this purpose. As the program expands, there exists a 
barrier to adoption due to limited time and resources needed to manually edit 
audio conversations. To address this, we propose an automated solution using a 
large language model to create meaningful and concise audio summaries.
METHODS: We analyzed 24 TIMS audio interviews and created three edited versions 
of each: (1) expert-edited, (2) artificial intelligence (AI)-edited using a 
fully automated large language model pipeline, and (3) novice-edited by two 
medical students trained by the expert. A second expert, blinded to the editor, 
rated the audio interviews in a randomized order. This expert scored both the 
audio quality and content quality of each interview on 5-point Likert scales. We 
quantified transcript similarity to the expert-edited reference using lexical 
and semantic similarity metrics and identified omitted content relative to that 
same expert interview.
RESULTS: Audio quality (flow, pacing, clarity) and content quality (coherence, 
relevance, nuance) were each rated on 5-point Likert scales. Expert-edited 
interviews received the highest mean ratings for both audio quality (4.84) and 
content quality (4.83). Novice-edited scored moderately (3.84 audio, 3.63 
content), while AI-edited scored slightly lower (3.49 audio, 3.20 content). 
Novice and AI edits were rated significantly lower than the expert edits 
(P<.001), but not significantly different from each other. AI and novice-edited 
interview transcripts had comparable overlap with the expert reference 
transcript, while qualitative review found frequent omissions of patient 
identity, actionable insights, and overall context in both the AI and 
novice-edited interviews. AI editing was fully automated and significantly 
reduced the editing time compared to both human editors.
CONCLUSIONS: An AI-based editing pipeline can generate TIMS audio summaries with 
comparable content and audio quality to novice human editors with one hour of 
training. AI significantly reduces editing time and removes the need for manual 
training; with further validation, it could offer a solution to scale TIMS to a 
large range of health care settings.

© Bikram Bains, Sampath Rapuri, Edgar Robitaille, Jonathan Wang, Arnav Khera, 
Catalina Gomez, Eduardo Reyes, Cole Perry, Jason Wilson, Elizabeth Tracey. 
Originally published in JMIR Medical Informatics (https://medinform.jmir.org).

DOI: 10.2196/80205
PMCID: PMC12788710
PMID: 41512195 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


477. Bioethics. 2026 Jan;40(1):52-60. doi: 10.1111/bioe.70019. Epub 2025 Jul 18.

Assisting, Replicating, or Autonomously Acting? An Ethical Framework for 
Integrating AI Tools and Technologies in Healthcare.

Padela AI(1)(2), Hayek R(3), Tabassum A(4), Jotterand F(2)(5), Qadir J(4).

Author information:
(1)Department of Emergency Medicine, Medical College of Wisconsin, Milwaukee, 
Wisconsin, USA.
(2)Center for Bioethics and the Medical Humanities, Institute for Health and 
Equity, Medical College of Wisconsin, Milwaukee, Wisconsin, USA.
(3)Medical College of Georgia, Augusta, Georgia, USA.
(4)College of Engineering, Qatar University, Doha, Qatar.
(5)Institute for Biomedical Ethics, University of Basel, Basel, Switzerland.

Artificial intelligence (AI)-based technologies are increasingly being utilized, 
tested, and integrated into conventional healthcare delivery. Technological 
opportunities, ranging from machine-learning-based data analysis tools to large 
language model-based virtual healthcare assistants, offer significant potential 
to enhance healthcare access and improve outcomes. Researchers have discussed 
potential benefits, including improved resource allocation, diagnostic accuracy, 
and patient outcomes from greater AI integration in healthcare, and also have 
voiced concerns around data privacy, algorithmic bias, and diffused 
accountability. This paper adds to the literature by proposing an ethical 
framework that allows for both describing and normatively evaluating AI-mediated 
healthcare delivery based on its potential impact on human-centered patient 
care. Drawing upon Pelligrino's notions of the patient-doctor relationship, we 
propose a framework with two axes, one related to spectrum of patient engagement 
and the other related to the clinician's role, through which to assess the use 
of AI in healthcare. Technologies and tools that have minimal to no interaction 
with patients and primarily assist physicians in making clinical decisions tend 
to have the least ethical challenges. On the other hand, those that are fully 
patient-facing and work in parallel with doctors or autonomously in therapeutic 
or decisional roles are the most controversial, as they risk making healthcare 
less human-centric. As we advance toward more pervasive integration of AI in 
healthcare, our framework can facilitate upfront design and downstream 
implementation-related decisions.

© 2025 John Wiley & Sons Ltd.

DOI: 10.1111/bioe.70019
PMID: 40679067 [Indexed for MEDLINE]


478. Obes Surg. 2025 Apr;35(4):1469-1475. doi: 10.1007/s11695-025-07794-9. Epub 2025 
Mar 18.

The Performance of Artificial Intelligence in One Anastomosis Gastric Bypass 
Surgery: Comparative Efficacy of ChatGPT-4.0, ChatGPT-Omni, and Gemini AI.

Aksoy E(1).

Author information:
(1)Department of General Surgery, Private Meditech Hospital, Ordu, Turkey. 
doktorerkanaksoy@hotmail.com.

BACKGROUND: The integration of artificial intelligence (AI) into medical 
practice opens up new frontiers for decision support, especially in intricate 
surgical procedures like one-anastomosis gastric bypass (OAGB). This study was 
designed to showcase the potential and performance of three AI 
models-ChatGPT-4.0, ChatGPT-Omni, and Gemini AI-in tackling complex clinical 
queries related to OAGB, thereby paving the way for a more efficient and 
effective surgical practice.
METHODS: The study utilized a comprehensive query evaluation methodology 
comprising 180 questions for ChatGPT-4.0, ChatGPT-Omni, and Gemini AI models, 
equally divided among true/false, multiple-choice, open-ended, and case-scenario 
queries. These questions covered various aspects of OAGB surgery, including 
preoperative assessment, surgical technique, management of complications, and 
long-term outcomes.
RESULTS: ChatGPT-Omni showed higher accuracy rates than Gemini AI and 
ChatGPT-4.0 in most question formats and difficulty levels (p < 0.0001). 
However, the performance gap varied depending on the complexity and type of the 
queries. In true-false and multiple-choice formats, ChatGPT-Omni excelled, 
particularly in complex scenarios (p = 0.017). With a mean of 5.62 on a 
six-point scale, ChatGPT-Omni demonstrated exceptional capability in providing 
accurate and comprehensive answers to both open-ended and case scenarios. 
ChatGPT-Omni demonstrated the highest performance metrics, including precision 
(0.947), recall (0.857), and F1-score (0.9), although these values were 
dependent on the specific query format and type.
CONCLUSIONS: While ChatGPT-Omni demonstrated superior accuracy in many clinical 
queries related to OAGB, especially in simpler decision-making scenarios, it is 
crucial to underscore the need for additional validation in complex clinical 
settings. This cautionary note serves as a reminder of the current limitations 
of AI in surgery and the importance of ongoing research and validation.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s11695-025-07794-9
PMID: 40100615 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics Approval: An ethics 
statement was not required for this study type since no human or animal subjects 
or materials were used. Consent to Participate: Not applicable. Consent for 
Publication: Not applicable. IRB Approval: Not applicable. Competing Interests: 
The authors declare no competing interests.


479. JMIR Med Educ. 2026 Jan 15;12:e82116. doi: 10.2196/82116.

AI-Driven Objective Structured Clinical Examination Generation in Digital Health 
Education: Comparative Analysis of Three GPT-4o Configurations.

Zouakia Z(1)(2), Logak E(1)(2), Szymczak A(1)(2), Jais JP(3)(4), Burgun A(1)(2), 
Tsopra R(1)(2)(5).

Author information:
(1)Clinical Bioinformatics Laboratory, Imagine Institute, Université Paris Cité, 
INSERM UMR1163, Paris, France.
(2)Department of Medical Informatics, Hôpital Européen Georges Pompidou, Hôpital 
Necker Enfants Malades, AP-HP, Paris, France.
(3)Imagine Institute, Université Paris Cité, Paris, France.
(4)Biostatistic Unit, Necker University Hospital, AP-HP, Paris, France.
(5)UPPERS US 007 F-75006, Université Paris Cité, Paris, France.

BACKGROUND: Objective Structured Clinical Examinations (OSCEs) are used as an 
evaluation method in medical education, but require significant pedagogical 
expertise and investment, especially in emerging fields like digital health. 
Large language models (LLMs), such as ChatGPT (OpenAI), have shown potential in 
automating educational content generation. However, OSCE generation using LLMs 
remains underexplored.
OBJECTIVE: This study aims to evaluate 3 GPT-4o configurations for generating 
OSCE stations in digital health: (1) standard GPT with a simple prompt and OSCE 
guidelines; (2) personalized GPT with a simple prompt, OSCE guidelines, and a 
reference book in digital health; and (3) simulated-agents GPT with a structured 
prompt simulating specialized OSCE agents and the digital health reference book.
METHODS: Overall, 24 OSCE stations were generated across 8 digital health topics 
with each GPT-4o configuration. Format compliance was evaluated by one expert, 
while educational content was assessed independently by 2 digital health 
experts, blind to GPT-4o configurations, using a comprehensive assessment grid. 
Statistical analyses were performed using Kruskal-Wallis tests.
RESULTS: Simulated-agents GPT performed best in format compliance and most 
content quality criteria, including accuracy (mean 4.47/5, SD 0.28; P=.01) and 
clarity (mean 4.46/5, SD 0.52; P=.004). It also had 88% (14/16) for usability 
without major revisions and first-place preference ranking, outperforming the 
other configurations. Personalized GPT showed the lowest format compliance, 
while standard GPT scored lowest for clarity and educational value.
CONCLUSIONS: Structured prompting strategies, particularly agents' simulation, 
enhance the reliability and usability of LLM-generated OSCE content. These 
results support the use of artificial intelligence in medical education, while 
confirming the need for expert validation.

©Zineb Zouakia, Emmanuel Logak, Alan Szymczak, Jean-Philippe Jais, Anita Burgun, 
Rosy Tsopra. Originally published in JMIR Medical Education 
(https://mededu.jmir.org), 15.01.2026.

DOI: 10.2196/82116
PMID: 41539673 [Indexed for MEDLINE]


480. Transl Vis Sci Technol. 2026 Jan 5;15(1):22. doi: 10.1167/tvst.15.1.22.

Assessing the Accuracy of Artificial Intelligence-Generated Clinical Summaries 
From Ambulatory Glaucoma Subspecialty Clinical Encounters.

Zhang Y(1), Shi M(2), Chung IY(1), Liebman DL(1), Barna LE(1), Pasquale LR(3), 
Friedman DS(1), Boland MV(1), Shen LQ(1), Wang M(4).

Author information:
(1)Department of Ophthalmology, Massachusetts Eye and Ear, Harvard Medical 
School, Boston, MA, USA.
(2)Schepens Eye Research Institute of Massachusetts Eye and Ear, Harvard Medical 
School, Boston, MA, USA.
(3)Eye and Vision Research Institute, Icahn School of Medicine at Mount Sinai, 
New York, NY, USA.
(4)Harvard Ophthalmology AI Lab, Schepens Eye Research Institute of 
Massachusetts Eye and Ear, Harvard Medical School, Boston, MA, USA.

PURPOSE: The purpose of this study was to evaluate the accuracy of large 
language model (LLM) LLaMA 2-70B in summarizing glaucoma clinic notes into 
patient-friendly language and generating educational material.
METHODS: A random sample of 147 clinic notes from unique patients who visited 
Glaucoma Service at a tertiary center was analyzed. LLaMA 2 generated paragraph 
and bullet-point summaries in five subjects: (1) glaucoma diagnosis and type, 
(2) disease progression, (3) treatment plan, (4) treatment changes, and (5) 
surgical/laser interventions. Two ophthalmologists reviewed responses for 
accuracy and categorized them as "correct," "partially correct," or "incorrect." 
Discrepancies were adjudicated by a glaucoma specialist. A comparison using 
identical prompts was performed on a subset (n = 50) with ChatGPT-4.
RESULTS: LLaMA 2 correctly summarized 97 notes (66%) in paragraph and 103 (70%) 
in bullet format. Another 44 (30%) and 41 (28%) were partially correct, 
respectively. Paragraph summaries were more accurate and complete for glaucoma 
suspects than diagnosed patients (82% vs. 53%, P < 0.001). For targeted clinical 
questions, LLaMA 2 accurately identified glaucoma diagnosis in 118 notes (80%), 
disease stability/progression in 129 (88%), treatment plans in 127 (87%), 
treatment changes in 134 (91%), and surgical/laser interventions in 124 (84%). 
ChatGPT-4 achieved 46% correct paragraph summaries, 50% correct bullet 
summaries, and accuracies of 96%, 88%, 64%, 78%, and 82%, respectively, for 
targeted questions.
CONCLUSIONS: Although LLaMA 2 is not yet reliable as a standalone clinical tool, 
it shows promise to improve clinical communication.
TRANSLATION RELEVANCE: LLMs may enhance patient experience and health literacy 
by standardizing patient-friendly language in clinical care.

DOI: 10.1167/tvst.15.1.22
PMCID: PMC12811871
PMID: 41532689 [Indexed for MEDLINE]

Conflict of interest statement: Disclosure: Y. Zhang, None; M. Shi, None; I.Y. 
Chung, None; D.L. Liebman, None; L.E. Barna, None; L.R. Pasquale, Twenty Twenty 
(C); D.S. Friedman, Perivision (F), Genentech (F), Abbvie (C), Myra Vision (C), 
Life Biosciences (C), Thea Pharmaceuticals (C); M.V. Boland, Carl Zeiss Meditec 
(C), Topcon Healthcare (C); L.Q. Shen, FireCyte Therapeutics (C); M. Wang, None


481. JMIR Med Educ. 2026 Jan 16;12:e75125. doi: 10.2196/75125.

Data Science Education for Residents, Researchers, and Students in Psychiatry 
and Psychology: Program Development and Evaluation Study.

Donnelly HK(1)(2), Mandell D(1), Hwang S(2), Schriver E(2)(3), Vurgun U(2), 
Neill G(4), Patel E(1), Reilly ME(1), Steinberg M(1), Calloway A(1), Gallop 
R(1), Oquendo MA(1), Brown GK(1), Mowery DL(2)(5).

Author information:
(1)Department of Psychiatry, Perelman School of Medicine, University of 
Pennsylvania, Philadelphia, PA, United States.
(2)Institute for Biomedical Informatics, Perelman School of Medicine, University 
of Pennsylvania, Philadelphia, PA, United States.
(3)University of Pennsylvania Health System, University of Pennsylvania, 
Philadelphia, PA, United States.
(4)PennDNA, Perelman School of Medicine, University of Pennsylvania, 
Philadelphia, PA, United States.
(5)Department of Biostatistics, Epidemiology and Informatics, Perelman School of 
Medicine, University of Pennsylvania, 3700 Hamilton Walk, Philadelphia, PA, 
19104, United States, +1 215 746-6677.

BACKGROUND: The use of artificial intelligence (AI) to analyze health care data 
has become common in behavioral health sciences. However, the lack of training 
opportunities for mental health professionals limits clinicians' ability to 
adopt AI in clinical settings. AI education is essential for trainees, equipping 
them with the literacy needed to implement AI tools in practice, collaborate 
effectively with data scientists, and develop skills as interdisciplinary 
researchers with computing skills.
OBJECTIVE: As part of the Penn Innovation in Suicide Prevention Implementation 
Research Center, we developed, implemented, and evaluated a virtual workshop to 
educate psychiatry and psychology trainees on using AI for suicide prevention 
research.
METHODS: The workshop introduced trainees to natural language processing (NLP) 
concepts and Python coding skills using Jupyter notebooks within a secure 
Microsoft Azure Databricks cloud computing and analytics environment. We 
designed a 3-hour workshop that covered 4 key NLP topics: data characterization, 
data standardization, concept extraction, and statistical analysis. To 
demonstrate real-world applications, we processed chief complaints from 
electronic health records to compare the prevalence of suicide-related 
encounters across populations by race, ethnicity, and age. Training materials 
were developed based on standard NLP techniques and domain-specific tasks, such 
as preprocessing psychiatry-related acronyms. Two researchers drafted and 
demonstrated the code, incorporating feedback from the Methods Core of the 
Innovation in Suicide Prevention Implementation Research to refine the 
materials. To evaluate the effectiveness of the workshop, we used the 
Kirkpatrick program evaluation model, focusing on participants' reactions (level 
1) and learning outcomes (level 2). Confidence changes in knowledge and skills 
before and after the workshop were assessed using paired t tests, and open-ended 
questions were included to gather feedback for future improvements.
RESULTS: A total of 10 trainees participated in the workshop virtually, 
including residents, postdoctoral researchers, and graduate students from the 
psychiatry and psychology departments. The participants found the workshop 
helpful (mean 3.17 on a scale of 1-4, SD 0.41). Their overall confidence in NLP 
knowledge significantly increased (P=.002) from 1.35 (SD 0.47) to 2.79 (SD 
0.46). Confidence in coding abilities also improved significantly (P=.01), 
increasing from 1.33 (SD 0.60) to 2.25 (SD 0.42). Open-ended feedback suggested 
incorporating thematic analysis and exploring additional datasets for future 
workshops.
CONCLUSIONS: This study illustrates the effectiveness of a tailored data science 
workshop for trainees in psychiatry and psychology, focusing on applying NLP 
techniques for suicide prevention research. The workshop significantly enhanced 
participants' confidence in conducting data science research. Future workshops 
will cover additional topics of interest, such as working with large language 
models, thematic analysis, diverse datasets, and multifaceted outcomes. This 
includes examining how participants' learning impacts their practice and 
research, as well as assessing knowledge and skills beyond self-reported 
confidence through methods such as case studies for deeper insights.

© Hayoung K Donnelly, David Mandell, Sy Hwang, Emily Schriver, Ugurcan Vurgun, 
Graydon Neill, Esha Patel, Megan E Reilly, Michael Steinberg, Amber Calloway, 
Robert Gallop, Maria A Oquendo, Gregory K Brown, Danielle L Mowery. Originally 
published in JMIR Medical Education (https://mededu.jmir.org).

DOI: 10.2196/75125
PMCID: PMC12810743
PMID: 41544003 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: MAO receives royalties 
from the Research Foundation for Mental Hygiene for commercial use of the 
Columbia Suicide Severity Rating Scale. She volunteers as a scientific advisor 
to Mind Medicine and reviews grants for Alkermes and is a Trustee of Tufts 
University. She also advises St. George’s University and Fundacion Jimenez Diaz. 
GKB receives royalties from the Research Foundation for Mental Hygiene for the 
commercial use of the Columbia Severity Rating Scale. 


482. JMIR Med Inform. 2026 Jan 7;14:e81422. doi: 10.2196/81422.

Large Language Models in Patient Health Communication for Atherosclerotic 
Cardiovascular Disease: Pilot Cross-Sectional Comparative Analysis.

Li P(#)(1), Xu Y(#)(1), Liu X(#)(2), Shen Z(#)(1), Wang Y(1), Lv X(1), Lu Z(1), 
Wu H(1), Zhuang J(1), Chen Y(1)(3)(4).

Author information:
(1)Department of Emergency and Critical Care Medicine, The Affiliated Suzhou 
Hospital of Nanjing Medical University, Suzhou Municipal Hospital, Gusu School, 
Nanjing Medical University, Suzhou, Jiangsu, China.
(2)Department of Rehabilitation, Shandong Provincial Hospital Affiliated to 
Shandong First Medical University, Jinan, Shandong, China.
(3)Department of Emergency Management, School of Health Policy & Management, 
Nanjing Medical University, Nanjing, Jiangsu, China.
(4)Department of Emergency and Critical Care Medicine, The First Affiliated 
Hospital of Nanjing Medical University, Nanjing, Jiangsu, China.
(#)Contributed equally

BACKGROUND: Large language models (LLMs) have emerged as promising tools for 
enhancing public access to medical information, particularly for chronic 
diseases such as atherosclerotic cardiovascular disease (ASCVD). However, their 
effectiveness in patient-centered health communication remains underexplored, 
especially in multilingual contexts.
OBJECTIVE: Our study aimed to conduct a comparative evaluation of 3 advanced 
LLMs-DeepSeek R1, ChatGPT-4o, and Gemini-in generating responses to 
ASCVD-related patient queries in both English and Chinese, assessing their 
performance across the domains of accuracy, completeness, and comprehensibility.
METHODS: We conducted a cross-sectional evaluation based on 25 clinically 
validated ASCVD questions spanning 5 domains-definitions, diagnosis, treatment, 
prevention, and lifestyle. Each question was submitted 5 times to each of the 3 
LLMs in both English and Chinese, yielding 750 responses in total, all generated 
under default settings to approximate real-world conditions. Three 
board-certified cardiologists blinded to model identity independently scored the 
responses using standardized Likert scales with predefined anchors. The 
assessment followed a rigorous multistage process that incorporated 
randomization, washout periods, and final consensus scoring.
RESULTS: DeepSeek R1 achieved the highest "good response" rates (24/25, 96% in 
both English and Chinese), substantially outperforming ChatGPT-4o (21/25, 84%) 
and Gemini (12/25, 48% in English and 17/25, 68% in Chinese). DeepSeek R1 
demonstrated superior median accuracy scores (6, IQR 6-6 in both languages) and 
completeness scores (3, IQR 2-3 in both languages) compared to the other models 
(P<.001). All models had a median comprehensibility score of 3; however, in 
English, DeepSeek R1 and ChatGPT-4o were rated significantly clearer than Gemini 
(P=.006 and P=.03, respectively), whereas no significant between-model 
differences were observed in Chinese (P=.08). Interrater reliability was 
moderate (Kendall W: accuracy=0.578; completeness=0.565; 
comprehensibility=0.486). Performance was consistently stronger for definitional 
and diagnostic questions than for treatment and prevention topics across all 
models. Specifically, none of the models consistently provided responses aligned 
with the latest clinical guidelines for the following key guideline-facing 
question "What is the standard treatment regimen for ASCVD?"
CONCLUSIONS: DeepSeek R1 exhibited promising and consistent performance in 
generating high-quality, patient-facing ASCVD information across both English 
and Chinese, highlighting the potential of open-source LLMs in promoting digital 
health literacy and equitable access to chronic disease information. However, a 
clinically critical weakness was observed in guideline-sensitive treatment: the 
models did not reliably provide guideline-concordant standard treatment 
regimens, suggesting that LLM use should be limited to lower-risk informational 
subqueries (eg, definitions, diagnosis, and lifestyle education) unless 
augmented by expert oversight and safety controls.

©Pengfei Li, Yinfei Xu, Xiang Liu, Zhean Shen, Yi Wang, Xinyi Lv, Ziyi Lu, Hui 
Wu, Jiaqi Zhuang, Yan Chen. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org), 07.01.2026.

DOI: 10.2196/81422
PMCID: PMC12824577
PMID: 41499171 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


483. Ocul Immunol Inflamm. 2026 Jan 13:1-6. doi: 10.1080/09273948.2026.2615858. 
Online ahead of print.

Exploring Artificial Intelligence's Role in Citation Generation for Ocular 
Inflammation and Uveal Diseases Research: A Comparative Evaluation Across Four 
Models.

Civelekler M(1), Citirik M(1).

Author information:
(1)Department of Ophthalmology, Ankara Etlik City Hospital, University of Health 
Sciences, Ankara, Turkey.

PURPOSE: This study evaluated four artificial intelligence (AI) models-ChatGPT, 
Copilot, DeepSeek, and Gemini-for their ability to generate PubMed citations 
related to ocular inflammation and uveal disease. The aim was to assess their 
performance in a specialized clinical context and determine whether these tools 
can support accurate academic referencing.
METHODS: Thirty-five clinical paragraphs from The Review of Ophthalmology (4th 
edition) were provided to each model, which was instructed to generate AMA 
11-style PubMed citations. Outputs were examined for accuracy, DOI matching, and 
clinical relevance. Expert reviewers classified each citation as Fully Cited, 
Partially Cited, or Not Cited. Statistical differences among the models were 
assessed using ANOVA with post hoc analysis.
RESULTS: DeepSeek, a domain-specific model, outperformed other models with an 
accuracy of 65.7% (p < 0.001). Copilot and ChatGPT achieved moderate accuracy 
rates of 42.9% and 37.1% (p = 0.042), respectively, whereas Gemini performed the 
worst, with an accuracy of 5.7% (p < 0.001). These results were statistically 
significant, as confirmed by ANOVA and post hoc analysis. Additional errors 
included incorrect citations, DOI mismatches, and incomplete reference lists. 
Expert validation also showed that DeepSeek produced the highest number of fully 
accurate citations, while the remaining models generated more partial or uncited 
references.
CONCLUSION: AI tools can assist with citation generation, but their reliability 
varies significantly. Domain-specific systems perform better, yet 
inconsistencies such as partial citations and hallucinated details highlight the 
continued need for expert oversight. Accurate academic referencing still depends 
on combining AI-generated material with careful human review.

DOI: 10.1080/09273948.2026.2615858
PMID: 41531109


484. Int J Med Inform. 2026 Feb;206:106171. doi: 10.1016/j.ijmedinf.2025.106171. Epub 
2025 Oct 30.

Learning from experts: A self-improving LLM framework for study population 
generation in clinical research.

Sun Y(1), Chen Z(1), Cai H(1), Lu X(2).

Author information:
(1)College of Biomedical Engineering and Instrument Science, Zhejiang 
University, Zheda Road, 310027 Hangzhou, Zhejiang Province, China.
(2)College of Biomedical Engineering and Instrument Science, Zhejiang 
University, Zheda Road, 310027 Hangzhou, Zhejiang Province, China. Electronic 
address: lvxd@zju.edu.cn.

INTRODUCTION: The widespread adoption of electronic health records has led to 
the rapid accumulation of real-world data (RWD), an essential basis for 
generating real-world evidence (RWE). While large language models (LLMs) have 
supported multiple stages of RWD-driven research, their application to the 
design of study populations with both interpretability and credibility remains a 
challenge, which serves as a bridging role between study objectives and 
downstream analyses.
METHODS: In this study, we propose CriteriaLLM, a framework that enables LLMs to 
generate eligible study populations directly from clinical research objectives 
by incorporating clinician feedback. Inspired by the after-action review method, 
which facilitates learning from past experiences and feedback, we build an 
expert knowledge base that records the LLM output study populations and the 
modifications made by the clinician. A dual-retrieval algorithm, combining 
disease domain relevance and lexical similarity, then identifies relevant 
historical cases from the expert knowledge base to guide future generations. To 
ensure clinical relevance and real-world applicability, we introduce a 
continuous validation loop where expert feedback is iteratively integrated, 
refining model performance over time.
RESULTS: We evaluated our framework on 254 published clinical studies based on 
the MIMIC-III database using four representative LLMs: GPT-4o, Deepseek-R1, and 
two LLaMA models. The experimental results indicate that the proposed framework 
could effectively generate a high-quality study population with the highest 
Macro F1 score on 0.9180, maintaining generalizability across foundation models 
with varying parameter sizes and deployment methods.
CONCLUSION: Our expert-in-the-loop framework allows LLMs to generate eligible 
study populations from clinical objectives without additional fine-tuning. By 
integrating structured expert feedback and retrieval guidance, it enhances the 
quality and reliability of study criteria. With continuous validation, the 
framework highlights a scalable approach toward self-improving systems that 
bridge generative AI with the demands for clinical appropriateness, reliability, 
and interpretability in clinical research.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106171
PMID: 41197329 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


485. PLoS One. 2026 Jan 16;21(1):e0341307. doi: 10.1371/journal.pone.0341307. 
eCollection 2026.

Knowledge graph-based intelligent data management and information innovation 
service model for university library systems.

Liu F(1).

Author information:
(1)Dalian University of Technology Library, Dalian University of Technology, 
Dalian, Liaoning, China.

With the rapid development of digitalization, university libraries find 
themselves under great pressure in adapting to data management, whereas 
traditional management has limitations in meeting personalized needs for 
information. This study constructs a knowledge graph-based intelligent data 
management and information innovation service model for university library 
systems, which adopts a hierarchical design philosophy encompassing five core 
layers: data source layer, data processing layer, knowledge construction layer, 
service application layer, and user interaction layer. By integrating 
multi-source heterogeneous data resources and establishing a unified knowledge 
representation framework, the model facilitates semantic organization as well as 
automatic management of library information. The model employs dynamic fusion 
methods combining large language models and graph embedding to address 
heterogeneous data integration challenges, while leveraging knowledge graph 
semantic association capabilities to provide precise personalized information 
recommendation services. A systematic evaluation conducted for a period of six 
months shows that score for an user experience is 4.40, pointing to an 
improvement of 45.2% from 3.03, with accuracy in search results increasing by 
41.1%, as well as enhancement of service quality and learning effectiveness by 
32.5% and 41.7% respectively, and all 16 technical indexes having met and 
exceeded set standards. This study proposes a realistic solution to help 
university libraries deal with challenges brought by big data, which also 
facilitates intelligent service transformation for university libraries.

Copyright: © 2026 Fanru Liu. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0341307
PMCID: PMC12810841
PMID: 41544095 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


486. J Cosmet Dermatol. 2025 Feb;24(2):e70022. doi: 10.1111/jocd.70022.

A Novel Technique for Eye Rejuvenation: A Case Series of the Combined Use of 
CO(2) Laser Blepharoplasty and Erbium: YAG Resurfacing and A Novel Artificial 
Intelligence Model to Quantify Laser Results.

Kesty CE(1)(2), Kesty KR(1)(2).

Author information:
(1)St. Petersburg Skin and Laser, St. Petersburg, Florida, USA.
(2)Kesty AI, St. Petersburg, Florida, USA.

BACKGROUND: Consumers are searching for a solution to rejuvenate the eye area. 
Surgical blepharoplasties are a common solution, but they lack improvement in 
skin quality.
AIMS: To present a novel procedure of a laser upper blepharoplasty in 
combination with erbium laser resurfacing of the lower eyelid for optimal 
rejuvenation and minimal complications.
METHODS: The authors present a laser upper blepharoplasty with the CO2 laser 
performed at the same time as lower eyelid resurfacing using an erbium laser. 
The authors used an artificial intelligence large language model to assess the 
patient before and after photographs to quantify cosmetic improvement.
RESULTS: After this novel procedure, patients demonstrated significant 
improvements in upper eyelid contour, reduced skin laxity, and smoother lower 
eyelid texture. Patient satisfaction was high, with each patient reporting an 
overall rejuvenated appearance and a more "awake" and youthful look. The 
artificial intelligence algorithm showed cosmetic improvement in line with the 
clinical evaluations by the patient and physician.
CONCLUSIONS: The combination of CO2 laser blepharoplasty and Er:YAG laser 
resurfacing addresses both upper eyelid dermatochalasis and lower eyelid 
wrinkles effectively while minimizing recovery time and the potential for 
complications. Artificial intelligence models were used to enhance this study 
and corroborate evaluator cosmetic improvement.

© 2025 The Author(s). Journal of Cosmetic Dermatology published by Wiley 
Periodicals LLC.

DOI: 10.1111/jocd.70022
PMCID: PMC11806243
PMID: 39921282 [Indexed for MEDLINE]

Conflict of interest statement: K.R.K. is the Founder of Kesty AI, which was 
used in this paper.


487. Br J Oral Maxillofac Surg. 2026 Jan;64(1):66-71. doi: 
10.1016/j.bjoms.2025.08.008. Epub 2025 Sep 3.

Online patient information on temporomandibular disorders provided by UK NHS 
hospitals: assessment and improvement of readability standards using 
AI-chatbots.

Gohari SS(1), Baczynska AJ(2), Weber V(3), Sarwar MS(4), Rehman U(5), Brennan 
PA(6).

Author information:
(1)Department of Otolaryngology and Head and Neck, St George's University 
Hospital NHS Foundation Trust, London, UK. Electronic address: s.gohari@nhs.net.
(2)Imperial College London, London, UK.
(3)Department of Anaesthesia, Whittington Health NHS Trust, London, UK.
(4)Department of Oral and Maxillofacial Surgery, The Queen Victoria Hospital, 
East Grinstead, UK.
(5)UCL Division of Surgery and Interventional Sciences, UCL, London, UK.
(6)Department of Oral and Maxillofacial Surgery, Queen Alexandra Hospital, 
Portsmouth, UK.

Temporomandibular disorders (TMD) are complex conditions that burden patients 
and healthcare systems. Disparities in health literacy may hinder patient 
comprehension of online educational materials, potentially influencing outcomes. 
Artificial intelligence (AI)-driven chatbots offer a promising solution to 
improve the readability of patient information materials. We assessed the 
readability of available online materials on TMD in the United Kingdom (UK) and 
evaluate the ability of three AI-chatbots to improve readability. A search was 
done of all UK public hospital websites with Oral and Maxillofacial Surgery 
(OMFS) or Ear, Nose, and Throat (ENT) units for TMD-related patient information. 
Readability was assessed using five standard scoring systems. Three AI-chatbots 
(ChatGPT, Claude, and Google Gemini) were used to revise the content to an 
11-year-old (sixth-grade) reading level. A total of 31 out of 122 of UK hospital 
Trusts provided online TMD materials. Of these, 12/31 and 1/31 met the target 
readability according to the Flesch-Kincaid Grade Level (FKGL), and the Gunning 
Fog Index/Coleman-Liau Index (GFI/CLI), respectively, with mean (SD) readability 
at 64.68 (6.79) for Flesch Reading Ease Score (FRES). After AI modification by 
Gemini, 96.8% met the target readability per FKGL, 54.8% per GFI, and 29.0% per 
CLI. Gemini improved the mean (SD) score significantly to 82.59 (5.73) 
(p < 0.001) for FRES, meeting the target readability level. Online patient 
information on TMD exceeds the recommended Year Six (sixth grade in the US) 
reading level. AI chatbots, particularly Gemini, can significantly enhance the 
readability of these materials, enabling them to meet health literacy standards 
according to certain readability tools.

Copyright © 2025 The British Association of Oral and Maxillofacial Surgeons. 
Published by Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.bjoms.2025.08.008
PMID: 41033960 [Indexed for MEDLINE]


488. Int J Med Inform. 2026 Mar 1;207:106202. doi: 10.1016/j.ijmedinf.2025.106202. 
Epub 2025 Nov 23.

An AI-Assisted Adaptive Boolean Rubric for exercise prescription evaluation: A 
pilot validation study.

Lai X(1), Lai Y(2), Chen J(3), Huang S(3), Gao Q(4), Huang C(5).

Author information:
(1)Research and Communication Center for Exercise and Health, Xiamen University 
of Technology, Xiamen, Fujian Province, China; School of Sport Medicine and 
Rehabilitation, Beijing Sport University, Beijing, China.
(2)Department of Mathematics and Digital Science, Chengyi College, Jimei 
University, Xiamen, China.
(3)Research and Communication Center for Exercise and Health, Xiamen University 
of Technology, Xiamen, Fujian Province, China.
(4)School of Sport Medicine and Rehabilitation, Beijing Sport University, 
Beijing, China.
(5)Research and Communication Center for Exercise and Health, Xiamen University 
of Technology, Xiamen, Fujian Province, China. Electronic address: 
caihua.huang@foxmail.com.

BACKGROUND: The quality assessment of personalized exercise prescriptions is 
currently hampered by the subjectivity and inefficiency of traditional rating 
scales. Artificial intelligence (AI) presents a transformative opportunity for 
objective, scalable evaluation.
OBJECTIVES: This pilot study aimed to develop an AI-assisted evaluation 
framework and assess the feasibility, reliability, and efficiency of its core 
component, the Adaptive Precision Boolean Rubric (Adaptive-PBR).
METHODS: Based on ACSM guidelines, we developed a 50-item Precision Boolean 
Rubric (PBR) and a 10-item Likert scale. To maximize ecological validity, we 
utilized GPT-4 (via ChatGPT Plus Web Interface) with a dual-run consistency 
protocol to generate case-specific, 20-item Adaptive-PBRs. Twelve experts 
evaluated five diverse clinical cases (yielding 180 rating points) using all 
three instruments under randomized conditions.
RESULTS: The Adaptive-PBR demonstrated excellent inter-rater reliability 
(ICC = 0.83), significantly outperforming the Likert scale (ICC = 0.65) and 
matching the full PBR (ICC = 0.82). Quantitatively, it achieved high scoring 
precision (Median 0.78, IQR 0.70-0.85) while reducing evaluation time by 
approximately 63 % (mean 7.1 vs. 19.5 min) compared to the full PBR. Crucially, 
the Adaptive-PBR mitigated the subjective variability and experience-related 
bias observed with the Likert scale.
CONCLUSIONS: The AI-assisted Adaptive-PBR establishes a feasible, reliable, and 
efficient evaluation standard. By combining granular criteria with AI-driven 
adaptability, it offers a robust alternative to subjective scales, with 
immediate potential as a quality assurance tool in clinical training and 
practice.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106202
PMID: 41351959 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


489. J Infect Public Health. 2025 Sep;18(9):102881. doi: 10.1016/j.jiph.2025.102881. 
Epub 2025 Jun 20.

Letter to the Editor: Comments on "Evaluation of ChatGPT-4 for the detection of 
surgical site infections from electronic health records after colorectal 
surgery: A pilot diagnostic accuracy study".

Namdar AB(1), Keikha M(2).

Author information:
(1)Department of Internal Medicine, School of Medicine, Mashhad University of 
Medical Sciences, Mashhad, Iran.
(2)Tropical and Communicable Diseases Research Center, Iranshahr University of 
Medical Sciences, Iranshahr, Iran; Department of Medical Microbiology, School of 
Medicine, Iranshahr University of Medical Sciences, Iranshahr, Iran. Electronic 
address: masoud.keykha90@gmail.com.

DOI: 10.1016/j.jiph.2025.102881
PMID: 40639989

Conflict of interest statement: Declaration of Competing Interest The authors 
have no conflict of interest.


490. Health Informatics J. 2026 Jan-Mar;32(1):14604582251414578. doi: 
10.1177/14604582251414578. Epub 2026 Jan 6.

Inferential performance and temporal stability of large language models in 
suicide method prediction: A forensic psychiatric analysis.

Aydogan HC(1), Yaşar Teke H(1), Sevindik M(2), Öztürk ZU(1).

Author information:
(1)Department of Forensic Medicine, Ordu University Training and Research 
Hospital, Ordu, Turkey.
(2)Department of Psychiatry, Ordu University Training and Research Hospital, 
Ordu, Turkey.

Objective: This study presents a structured evaluation of large language models 
(LLMs) in predicting suicide methods based exclusively on indirect forensic 
psychiatric indicators. Methods: Ninety-two forensic psychiatric cases 
(2019-2024), involving survivors of suicide attempts formally examined in 
medico-legal contexts, were retrospectively analyzed. Variables included age, 
sex, psychiatric diagnosis, previous suicide attempts, psychiatric medication 
use, impulsivity, and consciousness at emergency admission. Six LLMs were 
tested: ChatGPT-4o, ChatGPT-4o Mini, ChatGPT-O3 (OpenAI), Gemini 2.0 Flash, 
Gemini 2.5 Pro, and Gemini 2.5 Flash (Google DeepMind). Each case was converted 
into a standardized anonymized prompt. Model predictions were categorized by 
blinded forensic physicians and evaluated using accuracy, precision, recall, 
F1-score, and Cohen's Kappa for 1-month reproducibility. Results: Gemini 2.5 
Flash achieved the highest performance with 76.09% accuracy, 46.9% F1-score, and 
45.2% recall. It accurately predicted the dominant method, medication overdose, 
but underperformed for rare categories. Temporal reproducibility was moderate (κ 
= 0.582), while other models exhibited lower and less stable performance. 
Conclusion: LLMs can infer suicide methods from indirect psychiatric data with 
encouraging accuracy. However, limitations in detecting rare methods and 
maintaining temporal consistency suggest the need for further methodological 
refinement and external validation prior to forensic application.

DOI: 10.1177/14604582251414578
PMID: 41492705 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of conflicting interestsThe authors 
declared no potential conflicts of interest with respect to the research, 
authorship, and/or publication of this article.


491. J Crit Care. 2026 Apr;92:155358. doi: 10.1016/j.jcrc.2025.155358. Epub 2025 Nov 
19.

Can large language models approximate the results of meta-analyses in critical 
care? A meta-research study.

Pratte M(1), Thirukumar S(2), Zhang C(3), Slessarev M(4), Basmaji J(4), Prager 
R(4).

Author information:
(1)University of Toronto, Interdivisional Department of Critical Care, Canada. 
Electronic address: michael.pratte@mail.utoronto.ca.
(2)Western University, Faculty of Medicine, Canada.
(3)University of Ottawa, Faculty of Medicine, Canada.
(4)Western University, Division of Critical Care, Canada.

BACKGROUND: Large language models (LLMs) are capable of processing extensive 
textual data and synthesizing evidence to answer complex clinical questions. The 
labor-intensive nature of systematic reviews with meta-analyses (SRMAs) present 
a unique opportunity to evaluate the utility of LLMs as a novel method for 
evidence synthesis.
OBJECTIVE: This study assessed the ability of OpenAI's o3 DeepResearch model to 
approximate the direction of effect, magnitude of effect and certainty of 
evidence for clinical questions addressed by published meta-analyses in top 
critical care medicine journals.
METHODS: We constructed standardized prompts based on the PICO (Population, 
Intervention, Comparator, Outcome) from a convenience sample of 23 systematic 
reviews with meta-analyses published in high-impact critical care journals. The 
LLM's estimates of effect size and certainty of evidence ratings were compared 
to those reported in the original SRMAs.
RESULTS: The LLM demonstrated a concordance rate of 83 % (19 of 23 studies) for 
the magnitude of effect size and 91 % (21 of 23 studies) for the direction of 
effect. Concordance for certainty of evidence was also 91 %. Discrepancies were 
due to differences in study selection between the LLM and SRMAs, rather than 
model hallucination or misinterpretation.
CONCLUSIONS: LLMs show promise as a new tool for rapid evidence synthesis in 
critical care, with outputs comparable to traditional meta-analyses in many 
cases. While not a replacement for systematic reviews, LLMs may enhance clinical 
decision-making, perform rapid evidence synthesis, and streamline future 
research workflows.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jcrc.2025.155358
PMID: 41265262 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest None of the 
authors have relevant conflict of interests to declare.


492. J Med Syst. 2026 Jan 9;50(1):6. doi: 10.1007/s10916-025-02334-5.

Assessment of ChatGPT-5 as an Artificial Intelligence Tool for Exploring 
Emerging Dimensions of Clinical Simulation: A Proof-of-concept Study.

Rios-Garcia W(1)(2), Silva-Jiménez S(3)(4), Gálvez-Rodríguez E(5), Alberca-Naira 
Y(6), Via-Y-Rada-Torres AD(7), Rios-Garcia AA(8).

Author information:
(1)Research Network on Digital Health, Artificial Intelligence, and Education 
(NET-IA WORLD), Lima, Peru. wagner16rg@gmail.com.
(2)Hospital San Juan de Dios de Pisco, Pisco, Perú. wagner16rg@gmail.com.
(3)Facultad de Ciencias Médicas, Universidad de Cuenca, Cuenca, Ecuador.
(4)Asociación Científica de Estudiantes de Medicina de la Universidad de Cuenca 
(ASOCEM-UCuenca), Cuenca, Ecuador.
(5)Universidad Nacional de Trujillo, Trujillo, Perú.
(6)Escuela de Medicina, Universidad Nacional de Piura, Piura, 20002, Perú.
(7)Facultad de Medicina, Universidad Científica del Sur, Lima, Perú.
(8)Research Network on Digital Health, Artificial Intelligence, and Education 
(NET-IA WORLD), Lima, Peru.

Artificial intelligence (AI) and large language models (LLMs) such as ChatGPT-5 
are increasingly applied in medical education. However, their potential role in 
clinical simulation remains largely unexplored. This descriptive 
proof-of-concept study aimed to examine ChatGPT-5's ability to synthesize and 
generate educational content related to clinical simulation, focusing on the 
coherence, factual accuracy, and understandability of its outputs. Seven 
exploratory questions covering conceptual, historical, and technological aspects 
of clinical simulation were submitted to ChatGPT-5. Each query was regenerated 
three times to assess consistency. Responses were independently evaluated by 
multiple reviewers using a five-point Likert scale for content quality and 
accuracy, and the Patient Education Materials Assessment Tool (PEMAT) for 
understandability. Authenticity of AI-generated references was verified through 
PubMed and Google Scholar. ChatGPT-5 produced coherent and organized responses 
reflecting major milestones and trends in clinical simulation. Approximately 80% 
of cited references were verifiable, while some inconsistencies indicated 
residual fabrication. The average agreement score for accuracy and coherence was 
4 ("agree"), suggesting generally acceptable quality. PEMAT analysis showed that 
content was structured and clear but occasionally used complex terminology, 
limiting accessibility. Within the exploratory scope of this proof-of-concept 
study, ChatGPT-5 demonstrated potential as a supportive tool for synthesizing 
information about clinical simulation. Nonetheless, interpretive depth, citation 
reliability, and pedagogical adaptation require further refinement. Future 
research should assess the integration of LLMs into immersive simulation 
environments under robust ethical and educational frameworks.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02334-5
PMID: 41507587 [Indexed for MEDLINE]

Conflict of interest statement: Ethics Declarations. Ethical Approval: Not 
applicable. Consent to Participate: We have not worked with human participants. 
Competing Interests: The authors declare no competing interests.


493. Int J Gynaecol Obstet. 2026 Jan;172(1):482-491. doi: 10.1002/ijgo.70348. Epub 
2025 Jul 1.

Assessment of readability, reliability, and quality of large language models in 
addressing frequently asked questions regarding prenatal screening for fetal 
chromosomal anomalies.

Taşkum İ(1), Sınacı S(2), Aslan F(1), Taşkum GM(3), Sucu S(2).

Author information:
(1)Department of Obstetrics and Gynecology, Gaziantep City Hospital, Gaziantep, 
Turkey.
(2)Department of Obstetrics and Gynecology, Division of Perinatology, Gaziantep 
City Hospital, Gaziantep, Turkey.
(3)Department of Family Medicine, Gaziantep Provincial Health Directorate, 
Gaziantep, Turkey.

OBJECTIVE: To evaluate the responses of large language models (LLMs) to prenatal 
screening questions for fetal chromosomal anomalies in terms of scientific 
accuracy, guideline adherence, depth of response, and clarity, as well as their 
potential roles in patient education and health communication.
METHODS: Responses generated by ChatGPT-4o and Gemini Advanced 1.5 Pro to 
frequently asked questions (FAQs) on prenatal screening for fetal chromosomal 
anomalies were systematically compared. Expert reviewers assessed each reply 
using a Likert scale across four criteria: adherence to clinical guidelines, 
scientific accuracy, clarity, and depth of response. Readability scores were 
calculated with the Flesch Reading Ease (FRE) and Flesch-Kincaid Grade Level 
(FKGL) formulas.
RESULTS: LLMs were evaluated based on their responses to prenatal screening 
questions. ChatGPT-4o received higher proportions of favorable ratings ("good" 
or "excellent") across all evaluation criteria, including scientific accuracy, 
guideline adherence, clarity, and depth of response. The difference in average 
Global Quality Scale scores between ChatGPT and Gemini was statistically 
significant (3.87 vs. 3.70; P = 0.003). The mean FRE scores were 20.11 for 
ChatGPT-4o and 32.25 for Gemini Advanced 1.5 Pro, and the mean FKGL scores were 
15.09 and 12.64, respectively. The differences in both FRE and FKGL scores were 
statistically significant (P = 0.001 and P = 0.002, respectively).
CONCLUSION: LLMs like ChatGPT-4o and Gemini Advanced 1.5 Pro can provide highly 
accurate responses for prenatal screening for fetal chromosomal anomalies, but 
they should only be used for informational purposes. A healthcare professional 
should always be consulted before making any final decisions.

© 2025 International Federation of Gynecology and Obstetrics.

DOI: 10.1002/ijgo.70348
PMID: 40590442 [Indexed for MEDLINE]


494. JMIR Form Res. 2026 Jan 12;10:e79676. doi: 10.2196/79676.

Evaluating Spanish Translations of Emergency Department Discharge Instructions 
by a Large Language Model: Tool Validation and Reliability Study.

Carreras Tartak JA(1), Brewster RC(2), Arango Isaza D(3), Berumen Martinez A(3), 
Grafals A(1), Adusumilli P(4), Fitzgerald T(4), Orcutt R(1), Nathanson LA(1), 
Haimovich AD(1).

Author information:
(1)Department of Emergency Medicine, Beth Israel Deaconess Medical Center, 
Boston, MA, United States.
(2)Department of Pediatrics, Beth Israel Deaconess Medical Center, Boston, MA, 
United States.
(3)Department of Medicine, Beth Israel Deaconess Medical Center, Boston, MA, 
United States.
(4)Department of Technology and Innovation, Beth Israel Lahey Health, Boston, 
MA, United States.

When given a sample of 100 emergency department discharge instructions, Claude 
Sonnet, a large language model, produced accurate Spanish translations as 
evaluated by Spanish-speaking physicians and medical interpreters.

©Jossie A Carreras Tartak, Ryan CL Brewster, Daniela Arango Isaza, Antonio 
Berumen Martinez, Ana Grafals, Phanidhar Adusumilli, Ted Fitzgerald, Roger 
Orcutt, Larry A Nathanson, Adrian D Haimovich. Originally published in JMIR 
Formative Research (https://formative.jmir.org), 12.01.2026.

DOI: 10.2196/79676
PMID: 41525688 [Indexed for MEDLINE]


495. Int J Med Inform. 2026 Feb;206:106164. doi: 10.1016/j.ijmedinf.2025.106164. Epub 
2025 Oct 23.

Comparative evaluation of ChatGPT and Gemini in brain-computer interfaces 
patient education: A multi-dimensional analysis of reliability, accuracy, 
comprehensibility, and readability.

Liu S(1), Su L(2), He Q(3), Qiu M(4), Liang R(5).

Author information:
(1)Department of Neurosurgery, Fujian Medical University Union Hospital, Fuzhou, 
Fujian 350001, China. Electronic address: lsc@fjmu.edu.cn.
(2)Department of Neurosurgery, Fujian Medical University Union Hospital, Fuzhou, 
Fujian 350001, China. Electronic address: sulingning@qq.com.
(3)Department of Neurosurgery, Fujian Medical University Union Hospital, Fuzhou, 
Fujian 350001, China. Electronic address: 15392307908@163.com.
(4)Department of Neurosurgery, Fujian Medical University Union Hospital, Fuzhou, 
Fujian 350001, China. Electronic address: trustqms@qq.com.
(5)Department of Neurosurgery, Fujian Medical University Union Hospital, Fuzhou, 
Fujian 350001, China. Electronic address: doctorlr123@126.com.

BACKGROUND: Brain-Computer Interfaces (BCI) are a type of life-altering 
neurotechnology, but their inherent complexity poses significant challenges to 
patient education. Large Language Models (LLMs), such as ChatGPT and Gemini, 
offer new possibilities to address this challenge. This study aims to conduct a 
multi-dimensional, rigorous comparative analysis of the performance of these two 
mainstream AI models in responding to common patient questions related to BCI.
METHODS: Through a structured process combining clinical expert consensus, 
literature review, and online patient community analysis, we identified 13 key 
patient questions covering the entire BCI treatment cycle. We then obtained 
responses to these questions from ChatGPT and Gemini on September 1, 2025. An 
evaluation panel, composed of clinical experts and non-medical professionals, 
conducted a blinded assessment of the response quality using standardized Likert 
scales across three dimensions: reliability, accuracy, and comprehensibility. 
Concurrently, we performed an objective, quantitative analysis of the response 
texts using the Flesch-Kincaid readability tests.
RESULTS: On core quality metrics such as reliability, accuracy, and 
comprehensibility, the performance of the two models was generally comparable, 
both demonstrating a high level of proficiency with only sporadic statistical 
differences on a few technical questions. However, a clear significant disparity 
emerged in the dimension of readability: for 12 of the 13 questions, the text 
generated by Gemini required a significantly lower reading grade level than that 
of ChatGPT (p < 0.05) and had significantly higher reading ease scores. This 
difference stemmed from Gemini's tendency to use shorter sentences and simpler 
vocabulary.
CONCLUSION: AI chatbots possess immense potential in the field of BCI patient 
education. Although both ChatGPT and Gemini can provide high-quality 
information, Gemini demonstrates a clear advantage in the accessibility and 
approachability of information, making it a potentially more suitable tool for 
initial application across diverse patient populations. Nevertheless, the 
limitations of AI in handling highly specialized and dynamically changing 
knowledge underscore the indispensable role of human expert supervision and 
validation in any clinical application.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106164
PMID: 41151221 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


496. JMIR Med Educ. 2026 Jan 12;12:e75452. doi: 10.2196/75452.

GPT-4o and OpenAI o1 Performance on the 2024 Spanish Competitive Medical 
Specialty Access Examination: Cross-Sectional Quantitative Evaluation Study.

Benito P(1), Isla-Jover M(2), González-Castro P(3), Fernández Esparcia PJ(4), 
Carpio M(5), Blay-Simón I(6), Gutiérrez-Bedia P(7), Lapastora MJ(8), Carratalá 
B(9), Carazo-Casas C(10).

Author information:
(1)Department of Preventive Medicine and Epidemiology, Clinical Institute of 
Medicine and Dermatology (ICMiD), Hospital Clínic de Barcelona, Rosselló, 138, 
ground floor, Barcelona, 08036, Spain, 34 932 27 54 00 ext 4046.
(2)Department of Radiology, Hospital de Cruces, Barakaldo, Spain.
(3)Department of Plastic and Reconstructive Surgery, Hospital Universitario 
Virgen del Rocío, Sevilla, Spain.
(4)Department of Dermatology, Hospital Universitario Ramón y Cajal, Madrid, 
Spain.
(5)Department of Endocrinology and Nutrition, Santa Lucía University General 
Hospital, Cartagena, Spain.
(6)Department of Dermatology, Hospital Universitario Doctor Peset, Valencia, 
Spain.
(7)Department of Neurology, Hospital Clínico San Carlos, Madrid, Spain.
(8)Department of Intensive Care Medicine, Hospital Universitario 12 De Octubre, 
Madrid, Spain.
(9)Innovation and Digital Projects Academic Department, Healthcademia, Madrid, 
Spain.
(10)Department of Otolaryngology, Hospital Universitario Ramón y Cajal, Madrid, 
Spain.

BACKGROUND: In recent years, generative artificial intelligence and large 
language models (LLMs) have rapidly advanced, offering significant potential to 
transform medical education. Several studies have evaluated the performance of 
chatbots on multiple-choice medical examinations.
OBJECTIVE: The study aims to assess the performance of two LLMs-GPT-4o and 
OpenAI o1-on the Médico Interno Residente (MIR) 2024 examination, the Spanish 
national medical test that determines eligibility for competitive medical 
specialist training positions.
METHODS: A total of 176 questions from the MIR 2024 examination were analyzed. 
Each question was presented individually to the chatbots to ensure independence 
and prevent memory retention bias. No additional prompts were introduced to 
minimize potential bias. For each LLM, response consistency under verification 
prompting was assessed by systematically asking, "Are you sure?" after each 
response. Accuracy was defined as the percentage of correct responses compared 
to the official answers provided by the Spanish Ministry of Health. It was 
assessed for GPT-4o, OpenAI o1, and, as a benchmark, for a consensus of medical 
specialists and for the average MIR candidate. Subanalyses included performance 
across different medical subjects, question difficulty (quintiles based on the 
percentage of examinees correctly answering each question), and question types 
(clinical cases vs theoretical questions; positive vs negative questions).
RESULTS: Overall accuracy was 89.8% (158/176) for GPT-4o and 90% (160/176) after 
verification prompting, 92.6% (163/176) for OpenAI o1 and 93.2% (164/176) after 
verification prompting, 94.3% (166/176) for the consensus of medical 
specialists, and 56.6% (100/176) for the average MIR candidate. Both LLMs and 
the consensus of medical specialists outperformed the average MIR candidate 
across all 20 medical subjects analyzed, with ≥80% LLMs' accuracy in most 
domains. A performance gradient was observed: LLMs' accuracy gradually declined 
as question difficulty increased. Slightly higher accuracy was observed for 
clinical cases compared to theoretical questions, as well as for positive 
questions compared to negative ones. Both models demonstrated high response 
consistency, with near-perfect agreement between initial responses and those 
after the verification prompting.
CONCLUSIONS: These findings highlight the excellent performance of GPT-4o and 
OpenAI o1 on the MIR 2024 examination, demonstrating consistent accuracy across 
medical subjects and question types. The integration of LLMs into medical 
education presents promising opportunities and is likely to reshape how students 
prepare for licensing examinations and change our understanding of medical 
education. Further research should explore how the wording, language, prompting 
techniques, and image-based questions can influence LLMs' accuracy, as well as 
evaluate the performance of emerging artificial intelligence models in similar 
assessments.

© Pau Benito, Mikel Isla-Jover, Pablo González-Castro, Pedro José Fernández 
Esparcia, Manuel Carpio, Iván Blay-Simón, Pablo Gutiérrez-Bedia, Maria J 
Lapastora, Beatriz Carratalá, Carlos Carazo-Casas. Originally published in JMIR 
Medical Education (https://mededu.jmir.org).

DOI: 10.2196/75452
PMCID: PMC12795474
PMID: 41525685 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


497. JMIR Med Inform. 2026 Jan 15;14:e77561. doi: 10.2196/77561.

Prompting and Fine-Tuning Large Language Models for Parkinson Disease Diagnosis: 
Comparative Evaluation Study Using the PPMI Structured Dataset.

Shin HJ(1)(2), Jeong YJ(1)(3)(4), Jun S(2)(3)(4), Kang DY(1)(3)(4).

Author information:
(1)Department of Data Sciences Convergence, Graduate School, Dong-A University, 
Busan, Republic of Korea.
(2)Institute of Convergence Bio-Health, Dong-A University, Busan, Republic of 
Korea.
(3)Department of Nuclear Medicine, Dong-A University Hospital, Busan, Republic 
of Korea.
(4)Department of Nuclear Medicine, College of Medicine, Dong-A University, 
Busan, Republic of Korea.

BACKGROUND: Parkinson disease (PD) presents diagnostic challenges due to its 
heterogeneous motor and nonmotor manifestations. Traditional machine learning 
(ML) approaches have been evaluated on structured clinical variables. However, 
the diagnostic utility of large language models (LLMs) using natural language 
representations of structured clinical data remains underexplored.
OBJECTIVE: This study aimed to evaluate the diagnostic classification 
performance of multiple LLMs using natural language prompts derived from 
structured clinical data and to compare their performance with traditional ML 
baselines.
METHODS: We reformatted structured clinical variables from the Parkinson's 
Progression Markers Initiative (PPMI) dataset into natural language prompts and 
used them as inputs for several LLMs. Variables with high multicollinearity were 
removed, and the top 10 features were selected using Shapley additive 
explanations (SHAP)-based feature ranking. LLM performance was examined across 
few-shot prompting, dual-output prompting that additionally generated post hoc 
explanatory text as an exploratory component, and supervised fine-tuning. 
Logistic regression (LR) and support vector machine (SVM) classifiers served as 
ML baselines. Model performance was evaluated using F1-scores on both the test 
set and a temporally independent validation set (temporal validation set) of 
limited size, and repeated output generation was carried out to assess 
stability.
RESULTS: On the test set of 122 participants, LR and SVM trained on the 10 
SHAP-selected clinical variables each achieved a macro-averaged F1-score of 
0.960 (accuracy 0.975). LLMs receiving natural language prompts derived from the 
same variables reached comparable performance, with the best few-shot 
configurations achieving macro-averaged F1-scores of 0.987 (accuracy 0.992). In 
the temporal validation set of 31 participants, LR maintained a macro-averaged 
F1-score of 0.903, whereas SVM showed substantial performance degradation. In 
contrast, multiple LLMs sustained high diagnostic performance, reaching 
macro-averaged F1-scores up to 0.968 and high recall for PD. Repeated output 
generation across LLM conditions produced generally stable predictions, with 
rare variability observed across runs. Under dual-output prompting, diagnostic 
performance showed a reduction relative to few-shot prompting while remaining 
generally stable. Supervised fine-tuning of lightweight models improved 
stability and enabled GPT-4o-mini to achieve a macro-averaged F1-score of 0.987 
on the test set, with uniformly correct predictions observed in the small 
temporal validation set, which should be interpreted cautiously given the 
limited sample size and exploratory nature of the evaluation.
CONCLUSIONS: This study provides an exploratory benchmark of how modern LLMs 
process structured clinical variables in natural language form. While several 
models achieved diagnostic performance comparable to LR across both the test and 
temporal validation datasets, their outputs were sensitive to prompting formats, 
model choice, and class distributions. Occasional variability across repeated 
output generations reflected the stochastic nature of LLMs, and lightweight 
models required supervised fine-tuning for stable generalization. These findings 
highlight the capabilities and limitations of current LLMs in handling tabular 
clinical information and underscore the need for cautious application and 
further investigation.

©Hyun-Ji Shin, Young Jin Jeong, Sungmin Jun, Do-Young Kang. Originally published 
in JMIR Medical Informatics (https://medinform.jmir.org), 15.01.2026.

DOI: 10.2196/77561
PMID: 41539675 [Indexed for MEDLINE]


498. Int J Gynaecol Obstet. 2026 Feb;172(2):1104-1113. doi: 10.1002/ijgo.70407. Epub 
2025 Jul 29.

Leveraging artificial intelligence for evidence-based recommendations in uterine 
fibroid therapy: Addressing the unmet need in German healthcare-A clinical 
trial.

Psilopatis I(1), Lotz L(2), Sipulina N(2), Heindl F(2), Levidou G(3), Emons 
J(2).

Author information:
(1)Department of Gynecology and Obstetrics, Universitätsspital Basel, Basel, 
Switzerland.
(2)Department of Gynecology and Obstetrics, Universitätsklinikum Erlangen, 
Comprehensive Cancer Center Erlangen-EMN (CCC ER-EMN), 
Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Erlangen, Germany.
(3)Department of Pathology, Nuremberg Clinic, Paracelsus Medical University, 
Nuremberg, Germany.

OBJECTIVE: To evaluate the potential of an artificial intelligence (AI)-driven 
large language model, ChatGPT 4.0, to provide personalized, evidence-based 
treatment recommendations for uterine fibroids.
METHODS: ChatGPT 4.0 was trained using evidence-based data from Uptodate and 
German medical literature. The algorithm generated individualized 
recommendations based on clinical characteristics and patient preferences. 
Usability and quality were assessed through questionnaires completed by 40 
gynecologists and 45 women with fibroids.
RESULTS: Most gynecologists found the algorithm user-friendly and comprehensive, 
with 15 expressing a willingness to integrate it into practice and 24 
acknowledging its potential to enhance healthcare efficiency. Although only half 
believed it would improve patient outcomes, the tool was generally well 
received. Patients found the algorithm easy to understand and helpful for 
exploring treatment options, with the majority feeling it empowered informed 
discussions with their healthcare providers. A minority expressed 
dissatisfaction with usability or helpfulness.
CONCLUSION: ChatGPT 4.0 offers a promising AI-driven tool for personalized 
fibroid management in the absence of formal guidelines. Although not a 
substitute for official recommendations, it could support clinical decision 
making and enhance patient education. Further integration with standardized 
guidelines and prospective trials is needed to optimize its clinical utility.

© 2025 The Author(s). International Journal of Gynecology & Obstetrics published 
by John Wiley & Sons Ltd on behalf of International Federation of Gynecology and 
Obstetrics.

DOI: 10.1002/ijgo.70407
PMCID: PMC12790660
PMID: 40728151 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no conflicts of interest.


499. BMC Nurs. 2026 Jan 12. doi: 10.1186/s12912-026-04295-7. Online ahead of print.

Comparative analysis of nursing care plans produced by artificial intelligence 
models (ChatGPT, Gemini, and DeepSeek) in terms of readability, reliability, and 
quality.

Gokalp MG(1), Yucel SC(2).

Author information:
(1)Department of Nursing, Faculty of Health Sciences, Amasya University, Amasya, 
05100, Turkey. mucahide.gokcen@amasya.edu.tr.
(2)Department of Fundamentals of Nursing, Faculty of Health Sciences, Ege 
University, Izmir, 35100, Turkey.

DOI: 10.1186/s12912-026-04295-7
PMID: 41526908

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Researchers utilized AI platforms provided as a “free research 
preview,” which did not require formal ethical approval or institutional 
authorization. Because the data are transparent and verifiable, the study 
clearly details the methods of AI use, including the selection, randomization, 
and organization of nursing diagnoses. Therefore, formal ethical approval was 
not required. Consent for publication: Not applicable. Competing interests: The 
authors declare no competing interests.


500. BMC Anesthesiol. 2026 Jan 8. doi: 10.1186/s12871-025-03605-x. Online ahead of 
print.

Comparative analysis of six large language models in perioperative decision 
support for geriatric patients with multimorbidity: a three-dimensional 
evaluation framework.

Lu J(#)(1), Huang J(#)(1), Guo Y(#)(1), Wu Q(1), Jiang Z(1), Yang T(1), Bian 
J(2), Bo L(3).

Author information:
(1)Department of Anesthesiology, Changhai Hospital, Naval Medical University, 
Shanghai, 200433, China.
(2)Department of Anesthesiology, Changhai Hospital, Naval Medical University, 
Shanghai, 200433, China. jinjunbian@smmu.edu.cn.
(3)Department of Anesthesiology, Changhai Hospital, Naval Medical University, 
Shanghai, 200433, China. bartbo@smmu.edu.cn.
(#)Contributed equally

DOI: 10.1186/s12871-025-03605-x
PMID: 41501659

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. There is no ethics requirement for this as this is 
a simulated case. Competing interests: The authors declare no competing 
interests.


501. J Tradit Complement Med. 2025 Feb 21;15(3):215-228. doi: 
10.1016/j.jtcme.2025.02.009. eCollection 2025 May.

A review of recent artificial intelligence for traditional medicine.

Hou C(1), Gao Y(2), Lin X(1), Wu J(1), Li N(1), Lv H(3), Chu WC(1).

Author information:
(1)School of Computing and Artificial Intelligence, Fuyao University of Science 
and Technology, Fujian, 350109, China.
(2)Maynooth International Engineering College, Fuzhou University, Fuzhou, 
350108, China.
(3)MOE Key Laboratory of Bioinformatics, BNRIST Bioinformatics Division, 
Department of Automation, Tsinghua University, Beijing, 100084, China.

Traditional Medicine (TM) has played a crucial role in global healthcare due to 
its long history and holistic approach. Artificial Intelligence (AI) has emerged 
as a revolutionary technology, offering exceptional capabilities in areas such 
as data mining, pattern recognition, and decision-making. The integration of 
Artificial Intelligence for Traditional Medicine (AITM) presents a promising 
frontier in advancing medicine and healthcare. In this review, we explore AITM 
from two perspectives: recent AI techniques and TM applications. Specifically, 
we investigate how Machine Learning, Deep Learning, and Large Language Models 
are applied to TM, covering applications such as diagnosis (before, during, 
after) and research (drug research, structured knowledge, data analysis). By 
leveraging advanced algorithms and models, AI can improve decision-making 
efficiency, optimize diagnosis accuracy, enhance patient experience, and reduce 
costs. We anticipate this review can bridge the gap between AI and TM 
communities. And the goal is to foster collaboration and innovation between both 
communities, enabling them to exploit the state-of-the-art AI techniques to 
advance TM diagnosis and research, ultimately contributing to the enhancement of 
human health.

© 2025 Center for Food and Biomolecules, National Taiwan University. Production 
and hosting by Elsevier Taiwan LLC.

DOI: 10.1016/j.jtcme.2025.02.009
PMCID: PMC12143335
PMID: 40486280

Conflict of interest statement: The authors declare that they have no known 
competing financial interests or personal relationships that could have appeared 
to influence the work reported in this paper.


502. Digit Health. 2025 Nov 19;11:20552076251393302. doi: 10.1177/20552076251393302. 
eCollection 2025 Jan-Dec.

The double-edged algorithm: A rapid review exploring the trustworthy and 
responsible use of generative AI in public health.

MacKay M(1), Kukan A(1), McWhirter JE(1).

Author information:
(1)Department of Population Medicine, University of Guelph, Guelph, Ontario, 
Canada.

OBJECTIVE: Generative artificial intelligence (genAI) technologies have rapidly 
evolved, offering potential to strengthen core public health functions such as 
health communication, surveillance, and emergency preparedness. While genAI may 
enhance public health outcomes by enabling tailored messaging, helping to combat 
misinformation, and supporting data-driven decision-making, its integration 
raises significant concerns about equity, privacy, and trust.
METHODS: This rapid review explores guiding principles for the trustworthy and 
responsible use of genAI in public health contexts. Following established rapid 
review protocols, peer-reviewed and grey literature published since 2014 were 
identified and analyzed thematically.
RESULTS: Ten articles met the inclusion criteria, focusing on genAI applications 
across various public health settings. Ten themes were generated that describe 
guiding principles for the trustworthy and responsible use of genAI in public 
health. The themes emphasize the importance of human oversight, transparency, 
equity, accountability, and culturally relevant communication. While genAI can 
be used to support health behavior change, enhance health communication across 
literacy levels, and promote community engagement, risks such as algorithmic 
bias, data misuse, and the amplification of health disinformation must be 
mitigated.
CONCLUSION: Organizational policies must reflect ethical considerations and 
address current regulatory gaps to help mitigate these risks. Workforce 
training, interdisciplinary collaboration, and policy development are vital to 
support responsible and trustworthy implementation. This review provides 
preliminary insights that can help public health organizations begin to consider 
guidling principles for policies for guiding genAI adoption and use, emphasizing 
the importance of human-centered and ethically grounded approaches. Findings 
also identify future research needs, including the evaluation of genAI tools in 
diverse public health contexts, assessment of real-world impacts, and 
exploration of governance frameworks. This review offers an initial foundation 
for public health organizations to consider potential applications of genAI and 
develop policies that support its responsible and trustworthy use.

© The Author(s) 2025.

DOI: 10.1177/20552076251393302
PMCID: PMC12638740
PMID: 41278377

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


503. J Med Internet Res. 2025 Sep 17;27:e73391. doi: 10.2196/73391.

Bridging Technology and Pretest Genetic Services: Quantitative Study of Chatbot 
Interaction Patterns, User Characteristics, and Genetic Testing Decisions.

Yi Y(1)(2), Kaiser-Jackson L(2), Bather JR(3)(4), Goodman MS(3)(4), 
Chavez-Yenter D(5)(6), Bradshaw RL(7), Chambers RL(8), Espinel WF(2), Hess 
R(9)(10), Mann DM(11), Monahan R(8)(11), Wetter DW(9)(12), Ginsburg O(13), 
Sigireddi M(8), Kawamoto K(7), Del Fiol G(7), Buys SS(2)(10), Kaphingst 
KA(1)(2).

Author information:
(1)Department of Communication, University of Utah, Salt Lake City, UT, United 
States.
(2)Huntsman Cancer Institute, Salt Lake City, UT, United States.
(3)Center for Anti-Racism, Social Justice & Public Health, School of Global 
Public Health, New York University, New York, United States.
(4)Department of Biostatistics, School of Global Public Health, New York 
University, New York, NY, United States.
(5)Division of Hematology-Oncology, Perelman School of Medicine, University of 
Pennsylvania, Philadelphia, United States.
(6)Department of Medical Ethics and Health Policy, Perelman School of Medicine, 
University of Pennsylvania, Philadelphia, PA, United States.
(7)Department of Biomedical Informatics, Spencer Fox Eccles School of Medicine, 
University of Utah, Salt Lake City, UT, United States.
(8)Perlmutter Cancer Center, NYU Langone Health, New York, NY, United States.
(9)Department of Population Health Sciences, Spencer Fox Eccles School of 
Medicine, University of Utah, Salt Lake City, UT, United States.
(10)Department of Internal Medicine, Spencer Fox Eccles School of Medicine, 
University of Utah, Salt Lake City, UT, United States.
(11)Department of Population Health, NYU Grossman School of Medicine, New York 
City, NY, United States.
(12)Center for Health Outcomes and Population Equity (HOPE), Huntsman Cancer 
Institute, Salt Lake City, UT, United States.
(13)Center for Global Health, National Cancer Institute, Rockville, MD, United 
States.

BACKGROUND: Among the alternative solutions being tested to improve access to 
genetic services, chatbots (or conversational agents) are being increasingly 
used for service delivery. Despite the growing number of studies on the 
accessibility and feasibility of chatbot genetic service delivery, limited 
attention has been paid to user interactions with chatbots in a real-world 
health care context.
OBJECTIVE: We examined users' interaction patterns with a pretest cancer 
genetics education chatbot as well as the associations between users' clinical 
and sociodemographic characteristics, chatbot interaction patterns, and genetic 
testing decisions.
METHODS: We analyzed data from the experimental arm of Broadening the Reach, 
Impact, and Delivery of Genetic Services, a multisite genetic services pragmatic 
trial in which participants eligible for hereditary cancer genetic testing based 
on family history were randomized to receive a chatbot intervention or standard 
care. In the experimental chatbot arm, participants were offered access to core 
educational content delivered by the chatbot with the option to select up to 9 
supplementary informational prompts and ask open-ended questions. We computed 
descriptive statistics for the following interaction patterns: prompt 
selections, open-ended questions, completion status, dropout points, and 
postchat decisions regarding genetic testing. Logistic regression models were 
used to examine the relationships between clinical and sociodemographic factors 
and chatbot interaction variables, examining how these factors affected genetic 
testing decisions.
RESULTS: Of the 468 participants who initiated a chat, 391 (83.5%) completed it, 
with 315 (80.6%) of the completers expressing a willingness to pursue genetic 
testing. Of the 391 completers, 336 (85.9%) selected at least one informational 
prompt, 41 (10.5%) asked open-ended questions, and 3 (0.8%) opted for extra 
examples of risk information. Of the 77 noncompleters, 57 (74%) dropped out 
before accessing any informational content. Interaction patterns were not 
associated with clinical and sociodemographic factors except for prompt 
selection (varied by study site) and completion status (varied by family cancer 
history type). Participants who selected ≥3 prompts (odds ratio 0.33, 95% CI 
0.12-0.91; P=.03) or asked open-ended questions (odds ratio 0.46, 95% CI 
0.22-0.96; P=.04) were less likely to opt for genetic testing.
CONCLUSIONS: Findings highlight the chatbot's effectiveness in engaging users 
and its high acceptability, with most participants completing the chat, opting 
for additional information, and showing a high willingness to pursue genetic 
testing. Sociodemographic factors were not associated with interaction patterns, 
potentially indicating the chatbot's scalability across diverse populations 
provided they have internet access. Future efforts should address the concerns 
of users with high information needs and integrate them into chatbot design to 
better support informed genetic decision-making.

©Yang Yi, Lauren Kaiser-Jackson, Jemar R Bather, Melody S Goodman, Daniel 
Chavez-Yenter, Richard L Bradshaw, Rachelle Lorenz Chambers, Whitney F Espinel, 
Rachel Hess, Devin M Mann, Rachel Monahan, David W Wetter, Ophira Ginsburg, 
Meenakshi Sigireddi, Kensaku Kawamoto, Guilherme Del Fiol, Saundra S Buys, 
Kimberly A Kaphingst. Originally published in the Journal of Medical Internet 
Research (https://www.jmir.org), 17.09.2025.

DOI: 10.2196/73391
PMCID: PMC12489413
PMID: 40961494 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: KAK reports receiving 
grants from the National Cancer Institute (NCI) and the National Institutes of 
Health (NIH) during the conduct of this study. MSG reports receiving grants from 
the NIH during the conduct of this study. RH reports serving as a data and 
safety monitoring board member for Astellas Pharma outside the submitted work. 
RM reports receiving grants from the NIH during the conduct of this study. KK 
reports receiving grants from the NCI during the conduct of this study. In 
addition, KK reports receiving grants from Hitachi and personal fees from 
Pfizer; RTI International; the University of California, San Francisco; Indiana 
University; the National Opinion Research Center at the University of Chicago; 
the University of Pennsylvania; Yale University; Elsevier; MD Aware; Custom 
Clinical Decision Support; the Regenstrief Foundation; the Korean Society of 
Medical Informatics; the University of Nebraska; and the US Office of the 
National Coordinator for Health IT (via Security Risk Solutions) outside the 
submitted work. GDF reports receiving grants from the NCI during the conduct of 
this study. All other authors report no other conflicts of interest.


504. J Am Med Inform Assoc. 2025 Nov 1;32(11):1756-1766. doi: 10.1093/jamia/ocaf150.

Bridging language gaps in healthcare: a systematic review of the practical 
implementation of neural machine translation technologies in clinical settings.

Karakus IS(1), Strechen I(1), Gupta A(1), Nalaie K(1)(2), Chen CL(3), Hassett 
LC(4), Barwise AK(5)(6).

Author information:
(1)Department of Anesthesiology and Perioperative Medicine, Mayo Clinic, 
Rochester, MN 55905, United States.
(2)Division of Nursing Research, Department of Nursing, Mayo Clinic, Rochester, 
MN 55905, United States.
(3)Department of Internal Medicine, Mayo Clinic, Rochester, MN 55905, United 
States.
(4)Mayo Clinic Libraries, Mayo Clinic, Rochester, MN 55905, United States.
(5)Division of Pulmonary and Critical Care Medicine, Mayo Clinic, Rochester, MN 
55905, United States.
(6)Biomedical Ethics Research Program, Mayo Clinic, Rochester, MN 55905, United 
States.

OBJECTIVES: Effective communication is crucial in healthcare, and for patients 
with a non-English language preference (NELP), professional interpreters are 
recognized as the gold standard in supporting bidirectional communication. 
However, interpreters are not always readily available, prompting the 
exploration of other options for translation and interpretation. The recent 
developments in artificial intelligence-based neural network translation tools, 
namely neural machine translation (NMT) may enable robust interpretation and 
translation.
MATERIALS AND METHODS: We conducted a systematic review (SR) to evaluate the 
literature on NMT for this purpose. We did a comprehensive search of several 
databases with guidance from a professional librarian. The search was limited to 
the year 2000 onwards and English language. Title and abstract screening and 
full-text review were independently conducted by two reviewers with conflicts 
resolved by a third reviewer.
RESULTS: 2867 studies were identified with 10 studies included in the final 
analysis. Among these, six evaluated interpretation in real or simulated 
clinical settings and four examined translation of discharge materials. Google 
Translate and ChatGPT were assessed in several studies. Accuracy differed by 
language, with low-resource languages performing worse.
DISCUSSION: NMT technologies in healthcare have several advantages including 
broad language accessibility and potential cost savings for institutions. 
Despite improved accuracy of these novel tools, due to possible critical errors 
NMT tools are not yet ready for widespread clinical use.
CONCLUSION: Future studies should focus on optimizing evaluation methods as well 
as how best to integrate these technologies into real-time clinical settings.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf150
PMCID: PMC12626213
PMID: 40966445 [Indexed for MEDLINE]

Conflict of interest statement: None declared.


505. PLoS One. 2026 Jan 23;21(1):e0341631. doi: 10.1371/journal.pone.0341631. 
eCollection 2026.

User experience and safety of generative AI-based mental health chatbots: 
Scoping review protocol.

Olisaeloka L(1)(2), Richardson C(1)(2), Vigo D(1)(2).

Author information:
(1)Department of Psychiatry, Faculty of Medicine, University of British 
Columbia, Vancouver, Canada.
(2)School of Population and Public Health, Faculty of Medicine, University of 
British Columbia, Vancouver, Canada.

INTRODUCTION: Mental health problems constitute a significant global health 
challenge due to their rising prevalence and substantial treatment gap. Digital 
Mental Health Interventions (DMHIs) including mental health chatbots have 
emerged as promising solutions due to their effectiveness and scalability. 
Recent advances in Generative Artificial Intelligence (GenAI) have improved the 
conversational abilities of these chatbots, further amplifying their potential. 
However, despite instances of inadvertent harm stemming from the unpredictable 
nature of GenAI, little attention has been paid to user experience and safety of 
these chatbots.
OBJECTIVE: This proposed review will explore existing research on GenAI-based 
mental health chatbots. Specifically, it aims to identify and describe current 
chatbots, focusing on user experience, safety and risk mitigation strategies.
METHODS: The review will follow the Joanna Briggs Institute (JBI) guidelines for 
conducting scoping reviews. It will also adhere to the Preferred Reporting Items 
for Systematic Reviews and Meta-Analyses Extension for Scoping Review 
(PRISMA-ScR). A systematic database search of Medline (PubMed), Scopus, 
PsycINFO, ACM Digital Library, and IEEE Xplore will be conducted. The database 
search will be complimented by research-based search engines (Google Scholar and 
Consensus). Studies focusing on the development, evaluation or implementation of 
GenAI-based mental health chatbots will be included without limitations to 
specific disorders or population groups. Two independent reviewers will perform 
screening and data extraction. The analysis will include descriptive summary and 
thematic analysis, with results presented in tabular, graphical, and narrative 
formats.
CONCLUSION: This review will provide a comprehensive overview of GenAI-based 
mental health chatbots while identifying innovative practices and knowledge gaps 
relating to user experience and safety. Findings will inform the ethical 
development, evaluation and implementation of GenAI-based mental health 
interventions.

Copyright: © 2026 Olisaeloka et al. This is an open access article distributed 
under the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0341631
PMID: 41576176 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


506. Ann Biomed Eng. 2025 Jul;53(7):1754-1756. doi: 10.1007/s10439-025-03738-7. Epub 
2025 Apr 24.

DeepSeek's Readiness for Medical Research and Practice: Prospects, Bottlenecks, 
and Global Regulatory Constraints.

MohanaSundaram A(1), Sathanantham ST(2), Ivanov A(3), Mofatteh M(4).

Author information:
(1)School of Pharmacy, Sathyabama Institute of Science and Technology, Jeppiaar 
Nagar, Rajiv Gandhi Salai, Chennai, Tamilnadu, 600 119, India. 
arun.laureate@gmail.com.
(2)School of Pharmaceutical Sciences, Vels Institute of Science, Technology and 
Advanced Studies (VISTAS), Chennai, Tamilnadu, India.
(3)Department of Physiology, Anatomy and Genetics, University of Oxford, Oxford, 
UK.
(4)Queen's University Belfast, Belfast, UK.

DeepSeek, an open-source multimodal Large Language Model (LLM), was launched by 
the Chinese startup (Hangzhou DeepSeek Artificial Intelligence Basic Technology 
Research Co., Ltd.). Despite the lack of advanced artificial intelligence (AI) 
chips, the performance of its milestone version, "DeepSeek-V3," has set an 
unprecedented benchmark among LLMs, surpassing existing models. Notably, the 
opportunity to deploy this model in the local system helps build 
better-performing "distilled versions" suitable for medical research (hypothesis 
generation, drafting patient consent forms and biostatistical analysis, etc.) 
and clinical practice (differential diagnosis from symptom clusters, current 
guideline-based treatment protocol design, interactive medical training, 
personalized patient education, etc.). However, privacy and security risks, 
ethical uncertainties, and diversified global AI regulations hinder its 
potential for sustainable integration into real-world applications.

© 2025. The Author(s) under exclusive licence to Biomedical Engineering Society.

DOI: 10.1007/s10439-025-03738-7
PMID: 40272697 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflicts of interest: The authors 
do not have any conflicts of interest.


507. Digit Health. 2025 Nov 20;11:20552076251395449. doi: 10.1177/20552076251395449. 
eCollection 2025 Jan-Dec.

Hybrid artificial intelligence frameworks for otoscopic diagnosis: Integrating 
convolutional neural networks and large language models toward real-time mobile 
health.

Chu YC(1)(2)(3), Chen YC(4)(5), Hsu CY(3)(6), Kuo CT(1)(3), Cheng 
YF(5)(7)(8)(9), Lin KH(1)(3), Liao WH(7)(8).

Author information:
(1)Department of Information Management, Taipei Veterans General Hospital, 
Taipei.
(2)Big Data Canter, Taipei Veterans General Hospital, Taipei.
(3)Department of Information Management, National Taipei University of Nursing 
and Health Sciences, Taipei.
(4)Department of Otolaryngology-Head and Neck Surgery, Kaohsiung Municipal 
Gangshan Hospital (Outsourced by Show-Chwan Memorial Hospital), Kaohsiung.
(5)Institute of Brain Science, National Yang Ming Chiao Tung University, Taipei.
(6)Master Program in Global Health and Health Security, College of Public 
Health, Taipei Medical University, Taipei.
(7)Department of Otolaryngology-Head and Neck Surgery, Taipei Veterans General 
Hospital, Taipei.
(8)Faculty of Medicine, National Yang Ming Chiao Tung University, Taipei.
(9)Department of Medical Research, Taipei Veterans General Hospital, Taipei.

BACKGROUND: Otitis media remains a significant global health concern, 
particularly in resource-limited settings where timely diagnosis is challenging. 
Artificial intelligence (AI) offers promising solutions to enhance diagnostic 
accuracy in mobile health applications.
OBJECTIVE: This study introduces a hybrid AI framework that integrates 
convolutional neural networks (CNNs) for image classification with large 
language models (LLMs) for clinical reasoning, enabling real-time otoscopic 
diagnosis.
METHODS: We developed a dual-path system combining CNN-based feature extraction 
with LLM-supported interpretation. The framework was optimized for mobile 
deployment, with lightweight models operating on-device and advanced reasoning 
performed via secure cloud APIs. A dataset of 10,465 otoendoscopic images 
(expanded from 2820 original clinical images through data augmentation) across 
10 middle-ear conditions was used for training and validation. Diagnostic 
performance was benchmarked against clinicians of varying expertise.
RESULTS: The hybrid CNN-LLM system achieved an overall diagnostic accuracy of 
97.6%, demonstrating the synergistic benefit of combining CNN-driven visual 
analysis with LLM-based clinical reasoning. The system delivered sub-200 ms 
feedback and achieved specialist-level performance in identifying common ear 
pathologies.
CONCLUSIONS: This hybrid AI framework substantially improves diagnostic 
precision and responsiveness in otoscopic evaluation. Its mobile-friendly design 
supports scalable deployment in telemedicine and primary care, offering a 
practical solution to enhance ear disease diagnosis in underserved regions.

© The Author(s) 2025.

DOI: 10.1177/20552076251395449
PMCID: PMC12639226
PMID: 41278373

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


508. J Am Acad Psychiatry Law. 2025 Jun 10;53(2):157-159. doi: 
10.29158/JAAPL.250022-25.

AI-Assisted Deception and the Emerging Challenge of LLMs in Forensic Psychiatry.

Roof JG(1).

Author information:
(1)Dr. Roof is a clinical professor of Psychiatry, Associate Training Director, 
Division of Psychiatry and the Law, Department of Psychiatry & Behavioral 
Sciences, University of California, Davis, Davis, CA. jgroof@ucdavis.edu.

Generative artificial intelligence (AI), including the large language model 
ChatGPT, has introduced potential new opportunities and challenges to the 
practice of forensic psychiatry. These powerful AI-based tools may offer 
substantial benefits in administrative tasks, report generation, and record 
summarization yet simultaneously present areas for further consideration, such 
as aiding evaluees in feigning psychiatric symptoms. Additional ethics and legal 
considerations exist regarding privacy, bias within AI models, and the 
introduction of fabricated or misleading AI-generated content into forensic 
assessments. Legislative efforts, privacy safeguards, and professional 
guidelines essential for responsible AI use are being developed. Forensic 
psychiatrists are uniquely positioned to influence responsible AI integration 
through education, advocacy, and development of best practices within 
psychiatry.

© 2025 American Academy of Psychiatry and the Law.

DOI: 10.29158/JAAPL.250022-25
PMID: 40425270 [Indexed for MEDLINE]


509. BMC Emerg Med. 2025 Oct 30;25(1):220. doi: 10.1186/s12873-025-01396-5.

Evaluation of ChatGPT-5 responses in obstetric and gynecological emergencies: 
concordance, readability, and clinical reliability.

Arslan HÇ(1), Arslan K(2), Gün M(3), Karkın PO(4), Arslanoğlu T(4).

Author information:
(1)Department of Gynecology and Obstetrics, Kanuni Sultan Süleyman Training and 
Research Hospital, University of Health Sciences, Istanbul, Turkey. 
halecetin90@gmail.com.
(2)Department of Anesthesiology and Reanimation, Kanuni Sultan Süleyman Training 
and Research Hospital, University of Health Sciences, Istanbul, Turkey.
(3)Emergency Medicine, Ministry of Health, Şile State Hospital, Istanbul, 
Turkey.
(4)Department of Gynecology and Obstetrics, Kanuni Sultan Süleyman Training and 
Research Hospital, University of Health Sciences, Istanbul, Turkey.

BACKGROUND: This study aimed to evaluate the compliance with guidelines, 
clinical safety, and applicability of ChatGPT-5 responses in obstetric and 
gynecological emergency scenarios. With the increasing role of AI-powered large 
language models (LLMs) in healthcare, there is a need to examine their 
performance in obstetric emergencies systematically.
METHODS: This study was designed as a prospective, scenario-based, double-blind 
study. A total of 15 obstetric and gynecologic emergency scenarios were created 
based on the literature and current international guidelines (ACOG, RCOG, WHO). 
Five standard questions were posed to ChatGPT-5 for each scenario: (1) Most 
likely diagnosis, (2) Investigations to confirm the diagnosis, (3) Hemodynamic 
stability assessment, (4) Initial treatment approach, and (5) Advanced 
management options. The same scenarios were independently answered by two 
obstetricians, an emergency medicine specialist, and an anesthesiologist, and 
were considered the "gold standard." Responses were scored for guideline 
compliance, patient safety, and critical information gaps. In addition, quality 
and understandability were evaluated with modified DISCERN (mDISCERN), Global 
Quality Score (GQS), and readability indexes [Flesch Reading Ease Score (FRES), 
Flesch-Kincaid Grade Level (FKGL), Simple Measure of Gobbledygook (SMOG), 
Coleman-Liau Index (CLI)].
RESULTS: A total of 75 responses were reviewed. High agreement (5/5) was 
observed in 5 scenarios (33.3%), moderate agreement (4/5) in 7 scenarios 
(46.7%), and low agreement (≤ 3/5) in 3 scenarios (20.0%). High agreement was 
particularly evident for well-defined guideline algorithms, such as postpartum 
hemorrhage, eclampsia, HELLP syndrome, shoulder dystocia, and ruptured ectopic 
pregnancy. Deficiencies in moderate agreement scenarios included insufficient 
emphasis on mortality risk, omission of scoring systems, incomplete steps in 
sepsis management, and inadequate specification of fertility-sparing approaches. 
Low agreement scenarios included severe vaginal hemorrhage, acute bleeding due 
to malignancy, and traumatic gynecologic emergencies. The mean mDISCERN score of 
the responses was 4.0 ± 0.7, and the mean GQS was 4.1 ± 0.7. Readability 
analyses showed that responses contained a moderate amount of technical language 
(FRES score = 40.5 ± 2.5; FKGL score = 11.6 ± 1.2; SMOG score = 10.9 ± 0.8; and 
CLI score = 10.9 ± 0.8). The mean lexical density was 0.63.
CONCLUSIONS: ChatGPT-5 generally produced moderate to good guideline-compliant 
and confident responses in obstetric and gynecological emergency scenarios. 
However, its performance was limited in complex cases requiring a 
multidisciplinary approach. The findings suggest that AI-powered large language 
models can be a complementary tool in obstetric emergency management, but should 
not be used alone without expert clinician supervision. Larger, comparative, and 
multidisciplinary studies will provide more reliable evidence for the clinical 
integration of these technologies.

© 2025. The Author(s).

DOI: 10.1186/s12873-025-01396-5
PMCID: PMC12574122
PMID: 41163202 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study did not involve any human patients or identifiable 
personal data. It was based entirely on simulated clinical scenarios and 
AI-generated outputs. However, all human experts who participated as evaluators 
(two obstetricians, one emergency medicine specialist, and one anesthesiologist) 
provided written informed consent before participation. The study was conducted 
in accordance with the ethical principles of the Declaration of Helsinki. 
Competing interests: The authors declare no competing interests.


510. Int J Surg. 2025 Aug 1;111(8):5071-5087. doi: 10.1097/JS9.0000000000002582. Epub 
2025 Jun 5.

Performance analysis of large language models in multi-disease detection from 
chest computed tomography reports: a comparative study.

Luo P(1)(2), Fan C(3), Li A(4), Jiang T(3), Jiang A(5), Qi C(6), Gan W(7), Zhu 
L(4), Mou W(8), Zeng D(9)(10), Tang B(11), Xiao M(12), Chu G(13), Liang Z(14), 
Shen J(4), Liu Z(15), Wei T(4), Cheng Q(16)(17), Lin A(1)(2), Chen X(3).

Author information:
(1)Donghai County People's Hospital (Affiliated Kangda College of Nanjing 
Medical University); Department of Oncology, Zhujiang Hospital, Southern Medical 
University, Lianyungang, China.
(2)Cancer Centre and Institute of Translational Medicine, Faculty of Health 
Sciences, University of Macau, Macau SAR, China.
(3)Department of Pulmonary and Critical Care Medicine, Zhujiang Hospital, 
Southern Medical University, Guangzhou, China.
(4)Department of Oncology, Zhujiang Hospital, Southern Medical University, 
Guangzhou, China.
(5)Department of Urology, Changhai Hospital, Naval Medical University (Second 
Military Medical University), Shanghai, China.
(6)Department of Microbiology, State Key Laboratory of Emerging Infectious 
Diseases, Carol Yu Centre for Infection, School of Clinical Medicine, Li Ka 
Shing Faculty of Medicine, The University of Hong Kong, Hong Kong, China.
(7)Department of Joint Surgery and Sports Medicine, Zhuhai People's Hospital 
(Zhuhai Hospital Affiliated with Jinan University), Guangdong, China.
(8)Department of Urology, Shanghai General Hospital, Shanghai Jiao Tong 
University School of Medicine, Shanghai, China.
(9)Department of Oncology, Nanfang Hospital, Southern Medical University, 
Nanfang, China.
(10)Cancer Center, The Sixth Affiliated Hospital, School of Medicine, South 
China University of Technology, Foshan, China.
(11)Department of Radiation Oncology, Zhongshan Hospital Affiliated to Fudan 
University, Shanghai, China.
(12)Hepatobiliary Surgery Department, Quzhou Affiliated Hospital of Wenzhou 
Medical University, Quzhou People's Hospital, Quzhou, China.
(13)Department of Urology, The Affiliated Hospital of Qingdao University, 
Qingdao, China.
(14)State Key Laboratory of Respiratory Disease, National Clinical Research 
Center for Respiratory Disease, Guangzhou Institute of Respiratory Health, The 
First Affiliated Hospital of Guangzhou Medical University, Guangzhou, China.
(15)Institute of Basic Medical Sciences, Chinese Academy of Medical Sciences and 
Peking Union Medical College, Beijing, China.
(16)Department of Neurosurgery, Xiangya Hospital, Central South University, 
Changsha, China.
(17)National Clinical Research Center for Geriatric Disorders, Xiangya Hospital, 
Central South University, Changsha, China.

BACKGROUND: Computed tomography (CT) is widely acknowledged as the gold standard 
for diagnosing thoracic diseases. However, the accuracy of interpretation 
significantly depends on radiologists' expertise. Large language models (LLMs) 
have shown considerable promise in various medical applications, particularly in 
radiology. This study aims to assess the performance of leading LLMs in 
analyzing unstructured chest CT reports and to examine how different questioning 
methodologies and fine-tuning strategies influence their effectiveness in 
enhancing chest CT diagnosis.
MATERIALS AND METHODS: This retrospective analysis evaluated 13 489 chest CT 
reports encompassing 13 common thoracic conditions across pulmonary, 
cardiovascular, pleural, and upper abdominal systems. Five LLMs 
(Claude-3.5-Sonnet, GPT-4, GPT-3.5-Turbo, Gemini-Pro, Qwen-Max) were assessed 
using dual questioning methodologies: multiple-choice and open-ended. 
Radiologist-curated datasets underwent rigorous preprocessing, including RadLex 
terminology standardization, multi-step diagnostic validation, and exclusion of 
ambiguous cases. Model performance was quantified via Subjective Answer Accuracy 
Rate (SAAR), Reference Answer Accuracy Rate (RAAR), and Area Under the Receiver 
Operating Characteristic (ROC) Curve analysis. GPT-3.5-Turbo underwent 
fine-tuning (100 iterations with one training epoch) on 200 high-performing 
cases to enhance diagnostic precision for initially misclassified conditions.
RESULTS: GPT-4 demonstrated superior performance with the highest RAAR of 75.1% 
in multiple-choice questioning, followed by Qwen-Max (66.0%) and Claude-3.5 
(63.5%), significantly outperforming GPT-3.5-Turbo (41.8%) and Gemini-Pro 
(40.8%) across the entire patient cohort. Multiple-choice questioning 
consistently improved both RAAR and SAAR for all models compared to open-ended 
questioning, with RAAR consistently surpassing SAAR. Model performance 
demonstrated notable variations across different diseases and organ conditions. 
Notably, fine-tuning substantially enhanced the performance of GPT-3.5-Turbo, 
which initially exhibited suboptimal results in most scenarios.
CONCLUSION: This study demonstrated that general-purpose LLMs can effectively 
interpret chest CT reports, with performance varying significantly across models 
depending on the questioning methodology and fine-tuning approaches employed. 
For surgical practice, these findings provided evidence-based guidance for 
selecting appropriate AI tools to enhance preoperative planning, particularly 
for thoracic procedures. The integration of optimized LLMs into surgical 
workflows may improve decision-making efficiency, risk stratification, and 
diagnostic speed, potentially contributing to better surgical outcomes through 
more accurate preoperative assessment.

Copyright © 2025 The Author(s). Published by Wolters Kluwer Health, Inc.

DOI: 10.1097/JS9.0000000000002582
PMID: 40497825 [Indexed for MEDLINE]


511. Curr Opin Psychol. 2025 Oct;65:102093. doi: 10.1016/j.copsyc.2025.102093. Epub 
2025 Jun 28.

Psychometric properties of personality assessment using machine learning.

Koutsoumpis A(1).

Author information:
(1)Department of Social Psychology, Tilburg School of Social and Behavioral 
Sciences, Tilburg University, Netherlands. Electronic address: 
a.koutsoumpis@tilburguniversity.edu.

Technological advancements have enabled personality psychologists to move beyond 
traditional questionnaire-based assessment toward machine learning-based 
personality assessment (ML-PA). This manuscript provides a non-systematic 
overview of the validity and reliability of ML-PA, where behavioral features 
(e.g., text, voice, digital footprints) serve as predictors of personality 
traits. ML-PA shows promising construct validity, particularly for observer 
reports, and ML-PA values are similarly correlated with external variables as 
questionnaire-based values. However, reliability indices, especially for 
self-report-based ML-PAs, have been found to be lower. Factors such as sample 
size, input data quantity, and trait activation significantly impact ML-PA 
accuracy. Algorithmic bias might pose a threat to ML-PA, and there is a 
trade-off between applying bias mitigation techniques and maximizing ML-PA 
performance. Future advancements, including the use of large language models and 
a focus on explainability, are expected to further enhance personality 
measurement using computational methods.

Copyright © 2025 The Author(s). Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.copsyc.2025.102093
PMID: 40651430 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The author 
declares no conflict of interest.


512. Biofabrication. 2026 Jan 9. doi: 10.1088/1758-5090/ae3644. Online ahead of 
print.

Tumor organoids on-a-chip and the role of AI in predictive oncology and 
personalized cancer medicine.

Mirlohi MS(1), Hamdi E(2), Karimi H(1), Salami S(3), Aref AR(4), Gilzad Kohan 
M(5), Ghayoor A(6), Seyfoori A(7), Akbari M(8).

Author information:
(1)Department of Mechanical Engineering, University of Victoria, 3800 Finnerty 
Road, Rm#533, Victoria, V8P 5C2, CANADA.
(2)Department of Mechanical Engineering, Boston University, Boston University, 
Boston, 02215, UNITED STATES.
(3)True North Veterinary Diagnostics Inc, Langly, Langley, British Columbia, V2Y 
3B3, CANADA.
(4)Mass General Cancer Center, Massachusetts General Hospital, Mass General 
Cancer Center, Boston, 02114, UNITED STATES.
(5)Department of Pharmaceutical and Administrative Sciences, Western New England 
University, Western New England, Springfield, Massachusetts, 01119-2612, UNITED 
STATES.
(6)Department of Electrical and Computer Engineering, The University of Iowa, 
Department of Electrical and Computer Engineering, Iowa City, Iowa, 52242-1002, 
UNITED STATES.
(7)Apricell Biotech, Victoria, Victoria, BC, V8P 2G2, CANADA.
(8)University of Victoria, 3800 Finnerty Road, Rm#533, Victoria, British 
Columbia, V8P 5C2, CANADA.

The drug development process in cancer faces significant challenges due to high 
failure rates in translational studies despite promising in vitro results. 
Additionally, conventional animal models exhibit inherent limitations and 
ethical concerns, constraining their relevance to cancer studies. Recognizing 
the pivotal role of the tumor microenvironment (TME) on cancer development and 
treatment outcome, recent advancements in 3D microfluidic devices and 
tumor-on-a-chip models enabled researchers to explore the TME with enhanced 
accuracy and reliability, yielding novel insights. Notably, the emergence of 
physiological tumor models, particularly 3D models such as organoids derived 
from human tissues, provides a more accurate representation of in vivo tumor 
features. Moreover, 3D tumor models hold promise for diverse applications, 
including highthroughput drug testing, disease modeling, and regenerative 
medicine. Meanwhile, combining artificial intelligence (AI) with patient-derived 
tumor organoids has become a key strategy in predictive oncology and 
personalized cancer treatment. Furthermore, incorporating quantitative systems 
pharmacology (QSP) and physiologically based pharmacokinetic (PBPK) modeling, 
and pharmacokinetics/pharmacodynamics (PK/PD) analysis with generative 
artificial intelligence (Gen-AI) has revolutionized predictive oncology by 
enabling precise simulations of drug interactions and patient-specific 
responses, thereby enhancing the predictive accuracy of personalized cancer 
treatments. These advanced methodologies harness the power of AI algorithms to 
analyze intricate datasets derived from patient-specific tumor organoids. 
Moreover, the predictive modeling capabilities of generative AI facilitate the 
development of personalized treatment strategies customized for each patient, 
thereby revolutionizing oncology practice. This review explores the synergistic 
impact of tumor-on-a-chip models, organoids derived from patient tumors, and 
generative AI. Together, these technologies mark a significant advancement in 
precision medicine, offering promising chances to improve therapeutic 
effectiveness and treatment outcomes in cancer care.

Creative Commons Attribution license.

DOI: 10.1088/1758-5090/ae3644
PMID: 41512327


513. JHEP Rep. 2025 Jul 31;7(11):101539. doi: 10.1016/j.jhepr.2025.101539. 
eCollection 2025 Nov.

Artificial intelligence as a ploy to delve into the intricate link between 
genetics and mitochondria in patients with MASLD.

Longo M(1), Paolini E(1), Meroni M(1), Ripolone M(2), Napoli L(2), Gentile F(3), 
Cespiati A(1)(4), Trombetta E(5), Lombardi R(1)(4), Maggioni M(6), Alisi A(7), 
Miele L(8)(9), Liguori A(8), Soardo G(10), Gasbarrini A(8)(9), Moggio M(2), 
Fracanzani AL(1)(4), Dongiovanni P(1).

Author information:
(1)Medicine and Metabolic Diseases, Fondazione IRCCS Cà Granda Ospedale Maggiore 
Policlinico, Milan, Italy.
(2)Neuromuscular and Rare Diseases Unit, Department of Neuroscience, Fondazione 
IRCCS Ca' Granda Ospedale Maggiore Policlinico, Milan, Italy.
(3)Biology of Myelin Unit, Division of Genetics and Cell Biology, IRCCS San 
Raffaele Scientific Institute, Milan, Italy.
(4)Department of Pathophysiology and Transplantation, Università Degli Studi di 
Milano, Milan, Italy.
(5)Clinical Pathology, Fondazione IRCCS Ca' Granda Ospedale Maggiore 
Policlinico, Milan, Italy.
(6)Division of Pathology, Fondazione IRCCS Ca' Granda Ospedale Maggiore 
Policlinico, Milan, Italy.
(7)Research Unit of Genetics of Complex Phenotypes, "Bambino Gesù" Children's 
Hospital, IRCCS, Rome, Italy.
(8)Department of Translational Medicine and Surgery, Catholic University, 
Fondazione Policlinico Universitario A. Gemelli IRCCS, Rome, Italy.
(9)CEMAD Unit, Digestive Disease Center, Fondazione Policlinico Universitario A. 
Gemelli IRCCS, Rome, Italy.
(10)Department of Medical Area (DAME), University of Udine and Italian Liver 
Foundation, Bldg Q AREA Science Park - Basovizza Campus, Trieste, Italy.

BACKGROUND & AIMS: Mitochondrial (mt-) D-loop and cell-free circulating (ccf-) 
mtDNA fragments, respectively reflecting mt-mass and tissue damage, are 
promising metabolic dysfunction-associated steatotic liver disease (MASLD) 
biomarkers. We previously found that PNPLA3/MBOAT7/TM6SF2 deficiency in HepG2 
cells increased mt-mass, D-loop levels, and ccf-COXIII release. We explored 
mt-biogenesis and mt-biomarkers in patients with MASLD stratified by the number 
of risk variants (NRV = 3). We exploited GPT-4 to develop and validate new risk 
scores, predicting MASLD evolution, in two independent cohorts by integrating 
anthropometric and genetic data with mt-biomarkers.
METHODS: A cohort of 28 patients with MASLD (Discovery cohort) was consecutively 
enrolled for hepatic mt-dynamics assessment by transmission electron microscopy 
and immunohistochemistry. Data were confirmed by quantitative real time-PCR in a 
retrospective cohort (Hepatic Validation, n = 184). D-loop and ccf-COXIII were 
retrospectively measured in peripheral blood mononuclear cells and serum samples 
of biopsied outpatients with MASLD (Serum Validation cohort, n = 824) and 
individuals with non-invasive MASLD diagnosis (n = 386, Non-invasive cohort). 
Risk scores were developed using random forest algorithms.
RESULTS: In the Discovery and Hepatic Validation cohorts, the 
PNPLA3/MBOAT7/TM6SF2 variants altered hepatic mt-dynamics, enhancing mt-content 
and D-loop levels (p <0.05) through the p38/PGC-1α pathway. Furthermore, NRV = 3 
patients showed an increase in mt-fragmentation at transmission electron 
microscopy (TEM) and ccf-COXIII release (p <0.05). In the Serum Validation 
cohort, circulating D-loop and ccf-COXIII positively correlated with genetics [β 
D-loop :0.17 (95% CI: 0.04-0.29), p = 0.01; βccf-COXIII:0.33 (95% CI: 
0.19-0.46), p <0.0001] and MASLD severity [OR D-loop :1.31 (95% CI: 1.01-1.71), 
p = 0.03; ORccf-COXIII:2.41 (95% CI: 1.69-3.44), p <0.0001] at multivariate 
analysis. Random forest allowed prediction models named Mitochondrial, 
Anthropometric, and Genetic Integration with Computational intelligence for 
assessing hepatocellular carcinoma risk (MAGIC-H), considering age, BMI 
genetics, D-loop, and ccf-COXIII. In both Serum and Non-invasive cohorts, the 
MAGIC-H score reached AUC >85% in identifying HCC cases regardless of cirrhosis, 
outperforming existing non-invasive tests.
CONCLUSIONS: Mt-biomarkers have a prognostic significance in 
genetically-predisposed patients with MASLD.
IMPACT AND IMPLICATIONS: The study highlights that genetic variants in PNPLA3, 
MBOAT7, and TM6SF2 genes deeply contribute to metabolic dysfunction-associated 
steatotic liver disease (MASLD) progression by affecting hepatic mitochondrial 
adaptability. It also identified two novel biomarkers of mitochondrial origin 
which are strongly linked to disease severity and genetic background of patients 
with MASLD. The use of generative artificial intelligence tools, such as GPT-4, 
can enhance the use of biomarkers and polygenic risk scores for clinical risk 
stratification. We developed a customized version of GPT-4 (rsGPT-4), which 
identified a machine-learning approach (random forest) as the best method for 
creating prediction models for metabolic dysfunction-associated steatohepatitis, 
fibrosis, and hepatocellular carcinoma. The new scores combined the two 
mitochondrial biomarkers, genetic data, and anthropometric data and outperformed 
existing non-invasive tests for monitoring patients with MASLD.

© 2025 The Author(s).

DOI: 10.1016/j.jhepr.2025.101539
PMCID: PMC12550784
PMID: 41143242

Conflict of interest statement: The authors declare that they have no conflicts 
of interest. Please refer to the accompanying ICMJE disclosure forms for further 
details.


514. Arthroscopy. 2025 Mar;41(3):565-573.e6. doi: 10.1016/j.arthro.2024.10.042. Epub 
2024 Nov 7.

Custom Large Language Models Improve Accuracy: Comparing Retrieval Augmented 
Generation and Artificial Intelligence Agents to Noncustom Models for 
Evidence-Based Medicine.

Woo JJ(1), Yang AJ(1), Olsen RJ(2), Hasan SS(3), Nawabi DH(4), Nwachukwu BU(4), 
Williams RJ 3rd(4), Ramkumar PN(5).

Author information:
(1)Brown University/The Warren Alpert School of Brown University, Providence, 
Rhode Island, U.S.A.
(2)Tufts University School of Medicine, Boston, Massachusetts, U.S.A.
(3)Rush University Medical College, Chicago, Illinois, U.S.A.
(4)Hospital for Special Surgery, New York, NY, U.S.A.
(5)Commons Clinic, Long Beach, California, U.S.A.. Electronic address: 
premramkumar@gmail.com.

PURPOSE: To show the value of custom methods, namely Retrieval Augmented 
Generation (RAG)-based Large Language Models (LLMs) and Agentic Augmentation, 
over standard LLMs in delivering accurate information using an anterior cruciate 
ligament (ACL) injury case.
METHODS: A set of 100 questions and answers based on the 2022 AAOS ACL 
guidelines were curated. Closed-source (open AI GPT4/GPT 3.5 and Anthropic's 
Claude3) and open-source models (LLama3 8b/70b and Mistral 8×7b) were asked 
questions in base form and again with AAOS guidelines embedded into a RAG 
system. The top-performing models were further augmented with artificial 
intelligence (AI) agents and reevaluated. Two fellowship-trained surgeons 
blindly evaluated the accuracy of the responses of each cohort. Recall-Oriented 
Understudy of Gisting Evaluation and Metric for Evaluation of Translation with 
Explicit Ordering scores were calculated to assess semantic similarity in the 
response.
RESULTS: All noncustom LLM models started below 60% accuracy. Applying RAG 
improved the accuracy of every model by an average 39.7%. The highest performing 
model with just RAG was Meta's open-source Llama3 70b (94%). The highest 
performing model with RAG and AI agents was Open AI's GPT4 (95%).
CONCLUSIONS: RAG improved accuracy by an average of 39.7%, with the highest 
accuracy rate of 94% in the Meta Llama3 70b. Incorporating AI agents into a 
previously RAG-augmented LLM improved ChatGPT4 accuracy rate to 95%. Thus, 
Agentic and RAG augmented LLMs can be accurate liaisons of information, 
supporting our hypothesis.
CLINICAL RELEVANCE: Despite literature surrounding the use of LLM in medicine, 
there has been considerable and appropriate skepticism given the variably 
accurate response rates. This study establishes the groundwork to identify 
whether custom modifications to LLMs using RAG and agentic augmentation can 
better deliver accurate information in orthopaedic care. With this knowledge, 
online medical information commonly sought in popular LLMs, such as ChatGPT, can 
be standardized and provide relevant online medical information to better 
support shared decision making between surgeon and patient.

Copyright © 2024 Arthroscopy Association of North America. Published by Elsevier 
Inc. All rights reserved.

DOI: 10.1016/j.arthro.2024.10.042
PMID: 39521391 [Indexed for MEDLINE]

Conflict of interest statement: Disclosures The authors declare the following 
financial interests/personal relationships which may be considered as potential 
competing interests: P.N.R. has received support from Pacira for consulting, 
Stryker for consulting, Globus for consulting and royalties, Intelligent Health 
Analytics, Inc., for ownership and overture for ownership. The remaining authors 
(J.J.W., A.J.Y., R.J.O., S.S.H., D.H.N., B.U.N., R.J.W.) declare that they have 
no known competing financial interests or personal relationships that could have 
appeared to influence the work reported in this article.


515. Digit Health. 2025 Jun 9;11:20552076251348850. doi: 10.1177/20552076251348850. 
eCollection 2025 Jan-Dec.

Answering real-world clinical questions using large language model, 
retrieval-augmented generation, and agentic systems.

Low YS(1), Jackson ML(1), Hyde RJ(1), Brown RE(1), Sanghavi NM(1), Baldwin 
JD(1), Pike CW(1), Muralidharan J(1), Hui G(1)(2), Alexander N(3), Hassan 
H(4)(5), Nene RV(6), Pike M(7), Pokrzywa CJ(8), Vedak S(9), Yan AP(3), Yao 
DH(10), Zipursky AR(3), Dinh C(1), Ballentine P(1), Derieg DC(1), Polony V(1), 
Chawdry RN(1), Davies J(1), Hyde BB(1), Shah NH(1)(9), Gombar S(1)(11).

Author information:
(1)Atropos Health, New York, NY, USA.
(2)Department of Medicine, University of California, Los Angeles, CA, USA.
(3)Department of Pediatrics, The Hospital for Sick Children, Toronto, Ontario, 
Canada.
(4)Division of Hematology/Oncology, The Hospital for Sick Children, Toronto 
Ontario, Canada.
(5)Program in Child Health Evaluative Sciences, Peter Gilgan Centre for Research 
and Learning, The Hospital for Sick Children, Toronto, Ontario, Canada.
(6)Department of Emergency Medicine, University of California, San Diego, CA, 
USA.
(7)Department of Emergency Medicine, University of Michigan, Ann Arbor, MI, USA.
(8)Department of Surgery, Columbia University, New York, NY, USA.
(9)Division of Clinical Informatics, Stanford University, Stanford, CA, USA.
(10)Department of Emergency Medicine, Stanford University, Stanford, CA, USA.
(11)Department of Pathology, Stanford University, Stanford, CA, USA.

OBJECTIVE: The practice of evidence-based medicine can be challenging when 
relevant data are lacking or difficult to contextualize for a specific patient. 
Large language models (LLMs) could potentially address both challenges by 
summarizing published literature or generating new studies using real-world 
data.
MATERIALS AND METHODS: We submitted 50 clinical questions to five LLM-based 
systems: OpenEvidence, which uses an LLM for retrieval-augmented generation 
(RAG); ChatRWD, which uses an LLM as an interface to a data extraction and 
analysis pipeline; and three general-purpose LLMs (ChatGPT-4, Claude 3 Opus, 
Gemini 1.5 Pro). Nine independent physicians evaluated the answers for 
relevance, quality of supporting evidence, and actionability (i.e., sufficient 
to justify or change clinical practice).
RESULTS: General-purpose LLMs rarely produced relevant, evidence-based answers 
(2-10% of questions). In contrast, RAG-based and agentic LLM systems, 
respectively, produced relevant, evidence-based answers for 24% (OpenEvidence) 
to 58% (ChatRWD) of questions. OpenEvidence produced actionable results for 48% 
of questions with existing evidence, compared to 37% for ChatRWD and <5% for the 
general-purpose LLMs. ChatRWD provided actionable results for 52% of questions 
that lacked existing literature compared to <10% for other LLMs.
DISCUSSION: Special-purpose LLM systems greatly outperformed general-purpose 
LLMs in producing answers to clinical questions. Retrieval-augmented 
generation-based LLM (OpenEvidence) performed well when existing data were 
available, while only the agentic ChatRWD was able to provide actionable answers 
when preexisting studies were lacking.
CONCLUSION: Synergistic systems combining RAG-based evidence summarization and 
agentic generation of novel evidence could improve the availability of pertinent 
evidence for patient care.

© The Author(s) 2025.

DOI: 10.1177/20552076251348850
PMCID: PMC12159471
PMID: 40510193

Conflict of interest statement: The authors declared the following potential 
conflicts of interest with respect to the research, authorship, and/or 
publication of this article: ChatRWD, the LLM system evaluated in this study, is 
developed by Atropos Health where many of the authors are employed. NHS is not 
an Atropos Health employee but sits on its board. OpenEvidence, another LLM 
system evaluated here, is provided by OpenEvidence whom we consulted during the 
writing of this manuscript. Non-Atropos employees NA, HH, RVN, MP, CJP, SV, APY, 
D-HY, and ARZ, have nothing to disclose.


516. Semin Nucl Med. 2025 May;55(3):313-327. doi: 10.1053/j.semnuclmed.2025.01.006. 
Epub 2025 Feb 10.

The Evolution of Artificial Intelligence in Nuclear Medicine.

Lopes L(1), Lopez-Montes A(2), Chen Y(3), Koller P(4), Rathod N(2), Blomgren 
A(3), Caobelli F(2), Rominger A(2), Shi K(5), Seifert R(2).

Author information:
(1)Department of Nuclear Medicine, Inselspital, Bern University Hospital, 
University of Bern, Bern, Switzerland; Graduate School for Cellular and 
Biomedical Sciences, University of Bern, Bern, Switzerland. Electronic address: 
leonor.serranolopes@unibe.ch.
(2)Department of Nuclear Medicine, Inselspital, Bern University Hospital, 
University of Bern, Bern, Switzerland.
(3)Department of Nuclear Medicine, Inselspital, Bern University Hospital, 
University of Bern, Bern, Switzerland; Graduate School for Cellular and 
Biomedical Sciences, University of Bern, Bern, Switzerland.
(4)Department of Computer Science, Ludwig-Maximilians-University of Munich, 
Munich, Germany.
(5)Department of Nuclear Medicine, Inselspital, Bern University Hospital, 
University of Bern, Bern, Switzerland; Department of Informatics, Technical 
University of Munich, Munich, Germany.

Nuclear medicine has continuously evolved since its beginnings, constantly 
improving the diagnosis and treatment of various diseases. The integration of 
artificial intelligence (AI) is one of the latest revolutionizing chapters, 
promising significant advancements in diagnosis, prognosis, segmentation, image 
quality enhancement, and theranostics. Early AI applications in nuclear medicine 
focused on improving diagnostic accuracy, leveraging machine learning algorithms 
for disease classification and outcome prediction. Advances in deep learning, 
including convolutional and more recently transformer-based neural networks, 
have further enabled more precise diagnosis and image segmentation as well as 
low-dose imaging, and patient-specific dosimetry for personalized treatment. 
Generative AI, driven by large language models and diffusion techniques, is now 
allowing the process, interpretation, and generation of complex medical language 
and images. Despite these achievements, challenges such as data scarcity, 
heterogeneity, and ethical concerns remain barriers to clinical translation. 
Addressing these issues through interdisciplinary collaboration will pave the 
way for a broader adoption of AI in nuclear medicine, potentially enhancing 
patient care and optimizing diagnosis and therapeutic outcomes.

Copyright © 2025 The Author(s). Published by Elsevier Inc. All rights reserved.

DOI: 10.1053/j.semnuclmed.2025.01.006
PMID: 39934005 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: Robert Seifert reports a 
relationship with Boehringer Ingelheim Fund that includes: funding grants. 
Robert Seifert reports a relationship with Else Kroner-Fresenius Foundation that 
includes: funding grants. Robert Seifert, Kuangyu Shi reports a relationship 
with Novartis AG that includes: funding grants, speaking and lecture fees, and 
travel reimbursement. Robert Seifert, Kuangyu Shi reports a relationship with 
Boston Scientific Corporation that includes: funding grants and speaking and 
lecture fees. Kuangyu Shi, Axel Rominger, Federico Caobelli reports a 
relationship with Siemens Healthineers AG that includes: funding grants. Kuangyu 
Shi reports a relationship with Hermes Medical Solutions AB that includes: 
funding grants. Kuangyu Shi reports a relationship with DOSIsoft that includes: 
funding grants. Kuangyu Shi reports a relationship with Varian Medical Systems 
Inc that includes: funding grants. Kuangyu Shi reports a relationship with THQ 
Medical Products that includes: funding grants. Kuangyu Shi reports a 
relationship with QDose that includes: funding grants. Kuangyu Shi reports a 
relationship with PMOD that includes: funding grants. Kuangyu Shi reports a 
relationship with Sirtex Medical Inc that includes: funding grants. Kuangyu Shi 
reports a relationship with MIM Software Inc that includes: funding grants. 
Kuangyu Shi reports a relationship with ICPO Foundation that includes: funding 
grants. Federico Caobelli reports a relationship with Bracco AG that includes: 
speaking and lecture fees. Federico Caobelli reports a relationship with Pfizer 
AG that includes: speaking and lecture fees. If there are other authors, they 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


517. Diagnostics (Basel). 2025 Dec 19;16(1):3. doi: 10.3390/diagnostics16010003.

Evaluating Medical Text Summaries Using Automatic Evaluation Metrics and 
LLM-as-a-Judge Approach: A Pilot Study.

Vasilev Y(1), Raznitsyna I(1), Pamova A(1)(2), Burtsev T(1), Bobrovskaya T(1), 
Kosov P(1), Vladzymyrskyy A(1)(3), Omelyanskaya O(1)(2), Arzamasov K(1).

Author information:
(1)Research and Practical Clinical Center for Diagnostics and Telemedicine 
Technologies of the Moscow Health Care Department, 127051 Moscow, Russia.
(2)Institute of Artificial Intelligence, MIREA-Russian Technological University, 
119454 Moscow, Russia.
(3)Department of Information Technology and Medical Data Processing, Federal 
State I.M. Sechenov First Moscow State Medical University of the Ministry of 
Health of the Russian Federation (Sechenov University), 119991 Moscow, Russia.

Background: Electronic health records (EHRs) remain a vital source of clinical 
information, yet processing these heterogeneous data is extremely 
labor-intensive. Summarization of these data using Large Language Models (LLMs) 
is considered a promising tool to support practicing physicians. Unbiased, 
automated quality control is crucial for integrating the tools into routine 
practice, saving time and labor. This pilot study aimed to assess the potential 
and constraints of self-contained evaluation of summarization quality (without 
expert involvement) based on automatic evaluation metrics and LLM-as-a-judge. 
Methods: The summaries of text data from 30 EHRs were generated by six 
open-source low-parameter LLMs. The medical summaries were evaluated using 
standard automatic metrics (BLEU, ROUGE, METEOR, BERTScore) as well as the 
LLM-as-a-judge approach using the following criteria: relevance, completeness, 
redundancy, coherence and structure, grammar and terminology, and 
hallucinations. Expert evaluation was conducted using the same criteria. 
Results: The results showed that LLMs hold great promise for summarizing medical 
data. Nevertheless, neither the evaluation metrics nor LLM judges are reliable 
in detecting factual errors and semantic distortions (hallucinations). In terms 
of relevance, the Pearson correlation between the summary quality score and the 
expert opinions was 0.688. Conclusions: Completely automating the evaluation of 
medical summaries remains challenging. Further research should focus on 
dedicated methods for detecting hallucinations, along with investigating larger 
or specialized models trained on medical texts. Additionally, the potential 
integration of retrieval-augmented generation (RAG) within the LLM-as-a-judge 
architecture deserves attention. Nevertheless, even now, the combination of LLMs 
and the automatic evaluation metrics can underpin medical decision support 
systems by performing initial evaluations and highlighting potential 
shortcomings for expert review.

DOI: 10.3390/diagnostics16010003
PMCID: PMC12786185
PMID: 41515497

Conflict of interest statement: The authors declare no conflicts of interest.


518. Nurs Open. 2025 Oct;12(10):e70317. doi: 10.1002/nop2.70317.

Performance of Large Language Models in Nursing Examinations: Comparative 
Analysis of ChatGPT-3.5, ChatGPT-4 and iFLYTEK Spark in China.

Li P(1), Jiang M(2), Chen J(1), Ning N(1).

Author information:
(1)Department of Orthopedic Surgery, West China Hospital, Sichuan 
University/West China School of Nursing, Sichuan University, Chengdu, China.
(2)West China Xiamen Hospital of Sichuan University, Xiamen, China.

BACKGROUND: While large language models (LLMs) have been widely utilised in 
nursing education, their performance in Chinese nursing examinations remains 
unexplored, particularly in the context of ChatGPT-3.5, ChatGPT-4 and iFLYTEK 
Spark.
PURPOSE: This study assessed the performance of ChatGPT-3.5, ChatGPT-4 and 
iFLYTEK Spark on the 2022 China National Nursing Professional Qualification Exam 
(CNNPQE) at both the Junior and Intermediate levels. It also investigated 
whether the accuracy of these language models' responses correlated with the 
exam's difficulty or subject matter.
METHODS: We inputted 800 questions from the 2022 CNNPQE-Junior and 
CNNPQE-Intermediate exams into ChatGPT-3.5, ChatGPT-4 and iFLYTEK Spark to 
determine their accuracy rates in correctly answering the questions. We then 
analysed the correlation between these accuracy rates and the exams' difficulty 
levels or subjects.
RESULTS: The accuracy of ChatGPT-3.5, ChatGPT-4 and iFLYTEK Spark in the 
CNNPQE-Junior was 49.3% (197/400), 68.5% (274/400), and 61% (244/400), 
respectively, whereas it was 56.4% (225/399), 70.7% (282/399) and 57.6% 
(230/399) in the CNNPQE-Intermediate. When considering different grades, the 
differences in accuracy rates among the three models were statistically 
significant (M2 = 95.531, degrees of freedom (df) = 4, p < 0.001). These 
accuracy rates of ChatGPT-4 in the elementary knowledge, relevant professional 
knowledge, professional knowledge, and professional practice ability were 74.5%, 
63.5%, 79% and 62.3%, respectively, leading in accuracy in other subjects in the 
CNNPQE. The results of the Cochran-Mantel-Haenszel (CMH) test showed that when 
considering different subjects, there was a statistically significant difference 
in accuracy rates of three LLMs (M2 = 97.435, df = 4, p < 0.001).
CONCLUSIONS: ChatGPT-4 and iFLYTEK Spark performed well on Chinese nursing 
examinations and demonstrated potential as valuable tools in nursing education.

© 2025 The Author(s). Nursing Open published by John Wiley & Sons Ltd.

DOI: 10.1002/nop2.70317
PMCID: PMC12491847
PMID: 41039969 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


519. BMC Pulm Med. 2025 Nov 22;26(1):8. doi: 10.1186/s12890-025-04041-w.

Evaluation of the effectiveness of the ChatGPT artificial intelligence 
application in the diagnosis of spontaneous pneumothorax on chest radiograph 
interpretation.

Akçay O(1), Özel A(2), Öztürk Ö(3), Acar T(2), Tekneci AK(4), Akçam Tİ(4), 
Gürsoy S(2).

Author information:
(1)Department of Thoracic Surgery, Bakircay University, Cigli Training and 
Research Hospital, Yeni Mahalle, Murat Karayalçin Bulvari No:18, Izmir, 35620, 
Turkey. onur_akcay@yahoo.com.
(2)Department of Thoracic Surgery, Bakircay University, Cigli Training and 
Research Hospital, Yeni Mahalle, Murat Karayalçin Bulvari No:18, Izmir, 35620, 
Turkey.
(3)Department of Thoracic Surgery, Bakircay University, Cigli Training and 
Research Hospital, Yeni Mahalle, Murat Karayalçin Bulvari No:18, Izmir, 35620, 
Turkey. ozgur.ozturk54@hotmail.com.
(4)Department of Thoracic Surgery, Ege University Faculty of Medicine, 
Kazımdirik, Üniversite Cd. No:9, Bornova/İzmir, 35100, Turkey.

BACKGROUND: Spontaneous pneumothorax is a potentially life-threatening condition 
commonly diagnosed using chest radiographs. However, interpreting chest X-rays 
can be challenging due to anatomical overlap and observer variability. This 
study aimed to evaluate the diagnostic accuracy of ChatGPT, a large language 
model (LLM), in detecting pneumothorax on chest radiographs compared to expert 
thoracic surgeons.
METHODS: In this retrospective study, 220 chest radiographs were assessed. 
Expert consensus classified 110 cases with pneumothorax and 110 without. The 
images were uploaded to the GPT-4o model without any clinical information, and 
ChatGPT was asked to identify the presence or absence of pneumothorax. 
Diagnostic performance was evaluated by calculating sensitivity, specificity, 
accuracy, positive and negative predictive values, and area under the receiver 
operating characteristic curve (AUC). Subgroup analyses were performed based on 
pneumothorax size.
RESULTS: ChatGPT demonstrated an overall diagnostic accuracy of 83.7%, 
sensitivity of 70.9%, specificity of 96.4%, positive predictive value of 95.1%, 
and negative predictive value of 76.8%. The AUC was 0.836 (95% CI: 0.780-0.893). 
Diagnostic performance was higher for large pneumothoraces (AUC: 0.894) compared 
to small pneumothoraces (AUC: 0.439). Cohen’s kappa coefficient indicated 
substantial agreement (κ=0.673; 95%CI: 0.575-0.771) with expert evaluations.
CONCLUSIONS: ChatGPT demonstrates potential in detecting pneumothorax on chest 
radiographs, particularly in cases of large pneumothorax. However, its limited 
sensitivity for small pneumothoraces raises significant concerns about its 
reliability in clinical decision-making. Any use of ChatGPT in diagnostic 
workflows should be approached with caution, as unverified outputs may lead to 
inappropriate interventions or under-triaging. Therefore, the model is not 
suitable as a standalone diagnostic or triage tool. Its potential utility may 
lie in exploratory or supervised settings where expert oversight is available, 
but further validation is required before clinical implementation can be 
considered.

DOI: 10.1186/s12890-025-04041-w
PMCID: PMC12777132
PMID: 41275154

Conflict of interest statement: Declarations. Ethical approval and consent to 
participate: This study was approved by the Ethics Committee of the Bakircay 
University Non-Interventional Clinical Research Ethics Committee (Ethics 
Committee Approval No: 2140-19/03/2025). All procedures were conducted in 
accordance with the ethical committee of the institutional and/or national 
research comittee and with 1964 Helsinki Declaration and its later amendments. 
Informed consent to participate was obtained from all participants prior to 
inclusion in the study. Consent for publication: The study used anonymized chest 
radiographs without any personally identifiable information. Therefore, consent 
for publication was not required. Competing interests: The authors declare no 
competing interests.


520. BMC Med Inform Decis Mak. 2025 Jul 18;25(1):271. doi: 
10.1186/s12911-025-03114-4.

Application of ChatGPT-based artificial intelligence in the diagnosis and 
management of polycystic ovary syndrome.

Zhu Y(#)(1)(2), Luo D(#)(3), Shen X(1)(2), Shi Q(1)(2), Lv H(1)(2), Zhang S(4), 
Ye F(5), Kong N(6)(7).

Author information:
(1)Center for Reproductive Medicine and Obstetrics and Gynecology, Nanjing Drum 
Tower Hospital, Affiliated Hospital of Medical School, Nanjing University, 
Nanjing, China.
(2)Center for Molecular Reproductive Medicine, Nanjing University, Nanjing, 
China.
(3)School of Economics and Management, South China Normal University, Guangzhou, 
China.
(4)Guangzhou Shuntu Management Consulting Co., Ltd., Guangzhou, China.
(5)School of Chemistry, South China Normal University, Guangzhou, China.
(6)Center for Reproductive Medicine and Obstetrics and Gynecology, Nanjing Drum 
Tower Hospital, Affiliated Hospital of Medical School, Nanjing University, 
Nanjing, China. xtalkn@163.com.
(7)Center for Molecular Reproductive Medicine, Nanjing University, Nanjing, 
China. xtalkn@163.com.
(#)Contributed equally

This study systematically develops and evaluates the application value of the 
PCOS-GPT system, an artificial intelligence (AI) assistant based on ChatGPT 
technology, in the diagnosis and management of polycystic ovary syndrome (PCOS). 
The research explores innovative pathways for AI-enabled PCOS diagnosis and 
treatment, aiming to provide an adjunctive diagnostic tool for standardized 
clinical decision support. Methods: An evidence-based PCOS knowledge base was 
constructed, covering dimensions such as epidemiology, etiology, clinical 
manifestations, diagnosis, treatment, and prognosis. The PCOS-GPT system was 
developed using GPT-3.5 pretraining combined with fine-tuning on domain-specific 
datasets. Using data from 85 patients, the diagnostic and therapeutic 
performance of PCOS-GPT was evaluated multidimensionally-accuracy, readability, 
and operability-using diagnoses by three expert physicians as the gold standard. 
Compared with GPT-4, PCOS-GPT demonstrated advantages in diagnostic accuracy for 
PCOS (95.63% vs. 96.40%). Conclusion: PCOS-GPT is an intelligent diagnostic and 
therapeutic support tool with advantages in diagnostic accuracy. It holds 
promise for improving standardization in diagnosis and treatment, empowering 
patient self-management, enhancing access to high-quality healthcare resources, 
and offering comprehensive health management for PCOS patients. This innovation 
promotes the development of smart healthcare, benefiting women's health.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-03114-4
PMCID: PMC12275324
PMID: 40682078 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This retrospective study received ethical approval from the ethics 
committee of Nanjing Drum Tower Hospital (No. 2021-384-01). All patient couples 
provided written informed consent. All methods were carried out in accordance 
with relevant guidelines and regulations. Consent for publication: Informed 
consent for publication of data and findings was obtained from all participants, 
ensuring that their confidentiality and privacy were strictly maintained. 
Competing interests: The authors declare no competing interests.


521. NPJ Digit Med. 2025 Nov 5;8(1):640. doi: 10.1038/s41746-025-02005-2.

Evaluating clinical AI summaries with large language models as judges.

Croxford E(1), Gao Y(2), First E(3), Pellegrino N(3), Schnier M(3), Caskey J(4), 
Oguss M(4), Wills G(5), Chen G(1), Dligach D(4)(6), Churpek MM(1)(4)(5), 
Mayampurath A(1)(4), Liao F(5)(7), Goswami C(5), Wong KK(3)(5), Patterson 
BW(5)(7), Afshar M(8)(9).

Author information:
(1)Department of Biostatistics and Medical Informatics, University of Wisconsin, 
Madison, USA.
(2)Department of Biomedical Informatics, University of Colorado - Anschutz 
Medical, Aurora, USA.
(3)Epic Systems, Verona, USA.
(4)Department of Medicine, University of Wisconsin, Madison, USA.
(5)UW Health, Madison, USA.
(6)Department of Computer Science, Loyola University, Chicago, USA.
(7)BerbeeWalsh Department of Emergency Medicine, University of Wisconsin, 
Madison, USA.
(8)Department of Medicine, University of Wisconsin, Madison, USA. 
mafshar@medicine.wisc.edu.
(9)UW Health, Madison, USA. mafshar@medicine.wisc.edu.

Electronic Health Records (EHRs) contain vast clinical data that are difficult 
for providers to synthesize. Generative AI with Large Language Models (LLMs) can 
summarize records to reduce cognitive burden, but ensuring accuracy requires 
reliable evaluation. Human review is the gold standard but is costly and slow. 
To address this, we introduce and validate an automated LLM-based method to 
assess real-world EHR multi-document summaries. Benchmarking against the 
validated Provider Documentation Summarization Quality Instrument (PDSQI), our 
LLM-as-a-Judge framework demonstrated strong inter-rater reliability with human 
evaluators. GPT-o3-mini achieved an intraclass correlation coefficient of 0.818 
(95% CI 0.772-0.854), a median score difference of 0 from humans, and completed 
evaluations in 22 seconds. Overall, reasoning models excelled in inter-rater 
reliability, particularly for evaluations requiring advanced reasoning and 
domain expertise, outperforming non-reasoning, task-trained, and multi-agent 
approaches. By automating high-quality evaluations, a medical LLM-as-a-Judge 
provides a scalable, efficient way to identify accurate, safe AI-generated 
clinical summaries.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-02005-2
PMCID: PMC12589481
PMID: 41193667

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


522. PLoS One. 2025 Jun 2;20(6):e0323265. doi: 10.1371/journal.pone.0323265. 
eCollection 2025.

Early detection of occupational stress: Enhancing workplace safety with machine 
learning and large language models.

Hasan MJ(1), Sultana J(1), Ahmed S(1), Momen S(1).

Author information:
(1)Department of Electrical and Computer Engineering, North South University, 
Dhaka, Bangladesh.

Occupational stress is a major concern for employers and organizations as it 
compromises decision-making and overall safety of workers. Studies indicate that 
work-stress contributes to severe mental strain, increased accident rates, and 
in extreme cases, even suicides. This study aims to enhance early detection of 
occupational stress through machine learning (ML) methods, providing 
stakeholders with better insights into the underlying causes of stress to 
improve occupational safety. Utilizing a newly published workplace survey 
dataset, we developed a novel feature selection pipeline identifying 39 key 
indicators of work-stress. An ensemble of three ML models achieved a 
state-of-the-art accuracy of 90.32%, surpassing existing studies. The 
framework's generalizability was confirmed through a three-step validation 
technique: holdout-validation, 10-fold cross-validation, and external-validation 
with synthetic data generation, achieving an accuracy of 89% on unseen data. We 
also introduced a 1D-CNN to enable hierarchical and temporal learning from the 
data. Additionally, we created an algorithm to convert tabular data into texts 
with 100% information retention, facilitating domain analysis with large 
language models, revealing that occupational stress is more closely related to 
the biomedical domain than clinical or generalist domains. Ablation studies 
reinforced our feature selection pipeline, and revealed sociodemographic 
features as the most important. Explainable AI techniques identified excessive 
workload and ambiguity (27%), poor communication (17%), and a positive work 
environment (16%) as key stress factors. Unlike previous studies relying on 
clinical settings or biomarkers, our approach streamlines stress detection from 
simple survey questions, offering a real-time, deployable tool for periodic 
stress assessment in workplaces.

Copyright: © 2025 Hasan et al. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0323265
PMCID: PMC12129211
PMID: 40455805 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


523. Eur Radiol. 2025 Oct;35(10):6150-6159. doi: 10.1007/s00330-025-11608-y. Epub 
2025 Apr 27.

Comparison of CT referral justification using clinical decision support and 
large language models in a large European cohort.

Saban M(1)(2), Alon Y(3), Luxenburg O(4), Singer C(3), Hierath M(5), Karoussou 
Schreiner A(6), Brkljačić B(7), Sosna J(8).

Author information:
(1)School of Health Sciences, Faculty of Medical and Health Sciences, Tel Aviv 
University, Tel Aviv, Israel. Morsaban1@tauex.tau.ac.il.
(2)The Gertner Institute for Epidemiology and Health Policy Research, Sheba 
Medical Center, Tel Hashomer, Ramat-Gan, Israel. Morsaban1@tauex.tau.ac.il.
(3)School of Health Sciences, Faculty of Medical and Health Sciences, Tel Aviv 
University, Tel Aviv, Israel.
(4)Medical Technology, Health Information and Research Directorate, Ministry of 
Health, Jerusalem, Israel.
(5)European Society of Radiology, Vienna, Austria.
(6)Radiation Protection Department, Health Directorate, Ministry of Health, 
Luxembourg City, Luxembourg.
(7)Department of Radiology, University Hospital Dubrava, University of Zagreb 
School of Medicine, Zagreb, Croatia.
(8)Department of Radiology, Hadassah Medical Center, Faculty of Medicine, Hebrew 
University of Jerusalem, Jerusalem, Israel.

BACKGROUND: Ensuring appropriate use of CT scans is critical for patient safety 
and resource optimization. Decision support tools and artificial intelligence 
(AI), such as large language models (LLMs), have the potential to improve CT 
referral justification, yet require rigorous evaluation against established 
standards and expert assessments.
AIM: To evaluate the performance of LLMs (Generation Pre-trained Transformer 4 
(GPT-4) and Claude-3 Haiku) and independent experts in justifying CT referrals 
compared to the ESR iGuide clinical decision support system as the reference 
standard.
METHODS: CT referral data from 6356 patients were retrospectively analyzed. 
Recommendations were generated by the ESR iGuide, LLMs, and independent experts, 
and evaluated for accuracy, precision, recall, F1 score, and Cohen's kappa 
across medical test, organ, and contrast predictions. Statistical analysis 
included demographic stratification, confidence intervals, and p-values to 
ensure robust comparisons.
RESULTS: Independent experts achieved the highest accuracy (92.4%) for medical 
test justification, surpassing GPT-4 (88.8%) and Claude-3 Haiku (85.2%). For 
organ predictions, LLMs performed comparably to experts, achieving accuracies of 
75.3-77.8% versus 82.6%. For contrast predictions, GPT-4 showed the highest 
accuracy (57.4%) among models, while Claude demonstrated poor agreement with 
guidelines (kappa = 0.006).
CONCLUSION: Independent experts remain the most reliable, but LLMs show 
potential for optimization, particularly in organ prediction. A hybrid human-AI 
approach could enhance CT referral appropriateness and utilization. Further 
research should focus on improving LLM performance and exploring their 
integration into clinical workflows.
KEY POINTS: Question Can GPT-4 and Claude-3 Haiku justify CT referrals as 
accurately as independent experts, using the ESR iGuide as the gold standard? 
Findings Independent experts outperformed large language models in test 
justification. GPT-4 and Claude-3 showed comparable organ prediction but 
struggled with contrast selection, limiting full automation. Clinical relevance 
While independent experts remain most reliable, integrating AI with expert 
oversight may improve CT referral appropriateness, optimizing resource 
allocation and enhancing clinical decision-making.

© 2025. The Author(s).

DOI: 10.1007/s00330-025-11608-y
PMCID: PMC12417242
PMID: 40287868 [Indexed for MEDLINE]

Conflict of interest statement: Compliance with ethical standards. Guarantor: 
The scientific guarantor of this publication is Dr. Mor Saban. Conflict of 
interest: Monika Hierath is an employee of ESR. The remaining authors of this 
manuscript declare no relationships with any companies, whose products or 
services may be related to the subject matter of the article. Statistics and 
biometry: Two of the authors have significant statistical expertise (Dr Mor 
Saban and Dr Clara Singer). Informed consent: Written informed consent was not 
required for this study. Ethical approval: Approval was obtained from the local 
research ethics committees or Institutional Review Boards (IRBs) at each of the 
125 participating imaging facilities across the seven countries. At each site, 
the IRB either approved the study or waived the requirement for informed consent 
due to the retrospective nature and de-identification of the data. Study 
subjects or cohorts overlap: We have published a study aiming to assess CT 
appropriateness rates using the European Society of Radiology (ESR) iGuide 
criteria across seven European countries. Additional objectives were to identify 
factors associated with appropriateness variability and examine recommended 
alternative exams. As part of the European Commission-funded EU-JUST-CT project, 
6734 anonymized CT referrals were audited across 125 centers in Belgium, 
Denmark, Estonia, Finland, Greece, Hungary, and Slovenia. In the current study, 
we used the same cohort and compared that to LLMs. Methodology: Observational 
Performed at 125 institutions, 7 countries


524. BMC Med Educ. 2026 Jan 23. doi: 10.1186/s12909-026-08656-3. Online ahead of 
print.

Consistency over accuracy: run-to-run stability of contemporary large language 
models on Turkish curriculum-aligned theoretical anatomy multiple-choice 
questions.

Gürses ÖA(1), Ceylan İ(2).

Author information:
(1)School of Physical Therapy and Rehabilitation, Department of Physiotherapy 
and Rehabilitation, Kırşehir Ahi Evran University, 40100, Merkez, Kırşehir, 
Türkiye. omeralperengurses@gmail.com.
(2)School of Physical Therapy and Rehabilitation, Department of Physiotherapy 
and Rehabilitation, Kırşehir Ahi Evran University, 40100, Merkez, Kırşehir, 
Türkiye.

BACKGROUND: Stability across repeated administrations is essential for 
educational use of large language models (LLMs), yet it is rarely quantified in 
non-English, curriculum-aligned anatomy contexts.
METHODS: Eleven contemporary LLMs answered 100 Turkish, faculty-authored, 
curriculum-aligned anatomy multiple-choice questions from AYDEP, targeting the 
undergraduate Physiotherapy and Rehabilitation anatomy curriculum in three 
independent runs (≥ 12-hour intervals). Testing used developers' web interfaces 
in Turkey with browsing disabled and default generation settings 
(August-September 2025). Performance was summarized with a stability-aware 0-3 
item score (number of correct responses across three runs) and predefined 
response-consistency classes.
RESULTS: A subset of models achieved near-ceiling totals with high run-to-run 
stability, whereas others showed greater session-to-session variability (i.e., 
changes in the selected option across independent runs initiated as separate 
sessions under identical inputs). Several nominal differences among 
higher-performing systems did not remain significant after multiplicity control. 
Within-family updates produced selective, not universal, gains. Many models 
exhibited medians of 3 (IQR 3-3) on the 0-3 scale (ceiling effects), and lower 
means were accompanied by larger dispersion. Consistency profiles provided 
information beyond mean accuracy by distinguishing reliably correct from 
volatile behavior. In addition, we observed "consistent & wrong" patterns on a 
subset of items, where the same incorrect option was repeatedly selected across 
runs.
CONCLUSION: In Turkish, curriculum-aligned anatomy items, contemporary LLMs can 
be both accurate and stable, but single-trial accuracy can mask volatility and 
stable systematic errors. Adoption decisions should prioritize stability-aware 
appraisal (including consistent-correct and consistent-wrong rates), with local 
validation on institutional item banks and periodic re-evaluation as models 
evolve. Extending this framework to multimodal anatomy and constructed-response 
tasks will further inform trustworthy, learner-facing use.

© 2026. The Author(s).

DOI: 10.1186/s12909-026-08656-3
PMID: 41572253

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. This study did not involve human participants, 
human data, or human tissue. This study is not a clinical trial and therefore 
does not require registration. Consent for publication: Not applicable. 
Competing interests: The authors declare no competing interests.


525. Radiology. 2025 Feb;314(2):e241051. doi: 10.1148/radiol.241051.

Assessing Completeness of Clinical Histories Accompanying Imaging Orders Using 
Adapted Open-Source and Closed-Source Large Language Models.

Larson DB(1)(2), Koirala A(1)(2), Cheuy LY(1)(2), Paschali M(1)(2), Van Veen 
D(3), Na HS(1)(2), Petterson MB(1), Fang Z(1)(2), Chaudhari AS(1)(2)(4).

Author information:
(1)Department of Radiology, Stanford University School of Medicine, 453 Quarry 
Rd, MC 5659, Stanford, CA 94304.
(2)AI Development and Evaluation Laboratory, Stanford University School of 
Medicine, Palo Alto, Calif.
(3)Department of Electrical Engineering, Stanford University, Stanford, Calif.
(4)Department of Biomedical Data Science, Stanford University, Stanford, Calif.

Background Incomplete clinical histories are a well-known problem in radiology. 
Previous dedicated quality improvement efforts focusing on reproducible 
assessments of the completeness of free-text clinical histories have relied on 
tedious manual analysis. Purpose To adapt and evaluate open-source and 
closed-source large language models (LLMs) for their ability to automatically 
extract clinical history elements within imaging orders and to use the 
best-performing adapted open-source model to assess the completeness of a large 
sample of clinical histories as a benchmark for clinical practice. Materials and 
Methods This retrospective single-site study used previously extracted 
information accompanying CT, MRI, US, and radiography orders from August 2020 to 
May 2022 at an adult and pediatric emergency department of a 613-bed tertiary 
academic medical center. Two open-source (Llama 2-7B [Meta], Mistral-7B [Mistral 
AI]) and one closed-source (GPT-4 Turbo [OpenAI]) LLMs were adapted using prompt 
engineering, in-context learning, and fine-tuning (open-source only) to extract 
the elements "past medical history," "what," "when," "where," and "clinical 
concern" from clinical histories. Model performance, interreader agreement using 
Cohen κ (none to slight, 0.01-0.20; fair, 0.21-0.40; moderate, 0.41-0.60; 
substantial, 0.61-0.80; almost perfect, 0.81-1.00), and semantic similarity 
between the models and the adjudicated manual annotations of two board-certified 
radiologists with 16 and 3 years of postfellowship experience, respectively, 
were assessed using accuracy, Cohen κ, and BERTScore, an LLM metric that 
quantifies how well two pieces of text convey the same meaning; 95% CIs were 
also calculated. The best-performing open-source model was then used to assess 
completeness on a large dataset of unannotated clinical histories. Results A 
total of 50 186 clinical histories were included (794 training, 150 validation, 
300 initial testing, 48 942 real-world application). Of the two open-source 
models, Mistral-7B outperformed Llama 2-7B in assessing completeness and was 
further fine-tuned. Both Mistral-7B and GPT-4 Turbo showed substantial overall 
agreement with radiologists (mean κ, 0.73 [95% CI: 0.67, 0.78] to 0.77 [95% CI: 
0.71, 0.82]) and adjudicated annotations (mean BERTScore, 0.96 [95% CI: 0.96, 
0.97] for both models; P = .38). Mistral-7B also rivaled GPT-4 Turbo in 
performance (weighted overall mean accuracy, 91% [95% CI: 89, 93] vs 92% [95% 
CI: 90, 94]; P = .31) despite being a smaller model. Using Mistral-7B, 26.2% 
(12 803 of 48 942) of unannotated clinical histories were found to contain all 
five elements. Conclusion An easily deployable fine-tuned open-source LLM 
(Mistral-7B), rivaling GPT-4 Turbo in performance, could effectively extract 
clinical history elements with substantial agreement with radiologists and 
produce a benchmark for completeness of a large sample of clinical histories. 
The model and code will be fully open-sourced. © RSNA, 2025 Supplemental 
material is available for this article.

DOI: 10.1148/radiol.241051
PMCID: PMC11868845
PMID: 39998369 [Indexed for MEDLINE]

Conflict of interest statement: Disclosures of conflicts of interest: D.B.L. 
Member of the Board of Chancellors of the American College of Radiology and 
Board of Trustees of the American Board of Radiology, shareholder in Bunkerhill 
Health; receives research funding from the Gordon and Betty Moore Foundation. 
A.K. No relevant relationships. L.Y.C. No relevant relationships. M.P. No 
relevant relationships. D.V.V. No relevant relationships. H.S.N. No relevant 
relationships. M.B.P. No relevant relationships. Z.F. No relevant relationships. 
A.S.C. Grants to institution from the NIH (R01HL167974, R01AR077604, 
R01EB002524, R01AR079431, and P41EB027060) and contracts (75N92020C00008 and 
75N92020C00021), ARPA-H, GE HealthCare, and Philips; royalties or licenses from 
LVIS though Stanford; consulting fees from Elucid Bioimaging, Patient Square 
Capital, and Chondrometrics; payment from Genentech for expert testimony; 
patents planned, issued or pending from GE HealthCare through Stanford; 
participation on an Advisory Board for Brain Key and Chondrometrics GmbH; stock 
or stock options in LVIS, Subtle Medical, and Brain Key; Microsoft Azure OpenAI 
credits.


526. Front Digit Health. 2025 Nov 3;7:1653369. doi: 10.3389/fdgth.2025.1653369. 
eCollection 2025.

Generative AI in clinical (2020-2025): a mini-review of applications, emerging 
trends, and clinical challenges.

Fahad N(1), Rabbi RI(2), Benta Hasan S(2), Sultana Prity F(3), Ahmed R(4), Ahmed 
F(2), Hossen MJ(5), Liew TH(6), Sayeed MS(6), Ong Michael Goh K(7).

Author information:
(1)Faculty of Information Science and Technology (FIST), Multimedia University, 
Melaka, Malaysia.
(2)Faculty of Engineering and Technology, Multimedia University, Melaka, 
Malaysia.
(3)Department of Computer Science, American International University-Bangladesh, 
Dhaka, Bangladesh.
(4)Faculty of Science and Technology, American International 
University-Bangladesh, Dhaka, Bangladesh.
(5)Center for Advanced Analytics (CAA), COE for Artificial Intelligence, Faculty 
of Engineering & Technology (FET), Multimedia University, Melaka, Malaysia.
(6)Centre for Intelligent Cloud Computing (CICC), COE of Advanced Cloud, Faculty 
of Information Science & Technology, Multimedia University, Melaka, Malaysia.
(7)Center for Image and Vision Computing, COE for Artificial Intelligence, 
Faculty of Information Science and Technology, Multimedia University, Melaka, 
Malaysia.

Generative artificial intelligence (G-AI) has moved from proof-of-concept 
demonstrations to practical tools that augment radiology, dermatology, genetics, 
drug discovery, and electronic-health-record analysis. This mini-review 
synthesizes fifteen studies published between 2020 and 2025 that collectively 
illustrate three dominant trends: data augmentation for imbalanced or 
privacy-restricted datasets, automation of expert-intensive tasks such as 
radiology reporting, and generation of new biomedical knowledge ranging from 
molecular scaffolds to fairness insights. Image-centric work still dominates, 
with GANs, diffusion models, and Vision-Language Models expanding limited 
datasets and accelerating diagnosis. Yet narrative (EHR) and molecular design 
domains are rapidly catching up. Despite demonstrated accuracy gains, recurring 
challenges persist: synthetic samples may overlook rare pathologies, large 
multimodal systems can hallucinate clinical facts, and demographic biases can be 
amplified. Robust validation, interpretability techniques, and governance 
frameworks therefore, remain essential before G-AI can be safely embedded in 
routine care.

© 2025 Fahad, Rabbi, Benta Hasan, Sultana Prity, Ahmed, Ahmed, Hossen, Liew, 
Sayeed and Ong Michael Goh.

DOI: 10.3389/fdgth.2025.1653369
PMCID: PMC12620437
PMID: 41257033

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


527. Digit Health. 2025 Oct 17;11:20552076251390447. doi: 10.1177/20552076251390447. 
eCollection 2025 Jan-Dec.

HealthBench: Advancing AI evaluation in healthcare, but not yet clinically 
ready.

Liu J(1)(2), Liu S(3).

Author information:
(1)Department of Otolaryngology-Head and Neck Surgery, West China Hospital, 
Sichuan University, Chengdu, China.
(2)Department of Medical Informatics, West China Hospital, Sichuan University, 
Chengdu, China.
(3)Department of Biomedical Informatics, Vanderbilt University Medical Center, 
Nashville, TN, USA.

Large language models (LLMs) are increasingly applied in healthcare, yet their 
evaluation remains inconsistent and often disconnected from clinical practice. 
OpenAI's HealthBench represents an important advancement, encompassing 5000 
multiturn synthetic clinical conversations benchmarked against 48,562 
clinician-developed criteria across accuracy, completeness, context awareness, 
communication, and instruction-following. Key strengths include broad scenario 
coverage, contributions from 262 clinicians across 60 countries, and automated 
grading methods that show high concordance with physician ratings. HealthBench 
provides a scalable and globally relevant framework. Nevertheless, important 
limitations constrain its clinical applicability. Exclusive reliance on 
synthetic dialogs limits ecological validity, and model-based graders may 
reinforce shared blind spots. Moreover, HealthBench assesses static, offline 
interactions while omitting multimodal inputs, longitudinal care, and patient 
outcomes-factors critical to real-world decision-making. Without external 
validation, strong benchmark performance may not translate into improved 
diagnostic accuracy, workflow efficiency, or patient safety. To ensure safe and 
effective integration of LLMs into practice, future benchmarks must incorporate 
authentic clinical data, longitudinal outcomes, and system-level considerations. 
HealthBench is a valuable step, but evaluation strategies must evolve to capture 
the complexity and demands of frontline care.

© The Author(s) 2025.

DOI: 10.1177/20552076251390447
PMCID: PMC12547120
PMID: 41143093

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


528. NPJ Digit Med. 2025 Nov 28;8(1):798. doi: 10.1038/s41746-025-02170-4.

Enhancing clinical documentation with voice processing and large language 
models: a study on the LAOS system.

Xu Y(#)(1), Jia H(#)(1), Wang M(#)(2), Feng J(#)(3), Xu X(1), Wang H(1), Chen 
J(1), Zheng Z(1), Yang X(3), Shen Y(3), Wang J(3), Zhuang C(3), Wei P(3), Guo 
R(2), Zhao X(2), Fan J(4), Sun X(5).

Author information:
(1)Department of Ophthalmology, Shanghai General Hospital, Shanghai Jiao Tong 
University School of Medicine, National Clinical Research Center for Eye 
Diseases, Shanghai Key Laboratory of Fundus Diseases, Shanghai Engineering 
Center for Visual Science and Photomedicine, Shanghai Gene Therapy Center, 
Shanghai, China.
(2)City University of Hong Kong, Hong Kong, China.
(3)Ant Group, Hangzhou, China.
(4)Department of Information, Shanghai General Hospital, Shanghai Jiao Tong 
University School of Medicine, Shanghai, China. junxiang.fan@shgh.cn.
(5)Department of Ophthalmology, Shanghai General Hospital, Shanghai Jiao Tong 
University School of Medicine, National Clinical Research Center for Eye 
Diseases, Shanghai Key Laboratory of Fundus Diseases, Shanghai Engineering 
Center for Visual Science and Photomedicine, Shanghai Gene Therapy Center, 
Shanghai, China. xdsun@sjtu.edu.cn.
(#)Contributed equally

The growing volume of Electronic Health Records (EHRs) has enhanced patient care 
quality but significantly increased the cognitive workload on clinicians, 
particularly in ophthalmology where specialists handle 1.6 times more patient 
consultations than other specialties. This study introduces the "LLM-based 
Auxiliary Ophthalmic System (LAOS)," an integrated framework leveraging Large 
Language Models (LLMs) and audio processing to improve clinical documentation 
accuracy and efficiency. LAOS combines voice recognition with 
Retrieval-Augmented Generation (RAG) and Low-Rank Adaptation (LoRA) to convert 
clinical conversations into structured documentation while dynamically 
retrieving relevant medical knowledge. The system was evaluated across three 
critical documentation tasks: Admission Reports, Surgery Records, and Discharge 
Summaries. Through both quantitative metrics (BLEU, ROUGE-L, BERT Score) and 
clinical validation by board-certified physicians, LAOS demonstrated significant 
improvements in documentation completeness, accuracy, and efficiency. While 
challenges remain in balancing comprehensiveness with conciseness, this research 
highlights the potential of speech-enabled LLM systems to alleviate physician 
burnout, enhance documentation quality, and improve healthcare delivery.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-02170-4
PMCID: PMC12753627
PMID: 41315671

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


529. J Palliat Med. 2025 Aug;28(8):1102-1107. doi: 10.1089/jpm.2024.0526. Epub 2025 
Mar 26.

Performance of Three Conversational Artificial Intelligence Agents in Defining 
End-of-Life Care Terms.

Admane S(1), Kim MJ(1), Reddy A(1), Tang M(1), Chang YK(1), Shih KK(2), De La 
Cruz M(1), Cepeda SJ(1), Bruera E(1), Hui D(1).

Author information:
(1)Department of Palliative, Rehabilitation, and Integrative Medicine, The 
University of Texas MD Anderson Cancer, Houston, Texas, USA.
(2)Division of Geriatrics and Palliative Medicine, Baylor College of Medicine, 
Houston, Texas, USA.

Background: Conversational artificial intelligence agents, or chatbots, are a 
transformational technology understudied in end-of-life care. Methods: OpenAI's 
ChatGPT, Google's Bard, and Microsoft's Bing were asked to define "terminally 
ill," "end of life," "transitions of care," "actively dying," and provide three 
references. Outputs were scored by six physicians on a scale of 0-10 for 
accuracy, comprehensiveness, and credibility. Flesch-Kincaid Grade Level and 
Flesch Reading Ease (FRE) were used to calculate readability. Results: Mean 
(standard deviation) scores for accuracy were 9 (1.9) for ChatGPT, 7.5 (2.4) for 
Bard, and 8.3 (2.4) for Bing. Comprehensiveness scores averaged 8.5 (1.7) for 
ChatGPT, 7.3 (2.1) for Bard, and 6.5 (2.3) for Bing. Credibility was low with a 
mean score of 3 (1.8). The mean FRE score was 41.7, and the mean grade level was 
14.1, indicating low readability. Conclusion: Chatbot outputs had important 
deficiencies that necessitated clinician oversight to prevent misinformation.

DOI: 10.1089/jpm.2024.0526
PMID: 40138176 [Indexed for MEDLINE]


530. J Clin Nurs. 2025 Jul;34(7):2898-2907. doi: 10.1111/jocn.17384. Epub 2024 Aug 5.

Augmenting intensive care unit nursing practice with generative AI: A formative 
study of diagnostic synergies using simulation-based clinical cases.

Levin C(1)(2), Suliman M(3), Naimi E(4), Saban M(4).

Author information:
(1)Nursing Department, Faculty of School of Life and Health Sciences, The 
Jerusalem College of Technology-lev Academic Center, Jerusalem, Israel.
(2)Department of Vascular Surgery, The Chaim Sheba Medical Center, Ramat Gan, 
Tel Aviv, Israel.
(3)Intensive Care Unit, The Chaim Sheba Medical Center, Ramat Gan, Tel Aviv, 
Israel.
(4)Department of Nursing, School of Health Professions, Faculty of Medical and 
Health Sciences, Tel Aviv University, Tel Aviv, Israel.

BACKGROUND: As generative artificial intelligence (GenAI) tools continue 
advancing, rigorous evaluations are needed to understand their capabilities 
relative to experienced clinicians and nurses. The aim of this study was to 
objectively compare the diagnostic accuracy and response formats of ICU nurses 
versus various GenAI models, with a qualitative interpretation of the 
quantitative results.
METHODS: This formative study utilized four written clinical scenarios 
representative of real ICU patient cases to simulate diagnostic challenges. The 
scenarios were developed by expert nurses and underwent validation against 
current literature. Seventy-four ICU nurses participated in a simulation-based 
assessment involving four written clinical scenarios. Simultaneously, we asked 
ChatGPT-4 and Claude-2.0 to provide initial assessments and treatment 
recommendations for the same scenarios. The responses from ChatGPT-4 and 
Claude-2.0 were then scored by certified ICU nurses for accuracy, completeness 
and response.
RESULTS: Nurses consistently achieved higher diagnostic accuracy than AI across 
open-ended scenarios, though certain models matched or exceeded human 
performance on standardized cases. Reaction times also diverged substantially. 
Qualitative response format differences emerged such as concision versus 
verbosity. Variations in GenAI models system performance across cases 
highlighted generalizability challenges.
CONCLUSIONS: While GenAI demonstrated valuable skills, experienced nurses 
outperformed in open-ended domains requiring holistic judgement. Continued 
development to strengthen generalized decision-making abilities is warranted 
before autonomous clinical integration. Response format interfaces should 
consider leveraging distinct strengths. Rigorous mixed methods research 
involving diverse stakeholders can help iteratively inform safe, beneficial 
human-GenAI partnerships centred on experience-guided care augmentation.
RELEVANCE TO CLINICAL PRACTICE: This mixed-methods simulation study provides 
formative insights into optimizing collaborative models of GenAI and nursing 
knowledge to support patient assessment and decision-making in intensive care. 
The findings can help guide development of explainable GenAI decision support 
tailored for critical care environments.
PATIENT OR PUBLIC CONTRIBUTION: Patients or public were not involved in the 
design and implementation of the study or the analysis and interpretation of the 
data.

© 2024 The Author(s). Journal of Clinical Nursing published by John Wiley & Sons 
Ltd.

DOI: 10.1111/jocn.17384
PMCID: PMC12181158
PMID: 39101368 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that they have no known 
competing financial interests or personal relationships that could have appeared 
to influence the work reported in this paper.


531. JMIR Med Inform. 2025 Oct 7;13:e77837. doi: 10.2196/77837.

Large Language Model-Enhanced Drug Repositioning Knowledge Extraction via Long 
Chain-of-Thought: Development and Evaluation Study.

Kang H(1)(2), Li J(2), Hou L(2), Xu X(2), Zheng S(2), Li Q(1).

Author information:
(1)School of Medical Technology, Beijing Institute of Technology, No. 5 
Zhongguancun South Street, Haidian District, Beijing, 100081, China, 86 
13693067129.
(2)Medical Information Innovation Research Center, Institute of Medical 
Information, Chinese Academy of Medical Sciences and Peking Union Medical 
College, Beijing, China.

BACKGROUND: Drug repositioning is a pivotal strategy in pharmaceutical research, 
offering accelerated and cost-effective therapeutic discovery. However, 
biomedical information relevant to drug repositioning is often complex, 
dispersed, and underutilized due to limitations in traditional extraction 
methods, such as reliance on annotated data and poor generalizability. Large 
language models (LLMs) show promise but face challenges such as hallucinations 
and interpretability issues.
OBJECTIVE: This study proposed long chain-of-thought for drug repositioning 
knowledge extraction (LCoDR-KE), a lightweight and domain-specific framework to 
enhance LLMs' accuracy and adaptability in extracting structured biomedical 
knowledge for drug repositioning.
METHODS: A domain-specific schema defined 11 entities (eg, drug, disease) and 18 
relationships (eg, treats, is biomarker of). Following the established schema 
architecture, we constructed automatic annotation based on 10,000 PubMed 
abstracts via chain-of-thought prompt engineering. A total of 1000 
expert-validated abstracts were curated into a drug repositioning corpus, a 
high-quality specialized corpus, while the remaining entries were allocated for 
model training purposes. Then, the proposed LCoDR-KE framework combined 
supervised fine-tuning of the Qwen2.5-7B-Instruct model with reinforcement 
learning and dual-reward mechanisms. Performance was evaluated against 
state-of-the-art models (eg, conditional random fields, Bidirectional Encoder 
Representations From Transformers, BioBERT, Qwen2.5, DeepSeek-R1, 
OpenBioLLM-70B, and model variants) using precision, recall, and F1-score. In 
addition, the convergence of the training method was assessed by analyzing 
performance progression across iteration steps.
RESULTS: LCoDR-KE achieved an entity F1 of 81.46% (eg, drug 95.83%, disease 
90.52%) and triplet F1 of 69.04%, outperforming traditional models and rivaling 
larger LLMs (DeepSeek-R1: entity F1=84.64%, triplet F1=69.02%). Ablation studies 
confirmed the contributions of supervised fine-tuning (8.61% and 20.70% F1 drop 
if removed) and reinforcement learning (6.09% and 14.09% F1 drop if removed). 
The training process demonstrated stable convergence, validated through 
iterative performance monitoring. Qualitative analysis of the model's 
chain-of-thought outputs showed that LCoDR-KE performed structured and 
schema-aware reasoning by validating entity types, rejecting incompatible 
relations, enforcing constraints, and generating compliant JSON. Error analysis 
revealed 4 main types of mistakes and challenges for further improvement.
CONCLUSIONS: LCoDR-KE enhances LLMs' domain-specific adaptability for drug 
repositioning by offering an open-source drug repositioning corpus and a long 
chain-of-thought framework based on a lightweight LLM model. This framework 
supports drug discovery and knowledge reasoning while providing scalable, 
interpretable solutions applicable to broader biomedical knowledge extraction 
tasks.

© Hongyu Kang, Jiao Li, Li Hou, Xiaowei Xu, Si Zheng, Qin Li. Originally 
published in JMIR Medical Informatics (https://medinform.jmir.org).

DOI: 10.2196/77837
PMCID: PMC12503436
PMID: 41056561 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


532. NPJ Digit Med. 2025 Dec 13;8(1):773. doi: 10.1038/s41746-025-02139-3.

Enhancing privacy-preserving deployable large language models for perioperative 
complication detection: a targeted strategy with LoRA fine-tuning.

Gao S(#)(1), Zhao X(#)(2), Chen L(#)(3), Yu J(4), Tian S(3), Zhou H(5), Chen 
J(2), Long S(4), He Q(6), Feng X(7).

Author information:
(1)Department of Anesthesiology, First Affiliated Hospital of Sun Yat-sen 
University, Guangzhou, China. gaoshw5@mail.sysu.edu.cn.
(2)Department of Anesthesiology, First Affiliated Hospital of Sun Yat-sen 
University, Guangzhou, China.
(3)Department of Anesthesiology, Jiangmen Central Hospital, Jiangmen, China.
(4)Information and Data Center, First Affiliated Hospital of Sun Yat-sen 
University, Guangzhou, China.
(5)Department of Medical Oncology, Sun Yat-sen University Cancer Center, 
Guangzhou, China.
(6)Department of Anesthesiology, First Affiliated Hospital of Sun Yat-sen 
University, Guangzhou, China. heqiulan@mail.sysu.edu.cn.
(7)Department of Anesthesiology, First Affiliated Hospital of Sun Yat-sen 
University, Guangzhou, China. fengxia@mail.sysu.edu.cn.
(#)Contributed equally

Perioperative complications are a major global concern, yet manual detection 
suffers from 27% under‑reporting and frequent misclassification. Clinical LLM 
deployment is constrained by data sovereignty, compute cost, and limited locally 
deployable model performance. We show targeted prompt engineering plus Low‑Rank 
Adaptation (LoRA) fine‑tuning converts smaller open‑source LLMs into 
expert‑level diagnostic tools. In dual‑center validation, we built a framework 
simultaneously identifying and grading 22 complication severities. 
State‑of‑the‑art models outperformed human experts; Chain‑of‑Thought prompting 
significantly improved general models (p < 0.001) while preserving reasoning 
models' performance. Across documentation length quartiles, AI models maintained 
F1 > 0.64, whereas human performance declined from 0.73 to 0.45, demonstrating 
superior robustness to documentation complexity. Our targeted 
strategy-decomposing detection into focused single‑complication 
assessments-improved small models, with further gains from LoRA. On external 
validation (Center 2), the optimized 4B model's micro‑F1 rose from 0.28 to 0.64, 
approaching human experts (F1 = 0.69), driven by the targeted strategy 
(ΔF1 = 0.256, 95% CI [0.181, 0.336]) and LoRA (ΔF1 = 0.103, 95% CI [0.023, 
0.186]). Concurrently, the 8B model surpassed human experts (F1 > 0.70). 
Optimized small models enable expert‑level accuracy with local deployment and 
preserved data sovereignty, offering a practical path for resource‑limited 
healthcare.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-02139-3
PMCID: PMC12717251
PMID: 41390570

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


533. JMIR Form Res. 2025 Jul 31;9:e71966. doi: 10.2196/71966.

Use of a Medical Communication Framework to Assess the Quality of Generative 
Artificial Intelligence Replies to Primary Care Patient Portal Messages: Content 
Analysis.

Lee NS(1), Richards N(1), Grandominico J(1), Cronin RM(1), Hendricks AK(2), 
Tripathi RS(2), Jonas DE(1).

Author information:
(1)Department of Internal Medicine, College of Medicine, The Ohio State 
University, 2050 Kenny Rd, suite 2400, Columbus, OH, 43221, United States, 1 
614-814-1361.
(2)Health System Informatics, The Ohio State University Wexner Medical Center, 
Columbus, OH, United States.

BACKGROUND: There is growing interest in applying generative artificial 
intelligence (GenAI) to respond to electronic patient portal messages, 
particularly in primary care where message volumes are highest. However, 
evaluations of GenAI as an inbox communication tool are limited. Qualitative 
analysis of when and how often GenAI responses achieve communication goals can 
inform estimates of impact and guide continuous improvement.
OBJECTIVE: This study aims to evaluate GenAI responses to primary care messages 
using a medical communication framework.
METHODS: This was a descriptive quality improvement study of 201 GenAI replies 
to a purposively sampled, diverse pool of real primary care patient messages in 
a large midwestern academic medical center. Two physician reviewers (NSL and NR) 
used a hybrid deductive-inductive approach to qualitatively identify and define 
themes, guided by constructs from the "best practice" medical communication 
framework. After achieving thematic saturation, the reviewers assessed the 
presence or absence of identified communication themes, both independently and 
collaboratively. Discrepant observations were reconciled via discussion. 
Frequencies of identified themes were tallied.
RESULTS: Themes in strengths and limitations emerged across 5 communication 
domains. In the domain of rapport building, expressing respect and restating key 
phrases were strengths, while inappropriate or inadequate rapport building 
statements were limitations. For information gathering, questions that built 
toward a plan or elicited patient needs were strengths, while questions that 
were out of place or redundant were limitations. For information delivery, 
accurate content delivered clearly and professionally was a strength, but 
delivery of inaccurate content was an observed limitation. GenAI responses could 
facilitate next steps by outlining choices or providing instruction, but 
sometimes those next steps were inappropriate or premature. Finally, in 
responding to emotion, strengths were that emotions were named and validated, 
while inadequate or absent acknowledgment of emotion was a limitation. Overall, 
26.4% (53/201) of all messages displayed communication strengths without 
limitations, 27.4% (55/201) had limitations without strengths, and the remaining 
46.3% (93/201) had both. Strengths outnumbered limitations in rapport building 
(87/201, 43.3% vs 35/201, 17.4%) and facilitating next steps (73/201, 36.3% vs 
39/201, 19.4%). Limitations outnumbered strengths in the remaining domains of 
information delivery (89/201, 44.3% vs 43/201, 21.4%), information gathering 
(60/201, 29.9% vs 43/201, 21.4%), and responding to emotion (7/201, 8.5% vs 
9/201, 4.5%).
CONCLUSIONS: GenAI response quality on behalf of primary care physicians and 
advanced practice providers may vary by communication function. Expressions of 
respect or descriptions of common next steps may be appropriate, but gathering 
and delivering appropriate information, or responding to emotion, may be 
limited. While communication standards were often met, they were also often 
compromised. Understanding these strengths and limitations can inform decisions 
about whether, when, and how to apply GenAI as a tool for primary care inbox 
communication.

© Natalie S Lee, Nathan Richards, Jodi Grandominico, Robert M Cronin, Amanda K 
Hendricks, Ravi S Tripathi, Daniel E Jonas. Originally published in JMIR 
Formative Research (https://formative.jmir.org).

DOI: 10.2196/71966
PMCID: PMC12313158
PMID: 40743559 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


534. JMIR Form Res. 2026 Jan 13;10:e78401. doi: 10.2196/78401.

Evaluating the Efficacy of AI-Based Interactive Assessments Using Large Language 
Models for Depression Screening: Development and Usability Study.

Jin Z(1), Hu J(2), Bi D(1), Zhao K(1)(3), Yu H(1).

Author information:
(1)International Joint Laboratory of Behavior and Cognitive Science, Zhengzhou 
Normal University, Zhengzhou, Henan, China.
(2)School of Electrical, Computer and Telecommunications Engineering, University 
of Wollongong, Wollongong, Australia.
(3)Department of Psychology, Zhejiang Normal University, Jinhua, Zhejiang, 
China.

BACKGROUND: The evolution of language models, particularly large language 
models, has introduced transformative potential for psychological assessment, 
challenging traditional rating scale methods that have dominated clinical 
practice for over a century.
OBJECTIVE: This study aimed to develop and validate an automated assessment 
paradigm that integrates natural language processing with conventional 
measurement tools to assess depressive symptoms, exploring its feasibility as a 
novel approach in psychological evaluation.
METHODS: A cohort of 115 participants, including 28 (24.3%) individuals 
diagnosed with depression, completed the Beck Depression Inventory Fast Screen 
via a custom ChatGPT interface (BDI-FS-GPT) and the Chinese version of the 
Patient Health Questionnaire-9 (PHQ-9). Statistical analyses included the 
Spearman correlation (PHQ-9 vs BDI-FS-GPT scores), Cohen κ (diagnostic 
agreement), and area under the curve (AUC) evaluation.
RESULTS: Spearman analysis revealed a moderate correlation between PHQ-9 and 
BDI-FS-GPT scores. The Cohen κ indicated moderate diagnostic agreement between 
the PHQ-9 and the BDI-FS-GPT (κ=0.43; 76.5% agreement), substantial agreement 
between the BDI-FS-GPT and the clinical diagnosis (κ=0.72; 88.7% agreement), and 
moderate agreement between the PHQ-9 and the clinical diagnosis (κ=0.55; 71.4% 
agreement). The BDI-FS-GPT demonstrated excellent diagnostic accuracy 
(AUC=0.953) at a cutoff of 3, detecting 89.3% of participants with depression 
with an 11.5% false-positive rate compared to the PHQ-9 (AUC=0.859) at a cutoff 
of 5 (sensitivity=71.4%; false-positive rate=13.8%). Participants also reported 
significantly higher satisfaction with the automated assessment compared to the 
traditional scale (P=.02).
CONCLUSIONS: The automated assessment paradigm framework combines the 
interactivity and personalization of natural language processing-powered tools 
with the psychometric rigor of traditional scales, suggesting a preliminary 
feasibility paradigm for future psychological assessment. Its ability to enhance 
engagement while maintaining reliability and validity provides encouraging 
evidence, warranting validation in larger and more diverse studies as large 
language model technology advances.
INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): 
RR2-10.1101/2024.07.19.24310543.

©Zheng Jin, Jiaxing Hu, Dandan Bi, Kaibin Zhao, Huan Yu. Originally published in 
JMIR Formative Research (https://formative.jmir.org), 13.01.2026.

DOI: 10.2196/78401
PMID: 41529832 [Indexed for MEDLINE]


535. Lancet Digit Health. 2025 Jul;7(7):100864. doi: 10.1016/j.landig.2025.02.004. 
Epub 2025 Jun 14.

Medical digital twins: enabling precision medicine and medical artificial 
intelligence.

Sadée C(1), Testa S(2), Barba T(3), Hartmann K(4), Schuessler M(5), Thieme A(6), 
Church GM(7), Okoye I(8), Hernandez-Boussard T(1), Hood L(9), Shmulevich I(9), 
Kuhl E(10), Gevaert O(11).

Author information:
(1)Stanford Center for Biomedical Informatics Research, Department of Medicine, 
Stanford, CA, USA.
(2)Division of Medical Oncology, Department of Medicine, Beth Israel Deaconess 
Medical Center, Boston, MA, USA; Department of Medicine, Stanford University, 
Stanford, CA, USA.
(3)Stanford Center for Biomedical Informatics Research, Department of Medicine, 
Stanford, CA, USA; Department of Internal Medicine, Édouard Herriot Hospital, 
Lyon, France.
(4)Department of Radiology, Hospital of the University of Pennsylvania, 
Philadelphia, PA, USA.
(5)Department of Biomedical Data Science, Stanford University, Stanford, CA, 
USA.
(6)Stanford Center for Biomedical Informatics Research, Department of Medicine, 
Stanford, CA, USA; Department of Radiation Oncology, Charité-Universitätsmedizin 
Berlin, Berlin, Germany.
(7)Department of Molecular and Cellular Biology, Harvard University, Cambridge, 
MA, USA.
(8)University of Nigeria Teaching Hospital, Ituku-Ozalla, Enugu, Nigeria; 
University of Nigeria, Nsukka Center of Excellence for Clinical Trials, Enugu, 
Nigeria.
(9)Institute for Systems Biology, Seattle, WA, USA.
(10)Departments of Mechanical Engineering and Bioengineering, Stanford 
University, Stanford, CA, USA.
(11)Stanford Center for Biomedical Informatics Research, Department of Medicine, 
Stanford, CA, USA; Department of Biomedical Data Science, Stanford University, 
Stanford, CA, USA. Electronic address: ogevaert@stanford.edu.

The notion of medical digital twins is gaining popularity both within the 
scientific community and among the general public; however, much of the recent 
enthusiasm has occurred in the absence of a consensus on their fundamental 
make-up. Digital twins originate in the field of engineering, in which a 
constantly updating virtual copy enables analysis, simulation, and prediction of 
a real-world object or process. In this Health Policy paper, we evaluate this 
concept in the context of medicine and outline five key components of the 
medical digital twin: the patient, data connection, patient-in-silico, 
interface, and twin synchronisation. We consider how various enabling 
technologies in multimodal data, artificial intelligence, and mechanistic 
modelling will pave the way for clinical adoption and provide examples 
pertaining to oncology and diabetes. We highlight the role of data fusion and 
the potential of merging artificial intelligence and mechanistic modelling to 
address the limitations of either the AI or the mechanistic modelling approach 
used independently. In particular, we highlight how the digital twin concept can 
support the performance of large language models applied in medicine and its 
potential to address health-care challenges. We believe that this Health Policy 
paper will help to guide scientists, clinicians, and policy makers in creating 
medical digital twins in the future and translating this promising new paradigm 
from theory into clinical practice.

Copyright © 2025 The Author(s). Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.landig.2025.02.004
PMCID: PMC12412312
PMID: 40518342 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of interests LH is the CEO of 
Phenome Health, dedicated to the creation of a second genome-like programme, 
aiming to link 1 million genomes and phenomes over the next 10 years. In this 
context, the latest computational approaches will be used, including the medical 
digital twin paradigm. IO is the founder of the Breast Without Spot initiative, 
dedicated to the early detection of breast and prostate cancers in Nigeria. 
Additionally, she serves on the board of Uburu, a company that aims to bridge 
the gap in biomedical research and industry by facilitating access to African 
medical data and expertise. OG is involved in unrelated research funded by the 
National Cancer Institute; AstraZeneca; US Food and Drug Administration; 
National Center for Artificial Intelligence, Saudi Arabia; Owkin; Onc.AI; Union 
Chimique Belge; and Roche Molecular Systems. In addition, OG holds patents for 
Learning Gene Regulatory Networks Using Sparse Gaussian Mixture Models (patent 
S21-177, 11/22/2022), RNA to Image Synthetic Data Generator (provisional patent 
S22-425, 12/13/2022), and Explainable Computational Methods for Predicting 
Treatment Response to Immunotherapy from Histology Images of Non-Small-Cell Lung 
Cancer (provisional patent S24-079, 04/18/2024). GMC holds advisory roles in 
companies focusing on wearable health technology (LogicInk), liquid biopsy 
(4baseCare, Invitae, and Clinomics), and fourth-generation sequencing 
(INanoBio). An overview of all the advisory roles held by GMC is given at 
https://arep.med.harvard.edu/gmc/tech.html. CS and KH are married but maintain 
independent research careers at different research institutions and contributed 
separately to this work. All other authors declare no competing interests.


536. Reprod Biomed Online. 2025 Aug 30;52(2):105237. doi: 10.1016/j.rbmo.2025.105237. 
Online ahead of print.

Can artificial intelligence models provide reliable medical counselling to 
fertility patients?

Alcalay I(1), Weissman A(1), Ganer Herman H(2), Tsafrir A(3), Friedman M(1), 
Weiner E(1), Orvieto R(4), Polyzos NP(5), Dahan MH(6), Polyakov A(7), Fischer 
R(8), Esteves SC(9), Ata B(10), Franasiak JM(11), Mizrachi Y(12).

Author information:
(1)Fertility Unit, The Edith Wolfson Medical Center, Holon, and Faculty of 
Health and Medical Sciences, Tel-Aviv University, Tel Aviv, Israel.
(2)Twig Fertility, Toronto, Canada.
(3)Faculty of Medicine, Hebrew University of Jerusalem, and IVF Unit, Department 
of Obstetrics and Gynecology, Shaare Zedek Medical Center, Jerusalem, Israel.
(4)IVF Unit, Department of Obstetrics and Gynecology, Chaim Sheba Medical Center 
(Tel Hashomer), Ramat Gan, and Faculty of Medical and Health Science, Tel Aviv 
University, Tel Aviv, Israel.
(5)Dexeus Fertility, Department of Obstetrics, Gynecology and Reproductive 
Medicine, Hospital Universitari Dexeus, Barcelona, Spain.; Faculty of Medicine 
and Health Sciences, Ghent University, Ghent, Belgium.
(6)MUHC Reproductive Centre, and Department of Obstetrics and Gynecology, 
Division of Reproductive Endocrinology and Infertility, McGill University, 
Montréal, Quebec, Canada.
(7)Fertility Services Unit, The Royal Women's Hospital, and Faculty of Medicine, 
Dentistry and Health Sciences, University of Melbourne, Melbourne, Victoria, 
Australia.; Genea Fertility Melbourne, Melbourne, Victoria, Australia.
(8)MVZ Fertility Center Hamburg, Hamburg, Germany.
(9)ANDROFERT, Andrology and Human Reproduction Clinic, and Division of Urology, 
Department of Surgery, State University of Campinas (UNICAMP), Campinas, 
Brazil.; Department of Clinical Medicine, Aarhus University, Aarhus, Denmark.
(10)Department of Obstetrics and Gynecology, Koç University School of Medicine, 
Istanbul, Turkey.; ART Fertility Clinics, Dubai, UAE.
(11)IVIRMA New Jersey, Thomas Jefferson University, Philadelphia, Pennsylvania, 
USA.
(12)Fertility Unit, The Edith Wolfson Medical Center, Holon, and Faculty of 
Health and Medical Sciences, Tel-Aviv University, Tel Aviv, Israel.. Electronic 
address: mizrachi.yossi@gmail.com.

RESEARCH QUESTION: Can generative artificial intelligence (AI) models provide 
reliable counselling to fertility patients regarding real-world clinical 
questions?
DESIGN: In this cross-sectional study, 12 clinical questions were developed to 
reflect common, real-life dilemmas encountered during fertility workup and 
treatment. Responses to each question were generated by two experienced 
fertility specialists, and two AI models - ChatGPT and Gemini. Eight leading 
internationally recognized fertility experts, blinded to the source of each 
reply, independently rated all the responses on a scale from 1 (strongly 
disagree) to 10 (strongly agree). Ratings were compared across all four repliers 
using non-parametric statistical tests.
RESULTS: The replies authored by physicians received significantly higher 
overall scores than those generated by AI models (P < 0.001). The median scores 
were highest for Doctor A (9.0), followed by Doctor B (8.0), then ChatGPT (7.0) 
and finally Gemini, which received the lowest score (4.5). The proportion of 
high-scoring responses (≥8) was greatest for Doctor A (70.8%), followed by 
Doctor B (56.3%), then ChatGPT (47.9%) and finally Gemini (35.4%) (P < 0.001).
CONCLUSIONS: Experienced fertility specialists outperformed generative AI models 
in providing accurate responses to complex clinical questions. Despite the 
growing accessibility and sophistication of AI tools, their use for 
individualized fertility counselling remains limited. Continued refinement and 
clinical validation of AI tools are essential before they can be considered 
reliable for patient-specific guidance. At present, AI should be viewed as a 
complementary resource rather than a substitute for expert clinical judgement.

Crown Copyright © 2025. Published by Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.rbmo.2025.105237
PMID: 41475300


537. Front Digit Health. 2025 Nov 26;7:1623922. doi: 10.3389/fdgth.2025.1623922. 
eCollection 2025.

Optimized BERT-based NLP outperforms zero-shot methods for automated symptom 
detection in clinical practice.

Diaz Ochoa JG(1)(2), Layer N(3), Mahr J(1), Mustafa FE(1), Menzel CU(4), Müller 
M(5), Schilling T(4), Illerhaus G(3), Knott M(3), Krohn A(4)(5).

Author information:
(1)QuiBiQ GmbH, Stuttgart, Germany.
(2)PerMediQ GmbH, Stuttgart, Germany.
(3)Klinikum Stuttgart, Stuttgart Cancer Center - Tumorzentrum Eva Mayr-Stihl DE, 
Stuttgart, Germany.
(4)Department for Emergency and Intensive Care Medicine (DIANI), Klinikum 
Stuttgart, Stuttgart, Germany.
(5)Department of Internal Medicine I, University Hospital Regensburg, 
Regensburg, Germany.

BACKGROUND: Large Language Models (LLMs) have raised broad expectations for 
clinical use, particularly in the processing of complex medical narratives. 
However, in practice, more targeted Natural Language Processing (NLP) approaches 
may offer higher precision and feasibility for symptom extraction from 
real-world clinical texts. NLP provides promising tools for extracting clinical 
information from unstructured medical narratives. However, few studies have 
focused on integrating symptom information from free texts in German, 
particularly for complex patient groups such as emergency department (ED) 
patients. The ED setting presents specific challenges: high documentation 
pressure, heterogeneous language styles, and the need for secure, locally 
deployable models due to strict data protection regulations. Furthermore, German 
remains a low-resource language in clinical NLP.
METHODS: We implemented and compared two models for zero-shot learning-GLiNER 
and Mistral-and a fine-tuned BERT-based SCAI-BIO/BioGottBERT model for named 
entity recognition (NER) of symptoms, anatomical terms, and negations in German 
ED anamnesis texts in an on-premises environment in a hospital. Manual 
annotations of 150 narratives were used for model validation. The postprocessing 
steps included confidence-based filtering, negation exclusion, symptom 
standardization, and integration with structured oncology registry data. All 
computations were performed on local hospital servers in an on-premises 
implementation to ensure full data protection compliance.
RESULTS: The fine-tuned SCAI-BIO/BioGottBERT model outperformed both zero-shot 
approaches, achieving an F1 score of 0.84 for symptom extraction and 
demonstrating superior performance in negation detection. The validated pipeline 
enabled systematic extraction of affirmed symptoms from ED-free text, 
transforming them into structured data. This method allows large-scale analysis 
of symptom profiles across patient populations and serves as a technical 
foundation for symptom-based clustering and subgroup analysis.
CONCLUSIONS: Our study demonstrates that modern NLP methods can reliably extract 
clinical symptoms from German ED free text, even under strict data protection 
constraints and with limited training resources. Fine-tuned models offer a 
precise and practical solution for integrating unstructured narratives into 
clinical decision-making. This work lays the methodological foundation for a new 
way of systematically analyzing large patient cohorts on the basis of free-text 
data. Beyond symptoms, this approach can be extended to extracting diagnoses, 
procedures, or other clinically relevant entities. Building upon this framework, 
we apply network-based clustering methods (in a subsequent study) to identify 
clinically meaningful patient subgroups and explore sex- and age-specific 
patterns in symptom expression.

© 2025 Diaz Ochoa, Layer, Mahr, Mustafa, Menzel, Müller, Schilling, Illerhaus, 
Knott and Krohn.

DOI: 10.3389/fdgth.2025.1623922
PMCID: PMC12689901
PMID: 41383383

Conflict of interest statement: Authors JD, JM and FM were employed by company 
QuiBiQ GmbH. JD was employed by company PerMediQ GmbH. The remaining authors 
declare that the research was conducted in the absence of any commercial or 
financial relationships that could be construed as a potential conflict of 
interest. The author(s) declared that they were an editorial board member of 
Frontiers, at the time of submission. This had no impact on the peer review 
process and the final decision.


538. J Adv Nurs. 2025 Jul;81(7):3473-3484. doi: 10.1111/jan.16628. Epub 2024 Nov 18.

Exploring the Current Applications and Effectiveness of ChatGPT in Nursing: An 
Integrative Review.

Luo Y(1), Miao Y(1), Zhao Y(1), Li J(1), Wu Y(1).

Author information:
(1)School of Nursing, Capital Medical University, Beijing, China.

AIMS: To systematically review the current application status of ChatGPT in 
nursing and explore its application effects.
DESIGN: An integrative review.
METHODS: Following inclusion and exclusion criteria, two researchers summarised 
the selected literature and conducted a quality appraisal, followed by narrative 
synthesis.
DATA SOURCES: PubMed, Web of Science and Scopus were searched from January 2022 
to June 2024.
RESULTS: A total of 31 papers met the inclusion criteria. Fifteen empirical 
studies were rated as grade 5, while five were rated as grade 4. The references 
of a minireview were not recently published and lacked ChatGPT-related articles, 
and a systematic review was of low quality. The review focused on three main 
topics: (1) The subsidiary role of ChatGPT in nursing; (2) Comparison of 
different models' effectiveness and (3) Existing challenges.
CONCLUSIONS: While adopting new technologies such as ChatGPT, it is important to 
maintain a balanced perspective on both its benefits and limitations. Nursing 
professionals must actively address these deficiencies and explore solutions to 
improve ChatGPT's utility in the field.
IMPLICATIONS TO THE PROFESSION AND PATIENT CARE: This review synthesised 
evidence on ChatGPT's application and highlighted existing challenges in 
nursing. Nursing researchers, educators and practitioners can further validate 
these findings to explore its potential in various aspects of nursing practice.
IMPACT: For researchers, ChatGPT can enhance language quality and summarise 
findings effectively, but adherence to research standards is crucial. For 
educators, ChatGPT can serve as an effective information source for students, 
though caution should be taken to avoid overreliance. For practitioners, ChatGPT 
can offer useful suggestions for clinical practice, but these should be 
critically evaluated and not followed blindly, as issues of inaccuracy must be 
addressed.
REPORTING METHOD: This review adhered to Preferred Reporting Items for 
Systematic Reviews and Meta-Analyses.
PATIENT OR PUBLIC CONTRIBUTION: No patient or public contribution.

© 2024 John Wiley & Sons Ltd.

DOI: 10.1111/jan.16628
PMID: 39555676 [Indexed for MEDLINE]


539. Transl Vis Sci Technol. 2025 Oct 1;14(10):20. doi: 10.1167/tvst.14.10.20.

Predicting Intraocular Collamer Lens Vault in Myopic Patients With Shallow 
Anterior Chamber Using FedEYE Platform and ChatGPT.

Su Z(1), Wang Z(2), Wang S(2), Ni M(2), Hu J(2), Wang R(2), Yang J(2), Bai Y(2), 
Xu Y(2), Di S(2), Xu Y(2), Wang Z(1), Chen Z(3), Ortega-Usobiaga J(4), Wang 
MX(5), Jiang X(6), Chen Y(6), Dai W(1)(7), Chen J(1), Li K(1)(2)(3)(8).

Author information:
(1)Aier Academy of Ophthalmology, Central South University, Changsha, Hunan, 
China.
(2)Xi'an Aier Eye Hospital, Northwest University, Xi'an, Shaanxi, China.
(3)School of Medicine, Northwest University, Xi'an, Shaanxi, China.
(4)Department of Cataract & Refractive Surgery, Clínica Baviera, Aier Eye 
Hospital Group, Bilbao, Spain.
(5)Wang Vision Institute, Aier Eye Hospital Group, Nashville, TN, USA.
(6)Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 
China.
(7)Institute of Digital Ophthalmology and Visual Science, Changsha Aier Eye 
Hospital, Changsha, Hunan, China.
(8)Xi'an People's Hospital, Xi'an Fourth Hospital, Northwest University, Xi'an, 
Shaanxi, China.

PURPOSE: This study evaluated the effectiveness of the code-free platforms 
FedEYE and ChatGPT in predicting vault in patients with shallow anterior chamber 
depth (ACD) undergoing implantable collamer lens (ICL) surgery.
METHODS: This retrospective study included 160 eyes with shallow ACD (<2.8 mm) 
and developed two code-free artificial intelligence (AI) models. An integrated 
neural network (INN) was developed using the FedEYE platform with ResNet and 
gradient-boosting trees, combined with optical coherence tomography (OCT) images 
and examination data, to determine how different data combinations achieve the 
best model performance and the correlation between parameters and vault. ChatGPT 
prompted with memory function and also used to validate its capability to 
predict vault based on these preoperative multimodal data.
RESULTS: Based on the test set results, the INN model, using the combined OCT 
image and examination data, achieved favorable performance with accuracy of 
0.875, sensitivity of 0.857, specificity of 0.889, weighted F1 score of 0.86, 
and area under the curve (AUC) of 0.873. Anterior chamber angle and spherical 
equivalent positively correlated with vault, and age and white-to-white diameter 
negatively correlated. Following memory prompting, ChatGPT improved itself and 
reached accuracy of 0.813, recall of 0.571, weighted F1 score of 0.73, and AUC 
of 0.865.
CONCLUSIONS: Code-free tools are particularly notable for their straightforward 
development and deployment. These two code-free AI models demonstrated 
encouraging performance in predicting vault in shallow ACD patients.
TRANSLATIONAL RELEVANCE: Using different AI models to predict vault after ICL 
implantation could be of clinical significance for improving surgical safety and 
efficacy in shallow ACD patients.

DOI: 10.1167/tvst.14.10.20
PMCID: PMC12533507
PMID: 41090992 [Indexed for MEDLINE]

Conflict of interest statement: Disclosure: Z. Su, None; Z. Wang, None; S. Wang, 
None; M. Ni, None; J. Hu, None; R. Wang, None; J. Yang, None; Y. Bai, None; Y. 
Xu, None; S. Di, None; Y. Xu, None; Z. Wang, None; Z. Chen, None; J. 
Ortega-Usobiaga, None; M.X. Wang, None; X. Jiang, None; Y. Chen, None; W. Dai, 
None; J. Chen, None; K. Li, None


540. J Am Med Inform Assoc. 2025 Mar 1;32(3):526-534. doi: 10.1093/jamia/ocae314.

Utility of word embeddings from large language models in medical diagnosis.

Yazdani S(1), Henry RC(2), Byrne A(3), Henry IC(4).

Author information:
(1)Department of Pediatrics, David Geffen School of Medicine, University of 
California Los Angeles, Los Angeles, CA 90095, United States.
(2)Department of Civil Engineering, University of Southern California, Los 
Angeles, CA 90089, United States.
(3)San Francisco, CA 94129, United States.
(4)Kennewick, WA 99338, United States.

OBJECTIVE: This study evaluates the utility of word embeddings, generated by 
large language models (LLMs), for medical diagnosis by comparing the semantic 
proximity of symptoms to their eponymic disease embedding ("eponymic condition") 
and the mean of all symptom embeddings associated with a disease ("ensemble 
mean").
MATERIALS AND METHODS: Symptom data for 5 diagnostically challenging pediatric 
diseases-CHARGE syndrome, Cowden disease, POEMS syndrome, Rheumatic fever, and 
Tuberous sclerosis-were collected from PubMed. Using the Ada-002 embedding 
model, disease names and symptoms were translated into vector representations in 
a high-dimensional space. Euclidean and Chebyshev distance metrics were used to 
classify symptoms based on their proximity to both the eponymic condition and 
the ensemble mean of the condition's symptoms.
RESULTS: The ensemble mean approach showed significantly higher classification 
accuracy, correctly classifying between 80% (Cowden disease) to 100% (Tuberous 
sclerosis) of the sample disease symptoms using the Euclidean distance metric. 
In contrast, the eponymic condition approach using Euclidian distance metric and 
Chebyshev distances, in general, showed poor symptom classification performance, 
with erratic results (0%-100% accuracy), largely ranging between 0% and 3% 
accuracy.
DISCUSSION: The ensemble mean captures a disease's collective symptom profile, 
providing a more nuanced representation than the disease name alone. However, 
some misclassifications were due to superficial semantic similarities, 
highlighting the need for LLM models trained on medical corpora.
CONCLUSION: The ensemble mean of symptom embeddings improves classification 
accuracy over the eponymic condition approach. Future efforts should focus on 
medical-specific training of LLMs to enhance their diagnostic accuracy and 
clinical utility.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocae314
PMCID: PMC11833464
PMID: 39786898 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
declare.


541. IEEE J Biomed Health Inform. 2025 Nov 28;PP. doi: 10.1109/JBHI.2025.3638444. 
Online ahead of print.

NRAG: A Knowledge-Enhanced LLM Framework for Interpretable Neurosurgical Disease 
Diagnosis in Outpatient and Emergency Settings.

Tian H, Liu Y, Dai X, Dong X, Yu J, Wei W, Wang B, Zhou X.

Large language models (LLMs) have achieved state-of-the-art performance in 
numerous domains, yet their clinical deployment faces critical barriers, 
particularly insufficient reasoning in complex scenarios and limited 
interpretability. These challenges are exacerbated in neurosurgical diagnosis 
for outpatient and emergency settings, where time-sensitive decision-making, 
fragmented data, and complex comorbidities render conventional free-text-based 
modeling approaches unreliable. To address the limitations of existing LLMs in 
medical auxiliary diagnosis, particularly in interpretability and predictive 
performance, this study proposed NRAG, an auxiliary diagnosis method that 
combines LLMs with knowledge graphs (KGs). It extracts symptom descriptions from 
clinical records and performs personalized retrieval of associated paths in KG, 
and supplements potential patient symptoms to optimize the diagnosis model. 
Comparative experiments involving multiple general-domain and medical-domain 
LLMs, along with case studies, were conducted to validate the NRAG's 
effectiveness. Experimental results demonstrate that integrating KG 
significantly improves diagnosis accuracy, achieving an F1-score of 0.8150. It 
also substantially improves model interpretability and performs excellently in 
expert evaluations. Ablation studies and comparative experiments with other 
general-domain and medical-domain LLMs confirm the superior performance of the 
proposed NRAG. NRAG effectively supplements missing symptom information and 
provides knowledge-path-based evidence for diagnosis results, while improving 
the precision and interpretability of intelligent diagnosis. Furthermore, this 
approach sets the foundation for intelligent diagnoses in neurosurgery while 
providing a methodological framework for the integration of in-depth clinical 
data mining with medical knowledge base resources.

DOI: 10.1109/JBHI.2025.3638444
PMID: 41313699


542. J Allergy Clin Immunol Pract. 2025 Jun;13(6):1350-1357.e4. doi: 
10.1016/j.jaip.2025.03.030. Epub 2025 Mar 27.

ChatGPT Performance on 120 Interdisciplinary Allergology Questions-Systematic 
Evaluation With Clinical Error Impact Assessment for Critical Erroneous 
AI-Guided Chatbot Advice.

Mathes S(1), Seurig S(2), Bluhme F(3), Beyer K(3), Heizmann F(2), Wagner M(2), 
Neugärtner I(3), Biedermann T(4), Darsow U(4).

Author information:
(1)Department for Dermatology and Allergology, School of Medicine, Technical 
University of Munich, Munich, Germany. Electronic address: 
sonja.mathes@mri.tum.de.
(2)Department of Respiratory Medicine, Allergology and Sleep Medicine, General 
Hospital Nuremberg, Campus North, Paracelsus Medical University, Nuremberg, 
Germany.
(3)Department of Pediatric Respiratory Medicine, Immunology, and Critical Care 
Medicine, Charité-Universitätsmedizin Berlin, corporate member of Freie 
Universität Berlin and Humboldt-Universität zu Berlin, Berlin, Germany.
(4)Department for Dermatology and Allergology, School of Medicine, Technical 
University of Munich, Munich, Germany.

BACKGROUND: ChatGPT (Chatbot with Generative Pretrained Transformer), despite 
not being a medical device, may be used by patients for medical inquiries. Its 
accessibility and convenience, particularly amidst long waiting times for 
allergology appointments, make it an attractive but potentially erroneous source 
of advice.
OBJECTIVES: This study evaluates ChatGPT's performance on allergological 
questions from clinical practice, offering a systematic approach to rating its 
errors. An Allergological Error Impact Assessment is proposed to analyze the 
potential consequences of these errors on patients.
METHODS: A total of 120 multidisciplinary allergology questions from 
dermatology, pediatrics, and pulmonology were prompted to ChatGPT (3.5). Errors 
were assessed in terms of content, accuracy (ACC), completeness (CO), perceived 
humanness (PHU), and readability (Flesch Reading Ease). Erroneous responses were 
categorized on a 3-step severity scale (minor, major, and critical). Critical 
errors underwent allergological error impact analysis. Statistical evaluation 
included descriptive analyses and Kruskal-Wallis and Mann-Whitney U tests.
RESULTS: ChatGPT demonstrated good accuracy (mean ACC 4.1/5, standard deviation: 
0.78, range: 1-5). CO and PHU were sufficient but lowest for pediatric queries. 
Readability was at an academic level for most responses. Six critical errors 
were identified: 1 in dermatology, 2 in pediatrics, and 3 in pulmonology. 
Notably, a critical pediatric food allergen error carried a potentially 
life-threatening risk.
CONCLUSION: ChatGPT's imperfect reliability in allergology highlights the need 
for expert counseling in specialized fields. Tailoring these tools to allergy 
use cases could improve utility of models like ChatGPT for clinical 
applications, such as answering questions from allergological routine care.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jaip.2025.03.030
PMID: 40157421 [Indexed for MEDLINE]


543. NPJ Digit Med. 2026 Jan 19. doi: 10.1038/s41746-026-02362-6. Online ahead of 
print.

LLM-driven collaborative framework for knowledge-enhanced cancer pain assessment 
and management.

Liu H(#)(1), Hu Y(#)(1), Li D(#)(1), Shi B(2), Niu Y(3), Zhang X(2), Zheng G(4), 
Li C(5), Wang L(5), Bao Y(6).

Author information:
(1)Guang'anmen Hospital, China Academy of Chinese Medical Sciences, Beijing, 
China.
(2)School of Biomedical Engineering, Tsinghua University, Beijing, China.
(3)Tianjin Key Laboratory of Radiation Medicine and Molecular Nuclear Medicine, 
Institute of Radiation Medicine, Chinese Academy of Medical Sciences & Peking 
Union Medical College, Tianjin, China.
(4)Xiyuan Hospital, China Academy of Chinese Medical Sciences, Beijing, China.
(5)The First Clinical College, Liaoning University of Traditional Chinese 
Medicine, Shenyang, China.
(6)Guang'anmen Hospital, China Academy of Chinese Medical Sciences, Beijing, 
China. drbaoyanju@163.com.
(#)Contributed equally

Due to its multi-factor mechanism, variable opioid response, and high-risk 
adverse reactions, cancer pain remains a major challenge in oncology. To 
overcome these obstacles, we have developed a collaboration framework based on 
large language models (LLMs): OncoPainBot. This framework can simulate the 
reasoning and decision-making of multiple clinical experts to conduct 
comprehensive cancer pain assessment and management. Our OncoPainBot integrates 
four specialized agents: Pain-Extraction, Pain-Mechanism Reasoning, 
Treatment-Planning, and Safety-Check, each corresponding to a unique clinical 
role. In this paper, we compare seven LLMs and three Retrieval-Augmented 
Generation(RAG) strategies to determine the optimal model configuration. The 
final framework was verified on 516 real-world electronic medical records of 
cancer pain collected. We tested our solution through multiple dimensions. 
Ultimately, Claude-4 combined with RAG achieved the best overall performance, 
demonstrating outstanding semantic consistency and evidence-based reasoning in 
multiple metrics. In clinical validation, OncoPainBot achieved a high degree of 
consistency between the generated reports and actual clinical documents, while 
maintaining a high decision-making accuracy (0.841) in the analgesic 
recommendation task. At the same time, our error analysis shows that most of the 
differences are caused by patient-specific factors and monitoring 
recommendations rather than incorrect drug selection, which demonstrates the 
reliability of our framework. OncoPainBot has demonstrated the feasibility of a 
cancer pain management system based on LLMs, providing a transparent, 
evidence-based, and clinical-based framework for personalized analgesic care.

© 2026. The Author(s).

DOI: 10.1038/s41746-026-02362-6
PMID: 41554973

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


544. Cyberpsychol Behav Soc Netw. 2025 Jul;28(7):505-514. doi: 
10.1089/cyber.2024.0536. Epub 2025 May 7.

Artificial Intelligence and the Illusion of Understanding: A Systematic Review 
of Theory of Mind and Large Language Models.

Marchetti A(1), Manzi F(1), Riva G(2), Gaggioli A(3), Massaro D(1).

Author information:
(1)Department of Psychology, Research Center on Theory of Mind and Social 
Competences in the Lifespan, Università Cattolica del Sacro Cuore, Milan, Italy.
(2)Humane Technology Laboratory, Università Cattolica del Sacro Cuore, Milan, 
Italy.
(3)Department of Psychology, Research Center in Communication Psychology, 
Università Cattolica del Sacro Cuore, Milan, Italy.

The development of Large Language Models (LLMs) has sparked significant debate 
regarding their capacity for Theory of Mind (ToM)-the ability to attribute 
mental states to oneself and others. This systematic review examines the extent 
to which LLMs exhibit Artificial ToM (AToM) by evaluating their performance on 
ToM tasks and comparing it with human responses. While LLMs, particularly GPT-4, 
perform well on first-order false belief tasks, they struggle with more complex 
reasoning, such as second-order beliefs and recursive inferences, where humans 
consistently outperform them. Moreover, the review underscores the variability 
in ToM assessments, as many studies adapt classical tasks for LLMs, raising 
concerns about comparability with human ToM. Most evaluations remain constrained 
to text-based tasks, overlooking embodied and multimodal dimensions crucial to 
human social cognition. This review discusses the "illusion of understanding" in 
LLMs for two primary reasons: First, their lack of the developmental and 
cognitive mechanisms necessary for genuine ToM, and second, methodological 
biases in test designs that favor LLMs' strengths, limiting direct comparisons 
with human performance. The findings highlight the need for more ecologically 
valid assessments and interdisciplinary research to better delineate the 
limitations and potential of AToM. This set of issues is highly relevant to 
psychology, as language is generally considered just one component in the 
broader development of human ToM, a perspective that contrasts with the dominant 
approach in AToM studies. This discrepancy raises critical questions about the 
extent to which human ToM and AToM are comparable.

DOI: 10.1089/cyber.2024.0536
PMID: 40333375 [Indexed for MEDLINE]


545. JMIR Form Res. 2025 Dec 11;9:e81545. doi: 10.2196/81545.

Large Language Model Evaluation in Traditional Chinese Medicine for Stroke: 
Quantitative Benchmarking Study.

Long H(1), Deng Y(2), Guo Y(1), Shen Z(3), Zhang Y(4), Bao J(2), He Y(1).

Author information:
(1)Hospital of Chengdu University of Traditional Chinese Medicine, Chengdu, 
Sichuan Province, China.
(2)Institute of Clinical Pathology, West China Hospital of Sichuan University, 
Chengdu, China.
(3)Physical Education Institute, Southwest Medical University, Luzhou, China.
(4)Chengdu University of Traditional Chinese Medicine, Chengdu, China.

BACKGROUND: The application of large language models (LLMs) in medicine is 
rapidly advancing. However, evaluating LLM capabilities in specialized domains 
such as traditional Chinese medicine (TCM), which possesses a unique theoretical 
system and cognitive framework, remains a sizable challenge.
OBJECTIVE: This study aimed to provide an empirical evaluation of different LLM 
types in the specialized domain of TCM stroke.
METHODS: The Traditional Chinese Medicine-Stroke Evaluation Dataset (TCM-SED), a 
203-question benchmark, was systematically constructed. The dataset includes 3 
paradigms (short-answer questions, multiple-choice questions, and essay 
questions) and covers multiple knowledge dimensions, including diagnosis, 
pattern differentiation and treatment, herbal formulas, acupuncture, 
interpretation of classical texts, and patient communication. Gold standard 
answers were established through a multiexpert cross-validation and consensus 
process. The TCM-SED was subsequently used to comprehensively test 2 
representative LLM models: GPT-4o (a leading international general-purpose 
model) and DeepSeek-R1 (a large model primarily trained on Chinese corpora).
RESULTS: The test results revealed a differentiation in model capabilities 
across cognitive levels. In objective sections emphasizing precise knowledge 
recall, DeepSeek-R1 comprehensively outperformed GPT-4o, achieving an accuracy 
lead of more than 17% in the multiple-choice section (96/137, 70.1% vs 72/137, 
52.6%, respectively). Conversely, in the essay section, which tested knowledge 
integration and complex reasoning, GPT-4o's performance notably surpassed that 
of DeepSeek-R1. For instance, in the interpretation of classical texts category, 
GPT-4o achieved a scoring rate of 90.5% (181/200), far exceeding DeepSeek-R1 
(147/200, 73.5%).
CONCLUSIONS: This empirical study demonstrates that Chinese-centric models have 
a substantial advantage in static knowledge tasks within the TCM domain, whereas 
leading general-purpose models exhibit stronger dynamic reasoning and content 
generation capabilities. The TCM-SED, developed as the benchmark for this study, 
serves as an effective quantitative tool for evaluating and selecting 
appropriate LLMs for TCM scenarios. It also offers a valuable data foundation 
and a new research direction for future model optimization and alignment.

©Hulin Long, Yang Deng, Yaoguang Guo, Zifan Shen, Yuzhu Zhang, Ji Bao, Yang He. 
Originally published in JMIR Formative Research (https://formative.jmir.org), 
11.12.2025.

DOI: 10.2196/81545
PMCID: PMC12741655
PMID: 41380151 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


546. Eur Arch Otorhinolaryngol. 2025 Dec 22. doi: 10.1007/s00405-025-09932-6. Online 
ahead of print.

Diagnostic interpretation of pure tone audiograms by multimodal LLMs: A 
comparative study of ChatGPT-5.0 and Gemini 2.5.

Karaçaylı C(1)(2), Tahir E(3), Altuntaş EE(4).

Author information:
(1)Department of Audiology, University of Health Sciences, Ankara, Türkiye. 
ckaracayli@yahoo.com.
(2)Department of Otolaryngology, Gülhane Training and Research Hospital, Ankara, 
Türkiye. ckaracayli@yahoo.com.
(3)Faculty of Medicine, Department of Otolaryngology, Ondokuz Mayıs University, 
Samsun, Türkiye.
(4)Faculty of Medicine, Department of Otolaryngology, Lokman Hekim University, 
Ankara, Türkiye.

OBJECTIVES: This study aimed to evaluate and compare the diagnostic accuracy of 
two multimodal large language models-ChatGPT-5.0 Plus and Gemini 2.5-in 
interpreting pure tone audiograms. The primary hypothesis was that ChatGPT-5.0 
Plus would outperform Gemini 2.5 in identifying auditory thresholds, determining 
type and degree of hearing loss, detecting masking, and providing treatment 
recommendations based on standardized visual inputs.
DESIGN: A diagnostic simulation study was conducted using 80 software-generated 
audiograms representing common hearing loss profiles, including normal, 
conductive, sensorineural, and mixed types. Each audiogram was interpreted by 
both models using a structured seven-question diagnostic prompt aligned with 
professional audiological guidelines. Two independent evaluators-an audiologist 
and an otolaryngologist with audiology expertise-rated each model's response 
using a five-point Likert scale. Inter-rater agreement and comparative analyses 
were performed using non-parametric statistical tests.
RESULTS: ChatGPT-5.0 Plus outperformed Gemini on six of seven diagnostic items 
and in the overall score. Inter-rater agreement for ChatGPT was almost perfect 
overall (κ = 0.951), with moderate concordance on Q1 (κ = 0.490) and very high 
agreement across the remaining items (κ = 0.912-0.981). Gemini also showed 
strong but lower consistency (overall κ = 0.823; item-level 0.833-0.949), with 
the weakest agreement on treatment recommendation (Q7). Comparative analyses 
revealed statistically significant advantages for ChatGPT in air and bone 
conduction threshold identification, classification of hearing loss type and 
degree, and diagnostic accuracy. Differences in masking evaluation (Q5) were not 
significant. Median score differences of 1-2 points on the 5-point scale 
underscored the clinical relevance of ChatGPT's superior performance.
CONCLUSIONS: ChatGPT-5.0 Plus demonstrated superior accuracy and consistency in 
interpreting pure tone audiograms compared to Gemini 2.5. While not suitable as 
standalone diagnostic tools, large language models may serve as useful adjuncts 
in primary care and telehealth environments for preliminary audiological 
assessment. Further validation in real-world clinical settings is necessary 
before broader implementation.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00405-025-09932-6
PMID: 41430450

Conflict of interest statement: Declarations. Human Participants: This study did 
not involve human participants or the use of real patient data; instead, 
simulated audiograms were utilized exclusively. Therefore, ethical approval was 
not required. No external funding was received for the conduct of this research. 
Conflict of interest: The authors declare no conflicts of interest.


547. J Am Med Inform Assoc. 2026 Jan 13:ocaf233. doi: 10.1093/jamia/ocaf233. Online 
ahead of print.

Testing and evaluation of generative large language models in electronic health 
record applications: a systematic review.

Du X(1)(2)(3), Zhou Z(4), Wang Y(4), Chuang YW(5)(6)(7), Li Y(1)(2), Yang 
R(1)(2), Zhang W(1)(2)(3), Wang X(1)(2)(3), Chen X(1)(2)(3), Guan H(1)(2), Lian 
J(1), Hong P(4), Bates DW(1)(2)(8), Zhou L(1)(2)(3).

Author information:
(1)Division of General Internal Medicine and Primary Care, Brigham and Women's 
Hospital, Boston, MA 02115, United States.
(2)Department of Medicine, Harvard Medical School, Boston, MA 02115, United 
States.
(3)Department of Biomedical Informatics, Harvard Medical School, Boston, MA 
02115, United States.
(4)Department of Computer Science, Brandeis University, Waltham, MA 02453, 
United States.
(5)Division of Nephrology, Department of Internal Medicine, Taichung Veterans 
General Hospital, Taichung 407219, Taiwan.
(6)Department of Post-Baccalaureate Medicine, College of Medicine, National 
Chung Hsing University, Taichung 402202, Taiwan.
(7)School of Medicine, College of Medicine, China Medical University, Taichung 
404328, Taiwan.
(8)Department of Health Policy and Management, Harvard T.H. Chan School of 
Public Health, Boston, MA 02115, United States.

BACKGROUND: The use of generative large language models (LLMs) with electronic 
health record (EHR) data is rapidly expanding to support clinical and research 
tasks. This systematic review characterizes the clinical fields and use cases 
that have been studied and evaluated to date.
METHODS: We followed the Preferred Reporting Items for Systematic Review and 
Meta-Analyses guidelines to conduct a systematic review of articles from PubMed 
and Web of Science published between January 1, 2023, and November 9, 2024. 
Studies were included if they used generative LLMs to analyze real-world EHR 
data and reported quantitative performance evaluations. Through data extraction, 
we identified clinical specialties and tasks for each included article, and 
summarized evaluation methods.
RESULTS: Of the 18 735 articles retrieved, 196 met our criteria. Most studies 
focused on radiology (26.0%), oncology (10.7%), and emergency medicine (6.6%). 
Regarding clinical tasks, clinical decision support made up the largest 
proportion of studies (62.2%), while summarizations and patient communications 
made up the smallest, at 5.6% and 5.1%, respectively. In addition, GPT-4 and 
GPT-3.5 were the most commonly used generative LLMs, appearing in 60.2% and 
57.7% of studies, respectively. Across these studies, we identified 22 unique 
non-NLP metrics and 35 unique NLP metrics. While NLP metrics offer greater 
scalability, none demonstrated a strong correlation with gold-standard human 
evaluations.
CONCLUSION: Our findings highlight the need to evaluate generative LLMs on EHR 
data across a broader range of clinical specialties and tasks, as well as the 
urgent need for standardized, scalable, and clinically meaningful evaluation 
frameworks.

© The Author(s) 2026. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf233
PMID: 41528313


548. BMC Urol. 2026 Jan 19. doi: 10.1186/s12894-026-02054-z. Online ahead of print.

Estimation of IPSS and OABSS scores using ChatGPT-4o: a comparative validation 
study in Korea.

Bae H(1)(2), Lee GM(1)(2), Lee J(1)(2), Lee JH(1)(2), Yoo S(1)(2), Cho MC(1)(2), 
Jeong H(1)(2), Son H(3)(4).

Author information:
(1)Department of Urology, Seoul National University Boramae Medical Center, 
Sindaebang 2(i)-dong, Dongjak-gu, Seoul, 07061, Korea.
(2)Seoul National University College of Medicine, Seoul, Republic of Korea.
(3)Department of Urology, Seoul National University Boramae Medical Center, 
Sindaebang 2(i)-dong, Dongjak-gu, Seoul, 07061, Korea. volley@snu.ac.kr.
(4)Seoul National University College of Medicine, Seoul, Republic of Korea. 
volley@snu.ac.kr.

PURPOSE: To evaluate the performance of ChatGPT-4o in estimating International 
Prostate Symptom Score (IPSS) and Overactive Bladder Symptom Score (OABSS) based 
on patients' natural language descriptions and full outpatient records, compared 
to actual questionnaire scores.
MATERIALS AND METHODS: This study included 91 patients, of whom 52 completed 
IPSS and 77 completed OABSS. ChatGPT-4o was prompted with verbatim symptom 
statements and full medical records written by a urologist. Predicted scores 
were compared to actual scores using paired t-tests, weighted Cohen's kappa for 
item-level agreement, Spearman's correlation for total scores, and Bland-Altman 
plots for bias. Diagnostic classifications (lower urinary tract symptoms [LUTS]: 
IPSS ≥8; overactive bladder [OAB]: OABSS ≥3 with urgency ≥2) were assessed using 
McNemar's test and receiver operating characteristic curve analysis.
RESULTS: Mean IPSS scores estimated by ChatGPT-4o were statistically 
significantly lower than patient-reported scores (11.2 vs. 13.6, p = 0.006), 
whereas OABSS scores did not differ significantly between the two methods (6.99 
vs. 6.86, p = 0.686). Diagnostic agreement was high: LUTS in 42 (actual) vs. 38 
(GPT) patients, and OAB in 51 vs. 50 patients. Area under curve was 0.81 for 
IPSS and 0.91 for OABSS. Kappa values ranged from 0.23-0.81 (IPSS) and 0.44-0.71 
(OABSS), with highest concordance in quality of life (QoL) and urgency 
incontinence. Spearman's correlation coefficient was 0.60 (IPSS) and 0.70 
(OABSS). Accuracy was lower in first-visit patients.
CONCLUSIONS: GPT-4o estimated IPSS and OABSS with moderate but clinically 
acceptable accuracy. Its performance was comparable regarding diagnostic 
classification, particularly for QoL and OABSS. ChatGPT-4o may complement 
traditional questionnaires, particularly with missing or incomplete 
patient-reported data.

© 2026. The Author(s).

DOI: 10.1186/s12894-026-02054-z
PMID: 41555309

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was conducted in accordance with the Declaration of 
Helsinki. Ethical approval was obtained from the Institutional Review Board of 
Seoul National University Boramae Medical Center (IRB No. 30-2025-38). The 
requirement for informed consent was waived by the IRB because this was a 
retrospective medical record review that involved no direct patient contact, 
posed minimal risk, and used fully de-identified data. Consent for publication: 
Not applicable. Competing interests: The authors declare no competing interests.


549. Acad Radiol. 2025 Nov;32(11):6905-6912. doi: 10.1016/j.acra.2025.07.058. Epub 
2025 Aug 12.

Multimodal Large Language Models for Pediatric Bone-Age Assessment: A 
Comparative Accuracy Analysis.

Büyüktoka RE(1), Salbas A(2).

Author information:
(1)Izmir Foca State Hospital, Department of Radiolgy, 35680 Izmir, Turkey 
(R.E.B.). Electronic address: rasiterenbuyuktoka@hotmail.com.
(2)Izmir Katip Celebi University, Ataturk Training and Research Hospital, 
Department of Radiology, Izmir, Turkey (A.S.).

RATIONALE AND OBJECTIVES: Bone age assessment guides endocrine and orthopedic 
care in children, yet manual Greulich‑Pyle reading is subjective, and commercial 
deep‑learning tools are costly. We examined whether readily accessible 
closed‑source multimodal large language models can provide clinically acceptable 
bone age assessment accuracy.
MATERIALS AND METHODS: A balanced set of 180 left-hand wrist radiographs 
(ages 0-18 years; RSNA Pediatric Bone Age dataset) was analyzed by three models 
(ChatGPT‑o4-mini‑high, ChatGPT‑4.5, and Google Gemini 2.5 Pro) using an 
identical prompt. Reference ages were adjusted bone ages by consensus of 
radiologists. Accuracy metrics were mean absolute error (MAE) and 
root‑mean‑square error (RMSE); agreement was assessed with Bland-Altman 
analysis. Model performance was compared with a Friedman test followed by 
Bonferroni‑adjusted Wilcoxon signed‑rank tests.
RESULTS: Gemini 2.5 Pro yielded the lowest error (MAE 28.48 months, 
RMSE 35.23 months), comparing with ChatGPT‑4.5 (MAE 35.38 months, 
RMSE 43.64 months) and ChatGPT‑o4-mini‑high (MAE 35.87 months, 
RMSE 45.43 months; both P < .001). Bias was -9.38 months for Gemini, 
-4.80 months for ChatGPT‑4.5, and +5.26 months for ChatGPT‑o4-mini‑high; 95% 
limits of agreement exceeded ±4.5 years for every model.
CONCLUSION: Multimodal Large Language Models can estimate bone age directly from 
radiographs without task‑specific training, but current errors of 2-3years and 
wide errors remain unsuitable for clinical or medico‑legal decision‑making. 
Advances in vision encoder capacity and domain‑specific fine‑tuning are needed 
before LLMs can replace dedicated BAA software or expert readers.

Copyright © 2025 The Association of University Radiologists. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.acra.2025.07.058
PMID: 40803966 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


550. Front Digit Health. 2025 Jul 28;7:1636333. doi: 10.3389/fdgth.2025.1636333. 
eCollection 2025.

Rethinking survey development in health research with AI-driven methodologies.

Kuru H(1).

Author information:
(1)Human-Centered Design, Delft University of Technology, Delft, Netherlands.

Artificial intelligence (AI), particularly large language models (LLMs), offers 
new opportunities to address methodological challenges in survey development for 
health research. Traditional approaches, such as manual item generation, 
cognitive interviewing, and post-hoc psychometric validation, are time- and 
resource-consuming, and vulnerable to undetected issues that emerge only after 
large-scale data collection. These limitations, which appear in the early 
stages, can spread to later phases, leading to costly revisions and weakened 
construct validity. This paper introduces a conceptual framework for integrating 
AI-driven techniques throughout the survey development cycles. Drawing on 
natural language processing, automated text analysis, real-time data monitoring, 
and predictive modeling, the framework outlines how AI tools can help 
researchers proactively uncover linguistic nuances, identify hidden patterns, 
and refine instruments with greater speed and rigor, ultimately enhancing 
validity, inclusivity, and interpretive richness. Rather than replacing existing 
practices, these tools are positioned as a complementary support that, when used 
responsibly and contextually, can enhance methodological rigor, improve 
efficiency, and reduce respondent burden. The paper also emphasizes ethical 
considerations, including transparency, interpretability, and mitigation of 
bias. By combining AI's computational power with human expertise and critical 
reflexivity, this approach aims to foster more responsive, inclusive, and valid 
instruments for health-related research and interventions.

© 2025 Kuru.

DOI: 10.3389/fdgth.2025.1636333
PMCID: PMC12336171
PMID: 40792000

Conflict of interest statement: The author declares that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


551. Front Digit Health. 2025 Nov 6;7:1644041. doi: 10.3389/fdgth.2025.1644041. 
eCollection 2025.

Artificial intelligence in healthcare: applications, challenges, and future 
directions. A narrative review informed by international, multidisciplinary 
expertise.

Mohajer-Bastami A(#)(1), Moin S(#)(2), Ahmad S(#)(3), Ahmed AR(4), Pouwels 
S(5)(6), Hajibandeh S(7), Yang W(8), Parmar C(9)(10)(11), Kermansaravi M(12), 
Khalil M(13), Khalid AW(14), Khamise A(14), Rawaf D(15), Hosseini F(16), Agarwal 
A(17), Lala A(17), Ahmed S(18), Patel B(18), Fyntanidou B(19), Egan R(3), 
Mougiakakou SG(20), Jakob DA(21), Ribordy V(22), Hautz WE(21), Exadaktylos 
AK(21).

Author information:
(1)Brompton Primary Care Network, London, United Kingdom.
(2)Department of General Surgery, East Surrey Hospital, London, United Kingdom.
(3)Department of Surgery, Health Education and Improvement Wales (HEIW), Wales, 
United Kingdom.
(4)Department of Surgery, Imperial College London, London, United Kingdom.
(5)Department of Surgery, Bielefeld University-Campus Detmold, Detmold, Germany.
(6)Department of Intensive Care Medicine, Elisabeth-Tweesteden Hospital, 
Tilburg, Netherlands.
(7)Department of General Surgery, Morriston Hospital, Swansea, United Kingdom.
(8)Department of Surgery, The First Affiliated Hospital of Jinan University, 
Guangzhou, China.
(9)Department of Surgery, Whittington Hospital, London, United Kingdom.
(10)Department of Surgery, University College London, London, United Kingdom.
(11)Department of Surgery, Apollo Hospitals, Telengana, India.
(12)Department of Surgery, Division of Minimally Invasive and Bariatric Surgery, 
Hazrat-e Fatemeh Hospital, School of Medicine, Iran University of Medical 
Sciences, Tehran, Iran.
(13)St James University Hospital, Leeds, United Kingdom.
(14)School of Medicine, University of Buckingham, Buckingham, United Kingdom.
(15)WHO Collaborating Centre for Public Health Education and Training, Imperial 
College London, London, United Kingdom.
(16)Kingsmill Hospital, Nottingham, United Kingdom.
(17)Department of General Surgery, Betsi Cadwaladr University Health Board, 
Wales, United Kingdom.
(18)Bart's Health NHS Trust, London, United Kingdom.
(19)Director, University Emergency Department, Aristotle University of 
Thessaloniki, Thessaloniki, Greece.
(20)ARTORG Center for Biomedical Engineering Research, AI in Health and 
Nutrition, University of Bern, Bern, Switzerland.
(21)Department of Emergency Medicine, Inselspital University Hospital of Bern, 
Bern, Switzerland.
(22)Department of Emergency Medicine, HFR Fribourg-Cantonal Hospital, 
Villars-sur-Glâne, Switzerland.
(#)Contributed equally

OBJECTIVES: This narrative review evaluates the role of artificial intelligence 
(AI) in healthcare, summarizing its historical evolution, current applications 
across medical and surgical specialties, and implications for allied health 
professions and biomedical research.
METHODS: We conducted a structured literature search in Ovid MEDLINE (2018-2025) 
using terms related to AI, machine learning, deep learning, large language 
models, generative AI, and healthcare applications. Priority was given to 
peer-reviewed articles providing novel insights, multidisciplinary perspectives, 
and coverage of underrepresented domains.
KEY FINDINGS: AI is increasingly applied to diagnostics, surgical navigation, 
risk prediction, and personalized medicine. It also holds promise in allied 
health, drug discovery, genomics, and clinical trial optimization. However, 
adoption remains limited by challenges including bias, interpretability, legal 
frameworks, and uneven global access.
CONTRIBUTIONS: This review highlights underexplored areas such as generative AI 
and allied health professions, providing an integrated multidisciplinary 
perspective.
CONCLUSIONS: With careful regulation, clinician-led design, and global equity 
considerations, AI can augment healthcare delivery and research. Future work 
must focus on robust validation, responsible implementation, and expanding 
education in digital medicine.

© 2025 Mohajer-Bastami, Moin, Ahmad, Ahmed, Pouwels, Hajibandeh, Yang, Parmar, 
Kermansaravi, Khalil, Khalid, Khamise, Rawaf, Hosseini, Agarwal, Lala, Ahmed, 
Patel, Fyntanidou, Egan, Mougiakakou, Jakob, Ribordy, Hautz and Exadaktylos.

DOI: 10.3389/fdgth.2025.1644041
PMCID: PMC12645148
PMID: 41306935

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


552. IEEE J Biomed Health Inform. 2025 Jun 30;PP. doi: 10.1109/JBHI.2025.3584179. 
Online ahead of print.

Precision and Personalization: How Large Language Models Redefining Diagnostic 
Accuracy in Personalized Medicine - A Systematic Literature Review.

Aththanagoda AKNL, Kulathilake KASH, Abdullah NA.

Personalized medicine aims to tailor medical treatments to the unique 
characteristics of each patient, but its effectiveness relies on achieving 
diagnostic accuracy to fully understand individual variability in disease 
response and treatment efficacy. This systematic literature review explores the 
role of large language models (LLMs) in enhancing diagnostic precision and 
supporting the advancement of personalized medicine. A comprehensive search was 
conducted across Web of Science, Science Direct, Scopus, and IEEE Xplore, 
targeting peer-reviewed articles published in English between January 2020 and 
March 2025 that applied LLMs within personalized medicine contexts. Following 
PRISMA guidelines, 39 relevant studies were selected and systematically 
analyzed. The findings indicate a growing integration of LLMs across key domains 
such as clinical informatics, medical imaging, patient-specific diagnosis, and 
clinical decision support. LLMs have shown potential in uncovering subtle data 
patterns critical for accurate diagnosis and personalized treatment planning. 
This review highlights the expanding role of LLMs in improving diagnostic 
accuracy in personalized medicine, offering insights into their performance, 
applications, and challenges, while also acknowledging limitations in 
generalizability due to variable model performance and dataset biases. The 
review highlights the importance of addressing challenges related to data 
privacy, model interpretability, and reliability across diverse clinical 
scenarios. For successful clinical integration, future research must focus on 
refining LLM technologies, ensuring ethical standards, and validating models 
continuously to safeguard effective and responsible use in healthcare 
environments.

DOI: 10.1109/JBHI.2025.3584179
PMID: 40587361


553. BMC Med Educ. 2025 Apr 25;25(1):611. doi: 10.1186/s12909-025-07202-x.

AI-generated questions for urological competency assessment: a prospective 
educational study.

Başaranoğlu M(1), Akbay E(2), Erdem E(2).

Author information:
(1);Department of Urology, Mersin University Faculty of Medicine, Mersin, 
Turkey. mertbasaranoglu@gmail.com.
(2);Department of Urology, Mersin University Faculty of Medicine, Mersin, 
Turkey.

BACKGROUND: The integration of artificial intelligence (AI) in medical education 
assessment remains largely unexplored, particularly in specialty-specific 
evaluations during clinical rotations. Traditional question development methods 
are time-intensive and often struggle to keep pace with evolving medical 
knowledge. This study evaluated the effectiveness of AI-generated questions in 
assessing urological competency among medical interns during a standardized 
clinical rotation.
METHODS: Two state-of-the-art AI language models (ChatGPT and Gemini) generated 
300 multiple-choice questions across six urological subspecialties. Seven 
experienced urologists, each with over 10 years of clinical practice and active 
involvement in resident training programs, independently evaluated the questions 
using a modified Delphi approach with standardized scoring rubrics. The 
evaluation criteria encompassed technical accuracy based on current clinical 
guidelines, clinical relevance to core rotation objectives, construct validity 
assessed through cognitive task analysis, and alignment with rotation 
objectives. Questions achieving consensus approval from at least five experts 
were retained, resulting in 100 validated questions.
RESULTS: From the initial cohort of 45 eligible interns, 42 completed all three 
assessment points (93.3% completion rate). Performance improved significantly 
from baseline (mean: 45.2%, 95% CI: 42.6-47.8%) through mid-rotation (mean: 
62.8%, 95% CI: 60.4-65.2%) to final assessment (mean: 78.4%, 95% CI: 
76.5-80.3%). The technical accuracy was comparable between AI platforms 
(ChatGPT: 84.3%, Gemini: 83.8%, p = 0.86). Clinical scenario questions 
demonstrated better discrimination than recall questions (mean indices: 0.28 vs. 
0.14, p < 0.001). Subspecialty performance varied, with highest scores in 
uro-oncology (mean: 82.6%, 95% CI: 80.2-85.0%) and endourology (mean: 79.4%, 95% 
CI: 77.0-81.8%).
CONCLUSIONS: AI-generated questions showed appropriate technical accuracy and 
difficulty levels for assessing clinical competency in urology. While promising 
for formative assessment, particularly with clinical scenarios, current 
limitations in discrimination capability suggest careful consideration for 
high-stakes testing. The strong correlation between clinical exposure and 
improved performance validates their effectiveness in measuring knowledge 
acquisition. These findings support the potential integration of AI-generated 
questions in specialty-specific assessment, though careful implementation with 
expert oversight and continuous validation remains essential.
CLINICAL TRIAL NUMBER: Not applicable.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-07202-x
PMCID: PMC12032797
PMID: 40281621 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was conducted in accordance with the principles of the 
Declaration of Helsinki. The ethics committee of Mersin University Faculty of 
Medicine reviewed and approved this study (Ethics Committee Meeting Date: 
19.02.2025, Decision No: 181). All participants provided written informed 
consent before participation. Consent for publication: Not applicable as no 
individual person’s data was included. Competing interests: The authors declare 
no competing interests.


554. Curr Opin Psychol. 2026 Feb;67:102216. doi: 10.1016/j.copsyc.2025.102216. Epub 
2025 Nov 13.

Using conversational AI to reduce science skepticism.

Hornsey MJ(1), Smith AE(2), Pearson S(2), Bretter C(2), Nylund JL(2).

Author information:
(1)The University of Queensland, Queensland, Australia. Electronic address: 
m.hornsey@uq.edu.au.
(2)The University of Queensland, Queensland, Australia.

Mistrust of the scientific consensus around issues such as climate change and 
vaccination is mainstream, compromising our ability to respond to existential 
global threats. In the wrong hands, Generative AI can spread misinformation with 
unprecedented scale and psychological sophistication. However, large language 
models (LLMs) have also shown considerable promise for reducing misinformation 
and conspiracy theories, potentially revolutionizing science communication. This 
review summarizes the rapidly evolving frontier of empirical research on how 
conversational AI such as ChatGPT can be used to defuse mistrust of science 
around hot-button scientific issues. These studies find negligible evidence that 
LLM responds to human queries by reproducing conspiracy theories or 
misinformation about scientific topics. Rather, conversations with LLMs 
typically reduce participants' levels of science skepticism and misinformation 
endorsement. We conclude that LLMs (in their current form) have potential to 
complement existing science communication strategies, provided their use is 
accompanied by safeguards that preserve informational integrity and public 
trust.

Copyright © 2025 The Author(s). Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.copsyc.2025.102216
PMID: 41308550 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


555. Genome Med. 2025 Jun 6;17(1):67. doi: 10.1186/s13073-025-01496-8.

PhenoDP: leveraging deep learning for phenotype-based case reporting, disease 
ranking, and symptom recommendation.

Wen B(1), Shi S(1), Long Y(2), Dang Y(1), Tian W(3)(4)(5).

Author information:
(1)State Key Laboratory of Genetics and Development of Complex Phenotypes, 
Department of Computational Biology, School of Life Sciences, Fudan University, 
2005 Songhu Road, Shanghai, 200438, China.
(2)School of Medicine, Nankai University, Tianjin, 300071, China.
(3)State Key Laboratory of Genetics and Development of Complex Phenotypes, 
Department of Computational Biology, School of Life Sciences, Fudan University, 
2005 Songhu Road, Shanghai, 200438, China. weidong.tian@fudan.edu.cn.
(4)Children's Hospital of Fudan University, Shanghai, 201102, China. 
weidong.tian@fudan.edu.cn.
(5)Children's Hospital of Shandong University, Jinan, Shandong, 250022, China. 
weidong.tian@fudan.edu.cn.

BACKGROUND: Current phenotype-based diagnostic tools often struggle with 
accurate disease prioritization due to incomplete phenotypic data and the 
complexity of rare disease presentations. Additionally, they lack the ability to 
generate patient-centered clinical insights or recommend further symptoms for 
differential diagnosis.
METHODS: We developed PhenoDP, a deep learning-based toolkit with three modules: 
Summarizer, Ranker, and Recommender. The Summarizer fine-tuned a distilled large 
language model to create clinical summaries from a patient's Human Phenotype 
Ontology (HPO) terms. The Ranker prioritizes diseases by combining information 
content-based, phi-based, and semantic-based similarity measures. The 
Recommender employs contrastive learning to recommend additional HPO terms for 
enhanced diagnostic accuracy.
RESULTS: PhenoDP's Summarizer produces more clinically coherent and 
patient-centered summaries than the general-purpose language model FlanT5. The 
Ranker achieves state-of-the-art diagnostic performance, consistently 
outperforming existing phenotype-based methods across both simulated and 
real-world datasets. The Recommender also outperformed GPT-4o and PhenoTips in 
improving diagnostic accuracy when its suggested terms were incorporated into 
different ranking pipelines.
CONCLUSIONS: PhenoDP enhances Mendelian disease diagnosis through deep learning, 
offering precise summarization, ranking, and symptom recommendation. Its 
superior performance and open-source design make it a valuable clinical tool, 
with potential to accelerate diagnosis and improve patient outcomes. PhenoDP is 
freely available at https://github.com/TianLab-Bioinfo/PhenoDP .

© 2025. The Author(s).

DOI: 10.1186/s13073-025-01496-8
PMCID: PMC12143081
PMID: 40481598 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


556. World Psychiatry. 2025 Oct;24(3):383-394. doi: 10.1002/wps.21352.

Charting the evolution of artificial intelligence mental health chatbots from 
rule-based systems to large language models: a systematic review.

Hua Y(1)(2), Siddals S(2), Ma Z(3), Galatzer-Levy I(4)(5), Xia W(2), Hau C(2), 
Na H(6), Flathers M(2), Linardon J(7), Ayubcha C(7), Torous J(2).

Author information:
(1)Department of Epidemiology, Harvard T.H. Chan School of Public Health, 
Boston, MA, USA.
(2)Beth Israel Deaconess Medical Center, Harvard Medical School, Boston, MA, 
USA.
(3)Intelligent Interactive Systems Group, Harvard School of Engineering and 
Applied Sciences, Allston, MA, USA.
(4)Department of Psychiatry, New York University Grossman School of Medicine, 
New York, NY, USA.
(5)Google Research, New York, NY, USA.
(6)Australian Artificial Intelligence Institute, University of Technology 
Sydney, Sydney, NSW, Australia.
(7)SEED Lifespan Strategic Research Centre, School of Psychology, Faculty of 
Health, Deakin University, Geelong, VIC, Australia.

The rapid evolution of artificial intelligence (AI) chatbots in mental health 
care presents a fragmented landscape with variable clinical evidence and 
evaluation rigor. This systematic review of 160 studies (2020-2024) classifies 
chatbot architectures - rule-based, machine learning-based, and large language 
model (LLM)-based - and proposes a three-tier evaluation framework: foundational 
bench testing (technical validation), pilot feasibility testing (user 
engagement), and clinical efficacy testing (symptom reduction). While rule-based 
systems dominated until 2023, LLM-based chatbots surged to 45% of new studies in 
2024. However, only 16% of LLM studies underwent clinical efficacy testing, with 
most (77%) still in early validation. Overall, only 47% of studies focused on 
clinical efficacy testing, exposing a critical gap in robust validation of 
therapeutic benefit. Discrepancies emerged between marketed claims 
("AI-powered") and actual AI architectures, with many interventions relying on 
simple rule-based scripts. LLM-based chatbots are increasingly studied for 
emotional support and psychoeducation, yet they pose unique ethical concerns, 
including incorrect responses, privacy risks, and unverified therapeutic 
effects. Despite their generative capabilities, LLMs remain largely untested in 
high-stakes mental health contexts. This paper emphasizes the need for 
standardized evaluation and benchmarking aligned with medical AI certification 
to ensure safe, transparent and ethical deployment. The proposed framework 
enables clearer distinctions between technical novelty and clinical efficacy, 
offering clinicians, researchers and regulators ordered steps to guide future 
standards and benchmarks. To ensure that AI chatbots enhance mental health care, 
future research must prioritize rigorous clinical efficacy trials, transparent 
architecture reporting, and evaluations that reflect real-world impact rather 
than the well-known potential.

© 2025 World Psychiatric Association.

DOI: 10.1002/wps.21352
PMCID: PMC12434366
PMID: 40948070


557. J Med Internet Res. 2025 Oct 10;27:e79156. doi: 10.2196/79156.

Mentalizing Without a Mind: Psychotherapeutic Potential of Generative AI.

Yirmiya K(1)(2)(3), Fonagy P(1).

Author information:
(1)Clinical, Educational and Health Psychology, University College London, 
London, United Kingdom.
(2)Department of Psychology, Ben-Gurion University of the Negev, Be'er Sheva, 
Israel.
(3)Anna Freud Centre, London, United Kingdom.

This paper explores the integration of generative artificial intelligence (AI) 
into psychotherapeutic practice through the lens of mentalization theory, with a 
particular focus on epistemic trust-a critical relational mechanism that 
facilitates psychological change. We critically examine AI's capability to 
replicate core therapeutic components, such as empathy, embodied mentalizing, 
biobehavioral synchrony, and reciprocal mentalizing. Although current AI 
systems, especially large language models, demonstrate significant potential in 
simulating emotional responsiveness, cognitive empathy, and therapeutic 
dialogue, fundamental limitations persist. AI's inherent lack of genuine 
emotional presence, reciprocal intentionality, and affective commitment 
constrains its ability to foster authentic epistemic trust and meaningful 
therapeutic relationships. Additionally, we outline significant risks, notably 
for individuals with complex trauma or relational vulnerabilities, highlighting 
concerns regarding pseudo-empathy, mistaking phenomenal experience for objective 
reality (psychic equivalence), fruitless ungrounded pursuit of social 
understanding (hypermentalization), and epistemic exploitation of individuals in 
whom artificial understanding by AI triggers excessive credulity. Nonetheless, 
we propose ethically informed pathways for integrating AI to enhance clinical 
practice, therapist training, and client care, particularly in augmenting human 
capacities within group and adjunctive therapy contexts. Paradoxically, AI could 
support psychotherapists in improving their capacity to mentalize, improve their 
understanding of their clients, and provide such understanding within the moral 
constraints that normally govern their work. This paper calls for careful 
ethical regulation similar to that limiting genetic manipulation, 
interdisciplinary research, and clinician involvement in shaping future AI-based 
psychotherapeutic models, emphasizing that AI's role should complement rather 
than replace the irreplaceable relational core of psychotherapy.

©Karen Yirmiya, Peter Fonagy. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org), 10.10.2025.

DOI: 10.2196/79156
PMCID: PMC12552810
PMID: 41071597 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


558. J Med Internet Res. 2025 Dec 8;27:e77110. doi: 10.2196/77110.

Critical Appraisal Tools for Evaluating Artificial Intelligence in Clinical 
Studies: Scoping Review.

Cabello JB(1), Ruiz Garcia V(2), Torralba M(3), Maldonado Fernandez M(4), Ubeda 
M(5), Ansuategui E(6), Ramos-Ruperto L(7), Emparanza JI(8), Urreta I(8), 
Iglesias MT(8), Pijoan JI(9), Burls A(10).

Author information:
(1)Critical Appraisal Skills Program Spain, C/ Enriqueta Elizaizin, 2, E 5, 7C, 
Alicante, 03007, Spain, 34 619669243.
(2)Unidad de Hospitalización a Domicilio, Hospital Universitari i Politècnic La 
Fe, Valencia, Spain.
(3)Servicio de Medicina Interna, Hospital Universitario de Guadalajara, 
Guadalajara, Spain.
(4)Department of ENT, Hospital Vital Alvarez Buylla, Mieres, Spain.
(5)Hospital Donostia, Donostia - San Sebastian, Spain.
(6)Biblioteca virtual de salud de Euskadi, Vitoria, Spain.
(7)Unidad de VIH, Medicina Interna, Hospital Universitario La Paz, Madrid, 
Spain.
(8)Unidad de Epidemiologia Clínica e Investigación, CIBER-SP, Hospital 
Universitario Donostia, San Sebastian, Spain.
(9)Instituto de Investigación Sanitaria Biobizkaia-Hospital Universitario 
Cruces, Bizkaia, Baracaldo, Spain.
(10)City St George's, University of London, London, United Kingdom.

BACKGROUND: Health research that uses predictive and generative artificial 
intelligence (AI) is rapidly growing. As in traditional clinical studies, the 
way in which AI studies are conducted can introduce systematic errors. The 
translation of this AI evidence into clinical practice and research needs 
critical appraisal tools for clinical decision-makers and researchers.
OBJECTIVE: This study aimed to identify existing tools for the critical 
appraisal of clinical studies that use AI and to examine the concepts and 
domains these tools explore. The research question was framed using the 
Population-Concept-Context (PCC) framework. Population (P): AI clinical studies; 
Concept (C): tools for critical appraisal and associated constructs such as 
quality, reporting, validity, risk of bias, and applicability; and context (C): 
clinical practice. In addition, studies on bias classification and chatbot 
assessment were included.
METHODS: We searched medical and engineering databases (MEDLINE, Embase, CINAHL, 
PsycINFO, and IEEE) from inception to April 2024. We included clinical primary 
research with tools for critical appraisal. Classical reviews and systematic 
reviews were included in the first phase of screening and excluded in the 
secondary phase after identifying new tools by forward snowballing. We excluded 
nonhuman, computer, and mathematical research, and letters, opinion papers, and 
editorials. We used Rayyan (Qatar Computing Research Institute) for screening. 
Data extraction was done by two reviewers, and discrepancies were resolved 
through discussion. The protocol was previously registered in Open Science 
Framework. We adhered to the PRISMA-ScR (Preferred Reporting Items for 
Systematic reviews and Meta-Analyses extension for Scoping Reviews) and the 
PRISMA-S (PRISMA-Search) extension for reporting literature in systematic 
reviews.
RESULTS: We retrieved 4393 unique records for screening. After excluding 3803 
records, 119 were selected for full-text screening. From these, 59 were 
excluded. After inclusion of 10 studies via other methods, a total of 70 records 
were finally included. We found 46 tools (26 guides for reporting AI studies, 16 
tools for critical appraisal, 2 for study quality, and 2 for risk of bias). Nine 
papers focused on bias classification or mitigation. We found 15 chatbot 
assessment studies or systematic reviews of chatbot studies (6 and 9, 
respectively), which are a very heterogeneous group.
CONCLUSIONS: The results picture a landscape of evidence tools where reporting 
tools predominate, followed by critical appraisal, and a few tools for risk of 
bias. The mismatch of bias in AI and epidemiology should be considered for 
critical appraisal, especially regarding fairness and bias mitigation in AI. 
Finally, chatbot assessment studies represent a vast and evolving field in which 
progress in design, reporting, and critical appraisal is necessary and urgent.

© Juan B Cabello, Vicente Ruiz Garcia, Miguel Torralba, Miguel Maldonado 
Fernandez, Marimar Ubeda, Eukene Ansuategui, Luis Ramos-Ruperto, Jose I 
Emparanza, Iratxe Urreta, Maria Teresa Iglesias, Jose I Pijoan, Amanda Burls. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/77110
PMCID: PMC12685289
PMID: 41359958 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


559. JMIR Form Res. 2025 Nov 24;9:e71538. doi: 10.2196/71538.

Chatbots' Empathetic Conversations and Responses: A Qualitative Study of 
Help‑Seeking Queries on Depressive Moods Across 8 Commercial Conversational 
Agents.

Chin H(#)(1), Baek G(#)(2), Cha C(3), Cha M(4)(5).

Author information:
(1)Department of Computer Science and Engineering, Gyeongsang National 
University, Jinju, Republic of Korea.
(2)College of Nursing, System Health Science and Engineering Program, Ewha 
Womans University, Seoul, Republic of Korea.
(3)College of Nursing, Ewha Research Institute of Nursing Science, System Health 
and Engineering Major in Graduate School, Ewha Womans University, Seoul, 
Republic of Korea.
(4)Max Planck Institute for Security and Privacy, Universitätsstraße 140, 
Bochum, 44799, Germany, 49 55-772-1387, 49 55-772-3329.
(5)School of Computing, Korea Advanced Institute of Science and Technology, 
Daejeon, Republic of Korea.
(#)Contributed equally

BACKGROUND: While recent studies showed the potential of conversational agents 
(CAs) to help alleviate depressive moods, the dynamics of user-chatbot 
interactions in mental health support remain underexplored.
OBJECTIVE: We examine real-world conversations between users and chatbots on 
depression-related topics to identify patterns in how users seek help and how 
chatbots provide therapeutic support. We analyzed the responses of 8 commercial 
chatbots to user queries about depressive moods, examining whether they 
incorporated therapeutic communication techniques, such as empathy.
METHODS: Our method has 2 parts. First, we analyzed 13,700 utterances (6850 user 
queries and 6850 responses) about depressive moods from the commercial chatbot 
SimSimi, covering 5 English-speaking countries between 2016 and 2021. Using a 
human-annotated coding approach, we classified user queries into 5 groups based 
on Rickwood's help-seeking model and classified chatbot responses into 8 
therapeutic communication styles. Empathy was assessed as one of these styles, 
with responses coded as empathetic when they demonstrated emotional 
understanding, validation, and reflection. Next, we evaluated the responses of 3 
voice assistants (Amazon's Alexa, Google Assistant, and Apple's Siri) and 5 
chatbots (ChatGPT, Replika, Woebot, Wysa, and SimSimi) to user queries about 
depressive moods.
RESULTS: In study 1, we examined how SimSimi, a social chatbot trained to 
encourage users to share their emotions and build rapport, responded to user 
queries. The majority (3067/4073, 75.3%) indicated depressed feelings, and a 
smaller portion (168/4073, 4.1%) sought strategies to cope with depression. The 
chatbot's responses were largely therapeutic ( 2417/3108, 77.7%), demonstrating 
empathy (902/3108, 29%), active listening (836/3108, 26.9%), and open-ended 
questions (679/3108, 21.8%). In study 2, we qualitatively compared response 
patterns across commercial CAs, revealing that Replika expressed empathy in more 
than 75% (28/36) of its responses, similar to SimSimi. In contrast, Alexa 
(15/17, 88.2%), Google Assistant (18/30, 60%), Siri (20/36, 55.6%), and ChatGPT 
(40/42, 95.2%) typically responded to depression-related queries with search 
results rather than offering specific solutions for depressive feelings. Mental 
health chatbots such as Woebot responded to users with clarification questions 
(97.3%). We also report instances where CAs failed to meet users' help-seeking 
needs, instead giving irrelevant responses and ignoring emotional requests.
CONCLUSIONS: Our findings reveal a mixed landscape in the emotional support 
provided by CAs. While some social chatbots delivered empathetic responses that 
fostered deeper user engagement, most commercial chatbots offered merely 
informative replies to users' help-seeking inputs. Recognizing that users seek 
support from chatbots, we recommend equipping next-generation CAs with 
capabilities grounded in therapeutic communication, such as empathetic 
responses.

©Hyojin Chin, Gumhee Baek, Chiyoung Cha, Meeyoung Cha. Originally published in 
JMIR Formative Research (https://formative.jmir.org).

DOI: 10.2196/71538
PMCID: PMC12643404
PMID: 41284964 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


560. NPJ Digit Med. 2025 Jul 2;8(1):397. doi: 10.1038/s41746-025-01780-2.

Large language model trained on clinical oncology data predicts cancer 
progression.

Zhu M(#)(1), Lin H(#)(2), Jiang J(1), Jinia AJ(1), Jee J(1), Pichotta K(1), 
Waters M(1), Rose D(1), Schultz N(1), Chalise S(1), Valleru L(1), Morin O(2), 
Moran J(1), Deasy JO(1), Pilai S(1), Nichols C(1), Riely G(1), Braunstein LZ(3), 
Li A(4).

Author information:
(1)Memorial Sloan Kettering Cancer Center, New York, NY, USA.
(2)University of California San Francisco, San Francisco, CA, USA.
(3)Memorial Sloan Kettering Cancer Center, New York, NY, USA. 
braunstl@mskcc.org.
(4)Memorial Sloan Kettering Cancer Center, New York, NY, USA. lia5@mskcc.org.
(#)Contributed equally

Subspecialty knowledge barriers have limited the adoption of large language 
models (LLMs) in oncology. We introduce Woollie, an open-source, 
oncology-specific LLM trained on real-world data from Memorial Sloan Kettering 
Cancer Center (MSK) across lung, breast, prostate, pancreatic, and colorectal 
cancers, with external validation using University of California, San Francisco 
(UCSF) data. Woollie surpasses ChatGPT in medical benchmarks and excels in eight 
non-medical benchmarks. Analyzing 39,319 radiology impression notes from 4002 
patients, it achieved an overall area under the receiver operating 
characteristic curve (AUROC) of 0.97 for cancer progression prediction on MSK 
data, including a notable 0.98 AUROC for pancreatic cancer. On UCSF data, it 
achieved an overall AUROC of 0.88, excelling in lung cancer detection with an 
AUROC of 0.95. As the first oncology specific LLM validated across institutions, 
Woollie demonstrates high accuracy and consistency across cancer types, 
underscoring its potential to enhance cancer progression analysis.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01780-2
PMCID: PMC12223279
PMID: 40604229

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


561. Fam Pract. 2025 Aug 14;42(5):cmaf069. doi: 10.1093/fampra/cmaf069.

ChatGPT's performance in sample size estimation: a preliminary study on the 
capabilities of artificial intelligence.

Sebo P(1), Wang T(2).

Author information:
(1)University Institute for Primary Care (IuMFE), University of Geneva, 1211 
Geneva, Switzerland.
(2)School of Library and Information Management, Emporia State University, 
Emporia, KS 66801, United States.

BACKGROUND: Artificial intelligence tools, including large language models such 
as ChatGPT, are increasingly integrated into clinical and primary care research. 
However, their ability to assist with specialized statistical tasks, such as 
sample size estimation, remains largely unexplored.
METHODS: We evaluated the accuracy and reproducibility of ChatGPT-4.0 and 
ChatGPT-4o in estimating sample sizes across 24 standard statistical scenarios. 
Examples were selected from a statistical textbook and an educational website, 
covering basic methods such as estimating means, proportions, and correlations. 
Each example was tested twice per model. Models were accessed through the 
ChatGPT web interface, with a new independent chat session initiated for each 
round. Accuracy was assessed using mean and median absolute percentage error 
compared with validated reference values. Reproducibility was assessed using 
symmetric mean and median absolute percentage error between rounds. Comparisons 
were performed using Wilcoxon signed-rank tests.
RESULTS: For ChatGPT-4.0 and ChatGPT-4o, absolute percentage errors ranged from 
0% to 15.2% (except one case: 26.3%) and 0% to 14.3%, respectively, with most 
examples showing errors below 5%. ChatGPT-4o showed better accuracy than 
ChatGPT-4.0 (mean absolute percentage error: 3.1% vs. 4.1% in round#1, P-value = 
.01; 2.8% vs. 5.1% in round#2, P-value =.02) and lower symmetric mean absolute 
percentage error (0.8% vs. 2.5%), though not significant (P-value = .18).
CONCLUSIONS: ChatGPT-4.0 and ChatGPT-4o provided reasonably accurate sample size 
estimates across standard scenarios, with good reproducibility. However, 
inconsistencies were observed, underscoring the need for cautious interpretation 
and expert validation. Further research should assess performance in more 
complex contexts and across a broader range of AI models.

© The Author(s) 2025. Published by Oxford University Press.

DOI: 10.1093/fampra/cmaf069
PMCID: PMC12411907
PMID: 40910515 [Indexed for MEDLINE]

Conflict of interest statement: The author has declared that no competing 
interests exist.


562. NPJ Digit Med. 2025 Oct 24;8(1):628. doi: 10.1038/s41746-025-02015-0.

Artificial intelligence for autoimmune diseases.

Mahajan A(1), LaChance AH(1)(2), Rodman A(1)(3), Chung L(#)(4), Powell D(#)(5).

Author information:
(1)Harvard Medical School, Boston, MA, USA.
(2)Brigham and Women's Hospital, Boston, MA, USA.
(3)Beth Israel Deaconess Medical Center, Boston, MA, USA.
(4)Department of Rheumatology, Stanford University School of Medicine, Palo 
Alto, CA, USA.
(5)Faculty of Health Sciences & Sport, University of Stirling, Stirling, UK. 
dylan.powell@stir.ac.uk.
(#)Contributed equally

Emerging evidence suggests generative artificial intelligence (AI) may offer 
potential for autoimmune and rheumatic disease care, moving beyond traditional 
narrow AI applications to produce contextualized clinical content to support a 
wide spectrum of medical tasks. This article explores generative AI applications 
across autoimmune and rheumatologic clinical care, research, and administrative 
domains. However, significant implementation challenges remain, including 
clinical validation, model interpretability, data integration complexities, and 
evolving regulatory frameworks.

DOI: 10.1038/s41746-025-02015-0
PMCID: PMC12552663
PMID: 41136754

Conflict of interest statement: Competing interests: The authors declare no 
competing interests. D.P. is News & Views editor at npj Digital Medicine but 
played no role in the internal review or decision to publish this News & Views 
piece.


563. Artif Intell Med. 2025 Oct;168:103220. doi: 10.1016/j.artmed.2025.103220. Epub 
2025 Jul 19.

Medical radiology report generation: A systematic review of current deep 
learning methods, trends, and future directions.

Izhar A(1), Idris N(2), Japar N(3).

Author information:
(1)Faculty of Computer Science and Information Technology, Universiti Malaya, 
Kuala Lumpur, 50603, Malaysia. Electronic address: 22086758@siswa.um.edu.my.
(2)Faculty of Computer Science and Information Technology, Universiti Malaya, 
Kuala Lumpur, 50603, Malaysia. Electronic address: norisma@um.edu.my.
(3)Faculty of Computer Science and Information Technology, Universiti Malaya, 
Kuala Lumpur, 50603, Malaysia. Electronic address: nuruljapar@um.edu.my.

Medical radiology reports play a crucial role in diagnosing various diseases, 
yet generating them manually is time-consuming and burdens clinical workflows. 
Medical radiology report generation aims to automate this process using deep 
learning to assist radiologists and reduce patient wait times. This study 
presents the most comprehensive systematic review to date on deep learning-based 
MRRG, encompassing recent advances that span traditional architectures to large 
language models. We focus on available datasets, modeling approaches, and 
evaluation practices. Following PRISMA guidelines, we retrieved 323 articles 
from major academic databases and included 78 studies after eligibility 
screening. We critically analyze key components such as model architectures, 
loss functions, datasets, evaluation metrics, and optimizers - identifying 22 
widely used datasets, 14 evaluation metrics, around 20 loss functions, over 25 
visual backbones, and more than 30 textual backbones. To support reproducibility 
and accelerate future research, we also compile links to modern models, 
toolkits, and pretrained resources. Our findings provide technical insights and 
outline future directions to address current limitations, promoting 
collaboration at the intersection of medical imaging, natural language 
processing, and deep learning to advance trustworthy AI systems in radiology.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.artmed.2025.103220
PMID: 40700862 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


564. Med Biol Eng Comput. 2025 May;63(5):1249-1270. doi: 10.1007/s11517-024-03265-y. 
Epub 2024 Dec 21.

Advancement in medical report generation: current practices, challenges, and 
future directions.

Rehman M(1), Shafi I(1), Ahmad J(2), Garcia CO(3)(4)(5), Barrera AEP(3)(4)(6), 
Ashraf I(7).

Author information:
(1)College of Electrical and Mechanical Engineering, National University of 
Sciences and Technology (NUST), Islamabad, 44000, Pakistan.
(2)Department of Computing, Abasyn University, Islamabad Campus, Islamabad, 
44000, Pakistan.
(3)Universidad Europea del Atlantico, Isabel Torres 21, Santander, 39011, Spain.
(4)Universidad Internacional Iberoamericana, Campeche, 24560, Mexico.
(5)Universidade Internacional do Cuanza, Cuito, Bie, Angola.
(6)Universidad de La Romana, La Romana, Dominican Republic.
(7)Department of Information and Communication Engineering, Yeungnam University, 
Gyeongsan, 38541, Republic of Korea. imranashraf@ynu.ac.kr.

The correct analysis of medical images requires the medical knowledge and 
expertise of radiologists to understand, clarify, and explain complex patterns 
and diagnose diseases. After analyzing, radiologists write detailed and 
well-structured reports that contribute to the precise and timely diagnosis of 
patients. However, manually writing reports is often expensive and 
time-consuming, and it is difficult for radiologists to analyze medical images, 
particularly images with multiple views and perceptions. It is challenging to 
accurately diagnose diseases, and many methods are proposed to help 
radiologists, both traditional and deep learning-based. Automatic report 
generation is widely used to tackle this issue as it streamlines the process and 
lessens the burden of manual labeling of images. This paper introduces a 
systematic literature review with a focus on analyses and evaluating existing 
research on medical report generation. This SLR follows a proper protocol for 
the planning, reviewing, and reporting of the results. This review recognizes 
that the most commonly used deep learning models are encoder-decoder frameworks 
(45 articles), which provide an accuracy of around 92-95%. Transformers-based 
models (20 articles) are the second most established method and achieve an 
accuracy of around 91%. The remaining articles explored in this SLR are 
attention mechanisms (10), RNN-LSTM (10), Large language models (LLM-10), and 
graph-based methods (4) with promising results. However, these methods also face 
certain limitations such as overfitting, risk of bias, and high data dependency 
that impact their performance. The review not only highlights the strengths and 
challenges of these methods but also suggests ways to handle them in the future 
to increase the accuracy and timely generation of medical reports. The goal of 
this review is to direct radiologists toward methods that lessen their workload 
and provide precise medical diagnoses.

© 2024. International Federation for Medical and Biological Engineering.

DOI: 10.1007/s11517-024-03265-y
PMID: 39707049 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare no competing interests.


565. JMIR Form Res. 2025 Jul 8;9:e71358. doi: 10.2196/71358.

Assessment of Recommendations Provided to Athletes Regarding Sleep Education by 
GPT-4o and Google Gemini: Comparative Evaluation Study.

Masur L(1), Driller M(2), Suppiah H(2), Matzka M(3), Sperlich B(3), Düking P(1).

Author information:
(1)Department of Sports Science and Movement Pedagogy, Technische Universität 
Braunschweig, Pockelsstraße 11, Braunschweig, 38106, Germany, 49 531 391 3432.
(2)School of Allied Health, Human Services, and Sport, La Trobe University, 
Melbourne, Australia.
(3)Department of Sports Science, Integrative and Experimental Exercise Science & 
Training, University of Würzburg, Würzburg, Germany.

BACKGROUND: Inadequate sleep is prevalent among athletes, affecting adaptation 
to training and performance. While education on factors influencing sleep can 
improve sleep behaviors, large language models (LLMs) may offer a scalable 
approach to provide sleep education to athletes.
OBJECTIVE: This study aims (1) to investigate the quality of sleep 
recommendations generated by publicly available LLMs, as evaluated by 
experienced raters, and (2) to determine whether evaluation results vary with 
information input granularity.
METHODS: Two prompts with differing information input granularity (low and high) 
were created for 2 use cases and inserted into ChatGPT-4o (GPT-4o) and Google 
Gemini, resulting in 8 different recommendations. Experienced raters (n=13) 
evaluated the recommendations on a 1-5 Likert scale, based on 10 sleep criteria 
derived from recent literature. A Friedman test with Bonferroni correction was 
performed to test for significant differences in all rated items between the 
training plans. Significance level was set to P<.05. Fleiss κ was calculated to 
assess interrater reliability.
RESULTS: The overall interrater reliability using Fleiss κ indicated a fair 
agreement of 0.280 (range between 0.183 and 0.296). The highest summary rating 
was achieved by GPT-4o using high input information granularity, with 8 ratings 
>3 (tendency toward good), 3 ratings equal to 3 (neutral), and 2 ratings <3 
(tendency toward bad). GPT-4o outperformed Google Gemini in 9 of 10 criteria 
(P<.001 to P=.04). Recommendations generated with high input granularity 
received significantly higher ratings than those with low granularity across 
both LLMs and use cases (P<.001 to P=.049). High input granularity leads to 
significantly higher ratings in items pertaining to the used scientific sources 
(P<.001), irrespective of the analyzed LLM.
CONCLUSIONS: Both LLMs exhibit limitations, neglecting vital criteria of sleep 
education. Sleep recommendations by GPT-4o and Google Gemini were evaluated as 
suboptimal, with GPT-4o achieving higher overall ratings. However, both LLMs 
demonstrated improved recommendations with higher information input granularity, 
emphasizing the need for specificity and a thorough review of outputs to 
securely implement artificial intelligence technologies into sleep education.

© Lukas Masur, Matthew Driller, Haresh Suppiah, Manuel Matzka, Billy Sperlich, 
Peter Düking. Originally published in JMIR Formative Research 
(https://formative.jmir.org).

DOI: 10.2196/71358
PMCID: PMC12287982
PMID: 40627597 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


566. Digit Health. 2025 May 20;11:20552076251344385. doi: 10.1177/20552076251344385. 
eCollection 2025 Jan-Dec.

Ethical and privacy challenges of integrating generative AI into EHR systems in 
Tanzania: A scoping review with a policy perspective.

Mwogosi A(1).

Author information:
(1)Department of Information Systems and Technology, University of Dodoma, 
Dodoma, Tanzania.

OBJECTIVES: This study examines the ethical and privacy challenges of 
integrating generative artificial intelligence (AI) into electronic health 
record (EHR) systems, focusing on Tanzania's healthcare context. It critically 
analyses the extent to which Tanzania's Policy Framework for Artificial 
Intelligence in the Health Sector (2022) addresses these challenges and proposes 
regulatory and practical safeguards for responsible generative AI deployment.
METHODS: A systematic scoping review was conducted using PubMed, IEEE Xplore, 
Scopus and Google Scholar to identify relevant studies published between 2014 
and 2024. The Preferred Reporting Items for Systematic Reviews and Meta-Analyses 
extension for Scoping Reviews (PRISMA-ScR) guidelines informed the search and 
selection process. Fourteen studies met the inclusion criteria and were 
thematically analysed to identify key ethical and privacy concerns of generative 
AI in healthcare. Moreover, a policy analysis of Tanzania's AI framework was 
conducted to assess its alignment with global best practices and regulatory 
preparedness.
RESULTS: The review identified six key ethical and privacy challenges associated 
with generative AI in EHR systems: data privacy and security risks, algorithmic 
bias and fairness concerns, transparency and accountability issues, consent and 
autonomy challenges, human oversight gaps and risks of data re-identification. 
The policy analysis revealed that while Tanzania's AI framework aligns with 
national health priorities and promotes capacity building and ethical 
governance, it lacks generative AI-specific guidelines, regulatory clarity and 
resource mobilisation strategies necessary for healthcare settings.
CONCLUSION: Integrating generative AI into Tanzania's EHR systems presents 
transformative opportunities and significant ethical and privacy risks. 
Tanzania's policy framework should incorporate AI-specific ethical guidelines, 
operationalise regulatory mechanisms, foster stakeholder engagement through 
participatory co-design and strengthen infrastructural investments. These 
measures will promote ethical integrity, enhance patient trust and position 
Tanzania as a regional leader in responsible AI use in healthcare.

© The Author(s) 2025.

DOI: 10.1177/20552076251344385
PMCID: PMC12093014
PMID: 40400763

Conflict of interest statement: The author declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


567. PLoS One. 2026 Jan 2;21(1):e0339594. doi: 10.1371/journal.pone.0339594. 
eCollection 2026.

Protocol for a scoping review examining the application of large language models 
in healthcare education and public health learning spaces.

Ndukwe H(1), Otukpa EO(2).

Author information:
(1)School of Pharmacy and Medical Sciences, Griffith University, Brisbane, 
Australia.
(2)Health and Wellbeing Theme, African Population Health Research Center 
(APHRC), Nairobi, Kenya.

OBJECTIVE: Through this scoping review, we aim to explore and synthesize 
existing knowledge and evidence on the learning approaches for incorporating 
LLMs into healthcare education and public health research and learning spaces. 
Specifically, we will attempt to investigate methods for auditing prompts for 
accuracy, fairness, and effectiveness; tailoring prompts to improve 
task-specific accuracy and utility; and exploring how end-user feedback is used 
to refine and optimize LLM prompts over time. This review will provide a 
comprehensive understanding of how LLMs are being tailored and improved in these 
fields, contributing to the development of evidence-based strategies for their 
implementation. It will also identify areas for future research and innovation.
INTRODUCTION: The increasing integration of large language models (LLMs) into 
healthcare education and public health research and learning spaces, highlights 
their potential to revolutionize service delivery, decision-making, and 
ultimately patient care and outcomes. Despite these advancements, understanding 
how LLMs can be effectively tailored, audited, and refined for learning remains 
a critical area of inquiry. Key issues include, the accuracy of generated 
information, and their relevance to the medical and public health fields.
INCLUSION CRITERIA: Our focus will be on studies addressing LLM applications in 
healthcare education and public health research and learning spaces, prompt 
engineering techniques, prompt auditing methods, and processes geared towards 
integrating user feedback. Articles that do not focus on healthcare or public 
health contexts and lack relevance to LLM learning approaches will be excluded.
METHODS: The review is guided by the JBI methodology for scoping reviews 
complemented by updates from Levac et al. Databases including PubMed, Scopus, 
IEEE Xplore, and Web of Science will be searched for peer-reviewed articles, 
conference proceedings, and grey literature published in English and French from 
2015 to 2025. Data extraction will include information on study characteristics, 
LLM models, prompt engineering strategies, auditing methodologies, and user 
feedback mechanisms. We will synthesize to identify trends, gaps, and best 
practices in leveraging LLMs to generate baseline data for auditing prompts that 
optimize AI learning and education needs in the healthcare and public health 
sector.

Copyright: © 2026 Ndukwe, Otukpa. This is an open access article distributed 
under the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0339594
PMCID: PMC12758804
PMID: 41481763 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


568. Radiographics. 2025 Apr;45(4):e240073. doi: 10.1148/rg.240073.

Optimizing Large Language Models in Radiology and Mitigating Pitfalls: Prompt 
Engineering and Fine-tuning.

Kim TT(1), Makutonin M(1), Sirous R(1), Javan R(1).

Author information:
(1)From the Department of Radiology, George Washington University School of 
Medicine and Health Sciences, 2300 I St NW, Washington, DC 20052 (T.T.K., R.J.); 
Yale School of Medicine, New Haven, Conn (M.M.); and University of California 
San Francisco, San Francisco, Calif (R.S.).

Large language models (LLMs) such as generative pretrained transformers (GPTs) 
have had a major impact on society, and there is increasing interest in using 
these models for applications in medicine and radiology. This article presents 
techniques to optimize these models and describes their known challenges and 
limitations. Specifically, the authors explore how to best craft natural 
language prompts, a process known as prompt engineering, for these models to 
elicit more accurate and desirable responses. The authors also explain how 
fine-tuning is conducted, in which a more general model, such as GPT-4, is 
further trained on a more specific use case, such as summarizing clinical notes, 
to further improve reliability and relevance. Despite the enormous potential of 
these models, substantial challenges limit their widespread implementation. 
These tools differ substantially from traditional health technology in their 
complexity and their probabilistic and nondeterministic nature, and these 
differences lead to issues such as "hallucinations," biases, lack of 
reliability, and security risks. Therefore, the authors provide radiologists 
with baseline knowledge of the technology underpinning these models and an 
understanding of how to use them, in addition to exploring best practices in 
prompt engineering and fine-tuning. Also discussed are current proof-of-concept 
use cases of LLMs in the radiology literature, such as in clinical decision 
support and report generation, and the limitations preventing their current 
adoption in medicine and radiology. ©RSNA, 2025 See invited commentary by Chung 
and Mongan in this issue.

DOI: 10.1148/rg.240073
PMID: 40048389 [Indexed for MEDLINE]


569. Chin Med. 2025 Nov 17;20(1):191. doi: 10.1186/s13020-025-01249-0.

TCM-DS: a large language model for intelligent traditional Chinese medicine 
edible herbal formulas recommendations.

Li X(#)(1)(2), He H(#)(1)(3), Lu G(1)(2), Yue P(1)(3), Chen J(4), Yang Z(5), Hon 
C(6)(7).

Author information:
(1)Respiratory Disease AI Laboratory in Epidemic Intelligence and Applications 
of Medical Big Data Instruments, Faculty of Innovation Engineering, Macau 
University of Science and Technology, Macau, China.
(2)Institute of Systems Engineering, Macau University of Science and Technology, 
Macau, China.
(3)Faculty of Medicine, Macau University of Science and Technology, Macau, 
China.
(4)School of Software Engineering, South China University of Technology, 
Guangzhou, Guangdong, China.
(5)State Key Laboratory of Respiratory Disease, National Clinical Research 
Center for Respiratory Disease, Guangzhou Institute of Respiratory Health, The 
First Affiliated Hospital of Guangzhou Medical University, Guangzhou, Guangdong, 
China.
(6)Respiratory Disease AI Laboratory in Epidemic Intelligence and Applications 
of Medical Big Data Instruments, Faculty of Innovation Engineering, Macau 
University of Science and Technology, Macau, China. cthon@must.edu.mo.
(7)Institute of Systems Engineering, Macau University of Science and Technology, 
Macau, China. cthon@must.edu.mo.
(#)Contributed equally

BACKGROUND: The concept of medicine and food homology in traditional Chinese 
medicine (TCM) emphasized the dual role of certain material as both food and 
medicine, offering nutritional and therapeutic benefits. Edible herbal formulas, 
derived from this principle, are valuable for health management and chronic 
disease prevention.
METHODS: This study proposes a domain-specific prescription recommendation model 
enriched by TCM edible herbal formula knowledge called TCM-DS model. A dataset 
including symptoms, TCM constitutions, formulas and their corresponding 
ingredients was developed. DeepSeek R1 base model was fine-tuned utilizing 
Low-rank adaptation (LoRA) fine-tuning and a retrieval-augmented generation 
(RAG) module to increase recommendation accuracy. TCM-DS model was evaluated 
against general-purpose large language models.
RESULTS: The proposed TCM-DS model demonstrated superior performance, achieving 
a recommendation precision of 0.9924. Comparative experiments showed its 
robustness, with the highest precision scores for both forward and reverse 
symptom sequences compared with general-purpose large language models. A 
user-friendly platform was developed based on TCM-DS model, enabling automated 
constitution analysis and personalized formula recommendations.
CONCLUSIONS: In conclusion, we proposed an intelligent TCM edible herbal formula 
recommendation model called TCM-DS. Its accompanying platform automated 
constitution identification and formula recommendation, advancing intelligent 
applications in TCM practice.

© 2025. The Author(s).

DOI: 10.1186/s13020-025-01249-0
PMCID: PMC12621383
PMID: 41243110

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


570. Acad Radiol. 2025 Dec;32(12):7108-7118. doi: 10.1016/j.acra.2025.08.026. Epub 
2025 Sep 6.

Interpreting BI-RADS-Free Breast MRI Reports Using a Large Language Model: 
Automated BI-RADS Classification From Narrative Reports Using ChatGPT.

Tekcan Sanli DE(1), Sanli AN(2), Ozmen G(3), Ozmen A(3), Cihan I(3), Kurt A(3), 
Esmerer E(4).

Author information:
(1)Department of Radiology, Faculty of Medicine, Gaziantep University, 
Gaziantep, Turkey (D.E.T.S., G.O., A.O., I.C., A.K.). Electronic address: 
tekcandenizesin@gmail.com.
(2)Department of General Surgery, Abdulkadir Yüksel State Hospital, Gaziantep, 
Turkey (A.N.S.).
(3)Department of Radiology, Faculty of Medicine, Gaziantep University, 
Gaziantep, Turkey (D.E.T.S., G.O., A.O., I.C., A.K.).
(4)Department of Radiology, Başakşehir Çam and Sakura City Hospital, Istanbul, 
Turkey (E.E.).

PURPOSE: This study aimed to evaluate the performance of ChatGPT (GPT-4o) in 
interpreting free-text breast magnetic resonance imaging (MRI) reports by 
assigning BI-RADS categories and recommending appropriate clinical management 
steps in the absence of explicitly stated BI-RADS classifications.
METHODS: In this retrospective, single-center study, a total of 352 documented 
full-text breast MRI reports of at least one identifiable breast lesion with 
descriptive imaging findings between January 2024 and June 2025 were included in 
the study. Incomplete reports due to technical limitations, reports describing 
only normal findings, and MRI examinations performed at external institutions 
were excluded from the study. First, it was aimed to assess ChatGPT's ability to 
infer the correct BI-RADS category (2-3-4a-4b-4c-5 separately) based solely on 
the narrative imaging findings. Second, it was evaluated the model's ability to 
distinguish between benign versus suspicious/malignant imaging features in terms 
of clinical decision-making. Therefore, BI-RADS 2-3 categories were grouped as 
"benign," and BI-RADS 4-5 as "suspicious/malignant," in alignment with how 
BI-RADS categories are used to guide patient management, rather than to 
represent definitive diagnostic outcomes. Reports originally containing the term 
"BI-RADS" were manually de-identified by removing BI-RADS categories and 
clinical recommendations. Each narrative report was then processed through 
ChatGPT using two standardized prompts as follows: (1) What is the most 
appropriate BI-RADS category based on the findings in the report? (2) What 
should be the next clinical step (e.g., follow-up, biopsy)? Responses were 
evaluated in real time by two experienced breast radiologists, and consensus was 
used as the reference standard.
RESULTS: ChatGPT demonstrated moderate agreement with radiologists' consensus 
for BI-RADS classification (Cohen's Kappa (κ): 0.510, p<0.001). Classification 
accuracy was highest for BI-RADS 5 reports (77.9%), whereas lower agreement was 
observed in intermediate categories such as BI-RADS 3 (52.4% correct) and 4B 
(29.4% correct). In the binary classification of reports as benign or malignant, 
ChatGPT achieved almost perfect agreement (κ: 0.843), correctly identifying 
91.7% of benign and 93.2% of malignant reports. Notably, the model's management 
recommendations were 100% consistent with its assigned BI-RADS categories, 
advising biopsy for all BI-RADS 4-5 cases and short-interval follow-up or 
conditional biopsy for BI-RADS 3 reports.
CONCLUSION: ChatGPT accurately interprets unstructured breast MRI reports, 
particularly in benign/malignant discrimination and corresponding clinical 
recommendations. This technology holds potential as a decision support tool to 
standardize reporting and enhance clinical workflows, especially in settings 
with variable reporting practices. Prospective, multi-institutional studies are 
needed for further validation.

Copyright © 2025 The Association of University Radiologists. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.acra.2025.08.026
PMID: 40915935 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare no conflicts of interest.


571. Int J Surg. 2025 Mar 1;111(3):2722-2726. doi: 10.1097/JS9.0000000000002215.

AI-driven evidence synthesis: data extraction of randomized controlled trials 
with large language models.

Liu J(1)(2), Lai H(1)(2), Zhao W(1)(2), Huang J(3), Xia D(1)(2), Liu H(4), Luo 
X(4)(5)(6), Wang B(4), Pan B(4), Hou L(4)(7), Chen Y(4)(6)(8), Ge L(1)(2)(5); 
ADVANCED Working Group.

Author information:
(1)Department of Health Policy and Health Management, School of Public Health, 
Lanzhou University, Lanzhou, China.
(2)Evidence-Based Social Science Research Center, School of Public Health, 
Lanzhou University, Lanzhou, China.
(3)College of Nursing, Gansu University of Chinese Medicine, Lanzhou, China.
(4)Evidence-Based Medicine Center, School of Basic Medical Sciences, Lanzhou 
University, Lanzhou, China.
(5)Key Laboratory of Evidence-Based Medicine of Gansu Province, Lanzhou 
University, Lanzhou, China.
(6)Research Unit of Evidence-Based Evaluation and Guidelines, Chinese Academy of 
Medical Sciences (2021RU017), School of Basic Medical Sciences, Lanzhou 
University, Lanzhou, China.
(7)Department of Health Research Methods, Evidence, and Impact, McMaster 
University, Ontario, Canada.
(8)WHO Collaborating Center for Guideline Implementation and Knowledge 
Translation, Lanzhou, China.

The advancement of large language models (LLMs) presents promising opportunities 
to enhance evidence synthesis efficiency, particularly in data extraction 
processes, yet existing prompts for data extraction remain limited, focusing 
primarily on commonly used items without accommodating diverse extraction needs. 
This research letter developed structured prompts for LLMs and evaluated their 
feasibility in extracting data from randomized controlled trials (RCTs). Using 
Claude (Claude-2) as the platform, we designed comprehensive structured prompts 
comprising 58 items across six Cochrane Handbook domains and tested them on 10 
randomly selected RCTs from published Cochrane reviews. The results demonstrated 
high accuracy with an overall correct rate of 94.77% (95% CI: 93.66% to 95.73%), 
with domain-specific performance ranging from 77.97% to 100%. The extraction 
process proved efficient, requiring only 88 seconds per RCT. These findings 
substantiate the feasibility and potential value of LLMs in evidence synthesis 
when guided by structured prompts, marking a significant advancement in 
systematic review methodology.

Copyright © 2025 The Author(s). Published by Wolters Kluwer Health, Inc.

DOI: 10.1097/JS9.0000000000002215
PMCID: PMC12372713
PMID: 39903558 [Indexed for MEDLINE]

Conflict of interest statement: All the authors declare to have no conflicts of 
interest relevant to this study.


572. Acad Radiol. 2025 Oct;32(10):5693-5702. doi: 10.1016/j.acra.2025.07.046. Epub 
2025 Aug 7.

Structured Report Generation for Breast Cancer Imaging Based on Large Language 
Modeling: A Comparative Analysis of GPT-4 and DeepSeek.

Chen K(1), Hou X(2), Li X(3), Xu W(3), Yi H(4).

Author information:
(1)Department of Nuclear Medicine, Zhejiang Cancer Hospital, Hangzhou Institute 
of Medicine (HIM), Chinese Academy of Sciences, Hangzhou, Zhejiang 310022, China 
(K.C., H.Y.).
(2)Department of Medical Imaging, Tianjin Children's Hospital (Tianjin 
University Children's Hospital), Tianjin, China (X.H.).
(3)Department of Molecular Imaging and Nuclear Medicine, Tianjin Medical 
University Cancer Institute and Hospital, National Clinical Research Center for 
Cancer, Tianjin, China (X.L., W.X.).
(4)Department of Nuclear Medicine, Zhejiang Cancer Hospital, Hangzhou Institute 
of Medicine (HIM), Chinese Academy of Sciences, Hangzhou, Zhejiang 310022, China 
(K.C., H.Y.). Electronic address: yihq@zjcc.org.cn.

RATIONALE AND OBJECTIVES: The purpose of this study is to compare the 
performance of GPT-4 and DeepSeek large language models in generating structured 
breast cancer multimodality imaging integrated reports from free-text radiology 
reports including mammography, ultrasound, MRI, and PET/CT.
MATERIALS AND METHODS: A retrospective analysis was conducted on 1358 free-text 
reports from 501 breast cancer patients across two institutions. The study 
design involved synthesizing multimodal imaging data into structured reports 
with three components: primary lesion characteristics, metastatic lesions, and 
TNM staging. Input prompts were standardized for both models, with GPT-4 using 
predesigned instructions and DeepSeek requiring manual input. Reports were 
evaluated based on physician satisfaction using a Likert scale, descriptive 
accuracy including lesion localization, size, SUV, and metastasis assessment, 
and TNM staging correctness according to NCCN guidelines. Statistical analysis 
included McNemar tests for binary outcomes and correlation analysis for 
multiclass comparisons with a significance threshold of P < .05.
RESULTS: Physician satisfaction scores showed strong correlation between models 
with r-values of 0.665 and 0.558 and P-values below .001. Both models 
demonstrated high accuracy in data extraction and integration. The mean accuracy 
for primary lesion features was 91.7% for GPT-4% and 92.1% for DeepSeek, while 
feature synthesis accuracy was 93.4% for GPT4 and 93.9% for DeepSeek. Metastatic 
lesion identification showed comparable overall accuracy at 93.5% for GPT4 and 
94.4% for DeepSeek. GPT-4 performed better in pleural lesion detection with 
94.9% accuracy compared to 79.5% for DeepSeek, whereas DeepSeek achieved higher 
accuracy in mesenteric metastasis identification at 87.5% vs 43.8% for GPT4. TNM 
staging accuracy exceeded 92% for T-stage and 94% for M-stage, with N-stage 
accuracy improving beyond 90% when supplemented with physical exam data.
CONCLUSION: Both GPT-4 and DeepSeek effectively generate structured breast 
cancer imaging reports with high accuracy in data mining, integration, and TNM 
staging. Integrating these models into clinical practice is expected to enhance 
report standardization and physician productivity.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.acra.2025.07.046
PMID: 40780984 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


573. IEEE J Biomed Health Inform. 2025 Apr;29(4):3007-3020. doi: 
10.1109/JBHI.2024.3478809. Epub 2025 Apr 4.

LKAN: LLM-Based Knowledge-Aware Attention Network for Clinical Staging of Liver 
Cancer.

Li Y, Zheng X, Li J, Dai Q, Wang CD, Chen M.

Clinical staging of liver cancer (CSoLC), an important indicator for evaluating 
primary liver cancer (PLC), is key in the diagnosis, treatment, and 
rehabilitation of liver cancer. In China, the current CSoLC adopts the China 
liver cancer (CNLC) staging, which is usually evaluated by clinicians based on 
radiology reports. Therefore, inferring clinical information from unstructured 
radiology reports can provide auxiliary decision support for clinicians. The key 
to solving the challenging task is to guide the model to pay attention to the 
staging-related words or sentences, and the following issues may occur: 1) 
Imbalanced categories: Early- and mid-stage liver cancer symptoms are subtle, 
resulting in more data in the end-stage. 2) Domain sensitivity of liver cancer 
data: The liver cancer dataset contains substantial domain knowledge, leading to 
out-of-vocabulary issues and reduced classification accuracy. 3) Free-text and 
lengthy report: Radiology reports sparsely describe various lesions using 
domain-specific terms, making it hard to mine staging-related information. To 
address these, this article proposes a large language model (LLM)-based 
Knowledge-aware Attention Network (LKAN) for CSoLC. First, for maintaining 
semantic consistency, LLM and a rule-based algorithm are integrated to generate 
more diverse and reasonable data. Second, an unlabeled radiology corpus is 
pre-trained to introduce domain knowledge for subsequent representation 
learning. Third, attention is improved by incorporating both global and local 
features to guide the model's focus on staging-relevant information. Compared 
with the baseline models, LKAN has achieved the best results with 90.3% 
Accuracy, 90.0% Macro_F1 score, and 90.0% Macro_Recall.

DOI: 10.1109/JBHI.2024.3478809
PMID: 39392729 [Indexed for MEDLINE]


574. Nurse Educ Pract. 2025 Dec 23;91:104696. doi: 10.1016/j.nepr.2025.104696. Online 
ahead of print.

Reliance on ChatGPT in learning: Construction and validation of a reliance on 
ChatGPT scale for nursing students (ROCS- NS): Using structural equation 
modeling (SEM).

Elzeky MEH(1), Shahine NFM(2).

Author information:
(1)Medical-Surgical Nursing Department, College of Nursing, Jouf University, 
Sakaka, Saudi Arabia; Medical-Surgical Nursing Department, Faculty of Nursing, 
Mansoura University, Mansoura, Egypt. Electronic address: meelzeki@ju.edu.sa.
(2)Gerontological Nursing Department, Faculty of Nursing, Mansoura University, 
Mansoura, Egypt.

AIM: This study aimed to develop a scale to evaluate the reliance level on 
ChatGPT in learning for nursing students and then test and validate its 
psychometric properties.
BACKGROUND: The rapid adoption of ChatGPT has raised concerns regarding reliance 
on ChatGPT in learning. There is significance in categorizing students by their 
reliance level on ChatGPT.
DESIGN: A methodological study guided by established scale-development 
frameworks.
METHODS: A three-phase procedure was used to develop this scale and to determine 
its psychometric properties: (1) generating the item pool, (2) conducting a 
preliminary evaluation of the items and (3) analyzing its psychometric 
properties. The study examined the scale's face, content, construct validity; 
internal consistency; and temporal stability. A total of 633 students 
participated in testing the scale's psychometric properties, with data 
collection occurring between August and December 2024.
RESULTS: The scale's content validity index was 0.89. According to principal 
component analysis, the scale included 29 items and four sub-dimensions and the 
total variance was 70.031 %. In confirmatory factor analysis, the fit indices 
were acceptable for this construct. Its internal consistency was good or 
acceptable according to reliability analysis. Test-retest showed that the scale 
had stability.
CONCLUSION: Reliance on the ChatGPT Scale for Nursing Students is a valid and 
reliable instrument that can evaluate the level of reliance on ChatGPT in 
learning, which is useful for both researchers and educators. Future research 
should concentrate on longitudinal studies to evaluate how reliance on ChatGPT 
evolves over time and affects educational outcomes.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.nepr.2025.104696
PMID: 41455362

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


575. Digit Health. 2025 Sep 12;11:20552076251365059. doi: 10.1177/20552076251365059. 
eCollection 2025 Jan-Dec.

The top 100 most-cited articles on large language models in medicine: A 
bibliometric analysis.

Li ZQ(1), Xu R(2), Gong XR(3), Wang CL(1), Liu JP(1)(4).

Author information:
(1)Centre for Evidence-based Chinese Medicine, Beijing University of Chinese 
Medicine, Beijing, China.
(2)Department of Hematology and Oncology, Dongzhimen Hospital, Beijing 
University of Chinese Medicine, Beijing, China.
(3)Department of Environmental Health, Dalian Center for Disease Control and 
Prevention, Dalian, China.
(4)The National Research Center in Complementary and Alternative Medicine 
(NAFKAM), Department of Community Medicine, Faculty of Health Science, UiT the 
Arctic University of Norway, Tromsø, Norway.

OBJECTIVES: Large language models (LLMs) are revolutionizing medical research. 
However, there is a lack of bibliometric analysis that identifies citation 
trends shaping the history of this field. This study analyzes the top 100 (T100) 
most-cited articles on LLMs in medicine to assess their impact and 
characteristics.
METHODS: A bibliometric analysis of top-cited articles in the Web of Science 
database using search terms like "LLMs, generative artificial intelligence, GPT" 
from 2022 to 2025. Two reviewers identified the T100 papers, extracting 
publication details, citations, and research themes, adhering to BIBLIO 
reporting guidelines.
RESULTS: The T100 articles had contributed from 655 authors, and 92 articles 
were published in 2023. Original research constituted the majority of 
publications (60 articles). Collectively, these works accumulated 14,847 
citations, with individual citations ranging from 50 to 1057 (average 148.47). 
The U.S. led global contributions with 56 articles, Stanford University emerging 
as the most prolific institution (8 articles). The top seven journals 
contributed to 31% of the T100, and Journal of Medical Internet Research 
published the largest share (8 articles) in 70 peer-reviewed journals. The 
most-cited article is "Evolutionary-scale prediction of atomic-level protein 
structure with a language model" (Lin et al., Science 2023; 1057 citations). The 
research themes centered on evaluating LLMs' performance in exam-style 
evaluations, medical knowledge synthesis, and question-answering tasks in 
medicine.
CONCLUSION: This analysis provides a core overview of high-impact LLMs research 
in medicine, guiding future applications. The findings highlighted the 
remarkable progress in clinical decision support, drug discovery, multimodal 
medical imaging analysis, and personalized medical information-answering. They 
also stress the need for prospective trials to assess real-world clinical 
impacts, boost the reliability of LLMs-generated medical info, develop 
consensus-driven solutions to address ethical challenges, and launch global 
initiatives to democratize LLMs tools.

© The Author(s) 2025.

DOI: 10.1177/20552076251365059
PMCID: PMC12432328
PMID: 40949670

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


576. J Med Internet Res. 2025 Nov 3;27:e76296. doi: 10.2196/76296.

Effectiveness of Communication Competence in AI Conversational Agents for 
Health: Systematic Review and Meta-Analysis.

Qin J(1), Nan Y(2)(3), Li Z(4), Meng J(1).

Author information:
(1)School of Communication, The Ohio State University, 154 N Oval Mall, 
Columbus, OH, 43210, United States, 1 5173034870.
(2)Division of Infectious Diseases, Department of Medicine, Heersink School of 
Medicine, The University of Alabama at Birmingham, Birmingham, AL, United 
States.
(3)Center for AIDS Research, The University of Alabama at Birmingham, 
Birmingham, AL, United States.
(4)TH Chan School of Public Health, Harvard University, Boston, MA, United 
States.

BACKGROUND: With advancements in artificial intelligence and large language 
models, researchers and designers have increasingly focused on enhancing the 
conversational capacity of health-related conversational agents (CAs). 
Communication competence, a key concept in interpersonal communication 
influencing relational and health outcomes, has been extended to human-machine 
communication to emphasize the CAs' ability to demonstrate appropriate 
communicative behaviors in managing relationships with humans.
OBJECTIVE: This review aims to summarize the operationalization of communication 
competence in health CAs and assess its impact on 4 primary outcomes: users' 
evaluations of CA, use of CA, psychological outcomes, and health outcomes.
METHODS: A systematic literature search was conducted in 7 databases (ACM 
Digital Library, APA PsycInfo, Communication and Mass Media Complete, ProQuest 
Dissertations & Theses, Scopus, Web of Science Core Collection, and PubMed). 
Studies were included if they adopted experimental designs to manipulate CAs' 
communication competence in health-related conversations, recruited human 
participants, and reported at least 1 relevant outcome. Risk of bias was 
assessed using the revised Cochrane risk-of-bias tool. The systematic review 
summarized commonly used communication competence strategies. Three-level 
random-effects meta-analytic models were used to estimate pooled effect sizes 
for 4 primary outcomes. Moderator analyses were conducted to assess whether 
effect sizes varied across publication year, participants' average age, type of 
interaction with CAs, health topics, and publication outlet.
RESULTS: Of the 8309 identified papers, 31 independent experimental studies were 
included in the systematic review. Eleven strategies were identified to enhance 
CAs' communication competence: empathetic response, contingency, humor, small 
talk, emotional expressiveness, self-disclosure, personalization, social 
etiquette, explanation, open-ended questions, and partnership. Of the 31 
studies, 25 met the criteria for meta-analysis, which involved 4525 participants 
with a mean age of 29.7 (SD 9.2) years. The meta-analytic findings showed that 
communication competence has a significant small-to-medium effect on users' 
evaluations of CAs (Hedges g=0.45, 95% CI 0.24-0.66) and psychological outcomes 
(Hedges g=0.49, 95% CI 0.19-0.78). The effect sizes on the use of CA (Hedges 
g=0.11, 95% CI -0.05 to 0.26) and health outcomes (Hedges g=0.18, 95% CI -0.13 
to 0.50) are not significant. Moderator analyses showed that the effects remain 
stable across participants' age, type of interaction, and health topics.
CONCLUSIONS: This review highlights communication competence as a critical 
component in the design of health care CAs, particularly in improving users' 
evaluations and psychological outcomes. However, the limited number of studies 
examining health outcomes restricts the robustness of its effectiveness on this 
outcome. Future research is encouraged to directly evaluate the effects on 
tangible health outcomes.

© Jiaqi Qin, Yuanfeixue Nan, Zichao Li, Jingbo Meng. Originally published in the 
Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/76296
PMCID: PMC12582511
PMID: 41183242 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


577. J Am Med Inform Assoc. 2025 May 1;32(5):775-783. doi: 10.1093/jamia/ocaf034.

MedBot vs RealDoc: efficacy of large language modeling in physician-patient 
communication for rare diseases.

Weber MT(1), Noll R(1), Marchl A(1), Facchinello C(2), Grünewaldt A(3), Hügel 
C(4), Musleh K(3), Wagner TOF(5), Storf H(1), Schaaf J(1).

Author information:
(1)Institute of Medical Informatics, University Medicine Frankfurt, Goethe 
University Frankfurt, Frankfurt 60590, Germany.
(2)Santagostino Medical Center, Bologna 40138, Italy.
(3)Department of Respiratory Medicine and Allergology, University Medicine 
Frankfurt, Goethe University Frankfurt, Frankfurt 60590, Germany.
(4)HELIOS Dr Horst Schmidt Kliniken Wiesbaden, Klinik für Pneumologie, Wiesbaden 
65199, Germany.
(5)European Reference Network for Rare Respiratory Diseases (ERN-LUNG), 
University Medicine Frankfurt, Frankfurt 60590, Germany.

OBJECTIVES: This study assesses the abilities of 2 large language models (LLMs), 
GPT-4 and BioMistral 7B, in responding to patient queries, particularly 
concerning rare diseases, and compares their performance with that of 
physicians.
MATERIALS AND METHODS: A total of 103 patient queries and corresponding 
physician answers were extracted from EXABO, a question-answering forum 
dedicated to rare respiratory diseases. The responses provided by physicians and 
generated by LLMs were ranked on a Likert scale by a panel of 4 experts based on 
4 key quality criteria for health communication: correctness, comprehensibility, 
relevance, and empathy.
RESULTS: The performance of generative pretrained transformer 4 (GPT-4) was 
significantly better than the performance of the physicians and BioMistral 7B. 
While the overall ranking considers GPT-4's responses to be mostly correct, 
comprehensive, relevant, and emphatic, the responses provided by BioMistral 7B 
were only partially correct and empathetic. The responses given by physicians 
rank in between. The experts concur that an LLM could lighten the load for 
physicians, rigorous validation is considered essential to guarantee 
dependability and efficacy.
DISCUSSION: Open-source models such as BioMistral 7B offer the advantage of 
privacy by running locally in health-care settings. GPT-4, on the other hand, 
demonstrates proficiency in communication and knowledge depth. However, 
challenges persist, including the management of response variability, the 
balancing of comprehensibility with medical accuracy, and the assurance of 
consistent performance across different languages.
CONCLUSION: The performance of GPT-4 underscores the potential of LLMs in 
facilitating physician-patient communication. However, it is imperative that 
these systems are handled with care, as erroneous responses have the potential 
to cause harm without the requisite validation procedures.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf034
PMCID: PMC12012358
PMID: 39998911 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
declare.


578. J Neuroeng Rehabil. 2025 Nov 27;22(1):269. doi: 10.1186/s12984-025-01810-w.

Collaborative AI for precision neurorehabilitation: a roadmap.

Liew SL(1), Cotton RJ(2)(3), Burdet E(4), Bermúdez I Badia S(5)(6), Celnik P(3), 
Cole JH(7), Liu R(8), Soekadar SR(9), Winstein C(10), Schweighofer N(11)(12).

Author information:
(1)Mark and Mary Stevens Neuroimaging and Informatics Institute, Keck School of 
Medicine, Department of Neurology, Chan Division of Occupational Science and 
Occupational Therapy, Department of Biomedical Engineering, University of 
Southern California, 2025 Zonal Ave, Los Angeles, CA, 90033, USA. 
sliew@chan.usc.edu.
(2)Department of Physical Medicine and Rehabilitation, Northwestern University, 
710 N. Lake Shore Drive, Chicago, IL, 60611, USA.
(3)Shirley Ryan AbilityLab, 355 East Erie ST, Chicago, Il, 60611, USA.
(4)Department of Bioengineering, Imperial College of Science, Technology and 
Medicine, White City campus, London, SW7 2AZ, UK.
(5)Faculdade de Ciências Exatas e da Engenharia & NOVA LINCS, Universidade da 
Madeira Campus da Penteada, 9020-105, Funchal, Portugal.
(6)Agência Regional para o Desenvolvimento de Investigação, Tecnologia e 
Inovação (ARDITI), Campus da Penteada, 9020-105, Funchal, Madeira, Portugal.
(7)Hawkes Institute, Department of Computer Science, University College London, 
90 High Holborn, London, WC1V 6LJ, UK.
(8)Department of Computer Science, University of Southern California, 1031 
Downey Way, Los Angeles, CA, 90089, USA.
(9)Clinical Neurotechnology Lab, Dept. of Psychiatry and Neurosciences, Charité 
Campus Mitte (CCM), Charité - Universitätsmedizin Berlin, Charitéplatz 1, 10117, 
Berlin, Germany.
(10)Biokinesiology and Physical Therapy, University of Southern California, 1540 
Alcazar St, Los Angeles, CA, 90033, USA.
(11)Department of Computer Science, University of Southern California, 1031 
Downey Way, Los Angeles, CA, 90089, USA. schweigh@pt.usc.edu.
(12)Biokinesiology and Physical Therapy, University of Southern California, 1540 
Alcazar St, Los Angeles, CA, 90033, USA. schweigh@pt.usc.edu.

Precision rehabilitation seeks to improve care for individuals by identifying 
personalized treatments that enhance both the efficacy and efficiency of care. 
Rapid advances in artificial intelligence (AI) and data-driven methodologies 
stand to greatly enhance precision rehabilitation methods, particularly when 
paired with clinical expertise and patient-driven goals. Although AI has been 
successfully integrated into healthcare in fields such as precision oncology, 
the dynamic, multi-faceted, complex nature of rehabilitation require a different 
approach than standard predictive models. Here, we argue that precision 
rehabilitation may benefit from collaborative AI, in which humans and AI work 
together with AI distilling complex data that humans can use as part of their 
nuanced decision-making process. We review the current landscape of precision 
rehabilitation and explore how collaborative AI, and AI in general, can advance 
the field. We begin by outlining a roadmap for a general collaborative AI system 
in rehabilitation, noting four key challenges that must be addressed. Next, we 
examine three existing precision rehabilitation frameworks that have been 
developed independently by several of the co-authors, which share four common 
key elements of the roadmap. We then describe examples of how AI has already 
been applied to specific aspects of complex rehabilitation scenarios, as these 
may be integrated into larger collaborative AI models. We follow this with a 
discussion of how to make AI-based precision rehabilitation a reality, with an 
emphasis on the large data required for these models to make accurate 
predictions, as well as potential ethical issues. Finally, we conclude with 
recommendations for future directions. Ultimately, collaborative AI promises to 
transform rehabilitation by leveraging vast, diverse datasets to create 
individualized digital profiles, which can first be used to simulate the effects 
of rehabilitation interventions in silico, maximizing impacts before real-world 
implementation. By merging personalized rehabilitation strategies based on 
empirical evidence with the depth and complexity of clinical knowledge and 
reasoning, collaborative AI holds promise as a powerful tool to help clinicians 
advance patient care and improve long-term outcomes.

© 2025. The Author(s).

DOI: 10.1186/s12984-025-01810-w
PMCID: PMC12751918
PMID: 41310676 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: • Not applicable. Consent for publication: • Not applicable. 
Competing interests: • SLL is a consultant for Synchron. JHC is a shareholder 
and scientific advisor for BrainKey and Claritas HealthTech PTE.


579. Int J Med Inform. 2026 Mar 1;207:106186. doi: 10.1016/j.ijmedinf.2025.106186. 
Epub 2025 Nov 26.

Healthbots for conducting clinical screening and remote monitoring with patient 
mood assessment: A scoping review.

Cesar Abrantes P(1), Netto AV(2), Kazuo Takahata A(3).

Author information:
(1)Universidade Federal do ABC (UFABC), Santo André, SP, Brazil. Electronic 
address: paulo.abrantes@ufabc.edu.br.
(2)Universidade Federal de São Paulo (UNIFESP), São Paulo, SP, Brazil.
(3)Universidade Federal do ABC (UFABC), Santo André, SP, Brazil.

BACKGROUND: Patient mood assessment is key in managing chronic diseases but is 
often overlooked. Although conversational agents enhance telemonitoring and 
engagement, few healthbots incorporate automated mood analysis into routine 
clinical workflows or hybrid care. The rise of multimodal and large language 
models presents new opportunities to embed emotional assessment into daily 
healthcare interactions.
OBJECTIVE: This scoping review aims to identify existing AI-based healthbots 
that combine clinical screening and remote monitoring with mood assessment. Its 
secondary objectives are to (1) describe their technological architectures and 
AI methods, (2) examine validation and evaluation strategies, and (3) identify 
current research gaps.
METHODS: Following the Arksey and O'Malley framework and PRISMA-ScR guidelines, 
a comprehensive search was conducted across seven databases (ACM Digital 
Library, Embase, IEEE Xplore, PubMed, Scopus, SpringerLink, Web of Science), 
covering the period from January 2020 to December 2024. Studies were included if 
they presented empirical evidence of AI-based clinical screening with mood 
assessment. Ten studies met the inclusion criteria after screening and 
deduplication. Data were charted and synthesized based on key dimensions, 
including technological features, validation methods, and limitations.
RESULTS: Ten studies, mostly in mental health, used multimodal inputs (voice, 
facial expressions, text) with CNNs, LSTMs, NLP, and LLMs via web or mobile 
platforms. Some achieved high accuracy on public data and in cross-validation, 
but few conducted external or longitudinal validation in clinical settings. 
Integration with EHRs and standards was rarely reported. Limitations included 
small, homogeneous samples, limited generalizability, insufficient 
explainability, and privacy concerns.
CONCLUSIONS: AI-driven healthbots with mood assessment show promise but are 
still immature. While they can recognize emotions, few are validated in real 
clinical settings or integrated into workflows. Broader adoption depends on 
long-term validation, explainable, bias-aware algorithms, EHR interoperability, 
and ethical standards. Progress requires collaboration among technical, 
clinical, and policy experts to ensure the safe and equitable use.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106186
PMID: 41308276 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


580. Digit Health. 2026 Jan 5;12:20552076251412985. doi: 10.1177/20552076251412985. 
eCollection 2026 Jan-Dec.

Retrospective modeling study of ChatGPT (GPT 4) warfarin dose adjustment in 
patients with INRs outside the therapeutic range.

Tezcan H(1), Tunçez A(1), Özen Y(1), Yalçın MU(1), Gürses KM(1).

Author information:
(1)Department of Cardiology, Konya Selcuk University Medical Faculty Hospital, 
Konya, Turkey.

OBJECTIVE: This retrospective, modeled study evaluates the accuracy of ChatGPT 
(GPT-4)-based warfarin dose adjustments compared to clinician recommendations at 
the Cardiology Clinic of Konya City Hospital, focusing on patients with 
international normalized ratio (INR) values outside the therapeutic range (2-3). 
We hypothesized that ChatGPT could provide reliable, consistent dose guidance.
METHODS: We reviewed the records of warfarin-treated patients from 1 June 2022 
through 24 November 2024. Clinical data used by physicians (e.g. baseline INR, 
warfarin indication, comorbidities, and current dose) were provided to ChatGPT 
to generate hypothetical weekly dose recommendations. ChatGPT's impact on INR 
normalization was modeled using standard dose-response assumptions and compared 
with actual outcomes under physician-guided therapy.
RESULTS: A total of 180 patients met the inclusion criteria. ChatGPT's 
recommended doses were within ±1 mg/week of physician prescriptions in 74% of 
cases and within ±2 mg/week in 84%. The mean physician dose was 
28.0 ± 6.1 mg/week versus ChatGPT's 27.5 ± 5.9 mg/week (p = .12). Seventy-two 
percent of patients achieved therapeutic INR under physician-managed dosing, 
while the model suggested a 69% success rate for ChatGPT-guided dosing 
(p = .15). Real-world adverse events were infrequent under physician management 
(1.1% major bleeding, 0.6% thrombotic events).
CONCLUSION: In this retrospective, exploratory analysis with modeled outcomes, 
ChatGPT's weekly dose suggestions showed high concordance with clinician dosing. 
These findings are hypothesis-generating and do not establish clinical efficacy 
or safety; prospective, physician-supervised trials-potentially integrated with 
home INR monitoring-are required for validation.

© The Author(s) 2026.

DOI: 10.1177/20552076251412985
PMCID: PMC12775298
PMID: 41509866

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


581. BMC Med Ethics. 2025 May 27;26(1):68. doi: 10.1186/s12910-025-01198-1.

Ethical and social considerations of applying artificial intelligence in 
healthcare-a two-pronged scoping review.

Ratti E(1), Morrison M(2)(3), Jakab I(4).

Author information:
(1)Department of Philosophy, Cotham House University of Bristol, Bristol, BS6 
6JL, UK.
(2)Helex - Centre for Health, Law and Emerging Technologies, Faculty of Law, 
University of Oxford, St Cross Building, Room 201St Cross Road, Oxford, OX1 3UL, 
UK. michael.morrison@law.ox.ac.uk.
(3)Institute for Science, Innovation and Society, School of Anthropology and 
Museum Ethnography, University of Oxford, 64 Banbury Road, Oxford, OX2 6PN, UK. 
michael.morrison@law.ox.ac.uk.
(4)YAGHMA B.V., 6C , Poortweg, Delft, Netherlands.

BACKGROUND: Artificial Intelligence (AI) is being designed, tested, and in many 
cases actively employed in almost every aspect of healthcare from primary care 
to public health. It is by now well established that any application of AI 
carries an attendant responsibility to consider the ethical and societal aspects 
of its development, deployment and impact. However, in the rapidly developing 
field of AI, developments such as machine learning, neural networks, generative 
AI, and large language models have the potential to raise new and distinct 
ethical and social issues compared to, for example, automated data processing or 
more 'basic' algorithms.
METHODS: This article presents a scoping review of the ethical and social issues 
pertaining to AI in healthcare, with a novel two-pronged design. One strand of 
the review (SR1) consists of a broad review of the academic literature 
restricted to a recent timeframe (2021-23), to better capture up to date 
developments and debates. The second strand (SR2) consists of a narrow review, 
limited to prior systematic and scoping reviews on the ethics of AI in 
healthcare, but extended over a longer timeframe (2014-2024) to capture 
longstanding and recurring themes and issues in the debate. This strategy 
provides a practical way to deal with an increasingly voluminous literature on 
the ethics of AI in healthcare in a way that accounts for both the depth and 
evolution of the literature.
RESULTS: SR1 captures the heterogeneity of audience, medical fields, and ethical 
and societal themes (and their tradeoffs) raised by AI systems. SR2 provides a 
comprehensive picture of the way scoping reviews on ethical and societal issues 
in AI in healthcare have been conceptualized, as well as the trends and gaps 
identified.
CONCLUSION: Our analysis shows that the typical approach to ethical issues in 
AI, which is based on the appeal to general principles, becomes increasingly 
unlikely to do justice to the nuances and specificities of the ethical and 
societal issues raised by AI in healthcare, as the technology moves from 
abstract debate and discussion to real world situated applications and concerns 
in healthcare settings.

© 2025. The Author(s).

DOI: 10.1186/s12910-025-01198-1
PMCID: PMC12107984
PMID: 40420080 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: ER, MM, and IJ consent for 
this publication. Competing interests: IJ is a Board of Trustee at the European 
Patient Academy of Therapeutic Innovation (EUPATI) Foundation. Otherwise, all 
authors declare that they have no competing interests, whether financial, 
personal, or otherwise relevant to the content or choice of journal for this 
manuscript.


582. J Biomed Inform. 2025 Oct;170:104899. doi: 10.1016/j.jbi.2025.104899. Epub 2025 
Aug 21.

Perplexity and proximity: Large language model perplexity complements semantic 
distance metrics for the detection of incoherent speech.

Xu W(1), Pakhomov S(2), Heagerty P(3), Horvitz E(4), Bradley ER(5), Woolley 
J(5), Campbell A(6), Cohen A(7), Ben-Zeev D(8), Cohen T(9).

Author information:
(1)Biomedical Informatics and Medical Education (BIME), University of 
Washington, Seattle, WA, USA. Electronic address: xuweizhe@uw.edu.
(2)Pharmaceutical Care and Health Systems, University of Minnesota, Minneapolis, 
MN, USA.
(3)Department of Biostatistics, University of Washington, Seattle, WA, USA.
(4)Microsoft Research, Seattle, WA, USA.
(5)Department of Psychiatry, University of California, San Francisco, CA, USA.
(6)Department of Computer Science, Dartmouth College, Hanover, NH, USA.
(7)Department of Psychology, Louisiana State University, Baton Rouge, LA, USA.
(8)Behavioral Research in Technology (BRiTE) Center, Psychiatry and Behavioral 
Sciences, University of Washington, Seattle, WA, USA.
(9)Biomedical Informatics and Medical Education (BIME), University of 
Washington, Seattle, WA, USA; Behavioral Research in Technology (BRiTE) Center, 
Psychiatry and Behavioral Sciences, University of Washington, Seattle, WA, USA.

OBJECTIVE: Semantic coherence in speech is characterized by a logical, connected 
flow of ideas. A lack of coherence in speech may reflect disorganized thinking, 
a core feature of psychosis in schizophrenia spectrum disorders (SSDs). 
Developing tools that could help with automated assessment of semantic coherence 
in language could facilitate early detection of SSDs and improved monitoring of 
symptoms, enabling more timely intervention. Large language models (LLMs) have 
demonstrated strong capabilities on numerous language-centric tasks and have 
shown promise for analyzing semantic coherence due to the natural fit between 
their innate measures of language perplexity and the surprising turns that 
incoherent narrative often takes. This study aims to develop a novel 
representation and associated measure of semantic coherence using LLM-based 
perplexity metrics and to compare this measure with traditional vector 
distance-based coherence metrics.
METHOD: We evaluated "bag" and "chain" models based on LLM perplexities as 
measures of semantic coherence. Regression models were trained using both single 
and paired combinations of perplexity- and proximity-based features to predict 
human ratings of semantic coherence using standardized instruments. Performance 
was evaluated on held-out examples from a training set of speeches from 
individuals experiencing psychotic symptoms and a test set of clinical 
interviews with patients diagnosed with SSDs, both with labels from human 
assessments of disorganized thinking severity.
RESULTS: The best performance was achieved using a combination of perplexity and 
proximity features, yielding a Spearman correlation with human ratings of 0.61 
(vs. 0.56 with proximity features alone) on leave-one-out cross-validation in 
the training set, and 0.54 (vs. 0.52 with proximity features alone) on the test 
set.
CONCLUSION: We developed novel methods for assessing semantic coherence using 
LLM perplexities and found them complementary to proximity-based methods. 
Combined, these methods showed improved performance across two datasets, 
highlighting LLM's potential in enhancing automated diagnosis and monitoring of 
SSDs.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104899
PMID: 40849054 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


583. Comput Med Imaging Graph. 2025 Sep;124:102581. doi: 
10.1016/j.compmedimag.2025.102581. Epub 2025 May 31.

MedBLIP: A multimodal method of medical question-answering based on fine-tuning 
large language model.

Gong L(1), Yang J(2), Han S(2), Ji Y(3).

Author information:
(1)Jiangsu Key Lab of Big Data Security & Intelligent Processing, School of 
Computer Science, Nanjing University of Posts and Telecommunications, Nanjing 
210023, China. Electronic address: glj98226@njupt.edu.cn.
(2)Jiangsu Key Lab of Big Data Security & Intelligent Processing, School of 
Computer Science, Nanjing University of Posts and Telecommunications, Nanjing 
210023, China.
(3)Institute of High Performance Computing and Bigdata, Nanjing University of 
Posts and Telecommunications, Nanjing 210023, China.

Medical visual question answering is crucial for effectively interpreting 
medical images containing clinically relevant information. This study proposes a 
method called MedBLIP (Medical Treatment Bootstrapping Language-Image 
Pretraining) to tackle visual language generation tasks related to chest X-rays 
in the medical field. The method combine an image encoder with a large-scale 
language model, and effectively generates medical question-answering text 
through a strategy of freezing the image encoder based on the BLIP-2 model. 
Firstly, chest X-ray images are preprocessed, and an image sample generation 
algorithm is used to enhance the text data of doctor-patient question-answering, 
thereby increasing data diversity. Then, a multi-layer convolutional image 
feature extractor is introduced to better capture the feature representation of 
medical images. During the fine-tuning process of the large language generation 
model, a new unfreezing strategy is proposed, which is to unfreeze different 
proportions of the weights of the fully connected layer to adapt to the data in 
the medical field. The image feature extractor is responsible for extracting key 
features from images, providing the model with rich visual information, while 
the text feature extractor accurately captures the essential requirements of the 
user's question. Through their synergistic interaction, the model can more 
effectively integrate medical images and user inquiries, thereby generating more 
accurate and relevant output content. The experimental results show that 
unfreezing 31.25 % of the weights of the fully connected layer can significantly 
improve the performance of the model, with ROUGE-L reaching 66.12 %, and 
providing a more accurate and efficient answer generation solution for the 
medical field. The method of this study has potential applications in the field 
of medical language generation tasks. Although the proposed model cannot yet 
fully replace human radiologists, it plays an indispensable role in improving 
diagnostic efficiency, assisting decision-making, and supporting medical 
research. With continuous technological advancements, the model's performance 
will be further enhanced, and its application value in the medical field will 
become even more significant. The algorithm implementation can be obtained from 
https://github.com/JiminFohill/MedicalChat.git.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.compmedimag.2025.102581
PMID: 40483830 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest We declare 
that we have no financial and personal relationships with other people or 
organizations that can inappropriately influence our work, there is no 
professional or other personal interest of any nature or kind in any product, 
service and/or company that could be construed as influencing the position 
presented in the manuscript entitled “ MedBLIP: A Multimodal Method of Medical 
Question-Answering Based on Fine-Tuning Large Language Model ”.


584. Digit Health. 2025 Nov 20;11:20552076251389325. doi: 10.1177/20552076251389325. 
eCollection 2025 Jan-Dec.

Evaluating the potential of ChatGPT for patient identification in clinical 
breast cancer trials.

Krückel A(1)(2)(3), Fasching PA(1)(2)(3), Schleicher O(1)(2)(3), Gocke 
J(1)(2)(3), Brückner L(1)(2)(3), Seitz K(1)(2)(3), Häberle L(1)(2)(3)(4), Heindl 
F(1)(2)(3), Hack CC(1)(2)(3), Beckmann MW(1)(2)(3), Emons J(1)(2)(3).

Author information:
(1)Department of Gynecology and Obstetrics, Universitätsklinikum Erlangen, 
Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Erlangen, Germany.
(2)Comprehensive Cancer Center Erlangen-European Metropolitan Area of Nuremberg 
(CCC ER-EMN), Erlangen, Germany.
(3)Bavarian Cancer Research Center (BZKF), Erlangen, Germany.
(4)Biostatistics Unit, Department of Gynecology and Obstetrics, 
Universitätsklinikum Erlangen, Friedrich-Alexander-Universität Erlangen-Nürnberg 
(FAU), Erlangen, Germany.

OBJECTIVE: Growing complexity of oncological treatment is reflected in the 
requirements for current clinical trials, challenging clinical sites with 
recruiting suitable participants. This cross-sectional study evaluates the 
potential of artificial intelligence (AI), based on the example of ChatGPT-4.0, 
in identifying suitable study participants among patients with breast cancer, 
utilizing real-world tumor board data.
METHODS: ChatGPT-4.0 was trained on six fictitious study protocols for patients 
with breast cancer, mimicking real-world clinical trial scenarios. Anonymized 
data from 124 tumor board registrations from January 2024 were submitted to the 
AI to determine eligibility for study participation. A clinician control group 
also assessed the patients' eligibility. The evaluations of ChatGPT-4.0 and the 
medical professionals were benchmarked against an expert-validated reference 
standard. Sensitivity and specificity were calculated for the AI as well as for 
each member of the control group.
RESULTS: Overall, among the 124 tumor board registrations, 19 patients met 
eligibility criteria for at least one study. Both AI and clinicians reliably 
excluded ineligible patients (high specificity), but sensitivity varied. 
ChatGPT-4.0 proved especially ineffective at screening for neoadjuvant trials, 
whereas medical professionals showed better, but heterogeneous performance. 
Team-based assessment identified nearly all eligible patients, underscoring the 
value of collaborative decision making.
CONCLUSION: While model performance was limited by simplified input data and a 
small single-center cohort, the results suggest that ChatGPT-4.0, in its current 
form, is not yet suitable as a stand-alone tool for patient identification in 
clinical breast cancer trials. To ensure accurate and efficient recruitment, the 
involvement of a multiprofessional team remains essential. Ongoing model 
refinement and access to larger, more detailed datasets may enhance the future 
utility of AI systems in clinical trial screening.

© The Author(s) 2025.

DOI: 10.1177/20552076251389325
PMCID: PMC12639235
PMID: 41278370

Conflict of interest statement: The authors declared the following potential 
conflicts of interest with respect to the research, authorship, and/or 
publication of this article: AK received honoria from Gilead, Novartis and Georg 
Thieme Verlag as well as support for attending meetings from Lilly. PAF reports 
personal fees from Novartis, Pfizer, Daiichi-Sankyo, AstraZeneca, Eisai, Merck 
Sharp & Dohme, Lilly, Pierre Fabre, SeaGen, Roche, Hexal, Agendia, and Gilead 
and grants from Biontech and Cepheid. OS, JG, LH, FH, and MWB have no conflicts 
of interest to declare. LB received honoria from Gilead as well as support for 
attending meetings from Lilly, Gilead, and AstraZeneca. KS received honoria from 
Gilead as well as support for attending meetings from Novartis, Gilead, and 
Lilly. CCH received honoraria from AstraZeneca, Daiichi Sankyo, Eisai, Novartis, 
Pfizer, Roche, Gilead and MSD as well as support for attending meetings from 
Daiichi Sankyo. JE has received honoraria from Novartis, MSD, Eisai, Pfizer, and 
AstraZeneca.


585. Front Digit Health. 2025 Apr 25;7:1537907. doi: 10.3389/fdgth.2025.1537907. 
eCollection 2025.

Evaluating diversity and stereotypes amongst AI generated representations of 
healthcare providers.

Agrawal A(1), Gupta G(2), Agrawal A(3), Gupta H(4).

Author information:
(1)Department of Computer Science, The University of Texas at Austin, Austin, 
TX, United States.
(2)Ridgewood High School, Ridgewood, NJ, United States.
(3)Rouse High School, Leander, TX, United States.
(4)Heart and Vascular Institute, Valley Health System, Ridgewood, NJ, United 
States.

INTRODUCTION: Generative artificial intelligence (AI) can simulate existing 
societal data, which led us to explore diversity and stereotypes among 
AI-generated representations of healthcare providers.
METHODS: We used DALL-E 3, a text-to-image generator, to generate 360 images 
from healthcare profession terms tagged with specific race and sex identifiers. 
These images were evaluated for sex and race diversity using consensus scoring. 
To explore stereotypes present in the images, we employed Google Vision to label 
objects, actions, and backgrounds in the images.
RESULTS: We found modest levels of sex diversity (3.2) and race diversity (2.8) 
on a 5-point scale, where 5 indicates maximum diversity. These findings align 
with existing workforce statistics, suggesting that Generative AI reflects 
real-world diversity patterns. The analysis of Google Vision image labels 
revealed sex and race-linked stereotypes related to appearance, facial 
expressions, and attire.
DISCUSSION: This study is the first of its kind to provide a ML-based framework 
for quantifying diversity and biases amongst generated AI images of healthcare 
providers. These insights can guide policy decisions involving the use of 
Generative AI in healthcare workforce training and recruitment.

© 2025 Agrawal, Gupta, Agrawal and Gupta.

DOI: 10.3389/fdgth.2025.1537907
PMCID: PMC12062135
PMID: 40352327

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


586. NPJ Digit Med. 2025 Dec 26. doi: 10.1038/s41746-025-02277-8. Online ahead of 
print.

A novel evaluation benchmark for medical LLMs illuminating safety and 
effectiveness in clinical domains.

Wang S(#)(1), Tang Z(#)(2), Yang H(#)(3), Gong Q(#)(4), Gu T(#)(1), Ma H(#)(2), 
Wang Y(1), Sun W(1), Lian Z(1), Mao K(1), Jiang Y(5), Huang Z(6), Ma L(7), Shen 
W(8), Ji Y(9), Tan Y(10), Wang C(11), Gao Y(12), Ye Q(13), Lin R(14), Chen 
M(15), Niu L(16), Wang Z(17), Yu P(18), Lang M(17), Liu Y(17), Zhang H(19), Shen 
H(20), Chen L(21), Zhao Q(22), Liu SX(9), Zhou L(23), Gao H(1), Ye D(1), Meng 
L(1), Yu Y(24), Liang N(25), Wu J(26).

Author information:
(1)Medlinker Intelligent and Digital Technology Co. Ltd., Beijing, China.
(2)Peking University School of Stomatology, Haidian, Beijing, China.
(3)Department of Rheumatology and Clinical Immunology, Peking Union Medical 
College Hospital, Chinese Academy of Medical Sciences and Peking Union Medical 
College, Beijing, China.
(4)Center of Endocrinology, National Center of Cardiology & Fuwai Hospital, 
Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, 
China.
(5)Department of Psychological Medicine, Peking Union Medical College Hospital, 
Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, 
China.
(6)Department of Thoracic Surgery, Peking Union Medical College Hospital, 
Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, 
China.
(7)Department of Respiratory and Critical Care Medicine, the 8th Medical Center 
of PLA General Hospital, Beijing, China.
(8)Department of Obstetrics & Gynecology, the Fourth Medical Center of PLA 
General Hospital, Beijing, China.
(9)Shuguang Hospital Affiliated to Shanghai University of Traditional Chinese 
Medicine, Shanghai, China.
(10)Department of Urology, The Second Affiliated Hospital of Harbin Medical 
University, Harbin, Heilongjiang Province, China.
(11)Department of Radiation Oncology, Harbin Medical University Cancer Hospital, 
Harbin, Heilongjiang Province, China.
(12)Department of Dermatology, Shanghai Skin Disease Hospital, Tongji University 
School of Medicine, Shanghai, China.
(13)Department of Oncology, East Hospital Affiliated to Tongji University, 
Tongji University School of Medicine, Tongji University, Shanghai, China.
(14)General Surgery Department, Tongji Hospital, School of Medicine, Tongji 
University, Shanghai, China.
(15)Department of Neurosurgery, Huashan Hospital, Shanghai Medical College, 
Fudan University, Shanghai, China.
(16)Department of Ultrasound, National Cancer Center/National Clinical Research 
Center for Cancer/Cancer Hospital, Chinese Academy of Medical Sciences and 
Peking Union Medical College, Beijing, China.
(17)Department of Hepatobiliary Surgery, National Cancer Center/National 
Clinical Research Center for Cancer/Cancer Hospital, Chinese Academy of Medical 
Sciences and Peking Union Medical College, Beijing, China.
(18)Department of General Surgery, The Fourth Affiliated Hospital of Xinjiang 
Medical University, Urumqi, Xinjiang Uygur Autonomous Region, China.
(19)Department of Otolaryngology-Head and Neck Surgery, Shanxi Bethune Hospital, 
Shanxi Academy of Medical Sciences, Tongji Shanxi Hospital, Third Hospital of 
Shanxi Medical University, Taiyuan, Shanxi Province, China.
(20)Department of Clinical Laboratory, Seventh People's Hospital of Shanghai 
University of Traditional Chinese Medicine, Shanghai, China.
(21)Department of Orthopedics, Guangzhou Red Cross Hospital of Jinan University, 
Guangzhou, Guangdong Province, China.
(22)Department of Imageology, Anzhen Hospital. Capital Medical University, 
Beijing, China.
(23)Beijing EuroEyes, Beijing, China.
(24)Department of Interventional Radiology, the Fourth Medical Center of Chinese 
PLA General Hospital, Beijing, China. yuyoutao@126.com.
(25)Department of Thoracic Surgery, Peking Union Medical College Hospital, 
Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, 
China. pumchnelson@163.com.
(26)Department of Hepatobiliary Surgery, National Cancer Center/National 
Clinical Research Center for Cancer/Cancer Hospital, Chinese Academy of Medical 
Sciences and Peking Union Medical College, Beijing, China. Dr_wujx@163.com.
(#)Contributed equally

Large language models (LLMs) hold promise in clinical decision support but face 
major challenges in safety evaluation and effectiveness validation. We developed 
the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a 
multidimensional framework built on clinical expert consensus, encompassing 30 
metrics covering critical areas like critical illness recognition, guideline 
adherence, and medication safety, with weighted consequence measures. Thirty-two 
specialist physicians developed and revised 2069 open-ended Q&A items aligned 
with these criteria, spanning 26 clinical departments to simulate real-world 
scenarios. Benchmark testing of six LLMs revealed moderate overall performance 
(average total score 57.2%, safety 54.7%, effectiveness 62.3%), with a 
significant 13.3% performance drop in high-risk scenarios (p < 0.0001). 
Domain-specific medical LLMs showed consistent performance advantages over 
general-purpose models, with relatively higher top scores in safety (0.912) and 
effectiveness (0.861). The findings of this study not only provide a 
standardized metric for evaluating the clinical application of medical LLMs, 
facilitating comparative analyses, risk exposure identification, and improvement 
directions across different scenarios, but also hold the potential to promote 
safer and more effective deployment of large language models in healthcare 
environments.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-02277-8
PMID: 41454006

Conflict of interest statement: Competing interests: SW, TG, YW, WS, ZL, KM, DY, 
HG and LM are employees of Medlinker Intelligent and Digital Technology Co., 
Ltd, Beijing, China, the developers of the MedGPT model evaluated in this study. 
These authors contributed to the study concept only. The other authors declare 
no competing interests.


587. Int J Med Inform. 2025 Nov;203:106034. doi: 10.1016/j.ijmedinf.2025.106034. Epub 
2025 Jul 1.

Comparing ChatGPT and physicians' answers to endometriosis questions on Reddit: 
A blind expert evaluation.

Beaulieu C(1), Agostini A(2), Crochet P(3), Carcopino X(2), Touleimat S(3), 
Netter A(4).

Author information:
(1)Department of Gynecology and Obstetrics, AP-HM, Assistance Publique-Hôpitaux 
de Marseille, Marseille, France.
(2)Department of Gynecology and Obstetrics, AP-HM, Assistance Publique-Hôpitaux 
de Marseille, Marseille, France; EA 3279, CEReSS, Health Service Research and 
Quality of Life Center, Aix-Marseille University, Marseille, France.
(3)Department of Obstetrics and Gynaecology, Charles Nicolle Hospital, 
University of Rouen, Rouen, France.
(4)Department of Gynecology and Obstetrics, AP-HM, Assistance Publique-Hôpitaux 
de Marseille, Marseille, France; EA 3279, CEReSS, Health Service Research and 
Quality of Life Center, Aix-Marseille University, Marseille, France. Electronic 
address: antoine.netter@gmail.com.

OBJECTIVES: To compare the perceived quality, safety, and relevance of ChatGPT 
responses to those provided by verified physicians on Reddit, a large online 
discussion platform, in response to questions related to endometriosis.
METHODS: We selected 30 endometriosis-related questions posted on Reddit's 
r/AskDocs forum, each answered by a verified physician. Using the same question 
prompts, ChatGPT (GPT-3.5) generated matched-length responses. Responses were 
anonymized, randomized (A/B format), and assessed blindly by three 
university-affiliated physicians using a 11-item Likert-scale questionnaire 
covering medical accuracy, safety, clarity, empathy, and alignment with best 
practices. Evaluators also indicated which response they considered most 
pertinent and whether they suspected AI authorship.
RESULTS: ChatGPT responses were rated significantly higher than physicians' 
responses on most items, including medical coherence (mean 3.89 ± 0.89 vs. 
3.08 ± 0.92), clarity (3.93 ± 0.95 vs. 3.04 ± 0.99), and empathy (3.91 ± 0.93 
vs. 2.76 ± 1.09), all with p-values < 0.001. Experts selected ChatGPT as the 
most pertinent response in 63.3 % of cases. A substantial proportion of 
responses from both sources were considered potentially dangerous by at least 
one expert: 26.7 % for ChatGPT and 60.0 % for physicians (p = 0.019).
CONCLUSION: ChatGPT outperformed Reddit physicians on multiple expert-rated 
criteria, particularly in terms of clarity, empathy, and adherence to clinical 
recommendations. However, the non-negligible proportion of responses considered 
potentially dangerous by experts underscores the need for cautious use and 
appropriate supervision of such technologies.

Copyright © 2025. Published by Elsevier B.V.

DOI: 10.1016/j.ijmedinf.2025.106034
PMID: 40618644 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


588. Int J Surg. 2025 Nov 19. doi: 10.1097/JS9.0000000000004062. Online ahead of 
print.

Exploring the prognostic utility of large language models versus traditional 
clinical models in heart failure: a pilot study.

Shang L(1), Chen Y(2), Li R(3), Zhang X(4), Gao M(5), Hou Y(5), Zhang G(1).

Author information:
(1)Department of Health Management, Shandong Engineering Laboratory of Health 
Management, Shandong Institute of Health Management, the First Affiliated 
Hospital of Shandong First Medical University & Shandong Provincial Qianfoshan 
Hospital, Jinan, China.
(2)Department of Cardiology, the Fourth People's Hospital of Jinan, Jinan, 
China.
(3)Shandong Provincial Center for Disease Control and Prevention, Jinan, China.
(4)Medical Science and Technology Innovation Center, Shandong First Medical 
University & Shandong Academy of Medical Sciences, Jinan, China.
(5)Department of Cardiology, the First Affiliated Hospital of Shandong First 
Medical University & Shandong Provincial Qianfoshan Hospital, Jinan, China.

BACKGROUND: Large language models (LLMs) show promise in clinical decision 
support; however, their role in risk prediction for heart failure (HF) remains 
uncertain.
OBJECTIVE: This pilot study evaluated the prognostic performance and 
reproducibility of two general-purpose LLMs, ChatGPT and DeepSeek, using 
structured clinical data and unstructured discharge summaries, compared with a 
conventional clinical model.
METHODS: Structured data from the Zigong HF study included 473 hospitalized HF 
patients with 33 clinical variables predicting a 90-day composite outcome of 
all-cause death or rehospitalization. Discharge summaries from the MIMIC-IV 
cohort included 2,091 ICU HF patients predicting 1-year all-cause mortality. 
Standardized prompts were used to obtain predicted probabilities from each LLM. 
Model predictions were compared with logistic regression results, and 
reproducibility was assessed using intraclass correlation coefficients.
RESULTS: In the Zigong HF study, both LLMs showed limited discrimination (AUC 
0.59 for ChatGPT, 0.56 for DeepSeek), performing below the conventional model 
(AUC 0.63). In the MIMIC-IV cohort, ChatGPT achieved higher discrimination (AUC 
0.72) than DeepSeek (AUC 0.67, P < 0.001) and comparable performance to the 
clinical model (AUC 0.74, P = 0.31). Decision curve analysis showed modest 
benefit for ChatGPT at low-to-moderate thresholds, while DeepSeek offered 
minimal benefit. Repeated predictions showed significant variability for both 
models.
CONCLUSIONS: This pilot study provides preliminary evidence that LLMs have 
limited predictive value for structured data but show comparable performance in 
text-based risk prediction. These findings suggest potential for LLMs in 
processing unstructured clinical information and highlight the need for 
validation in larger, contemporary cohorts.

Copyright © 2025 The Author(s). Published by Wolters Kluwer Health, Inc.

DOI: 10.1097/JS9.0000000000004062
PMID: 41255298


589. Oral Surg Oral Med Oral Pathol Oral Radiol. 2025 Nov 28:S2212-4403(25)01320-3. 
doi: 10.1016/j.oooo.2025.11.008. Online ahead of print.

Secretary or SecretarAI: assessing the triage performance of AI vs human staff 
in a specialized dental clinic.

Wasserman G(1), Grinberg N(2), Peleg O(3), Shuster A(3), Ianculovici C(2), 
Hurvitz AZ(4), Kleinman S(2).

Author information:
(1)Oral Medicine Unit, Department of Otolaryngology Head and Neck Surgery and 
Maxillofacial Surgery, Tel-Aviv Sourasky Medical Center, Tel Aviv, Israel. 
Electronic address: giladwas@tlvmc.gov.il.
(2)Department of Oral and Maxillofacial Surgery, Tel-Aviv Sourasky Medical 
Center, Tel Aviv, Israel.
(3)Department of Oral and Maxillofacial Surgery, Tel-Aviv Sourasky Medical 
Center, Tel Aviv, Israel; The Maurice and Gabriela Goldschleger School of Dental 
Medicine, Sackler Faculty of Medicine, Tel Aviv University, Tel Aviv, Israel.
(4)Oral Medicine Unit, Department of Otolaryngology Head and Neck Surgery and 
Maxillofacial Surgery, Tel-Aviv Sourasky Medical Center, Tel Aviv, Israel; The 
Maurice and Gabriela Goldschleger School of Dental Medicine, Sackler Faculty of 
Medicine, Tel Aviv University, Tel Aviv, Israel.

OBJECTIVE: Effective patient triage in specialized clinics is crucial for 
managing long waiting lists and mitigating clinical risk. This is often 
performed by non-clinical administrative staff, whose prioritization may differ 
from clinical experts. This study aimed to compare the triage performance of 
Large Language Models (LLMs) against human staff, using senior clinicians as the 
gold standard.
STUDY DESIGN: A cross-sectional survey study was conducted at a tertiary medical 
center. A custom survey presenting 19 clinical vignettes was administered to 
secretaries, dental assistants, and five distinct LLMs (Chat GPT-o3, Chat 
GPT-o4, Gemini Pro2.5, Gemini Flash2.5, OpenEvidence). Participants ranked 
hypothetical patients by urgency. An expert panel of senior clinicians 
established a gold-standard ranking for comparison. Agreement was assessed using 
Fleiss' κ and Cohen's weighted κ.
RESULTS: Artificial intelligence (AI) models demonstrated significantly higher 
agreement with the expert gold standard (Fleiss' κ = 0.572) compared to dental 
assistants (κ = 0.249) and secretaries (κ = 0.227). The AI cohort also showed 
superior internal consistency (Cronbach's α = 0.950). Among individual models, 
GPT-o3 (rationalization model) achieved the highest agreement (weighted κ = 
0.681). Years of experience among human staff did not correlate with improved 
triage accuracy.
CONCLUSIONS: Contemporary LLMs can align more closely with expert clinical 
judgment in patient prioritization than non-clinical staff. While not a 
substitute for human oversight, AI shows significant promise as a reliable and 
consistent decision-support tool to augment existing triage processes, 
potentially enhancing patient safety and improving resource allocation in 
specialized care.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.oooo.2025.11.008
PMID: 41456988

Conflict of interest statement: DECLARATION OF INTERESTS None.


590. PLoS One. 2025 Nov 14;20(11):e0334120. doi: 10.1371/journal.pone.0334120. 
eCollection 2025.

Enhancing sarcasm detection on social media: A comprehensive study using LLMs 
and BERT with multi-headed attention on SARC.

Zhang L(1), Faseeh M(2), Naqvi SSA(3), Hu L(1), Ghani A(4)(5).

Author information:
(1)School of Foreign Studies, Hunan First Normal University, Changsha, Hunan, 
China.
(2)Department of Computer Science, COMSATS University Islamabad, Attock Campus, 
Punjab, Republic of Pakistan.
(3)Department of Electrical Engineering, COMSATS University Islamabad, Attock 
Campus, Punjab, Republic of Pakistan.
(4)Department of Computer Science, International Islamic University, Islamabad, 
Pakistan.
(5)Department of Computer Science, School of Engineering and Digital Sciences, 
Nazarbayev University, Astana, Kazakhstan.

Sarcasm detection in natural language processing (NLP) remains a complex 
challenge, especially in social media, where contextual clues are often subtle. 
This study addresses this challenge by leveraging transformer-based models, 
including BERT, GPT-3, Claude-2, and Llama-2, for sarcasm detection on a large 
dataset from the Self-Annotated Reddit Corpus (SARC). The proposed method 
utilizes multi-head attention mechanisms to enhance model performance by 
capturing nuanced contextual relationships in the text. Fine-tuning of BERT, 
GPT-3, and Llama-2 was conducted to ensure a fair comparison and to provide a 
more detailed understanding of sarcasm in context. Our BERT-based model achieved 
state-of-the-art performance, with precision, recall, F1 score, and accuracy of 
0.918, 0.917, 0.917, and 0.917, respectively, outperforming the other models. 
The effectiveness of our approach is demonstrated through rigorous statistical 
validation, ablation studies, and error analysis, providing robust evidence of 
its superiority. This study also highlights the significance of fine-tuning, 
machine translation, and multi-head attention in improving sarcasm detection.

Copyright: © 2025 Lihong Zhang et al. This is an open access article distributed 
under the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0334120
PMCID: PMC12617957
PMID: 41237169 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


591. BMC Infect Dis. 2025 Nov 17;25(1):1591. doi: 10.1186/s12879-025-12022-x.

Comparison of the accuracy and reliability of ChatGPT-4o and Gemini in answering 
HIV-related questions.

Tarhan MS(1), Sahin Ozdemir M(2).

Author information:
(1)Department of Infectious Diseases and Clinical Microbiology, Mardin Training 
and Research Hospital, Mardin, 47100, Türkiye. muhammetsalihtarhan@gmail.com.
(2)Department of Infectious Diseases and Clinical Microbiology, Basaksehir Cam 
and Sakura City Hospital, Istanbul, 34480, Türkiye.

BACKGROUND: Large language models (LLMs) such as ChatGPT and Gemini are 
increasingly being used to obtain health information, including topics such as 
HIV. This study aims to comparatively evaluate the accuracy, reliability, and 
reproducibility of ChatGPT and Gemini in answering HIV-related questions 
obtained from official public health sources, clinical guidelines, and social 
media.
METHODS: A total of 156 HIV-related questions were asked to ChatGPT-4o and 
Google Gemini 1.5 Flash across three categories: questions derived from the 
United States Centers for Disease Control and Prevention (CDC) resources (44.2%, 
n = 69), guidelines (30.8%, n = 48), and social media (25.0%, n = 39). Responses 
were rated on a 4-point scale (1 = completely wrong, 4 = completely correct) by 
two infectious disease specialists. The reproducibility of both LLMs was also 
evaluated.
RESULTS: The median score (IQR) of the answers generated for all questions was 
4.00 (0.00) for ChatGPT and 4.00 (1.00) for Gemini (p = 0.051). The rate of 
completely correct answers was 81.4% for ChatGPT and 71.8% for Gemini 
(p = 0.045). ChatGPT demonstrated significantly lower accuracy in 
guideline-based questions (47.9%) than in CDC-related (97.1%) and social 
media-derived (94.9%) questions (p < 0.001 for both). Similarly, Gemini 
demonstrated significantly lower accuracy in guideline-based questions (35.4%) 
compared to CDC-related (88.4%) and social media-derived (87.2%) questions 
(p < 0.001 for both). Considering the questions according to the topics, the 
lowest accuracy rate for both LLMs was in the subject of ‘Prevention and 
Treatment’ (67.2% for ChatGPT, 54.7% for Gemini). The reproducibility of the 
answers was 94.8% for ChatGPT and 90.3% for Gemini.
CONCLUSION: ChatGPT and Gemini, answered CDC- and social media–based questions 
with high accuracy. However, both LLMs showed lower accuracy for guideline-based 
and “Prevention and Treatment” questions. These findings suggest that while such 
models may provide useful general information, they are not yet reliable for 
clinical decision-making, and their outputs should be verified against 
evidence-based clinical guidelines.
SUPPLEMENTARY INFORMATION: The online version contains supplementary material 
available at 10.1186/s12879-025-12022-x.

DOI: 10.1186/s12879-025-12022-x
PMCID: PMC12625381
PMID: 41250016

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


592. BMC Med Educ. 2025 Dec 22;25(1):1705. doi: 10.1186/s12909-025-08292-3.

AI-generated biochemistry test item parameters in MST test conditions.

Polat M(1)(2), Karadag E(3)(4).

Author information:
(1)Anadolu University, Eskisehir, Turkey.
(2)Khazar University, Baku, Azerbaijan.
(3)Akdeniz University, Antalya, Türkiye. engin.karadag@hotmail.com.
(4)Khazar University, Baku, Azerbaijan. engin.karadag@hotmail.com.

BACKGROUND: This study investigated whether ChatGPT 4o could accurately estimate 
the difficulty of medical assessment items by comparing its predictions with 
empirically-derived parameters from multistage testing simulations.
METHODS: Using a hybrid simulation-validation design, the researchers had 
ChatGPT 4o generate 80 multiple-choice biochemistry questions with difficulty 
estimates (b-parameters), which were then administered via simulated multistage 
testing to 5,000 virtual examinees.
RESULTS: The analysis revealed moderate agreement between AI-generated and 
simulation-derived difficulty parameters (r = 0.612, 95% CI [0.472, 0.725]), 
though ChatGPT systematically overestimated item difficulty with a mean bias of 
0.240 (SD = 0.503). While the mean absolute error was relatively modest at 
0.447, with 91% of items showing errors below 1.0 logits, the AI's estimates 
were particularly inaccurate for very easy items, where 83% exhibited absolute 
errors exceeding 0.5 logits compared to only 29% for medium difficulty items. 
These findings suggest that while ChatGPT 4o demonstrates promise as a tool for 
preliminary item generation in medical education assessment, it requires 
empirical calibration and expert oversight before operational implementation, as 
the systematic bias indicates the AI lacks access to real-world performance 
feedback.
CONCLUSIONS: The study's conclusions are tempered by important limitations, 
including its reliance on simulation-based validation rather than actual student 
performance data and its single-institution sample, underscoring the need for 
rigorous psychometric validation when integrating artificial intelligence into 
medical education assessment.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-08292-3
PMCID: PMC12723931
PMID: 41430216 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: All procedures performed in studies involving in accordance with 
the ethical standards of the institutional and/or national research committee 
and with the 1964 Helsinki Declaration and its later amendments or comparable 
ethical standards. Consent for publication: Not applicable. Competing interests: 
The authors declare no competing interests.


593. Digit Health. 2025 Mar 25;11:20552076251327945. doi: 10.1177/20552076251327945. 
eCollection 2025 Jan-Dec.

Interpretable artificial intelligence (AI) for cervical cancer risk analysis 
leveraging stacking ensemble and expert knowledge.

Roy P(1)(2), Hasan M(1), Islam MR(1), Uddin MP(1).

Author information:
(1)Computer Science and Engineering, Hajee Mohammad Danesh Science and 
Technology University, Dinajpur, Bangladesh.
(2)Computer Science and Engineering, Sylhet International University, Sylhet, 
Bangladesh.

OBJECTIVES: This study develops a machine learning (ML)-based cervical cancer 
prediction system emphasizing explainability. A hybrid feature selection method 
is proposed to enhance predictive accuracy and stability, alongside evaluation 
of multiple classification algorithms. The integration of explainable artificial 
intelligence (XAI) techniques ensures transparency and interpretability in model 
decisions.
METHODS: A hybrid feature selection approach combining correlation-based 
selection and recursive feature elimination is introduced. An ensemble model 
integrating random forest, extreme gradient boosting, and logistic regression is 
compared against eight classical ML algorithms. Generative artificial 
intelligence methods, such as variational autoencoders and generative teaching 
networks, were evaluated but showed suboptimal performance. The research 
integrates global and local XAI techniques, including individual feature 
contributions and tree-based explanations, to interpret model decisions. The 
effects of feature selection and data balancing on classification performance 
are examined to stabilize precision, recall, and F1 scores. Classical ML models 
without preprocessing achieve 95-96% accuracy but exhibit instability.
RESULTS: The proposed feature selection and data balancing strategies 
significantly enhance classification stability, creating a robust predictive 
model. The ensemble model achieves 98% accuracy with an area under the curve of 
99.50%, outperforming other models. Domain experts validate critical 
contributing features, confirming practical relevance. Incorporating domain 
knowledge with XAI techniques significantly increases transparency, making 
predictions interpretable and trustworthy for clinical use.
CONCLUSION: Hybrid feature selection combined with ensemble learning 
substantially improves cervical cancer prediction accuracy and reliability. The 
integration of XAI techniques ensures transparency, supporting interpretability 
and trustworthiness, demonstrating significant potential in clinical 
decision-making.

© The Author(s) 2025.

DOI: 10.1177/20552076251327945
PMCID: PMC11938887
PMID: 40144051

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


594. Acad Radiol. 2025 Feb;32(2):604-611. doi: 10.1016/j.acra.2024.09.042. Epub 2024 
Oct 15.

Large Language Models can Help with Biostatistics and Coding Needed in Radiology 
Research.

Ghosh A(1), Li H(2), Trout AT(3).

Author information:
(1)Department of Radiology, Cincinnati Children's Hospital Medical Center, 
Cincinnati, Ohio (A.G., H.L., A.T.T.); Department of Radiology, Nationwide 
Children's Hospital, Columbus, Ohio (A.G.). Electronic address: 
adarsh.ghosh.rad@gmail.com.
(2)Department of Radiology, Cincinnati Children's Hospital Medical Center, 
Cincinnati, Ohio (A.G., H.L., A.T.T.); Department of Radiology, University of 
Cincinnati College of Medicine, Cincinnati, Ohio (H.L., A.T.T.); Imaging 
Research Center, Cincinnati Children's Hospital Medical Center, Cincinnati, Ohio 
(H.L.).
(3)Department of Radiology, Cincinnati Children's Hospital Medical Center, 
Cincinnati, Ohio (A.G., H.L., A.T.T.); Department of Radiology, University of 
Cincinnati College of Medicine, Cincinnati, Ohio (H.L., A.T.T.); Department of 
Pediatrics, University of Cincinnati College of Medicine, Cincinnati, Ohio 
(A.T.T.).

INTRODUCTION: Original research in radiology often involves handling large 
datasets, data manipulation, statistical tests, and coding. Recent studies show 
that large language models (LLMs) can solve bioinformatics tasks, suggesting 
their potential in radiology research. This study evaluates an LLM's ability to 
provide statistical and deep learning solutions and code for radiology research.
MATERIALS AND METHODS: We used web-based chat interfaces available for 
ChatGPT-4o, ChatGPT-3.5, and Google Gemini. EXPERIMENT 1: BIOSTATISTICS AND DATA 
VISUALIZATION: We assessed each LLMs' ability to suggest biostatistical tests 
and generate R code for the same using a Cancer Imaging Archive dataset. Prompts 
were based on statistical analyses from a peer-reviewed manuscript. The 
generated code was tested in R Studio for correctness, runtime errors and the 
ability to generate the requested visualization. EXPERIMENT 2: DEEP LEARNING: We 
used the RSNA-STR Pneumonia Detection Challenge dataset to evaluate ChatGPT-4o 
and Gemini's ability to generate Python code for transformer-based image 
classification models (Vision Transformer ViT-B/16). The generated code was 
tested in a Jupiter Notebook for functionality and run time errors.
RESULTS: Out of the 8 statistical questions posed, correct statistical answers 
were suggested for 7 (ChatGPT-4o), 6 (ChatGPT-3.5), and 5 (Gemini) scenarios. 
The R code output by ChatGPT-4o had fewer runtime errors (6 out of the 7 total 
codes provided) compared to ChatGPT-3.5 (5/7) and Gemini (5/7). Both ChatGPT4o 
and Gemini were able to generate visualization requested with a few run time 
errors. Iteratively copying runtime errors from the code generated by ChatGPT4o 
into the chat helped resolve them. Gemini initially hallucinated during code 
generation but was able to provide accurate code on restarting the experiment. 
ChatGPT4-o and Gemini successfully generated initial Python code for deep 
learning tasks. Errors encountered during implementation were resolved through 
iterations using the chat interface, demonstrating LLM utility in providing 
baseline code for further code refinement and resolving run time errors.
CONCLUSION: LLMs can assist in coding tasks for radiology research, providing 
initial code for data visualization, statistical tests, and deep learning models 
helping researchers with foundational biostatistical knowledge. While LLM can 
offer a useful starting point, they require users to refine and validate the 
code and caution is necessary due to potential errors, the risk of 
hallucinations and data privacy regulations.
SUMMARY STATEMENT: LLMs can help with coding and statistical problems in 
radiology research. This can help primary authors trouble shoot coding needed in 
radiology research.

Copyright © 2025 The Association of University Radiologists. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.acra.2024.09.042
PMID: 39406582 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


595. Transl Vis Sci Technol. 2025 Jul 1;14(7):9. doi: 10.1167/tvst.14.7.9.

Evaluating Large Language Models in Ptosis-Related inquiries: A Cross-Lingual 
Study.

Niu LH(1), Wei L(2), Qin B(3), Chen T(1), Dong L(1), He Y(1), Jiang X(1), Wang 
M(1), Ma L(1), Geng J(1), Wang L(1), Li D(1)(4).

Author information:
(1)Beijing Tongren Eye Center, and Beijing Ophthalmology Visual Science Key Lab, 
Beijing Tongren Hospital, Capital Medical University, Beijing, People's Republic 
of China.
(2)Mingsii Co., Ltd, Beijing, People's Republic of China.
(3)Aier Eye Hospital Group Co., Ltd., Beijing Aier Eye Hospital, Beijing, 
People's Republic of China.
(4)https://orcid.org/0000-0002-4226-662X.

PURPOSE: The purpose of this study was to evaluate the performance of large 
language models (LLMs)-GPT-4, GPT-4o, Qwen2, and Qwen2.5-in addressing patient- 
and clinician-focused questions on ptosis-related inquiries, emphasizing 
cross-lingual applicability and patient-centric assessment.
METHODS: We collected 11 patient-centric and 50 doctor-centric questions 
covering ptosis symptoms, treatment, and postoperative care. Responses generated 
by GPT-4, GPT-4o, Qwen2, and Qwen2.5 were evaluated using predefined criteria: 
accuracy, sufficiency, clarity, and depth (doctor questions); and helpfulness, 
clarity, and empathy (patient questions). Clinical assessments involved 30 
patients with ptosis and 8 oculoplastic surgeons rating responses on a 5-point 
Likert scale.
RESULTS: For doctor questions, GPT-4o outperformed Qwen2.5 in overall 
performance (53.1% vs. 18.8%, P = 0.035) and completeness (P = 0.049). For 
patient questions, GPT-4o scored higher in helpfulness (mean rank = 175.28 vs. 
155.72, P = 0.035), with no significant differences in clarity or empathy. 
Qwen2.5 exhibited superior Chinese-language clarity compared to English (P = 
0.023).
CONCLUSIONS: LLMs, particularly GPT-4o, demonstrate robust performance in 
ptosis-related inquiries, excelling in English and offering clinically valuable 
insights. Qwen2.5 showed advantages in Chinese clarity. Although promising for 
patient education and clinician support, these models require rigorous 
validation, domain-specific training, and cultural adaptation before clinical 
deployment. Future efforts should focus on refining multilingual capabilities 
and integrating real-time expert oversight to ensure safety and relevance in 
diverse healthcare contexts.
TRANSLATIONAL RELEVANCE: This study bridges artificial intelligence (AI) 
advancements with clinical practice by demonstrating how optimized LLMs can 
enhance patient education and cross-linguistic clinician support tools in 
ptosis-related inquiries.

DOI: 10.1167/tvst.14.7.9
PMCID: PMC12279073
PMID: 40668049 [Indexed for MEDLINE]

Conflict of interest statement: Disclosure: L.-H. Niu, None; L. Wei, None; B. 
Qin, None; T. Chen, None; L. Dong, None; Y. He, None; X. Jiang, None; M. Wang, 
None; L. Ma, None; J. Geng, None; L. Wang, None; D. Li, None


596. IEEE J Biomed Health Inform. 2025 Feb 21;PP. doi: 10.1109/JBHI.2025.3544560. 
Online ahead of print.

GPT-Based Automated Induction: Vulnerability Detection in Medical Software.

Deng L, Lei H, Khan F, Srivastava G, Chen J, Haque M.

Integrating Natural Language Processing (NLP) with Generative Pre-trained 
Transformer (GPT) models plays a pivotal role in enhancing the accuracy and 
efficiency of healthcare software, which is essential for patient safety and 
providing high-quality care. The precision of healthcare software is fundamental 
to protecting the well-being of the patient. In addition, it can ensure the 
delivery of superior care, maintain the integrity of healthcare systems, and 
promote trust and cost-effectiveness. It is necessary to emphasize the 
importance of software reliability in its development and deployment. Symbolic 
execution serves as a vital technology in automated vulnerability detection. 
However, symbolic execution often faces problems such as path explosion, which 
seriously affects efficiency. Although there have been several studies to reduce 
the number of computational paths in symbolic execution, this problem remains a 
major obstacle. Therefore, more efficient solutions are urgently needed to 
ensure the software security. This paper proposes a large-scale language 
model(LLM) induction method mitigating path explosion applied to symbolic 
execution engines. In contrast to traditional symbolic execution engines, which 
often result in timeout or out-of-memory detection, our approach achieves the 
task of detecting vulnerabilities in seconds. Furthermore, our proposal improves 
the scalability of symbolic execution, allowing more extensive and complex 
programs to be analyzed without significant increases in computational resources 
or time. This scalability is crucial to tackling modern software systems and 
improving the efficiency and effectiveness of automated defect verification in 
healthcare software.

DOI: 10.1109/JBHI.2025.3544560
PMID: 40031801


597. J Craniomaxillofac Surg. 2025 Apr;53(4):355-360. doi: 
10.1016/j.jcms.2024.12.009. Epub 2025 Jan 10.

Evaluation of a context-aware chatbot using retrieval-augmented generation for 
answering clinical questions on medication-related osteonecrosis of the jaw.

Steybe D(1), Poxleitner P(2), Aljohani S(3), Herlofson BB(4), Nicolatou-Galitis 
O(5), Patel V(6), Fedele S(7), Kwon TG(8), Fusco V(9), Pichardo SEC(10), 
Obermeier KT(2), Otto S(2), Rau A(11), Russe MF(12).

Author information:
(1)Department of Oral and Maxillofacial Surgery and Facial Plastic Surgery, 
University Hospital, LMU Munich, Munich, Germany. Electronic address: 
david.steybe@med.uni-muenchen.de.
(2)Department of Oral and Maxillofacial Surgery and Facial Plastic Surgery, 
University Hospital, LMU Munich, Munich, Germany.
(3)Department of Oral and Maxillofacial Surgery and Facial Plastic Surgery, 
University Hospital, LMU Munich, Munich, Germany; Department of Oral Diagnostic 
Sciences, Faculty of Dentistry, King Abdulaziz University, Jeddah, Saudi Arabia.
(4)Department of Oral Surgery and Oral Medicine, Faculty of Dentistry, 
University of Oslo and Department of Otorhinolaryngology - Head and Neck Surgery 
Division for Head, Neck and Reconstructive Surgery, Oslo University Hospital, 
Oslo, Norway.
(5)Oncology Patient Support Company PC, CureCancer - mycancer.gr, Athens, 
Greece.
(6)Department of Oral Surgery, Guy's and St Thomas' Hospital, London, United 
Kingdom.
(7)UCL Eastman Dental Institute and NIHR UCLH Biomedical Research Centre, 
University College London, United Kingdom.
(8)Department of Oral and Maxillofacial Surgery, School of Dentistry, Kyungpook 
National University, Daegu, South Korea.
(9)Oncology Unit, Department of Medicine and Translational Medicine Unit, DAIRI 
- Department of Integration, Research and Innovation, "SS Antonio e Biagio e 
C.Arrigo" Hospital, Alessandria, Italy.
(10)Department of Oral & Maxillofacial Surgery, University Medical Center 
Groningen, Groningen, the Netherlands.
(11)Department of Neuroradiology, University Medical Center Freiburg, Faculty of 
Medicine, University of Freiburg, Freiburg, Germany.
(12)Department of Diagnostic and Interventional Radiology, University Medical 
Center Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany.

Erratum in
    J Craniomaxillofac Surg. 2025 Dec;53(12):2354. doi: 
10.1016/j.jcms.2025.11.011.

The potential of large language models (LLMs) in medical applications is 
significant, and Retrieval-augmented generation (RAG) can address the weaknesses 
of these models in terms of data transparency and scientific accuracy by 
incorporating current scientific knowledge into responses. In this study, RAG 
and GPT-4 by OpenAI were applied to develop GuideGPT, a context aware chatbot 
integrated with a knowledge database from 449 scientific publications designed 
to provide answers on the prevention, diagnosis, and treatment of 
medication-related osteonecrosis of the jaw (MRONJ). A comparison was made with 
a generic LLM ("PureGPT") across 30 MRONJ-related questions. Ten international 
experts in MRONJ evaluated the responses based on content, language, scientific 
explanation, and agreement using 5-point Likert scales. Statistical analysis 
using the Mann-Whitney U test showed significantly better ratings for GuideGPT 
than PureGPT regarding content (p = 0.006), scientific explanation (p = 0.032), 
and agreement (p = 0.008), though not for language (p = 0.407). Thus, this study 
demonstrates RAG to be a promising tool to improve response quality and 
reliability of LLMs by incorporating domain-specific knowledge. This approach 
addresses the limitations of generic chatbots and can provide traceable and 
up-to-date responses essential for clinical practice.

Copyright © 2024 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.jcms.2024.12.009
PMID: 39799075 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of interest The authors declare that 
they have no conflicts of interest regarding the publication of this article.


598. J Perinat Med. 2025 Sep 23;53(8):993-999. doi: 10.1515/jpm-2025-0285. Print 2025 
Oct 27.

The FAIR framework: ethical hybrid peer review.

Grünebaum A(1), Dudenhausen J(2), Chervenak FA(1).

Author information:
(1)5799 Northwell Health, Zucker School of Medicine , Hempstead, NY, USA.
(2)Department of Obstetrics, Charité - Universitätsmedizin Berlin, Berlin, 
Germany.

OBJECTIVES: Traditional peer review faces critical challenges including 
systematic bias, prolonged delays, reviewer fatigue, and lack of transparency. 
These failures violate ethical obligations of beneficence, justice, and autonomy 
while hindering scientific progress and costing billions annually in 
academic labor. To propose an ethically-guided hybrid peer review system that 
integrates generative artificial intelligence with human expertise while 
addressing fundamental shortcomings of current review processes.
METHODS: We developed the FAIR Framework (Fairness, Accountability, Integrity, 
and Responsibility) through systematic analysis of peer review failures and 
integration of AI capabilities. The framework employs standardized prompt 
engineering to guide AI evaluation of manuscripts while maintaining human 
oversight throughout all stages.
RESULTS: FAIR addresses bias through algorithmic detection and standardized 
evaluation protocols, ensures accountability via transparent audit trails and 
documented decisions, maintains integrity through secure local AI processing and 
confidentiality safeguards, and upholds responsibility through ethical oversight 
and constructive feedback mechanisms. The hybrid model automates repetitive 
tasks including initial screening, methodological verification, and plagiarism 
detection while preserving human judgment for novelty assessment, ethical 
evaluation, and final decisions.
CONCLUSIONS: The FAIR Framework offers a principled solution to peer review 
inefficiencies by combining AI-enabled consistency and speed with essential 
human expertise. This hybrid approach reduces review delays, eliminates 
systematic bias, and enhances transparency while maintaining confidentiality and 
editorial control. Implementation could significantly reduce the estimated 100 
million hours of global reviewer time annually while improving review quality 
and equity across diverse research communities.

© 2025 the author(s), published by De Gruyter, Berlin/Boston.

DOI: 10.1515/jpm-2025-0285
PMID: 40981366 [Indexed for MEDLINE]


599. J Med Internet Res. 2025 Jun 18;27:e69929. doi: 10.2196/69929.

Performance of ChatGPT-4o and Four Open-Source Large Language Models in 
Generating Diagnoses Based on China's Rare Disease Catalog: Comparative Study.

Zhong W(1), Liu Y(1), Liu Y(1), Yang K(1), Gao H(1), Yan H(1), Hao W(1), Yan 
Y(#)(1), Yin C(#)(1).

Author information:
(1)Department of Prenatal Diagnosis, Beijing Obstetrics and Gynecology Hospital, 
Capital Medical University, Beijing Maternal and Child Health Care Hospital, No. 
251 Yaojiayuan Road, Chaoyang District, Beijing, China, 8618810963279.
(#)Contributed equally

BACKGROUND: Diagnosing rare diseases remains challenging due to their inherent 
complexity and limited physician knowledge. Large language models (LLMs) offer 
new potential to enhance diagnostic workflows.
OBJECTIVE: This study aimed to evaluate the diagnostic accuracy of ChatGPT-4o 
and 4 open-source LLMs (qwen2.5:7b, Llama3.1:8b, qwen2.5:72b, and Llama3.1:70b) 
for rare diseases, assesses the language effect on diagnostic performance, and 
explore retrieval augmented generation (RAG) and chain-of-thought (CoT) 
reasoning.
METHODS: We extracted clinical manifestations of 121 rare diseases from China's 
inaugural rare disease catalog. ChatGPT-4o generated a primary and 5 
differential diagnoses, while 4 LLMs were assessed in both English and Chinese 
contexts. The lowest-performing model underwent RAG and CoT re-evaluation. 
Diagnostic accuracy was compared via the McNemar test. A survey evaluated 11 
clinicians' familiarity with rare diseases.
RESULTS: ChatGPT-4o demonstrated the highest diagnostic accuracy with 90.1%. 
Language effects varied across models: qwen2.5:7b showed comparable performance 
in Chinese (51.2%) and English (47.9%; χ²1=0.32, P=.57), whereas Llama3.1:8b 
exhibited significantly higher English accuracy (67.8% vs 31.4%; χ²1=40.20, 
P<.001). Among larger models, qwen2.5:72b maintained cross-lingual consistency 
considering the odds ratio (OR; Chinese: 82.6% vs English: 83.5%; OR 0.88, 95% 
CI 0.27-2.76,P=1.000), contrasting with Llama3.1:70b's language-dependent 
variation (Chinese: 80.2% vs English: 90.1%; OR 0.29,95% CI 0.08-0.83, P=.02). 
Cross-model comparisons revealed Llama3.1:8b underperformed qwen2.5:7b in 
Chinese (χ²1=13.22,P<.001) but surpassed it in English (χ²1=13.92,P<.001). No 
significant differences were observed between qwen2.5:72b and Llama3.1:70b 
(English: OR 0.33, P=.08; Chinese: OR 1.5, 95% CI 0.48-5.12,P=.07); qwen2.5:72b 
matched ChatGPT-4o's performance in both languages (English: OR 0.33, P=.08; 
Chinese: OR 0.44, P=.09); Llama3.1:70b mirrored ChatGPT-4o's English accuracy 
(OR 1, P=1.000) but lagged in Chinese (OR 0.33; P=.02). RAG implementation 
enhanced qwen2.5:7b's accuracy to 79.3% (χ²1=31.11, P<.001) with 85.9% retrieval 
precision. The distilled model Deepseek-R1:7b markedly underperformed (9.9% vs 
qwen2.5:7b; χ²1=42.19, P<.001). Clinician surveys revealed significant knowledge 
gaps in rare disease management.
CONCLUSIONS: ChatGPT-4o demonstrated superior diagnostic performance for rare 
diseases. While Llama3.1:8b demonstrates viability for localized deployment in 
resource-constrained English diagnostic workflows, Chinese applications require 
larger models to achieve comparable diagnostic accuracy. This urgency is 
heightened by the release of open-source models like DeepSeek-R1, which may see 
rapid adoption without thorough validation. Successful clinical implementation 
of LLMs requires 3 core elements: model parameterization, user language, and 
pretraining data. The integration of RAG significantly enhanced open-source LLM 
accuracy for rare disease diagnosis, although caution remains warranted for 
low-parameter reasoning models showing substantial performance limitations. We 
recommend hospital IT departments and policymakers prioritize language relevance 
in model selection and consider integrating RAG with curated knowledge bases to 
enhance diagnostic utility in constrained settings, while exercising caution 
with low-parameter models.

© Wei Zhong, YiFan Liu, Yan Liu, Kai Yang, HuiMin Gao, HuiHui Yan, WenJing Hao, 
YouSheng Yan, ChengHong Yin. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org).

DOI: 10.2196/69929
PMCID: PMC12192912
PMID: 40532199 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


600. JMIR Med Inform. 2025 Apr 14;13:e62909. doi: 10.2196/62909.

Summarizing Online Patient Conversations Using Generative Language Models: 
Experimental and Comparative Study.

Nair RAS(1), Hartung M(2), Heinisch P(1), Jaskolski J(2), Starke-Knäusel C(2), 
Veríssimo S(2), Schmidt DM(1), Cimiano P(1)(2).

Author information:
(1)Cognitive Interaction Technology Center, Faculty of Technology, Bielefeld 
University, Bielefeld, Germany.
(2)Semalytix GmbH, Bielefeld, Germany.

BACKGROUND: Social media is acknowledged by regulatory bodies (eg, the Food and 
Drug Administration) as an important source of patient experience data to learn 
about patients' unmet needs, priorities, and preferences. However, current 
methods rely either on manual analysis and do not scale, or on automatic 
processing, yielding mainly quantitative insights. Methods that can 
automatically summarize texts and yield qualitative insights at scale are 
missing.
OBJECTIVE: The objective of this study was to evaluate to what extent 
state-of-the-art large language models can appropriately summarize posts shared 
by patients in web-based forums and health communities. Specifically, the goal 
was to compare the performance of different language models and prompting 
strategies on the task of summarizing documents reflecting the experiences of 
individual patients.
METHODS: In our experimental and comparative study, we applied 3 different 
language models (Flan-T5, Generative Pretrained Transformer [GPT], GPT-3, and 
GPT-3.5) in combination with various prompting strategies to the task of 
summarizing posts from patients in online communities. The generated summaries 
were evaluated with respect to 124 manually created summaries as a ground-truth 
reference. As evaluation metrics, we used 2 standard metrics from the field of 
text generation, namely, Recall-Oriented Understudy for Gisting Evaluation 
(ROUGE) and BERTScore, to compare the automatically generated summaries to the 
manually created reference summaries.
RESULTS: Among the zero-shot prompting-based large language models investigated, 
GPT-3.5 performed better than the other models with respect to the ROUGE 
metrics, as well as with respect to BERTScore. While zero-shot prompting seems 
to be a good prompting strategy, overall GPT-3.5 in combination with directional 
stimulus prompting in a 3-shot setting had the best results with respect to the 
aforementioned metrics. A manual investigation of the summarization of the 
best-performing method showed that the generated summaries were accurate and 
plausible compared to the manual summaries.
CONCLUSIONS: Taken together, our results suggest that state-of-the-art 
pretrained language models are a valuable tool to provide qualitative insights 
about the patient experience to better understand unmet needs, patient 
priorities, and how a disease impacts daily functioning and quality of life to 
inform processes aimed at improving health care delivery and ensure that drug 
development focuses more on the actual priorities and unmet needs of patients. 
The key limitations of our work are the small data sample as well as the fact 
that the manual summaries were created by 1 annotator only. Furthermore, the 
results hold only for the examined models and prompting strategies, potentially 
not generalizing to other models and strategies.

©Rakhi Asokkumar Subjagouri Nair, Matthias Hartung, Philipp Heinisch, Janik 
Jaskolski, Cornelius Starke-Knäusel, Susana Veríssimo, David Maria Schmidt, 
Philipp Cimiano. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org), 14.04.2025.

DOI: 10.2196/62909
PMCID: PMC12038288
PMID: 40228244 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: By the time of writing, 
MH was an employee of Semalytix GmbH. MH is a shareholder of Semalytix GmbH. PC 
is cofounder and shareholder of Semalytix GmbH. All other authors declare no 
conflicts of interest.


601. Digit Health. 2025 Nov 10;11:20552076251384604. doi: 10.1177/20552076251384604. 
eCollection 2025 Jan-Dec.

Validity of evidence-based recommendations by a large language model for 
interdisciplinary board decisions in neurooncology: An explorative study and 
critical evaluation.

Goldberg M(1), Eisenkolb VM(1), Aftahy AK(2), Negwer C(1), Meyer HS(3), Gempt 
J(3), Meyer B(1), Wagner A(1).

Author information:
(1)Department of Neurosurgery, School of Medicine, Klinikum rechts der Isar, 
Technical University Munich, Munich, Germany.
(2)International Neuroscience Institute, Hannover, Germany.
(3)Department of Neurosurgery, University Medical Center Hamburg-Eppendorf, 
Hamburg, Germany.

OBJECTIVES: This study aims to evaluate the stylistic and structural equivalence 
of Artificial Intelligence (AI)-generated summaries, particularly those by Large 
Language Models (LLMs) like ChatGPT, compared to traditional human-generated 
case summaries in neuro-oncological board decisions. The primary goal is to 
explore the stylistic alignment between AI-generated and human-authored 
summaries from board meeting audio recordings.
METHODS: The study compares 30 traditional human-generated case summaries with 
30 AI-generated summaries based on board meeting audio recordings. Two expert 
raters, blinded to the source of the summaries, evaluated a total of 60 cases. A 
Likert scale was used to assess the plausibility, linguistic style, evidence 
adherence, and reference accuracy of the summaries.
RESULTS: The results indicated that both LLM-generated and human-reviewed 
summaries demonstrated consistently high performance across all criteria 
evaluated. The general plausibility ratings were comparable (LLM: 4.7, Human: 
4.73, P = .959). Linguistic style ratings also showed similarity (LLM: 4.87, 
Human: 4.97, P = .512). In terms of adherence to evidence, the means were close 
(LLM: 4.8, Human: 4.87, P = .541). Reference accuracy was slightly higher for 
AI-generated summaries (LLM: 4.97, Human: 4.9, P = .664). These findings were 
consistent with the results from Rater 2, and statistical analysis using 
Kendall's tau showed no significant differences between methods (P > .05).
CONCLUSION: The study finds that LLM-generated summaries can effectively emulate 
the style and structure of human-authored ones, indicating their promise as an 
additional tool in neuro-oncology. These AI models can enhance documentation 
quality and serve as valuable support in clinical settings. While further 
research is necessary to explore broader applications, LLMs offer exciting 
potential as a complement to traditional decision-making processes.

© The Author(s) 2025.

DOI: 10.1177/20552076251384604
PMCID: PMC12602959
PMID: 41229928

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


602. Am J Gastroenterol. 2025 Dec 1. doi: 10.14309/ajg.0000000000003872. Online ahead 
of print.

Enhancing Patient Empowerment Through Artificial Intelligence in Liver Cancer.

Lee D(1), Maravic Z(2), Moon AM(3)(4), Langenbacher D(5), Kautz A(5), Peck R(5), 
Allaire M(6)(7)(8), Kather JN(9)(10)(11).

Author information:
(1)Inno Community Development Organisation, Guangzhou, China.
(2)Digestive Cancers Europe, Brussels, Belgium.
(3)Division of Gastroenterology and Hepatology, University of North Carolina, 
Chapel Hill, North Carolina, USA.
(4)Lineberger Comprehensive Cancer Center, University of North Carolina, Chapel 
Hill, North Carolina, USA.
(5)International Liver Cancer Movement, Colonne, Germany.
(6)AP-HP Sorbonne Université, Hôpital Universitaire Pitié-Salpêtrière, Service 
d'Hépato-gastroentérologie, Paris, France.
(7)Genomic Instability, Metabolism, Immunity and Liver Tumorigenesis Laboratory, 
Equipe Labellisée LIGUE 2023, Paris, France.
(8)Centre de Recherche des Cordeliers, Sorbonne Université, INSERM, Université 
de Paris, Paris, France.
(9)Else Kroener Fresenius Center for Digital Health, Faculty of Medicine and 
University Hospital Carl Gustav Carus, TUD Dresden University of Technology, 
Dresden, Germany.
(10)Department of Medicine I, Faculty of Medicine and University Hospital Carl 
Gustav Carus, TUD Dresden University of Technology, Dresden, Germany.
(11)Medical Oncology, National Center for Tumor Diseases (NCT), University 
Hospital Heidelberg, Heidelberg, Germany .

Chronic liver disease and liver cancer such as hepatocellular carcinoma have a 
growing global health burden. In many areas, liver disease and cancer have a 
rising incidence, later diagnosis, and higher mortality. Although guidelines 
recommend regular surveillance, the timely detection of liver disease and 
hepatocellular carcinoma remains inconsistent. This is largely due to low 
awareness, restricted access to care, and fragmented healthcare systems. It is 
well known that patient empowerment through knowledge, engagement, and shared 
decision-making could therefore help to improve outcomes. However, this is 
frequently complicated by stigma, low health literacy, and comorbidities. These 
challenges could be improved by artificial intelligence (AI). AI methods can 
analyze healthcare data and could directly affect screening and risk 
stratification. In addition, the emergence of large language models such as 
ChatGPT provides new tools that can support the patient journey. Here, we 
provide a systematic overview of the capabilities of AI methods to potentially 
improve liver cancer care. We highlight that AI tools in liver cancer care could 
be used in 2 ways: They can help healthcare professionals and patients alike. 
Help healthcare professionals-focused AI tools can constitute clinical 
decision-support systems and improve care continuity through telemedicine and 
remote monitoring. Patient-focused AI applications can have the potential to 
empower patients, by providing personalized education, counseling, and improved 
patient engagement. However, we also point out the need for caution in the 
implementation of this technology. Key concerns are related to ethical 
considerations, regulation, data privacy, transparency, algorithmic bias, 
rigorous clinical validation, and patient preferences and needs. When these 
concerns are resolved, AI could help to deliver more personalized, 
participatory, and equitable liver disease care.

Copyright © 2025 by The American College of Gastroenterology.

DOI: 10.14309/ajg.0000000000003872
PMID: 41404849


603. Insights Imaging. 2025 Oct 16;16(1):221. doi: 10.1186/s13244-025-02112-4.

Ultrasound-based deep learning model as an assistant improves the diagnosis of 
ovarian tumors: a multicenter study.

Wang Y(#)(1), Zhang J(#)(2), He Y(1), Wang X(3), Wu X(4), Zhang W(5), Gong M(6), 
Gao D(7), Liu S(1), Liu P(8), Li P(9), Shen L(10), Lyu G(11)(12).

Author information:
(1)Department of Ultrasound, The Second Affiliated Hospital of Fujian Medical 
University, Quanzhou, China.
(2)School of Artificial Intelligence, Shenzhen University, Shenzhen, China.
(3)Department of Clinical Medicine, Quanzhou Medical College, Quanzhou, China.
(4)Department of Ultrasound, Quanzhou First Hospital, Quanzhou, China.
(5)Department of Ultrasound, Zhangzhou Hospital, Zhangzhou, China.
(6)Department of Ultrasound, Chengdu Third People's Hospital, Chengdu, China.
(7)Department of Ultrasound, The First Affiliated Hospital of Dalian Medical 
University, Dalian, China.
(8)College of Engineering, Huaqiao University, Quanzhou, China.
(9)Department of Gynecology and Obstetrics, Quanzhou First Hospital, Quanzhou, 
China. 2165164421@qq.com.
(10)School of Artificial Intelligence, Shenzhen University, Shenzhen, China. 
llshen@szu.edu.cn.
(11)Department of Ultrasound, The Second Affiliated Hospital of Fujian Medical 
University, Quanzhou, China. lgr_feus@sina.com.
(12)Department of Clinical Medicine, Quanzhou Medical College, Quanzhou, China. 
lgr_feus@sina.com.
(#)Contributed equally

BACKGROUND: Deep learning (DL) models based on ultrasound (US) images can 
enhance the ability of radiologists to diagnose ovarian tumors.
MATERIALS AND METHODS: This retrospective study included 916 women with ovarian 
tumors in southeast China who underwent surgery with clear pathology and 
preoperative US examination. The data set was divided into a training (80%) and 
a validation (20%) set. The test set consisted of 81 women with ovarian tumors 
from southwest and northeast China. DL models based on three backbone 
architectures, ResNet-50 (residual CNN), VGG16 (plain CNN), and Vision 
Transformer (ViT), were trained to classify benign, borderline, and malignant 
ovarian tumors. The diagnostic efficiency of primary US doctors combined with 
the DL model was compared with the ADNEX model and a US expert. Additionally, we 
compared the diagnostic performance of primary US doctors before and after being 
assisted by the integrated framework combining visual DL models and large 
language models.
RESULTS: (1) The accuracy of the ResNet50-based DL model for benign, malignant, 
and borderline ovarian tumors was 91.8%, 84.61%, and 82.60% for the test sets, 
respectively. (2) After visual and linguistic DL assistance, the accuracy of 
primary US doctors all improved in the test set (doctor A: 76.62% to 90.90%, 
doctor B: 76.62% to 90.90%, doctor C: 79.22% to 94.54%, doctor D: 76.62% to 
95.95%, doctor E: 76.60% to 95.95%, respectively). (3) The diagnostic 
consistency of primary US doctors for validation and test sets also increased 
(doctor A: 0.671 to 0.912, doctor B: 0.762 to 0.916, doctor C: 0.412 to 0.629, 
doctor D: 0.588 to 0.701, doctor E: 0.528 to 0.710, respectively).
CONCLUSIONS: A DL system combining an image-based model (vision model) and a 
language model was developed to assist radiologists in classifying ovarian 
tumors in US images and enhance diagnostic efficacy.
CRITICAL RELEVANCE STATEMENT: The established model can assist primary US 
doctors in preoperative diagnosis and improve the early detection and timely 
treatment of ovarian tumors.
KEY POINTS: An ultrasound-based deep learning (DL) model was developed for 
ovarian tumors using multi-center patients. An image-based DL model was combined 
with a large language model to establish a diagnostic framework for ovarian 
tumor classification. Our DL model can improve the diagnosis of primary US 
doctors to the level of experts and might assist in surgical decision-making.

© 2025. The Author(s).

DOI: 10.1186/s13244-025-02112-4
PMCID: PMC12532985
PMID: 41102593

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The retrospective study was approved by the Institutional Review 
Board of the Second Affiliated Hospital of Fujian Medical University (IRB No. 
2024323), and written informed consent was waived. The data used in this article 
has been anonymized. The ethics review committee approves this process to avoid 
any form of privacy leakage. Consent for publication: Due to the retrospective 
design and the anonymization of patient data, informed consent was waived for 
this study. Competing interests: The authors declare that they have no competing 
interests.


604. Acad Radiol. 2025 Feb;32(2):888-898. doi: 10.1016/j.acra.2024.09.041. Epub 2024 
Sep 30.

Large Language Models for Simplified Interventional Radiology Reports: A 
Comparative Analysis.

Can E(1), Uller W(2), Vogt K(2), Doppler MC(2), Busch F(3), Bayerl N(4), Ellmann 
S(4), Kader A(3), Elkilany A(5), Makowski MR(3), Bressem KK(3), Adams LC(3).

Author information:
(1)Department of Interventional Radiology, Medical Center - University of 
Freiburg, Faculty of Medicine, University of Freiburg, Germany (E.C., W.U., 
K.V., M.C.D.). Electronic address: elif.can@uniklinik-freiburg.de.
(2)Department of Interventional Radiology, Medical Center - University of 
Freiburg, Faculty of Medicine, University of Freiburg, Germany (E.C., W.U., 
K.V., M.C.D.).
(3)Department of Radiology, Klinikum rechts der Isar, Technical University of 
Munich (TUM), Munich, Germany (F.B., A.K., M.R.M., K.K.B., L.C.A.).
(4)Institute of Radiology, Friedrich-Alexander-Universität Erlangen-Nürnberg, 
University Hospital Erlangen, Erlangen, Germany (N.B., S.E.).
(5)Department of Diagnostic and Interventional Radiology, University Hospital 
Leipzig, Leipzig, Saxony, Germany (A.E.).

PURPOSE: To quantitatively and qualitatively evaluate and compare the 
performance of leading large language models (LLMs), including proprietary 
models (GPT-4, GPT-3.5 Turbo, Claude-3-Opus, and Gemini Ultra) and open-source 
models (Mistral-7b and Mistral-8×7b), in simplifying 109 interventional 
radiology reports.
METHODS: Qualitative performance was assessed using a five-point Likert scale 
for accuracy, completeness, clarity, clinical relevance, naturalness, and error 
rates, including trust-breaking and post-therapy misconduct errors. Quantitative 
readability was assessed using Flesch Reading Ease (FRE), Flesch-Kincaid Grade 
Level (FKGL), SMOG Index, and Dale-Chall Readability Score (DCRS). Paired 
t-tests and Bonferroni-corrected p-values were used for statistical analysis.
RESULTS: Qualitative evaluation showed no significant differences between GPT-4 
and Claude-3-Opus for any metrics evaluated (all Bonferroni-corrected p-values: 
p = 1), while they outperformed other assessed models across five qualitative 
metrics (p < 0.001). GPT-4 had the fewest content and trust-breaking errors, 
with Claude-3-Opus second. However, all models exhibited some level of 
trust-breaking and post-therapy misconduct errors, with GPT-4-Turbo and 
GPT-3.5-Turbo with few-shot prompting showing the lowest error rates, and 
Mistral-7B and Mistral-8×7B showing the highest. Quantitatively, GPT-4 surpassed 
Claude-3-Opus in all readability metrics (all p < 0.001), with a median FRE 
score of 69.01 (IQR: 64.88-73.14) versus 59.74 (IQR: 55.47-64.01) for 
Claude-3-Opus. GPT-4 also outperformed GPT-3.5-Turbo and Gemini Ultra (both 
p < 0.001). Inter-rater reliability was strong (κ = 0.77-0.84).
CONCLUSIONS: GPT-4 and Claude-3-Opus demonstrated superior performance in 
generating simplified IR reports, but the presence of errors across all models, 
including trust-breaking errors, highlights the need for further refinement and 
validation before clinical implementation.
CLINICAL RELEVANCE/APPLICATIONS: With the increasing complexity of 
interventional radiology (IR) procedures and the growing availability of 
electronic health records, simplifying IR reports is critical to improving 
patient understanding and clinical decision-making. This study provides insights 
into the performance of various LLMs in rewriting IR reports, which can help in 
selecting the most suitable model for clinical patient-centered applications.

Copyright © 2025 The Association of University Radiologists. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.acra.2024.09.041
PMID: 39353826 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


605. J Med Syst. 2025 Mar 13;49(1):36. doi: 10.1007/s10916-025-02167-2.

Using Generative AI to Extract Structured Information from Free Text Pathology 
Reports.

Shahid F(#)(1), Hsu MH(#)(2)(3)(4)(5)(6), Chang YC(7)(8)(9)(10), Jian 
WS(7)(8)(9)(10).

Author information:
(1)Graduate Institute of Data Science, College of Management, Taipei Medical 
University, Taipei, Taiwan. fahad1.shahid@gmail.com.
(2)Graduate Institute of Data Science, College of Management, Taipei Medical 
University, Taipei, Taiwan. 701056@tmu.edu.tw.
(3)Department of Neurosurgery, Shuang-Ho Hospital-Taipei Medical University, 
Taipei, Taiwan. 701056@tmu.edu.tw.
(4)International Ph.D. Program in Biotech and Healthcare Management, Taipei 
Medical University, Taipei, Taiwan. 701056@tmu.edu.tw.
(5)School of Healthcare Administration, Taipei Medical University, Taipei, 
Taiwan. 701056@tmu.edu.tw.
(6)Professional Master Program in Artificial Intelligence in Medicine, Taipei 
Medical University, Taipei, Taiwan. 701056@tmu.edu.tw.
(7)Graduate Institute of Data Science, College of Management, Taipei Medical 
University, Taipei, Taiwan.
(8)International Ph.D. Program in Biotech and Healthcare Management, Taipei 
Medical University, Taipei, Taiwan.
(9)School of Healthcare Administration, Taipei Medical University, Taipei, 
Taiwan.
(10)Professional Master Program in Artificial Intelligence in Medicine, Taipei 
Medical University, Taipei, Taiwan.
(#)Contributed equally

Manually converting unstructured text pathology reports into structured 
pathology reports is very time-consuming and prone to errors. This study 
demonstrates the transformative potential of generative AI in automating the 
analysis of free-text pathology reports. Employing the ChatGPT Large Language 
Model within a Streamlit web application, we automated the extraction and 
structuring of information from 33 unstructured breast cancer pathology reports 
from Taipei Medical University Hospital. Achieving a 99.61% accuracy rate, the 
AI system notably reduced the processing time compared to traditional methods. 
This not only underscores the efficacy of AI in converting unstructured medical 
text into structured data but also highlights its potential to enhance the 
efficiency and reliability of medical text analysis. However, this study is 
limited to breast cancer pathology reports and was conducted using data obtained 
from hospitals associated with a single institution. In the future, we plan to 
expand the scope of this research to include pathology reports for other cancer 
types incrementally and conduct external validation to further substantiate the 
robustness and generalizability of the proposed system. Through this 
technological integration, we aimed to substantiate the capabilities of 
generative AI in improving both the speed and reliability of data processing. 
The outcomes of this study affirm that generative AI can significantly transform 
the handling of pathology reports, promising substantial advancements in 
biomedical research by facilitating the structured analysis of complex medical 
data.

© 2025. The Author(s).

DOI: 10.1007/s10916-025-02167-2
PMCID: PMC11906504
PMID: 40080229 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics Approval: This study was 
approved by the TMU-Joint Institutional Review Board (TMU-JIRB No: N202103047). 
Competing Interests: The authors declare no competing interests.


606. Ophthalmic Plast Reconstr Surg. 2025 Jul-Aug 01;41(4):372-387. doi: 
10.1097/IOP.0000000000002868. Epub 2025 Jul 7.

Artificial Intelligence in Oculoplastics: A Review.

Meer E(1), Kao B(2), Hekmatjah N(2), Lu J(1)(3), Winn B(1)(3), Grob SR(4).

Author information:
(1)Department of Ophthalmology, University of California, San Francisco, San 
Francisco, California.
(2)University of California, San Francisco School of Medicine, San Francisco, 
California.
(3)Division of Oculofacial Plastic and Orbital Surgery, Department of 
Ophthalmology, University of California, San Francisco, San Francisco, 
California.
(4)Division of Oculofacial Plastic and Orbital Surgery, Department of 
Ophthalmology, University of California, Irvine, Irvine, California, U.S.A.

PURPOSE: Innovations in artificial intelligence (AI) have paved the way for 
automated analysis and detection of various anterior and posterior segment 
diseases in ophthalmology. The goal of this study is to review the literature 
surrounding the utilization of AI for oculoplastic diseases or conditions.
METHODS: A literature search was conducted on Pubmed and Embase using a 
combination of terms including AI and oculoplastics, blepharoptosis, eyelid 
ptosis, brow lift, blepharoplasty, eyelid aesthetics, entropion, eyelid 
reconstruction, eyelid malposition, periocular cancer, periorbital melanoma, 
eyelid tumor, eyelid cancer, mohs reconstruction, orbital inflammation, orbital 
decompression, thyroid eye disease, thyroid ophthalmopathy, giant cell 
arteritis, lacrimal gland or lacrimal duct, dacryoadenitis, orbital fracture, 
orbital infections. AI large language models were excluded.
RESULTS: This review included 19 studies on eyelid and brow ptosis, 16 on 
periocular and intraocular cancer, 9 on orbital trauma and tumors, 17 on 
autoimmune, infectious, and inflammatory conditions of the orbit and 
malformations, and 30 on thyroid eye disease, with focus on clinical diagnosis, 
severity, and prediction of therapeutic outcomes.
CONCLUSIONS: Oculoplastics has seen an expansion of potential uses for AI, 
focusing on diagnosis of ptosis and brow ptosis, detection of eyelid cancer from 
pathology slides and clinical photographs, detection and diagnosis of orbital 
tumors and orbital processes, evaluation of radiographic images, and detection 
and monitoring of thyroid eye disease. Across applications, results may be 
promising, however, there are significant challenges such as data privacy, 
ethical considerations, acquisition and analysis of nonstandardized imaging 
data, and the need for continuous validation of AI algorithms in real-world 
clinical settings.

Copyright © 2025 The American Society of Ophthalmic Plastic and Reconstructive 
Surgery, Inc.

DOI: 10.1097/IOP.0000000000002868
PMID: 40622222 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no conflicts of interest to 
disclose.


607. J Med Internet Res. 2025 Apr 25;27:e68427. doi: 10.2196/68427.

Enhancing Bidirectional Encoder Representations From Transformers (BERT) With 
Frame Semantics to Extract Clinically Relevant Information From German 
Mammography Reports: Algorithm Development and Validation.

Reichenpfader D(1)(2), Knupp J(3), von Däniken SU(4), Gaio R(5), Dennstädt F(5), 
Cereghetti GM(4), Sander A(6), Hiltbrunner H(4), Nairz K(4), Denecke K(1).

Author information:
(1)Institute for Patient-Centered Digital Health, School of Engineering and 
Computer Science, Bern University of Applied Sciences, Biel/Bienne, Switzerland.
(2)PhD School of Life Sciences, Faculty of Medicine, University of Geneva, 
Geneva, Switzerland.
(3)ID SUISSE AG, St. Gallen, Switzerland.
(4)Department of Diagnostic, Interventional, and Pediatric Radiology, Bern 
University Hospital, University of Bern, Bern, Switzerland.
(5)Department of Radiation Oncology, Bern University Hospital, University of 
Bern, Bern, Switzerland.
(6)ID BERLIN, Berlin, Germany.

BACKGROUND: Structured reporting is essential for improving the clarity and 
accuracy of radiological information. Despite its benefits, the European Society 
of Radiology notes that it is not widely adopted. For example, while structured 
reporting frameworks such as the Breast Imaging Reporting and Data System 
provide standardized terminology and classification for mammography findings, 
radiology reports still mostly comprise free-text sections. This variability 
complicates the systematic extraction of key clinical data. Moreover, manual 
structuring of reports is time-consuming and prone to inconsistencies. Recent 
advancements in large language models have shown promise for clinical 
information extraction by enabling models to understand contextual nuances in 
medical text. However, challenges such as domain adaptation, privacy concerns, 
and generalizability remain. To address these limitations, frame semantics 
offers an approach to information extraction grounded in computational 
linguistics, allowing a structured representation of clinically relevant 
concepts.
OBJECTIVE: This study explores the combination of Bidirectional Encoder 
Representations from Transformers (BERT) architecture with the linguistic 
concept of frame semantics to extract and normalize information from free-text 
mammography reports.
METHODS: After creating an annotated corpus of 210 German reports for 
fine-tuning, we generate several BERT model variants by applying 3 pretraining 
strategies to hospital data. Afterward, a fact extraction pipeline is built, 
comprising an extractive question-answering model and a sequence labeling model. 
We quantitatively evaluate all model variants using common evaluation metrics 
(model perplexity, Stanford Question Answering Dataset 2.0 [SQuAD_v2], seqeval) 
and perform a qualitative clinician evaluation of the entire pipeline on a 
manually generated synthetic dataset of 21 reports, as well as a comparison with 
a generative approach following best practice prompting techniques using the 
open-source Llama 3.3 model (Meta).
RESULTS: Our system is capable of extracting 14 fact types and 40 entities from 
the clinical findings section of mammography reports. Further pretraining on 
hospital data reduced model perplexity, although it did not significantly impact 
the 2 downstream tasks. We achieved average F1-scores of 90.4% and 81% for 
question answering and sequence labeling, respectively (best pretraining 
strategy). Qualitative evaluation of the pipeline based on synthetic data shows 
an overall precision of 96.1% and 99.6% for facts and entities, respectively. In 
contrast, generative extraction shows an overall precision of 91.2% and 87.3% 
for facts and entities, respectively. Hallucinations and extraction 
inconsistencies were observed.
CONCLUSIONS: This study demonstrates that frame semantics provides a robust and 
interpretable framework for automating structured reporting. By leveraging frame 
semantics, the approach enables customizable information extraction and supports 
generalization to diverse radiological domains and clinical contexts with 
additional annotation efforts. Furthermore, the BERT-based model architecture 
allows for efficient, on-premise deployment, ensuring data privacy. Future 
research should focus on validating the model's generalizability across external 
datasets and different report types to ensure its broader applicability in 
clinical practice.

©Daniel Reichenpfader, Jonas Knupp, Sandro Urs von Däniken, Roberto Gaio, Fabio 
Dennstädt, Grazia Maria Cereghetti, André Sander, Hans Hiltbrunner, Knud Nairz, 
Kerstin Denecke. Originally published in the Journal of Medical Internet 
Research (https://www.jmir.org), 25.04.2025.

DOI: 10.2196/68427
PMCID: PMC12064967
PMID: 40279645 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: At the time this study 
was conducted, JK was employed by ID SUISSE AG. AS is a board member of ID 
BERLIN and an affiliate of ID SUISSE AG.


608. Telemed J E Health. 2025 Jul;31(7):821-828. doi: 10.1089/tmj.2024.0555. Epub 
2025 Mar 24.

Artificial Intelligence and Qualitative Analysis of Emergency Department 
Telemental Health Care Implementation Survey.

Keating C(1), Marcus SC(2)(3), Bowden CF(1), Worsley D(1), Doupnik SK(1)(4).

Author information:
(1)Division of General Pediatrics, Clinical Futures, and PolicyLab, Children's 
Hospital of Philadelphia, Philadelphia, Pennsylvania, USA.
(2)Leonard Davis Institute of Health Economics, University of Pennsylvania, 
Philadelphia, Pennsylvania, USA.
(3)Center for Mental Health, University of Pennsylvania, Philadelphia, 
Pennsylvania, USA.
(4)Department of Pediatrics, Vanderbilt University Medical Center, Nashville, 
Tennessee, USA.

Background: Implementation of telemental health care in emergency departments 
(EDs) in the United States (U.S.) has been increasing. Artificial intelligence 
(AI) can augment traditional qualitative research methods; little is known about 
its efficiency and accuracy. This study sought to understand ED directors' 
qualitative recommendations for improving telemental health care implementation 
and to understand how AI could facilitate analysis of qualitative survey 
responses. Methods: Directors at a nationally representative sample of 279 U.S. 
EDs that used telemental health care completed an open-ended survey question 
about improving telemental health care implementation between June 2022 and 
October 2023. Two groups of researchers completed independent qualitative coding 
of responses: one group used traditional qualitative methods, and one group used 
AI (ChatGPT 4.0) to facilitate analysis. Both groups independently developed a 
codebook, came to consensus on a combined codebook, and each group independently 
used it to code the survey responses. The two groups identified themes in ED 
directors' recommendations and compared codebooks and code application across 
traditional and AI approaches. Results: Themes included (1) recommendations for 
improving telemental health care directly and (2) recommendations for improving 
mental health care systems broadly to make telehealth more effective. ED 
directors' most common recommendation was enabling faster and more streamlined 
access to telemental health care. AI augmented human coding by identifying two 
valid codes not initially identified by human analysts. In codebook application, 
75% of responses were coded consistently across AI and human coders. Conclusions 
and Relevance: For US EDs using telemental health care, there is a need to 
improve timeliness and efficiency of access to telemental health care.

DOI: 10.1089/tmj.2024.0555
PMID: 40129004 [Indexed for MEDLINE]


609. Int J Pediatr Otorhinolaryngol. 2025 Jul;194:112369. doi: 
10.1016/j.ijporl.2025.112369. Epub 2025 May 4.

Artificial intelligence in pediatric otolaryngology: A state-of-the-art review 
of opportunities and pitfalls.

Navarathna N(1), Kanhere A(1), Gomez C(2), Isaiah A(3).

Author information:
(1)Department of Otorhinolaryngology-Head and Neck Surgery, University of 
Maryland School of Medicine, Baltimore, MD, USA; University of Maryland 
Institute for Health Computing, Bethesda, MD, USA.
(2)Department of Otorhinolaryngology-Head and Neck Surgery, University of 
Maryland School of Medicine, Baltimore, MD, USA.
(3)Department of Otorhinolaryngology-Head and Neck Surgery, University of 
Maryland School of Medicine, Baltimore, MD, USA; University of Maryland 
Institute for Health Computing, Bethesda, MD, USA; Department of Diagnostic 
Radiology and Nuclear Medicine, University of Maryland School of Medicine, 
Baltimore, MD, USA; Department of Pediatrics, University of Maryland School of 
Medicine, Baltimore, MD, USA. Electronic address: AIsaiah@som.umaryland.edu.

BACKGROUND: Artificial Intelligence (AI) and machine learning (ML) have 
transformative potential in enhancing diagnostics, treatment planning, and 
patient management. However, their application in pediatric otolaryngology 
remains limited as the unique physiological and developmental characteristics of 
children require tailored AI applications, highlighting a gap in knowledge.
PURPOSE: To provide a narrative review of current literature on the application 
of AI in pediatric otolaryngology, highlighting knowledge gaps, associated 
challenges and future directions.
RESULTS: ML models have demonstrated efficacy in diagnosing conditions such as 
otitis media, adenoid hypertrophy, and pediatric obstructive sleep apnea through 
deep learning-based image analysis and predictive modeling. AI systems also show 
potential in surgical settings such as landmark identification during otologic 
surgery and prediction of middle ear effusion during tympanostomy tube 
placement. Telemedicine solutions and large language models have shown potential 
to improve accessibility to care and patient education. The principal challenges 
include flawed generalization of adult training data and the relative lack of 
pediatric data.
CONCLUSIONS: AI holds significant promise in pediatric otolaryngology. However, 
its widespread clinical integration requires addressing algorithmic bias, 
enhancing model interpretability, and ensuring robust validation across 
pediatric population. Future research should prioritize federated learning, 
developmental trajectory modeling, and psychosocial integration to create 
patient-centered solutions.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijporl.2025.112369
PMCID: PMC12332797
PMID: 40334638 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
have no conflicts of interest to declare.


610. J Neurosurg Sci. 2025 Aug;69(4):362-370. doi: 10.23736/S0390-5616.25.06502-6.

Artificial intelligence applications in the screening and classification of 
glioblastoma.

Brown ED(1), Pelcher I(2), Leon S(2), Karkare AN(2), Barbero JA(2), Ward M(2), 
Schulder M(2).

Author information:
(1)Department of Neurosurgery, Donald and Barbara Zucker, School of Medicine at 
Hofstra/Northwell, Manhasset, NY, USA - ebrown35@northwell.edu.
(2)Department of Neurosurgery, Donald and Barbara Zucker, School of Medicine at 
Hofstra/Northwell, Manhasset, NY, USA.

Glioblastoma is the most aggressive primary brain tumor, with poor prognosis 
following initial identification. Current diagnostic methods, including 
neuroimaging and molecular pathology, face several limitations in tumor 
delineation, differentiation of progression from treatment effects, and 
classification of tumor grade. Artificial intelligence (AI) and machine learning 
(ML) have been increasingly investigated for its potential in addressing such 
challenges. This narrative review examines existing AI applications in 
glioblastoma screening and classification, as well as their associated 
methodological shortcomings. A comprehensive literature search was conducted in 
MEDLINE for studies published in the past five years applying ML methods to 
glioblastoma screening and classification. Studies which were not peer reviewed, 
did not discuss screening or classification, or lacked a clearly defined ML 
methodology were excluded. Study designs, training dataset type, and model 
efficacies were reviewed for narrative evidence synthesis. AI-based omics models 
frequently applied genomic, transcriptomic, methylation status, and Raman 
spectroscopy data to glioblastoma classification. Non-omics AI applications 
frequently involved imaging-based methods, in addition to histopathologic and 
clinical studies. Accuracies exceeding 90% were observed in several studies for 
the identification and classification of glioblastoma. Despite this, challenges 
remain in clinical implementation due to dataset heterogeneity, inconsistent 
model validation, and lack of standardized reporting methodology. While large 
language models are an emerging area of interest, few studies investigated their 
uses in the screening or classification of GBM. AI offers the potential for 
significant advancements in GBM screening and classification, but widespread 
clinical adoption requires improved application of existing reporting 
guidelines. Future research should focus on model interpretability, further 
development of high-quality datasets, and implementation.

DOI: 10.23736/S0390-5616.25.06502-6
PMID: 40662247 [Indexed for MEDLINE]


611. JMIR Form Res. 2025 Apr 4;9:e65726. doi: 10.2196/65726.

Using a Hybrid of AI and Template-Based Method in Automatic Item Generation to 
Create Multiple-Choice Questions in Medical Education: Hybrid AIG.

Kıyak YS(1)(2), Kononowicz AA(2).

Author information:
(1)Department of Medical Education and Informatics, Faculty of Medicine, Gazi 
University, Ankara, Turkey.
(2)Department of Bioinformatics and Telemedicine, Jagiellonian University 
Medical College, Medyczna Str 7, Kraków, 30-688, Poland, 48 12 3476908.

BACKGROUND: Template-based automatic item generation (AIG) is more efficient 
than traditional item writing but it still heavily relies on expert effort in 
model development. While nontemplate-based AIG, leveraging artificial 
intelligence (AI), offers efficiency, it faces accuracy challenges. Medical 
education, a field that relies heavily on both formative and summative 
assessments with multiple choice questions, is in dire need of AI-based support 
for the efficient automatic generation of items.
OBJECTIVE: We aimed to propose a hybrid AIG to demonstrate whether it is 
possible to generate item templates using AI in the field of medical education.
METHODS: This is a mixed-methods methodological study with proof-of-concept 
elements. We propose the hybrid AIG method as a structured series of 
interactions between a human subject matter expert and AI, designed as a 
collaborative authoring effort. The method leverages AI to generate item models 
(templates) and cognitive models to combine the advantages of the two AIG 
approaches. To demonstrate how to create item models using hybrid AIG, we used 2 
medical multiple-choice questions: one on respiratory infections in adults and 
another on acute allergic reactions in the pediatric population.
RESULTS: The hybrid AIG method we propose consists of 7 steps. The first 5 steps 
are performed by an expert in a customized AI environment. These involve 
providing a parent item, identifying elements for manipulation, selecting 
options and assigning values to elements, and generating the cognitive model. 
After a final expert review (Step 6), the content in the template can be used 
for item generation through a traditional (non-AI) software (Step 7). We showed 
that AI is capable of generating item templates for AIG under the control of a 
human expert in only 10 minutes. Leveraging AI in template development made it 
less challenging.
CONCLUSIONS: The hybrid AIG method transcends the traditional template-based 
approach by marrying the "art" that comes from AI as a "black box" with the 
"science" of algorithmic generation under the oversight of expert as a "marriage 
registrar". It does not only capitalize on the strengths of both approaches but 
also mitigates their weaknesses, offering a human-AI collaboration to increase 
efficiency in medical education.

© Yavuz Selim Kıyak, Andrzej A Kononowicz. Originally published in JMIR 
Formative Research (https://formative.jmir.org).

DOI: 10.2196/65726
PMCID: PMC11990652
PMID: 40184548 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


612. Nurse Educ Today. 2025 Apr;147:106570. doi: 10.1016/j.nedt.2025.106570. Epub 
2025 Jan 14.

Exploring artificial intelligence literacy and the use of ChatGPT and copilot in 
instruction on nursing academic report writing.

Tseng LP(1), Huang LP(2), Chen WR(3).

Author information:
(1)Department of Management Center, Sisters of our Lady of China Catholic 
Medical Foundation, St. Martin De Porres Hospital, No. 565, Sec. 2, Daya RD, 
Chiayi City 60069, Taiwan. Electronic address: sharonidnd9@gmail.com.
(2)Department of Nursing, Chung-Jen Junior College of Nursing, Health Sciences 
and Management, No. 1-10 Da-Hu, Hu-Bei Village, Da-Lin Township, Chia-Yi County 
62241, Taiwan. Electronic address: m108@cjc.edu.tw.
(3)Department of Industrial Engineering and Management, Chaoyang University of 
Technology, No. 168, Jifeng E. Rd., Wufeng District, Taichung 413310, Taiwan. 
Electronic address: wrchen@cyut.edu.tw.

BACKGROUND: Nursing education increasingly emphasizes academic writing and 
communication, critical for delivering quality patient care and professional 
advancement. Rapidly emerging artificial intelligence (AI) tools such as ChatGPT 
and Copilot are transforming educational methodologies, and a focus is being 
placed on embedding AI literacy to effectively bridge the gap between 
theoretical knowledge and clinical practice. These technologies have the 
potential to reshape nursing education in a technology-driven health-care 
landscape.
AIM: This study investigated the effectiveness of AI literacy and the 
application of ChatGPT and Copilot in academic nursing report writing. It 
assessed the level of AI literacy of nursing students, examined the integration 
of basic AI concepts into a curriculum, and analyzed the impact of these tools 
compared with traditional teaching methods.
METHODS: The study adopted a sample of 203 senior nursing students from Southern 
Taiwan to compare an AI-enhanced teaching approach using ChatGPT and Copilot 
with conventional methods. The curriculum, centered on the "Writing Case Reports 
and Seminars" course, employed the Analyze, Design, Develop, Implement, Evaluate 
model and incorporated scaffolding techniques to synergistically integrate 
clinical skills with academic learning. AI literacy was measured using the Meta 
AI Literacy Scale (MAILS). Summative assessments, adhering to the Taiwan Nursing 
Association standards, focused on individual and group case report evaluations.
FINDINGS: Following an 18-week AI intervention, the experimental group 
demonstrated significant improvements in all dimensions of the MAILS. A ChatGPT 
usage of 100 % was found, with a notable enhancement discovered in the "Nursing 
Plan" section of case reports. Although the experimental group outperformed the 
control group in overall case report evaluations, the connections between 
identified problems and proposed plans were weaker and nursing interventions 
tended to be less individualized for the experimental group.
CONCLUSIONS: The incorporation of AI tools such as ChatGPT and Copilot into a 
scaffolding teaching framework significantly boosted students' AI literacy and 
performance in summative assessments. Effective AI training for students, 
supervised use of these tools, and continuous professional development for 
educators are paramount to successful implementation. Addressing the current 
limitations of AI has the potential to further improve academic writing, foster 
critical thinking, and ensure responsible application in patient care, 
ultimately leading to higher-quality and more effective nursing education.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.nedt.2025.106570
PMID: 39827788 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare no conflict of interest.


613. Brain Sci. 2025 Sep 29;15(10):1061. doi: 10.3390/brainsci15101061.

Comparative Analysis of AI Models in Predicting Treatment Strategies for 
Unruptured Intracranial Aneurysms.

Overstijns M(1), Nazeeruddin S(1), Scheffler P(1), Roelz R(1), Beck J(1), El 
Rahal A(1)(2).

Author information:
(1)Department of Neurosurgery, Faculty of Medicine, Medical Centre, University 
of Freiburg, Breisacher-Str. 64, 79106 Freiburg, Germany.
(2)Faculty of Medicine of Geneva, University of Geneva, 1201 Geneva, 
Switzerland.

Objectives: The increasing incidence of unruptured intracranial aneurysms (UIAs) 
has led to significant demands on neurovascular boards. Large language models 
(LLMs), such as ChatGPT-4, ChatGPT-3.5, Claude, and Atlas GPT, have emerged as 
tools to support clinical decision-making. This study compares treatment 
recommendations from these AI models with those of an interdisciplinary 
neurovascular board to evaluate their accuracy and alignment. Methods: We 
retrospectively included all 57 patients with UIAs discussed by the 
neurovascular board in 2023. The board's consensus decision served as the 
reference standard. Key clinical and radiographic data, including PHASES, 
ELAPSS, and UIATS scores, were provided to the AI models. Each model was tasked 
with recommending either conservative or operative management and specifying the 
treatment modality (clipping, coiling, flow diverter, or WEB device/flow 
diverter) where appropriate. AI model recommendations were compared with the 
board's decisions for management and the specific treatment modality of the UIA. 
Results: ChatGPT-4 achieved the highest accuracy in correctly predicting 
conservative or operative management (89%) and specific treatment types (73%), 
followed by Atlas GPT (74% accuracy in conservative/operative decisions and 55% 
accuracy in specific treatment types), Claude (70% accuracy in 
conservative/operative decisions and 50% accuracy in specific treatment types), 
and ChatGPT-3.5 (82% accuracy in conservative/operative decisions and 27% 
accuracy in specific treatment types). ChatGPT-3.5 displayed a strong preference 
for clipping (94.3%). ELAPSS scores significantly influenced AI recommendations 
and decision-making, particularly for ChatGPT-4 and ChatGPT-3.5. Follow-up 
recommendations for conservative management were shorter among AI models, with 
Claude suggesting the shortest interval (7.72 months) compared to the 
neurovascular board's 13.36 months. Conclusions: AI models, particularly 
ChatGPT-4, align closely with expert neurovascular board decisions and offer 
promising support for initial clinical decision-making, particularly in 
resource-limited settings. However, interdisciplinary neurovascular boards 
remain unreplaceable for UIA management, and AI should be viewed as a 
complementary tool. The observed improvement from ChatGPT-3.5 to ChatGPT-4 
underscores the rapid evolution of AI technology, and further advancements are 
expected to enhance both performance and accuracy in the future.

DOI: 10.3390/brainsci15101061
PMCID: PMC12563265
PMID: 41154157

Conflict of interest statement: The authors declare no conflicts of interest.


614. Microsurgery. 2025 Dec;45(8):e70138. doi: 10.1002/micr.70138.

MicroRAG: Development of a Novel Artificial Intelligence Retrieval-Augmented 
Generation Model for Microsurgery Clinical Decision Support.

Ozmen BB(1), Singh N(2), Shah K(2), Berber I(3), Singh D(2), Pinsky E(2), Schulz 
SA(1), Bishop SN(1), Bernard S(1), Djohan RS(1), Schwarz GS(1).

Author information:
(1)Department of Plastic Surgery, Cleveland Clinic, Cleveland, Ohio, USA.
(2)Department of Computer Science, Metropolitan College, Boston University, 
Boston, Massachusetts, USA.
(3)Department of Computer Sciences, Case Western Reserve University, Cleveland, 
Ohio, USA.

BACKGROUND: Microsurgical decision-making requires integration of diverse 
patient-specific factors, advanced surgical techniques, and dynamic 
intraoperative insights. While artificial intelligence (AI), large language 
models (LLMs), and retrieval-augmented generation (RAG) models have advanced 
significantly in various fields, no AI-driven clinical decision support systems 
currently exist for microsurgery. We developed MicroRAG, the first AI-powered 
clinical decision support system specifically designed for microsurgery, capable 
of instantly providing evidence-based recommendations by searching and 
synthesizing the entire microsurgical literature.
METHODS: We developed an AI clinical decision support system integrating 4876 
peer-reviewed microsurgical publications (2000-2024) using advanced 
retrieval-augmented generation (RAG) technology. The system processes clinical 
queries through hierarchical document clustering and provides real-time, 
evidence-based recommendations with direct literature citations. We evaluated 
system performance using 10 standardized clinical scenarios covering common 
microsurgical decisions, measuring answer relevancy, faithfulness to source 
literature, and clinical accuracy.
RESULTS: MicroRAG demonstrated exceptional performance with an average answer 
relevancy score of 0.953 (range: 0.857-1.000) and faithfulness score of 0.907 
(range: 0.676-1.000). G-Eval correctness averaged 0.88 with Semantic Evaluation 
Metrics showing an average similarity score of 0.75 and confidence score of 
0.80. The system successfully provided comprehensive, immediately actionable 
guidance for complex scenarios including free flap monitoring protocols, 
vascular complication management, and surgical technique selection. All 
responses were grounded in peer-reviewed literature with direct citations.
CONCLUSION: MicroRAG represents a technological innovation in microsurgical 
practice, providing instant access to evidence-based recommendations that 
typically require hours of literature review. By delivering comprehensive, 
literature-grounded guidance in real-time, this system has the potential to 
standardize best practices, reduce decision-making uncertainty, and ultimately 
improve patient outcomes across all levels of surgical experience.

© 2025 Wiley Periodicals LLC.

DOI: 10.1002/micr.70138
PMID: 41235700 [Indexed for MEDLINE]


615. J Crit Care. 2026 Feb;91:155262. doi: 10.1016/j.jcrc.2025.155262. Epub 2025 Sep 
23.

AI in critical care: A roadmap to the future.

Workum JD(1), Meyfroidt G(2), Bakker J(3), Jung C(4), Tobin JM(5), Gommers D(6), 
Elbers PWG(7), van der Hoeven JG(8), Van Genderen ME(9).

Author information:
(1)Department of Adult Intensive Care, Erasmus University Medical Center, 
Rotterdam, the Netherlands; Erasmus MC Datahub, Erasmus University Medical 
Center, Rotterdam, the Netherlands; Department of Intensive Care, 
Elisabeth-TweeSteden Hospital, Tilburg, the Netherlands.
(2)Department and Laboratory of Intensive Care Medicine, University Hospitals 
Leuven and KU, Leuven, Belgium.
(3)Department of Adult Intensive Care, Erasmus University Medical Center, 
Rotterdam, the Netherlands; Pontificia Universidad Católica de Chile, Department 
of Intensive Care, Santiago, Chile.
(4)Medical Faculty, Department of Cardiology, Pulmonology and Vascular Medicine, 
Heinrich-Heine-University Dusseldorf, Dusseldorf, Germany; Cardiovascular 
Research Institute Düsseldorf (CARID), Dusseldorf, Germany.
(5)Department of Anesthesiology, University of Texas Health at San Antonio, San 
Antonio, USA.
(6)Department of Adult Intensive Care, Erasmus University Medical Center, 
Rotterdam, the Netherlands; Erasmus MC Datahub, Erasmus University Medical 
Center, Rotterdam, the Netherlands.
(7)Department of Intensive Care Medicine, Amsterdam Medical Data Science (AMDS), 
Amsterdam Cardiovascular Science (ACS), Amsterdam Institute for Infection and 
Immunity (AII), Amsterdam Public Health (APH), Amsterdam UMC, University of 
Amsterdam, Vrije Universiteit, Amsterdam, the Netherlands.
(8)Department of Intensive Care Medicine, Radboud University Medical Center, 
Nijmegen, the Netherlands.
(9)Department of Adult Intensive Care, Erasmus University Medical Center, 
Rotterdam, the Netherlands; Erasmus MC Datahub, Erasmus University Medical 
Center, Rotterdam, the Netherlands. Electronic address: 
m.vangenderen@erasmusmc.nl.

Artificial intelligence (AI) has the potential to revolutionize critical care 
medicine by enhancing patient care, improving resource allocation and reducing 
clinician workload. Despite this promise, many AI applications remain confined 
to scientific research rather than being integrated into everyday clinical 
practice. This manuscript aims to help intensivists prepare themselves and their 
intensive care units (ICUs) for AI implementation. It provides a comprehensive 
yet practical roadmap, detailing AI methods, applications, responsible AI 
principles, common roadblocks and implementation strategies. We propose a 
three-tiered risk-based approach to AI implementation, starting with low-risk 
low-complexity administrative AI, progressing to logistical AI, and finally 
integrating medical AI as clinical decision support systems. This ensures a 
gradual build-up of AI skills, technical AI readiness of the ICU, incremental 
value demonstration and alignment with evolving regulatory standards. For each 
AI project, responsible AI principles should be incorporated and adequately 
addressed throughout the entire AI lifecycle, from development to validation to 
implementation and scaling. Common roadblocks for AI implementation including 
technical issues (such as data quality and interoperability issues), 
organizational challenges (such as lack of a clear vision and strategy), and 
clinical concerns (such as limited AI literacy among staff), should be addressed 
proactively. By following this roadmap, ICUs can achieve sustainable AI 
integration, ultimately improving patient outcomes and clinician experience. The 
future of critical care lies in the responsible and strategic adoption of AI, 
with intensivists playing a central role in shaping its implementation.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jcrc.2025.155262
PMID: 40992126 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


616. Int J Surg. 2025 Sep 1;111(9):5970-5979. doi: 10.1097/JS9.0000000000002763. Epub 
2025 Jun 20.

DeepSeek-assisted LI-RADS classification: AI-driven precision in hepatocellular 
carcinoma diagnosis.

Zhang J(1), Liu J(1), Guo M(2), Zhang X(3), Xiao W(1), Chen F(1).

Author information:
(1)Department of Radiology, The First Affiliated Hospital, Zhejiang University 
School of Medicine, Hangzhou, Zhejiang Province, P.R. China.
(2)Department of Radiology, Affiliated Xiaoshan Hospital, Hangzhou Normal 
University, Hangzhou, Zhejiang Province, P.R. China.
(3)GE Healthcare, Shanghai, P.R. China.

BACKGROUND: The clinical utility of the DeepSeek-V3 (DSV3) model in enhancing 
the accuracy of Liver Imaging Reporting and Data System (LI-RADS, LR) 
classification remains underexplored. This study aimed to evaluate the 
diagnostic performance of DSV3 in LR classifications compared to radiologists 
with varying levels of experience and to assess its potential as a 
decision-support tool in clinical practice.
MATERIALS AND METHODS: A dual-phase retrospective-prospective study analyzed 426 
liver lesions (300 retrospective, 126 prospective) in high-risk hepatocellular 
carcinoma (HCC) patients who underwent magnetic resonance imaging or computed 
tomography. Three radiologists (one junior, two seniors) independently 
classified lesions using LR v2018 criteria, while DSV3 analyzed unstructured 
radiology reports to generate corresponding classifications. In the prospective 
cohort, DSV3 processed inputs in both Chinese and English to evaluate language 
impact. Performance was compared using chi-square test or Fisher's exact test, 
with pathology as the gold standard.
RESULTS: In the retrospective cohort, DSV3 significantly outperformed junior 
radiologists in diagnostically challenging categories: LR-3 (17.8% vs. 39.7%, P 
< 0.05), LR-4 (80.4% vs. 46.2%, P < 0.05), and LR-5 (86.2% vs. 66.7%, P < 0.05), 
while showing comparable accuracy in LR-1 (90.8% vs. 88.7%), LR-2 (11.9% vs. 
25.6%), and LR-M (79.5% vs. 62.1%) classifications (all P > 0.05). Prospective 
validation confirmed these findings, with DSV3 demonstrating superior 
performance for LR-3 (13.3% vs. 60.0%), LR-4 (93.3% vs. 66.7%), and LR-5 (93.5% 
vs. 67.7%) compared to junior radiologists (all P < 0.05). Notably, DSV3 
achieved diagnostic parity with senior radiologists across all categories ( P > 
0.05) and maintained consistent performance between Chinese and English inputs.
CONCLUSION: The DSV3 model effectively improves diagnostic accuracy of LR-3 to 
LR-5 classifications among junior radiologists. Its language-independent 
performance and ability to match senior-level expertise suggest strong potential 
for clinical implementation to standardize HCC diagnosis and optimize treatment 
decisions.

Copyright © 2025 The Author(s). Published by Wolters Kluwer Health, Inc.

DOI: 10.1097/JS9.0000000000002763
PMCID: PMC12430892
PMID: 40552875 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no conflicts of interest to 
disclose.


617. Med Educ. 2025 Nov 27. doi: 10.1111/medu.70100. Online ahead of print.

Reconfiguring tutor-peer roles to foster critical thinking in medical PBL 
assisted by tutor-managed chatGPT.

Wang R(1), Liu C(1), Yi F(2).

Author information:
(1)Department of Physiology and Pathophysiology, School of Basic Medical 
Sciences, Shandong University, Jinan, China.
(2)Department of Pharmacology, School of Basic Medical Sciences, Shandong 
University, Jinan, Shandong, China.

BACKGROUND: Critical thinking skills (CTS) are essential in medical education. 
Problem-based learning (PBL) effectively develops CTS, with generative AI (e.g., 
ChatGPT) offering enhancement potential.
OBJECTIVES: This study investigates how a tutor-managed ChatGPT-PBL model: (1) 
reconfigures tutor/peer roles and CTS compared to traditional PBL; (2) 
establishes predictive relationships between tutor/peer roles and CTS and (3) 
confirms optimal operational parameters for tutor-managed ChatGPT to enhance 
CTS.
METHODS: A quasi-experimental study assigned 170 medical students to either a 
ChatGPT-PBL or a traditional PBL group. Data from a 31-item survey assessing 
tutor and peer roles and CTS, along with learning logs and tutor observations 
for 36 participants, recorded AI engagement metrics (e.g., query frequency, 
multi-source validation, comparative questioning). The intervention used a 
pre-defined "3-2-60 Rule" (≥3 queries/session, ≥2 authoritative sources/claim, 
>60% comparative framing), derived from existing literature, to guide ChatGPT 
engagement. Interrelationships were examined.
RESULTS: ChatGPT-PBL improved CTS outcomes, enhancing tutor-guided 
constructive/self-directed/contextual/collaborative learning and peer-driven 
information processing/communication/critical analysis. Regression identified 
collaborative learning and information processing as CTS predictors in both 
approaches, but ChatGPT shifted their influence: collaborative learning's role 
decreased while information processing's increased. High adherence to the 
"3-2-60 Rule" correlated with CTS gains, with high-engagement groups (>3 
queries, ≥2 sources, comparative framing) outperforming peers.
CONCLUSIONS: Tutor-managed ChatGPT reconfigures tutor/peer roles in PBL to 
enhance CTS development. Structured peer-ChatGPT-tutor interactions (following 
the 3-2-60 rule) contributed to enhancing CTS. Given ChatGPT's socioemotional 
limitations, future implementations should embed adversarial ChatGPT tasks and 
ethical transparency protocols.

© 2025 Association for the Study of Medical Education and John Wiley & Sons Ltd.

DOI: 10.1111/medu.70100
PMID: 41308655


618. J Am Med Inform Assoc. 2025 Jul 1;32(7):1120-1129. doi: 10.1093/jamia/ocaf056.

A comparative analysis of privacy-preserving large language models for automated 
echocardiography report analysis.

Mahmoudi E(1)(2), Vahdati S(1), Chao CJ(1)(2), Khosravi B(1), Misra A(1), 
Lopez-Jimenez F(2), Erickson BJ(1).

Author information:
(1)Department of Radiology, Radiology Informatics Lab, Mayo Clinic, Rochester, 
MN 55905, United States.
(2)Department of Cardiovascular Medicine, Mayo Clinic Rochester, Rochester, MN 
55905, United States.

BACKGROUND: Automated data extraction from echocardiography reports could 
facilitate large-scale registry creation and clinical surveillance of valvular 
heart diseases (VHD). We evaluated the performance of open-source large language 
models (LLMs) guided by prompt instructions and chain of thought (CoT) for this 
task.
METHODS: From consecutive transthoracic echocardiographies performed in our 
center, we utilized 200 random reports from 2019 for prompt optimization and 
1000 from 2023 for evaluation. Five instruction-tuned LLMs (Qwen2.0-72B, 
Llama3.0-70B, Mixtral8-46.7B, Llama3.0-8B, and Phi3.0-3.8B) were guided by 
prompt instructions with and without CoT to classify prosthetic valve presence 
and VHD severity. Performance was evaluated using classification metrics against 
expert-labeled ground truth. Mean squared error (MSE) was also calculated for 
predicted severity's deviation from actual severity.
RESULTS: With CoT prompting, Llama3.0-70B and Qwen2.0 achieved the highest 
performance (accuracy: 99.1% and 98.9% for VHD severity; 100% and 99.9% for 
prosthetic valve; MSE: 0.02 and 0.05, respectively). Smaller models showed lower 
accuracy for VHD severity (54.1%-85.9%) but maintained high accuracy for 
prosthetic valve detection (>96%). Chain of thought reasoning yielded higher 
accuracy for larger models while increasing processing time from 2-25 to 67-154 
seconds per report. Based on CoT reasonings, the wrong predictions were mainly 
due to model outputs being influenced by irrelevant information in the text or 
failure to follow the prompt instructions.
CONCLUSIONS: Our study demonstrates the near-perfect performance of open-source 
LLMs for automated echocardiography report interpretation with the purpose of 
registry formation and disease surveillance. While larger models achieved 
exceptional accuracy through prompt optimization, practical implementation 
requires balancing performance with computational efficiency.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf056
PMCID: PMC12257941
PMID: 40334045 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
declare.


619. J Med Syst. 2025 Apr 2;49(1):42. doi: 10.1007/s10916-025-02176-1.

Hyper-DREAM, a Multimodal Digital Transformation Hypertension Management 
Platform Integrating Large Language Model and Digital Phenotyping: Multicenter 
Development and Initial Validation Study.

Wang Y(#)(1)(2), Zhu T(#)(3), Zhou T(#)(1), Wu B(#)(4), Tan W(#)(5), Ma K(3), 
Yao Z(1), Wang J(1), Li S(3), Qin F(6), Xu Y(7), Tan L(8), Liu J(9), Wang J(10).

Author information:
(1)Department of Cardiology, The First Affiliated Hospital of Bengbu Medical 
Universtiy, 287 Changhuai Road, Longzihu District, Bengbu City, Anhui Province, 
430060, P.R. China.
(2)West China Hospital, Sichuan University, Chengdu, 610041, China.
(3)Department of Cardiology, Institute of Cardiovascular Diseases, Xiangyang 
Central Hospital, Affliated Hospital of Hubei University of Arts and Science, 
Xiangyang, Hubei, China.
(4)Institute of Clinical Medicine and Department of Cardiology, Renmin Hospital, 
Hubei University of Medicine, Shiyan, 442000, Hubei, China.
(5)Sun Yat-Sen Memorial Hospital, Sun Yat-Sen University, Guangzhou, China.
(6)Mental Health Center and Psychiatric Laboratory, West China Hospital, Sichuan 
University, Chengdu, 610041, China.
(7)Pulmonary and Critical Care Medicine, The First Affiliated Hospital of Bengbu 
Medical Universtiy, Bengbu, Anhui, China.
(8)Institute of Clinical Medicine and Department of Cardiology, Renmin Hospital, 
Hubei University of Medicine, Shiyan, 442000, Hubei, China. 
tanliguo2001@126.com.
(9)Department of Cardiology, The First Affiliated Hospital of Bengbu Medical 
Universtiy, 287 Changhuai Road, Longzihu District, Bengbu City, Anhui Province, 
430060, P.R. China. byyfyliujinjun@163.com.
(10)Department of Cardiology, The First Affiliated Hospital of Bengbu Medical 
Universtiy, 287 Changhuai Road, Longzihu District, Bengbu City, Anhui Province, 
430060, P.R. China. junwang0607@163.com.
(#)Contributed equally

Within the mHealth framework, systematic research that collects and analyzes 
patient data to establish comprehensive digital health archives for hypertensive 
patients, and leverages large language models (LLMs) to assist clinicians in 
health management and Blood Pressure (BP) control remains limited. In this 
study, our aims to describe the design, development and usability evaluation 
process of a management platform (Hyper-DREAM) for hypertension. Our 
multidisciplinary team employed an iterative design approach over the course of 
a year to develop the Hyper-DREAM platform. This platform's primary 
functionalities encompass multimodal data collection (personal hypertensive 
digital phenotype archive), multimodal interventions (BP measurement, medication 
assistance, behavior modification, and hypertension education) and multimodal 
interactions (clinician-patient engagement and BP Coach component). In August 
2024, the mHealth App Usability Questionnaire (MAUQ) was conducted involving 51 
hypertensive patients recruited from three distinct centers. In parallel, six 
clinicians engaged in management activities and contributed feedback via the 
Doctor's Software Satisfaction Questionnaire (DSSQ). Concurrently, a real-world 
comparative experiment was conducted to evaluate the usability of the BP Coach, 
ChatGPT-4o Mini, ChatGPT-4o and clinicians. The comparative experiment 
demonstrated that the BP Coach achieved significantly higher scores in utility 
(mean scores 4.05, SD 0.87) and completeness (mean scores 4.12, SD 0.78) when 
compared to ChatGPT-4o Mini, ChatGPT-4o, and clinicians. In terms of clarity, 
the BP Coach was slightly lower than clinicians (mean scores 4.03, SD 0.88). In 
addition, the BP Coach exhibited lower performance in conciseness (mean scores 
3.00, SD 0.96). Clinicians reported a marked improvement in work efficiency 
(2.67 vs. 4.17, P < .001) and experienced faster and more effective patient 
interactions (3.0 vs. 4.17, P = .004). Furthermore, the Hyper-DREAM platform 
significantly decreased work intensity (2.5 vs. 3.5, P = .01) and minimized 
disruptions to daily routines (2.33 vs. 3.55, P = .004). The Hyper-DREAM 
platform demonstrated significantly greater overall satisfaction compared to the 
WeChat-based standard management (3.33 vs. 4.17, P = .01). Additionally, 
clinicians exhibited a markedly higher willingness to integrate the Hyper-DREAM 
platform into clinical practice (2.67 vs. 4.17, P < .001). Furthermore, patient 
management time decreased from 11.5 min (SD 1.87) with Wechat-based standard 
management to 7.5 min (SD 1.84, P = .01) with Hyper-DREAM. Hypertensive patients 
reported high satisfaction with the Hyper-DREAM platform, including ease of use 
(mean scores 1.60, SD 0.69), system information arrangement (mean scores 1.69, 
SD 0.71), and usefulness (mean scores 1.57, SD 0.58). In conclusion, our study 
presents Hyper-DREAM, a novel artificial intelligence-driven platform for 
hypertension management, designed to alleviate clinician workload and exhibiting 
significant promise for clinical application. The Hyper-DREAM platform is 
distinguished by its user-friendliness, high satisfaction rates, utility, and 
effective organization of information. Furthermore, the BP Coach component 
underscores the potential of LLMs in advancing mHealth approaches to 
hypertension management.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02176-1
PMID: 40172683 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


620. Med Educ. 2025 Dec;59(12):1280-1289. doi: 10.1111/medu.15746. Epub 2025 Jun 18.

The impact of generative AI on health professional education: A systematic 
review in the context of student learning.

Pham TD(1), Karunaratne N(1), Exintaris B(1), Liu D(2), Lay T(1), Yuriev E(1), 
Lim A(1)(3).

Author information:
(1)Faculty of Pharmacy and Pharmaceutical Sciences, Monash University, 
Parkville, VIC, Australia.
(2)Office of the Deputy Vice-Chancellor (Education), The University of Sydney, 
Camperdown, NSW, Australia.
(3)Murdoch Childrens Research Institute, Royal Children's Hospital, Parkville, 
VIC, Australia.

BACKGROUND: Generative Artificial Intelligence (GenAI) is increasingly 
integrated into health professions education (HPE), offering new opportunities 
for student learning. However, current research lacks a comprehensive 
understanding of how HPE students actually use GenAI in practice. Laurillard's 
Conversational Framework outlines six learning types-acquisition, inquiry, 
practice, production, discussion and collaboration-commonly used to categorise 
learning activities supported by conventional and digital technologies. Gaining 
insight into how GenAI aligns with these six learning types could assist HPE 
academics in integrating it more thoughtfully and effectively into teaching and 
learning.
PURPOSE: This systematic review investigates how HPE students utilise GenAI and 
examines how these uses align with Laurillard's six learning types compared to 
conventional and digital technologies.
MATERIAL AND METHODS: A systematic review searching five major databases-ERIC, 
Education Database, Ovid Medline, Ovid Embase and Scopus including articles on 
HPE students' use of GenAI until 15th September 2024. Studies were included if 
they were conducted within formal HPE training programs in HPE and specifically 
mentioned how students interact with GenAI. Data were mapped to the six learning 
modes of the Laurillard's Framework. Study quality was assessed using the 
Medical Education Research Study Quality Instrument (MERSQI).
RESULTS: Thirty-three studies met inclusion criteria. GenAI supported learning 
most frequently in practice (73%), inquiry (70%), production (67%) and 
acquisition (55%). These studies highlight GenAI's varied educational 
applications, from clarifying complex concepts to simulating clinical scenarios 
and generating practice materials. Discussion and collaboration were less 
represented (12% each), suggesting a shift toward more individualised learning 
with GenAI. The findings highlight benefits such as efficiency and 
accessibility, alongside concerns about critical thinking, academic integrity 
and reduced peer interaction.
CONCLUSION: This review has provided insights into HPE students' learning 
aligned with Laurillard's existing six learning types. Although GenAI supports 
personalised and self-directed learning, its role in collaborative modes is 
under-explored.

© 2025 The Author(s). Medical Education published by Association for the Study 
of Medical Education and John Wiley & Sons Ltd.

DOI: 10.1111/medu.15746
PMCID: PMC12686775
PMID: 40533396 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


621. NPJ Digit Med. 2025 Aug 4;8(1):502. doi: 10.1038/s41746-025-01841-6.

A large language model digital patient system enhances ophthalmology history 
taking skills.

Luo MJ(#)(1), Bi S(#)(1), Pang J(#)(1), Liu L(#)(1), Tsui CK(1), Lai Y(1), Chen 
W(1), Yang Y(1), Xu K(1), Zhao L(1), Jin L(1), Lin D(1), Wu X(1), Chen J(1), 
Chen R(1), Liu Z(1), Zou Y(1), Yang Y(1), Li Y(1), Lin H(2)(3)(4).

Author information:
(1)State Key Laboratory of Ophthalmology, Zhongshan Ophthalmic Center, Sun 
Yat-sen University, Guangdong Provincial Key Laboratory of Ophthalmology and 
Visual Science, Guangdong Provincial Clinical Research Center for Ocular 
Diseases, Guangzhou, China.
(2)State Key Laboratory of Ophthalmology, Zhongshan Ophthalmic Center, Sun 
Yat-sen University, Guangdong Provincial Key Laboratory of Ophthalmology and 
Visual Science, Guangdong Provincial Clinical Research Center for Ocular 
Diseases, Guangzhou, China. linht5@mail.sysu.edu.cn.
(3)Center for Precision Medicine and Department of Genetics and Biomedical 
Informatics, Zhongshan School of Medicine, Sun Yat-sen University, Guangzhou, 
Guangdong, China. linht5@mail.sysu.edu.cn.
(4)Hainan Eye Hospital and Key Laboratory of Ophthalmology, Zhongshan Ophthalmic 
Center, Sun Yat-sen University, Haikou, China. linht5@mail.sysu.edu.cn.
(#)Contributed equally

Clinical trainees face limited opportunities to practice medical history-taking 
skills due to scarce case diversity and access to real patients. To address 
this, we developed a large language model-based digital patient (LLMDP) system 
that transforms de‑identified electronic health records into voice‑enabled 
virtual patients capable of free‑text dialog and adaptive feedback, based on our 
previously established open-source retrieval-augmented framework. In a 
single‑center randomized controlled trial (ClinicalTrials.gov: NCT06229379; 
N = 84), students trained with LLMDP achieved a 10.50-point increase in medical 
history-taking assessment scores (95% CI: 4.66-16.33, p < 0.001) compared to 
those using traditional methods. LLMDP-trained students also demonstrated 
greater empathy. Participants reported high satisfaction with LLMDP, emphasizing 
its role in reducing training costs and boosting confidence for real patient 
interactions. These findings provide evidence that LLM-driven digital patients 
enhance medical history-taking skills and offer a scalable, low-risk pathway for 
integrating generative AI into ophthalmology education.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01841-6
PMCID: PMC12322286
PMID: 40760042

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


622. J Med Internet Res. 2025 Mar 5;27:e64364. doi: 10.2196/64364.

Retrieval Augmented Therapy Suggestion for Molecular Tumor Boards: Algorithmic 
Development and Validation Study.

Berman E(1), Sundberg Malek H(2), Bitzer M(3), Malek N(3), Eickhoff C(1).

Author information:
(1)Center for Digital Health, University Hospital Tuebingen, Tuebingen, Germany.
(2)Center for Personalized Medicine, University Hospital Tuebingen, Tuebingen, 
Germany.
(3)Department of Internal Medicine I, University Hospital Tuebingen, Tuebingen, 
Germany.

BACKGROUND: Molecular tumor boards (MTBs) require intensive manual investigation 
to generate optimal treatment recommendations for patients. Large language 
models (LLMs) can catalyze MTB recommendations, decrease human error, improve 
accessibility to care, and enhance the efficiency of precision oncology.
OBJECTIVE: In this study, we aimed to investigate the efficacy of LLM-generated 
treatments for MTB patients. We specifically investigate the LLMs' ability to 
generate evidence-based treatment recommendations using PubMed references.
METHODS: We built a retrieval augmented generation pipeline using PubMed data. 
We prompted the resulting LLM to generate treatment recommendations with PubMed 
references using a test set of patients from an MTB conference at a large 
comprehensive cancer center at a tertiary care institution. Members of the MTB 
manually assessed the relevancy and correctness of the generated responses.
RESULTS: A total of 75% of the referenced articles were properly cited from 
PubMed, while 17% of the referenced articles were hallucinations, and the 
remaining were not properly cited from PubMed. Clinician-generated LLM queries 
achieved higher accuracy through clinician evaluation than automated queries, 
with clinicians labeling 25% of LLM responses as equal to their recommendations 
and 37.5% as alternative plausible treatments.
CONCLUSIONS: This study demonstrates how retrieval augmented generation-enhanced 
LLMs can be a powerful tool in accelerating MTB conferences, as LLMs are 
sometimes capable of achieving clinician-equal treatment recommendations. 
However, further investigation is required to achieve stable results with zero 
hallucinations. LLMs signify a scalable solution to the time-intensive process 
of MTB investigations. However, LLM performance demonstrates that they must be 
used with heavy clinician supervision, and cannot yet fully automate the MTB 
pipeline.

©Eliza Berman, Holly Sundberg Malek, Michael Bitzer, Nisar Malek, Carsten 
Eickhoff. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 05.03.2025.

DOI: 10.2196/64364
PMCID: PMC11923455
PMID: 40053768 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


623. NPJ Digit Med. 2025 Dec 23. doi: 10.1038/s41746-025-02274-x. Online ahead of 
print.

ARTEMIS: a pilot study comparing AI-based and expert therapeutic decisions in 
simulated clinical cases of neuroendocrine neoplasms.

Lamberti G(1), Panzuto F(2), Massironi S(3), Cives M(4)(5), La Salvia A(6), 
Spada F(7), Faggiano A(8), Pusceddu S(9), Albertelli M(10)(11), Tafuto S(12), 
Andrini E(1), Ricci C(1), Campana D(1).

Author information:
(1)Department of Medical and Surgical Sciences (DIMEC), Alma Mater Studiorum - 
University of Bologna, Bologna, Italy.
(2)Department of Medical-Surgical Sciences and Translational Medicine, Sapienza 
Università di Roma, Digestive Disease Unit, ENETS Center of Excellence, 
Sant'Andrea University Hospital, Rome, Italy. francesco.panzuto@uniroma1.it.
(3)Vita-Salute San Raffaele University, Faculty of Medicine and Surgery, Milan, 
Italy.
(4)Interdisciplinary Department of Medicine, University of Bari "Aldo Moro", 
Bari, Italy.
(5)Division of Medical Oncology, A.O.U. Consorziale Policlinico di Bari, Bari, 
Italy.
(6)National Center for Drug Research and Evaluation, Istituto Superiore di 
Sanità, Rome, Italy.
(7)Divisione di Oncologia Medica Gastrointestinale e Tumori Neuroendocrini, 
Istituto Europeo di Oncologia, IEO, IRCCS, Milano, Italy.
(8)Endocrinology Unit, Department of Clinical and Molecular Medicine, 
Sant'Andrea University Hospital, ENETS Center of Excellence, Sapienza University 
of Rome, Rome, Italy.
(9)Department of Medical Oncology, Fondazione IRCCS Istituto Nazionale dei 
Tumori di Milano, ENETS Center of Excellence, Milan, Italy.
(10)Endocrinology Unit, Department of Internal Medicine and Medical Specialties 
(DiMI), University of Genova, Genova, Italy.
(11)Endocrinology Unit, IRCCS Ospedale Policlinico San Martino, Genova, Italy.
(12)Department of Sarcoma and Rare Tumors, Istituto Nazionale Tumori I.R.C.C.S. 
ENETS Center of Excellence Fondazione "G.Pascale", Naples, Italy.

Neuroendocrine neoplasms (NENs) are rare and heterogeneous malignancies 
requiring multidisciplinary management. Large language models (LLMs) are 
emerging as decision-support tools, but their role in therapeutic 
decision-making is largely unexplored. ARTEMIS was a pilot cross-sectional study 
comparing three configurations-a baseline GPT, a customised GPT with static 
domain knowledge (GPTs), and a retrieval-augmented GPT (RAG)-against a panel of 
nine Italian NEN experts using twenty simulated, non-surgical cases. The primary 
endpoint was non-inferiority for systemic therapy recommendations; secondary 
endpoints included completeness, explicit uncertainty, parsimony of additional 
tests, costs, and variability metrics. RAG and GPTs achieved 70.0% agreement 
versus the expert benchmark (63.8%), meeting the exploratory -10% 
non-inferiority margin but not the stricter -5% threshold. Baseline GPT reached 
60.0% and was not non-inferior. All AI systems consistently produced complete 
recommendations and expressed uncertainty more often than experts; RAG tended to 
propose fewer additional tests and lower associated costs. Experts showed 
greater variability than AI systems, and Ki-67 correlated with disagreement, 
indicating biological aggressiveness as a source of uncertainty. This 
exploratory study suggests that LLMs can approximate expert therapeutic 
reasoning under controlled conditions, but concordance remains limited and 
external validation in real-world settings is needed before clinical use.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-02274-x
PMID: 41436590

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


624. IEEE J Biomed Health Inform. 2025 Sep;29(9):6381-6394. doi: 
10.1109/JBHI.2024.3496495.

Biomedical Information Integration via Adaptive Large Language Model 
Construction.

Xue X, Wu ME, Khan F.

Integrating diverse biomedical knowledge information is essential to enhance the 
accuracy and efficiency of medical diagnoses, facilitate personalized treatment 
plans, and ultimately improve patient outcomes. However, Biomedical Information 
Integration (BII) faces significant challenges due to variations in terminology 
and the complex structure of entity descriptions across different datasets. A 
critical step in BII is biomedical entity alignment, which involves accurately 
identifying and matching equivalent entities across diverse datasets to ensure 
seamless data integration. In recent years, Large Language Model (LLMs), such as 
Bidirectional Encoder Representations from Transformers (BERTs), have emerged as 
valuable tools for discerning heterogeneous biomedical data due to their deep 
contextual embeddings and bidirectionality. However, different LLMs capture 
various nuances and complexity levels within the biomedical data, and none of 
them can ensure their effectiveness in all heterogeneous entity matching tasks. 
To address this issue, we propose a novel Two-Stage LLM construction (TSLLM) 
framework to adaptively select and combine LLMs for Biomedical Information 
Integration (BII). First, a Multi-Objective Genetic Programming (MOGP) algorithm 
is proposed for generating versatile high-level LLMs, and then, a 
Single-Objective Genetic Algorithm (SOGA) employs a confidence-based strategy is 
presented to combine the built LLMs, which can further improve the 
discriminative power of distinguishing heterogeneous entities. The experiment 
utilizes OAEI's entity matching datasets, i.e., Benchmark and Conference, along 
with LargeBio, Disease and Phenotype datasets to test the performance of TSLLM. 
The experimental findings validate the efficiency of TSLLM in adaptively 
differentiating heterogeneous biomedical entities, which significantly 
outperforms the leading entity matching techniques.

DOI: 10.1109/JBHI.2024.3496495
PMID: 39527417 [Indexed for MEDLINE]


625. J Med Internet Res. 2025 Dec 5;27:e80770. doi: 10.2196/80770.

Promoting Responsible DeepSeek Deployment in Health Care: Scoping Review 
Comparing Grey and White Literature.

Jiang W(#)(1), Wang D(#)(1), Zeng Y(2), Huang J(3), Xu C(4), Liu C(3).

Author information:
(1)School of Management, Hubei University of Chinese Medicine, Wuhan, China.
(2)School of Pharmacy, Huazhong University of Science and Technology, Wuhan, 
China.
(3)School of Medicine and Health Management, Huazhong University of Science and 
Technology, Hangkong Road 13, Qiaokou District, Wuhan, Hubei, 430000, China, 86 
15623423595.
(4)Intelligent Hospital Research Academy, Peking University Shenzhen Hospital, 
Shenzhen, China.
(#)Contributed equally

BACKGROUND: DeepSeek is an open-source large language model (LLM), and it has 
greatly accelerated LLM adoption in health care. Its rapid deployment has 
sparked concerns regarding its impact on patient outcomes and safety. However, 
little is known about how DeepSeek is used and regulated in health care.
OBJECTIVE: This study aimed to (1) systematically review the characteristics of 
DeepSeek deployed in the top 100 hospitals in China, and (2) compare the 
performance and risks of DeepSeek between hospital disclosures and research 
evidence.
METHODS: We searched the official websites and WeChat accounts of the top 100 
hospitals in China and the databases of Web of Science and PubMed, using the 
terms "DeepSeek" and "large language models." Searches were limited to records 
after January 15, 2025, when DeepSeek was first released. All searches were 
conducted on May 20, 2025, with an update on June 28, 2025. We extracted the 
basic characteristics of DeepSeek; its aims, evaluation approach, performance, 
and risks; and hospital regulations. A coding framework was developed covering 
the application scenarios, evaluation dimensions, and risk sources of LLMs. The 
risk of bias was assessed using the Joanna Briggs Institute checklist.
RESULTS: We identified a total of 58 DeepSeek models in 48 out of the top 100 
Chinese hospitals and found 27 studies in the literature. The first hospital 
deployment of DeepSeek was recorded on February 10, 2025, and deployment rapidly 
expanded to 37 hospitals within a month. Concurrently, most related research 
studies (20/27, 74%) were published after May 2025. Among deployments and 
studies that reported version information, DeepSeek-reasoner (R1) was the most 
frequently used model, and private deployment was the predominant approach. 
DeepSeek was mainly used to assist in clinical decision-making, including 
patient diagnosis and treatment recommendation. Among hospital disclosures, only 
36% (21/58) clearly indicated a predeployment assessment, 22% (13/58) presented 
assessment results, and 9% (5/58) identified potential risks and 
countermeasures. We found poor transparency in hospital reporting, with none of 
the disclosures presenting evaluation details. Hospitals were more likely to 
report higher performance and fewer risks for DeepSeek.
CONCLUSIONS: This is one of the first scoping reviews to reveal the rapid, 
widespread deployment of DeepSeek in China's leading hospitals, primarily for 
clinical decision support. The deployment of DeepSeek in China's leading 
hospitals poses potential risks to patient outcomes and safety. We highlight the 
urgent need for existing regulations to be expanded to downstream developers and 
users to promote the responsible use of LLMs in health care. Hospitals need to 
use a more rigorous validation process and adopt a more transparent reporting 
policy. The main limitations of this review include the restriction to top-tier 
hospitals and the inherent constraints of gray literature. These factors should 
be considered when interpreting the findings.

© Wang Jiang, Dan Wang, Yihang Zeng, Jiaqi Huang, Chang Xu, Chenxi Liu. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/80770
PMCID: PMC12680131
PMID: 41348941 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


626. Nurse Educ Today. 2025 Dec;155:106872. doi: 10.1016/j.nedt.2025.106872. Epub 
2025 Sep 13.

Artificial intelligence in wound care education: Scoping review.

Encarnação R(1), Alves J(2), Marques A(3), Neves-Amado J(4), Alves P(5).

Author information:
(1)Centre for Interdisciplinary Research in Health, Faculty of Health Sciences 
and Nursing, Universidade Católica Portuguesa, 4169-005, Porto, Portugal; 
Cardiology Intensive Care Unit, São João Local Healthcare Unit, 4200-319, Porto, 
Portugal. Electronic address: rcencarnacao@ucp.pt.
(2)Centre for Interdisciplinary Research in Health, Faculty of Health Sciences 
and Nursing, Universidade Católica Portuguesa, 4169-005, Porto, Portugal; 
Intensive Care Unit, Braga Local Healthcare Unit, 4710-243, Braga, Portugal. 
Electronic address: jcfalves@ucp.pt.
(3)Centre for Interdisciplinary Research in Health, Faculty of Health Sciences 
and Nursing, Universidade Católica Portuguesa, 4169-005, Porto, Portugal; 
Intensive Care Unit, Gaia and Espinho Local Healthcare Unit, 4434-502, Vila Nova 
de Gaia, Portugal. Electronic address: s-anjomarques@ucp.pt.
(4)Centre for Interdisciplinary Research in Health, Faculty of Health Sciences 
and Nursing, Universidade Católica Portuguesa, 4169-005, Porto, Portugal. 
Electronic address: jamado@ucp.pt.
(5)Centre for Interdisciplinary Research in Health, Faculty of Health Sciences 
and Nursing, Universidade Católica Portuguesa, 4169-005, Porto, Portugal. 
Electronic address: pjalves@ucp.pt.

BACKGROUND: Artificial intelligence is transforming healthcare education, 
offering innovative teaching and skill development approaches. However, its 
implementation and effectiveness in wound care education remain unclear.
OBJECTIVE: To map and analyze the available evidence on the potential impact of 
artificial intelligence in wound care education, identify knowledge gaps, and 
provide recommendations for future research.
DESIGN/METHODS: This scoping review followed the Joanna Briggs Institute 
guidelines for scoping reviews and the PRISMA-ScR guidelines. The search was 
first conducted in December 2023 and updated on 30 November 2024 across the 
following databases: CINAHL Ultimate, MEDLINE, Cochrane Library, Academic Search 
Complete, Scientific Electronic Library Online (Scielo), Scopus, and Web of 
Science. Grey literature was accessed through Scientific Open Access Scientific 
Repositories of Portugal (RCAAP), ProQuest Dissertations and Theses, OpenAIRE, 
and Open Dissertations. Additional searches were performed in Google Scholar and 
specific journals, including the International Wound Journal, Skin Research and 
Technology, Journal of Wound Care, and Wound Repair and Regeneration. 
Eligibility criteria encompassed any study design exploring the use of 
artificial intelligence in wound care education, published in English, 
Portuguese, or Spanish, with no restrictions on publication date.
RESULTS: This review revealed diverse artificial intelligence applications in 
wound care education, including adaptive e-learning platforms, virtual and 
augmented reality simulations, generative artificial intelligence for 
educational content, and diagnostic and treatment tools. These technologies 
offer personalized learning experiences, real-time feedback, and interactive 
engagement to enhance clinical skills. Despite their promise, most studies 
lacked empirical validation, highlighting significant gaps in integrating 
artificial intelligence into wound care education.
CONCLUSIONS: This review highlights artificial intelligence's transformative 
potential to revolutionize wound care education by fostering interactive and 
evidence-based learning environments. This work highlights the need for 
collaboration among educators, policymakers, and researchers. Future research is 
needed to ensure effective, ethical, and equitable integration of artificial 
intelligence in wound care education.

Copyright © 2025 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.nedt.2025.106872
PMID: 40961828 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest None.


627. Front Digit Health. 2025 Jan 28;6:1448351. doi: 10.3389/fdgth.2024.1448351. 
eCollection 2024.

Voice EHR: introducing multimodal audio data for health.

Anibal J(1)(2), Huth H(1), Li M(1), Hazen L(1), Daoud V(#)(3), Ebedes D(#)(3), 
Lam YM(4), Nguyen H(4), Hong PV(4), Kleinman M(#)(5), Ost S(#)(5), Jackson 
C(#)(5), Sprabery L(#)(5), Elangovan C(#)(5), Krishnaiah B(#)(5), Akst 
L(#)(6)(7), Lina I(#)(7), Elyazar I(#)(8), Ekawati L(#)(8), Jansen S(#)(9), 
Nduwayezu R(#)(10), Garcia C(1), Plum J(1), Brenner J(1), Song M(1), Ricotta 
E(11)(12), Clifton D(2), Thwaites CL(3), Bensoussan Y(3), Wood B(1).

Author information:
(1)Center for Interventional Oncology, NIH Clinical Center, National Institutes 
of Health, Bethesda, MD, United States.
(2)Computational Health Informatics Lab, Oxford Institute of Biomedical 
Engineering, University of Oxford, Oxford, United Kingdom.
(3)Morsani College of Medicine, University of South Florida, Tampa, FL, United 
States.
(4)Social Science and Implementation Research Team, Oxford University Clinical 
Research Unit, Ho Chi Minh City, Vietnam.
(5)College of Medicine, University of Tennessee Health Sciences Center, Memphis, 
TN, United States.
(6)Johns Hopkins Voice Center, Johns Hopkins University, Baltimore, MD, United 
States.
(7)Department of Otolaryngology-Head and Neck Surgery, Johns Hopkins University 
School of Medicine, Baltimore, MD, United States.
(8)Geospatial Epidemiology Program, Oxford University Clinical Research Unit 
Indonesia, Jakarta, Indonesia.
(9)College of Medicine and Health Sciences, University of Rwanda, Kigali, 
Rwanda.
(10)King Faisal Hospital, Kigali, Rwanda.
(11)Epidemiology and Data Management Unit, National Institute of Allergy and 
Infectious Diseases, Bethesda, MD, United States.
(12)Department of Preventive Medicine and Biostatistics, Uniformed Services 
University, Bethesda, MD, United States.
(#)Contributed equally

INTRODUCTION: Artificial intelligence (AI) models trained on audio data may have 
the potential to rapidly perform clinical tasks, enhancing medical 
decision-making and potentially improving outcomes through early detection. 
Existing technologies depend on limited datasets collected with expensive 
recording equipment in high-income countries, which challenges deployment in 
resource-constrained, high-volume settings where audio data may have a profound 
impact on health equity.
METHODS: This report introduces a novel protocol for audio data collection and a 
corresponding application that captures health information through guided 
questions.
RESULTS: To demonstrate the potential of Voice EHR as a biomarker of health, 
initial experiments on data quality and multiple case studies are presented in 
this report. Large language models (LLMs) were used to compare transcribed Voice 
EHR data with data (from the same patients) collected through conventional 
techniques like multiple choice questions. Information contained in the Voice 
EHR samples was consistently rated as equally or more relevant to a health 
evaluation.
DISCUSSION: The HEAR application facilitates the collection of an audio 
electronic health record ("Voice EHR") that may contain complex biomarkers of 
health from conventional voice/respiratory features, speech patterns, and spoken 
language with semantic meaning and longitudinal context-potentially compensating 
for the typical limitations of unimodal clinical datasets.

© 2025 Anibal, Huth, Li, Hazen, Daoud, Ebedes, Lam, Nguyen, Hong, Kleinman, Ost, 
Jackson, Sprabery, Elangovan, Krishnaiah, Akst, Lina, Elyazar, Ekawati, Jansen, 
Nduwayezu, Garcia, Plum, Brenner, Song, Ricotta, Clifton, Thwaites, Bensoussan 
and Wood.

DOI: 10.3389/fdgth.2024.1448351
PMCID: PMC11812063
PMID: 39936096

Conflict of interest statement: The authors declare no competing non-financial 
interests but the following competing financial interests. NIH may own 
intellectual property in the field. NIH and BJW receive royalties for licensed 
patents from Philips, unrelated to this work. BW is Principal Investigator on 
the following CRADA's=Cooperative Research & Development Agreements, between NIH 
and industry: Philips, Philips Research, Celsion Corp, BTG Biocompatibles/Boston 
Scientific, Siemens, NVIDIA, XACT Robotics. Promaxo (in progress). The following 
industry partners also support research in CIO lab via equipment, personnel, 
devices and/or drugs: 3T Technologies (devices), Exact Imaging (data), 
AngioDynamics (equipment), AstraZeneca (pharmaceuticals, NCI CRADA), ArciTrax 
(devices and equipment), Imactis (Equipment), Johnson & Johnson (equipment), 
Medtronic (equipment), Theromics (Supplies), Profound Medical (equipment and 
supplies), QT Imaging (equipment and supplies). The content of this manuscript 
does not necessarily reﬂect the views, policies, or opinions of the Uniformed 
Services University of the Health Sciences, the National Institutes of Health, 
the US Department of Health and Human Services, the US Department of Defense, 
the U.K. National Health Service, the U.K. National Institute for Health 
Research, the U.K. Department of Health, InnoHK – ITC, or the University of 
Oxford. The mention of commercial products, their source, or their use in 
connection with material reported herein is not to be construed as an actual or 
implied endorsement of such products by the U.S. government. This work was 
prepared by a military or civilian employee of the US Government as part of the 
individual's official duties and therefore is in the public domain and does not 
possess copyright protection (public domain information may be freely 
distributed and copied; however, as a courtesy it is requested that the 
Uniformed Services University and the author be given an appropriate 
acknowledgement). The remaining authors declare that the research was conducted 
in the absence of any commercial or financial relationships that could be 
construed as a potential conflict of interest.


628. PLoS One. 2025 Sep 2;20(9):e0327613. doi: 10.1371/journal.pone.0327613. 
eCollection 2025.

Research of text paraphrase generation based on self-contrastive learning.

Yuan L(1), Yu HP(2), Ren J(1), Sun P(2).

Author information:
(1)School of Computing Science and Technology, Huazhong University of Science 
and Technology, Wuhan, Hubei, People's Republic of China.
(2)Wuhan Vocational College of Software and Engineering (Wuhan Open University), 
Wuhan, Hubei, People's Republic of China.

The goal of this study is to improve the quality and diversity of text 
paraphrase generation, a critical task in Natural Language Generation (NLG) that 
requires producing semantically equivalent sentences with varied structures and 
expressions. Existing approaches often fail to generate paraphrases that are 
both high-quality and diverse, limiting their applicability in tasks such as 
machine translation, dialogue systems, and automated content rewriting. To 
address this gap, we introduce two self-contrastive learning models designed to 
enhance paraphrase generation: the Contrastive Generative Adversarial Network 
(ContraGAN) for supervised learning and the Contrastive Model with Metrics 
(ContraMetrics) for unsupervised learning. ContraGAN leverages a learnable 
discriminator within an adversarial framework to refine the quality of generated 
paraphrases, while ContraMetrics incorporates multi-metric filtering and 
keyword-guided prompts to improve unsupervised generation diversity. Experiments 
on benchmark datasets demonstrate that both models achieve significant 
improvements over state-of-the-art methods. ContraGAN enhances semantic fidelity 
with a 0.46 gain in BERTScore and improves fluency with a 1.57 reduction in 
perplexity. In addition, ContraMetrics achieves gains of 0.37 and 3.34 in iBLEU 
and P-BLEU, respectively, reflecting greater diversity and lexical richness. 
These results validate the effectiveness of our models in addressing key 
challenges in paraphrase generation, offering practical solutions for diverse 
NLG applications.

Copyright: © 2025 Yuan et al. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0327613
PMCID: PMC12404545
PMID: 40892948 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


629. J Med Internet Res. 2025 Apr 28;27:e70703. doi: 10.2196/70703.

AI in Home Care-Evaluation of Large Language Models for Future Training of 
Informal Caregivers: Observational Comparative Case Study.

Pérez-Esteve C(1), Guilabert M(2), Matarredona V(3), Srulovici E(4), Tella S(5), 
Strametz R(6), Mira JJ(1)(2).

Author information:
(1)Fundación para el Fomento de la Investigación Sanitaria y Biomédica de la 
Comunitat Valenciana, Centro de Salud Hospital-Plá, Alicante, Spain.
(2)Health Psychology Department, Miguel Hernandez University, Elche, Spain.
(3)Fundación para el Fomento de la Investigación Sanitaria y Biomédica de la 
Comunitat Valenciana, Alicante, Spain.
(4)Department of Nursing, University of Haifa, Haifa, Israel.
(5)Health and Wellbeing Department, LAB University of Applied Sciences, 
Lappeenranta, Finland.
(6)Wiesbaden Institute for Healthcare Economics and Patient Safety, RheinMain 
University of Applied Sciences, Wiesbaden, Germany.

BACKGROUND: The aging population presents an accomplishment for society but also 
poses significant challenges for governments, health care systems, and 
caregivers. Elevated rates of functional limitations among older adults, 
primarily caused by chronic conditions, necessitate adequate and safe care, 
including in-home settings. Traditionally, informal caregiver training has 
relied on verbal and written instructions. However, the advent of digital 
resources has introduced videos and interactive platforms, offering more 
accessible and effective training. Large language models (LLMs) have emerged as 
potential tools for personalized information delivery. While LLMs exhibit the 
capacity to mimic clinical reasoning and support decision-making, their 
potential to serve as alternatives to evidence-based professional instruction 
remains unexplored.
OBJECTIVE: We aimed to evaluate the appropriateness of home care instructions 
generated by LLMs (including GPTs) in comparison to a professional gold 
standard. Furthermore, it seeks to identify specific domains where LLMs show the 
most promise and where improvements are necessary to optimize their reliability 
for caregiver training.
METHODS: An observational, comparative case study evaluated 3 LLMs-GPT-3.5, 
GPT-4o, and Microsoft Copilot-in 10 home care scenarios. A rubric assessed the 
models against a reference standard (gold standard) created by health care 
professionals. Independent reviewers evaluated variables including specificity, 
clarity, and self-efficacy. In addition to comparing each LLM to the gold 
standard, the models were also compared against each other across all study 
domains to identify relative strengths and weaknesses. Statistical analyses 
compared LLMs performance to the gold standard to ensure consistency and 
validity, as well as to analyze differences between LLMs across all evaluated 
domains.
RESULTS: The study revealed that while no LLM achieved the precision of the 
professional gold standard, GPT-4o outperformed GPT-3.5, and Copilot in 
specificity (4.6 vs 3.7 and 3.6), clarity (4.8 vs 4.1 and 3.9), and 
self-efficacy (4.6 vs 3.8 and 3.4). However, the models exhibited significant 
limitations, with GPT-4o and Copilot omitting relevant details in 60% (6/10) of 
the cases, and GPT-3.5 doing so in 80% (8/10). When compared to the gold 
standard, only 10% (2/20) of GPT-4o responses were rated as equally specific, 
20% (4/20) included comparable practical advice, and just 5% (1/20) provided a 
justification as detailed as professional guidance. Furthermore, error frequency 
did not differ significantly across models (P=.65), though Copilot had the 
highest rate of incorrect information (20%, 2/10 vs 10%, 1/10 for GPT-4o and 0%, 
0/0 for GPT-3.5).
CONCLUSIONS: LLMs, particularly GPT-4o subscription-based, show potential as 
tools for training informal caregivers by providing tailored guidance and 
reducing errors. Although not yet surpassing professional instruction quality, 
these models offer a flexible and accessible alternative that could enhance home 
safety and care quality. Further research is necessary to address limitations 
and optimize their performance. Future implementation of LLMs may alleviate 
health care system burdens by reducing common caregiver errors.

©Clara Pérez-Esteve, Mercedes Guilabert, Valerie Matarredona, Einav Srulovici, 
Susanna Tella, Reinhard Strametz, José Joaquín Mira. Originally published in the 
Journal of Medical Internet Research (https://www.jmir.org), 28.04.2025.

DOI: 10.2196/70703
PMCID: PMC12070015
PMID: 40294407 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


630. Int J Med Inform. 2025 Mar;195:105743. doi: 10.1016/j.ijmedinf.2024.105743. Epub 
2024 Dec 1.

Assessing AI Simplification of Medical Texts: Readability and Content Fidelity.

Picton B(1), Andalib S(2), Spina A(2), Camp B(2), Solomon SS(2), Liang J(2), 
Chen PM(3), Chen JW(4), Hsu FP(4), Oh MY(4).

Author information:
(1)Department of Neurological Surgery, University of California, Irvine, Orange, 
CA, USA. Electronic address: bpicton@hs.uci.edu.
(2)School of Medicine, University of California, Irvine, Orange, CA, USA.
(3)Department of Neurology, University of California, Irvine, Orange, CA, USA.
(4)Department of Neurological Surgery, University of California, Irvine, Orange, 
CA, USA.

INTRODUCTION: The escalating complexity of medical literature necessitates tools 
to enhance readability for patients. This study aimed to evaluate the efficacy 
of ChatGPT-4 in simplifying neurology and neurosurgical abstracts and patient 
education materials (PEMs) while assessing content preservation using Latent 
Semantic Analysis (LSA).
METHODS: A total of 100 abstracts (25 each from Neurosurgery, Journal of 
Neurosurgery, Lancet Neurology, and JAMA Neurology) and 340 PEMs (66 from the 
American Association of Neurological Surgeons, 274 from the American Academy of 
Neurology) were transformed by a GPT-4.0 prompt requesting a 5th grade reading 
level. Flesch-Kincaid Grade Level (FKGL) and Flesch Reading Ease (FKRE) scores 
were used before/after transformation. Content fidelity was validated via LSA 
(ranging 0-1, 1 meaning identical topics) and by expert assessment (0-1) for a 
subset (n = 40). Pearson correlation coefficient compared assessments.
RESULTS: FKGL decreased from 12th to 5th grade for abstracts and 13th to 5th for 
PEMs (p < 0.001). FKRE scores showed similar improvement (p < 0.001). LSA 
confirmed high content similarity for abstracts (mean cosine similarity 0.746) 
and PEMs (mean 0.953). Expert assessment indicated a mean topic similarity of 
0.775 for abstracts and 0.715 for PEMs. The Pearson coefficient between LSA and 
expert assessment of textual similarity was 0.598 for abstracts and -0.167 for 
PEMs. Segmented analysis of similarity correlations revealed a correlation of 
0.48 (p = 0.02) below 450 words and a -0.20 (p = 0.43) correlation above 450 
words.
CONCLUSION: GPT-4.0 markedly improved the readability of medical texts, 
predominantly maintaining content integrity as substantiated by LSA and expert 
evaluations. LSA emerged as a reliable tool for assessing content fidelity 
within moderate-length texts, but its utility diminished for longer documents, 
overestimating similarity. These findings support the potential of AI in 
combating low health literacy, however, the similarity scores indicate expert 
validation is crucial. Future research must strive to improve transformation 
precision and develop validation methodologies.

Copyright © 2024 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2024.105743
PMID: 39667051 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


631. Acad Radiol. 2025 Feb;32(2):624-633. doi: 10.1016/j.acra.2024.08.052. Epub 2024 
Sep 7.

The Potential of Gemini and GPTs for Structured Report Generation based on 
Free-Text (18)F-FDG PET/CT Breast Cancer Reports.

Chen K(1), Xu W(2), Li X(3).

Author information:
(1)Department of Nuclear Medicine, Zhejiang Cancer Hospital, Hangzhou Institute 
of Medicine (HIM), Chinese Academy of Sciences, Hangzhou, Zhejiang 310022, China 
(K.C.).
(2)Department of Molecular Imaging and Nuclear Medicine, Tianjin Medical 
University Cancer Institute and Hospital, National Clinical Research Center for 
Cancer, Huanhuxi Road, Hexi District, Tianjin 300060, China (W.X., X.L.); 
Tianjin's Clinical Research Center for Cancer, Tianjin 300060, China (W.X., 
X.L.).
(3)Department of Molecular Imaging and Nuclear Medicine, Tianjin Medical 
University Cancer Institute and Hospital, National Clinical Research Center for 
Cancer, Huanhuxi Road, Hexi District, Tianjin 300060, China (W.X., X.L.); 
Tianjin's Clinical Research Center for Cancer, Tianjin 300060, China (W.X., 
X.L.). Electronic address: xli03@tmu.edu.cn.

RATIONALE AND OBJECTIVE: To compare the performance of large language model 
(LLM) based Gemini and Generative Pre-trained Transformers (GPTs) in data mining 
and generating structured reports based on free-text PET/CT reports for breast 
cancer after user-defined tasks.
MATERIALS AND METHODS: Breast cancer patients (mean age, 50 years ± 11 [SD]; all 
female) who underwent consecutive 18F-FDG PET/CT for follow-up between July 2005 
and October 2023 were retrospectively included in the study. A total of twenty 
reports from 10 patients were used to train user-defined text prompts for Gemini 
and GPTs, by which structured PET/CT reports were generated. The natural 
language processing (NLP) generated structured reports and the structured 
reports annotated by nuclear medicine physicians were compared in terms of data 
extraction accuracy and capacity of progress decision-making. Statistical 
methods, including chi-square test, McNemar test and paired samples t-test, were 
employed in the study.
RESULTS: The structured PET/CT reports for 131 patients were generated by using 
the two NLP techniques, including Gemini and GPTs. In general, GPTs exhibited 
superiority over Gemini in data mining in terms of primary lesion size (89.6% 
vs. 53.8%, p < 0.001) and metastatic lesions (96.3% vs 89.6%, p < 0.001). 
Moreover, GPTs outperformed Gemini in making decision for progress (p < 0.001) 
and semantic similarity (F1 score 0.930 vs 0.907, p < 0.001) for reports.
CONCLUSION: GPTs outperformed Gemini in generating structured reports based on 
free-text PET/CT reports, which is potentially applied in clinical practice.
DATA AVAILABILITY: The data used and/or analyzed during the current study are 
available from the corresponding author on reasonable request.

Copyright © 2024 The Association of University Radiologists. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.acra.2024.08.052
PMID: 39245597 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest None.


632. IEEE J Biomed Health Inform. 2025 Jun;29(6):3886-3893. doi: 
10.1109/JBHI.2023.3307216.

Data Augmentation for Medical Image Classification Based on Gaussian Laplacian 
Pyramid Blending With a Similarity Measure.

Kumar A, Sharma A, Singh AK, Singh SK, Saxena S.

Breast cancer is a devastating disease that affects women worldwide, and 
computer-aided algorithms have shown potential in automating cancer diagnosis. 
Recently Generative Artificial Intelligence (GenAI) opens new possibilities for 
addressing the challenges of labeled data scarcity and accurate prediction in 
critical applications. However, a lack of diversity, as well as unrealistic and 
unreliable data, have a detrimental impact on performance. Therefore, this study 
proposes an augmentation scheme to address the scarcity of labeled data and data 
imbalance in medical datasets. This approach integrates the concepts of the 
Gaussian-Laplacian pyramid and pyramid blending with similarity measures. In 
order to maintain the structural properties of images and capture 
inter-variability of patient images of the same category similarity-metric-based 
intermixing has been introduced. It helps to maintain the overall quality and 
integrity of the dataset. Subsequently, deep learning approach with significant 
modification, that leverages transfer learning through the usage of concatenated 
pre-trained models is applied to classify breast cancer histopathological 
images. The effectiveness of the proposal, including the impact of data 
augmentation, is demonstrated through a detailed analysis of three different 
medical datasets, showing significant performance improvement over baseline 
models. The proposal has the potential to contribute to the development of more 
accurate and reliable approach for breast cancer diagnosis.

DOI: 10.1109/JBHI.2023.3307216
PMID: 37603476 [Indexed for MEDLINE]


633. PLoS One. 2026 Jan 9;21(1):e0339769. doi: 10.1371/journal.pone.0339769. 
eCollection 2026.

Machine translationese of large language models: Dependency triplets, text 
classification, and SHAP analysis.

Zhang S(1), Zhao C(1).

Author information:
(1)School of Foreign Languages, East China Normal University, Shanghai, China.

This study addresses the challenge of distinguishing human translations from 
those generated by Large Language Models (LLMs) by utilizing dependency triplet 
features and evaluating 16 machine learning classifiers. Using 10-fold 
cross-validation, the SVM model achieves the highest mean F1-score of 93%, while 
all other classifiers consistently differentiate between human and machine 
translations. SHAP analysis helps identify key dependency features that 
distinguish human and machine translations, improving our understanding of how 
LLMs produce translationese. The findings provide practical insights for 
enhancing translation quality assessment and refining translation models across 
various languages and text genres, contributing to the advancement of natural 
language processing techniques. The dataset and implementation code of our study 
are available at: https://github.com/KiemaG5/LLM-translationese.

Copyright: © 2026 Zhang, Zhao. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0339769
PMCID: PMC12788636
PMID: 41511938 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


634. IEEE J Biomed Health Inform. 2025 Apr;29(4):2629-2641. doi: 
10.1109/JBHI.2024.3496996. Epub 2025 Apr 4.

A Hybrid Artificial Intelligence System for Automated EEG Background Analysis 
and Report Generation.

Tung CS, Liang SF, Chang SF, Young CP.

Electroencephalography (EEG) plays a crucial role in the diagnosis of various 
neurological disorders. However, small hospitals and clinics often lack advanced 
EEG signal analysis systems and are prone to misinterpretation in manual EEG 
reading. This study proposes an innovative hybrid artificial intelligence (AI) 
system for automatic interpretation of EEG background activity and report 
generation. The system combines deep learning models for posterior dominant 
rhythm (PDR) prediction, unsupervised artifact removal, and expert-designed 
algorithms for abnormality detection. For PDR prediction, 1530 labeled EEGs were 
used, and the best ensemble model achieved a mean absolute error (MAE) of 0.237, 
a root mean square error (RMSE) of 0.359, an accuracy of 91.8% within a 0.6 Hz 
error, and an accuracy of 99% within a 1.2 Hz error. The AI system significantly 
outperformed neurologists in detecting generalized background slowing (p = 0.02; 
F1: AI 0.93, neurologists 0.82) and demonstrated improved focal abnormality 
detection, although not statistically significant (p = 0.79; F1: AI 0.71, 
neurologists 0.55). Validation on both an internal dataset and the Temple 
University Abnormal EEG Corpus showed consistent performance (F1: 0.884 and 
0.835, respectively; p = 0.66), demonstrating generalizability. The use of large 
language models (LLMs) for report generation demonstrated 100% accuracy, 
verified by three other independent LLMs. This hybrid AI system provides an 
easily scalable and accurate solution for EEG interpretation in resource-limited 
settings, assisting neurologists in improving diagnostic accuracy and reducing 
misdiagnosis rates.

DOI: 10.1109/JBHI.2024.3496996
PMID: 40030193 [Indexed for MEDLINE]


635. J Voice. 2025 Apr 30:S0892-1997(25)00158-4. doi: 10.1016/j.jvoice.2025.04.010. 
Online ahead of print.

Artificial Intelligence in Speech-Language Pathology and Dysphagia: A Review 
From Latin American Perspective and Pilot Test of LLMs for Rehabilitation 
Planning.

Gallano G(1), Giglio A(2), Ferre A(3).

Author information:
(1)Independent Scholar, Santiago, Chile.
(2)Critical Care Department, Clinica Las Condes Hospital, Santiago, Chile; 
Critical Care Department, Finis Terrae University, Santiago, Chile. Electronic 
address: agiglioj@gmail.com.
(3)Critical Care Department, Clinica Las Condes Hospital, Santiago, Chile; 
Critical Care Department, Finis Terrae University, Santiago, Chile.

Artificial Intelligence (AI) is transforming speech-language pathology (SLP) and 
dysphagia management, offering innovative solutions for assessment, diagnosis, 
and rehabilitation. This narrative review examines AI applications in these 
fields from 2014 to 2024, with particular focus on implementation challenges in 
Latin America. We analyze key AI technologies-including deep learning, machine 
learning algorithms, and natural language processing-that have demonstrated high 
accuracy in detecting voice disorders, analyzing swallowing function, and 
supporting personalized rehabilitation. The review identifies three primary 
domains of AI application: diagnostic tools with improved sensitivity for 
speech-language disorders, rehabilitation technologies that enable customized 
therapy, and telehealth platforms that expand access to specialized care in 
underserved regions. However, significant barriers persist, particularly in 
Latin America, where limited infrastructure, insufficient linguistic adaptation, 
and scarce regional datasets hamper widespread implementation. Our pilot study 
evaluating commercially available large language models for rehabilitation 
planning demonstrates their potential utility in generating structured therapy 
activities, especially in resource-constrained settings. While AI shows promise 
in enhancing clinical workflows and expanding service delivery, the evidence 
suggests that current applications remain predominantly focused on diagnosis 
rather than integrated rehabilitation. This review highlights the need for 
culturally and linguistically adapted AI models, expanded regional research 
collaborations, and regulatory frameworks that ensure ethical AI integration 
into SLP and dysphagia care, positioning these technologies as complementary 
tools that enhance rather than replace clinical expertise.

Copyright © 2025 The Voice Foundation. Published by Elsevier Inc. All rights 
reserved.

DOI: 10.1016/j.jvoice.2025.04.010
PMID: 40312192

Conflict of interest statement: Declaration of Competing Interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: Andres Giglio Jiménez: Received 
grants or contracts from Pfizer within the last 36 months about the Infection 
diseases in immunocompromised critical patients course funding, to institution.


636. NPJ Digit Med. 2025 Nov 6;8(1):644. doi: 10.1038/s41746-025-02135-7.

The perils of politeness: how large language models may amplify medical 
misinformation.

Rosen KL(1), Sui M(2)(3), Heydari K(2), Enichen EJ(2), Kvedar JC(2).

Author information:
(1)Harvard Medical School, Boston, MA, USA. kyra_rosen@hms.harvard.edu.
(2)Harvard Medical School, Boston, MA, USA.
(3)Dana-Farber Cancer Institute, Boston, MA, USA.

Chen et al. demonstrate that large language models (LLMs) frequently prioritize 
agreement over accuracy when responding to illogical medical prompts, a behavior 
known as sycophancy. By reinforcing user assumptions, this tendency may amplify 
misinformation and bias in clinical contexts. The authors find that simple 
prompting strategies and LLM fine-tuning can markedly reduce sycophancy without 
impairing performance, highlighting a path toward safer, more trustworthy 
applications of LLMs in medicine.

DOI: 10.1038/s41746-025-02135-7
PMCID: PMC12592531
PMID: 41198821

Conflict of interest statement: Competing interests: Authors K.R., M.S., K.H., 
and E.E. declare no financial or non-financial competing interests. Author J.K. 
serves as Editor-in-Chief of this journal and had no role in the peer-review or 
decision to publish this manuscript. Author J.K. declares no financial competing 
interests.


637. Nurse Educ Pract. 2025 Oct;88:104541. doi: 10.1016/j.nepr.2025.104541. Epub 2025 
Aug 31.

Question-centered prompt engineering for nursing simulation debriefing: Model 
development and validation.

Jang A(1), Oh M(2), Song MO(3).

Author information:
(1)Department of Nursing, Mokpo National University, Muan-gun, Jeollanam-do, 
Republic of Korea. Electronic address: sseillerjjang79@gmail.com.
(2)Department of Nursing, Honam University, Gwangsan-gu, Gwangju, Republic of 
Korea. Electronic address: ohmoonju@honam.ac.kr.
(3)Department of Nursing, Mokpo National University, Muan-gun, Jeollanam-do, 
Republic of Korea. Electronic address: coffeesong79@gmail.com.

AIM: This study aims to develop and validate an instructional debriefing model 
that combines question-centered learning methodology with AI prompt engineering 
techniques for nursing simulations.
BACKGROUND: Integrating artificial intelligence (AI)-based prompt engineering 
into nursing simulation offers structured strategies to enhance clinical 
reasoning. However, current debriefing models insufficiently incorporate AI 
methodologies such as question-centered learning and prompt engineering, 
indicating a lack of theoretical and procedural frameworks METHODS: The model 
was developed using a four-phase approach: (1) literature review, (2) instructor 
interviews, (3) expert validation and (4) external evaluation of effectiveness. 
The study was conducted in nursing simulation learning environments with 
participation from nursing simulation instructors and educational technology 
experts.
RESULTS: The literature review successfully established a framework that aligned 
specific question types with debriefing phases and prompt engineering 
strategies. A structured implementation worksheet was developed based on 
instructor interviews. The model demonstrated strong validity with a Content 
Validity Index of 3.67 and an Inter-Rater Agreement of 1.0. Implementation of 
the model showed statistically significant improvements across multiple domains: 
AI competency, class interest, student knowledge and confidence levels.
CONCLUSIONS: The validated instructional model provides a structured framework 
for integrating AI capabilities into nursing simulation debriefing. The findings 
indicate potential broader applications in developing AI-enhanced clinical 
problem-solving competencies for future healthcare professionals.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.nepr.2025.104541
PMID: 40913900 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


638. Digit Health. 2025 Nov 13;11:20552076251389992. doi: 10.1177/20552076251389992. 
eCollection 2025 Jan-Dec.

Evaluating empathy in GPT-4-generated vs. physician-written emergency department 
discharge letters.

Ben-Haim G(1)(2)(3), Livne A(3)(4), Manor U(3)(5), Hochstein D(3)(5), Saban 
M(3)(6), Blaier O(1), Iram YA(1), Balzam MG(1), Lutenberg A(1), Eyade R(1), 
Qassem R(1), Trabelsi D(3)(7), Dahari Y(3)(7), Eisenmann BZ(1), Shechtman Y(8), 
Nadkarni GN(9)(10), Glicksberg BS(9)(10), Zimlichman E(3)(11), Perry A(12), 
Klang E(9)(10).

Author information:
(1)Department of Emergency Medicine, Sheba Medical Center, Ramat-Gan, Israel.
(2)Emergency Oncology Department, Sheba Medical Center, Ramat-Gan, Israel.
(3)Faculty of Medicine, Tel Aviv University, Tel Aviv, Israel.
(4)Division of Diagnostic Imaging, Sheba Medical Center, Tel Hashomer, Israel.
(5)Internal Medicine "C" Department, Sheba Medical Center, Tel HaShomer, Israel.
(6)The Gertner Institute for Health Policy and Epidemiology, Sheba Medical 
Center, Ramat Gan, Israel.
(7)The Edmond and Lily Safra Children Hospital, Sheba Medical Center, Tel 
Hashomer, Israel.
(8)Bnei-Zion Medical Center, Haifa, Israel.
(9)The Division of Data-Driven and Digital Medicine (D3M), Icahn School of 
Medicine at Mount Sinai, New York, NY, USA.
(10)The Charles Bronfman Institute of Personalized Medicine, Icahn School of 
Medicine at Mount Sinai, New York, NY, USA.
(11)Central Management, Sheba Medical Center, Tel Hashomer, Israel.
(12)Psychology Department, The Hebrew University of Jerusalem, Jerusalem, 
Israel.

BACKGROUND AND AIMS: Empathy improves clinical outcomes, patient satisfaction, 
and adherence to treatment. Few studies have explored the real-world use of 
large language models in conveying empathy. We compared the empathy in emergency 
department (ED) discharge letters written by GPT-4 and physicians.
METHODS: We conducted a retrospective, blinded, comparative study in a tertiary 
ED. All patients discharged for one 8-h shift were included. For each patient, 
we compared the original ED discharge letter to a GPT-4 generated letter. GPT-4 
generated the letters using ED notes, excluding the original discharge letter. 
Seventeen evaluators (seven physicians, five nurses, five patients) compared the 
letters side by side. They were blinded to the source. Evaluators first chose 
between the AI and human letters. Then they rated each letter for empathy, 
overall quality, clarity of summary, and clarity of recommendations using a 
5-point Likert scale.
RESULTS: Evaluators preferred GPT-4 over physician letters in 83.7% of 
comparisons (1009 vs. 197; p < 0.001). GPT-4 letters received higher scores for 
empathy (median 4.0 vs. 3.0; p < 0.001), overall quality, and clarity of summary 
across all evaluator groups. Among patients, no significant difference was found 
in the clarity of recommendations (p = 0.771). Qualitative analysis showed that 
GPT-4's empathetic expressions, though sometimes generic, were perceived as 
effective.
CONCLUSION: GPT-4 shows strong potential in generating empathetic ED discharge 
letters. These letters are preferred by healthcare professionals and patients. 
GPT-4 offers a promising tool to reduce the workload of ED physicians. Further 
research is necessary to explore patient perceptions and best practices for 
integrating AI with physicians in clinical practice.

© The Author(s) 2025.

DOI: 10.1177/20552076251389992
PMCID: PMC12615915
PMID: 41246211

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


639. Front Digit Health. 2025 Nov 14;7:1702082. doi: 10.3389/fdgth.2025.1702082. 
eCollection 2025.

Open LLM-based actionable incidental finding extraction from 
[(18)F]fluorodeoxyglucose PET-CT radiology reports.

Barlow SH(1), Chicklore S(1)(2), He Y(3)(4)(5), Ourselin S(1), Wagner T(6)(7), 
Barnes A(1)(8), Cook GJR(1)(2).

Author information:
(1)School of Biomedical Engineering and Imaging Sciences, King's College London, 
London, United Kingdom.
(2)King's College London and Guy's and St. Thomas' PET Centre, St. Thomas' 
Hospital, London, United Kingdom.
(3)Department of Informatics, King's College London, London, United Kingdom.
(4)Department of Computer Science, University of Warwick, Coventry, United 
Kingdom.
(5)Alan Turing Institute, London, United Kingdom.
(6)Department of Nuclear Medicine, Royal Free Hospital, London, United Kingdom.
(7)Department of Imaging, Centre for Medical Imaging, University College London, 
London, United Kingdom.
(8)King's Technology Evaluation Centre (KiTEC), School of Biomedical Engineering 
& Imaging Science, King's College London, London, United Kingdom.

INTRODUCTION: We developed an open, large language model (LLM)-based pipeline to 
extract actionable incidental findings (AIFs) from [18F]fluorodeoxyglucose 
positron emission tomography-computed tomography ([18F]FDG PET-CT) reports. This 
imaging modality often uncovers AIFs, which can affect a patient's treatment. 
The pipeline classifies reports for the presence of AIFs, extracts the relevant 
sentences, and stores the results in structured JavaScript Object Notation 
format, enabling use in both short- and long-term applications.
METHODS: Training, validation, and test datasets of 1,999, 248, and 250 lung 
cancer [18F]FDG PET-CT reports, respectively, were annotated by a nuclear 
medicine physician. An external test dataset of 460 reports was annotated by two 
nuclear medicine physicians. The training dataset was used to fine-tune an LLM 
using QLoRA and chain-of-thought (CoT) prompting. This was evaluated 
quantitatively and qualitatively on both test datasets.
RESULTS: The pipeline achieved document-level F1 scores of 0.917 ± 0.016 and 
0.79 ± 0.025 on the internal and external test datasets. At the sentence-level, 
F1 scores of 0.754 ± 0.011 and 0.522 ± 0.012 were recorded, and qualitative 
analysis demonstrated even higher practical utility. This qualitative analysis 
revealed how sentence-level performance is better in practice.
DISCUSSION: Llama-3.1-8B Instruct was the base LLM that provided the best 
combination of performance and computational efficiency. The utilisation of CoT 
prompting improved performance further. Radiology reporting characteristics such 
as length and style affect model generalisation.
CONCLUSION: We find that a QLoRA-adapted LLM utilising CoT prompting 
successfully extracts AIF information at both document- and sentence-level from 
both internal and external PET-CT reports. We believe this model can assist with 
short-term clinical challenges like clinical alerts and reminders, and long-term 
tasks like investigating comorbidities.

© 2025 Barlow, Chicklore, He, Ourselin, Wagner, Barnes and Cook.

DOI: 10.3389/fdgth.2025.1702082
PMCID: PMC12660291
PMID: 41324019

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


640. Nurse Educ Pract. 2025 Nov;89:104610. doi: 10.1016/j.nepr.2025.104610. Epub 2025 
Oct 26.

Use of ChatGPT in nursing education: A mixed method research on student 
perceptions and experiential practice recommendations.

Uysal Yalçın S(1), Dikmen Y(2).

Author information:
(1)Department of Nursing, Kocaeli Health and Technology University, Kocaeli, 
Turkey. Electronic address: suna.yalcin@kocaelisaglik.edu.tr.
(2)Department of Nursing, Kocaeli Health and Technology University, Kocaeli, 
Turkey. Electronic address: yurdanur.dikmen@kocaelisaglik.edu.tr.

AIM: This study aims to evaluate the effects of using ChatGPT, an artificial 
intelligence tool, in nursing education on the learning process in a 
multidimensional way BACKGROUND: Integrating artificial intelligence 
technologies into nursing education can transform educational processes while 
helping students develop ethical decision-making and personalized care skills in 
patient care. In this context, the study examining the impact of artificial 
intelligence tools, such as ChatGPT, on the perceptions and experiences of 
nursing students is crucial in understanding the transformation in education and 
providing effective implementation suggestions for the future.
DESIGN: A mixed method METHODS: Focus group interviews were conducted as part of 
the qualitative research. In the quantitative part, an 'academic achievement 
test' was applied. The study was conducted between December 2024 and October 
2025 with third-year nursing students. In analyzing the quantitative data, 
independent samples t-tests were conducted using SPSS software, while NVivo 15 
software was employed for the qualitative data analysis.
RESULT: Our findings revealed three main themes: (1) 'the importance of the 
physical environment in education,' (2) 'limitations of artificial intelligence 
-supported education,' and (3) 'the supporting role of artificial intelligence 
and the hybrid learning model.'
CONCLUSION: The use of artificial intelligence tools, such as ChatGPT, in 
nursing education had an impact.on students' perceptions and experiences of the 
learning process, highlighting the absence of elements such as social 
interaction, motivation and teacher support, which are typically provided 
through face-to-face education.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.nepr.2025.104610
PMID: 41151227 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: Suna Uysal Yalcin reports was 
provided by Kocaeli Health and Technology University. If there are other 
authors, they declare that they have no known competing financial interests or 
personal relationships that could have appeared to influence the work reported 
in this paper.


641. Int J Med Inform. 2026 Mar 1;207:106224. doi: 10.1016/j.ijmedinf.2025.106224. 
Epub 2025 Dec 13.

Generative artificial intelligence as a source of advice on resuscitation and 
first aid for laypeople: A scoping review.

Birkun AA(1).

Author information:
(1)Department of Anaesthesiology, Resuscitation and Emergency Medicine, Medical 
Institute Named After S.I. Georgievsky of V.I. Vernadsky Crimean Federal 
University, Lenin Blvd, 5/7, Simferopol 295051, Russian Federation. Electronic 
address: birkunalexei@gmail.com.

INTRODUCTION: The performance of cutting-edge generative artificial intelligence 
(GenAI) in guiding laypeople on how to give help in health emergencies is 
attracting growing attention. This study aimed to map and summarise original 
research evidence on the quality of GenAI-synthesised advice on resuscitation 
and first aid.
METHODS: The review encompassed journal publications that reported original 
quantitative data on the quality (accuracy, correctness, completeness, 
appropriateness) of GenAI-synthesised advice on how laypeople should perform 
cardiopulmonary resuscitation or provide first aid. Relevant papers were 
identified through PubMed, Scopus, and Google Scholar. Studies were included if 
they were published in English as an article, short report, letter, or note 
during the period 2017-2025. The review was conducted following the 
recommendations of the PRISMA extension for Scoping Reviews.
RESULTS: Among the 19 eligible studies, 17 evaluated the performance of 
text-generating GenAI tools, one tested user-to-GenAI voice interaction and 
another one investigated text-to-video generation capabilities. The studies 
exhibited substantial heterogeneity in research design, methods, and reporting. 
Most of them (89.5 %) presented evidence of flaws in the generation of advice on 
resuscitation or first aid, including a failure to synthesise requested content 
(reported by 15.8 % of the studies), the creation of incomplete instructions 
(57.9 %), inaccurate instructions (57.9 %), or superfluous guidance (36.8 %), 
irrelevant or potentially harmful. The prevalence of misinformation varied from 
study to study, at times encompassing the whole sample of evaluated GenAI 
responses. Some authors did not accentuate the issue of misinformation despite 
the reported data indicating quality defects.
CONCLUSIONS: Current evidence indicates risks associated with the unsupervised 
generation of resuscitation and first aid guidance by publicly available GenAI, 
as the synthesised content often contains misinformation that may mislead users 
and induce harmful actions. There is a growing need for international 
collaboration to develop coordinated strategies to limit GenAI-driven 
misinformation and mitigate potential health risks.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106224
PMID: 41391284 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The author 
declares that the topic under review represents a particular area of research 
interest and that he served as lead investigator in four of the studies included 
in this review.


642. Int Nurs Rev. 2025 Jun;72(2):e13012. doi: 10.1111/inr.13012. Epub 2024 Jul 9.

Attitude and utilization of ChatGPT among registered nurses: A cross-sectional 
study.

Lin HL(1)(2)(3)(4), Liao LL(5)(6), Wang YN(2), Chang LC(1)(2)(3).

Author information:
(1)Department of Nursing, Linkou Branch, Chang Gung Memorial Hospital, Taoyuan, 
Taiwan, ROC.
(2)School of Nursing, College of Medicine, Chang Gung University, Taoyuan, 
Taiwan, ROC.
(3)School of Nursing, Chang Gung University of Science and Technology, Gui-Shan 
Town, Taoyuan, Taiwan, ROC.
(4)Taipei Medical University, Taipei, Taiwan.
(5)Department of Public Health, College of Health Science, Kaohsiung Medical 
University, Kaohsiung City, Taiwan.
(6)Department of Medical Research, Kaohsiung Medical University Hospital, 
Kaohsiung City, Taiwan.

AIM: This study explores the influencing factors of attitudes and behaviors 
toward use of ChatGPT based on the Technology Acceptance Model among registered 
nurses in Taiwan.
BACKGROUND: The complexity of medical services and nursing shortages increases 
workloads. ChatGPT swiftly answers medical questions, provides clinical 
guidelines, and assists with patient information management, thereby improving 
nursing efficiency.
INTRODUCTION: To facilitate the development of effective ChatGPT training 
programs, it is essential to examine registered nurses' attitudes toward and 
utilization of ChatGPT across diverse workplace settings.
METHODS: An anonymous online survey was used to collect data from over 1000 
registered nurses recruited through social media platforms between November 2023 
and January 2024. Descriptive statistics and multiple linear regression analyses 
were conducted for data analysis.
RESULTS: Among respondents, some were unfamiliar with ChatGPT, while others had 
used it before, with higher usage among males, higher-educated individuals, 
experienced nurses, and supervisors. Gender and work settings influenced 
perceived risks, and those familiar with ChatGPT recognized its social impact. 
Perceived risk and usefulness significantly influenced its adoption.
DISCUSSION: Nurse attitudes to ChatGPT vary based on gender, education, 
experience, and role. Positive perceptions emphasize its usefulness, while risk 
concerns affect adoption. The insignificant role of perceived ease of use 
highlights ChatGPT's user-friendly nature.
CONCLUSION: Over half of the surveyed nurses had used or were familiar with 
ChatGPT and showed positive attitudes toward its use. Establishing rigorous 
guidelines to enhance their interaction with ChatGPT is crucial for future 
training.
IMPLICATIONS FOR NURSING AND HEALTH POLICY: Nurse managers should understand 
registered nurses' attitudes toward ChatGPT and integrate it into in-service 
education with tailored support and training, including appropriate prompt 
formulation and advanced decision-making, to prevent misuse.

© 2024 International Council of Nurses.

DOI: 10.1111/inr.13012
PMID: 38979771 [Indexed for MEDLINE]


643. NPJ Digit Med. 2025 Oct 27;8(1):631. doi: 10.1038/s41746-025-01996-2.

Benchmarking large language models for personalized, biomarker-based health 
intervention recommendations.

Jarchow H(1), Bobrowski C(2), Falk S(3), Hermann A(4)(5), Kulaga A(1), Põder 
JC(6), Unfried M(7)(8), Usanov N(9), Zendeh B(10), Kennedy BK(11)(12)(13), 
Lobentanzer S(14)(15), Fuellen G(16)(17).

Author information:
(1)Institute for Biostatistics and Informatics in Medicine and Ageing Research, 
Rostock University Medical Center, Rostock, Germany.
(2)Klinik für Neurologie und Geriatrie, Johanniter-Krankenhaus Stendal, Stendal, 
Germany.
(3)Klinik für Unfall-, Hand- und Wiederherstellungschirurgie, Rostock University 
Medical Center, Rostock, Germany.
(4)Translational Neurodegeneration Section "Albrecht Kossel", and Rostock 
University Medical Center, Rostock, Germany.
(5)German Center for Neurodegenerative Diseases (DZNE), Rostock/Greifswald, 
Rostock, Germany.
(6)Ethics in Theology and Medicine, Faculty of Theology, Rostock University 
Faculty of Theology, Rostock, Germany.
(7)Healthy Longevity Translational Research Program, Yong Loo Lin School of 
Medicine, National University of Singapore, Singapore, Singapore.
(8)Department of Biochemistry, Yong Loo Lin School of Medicine, National 
University of Singapore, Singapore, Singapore.
(9)HEAlthy Life Extension Society (HEALES), Brussels, Belgium.
(10)Dept. of Neurology, Rostock University Medical Center, Rostock, Germany.
(11)Healthy Longevity Translational Research Program, Yong Loo Lin School of 
Medicine, National University of Singapore, Singapore, Singapore. 
bkennedy@nus.edu.sg.
(12)Department of Biochemistry, Yong Loo Lin School of Medicine, National 
University of Singapore, Singapore, Singapore. bkennedy@nus.edu.sg.
(13)Department of Physiology, Yong Loo Lin School of Medicine, National 
University of Singapore, Singapore, Singapore. bkennedy@nus.edu.sg.
(14)Institute of Computational Biology, Computational Health Center, Helmholtz 
Center, Munich, Germany. sebastian.lobentanzer@helmholtz-munich.de.
(15)Open Targets, European Bioinformatics Institute, Hinxton, Cambridge, UK. 
sebastian.lobentanzer@helmholtz-munich.de.
(16)Institute for Biostatistics and Informatics in Medicine and Ageing Research, 
Rostock University Medical Center, Rostock, Germany. fuellen@alum.mit.edu.
(17)UCD Conway Institute of Biomolecular and Biomedical Research, School of 
Medicine, University College Dublin, Dublin, Ireland. fuellen@alum.mit.edu.

The use of large language models (LLMs) in clinical diagnostics and intervention 
planning is expanding, yet their utility for personalized recommendations for 
longevity interventions remains opaque. We extended the BioChatter framework to 
benchmark LLMs' ability to generate personalized longevity intervention 
recommendations based on biomarker profiles while adhering to key medical 
validation requirements. Using 25 individual profiles across three different age 
groups, we generated 1000 diverse test cases covering interventions such as 
caloric restriction, fasting and supplements. Evaluating 56000 model responses 
via an LLM-as-a-Judge system with clinician validated ground truths, we found 
that proprietary models outperformed open-source models especially in 
comprehensiveness. However, even with Retrieval-Augmented Generation (RAG), all 
models exhibited limitations in addressing key medical validation requirements, 
prompt stability, and handling age-related biases. Our findings highlight 
limited suitability of LLMs for unsupervised longevity intervention 
recommendations. Our open-source framework offers a foundation for advancing AI 
benchmarking in various medical contexts.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01996-2
PMCID: PMC12559286
PMID: 41145883

Conflict of interest statement: Competing interests: B.K.K. reports a 
relationship with Ponce de Leon Health that includes: consulting or advisory and 
equity or stocks. C.B. has received lecturing fees from Novartis Deutschland 
GmbH and Bayer Vital GmbH. C.B. serves on the expert board for statutory health 
insurance data of IQTIG, the Institute for Quality and Transparency in German 
Healthcare (Institut für Qualitätssicherung und Transparenz im 
Gesundheitswesen). G.F. is a consultant to BlueZoneTech GmbH, who distribute 
supplements. Statement on the use of AI: The first draft was written by H.J., 
with help from G.F. and S.L.; No writing assistance was employed. While the 
topic of the paper is the use of generative AI/LLMs, no such tools were used to 
generate text or content of the manuscript. GPT4o was used for copy-editing 
(grammar, spelling) assistance and research queries on related work and 
references.


644. BMC Nurs. 2025 Apr 7;24(1):386. doi: 10.1186/s12912-025-03044-6.

Neonatal nurses' experiences with generative AI in clinical decision-making: a 
qualitative exploration in high-risk nicus.

Alruwaili AN(1), Alshammari AM(2), Alhaiti A(3), Elsharkawy NB(2), Ali SI(4), 
Elsayed Ramadan OM(5).

Author information:
(1)College of Nursing, Nursing Administration and Education Department, Jouf 
University, Sakaka, 72388, Saudi Arabia. analrwili@ju.edu.sa.
(2)College of Nursing, Department of Maternity and Pediatric Health Nursing, 
Jouf University, Sakaka, 72388, Saudi Arabia.
(3)Department of Nursing, College of Applied Sciences, Almaarefa University, 
Diriyah, Riyadh, 13713, Saudi Arabia.
(4)College of Medicine, Department of Family and Community Medicine, King Faisal 
University, Alhssa, 31982, Saudi Arabia.
(5)Pediatric Nursing Department, Faculty of Nursing, Cairo University, Cairo, 
11562, Egypt. osama_ramadan_85@cu.edu.eg.

BACKGROUND: Neonatal nurses in high-risk Neonatal Intensive Care Units (NICUs) 
navigate complex, time-sensitive clinical decisions where accuracy and judgment 
are critical. Generative artificial intelligence (AI) has emerged as a 
supportive tool, yet its integration raises concerns about its impact on nurses' 
decision-making, professional autonomy, and organizational workflows.
AIM: This study explored how neonatal nurses experience and integrate generative 
AI in clinical decision-making, examining its influence on nursing practice, 
organizational dynamics, and cultural adaptation in Saudi Arabian NICUs.
METHODS: An interpretive phenomenological approach, guided by Complexity 
Science, Normalization Process Theory, and Tanner's Clinical Judgment Model, was 
employed. A purposive sample of 33 neonatal nurses participated in 
semi-structured interviews and focus groups. Thematic analysis was used to code 
and interpret data, supported by an inter-rater reliability of 0.88. Simple 
frequency counts were included to illustrate the prevalence of themes but were 
not used as quantitative measures. Trustworthiness was ensured through reflexive 
journaling, peer debriefing, and member checking.
RESULTS: Five themes emerged: (1) Clinical Decision-Making, where 93.9% of 
nurses reported that AI-enhanced judgment but required human validation; (2) 
Professional Practice Transformation, with 84.8% noting evolving role boundaries 
and workflow changes; (3) Organizational Factors, as 97.0% emphasized the 
necessity of infrastructure, training, and policy integration; (4) Cultural 
Influences, with 87.9% highlighting AI's alignment with family-centered care; 
and (5) Implementation Challenges, where 90.9% identified technical barriers and 
adaptation strategies.
CONCLUSIONS: Generative AI can support neonatal nurses in clinical 
decision-making, but its effectiveness depends on structured training, reliable 
infrastructure, and culturally sensitive implementation. These findings provide 
evidence-based insights for policymakers and healthcare leaders to ensure AI 
integration enhances nursing expertise while maintaining safe, patient-centered 
care.

© 2025. The Author(s).

DOI: 10.1186/s12912-025-03044-6
PMCID: PMC11977934
PMID: 40197527

Conflict of interest statement: Declarations. Institutional review board 
statement: The present study was conducted in accordance with institutional 
ethical standards and received approval from the Standing Committee of Bioethics 
Research, King Faisal University Ethics Committee (Protocol approval number: 
KFU-2024-ETHICS2926). Informed consent: Informed consent was obtained from all 
participants involved in the study. Consent for publication: Not applicable. 
Clinical trial number: Not applicable. Competing interests: The authors declare 
no competing interests.


645. NPJ Digit Med. 2025 Feb 22;8(1):119. doi: 10.1038/s41746-025-01516-2.

An active inference strategy for prompting reliable responses from large 
language models in medical practice.

Shusterman R(1)(2), Waters AC(3), O'Neill S(3), Bangs M(4)(5), Luu P(4)(5), 
Tucker DM(6)(7).

Author information:
(1)Brain Electrophysiology Laboratory Company, Eugene, OR, USA. 
roma.shusterman@bel.company.
(2)Neurosom, Inc, Eugene, OR, USA. roma.shusterman@bel.company.
(3)Icahn School of Medicine at Mount Sinai, New York, NY, USA.
(4)Brain Electrophysiology Laboratory Company, Eugene, OR, USA.
(5)Neurosom, Inc, Eugene, OR, USA.
(6)Brain Electrophysiology Laboratory Company, Eugene, OR, USA. 
don.tucker@bel.company.
(7)Neurosom, Inc, Eugene, OR, USA. don.tucker@bel.company.

Continuing advances in Large Language Models (LLMs) are transforming medical 
knowledge access across education, training, and treatment. Early literature 
cautions their non-determinism, potential for harmful responses, and lack of 
quality control. To address these issues, we propose a domain-specific, 
validated dataset for LLM training and an actor-critic prompting protocol 
grounded in active inference. A Therapist agent generates initial responses to 
patient queries, while a Supervisor agent refines them. In a blind validation 
study, experienced cognitive behavior therapy for insomnia (CBT-I) therapists 
evaluated 100 patient queries. For each query, they were given either the LLM's 
response or one of two therapist-crafted responses-one appropriate and one 
deliberately inappropriate-and asked to rate the quality and accuracy of each 
reply. The LLM often received higher ratings than the appropriate responses, 
indicating effective alignment with expert standards. This structured approach 
lays the foundation for safely integrating advanced LLM technology into medical 
applications.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01516-2
PMCID: PMC11847020
PMID: 39987335

Conflict of interest statement: Competing interests: R.S., M.B., P.L., and 
D.M.T. are employed by BEL, A.C.W., and S.O. declare no competing interests.


646. Radiology. 2025 Jun;315(3):e240982. doi: 10.1148/radiol.240982.

Navigating the AI Landscape in Medical Imaging: A Critical Analysis of 
Technologies, Implementation, and Implications.

Sosna J(1), Joskowicz L(2), Saban M(3).

Author information:
(1)Department of Radiology, Hadassah Medical Center, Faculty of Medicine, The 
Hebrew University of Jerusalem, Israel 19000 Ein Karem, Jerusalem, Israel 91120.
(2)School of Computer Science and Engineering, The Hebrew University of 
Jerusalem, Jerusalem, Israel.
(3)Department of Nursing, School of Health Professions, Faculty of Medical and 
Health Sciences, Tel Aviv University, Tel Aviv, Israel.

The growing volume and complexity of medical imaging outpaces the available 
radiologist workforce, risking timely diagnosis. Comprehensive artificial 
intelligence (AI) that integrates multimodal imaging data, clinical notes, and 
large language models has the potential to support radiologists. Accordingly, 
the U.S. Food and Drug Administration has cleared more than 770 AI medical 
devices that focus on radiology, primarily based on deep learning. However, 
algorithm development and validation remain challenging. Limitations include 
sparse expert-annotated data and regulatory hurdles. Clinical implementation and 
the adaptation of the radiologic community is also lagging behind. Additionally, 
technical barriers exist regarding data availability, large language model 
explainability, deep learning model generalization, and clinical integration. 
Advances in few-shot learning, self-supervised models, and centralized platforms 
may support consolidated AI ecosystems. Although progress has been made, much 
work is still needed on data infrastructure, responsible clinical translation, 
and workflow integration. Continuous multidisciplinary efforts are required to 
optimize AI safety and truly augment radiologists' work through comprehensive 
solutions. By overcoming the remaining challenges, AI may strengthen health care 
systems through improved diagnosis. This review addresses integration 
challenges, pathways for responsible progress, and the viewpoints of all 
stakeholders.

© RSNA, 2025.

DOI: 10.1148/radiol.240982
PMID: 40552997 [Indexed for MEDLINE]


647. J Hepatol. 2025 Dec;83(6):1410-1426. doi: 10.1016/j.jhep.2025.07.003. Epub 2025 
Jul 18.

The barriers to uptake of artificial intelligence in hepatology and how to 
overcome them.

Clusmann J(1), Balaguer-Montero M(2), Bassegoda O(3), Schneider CV(1), Seraphin 
T(4), Paintsil E(5), Luedde T(4), Lopez RP(2), Calderaro J(6), Gilbert S(7), 
Marjot T(8), Spann A(9), Shawcross DL(5), Lens S(10), Trépo E(11), Kather 
JN(12).

Author information:
(1)Else Kroener Fresenius Center for Digital Health, Technical University 
Dresden, Dresden, Germany; Department of Medicine III, University Hospital RWTH 
Aachen, Aachen, Germany.
(2)Radiomics Group, Vall d'Hebron Institute of Oncology (VHIO), 08035 Barcelona, 
Spain.
(3)Liver Unit, Hospital Clinic, University of Barcelona, IDIBAPS, Barcelona, 
Spain.
(4)Department for Gastroenterology, Hepatology and Infectiology, University 
Hospital Düsseldorf, Germany.
(5)Roger Williams Institute of Liver Studies, Faculty of Life Sciences and 
Medicine, King's College London, London, UK.
(6)Université Paris Est Créteil, INSERM, IMRB, F-94010, Créteil, France; 
Assistance Publique-Hôpitaux de Paris, Henri Mondor-Albert Chenevier University 
Hospital, Department of Pathology, Créteil, France; Inserm, U955, Team 18, 
Créteil, France; European Reference Network (ERN) RARE-LIVER, Créteil, France.
(7)Else Kroener Fresenius Center for Digital Health, Technical University 
Dresden, Dresden, Germany.
(8)Translational Gastroenterology and Liver Unit (TGLU), Nuffield Department of 
Medicine, University of Oxford, UK.
(9)Department of Biomedical Informatics, Vanderbilt University Medical Center, 
Nashville, TN, USA.
(10)Liver Unit, Hospital Clinic, University of Barcelona, IDIBAPS, Barcelona, 
Spain; Centro de Investigación Biomédica En Red de Enfermedades Hepáticas y 
Digestivas (CIBERehd), Instituto de Salud Carlos III, Madrid, Spain.
(11)Department of Gastroenterology, Hepatopancreatology and Digestive Oncology, 
Hôpital Universitaire de Bruxelles, Université Libre de Bruxelles, Brussels, 
Belgium; Laboratory of Experimental Gastroenterology, Université Libre de 
Bruxelles, Brussels, Belgium.
(12)Else Kroener Fresenius Center for Digital Health, Technical University 
Dresden, Dresden, Germany; Department of Medicine I, Faculty of Medicine and 
University Hospital Carl Gustav Carus, TUD Dresden University of Technology, 
01307 Dresden, Germany; Medical Oncology, National Center for Tumor Diseases 
(NCT), University Hospital Heidelberg, Heidelberg, Germany. Electronic address: 
jakob-nikolas.kather@alumni.dkfz.de.

Artificial intelligence (AI) methods in hepatology have proliferated since the 
mid-2010s, with numerous publications and some regulatory approvals. Yet, 
adoption of AI methods in real-world clinical practice and clinical research 
remains limited. Despite clear benefits of using AI to analyse complex data 
types in hepatology, such as histopathology, radiology images, multi-omics and 
more recently, natural language patient data, there are still substantial 
barriers and challenges to its integration into routine clinical workflows. In 
this position paper, we assess limitations and propose a set of clear 
recommendations aimed at both the development of AI systems and the broader 
hepatology environment to facilitate the transition of AI-based diagnostic, 
prognostic, and predictive tools into clinical care. In particular, we argue 
that the use of AI in clinical trials, seamless integration into hospital 
information systems and building AI literacy among clinicians will ultimately 
drive clinical adoption. We validate this perspective through a Delphi consensus 
involving 34 international experts from hepatology, AI, and data science, 
ensuring a comprehensive and consensus-driven evaluation of our recommendations.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.jhep.2025.07.003
PMID: 40920593 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of interest JNK declares consulting 
services for Bioptimus, France; Owkin, France; DoMore Diagnostics, Norway; 
Panakeia, UK; AstraZeneca, UK; Mindpeak, Germany; and MultiplexDx, Slovakia. 
Furthermore, he holds shares in StratifAI GmbH, Germany, Synagen GmbH, Germany, 
and has received a research grant by GSK, and has received honoraria by 
AstraZeneca, Bayer, Daiichi Sankyo, Eisai, Janssen, Merck, MSD, BMS, Roche, 
Pfizer and Fresenius. DS has undertaken consultancy for Enterobiotix, Alfa 
Sigma, Norgine, Satellite Bio, MRM Health, GenFit and Apollo Therapeutics and 
has received an investigator-initiated research grant funding from Norgine. 
Please refer to the accompanying ICMJE disclosure forms for further details.


648. BMC Nurs. 2026 Jan 16. doi: 10.1186/s12912-025-04253-9. Online ahead of print.

Applications, attitudes and ethical considerations of Generative Artificial 
Intelligence (Gen AI) in nursing education: a scoping review.

Hardie P(1), Darley A(2), Derwin R(3), Eustace-Cook J(4), Kearns S(2), Mc Brien 
B(3), Siddiquee A(5), Zheng D(6), Mooney M(3).

Author information:
(1)School of Nursing & Midwifery, Trinity College Dublin, Dublin, Ireland. 
phardie@tcd.ie.
(2)School of Nursing, Midwifery & Health Systems, University College Dublin, 
Dublin, Ireland.
(3)School of Nursing & Midwifery, Trinity College Dublin, Dublin, Ireland.
(4)The Library, Trinity College Dublin, Dublin, Ireland.
(5)Perelman School of Medicine, University of Pennsylvania, Philadelphia, United 
States.
(6)School of Nursing, University of Pennsylvania, Philadelphia, United States.

BACKGROUND: Generative Artificial Intelligence (Gen AI) is a type of artificial 
intelligence that can learn from and mimic large amounts of data to create 
content such as text, images, music, videos, code, and more, based on inputs or 
prompts. Gen AI technologies are being increasingly integrated into healthcare 
education, including the field of nursing, where they are utilised to support a 
range of pedagogical activities.
PURPOSE: This scoping review examined and described the application of Gen AI as 
a teaching, learning and assessment strategy in Nursing education and examined 
the ethical implications of and attitudes towards its implementation.
METHODS: We conducted a scoping review using a combination of methodological 
approaches, including Arksey and O'Malley's 5-step framework, the PRISMA-ScR 
guidelines, and JBI evidence synthesis methods and searched five databases: 
EMBASE (Elsevier), Web of Science Core (Clarivate), CINAHL & Medline (EBSCO), 
Applied Social Science Index and Abstracts, and ERIC (ProQuest). A wide search 
of grey literature was also conducted. Literature published in English between 
January 1st 2014, and July 1st 2025 was included in the review.
RESULTS: Of the 1,251 articles retrieved, we identified 103 articles for 
inclusion in the review. There were 44 discussion/opinion/conference papers and 
59 empirical research papers. Gen AI has predominantly been used for content 
creation simulation, personalised learning, tutoring, skill development and 
assessment. Students and Educators describe mixed attitudes towards the 
implementation of Gen AI, with several ethical concerns regarding the 
application of Gen AI in nursing education evident, including privacy, 
transparency, bias, and accountability issues.
CONCLUSION: While there is growing openness to Gen AI, a body of work remains 
regarding ethical and educational challenges. Recommendations for educational 
practice and curriculum development include a need for clear policies and 
guidelines to ensure the ethical use of Gen AI resources by educators and 
students. Further research is needed to understand long-term effects and promote 
responsible implementation within the context of nursing education.

© 2026. The Author(s).

DOI: 10.1186/s12912-025-04253-9
PMID: 41545825

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


649. Clin Neurol Neurosurg. 2025 Aug;255:108986. doi: 10.1016/j.clineuro.2025.108986. 
Epub 2025 May 28.

Comparison of quality, empathy and readability of physician responses versus 
chatbot responses to common cerebrovascular neurosurgical questions on a social 
media platform.

Roy JM(1), Atallah E(1), Piper K(1), Majmundar S(1), Mouchtouris N(1), Self 
DM(1), Kaul A(1), Sizdahkhani S(1), Musmar B(1), Tjoumakaris SI(1), Gooch MR(1), 
Rosenwasser RH(1), Jabbour PM(2).

Author information:
(1)Department of Neurological Surgery, Thomas Jefferson University Hospital, 
Philadelphia, PA, USA.
(2)Department of Neurological Surgery, Thomas Jefferson University Hospital, 
Philadelphia, PA, USA. Electronic address: pascal.jabbour@jefferson.edu.

BACKGROUND: Social media platforms are utilized by patients prior to scheduling 
formal consultations and also serve as a means of pursuing second opinions. 
Cerebrovascular pathologies require regular surveillance and specialized care. 
In recent years, chatbots have been trained to provide information on 
neurosurgical conditions. However, their ability to answer questions in vascular 
neurosurgery have not been evaluated in comparison to physician responses. Our 
study is a pilot study evaluating the accuracy, completeness, empathy, and 
readability of responses provided by ChatGPT 3.5 (Open AI, San Francisco) to 
standard specialist physician responses on social media.
METHODS: We identified the top 50 cerebrovascular questions and their verified 
physician responses from Reddit. These questions were inputted into ChatGPT. 
Responses were anonymized and ranked on a Likert scale for accuracy regarding 
neurosurgical guidelines, completeness and empathy by four independent 
reviewers. Readability was assessed using standardized indexes (Flesch Reading 
Ease, Flesch Kincaid Grade, Gunning Fox Index, Simple Measure of "Gobbledygook" 
(SMOG) Index, Automated Readability Index and Coleman Liau Index).
RESULTS: Responses provided by ChatGPT had significantly higher ratings of 
completeness (median (IQR) 3 (2-3) vs. 2 (1-3) and empathy 4 (3-5) vs. 2 (1-3) 
compared to physician responses, respectively (p < 0.001). Accuracy of 
healthcare information did not differ significantly (4 (3-4) vs. (4 (3-4), 
p = 0.752). Physician responses had significantly higher ease of readability and 
lower grade-level readability compared to ChatGPT (p < 0.001).
CONCLUSION: Our results suggest higher empathy and completeness of information 
provided by ChatGPT compared to physicians. However, these responses are at 
readability levels higher than the literacy of the average American population. 
Future research could emphasize incorporating chatbot responses while drafting 
physician responses to provide more balanced information to healthcare 
questions.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.clineuro.2025.108986
PMID: 40451125 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest Dr. Jabbour is 
a consultant for Medtronic, MicroVention, Balt and Cerus Endovascular. Dr. 
Tjoumakaris is a consultant for MicroVention. Dr. Gooch is a consultant for 
Stryker. The other authors have no personal, financial, or institutional 
interest in any of the drugs, materials, or devices described in this article.


650. Eur Arch Otorhinolaryngol. 2025 Aug 23. doi: 10.1007/s00405-025-09630-3. Online 
ahead of print.

Does ChatGPT update itself? Accuracy of ChatGPT in tympanostomy tube guidance: A 
comparative analysis with current literature.

Durgut O(1), Dikici O(2).

Author information:
(1)Department of Otorhinolaryngology, Health Science University, Bursa City 
Hospital, T.C. Sağlık Bakanlığı Bursa Şehir Hastanesi Doğanköy Mahallesi, 
Nilüfer /Bursa, 16110, Turkey. durgutosman@yahoo.com.
(2)Department of Otorhinolaryngology, Health Science University, Bursa City 
Hospital, T.C. Sağlık Bakanlığı Bursa Şehir Hastanesi Doğanköy Mahallesi, 
Nilüfer /Bursa, 16110, Turkey.

OBJECTIVE: This study aims to evaluate the accuracy of ChatGPT-4.0 in providing 
information on tympanostomy tube indications in children, comparing its 
responses with established clinical guidelines and examining its ability to 
update itself over time.
METHODS: Sixteen clinical scenarios from the American Academy of 
Otolaryngology-Head and Neck Surgery Foundation (AAO-HNSF) guidelines were 
assessed using 18 specific questions. Responses were evaluated by two 
otolaryngologists and ChatGPT itself. The final validation was conducted by a 
senior otolaryngologist. Cohen's Kappa analysis was performed to assess 
inter-rater reliability.
RESULTS: ChatGPT-4.0 correctly answered 15.5 out of 16 scenarios (96.8%). The 
second-stage question of scenario 7 was evaluated as incorrect. When current 
literature was referenced, all responses reached 100% accuracy. Among the 
correct answers, 4 scenarios were not fully aligned with the guidelines. 
However, when responses were based on current literature, all of these answers 
were found to be fully compliant. The agreement among the three evaluators was 
perfect, as confirmed by Cohen's Kappa analysis. Despite using an updated 
version (ChatGPT-4.0) and over a year having passed, it was observed that 
ChatGPT-3.5 answered a previously incorrect scenario in the same incorrect 
manner. This suggests that the model may have limited capacity for self-updating 
over time. These findings are consistent with previous research, indicating that 
ChatGPT provides highly accurate responses regarding tympanostomy tube placement 
and largely aligns with existing guidelines.
CONCLUSION: ChatGPT-4.0 demonstrates high accuracy in providing guideline-based 
medical information, but its ability to update itself over time appears to be 
limited. However, when prompted to reference current literature, its accuracy 
improves significantly. These findings highlight the importance of structured 
prompting and critical evaluation of AI-generated medical guidance.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00405-025-09630-3
PMID: 40849397

Conflict of interest statement: Declarations. Conflict of interest: Osman 
Durgut, M.D. declares that he has no conflicts of interest. Oğuzhan Dikici, M.D. 
declares that he has no conflicts of interest.


651. Int J Med Inform. 2025 Nov;203:106006. doi: 10.1016/j.ijmedinf.2025.106006. Epub 
2025 Jun 3.

Development and evaluation of a retrieval-augmented large language model 
framework for enhancing endodontic education.

Xu X(1), Liu S(2), Zhu L(2), Long Y(3), Zeng Y(4), Lu X(5), Li J(6), Dong Y(7).

Author information:
(1)College of Biomedical Engineering & Instrument Science, Zhejiang University, 
Hangzhou 310058, China; Institute of Medical Information/Library, Chinese 
Academy of Medical Sciences & Peking Union Medical College, Beijing 100020, 
China.
(2)Department of Cariology and Endodontology, Peking University School and 
Hospital of Stomatology & National Center for Stomatology & National Clinical 
Research Center for Oral Diseases & National Engineering Research Center of Oral 
Biomaterials and Digital Medical Devices & Beijing Key Laboratory of Digital 
Stomatology & NHC Key Laboratory of Digital Stomatology & NMPA Key Laboratory 
for Dental Materials, Beijing 100081, China.
(3)Department of Cariology and Endodontology, Peking University School and 
Hospital of Stomatology & National Center for Stomatology & National Clinical 
Research Center for Oral Diseases & National Engineering Research Center of Oral 
Biomaterials and Digital Medical Devices & Beijing Key Laboratory of Digital 
Stomatology & NHC Key Laboratory of Digital Stomatology & NMPA Key Laboratory 
for Dental Materials, Beijing 100081, China; Department of General Dentistry II, 
Peking University School and Hospital of Stomatology, Beijing 100081, China.
(4)Institute of Medical Information/Library, Chinese Academy of Medical Sciences 
& Peking Union Medical College, Beijing 100020, China.
(5)College of Biomedical Engineering & Instrument Science, Zhejiang University, 
Hangzhou 310058, China.
(6)Institute of Medical Information/Library, Chinese Academy of Medical Sciences 
& Peking Union Medical College, Beijing 100020, China. Electronic address: 
li.jiao@imicams.ac.cn.
(7)Department of Cariology and Endodontology, Peking University School and 
Hospital of Stomatology & National Center for Stomatology & National Clinical 
Research Center for Oral Diseases & National Engineering Research Center of Oral 
Biomaterials and Digital Medical Devices & Beijing Key Laboratory of Digital 
Stomatology & NHC Key Laboratory of Digital Stomatology & NMPA Key Laboratory 
for Dental Materials, Beijing 100081, China. Electronic address: 
kqdongyanmei@bjmu.edu.cn.

BACKGROUND: Integrating domain-specific knowledge into large language models 
(LLMs) remains a critical challenge in medical education. In dental specialties 
such as endodontics, effective learning requires access to both textual clinical 
evidence and visual procedural demonstrations. However, generic LLMs often 
produce content that lacks clinical accuracy, contextual grounding, or 
pedagogical clarity, thereby limiting their applicability in specialized 
training environments.
OBJECTIVE: To develop and evaluate a Retrieval-Augmented Generation 
(RAG)-enhanced LLMs framework that addresses the challenge of integrating 
domain-specific knowledge in AI-driven endodontic education.
METHOD: We present Endodontics-KB, a multimodal knowledge integration platform 
that combines evidence-based dental literature (e.g., textbooks, clinical 
guidelines) with visual instructional materials (e.g., procedural videos) 
through a hierarchical RAG architecture. The system's core component, the EndoQ 
chatbot, utilizes LLMs augmented with multimodal dental datasets to enable 
context-aware clinical reasoning. Benchmarking was conducted against three 
general-purpose LLMs: GPT-4, Qwen2.5, and DeepSeek R1, using a structured 
question bank comprising 11 expert-validated endodontic questions. Two domain 
experts performed a blinded evaluation across five performance dimensions: 
clinical accuracy, contextual relevance, completeness, decision-making 
professionalism, and communication fluency.
RESULTS: The framework integrated 2,200 multimodal knowledge units through 
dynamic semantic indexing. EndoQ demonstrated statistically significant 
improvements across all evaluation metrics compared to general purpose LLMs: 
accuracy (4.45 ± 0.96), clinical relevance (4.59 ± 0.8), completeness 
(4.27 ± 0.83), professionalism judgment (4.45 ± 1.06), and language fluency 
(4.86 ± 0.47), as measured on a 5-point Likert scale.
CONCLUSION: This proposed framework improves educational outcomes through 
precise and context-aware knowledge delivery. Furthermore, it represents a 
scalable and transferable model for AI-enhanced clinical training across medical 
specialties, significantly advancing competency-based pedagogy in dental 
education.

Copyright © 2025 The Author(s). Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106006
PMID: 40479778 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


652. Mod Pathol. 2025 Mar;38(3):100663. doi: 10.1016/j.modpat.2024.100663. Epub 2024 
Nov 22.

Statistics of Generative Artificial Intelligence and Nongenerative Predictive 
Analytics Machine Learning in Medicine.

Rashidi HH(1), Hu B(2), Pantanowitz J(3), Tran N(4), Liu S(5), Chamanzar A(6), 
Gur M(7), Chang CH(8), Wang Y(9), Tafti A(9), Pantanowitz L(10), Hanna MG(11).

Author information:
(1)Department of Pathology, University of Pittsburgh Medical Center, Pittsburgh, 
Pennsylvania; Computational Pathology and AI Center of Excellence, School of 
Medicine, University of Pittsburgh, Pittsburgh, Pennsylvania. Electronic 
address: rashidihh@upmc.edu.
(2)Department of Quantitative Health Sciences, Cleveland Clinic, Cleveland, 
Ohio.
(3)School of Medicine, University of Pittsburgh, Pittsburgh, Pennsylvania.
(4)Department of Pathology, UC Davis School of Medicine, Sacramento, California.
(5)Department of Pathology, University of Pittsburgh Medical Center, Pittsburgh, 
Pennsylvania.
(6)Computational Pathology and AI Center of Excellence, School of Medicine, 
University of Pittsburgh, Pittsburgh, Pennsylvania; Department of Electrical and 
Computer Engineering, Carnegie Mellon University, Pittsburgh, Pennsylvania.
(7)Department of Computational and Systems Biology, School of Medicine, 
University of Pittsburgh, Pittsburgh, Pennsylvania; Department of Mechanical 
Engineering, Istanbul Technical University, Istanbul, Turkey.
(8)Department of Medicine and Biostatistics, School of Medicine, University of 
Pittsburgh, Pittsburgh, Pennsylvania.
(9)Computational Pathology and AI Center of Excellence, School of Medicine, 
University of Pittsburgh, Pittsburgh, Pennsylvania; Department of Health 
Information Management, University of Pittsburgh, Pittsburgh, Pennsylvania.
(10)Department of Pathology, University of Pittsburgh Medical Center, 
Pittsburgh, Pennsylvania; Computational Pathology and AI Center of Excellence, 
School of Medicine, University of Pittsburgh, Pittsburgh, Pennsylvania.
(11)Department of Pathology, University of Pittsburgh Medical Center, 
Pittsburgh, Pennsylvania; Computational Pathology and AI Center of Excellence, 
School of Medicine, University of Pittsburgh, Pittsburgh, Pennsylvania. 
Electronic address: hannamg3@upmc.edu.

The rapidly evolving landscape of artificial intelligence (AI) and machine 
learning (ML) in medicine has prompted medical professionals to increasingly 
familiarize themselves with related topics. This also demands grasping the 
underlying statistical principles that govern their design, validation, and 
reproducibility. Uniquely, the practice of pathology and medicine produces vast 
amount of data that can be exploited by AI/ML. The emergence of generative AI, 
especially in the area of large language models and multimodal frameworks, 
represents approaches that are starting to transform medicine. Fundamentally, 
generative and traditional (eg, nongenerative predictive analytics) ML 
techniques rely on certain common statistical measures to function. However, 
unique to generative AI are metrics such as, but not limited to, perplexity and 
BiLingual Evaluation Understudy score that provide a means to determine the 
quality of generated samples that are typically unfamiliar to most medical 
practitioners. In contrast, nongenerative predictive analytics ML often uses 
more familiar metrics tailored to specific tasks as seen in the typical 
classification (ie, confusion metrics measures, such as accuracy, sensitivity, 
F1 score, and receiver operating characteristic area under the curve) or 
regression studies (ie, root mean square error and R2). To this end, the goal of 
this review article (as part 4 of our AI review series) is to provide an 
overview and a comparative measure of statistical measures and methodologies 
used in both generative AI and traditional (ie, nongenerative predictive 
analytics) ML fields along with their strengths and known limitations. By 
understanding their similarities and differences along with their respective 
applications, we will become better stewards of this transformative space, which 
ultimately enables us to better address our current and future needs and 
challenges in a more responsible and scientifically sound manner.

Copyright © 2024 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.modpat.2024.100663
PMID: 39579984 [Indexed for MEDLINE]


653. PLoS One. 2025 Aug 1;20(8):e0324369. doi: 10.1371/journal.pone.0324369. 
eCollection 2025.

The role of music in ADHD: A multi-dimensional computational and theoretical 
analysis.

Achuthan K(1), Khobragade S(1).

Author information:
(1)Center for Cybersecurity Systems and Networks, Amrita Vishwa Vidyapeetham, 
Amritapuri, Kollam, Kerala, India.

Attention-Deficit Hyperactivity Disorder (ADHD), a neurodevelopmental disorder 
affecting children and adults worldwide, has seen a significant rise in 
diagnoses and medication prescriptions in recent decades. This trend has 
emphasized the need for non-pharmacological interventions such as music to aid 
ADHD management. This study explores the musical experiences of individuals with 
ADHD through a comprehensive analysis of user-generated content from the Reddit 
r/ADHD community between 2014-2024. Advanced computational techniques, including 
large language models such as Gemini 1.5 Pro and LLAMA 3.1 were employed for 
data extraction and categorization. Additionally, APIs from digital streaming 
platforms were utilized to analyze musical characteristics and lyrical content 
of 9,215 tracks across three distinct categories: focus music, stuck songs, and 
general purpose. Insights from selective attention, emotion arousal and mood 
congruence theories were used to interpret the findings. Statistical analysis 
revealed significant variations in musical characteristics, with 
instrumentalness showing the largest effect size across contexts, suggesting 
unique musical preferences among individuals with ADHD. Correlation analyses 
uncovered complex interrelationships between musical attributes, particularly in 
focus music, where energy, speechiness, and instrumental characteristics 
displayed distinctive patterns. The sentiment and popularity analysis of lyrics 
further illuminated the emotional landscape of music in ADHD experiences, 
revealing a strategic approach to musical selection as a potential cognitive and 
emotional self-regulation mechanism.

Copyright: © 2025 Achuthan, Khobragade. This is an open access article 
distributed under the terms of the Creative Commons Attribution License, which 
permits unrestricted use, distribution, and reproduction in any medium, provided 
the original author and source are credited.

DOI: 10.1371/journal.pone.0324369
PMCID: PMC12316199
PMID: 40748879 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that they have no known 
competing financial interests or personal relationships that could have appeared 
to influence the work reported in this article.


654. Eur J Radiol. 2026 Jan;194:112519. doi: 10.1016/j.ejrad.2025.112519. Epub 2025 
Nov 15.

Evaluating methodological quality in radiomics research using large language 
models: Added value of METRICS-E3 framework.

Mese I(1), Kocak B(2).

Author information:
(1)Department of Radiology, Uskudar State Hospital, Istanbul, Turkey.
(2)Department of Radiology, Basaksehir Cam and Sakura City Hospital, Istanbul, 
Turkey. Electronic address: drburakkocak@gmail.com.

OBJECTIVES: To determine whether providing the METRICS-E3 (Explanation and 
Elaboration with Examples) resource improves the ability of a large language 
model (LLM) to appraise methodological quality in radiomics research using the 
METhodological RadiomICs Score (METRICS).
METHODS: In this meta-research study, 48 open-access radiomics articles from 
previous research were assessed with METRICS under two GPT-5 pipelines: (i) a 
baseline single-prompt evaluation using only the original METRICS tool (GPT-5) 
and (ii) a two-step evaluation with the baseline METRICS prompt supplemented by 
METRICS-E3 (GPT-5 E3). Each article was run three times per pipeline with 
majority voting. For comparison, we sourced previously reported GPT-4o results 
and the reference human consensus from the published cohort.
RESULTS: Median METRICS scores (IQR) were 79.50 % (13.25 %) for GPT-4o, 68.95 % 
(22.58 %) for human consensus, 66.20 % (17.85 %) for GPT-5, and 64.55 % 
(16.80 %) for GPT-5 E3. Pairwise differences were significant for most 
comparisons (e.g., human vs GPT-4o and human vs GPT-5 E3, both p < 0.001) but 
not for human vs GPT-5 (p = 1.000). Concordance with human ratings improved with 
METRICS-E3: Kendall's τ increased from 0.474 (GPT-5) to 0.626 (GPT-5 E3). 
Agreement measured by intra-class correlation coefficient similarly increased 
from 0.539 to 0.793. Item-wise, GPT-5 E3 improved agreement on 25/30 (83.3 %) 
items and 4/5 (80 %) conditions.
CONCLUSION: Supplying METRICS-E3 to GPT-5 increased alignment with human 
consensus and improved reliability in the task of assessing methodological 
quality of radiomics studies using the METRICS framework. LLM-assisted, METRICS 
and METRICS-E3-guided appraisal may serve as a scalable pre-screening or 
auditing aid under expert supervision. Further validation across datasets, 
readers, and models is warranted.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ejrad.2025.112519
PMID: 41273921 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest Burak Kocak is 
on the editorial board of the European Journal of Radiology. He has taken no 
part in this article’s peer review or selection. He took part in the development 
of METRICS and METRICS-E3.


655. JMIR Form Res. 2025 Nov 25;9:e76896. doi: 10.2196/76896.

Evaluating Locally Run Large Language Models (Gemma 2, Mistral Nemo, and Llama 
3) for Outpatient Otorhinolaryngology Care: Retrospective Study.

Buhr CR(1)(2), Seifen C(1), Bahr-Hamm K(1), Huppertz T(1), Pordzik J(1), Smith 
H(3), Kelsey T(3), Blaikie A(2), Matthias C(1), Kuhn S(4), Eckrich J(1).

Author information:
(1)Department of Otorhinolaryngology, University Medical Center of the Johannes 
Gutenberg-University Mainz, Langenbeckstraße 1, Mainz, 55131, Germany, +49 6131 
17 7362.
(2)School of Medicine, University of St Andrews, St Andrews, United Kingdom.
(3)School of Computer Science, University of St Andrews, St Andrews, United 
Kingdom.
(4)Institute for Digital Medicine, Philipps University Marburg, University 
Hospital Giessen and Marburg, Marburg, Germany.

BACKGROUND: Large language models (LLMs) have great potential to improve and 
make the work of clinicians more efficient. Previous studies have mainly focused 
on web-based services, such as ChatGPT, often with simulated cases. For the 
processing of personalized patient data, web-based services have major data 
protection concerns. Ensuring compliance with data protection and medical device 
regulations therefore remains a critical challenge for adopting LLMs in clinical 
settings.
OBJECTIVE: This retrospective single-center study aimed to evaluate locally run 
LLMs (Gemma 2, Mistral Nemo, and Llama 3) in providing diagnosis and treatment 
recommendation for real-world outpatient cases in otorhinolaryngology (ORL).
METHODS: Outpatient cases (n=30) from regular consultation hours and the 
emergency service at a university hospital ORL outpatient department were 
randomly selected. Documentation by ORL doctors, including anamnesis and 
examination results, was passed to the locally run LLMs (Gemma 2, Mistral Nemo, 
and Llama 3), which were asked to provide diagnostic and treatment strategies. 
Recommendations of the LLMs and the treating ORL doctors were rated by 3 
experienced ORL consultants on a 6-point Likert scale for medical adequacy, 
conciseness, coherence, and comprehensibility. Moreover, consultants were asked 
whether the answers pose a risk to the patient's safety. A modified Turing test 
was performed to distinguish responses generated by LLMs from those of doctors. 
Finally, the potential influence of the information generated by the LLMs on the 
raters' own diagnosis and treatment opinions was evaluated.
RESULTS: Over all categories, ORL doctors achieved superior (P<.0005) ratings 
compared to locally run LLMs (Llama 3, Mistral Nemo, and Gemma 2). ORL doctors' 
responses were considered hazardous for patients in only 1% of the ratings, 
whereas recommendations by Llama 3, Gemma 2, and Mistral Nemo were considered 
hazardous in 54%, 47%, and 32% of cases, respectively. According to the raters, 
the LLM's information rarely influenced their judgment, with Mistral Nemo, Gemma 
2, and Llama 3 achieving 1%, 3%, and 4% of the ratings, respectively.
CONCLUSIONS: Although locally run LLM models still underperform compared with 
their web-based counterparts, they achieved respectable results on outpatient 
treatment in this study. Nevertheless, the retrospective and single-center 
nature of the study, along with the clinicians' documentation style, may have 
introduced bias in favor of human recommendations. In the future, locally run 
LLMs will help address data protection concerns; however, further refinement and 
prospective validation are still needed to meet strict medical device 
requirements. As locally run LLMs continue to evolve, they are likely to become 
comparably powerful to web-based LLMs and become established as useful tools to 
support doctors in clinical practice.

© Christoph Raphael Buhr, Christopher Seifen, Katharina Bahr-Hamm, Tilman 
Huppertz, Johannes Pordzik, Harry Smith, Tom Kelsey, Andrew Blaikie, Christoph 
Matthias, Sebastian Kuhn, Jonas Eckrich. Originally published in JMIR Formative 
Research (https://formative.jmir.org).

DOI: 10.2196/76896
PMCID: PMC12646549
PMID: 41289564 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: SK is a Founder & 
shareholder of MED.digital. All other authors declare no conflicts of interest.


656. BMC Nurs. 2025 Jul 1;24(1):720. doi: 10.1186/s12912-025-03458-2.

The impact of an AI-focused ethics education program on nursing students' 
ethical awareness, moral sensitivity, attitudes, and generative AI adoption 
intention: a quasi-experimental study.

Abuadas M(1), Albikawi Z(2), Rayani A(3).

Author information:
(1)Nursing Faculty, Basic Nursing Department, Yarmouk University, Irbid, Jordan. 
mabuadas@yu.edu.jo.
(2)Nursing Faculty, Clinical Nursing Department, Yarmouk University, Irbid, 
Jordan.
(3)Community and Psychiatric Mental Health Nursing Department, College of 
Nursing, King Saud University, Riyadh, Saudi Arabia.

BACKGROUND: The integration of Generative artificial intelligence (GAI) into 
healthcare is rapidly evolving, necessitating ethical preparedness among nursing 
students. GAI technologies present ethical challenges related to patient 
privacy, algorithmic bias, and informed consent, underscoring the need for 
structured AI ethics education in nursing curricula. This study aims to examine 
the impact of an ethics education program on nursing students' AI ethical 
awareness, moral sensitivity, attitudes, and intentions to use GAI in 
healthcare.
METHODS: A quasi experimental, pretest‒posttest study was conducted with 115 
nursing students. The participants were randomly assigned to an intervention 
group (n = 57), which received a structured AI ethics education program, or a 
control group (n = 58), which did not receive the intervention. The validated 
scales measured AI ethical awareness, moral sensitivity, attitudes, and the 
intention to use AI before and after the intervention.
RESULTS: Compared with the control group, the intervention group demonstrated a 
significant increase in AI ethical awareness (M = 57.28, SD = 22.28) (M = 47.43, 
SD = 24.04; p = .025, η² = 0.044). Moral sensitivity also showed a notable 
improvement in the intervention group (M = 74.33, SD = 29.93) compared with the 
control group (M = 60.26, SD = 22.52; p = .005, η² = 0.067). Compared with the 
control group, positive attitudes toward AI significantly increased 
postintervention (M = 39.46, SD = 11.51) (M = 23.21, SD = 11.72; p < .001, η² = 
0.332), indicating a strong effect of ethics education. Furthermore, the 
intention to use AI technology improved significantly in the intervention group 
(M = 12.46, SD = 3.55) compared with the control group (M = 10.24, SD = 3.15; 
p = .001, η² = 0.099). However, negative attitudes toward GAI did not 
significantly change postintervention.
CONCLUSION: This study highlights the effectiveness of structured AI ethics 
education in enhancing ethical competencies among nursing students. Integrating 
such programs into nursing curricula is essential to prepare future nurses for 
ethical decision-making in AI-driven healthcare. These findings support the 
development of standardized ethics training modules to guide responsible AI use 
in clinical practice and inform future curriculum design.
CLINICAL TRIAL NUMBER: Not applicable.

© 2025. The Author(s).

DOI: 10.1186/s12912-025-03458-2
PMCID: PMC12211453
PMID: 40597065

Conflict of interest statement: Declarations. Human ethics and consent to 
participate: Ethical approval for this study was granted by the Al Yarmouk 
University Research Ethics Committee (Reference No. IRB/2025/149). Additionally, 
permission was obtained from the relevant institutional authorities. 
Participation in the study was entirely voluntary, and written informed consent 
was obtained from all nursing faculty students before their involvement. 
Students were informed that their participation would not impact their academic 
performance or evaluations. They were also provided with details regarding the 
purpose of data collection and how their information would be used. Throughout 
data collection, storage, analysis, and reporting, adherence to ethical 
guidelines, including the principles outlined in the Declaration of Helsinki, 
was ensured. To maintain confidentiality, each participant was assigned a unique 
research code, ensuring that all data were processed anonymously. Consent for 
publication: Not applicable. Competing interests: The authors declare no 
competing interests.


657. J Med Ethics. 2025 Nov 15:jme-2025-111452. doi: 10.1136/jme-2025-111452. Online 
ahead of print.

How will 'Chat-IRB' impact research ethics review in LMICs?

Moodley K(1), Malpani R(2), Reis AA(3).

Author information:
(1)Medicine, University of Stellenbosch, Stellenbosch, WC, South Africa 
km@sun.ac.za.
(2)Research for Health, World Health Organization, Geneva, GE, Switzerland.
(3)Health Ethics and Governance Unit, Science for Health, World Health 
Organization, Geneve, Switzerland.

While the use of generative artificial intelligence (AI) in research has sparked 
controversy internationally, the use of large language models (LLMs) in the 
ethics review of research protocols is particularly contentious. Against this 
backdrop, Porsdam Mann et al recently published a comprehensive, well-balanced 
and carefully considered paper on the use of application-specific LLMs by 
Research Ethics Committees (RECs). Although we support the potential advantages 
that such curated LLMs can bring in improving the speed and efficiency of REC 
processes globally, there are some challenges that are unique to 
resource-constrained settings. While many of these challenges relate to 
infrastructural constraints, linguistic diversity, paper-based submission 
systems and the digital divide, substantive concerns are linked to the 
availability, relevance and quality of training data required for LLMs. 
Protection of confidential data submitted to RECs over decades is another 
concern. This is especially important where clinical trials are concerned. 
Furthermore, RECs in low- and middle-income countries (LMICs) have nuanced and 
historical considerations relating to research ethics that have arisen in the 
context of asymmetrical power differentials in international collaborative 
research. Currently, such considerations are not easily delegated to AI systems. 
There are also risks, especially automation bias, in contexts where skills 
development in research ethics review is much needed and progressing well. 
Mitigating such risks may be possible in the future by implementing various 
guardrails. However, exploring potentially different ways in which LLMs could be 
used, especially in improving capacity development of REC members, is critical.

© Author(s) (or their employer(s)) 2025. No commercial re-use. See rights and 
permissions. Published by BMJ Group.

DOI: 10.1136/jme-2025-111452
PMID: 41241416

Conflict of interest statement: Competing interests: None declared.


658. Br J Anaesth. 2025 Feb 7:S0007-0912(25)00006-6. doi: 10.1016/j.bja.2025.01.001. 
Online ahead of print.

A comparison of large language model-generated and published perioperative 
neurocognitive disorder recommendations: a cross-sectional web-based analysis.

Saxena S(1), Barreto Chang OL(2), Suppan M(3), Meco BC(4), Vacas S(5), Radtke 
F(6), Matot I(7), Devos A(8), Maze M(9), Gisselbaek M(10), Berger-Estilita 
J(11).

Author information:
(1)Department of Surgery, Research Institute for Health Sciences and Technology, 
University of Mons, Mons, Belgium; Department of Anesthesiology, Helora, Mons, 
Belgium.
(2)Department of Anesthesia and Perioperative Care, University of California San 
Francisco, San Francisco, CA, USA. Electronic address: 
https://twitter.com/@OdmaraBarreto.
(3)Division of Anesthesiology, Department of Anesthesiology, Clinical 
Pharmacology, Intensive Care and Emergency Medicine, Geneva University Hospitals 
and Faculty of Medicine, Geneva, Switzerland.
(4)Department of Anaesthesia and Intensive Care, Ankara University Faculty of 
Medicine, Ankara, Turkey; Ankara University Brain Research Center (BAUM), 
Ankara, Turkey.
(5)Department of Anesthesiology, Critical Care, and Pain Medicine, Massachusetts 
General Hospital, Harvard Medical School, Boston, MA, USA.
(6)Department of Anaesthesia and Intensive Care, Hospital of Nykøbing Falster, 
University of Southern Denmark, Odense, Denmark.
(7)Division of Anesthesia, Intensive Care, and Pain Management, Tel-Aviv Medical 
Center, Tel-Aviv University, Tel-Aviv, Israel.
(8)ETH AI Center, Swiss Federal Institute of Technology Zurich (ETH Zurich), 
Zurich, Switzerland.
(9)Department of Anesthesia and Perioperative Care, University of California San 
Francisco, San Francisco, CA, USA.
(10)Division of Anesthesiology, Department of Anesthesiology, Clinical 
Pharmacology, Intensive Care and Emergency Medicine, Geneva University Hospitals 
and Faculty of Medicine, Geneva, Switzerland; Unit of Development and Research 
in Medical Education (UDREM), Faculty of Medicine, University of Geneva, Geneva, 
Switzerland.
(11)Institute for Medical Education, University of Bern, Bern, Switzerland; 
CINTESIS@RISE, Centre for Health Technology and Services Research, Faculty of 
Medicine, University of Porto, Porto, Portugal. Electronic address: 
joanamberger@gmail.com.

BACKGROUND: Perioperative neurocognitive disorders (PNDs) are common 
complications after surgery and anaesthesia, particularly in older adults, 
leading to increased morbidity, mortality, and healthcare costs. Therefore, 
major medical societies have developed recommendations for the prevention and 
treatment of PNDs. Our study evaluated the reliability of large language models, 
specifically ChatGPT-4 and Gemini, in generating recommendations for PND 
management and comparing them with published guidelines.
METHODS: We conducted an online cross-sectional web-based analysis over 48 h in 
June 2024. Artificial intelligence (AI)-generated recommendations were produced 
in six different locations across five countries (Switzerland, Belgium, Turkey, 
Canada, and the East and West Coasts of the USA). The English prompt 'a table of 
a bundle of care for perioperative neurocognitive disorders' was entered into 
ChatGPT-4 and Gemini, generating tables evaluated by independent reviewers. The 
primary outcomes were the Total Disagreement Score (TDS) and Quality Assessment 
of Medical Artificial Intelligence (QAMAI), which compared AI-generated 
recommendations with published guidelines.
RESULTS: The study generated 14 tables, with TDS and QAMAI scores showing 
similar results for ChatGPT-4 and Gemini (2 [1-3] vs 2 [2-3], P=0.636 and 4 
[4-4] vs 4 [3-4], P=0.424, respectively). AI-generated recommendations aligned 
well with published guidelines, with the highest alignment observed in 
ChatGPT-4-generated recommendations. No complete agreement with guidelines was 
achieved, and lack of cited sources was a noted limitation.
CONCLUSIONS: Large language models can generate perioperative neurocognitive 
disorder recommendations that align closely with published guidelines. However, 
further validation and integration of clinician feedback are required before 
clinical application.

Copyright © 2025 The Author(s). Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.bja.2025.01.001
PMID: 39922789

Conflict of interest statement: Declarations of interests OLBC participated as 
an investigator for the clinical trial OLIVER from Medtronic. SS has received 
speaker's fees from Medtronic/Merck. JBE is a member of the Board of Directors 
of the European Society of Anesthesiology and Intensive Care (ESAIC) and has 
received speaker's fees from Medtronic.


659. Front Digit Health. 2025 Nov 28;7:1615250. doi: 10.3389/fdgth.2025.1615250. 
eCollection 2025.

AI-driven dynamic psychological measurement: correcting university student 
mental health scales using daily behavioral and cognitive data.

Tong BG(1), Liang Z(1), He X(1), Yang F(1), Yang L(2), Gao L(3).

Author information:
(1)Department of Psychiatry, Inner Mongolia People's Hospital, Hohhot, China.
(2)Student Management Department, Inner Mongolia People's Hospital, Hohhot, 
China.
(3)Inner Mongolia Clinical College, Inner Mongolia Medical University, Hohhot, 
China.

OBJECTIVE: This study aimed to evaluate an Artificial Intelligence (AI)-driven 
dynamic psychological measurement method for correcting traditional mental 
health scales. We sought to validate its feasibility using daily behavioral and 
cognitive data from university students and assess its potential as an 
intervention tool.
METHODS: A total of 177 university students participated in a 
one-and-a-half-year study. Using a WeChat mini-program, we collected data from 
cognitive voting (87 instances), behavioral check-ins (66 instances), and 
standardized psychological scales (SAS, SDS, SCL-90). Scale scores were 
dynamically adjusted using Large Language Models (LLMs) and Retrieval-Augmented 
Generation (RAG) techniques. Paired-sample t-tests, MANOVA, and Cohen's d were 
used to compare the performance of the dynamic model against traditional scales. 
Intervention effects were validated using the Hamilton Anxiety Rating Scale 
(HAM-A) and Hamilton Depression Rating Scale (HAM-D).
RESULTS: The dynamic assessment demonstrated superior performance in identifying 
both anxiety (SAS: dynamic model AUC = 0.95 vs. traditional AUC = 0.86) and 
depression (SDS: dynamic model AUC = 0.93 vs. traditional AUC = 0.82). Over 
three semesters, participating students showed significant decreases in 
clinically-rated anxiety scores on the HAM-A (15.2% reduction; 95% CI for mean 
difference [1.00, 5.25], p = 0.004) and depression scores on the HAM-D (40.0% 
reduction; 95% CI for mean difference [2.71, 7.71], p < 0.001 ). High student 
engagement was observed (cognitive voting participation: 79%; behavioral 
check-ins: 42%). While the dynamic adjustment for the SCL-90 was initially 
effective ( R2 = 0.34 ), its specificity later decreased, potentially due to 
interference from life factors (dynamic model MSE = 102.74 vs. traditional MSE = 
84.17).
DISCUSSION: AI-driven dynamic assessment provides superior accuracy for anxiety 
(SAS) and depression (SDS) scales over static methods by effectively capturing 
psychological fluctuations. The significant reductions in clinically-rated 
anxiety and depression suggest the system may function as an integrated 
assessment-intervention loop, fostering self-awareness through continuous 
feedback. High user engagement confirms the method's feasibility. However, the 
model's diminished specificity for the complex SCL-90 scale over time highlights 
challenges in handling intricate, long-term symptom patterns. This research 
supports a shift towards continuous "digital phenotyping" and underscores the 
need for rigorous validation, multimodal data integration, and robust ethical 
considerations.

© 2025 Tong, Liang, He, Yang, Yang and Gao.

DOI: 10.3389/fdgth.2025.1615250
PMCID: PMC12698557
PMID: 41395629

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


660. JAMA Netw Open. 2025 Feb 3;8(2):e2457879. doi: 
10.1001/jamanetworkopen.2024.57879.

Large Language Models for Chatbot Health Advice Studies: A Systematic Review.

Huo B(1), Boyle A(2), Marfo N(3), Tangamornsuksan W(4), Steen JP(5), McKechnie 
T(1), Lee Y(1), Mayol J(6), Antoniou SA(7), Thirunavukarasu AJ(8), Sanger S(9), 
Ramji K(1), Guyatt G(10).

Author information:
(1)Division of General Surgery, Department of Surgery, McMaster University, 
Hamilton, Ontario, Canada.
(2)Michael G. DeGroote School of Medicine, McMaster University, Hamilton, 
Ontario, Canada.
(3)H. Ross University School of Medicine, Miramar, Florida.
(4)Department of Health Research Methods, Evidence, and Impact, McMaster 
University, Hamilton, Ontario, Canada.
(5)Institute of Health Policy, Management and Evaluation, University of Toronto, 
Toronto, Ontario, Canada.
(6)Hospital Clinico San Carlos, IdISSC, Universidad Complutense de Madrid, 
Madrid, Spain.
(7)Department of Surgery, Papageorgiou General Hospital, Thessaloniki, Greece.
(8)Oxford University Clinical Academic Graduate School, University of Oxford, 
Oxford, United Kingdom.
(9)Health Science Library, Faculty of Health Sciences, McMaster University, 
Hamilton, Ontario, Canada.
(10)Department of Clinical Epidemiology and Biostatistics, McMaster University, 
Hamilton, Ontario, Canada.

Erratum in
    JAMA Netw Open. 2025 Mar 3;8(3):e256511. doi: 
10.1001/jamanetworkopen.2025.6511.

IMPORTANCE: There is much interest in the clinical integration of large language 
models (LLMs) in health care. Many studies have assessed the ability of LLMs to 
provide health advice, but the quality of their reporting is uncertain.
OBJECTIVE: To perform a systematic review to examine the reporting variability 
among peer-reviewed studies evaluating the performance of generative artificial 
intelligence (AI)-driven chatbots for summarizing evidence and providing health 
advice to inform the development of the Chatbot Assessment Reporting Tool 
(CHART).
EVIDENCE REVIEW: A search of MEDLINE via Ovid, Embase via Elsevier, and Web of 
Science from inception to October 27, 2023, was conducted with the help of a 
health sciences librarian to yield 7752 articles. Two reviewers screened 
articles by title and abstract followed by full-text review to identify primary 
studies evaluating the clinical accuracy of generative AI-driven chatbots in 
providing health advice (chatbot health advice studies). Two reviewers then 
performed data extraction for 137 eligible studies.
FINDINGS: A total of 137 studies were included. Studies examined topics in 
surgery (55 [40.1%]), medicine (51 [37.2%]), and primary care (13 [9.5%]). Many 
studies focused on treatment (91 [66.4%]), diagnosis (60 [43.8%]), or disease 
prevention (29 [21.2%]). Most studies (136 [99.3%]) evaluated inaccessible, 
closed-source LLMs and did not provide enough information to identify the 
version of the LLM under evaluation. All studies lacked a sufficient description 
of LLM characteristics, including temperature, token length, fine-tuning 
availability, layers, and other details. Most studies (136 [99.3%]) did not 
describe a prompt engineering phase in their study. The date of LLM querying was 
reported in 54 (39.4%) studies. Most studies (89 [65.0%]) used subjective means 
to define the successful performance of the chatbot, while less than one-third 
addressed the ethical, regulatory, and patient safety implications of the 
clinical integration of LLMs.
CONCLUSIONS AND RELEVANCE: In this systematic review of 137 chatbot health 
advice studies, the reporting quality was heterogeneous and may inform the 
development of the CHART reporting standards. Ethical, regulatory, and patient 
safety considerations are crucial as interest grows in the clinical integration 
of LLMs.

DOI: 10.1001/jamanetworkopen.2024.57879
PMCID: PMC11795331
PMID: 39903463 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest Disclosures: Dr 
Thirunavukarasu reported receiving grants from HealthSense for research 
involving large language models outside the submitted work. Dr Ramji reported 
being Phelix AI cofounder and medical lead outside the submitted work. No other 
disclosures were reported.


661. JMIR Med Inform. 2025 Jul 14;13:e66973. doi: 10.2196/66973.

Performance of Natural Language Processing versus International Classification 
of Diseases Codes in Building Registries for Patients With Fall Injury: 
Retrospective Analysis.

Taseh A(1), Sasanfar S(1), Chan M(1), Sirls E(1), Nazarian A(2), Batmanghelich 
K(3), Bean JF(4)(5)(6), Ashkani-Esfahani S(1).

Author information:
(1)Foot & Ankle Research and Innovations Laboratory (FARIL), Department of 
Orthopaedic Surgery, Mass General Brigham, Harvard Medical School, 158 Boston 
Post Road, Weston, MA, 02493, United States, 1 7818279613.
(2)Musculoskeletal Translational Innovation Initiative, Carl J. Shapiro 
Department of Orthopaedic Surgery, Beth Israel Deaconess Medical Center, Harvard 
Medical School, Boston, MA, United States.
(3)Batman Laboratory, Department of Electrical and Computer Engineering, College 
of Engineering, Boston University, Boston, MA, United States.
(4)New England Geriatric Research Education and Clinical Center (GRECC), 
Veterans Affair Boston Healthcare System, Boston, MA, United States.
(5)Department of Physical Medicine and Rehabilitation, Harvard Medical School, 
Boston, MA, United States.
(6)Spaulding Rehabilitation, Boston, MA, United States.

BACKGROUND: Standardized registries, such as the International Classification of 
Diseases (ICD) codes, are commonly built using administrative codes assigned to 
patient encounters. However, patients with fall injury are often coded using 
subsequent injury codes, such as hip fractures. This necessitates manual 
screening to ensure the accuracy of data registries.
OBJECTIVE: This study aimed to automate the extraction of fall incidents and 
mechanisms using natural language processing (NLP) and compare this approach 
with the ICD method.
METHODS: Clinical notes for patients with fall-induced hip fractures were 
retrospectively reviewed by medical experts. Fall incidences were detected, 
annotated, and classified among patients who had a fall-induced hip fracture 
(case group). The control group included patients with hip fractures without any 
evidence of falls. NLP models were developed using the annotated notes of the 
study groups to fulfill two separate tasks: fall occurrence detection and fall 
mechanism classification. The performances of the models were compared using 
accuracy, sensitivity, specificity, positive predictive value, negative 
predictive value, F1-score, and area under the receiver operating characteristic 
curve.
RESULTS: A total of 1769 clinical notes were included in the final analysis for 
the fall occurrence task, and 783 clinical notes were analyzed for the fall 
mechanism classification task. The highest F1-score using NLP for fall 
occurrence was 0.97 (specificity=0.96; sensitivity=0.97), and for fall mechanism 
classification was 0.61 (specificity=0.56; sensitivity=0.62). Natural language 
processing could detect up to 98% of the fall occurrences and 65% of the fall 
mechanisms accurately, compared to 26% and 12%, respectively, by ICD codes.
CONCLUSIONS: Our findings showed promising performance with higher accuracy of 
NLP algorithms compared to the conventional method for detecting fall occurrence 
and mechanism in developing disease registries using clinical notes. Our 
approach can be introduced to other registries that are based on large data and 
are in need of accurate annotation and classification.

© Atta Taseh, Souri Sasanfar, Michelle Chan, Evan Sirls, Ara Nazarian, Kayhan 
Batmanghelich, Jonathan F Bean, Soheil Ashkani-Esfahani. Originally published in 
JMIR Medical Informatics (https://medinform.jmir.org).

DOI: 10.2196/66973
PMCID: PMC12279314
PMID: 40658984 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


662. JMIR Form Res. 2025 Aug 27;9:e76097. doi: 10.2196/76097.

Evaluating the Quality and Understandability of Radiology Report Summaries 
Generated by ChatGPT: Survey Study.

Sunshine A(1), Honce GH(2), Callen AL(1), Zander DA(1), Tanabe JL(1), Pisani 
Petrucci SL(1), Lin CT(3), Honce JM(1).

Author information:
(1)Department of Radiology, University of Colorado Anschutz Medical Campus, 19th 
Ave. Mail Stop C278, Aurora, CO, 80045, United States, 1 303-724-3796, 1 
303-724-3795.
(2)Hartway Evaluation Group, Denver, CO, United States.
(3)Department of Medicine, University of Colorado Anschutz Medical Campus, 
Aurora, CO, United States.

BACKGROUND: Radiology reports convey critical medical information to health care 
providers and patients. Unfortunately, they are often difficult for patients to 
comprehend, causing confusion and anxiety, thereby limiting patient engagement 
in health care decision-making. Large language models (LLMs) like ChatGPT 
(OpenAI) can create simplified, patient-friendly report summaries to increase 
accessibility, albeit with errors.
OBJECTIVE: We evaluated the accuracy and clarity of ChatGPT-generated summaries 
compared to original radiologist-assessed radiology reports, assessed patients' 
understanding and satisfaction with the summaries compared to the original 
reports, and compared the readability of the original reports and summaries 
using validated readability metrics.
METHODS: We anonymized 30 radiology reports created by neuroradiologists at our 
institution (6 brain magnetic resonance imaging, 6 brain computed tomography, 6 
head and neck computed tomography angiography, 6 neck computed tomography, and 6 
spine computed tomography). These anonymized reports were processed by ChatGPT 
to produce patient-centric summaries. Four board-certified neuroradiologists 
evaluated the ChatGPT-generated summaries on quality and accuracy compared to 
the original reports, and 4 patient volunteers separately evaluated the reports 
and summaries on perceived understandability and satisfaction. Readability was 
assessed using word count and validated readability scales.
RESULTS: After reading the summary, patient confidence in understanding (98%, 
116/118 vs 26%, 31/118) and satisfaction regarding the level of 
jargon/terminology (91%, 107/118 vs 8%, 9/118) and time taken to understand the 
content (97%, 115/118 vs 23%, 27/118) substantially improved. Ninety-two percent 
(108/118) of responses indicated the summary clarified patients' questions about 
the report, and 98% (116/118) of responses indicated patients would use the 
summary if available, with 67% (79/118) of responses indicating they would want 
access to both the report and summary, while 26% (31/118) of responses indicated 
only wanting the summary. Eighty-three percent (100/120) of radiologist 
responses indicated the summary represented the original report "extremely well" 
or "very well," with only 5% (6/120) of responses indicating it did so "slightly 
well" or "not well at all." Five percent (6/120) of responses indicated there 
was missing relevant medical information in the summary, 12% (14/120) reported 
instances of overemphasis of nonsignificant findings, and 18% (22/120) reported 
instances of underemphasis of significant findings. No fabricated findings were 
identified. Overall, 83% (99/120) of responses indicated that the summary would 
definitely/probably not lead patients to incorrect conclusions about the 
original report, with 10% (12/120) of responses indicating the summaries may do 
so.
CONCLUSIONS: ChatGPT-generated summaries could significantly improve perceived 
comprehension and satisfaction while accurately reflecting most key information 
from original radiology reports. Instances of minor omissions and 
under-/overemphasis were noted in some summaries, underscoring the need for 
ongoing validation and oversight. Overall, these artificial 
intelligence-generated, patient-centric summaries hold promise for enhancing 
patient-centered communication in radiology.

© Alexis Sunshine, Grace H Honce, Andrew L Callen, David A Zander, Jody L 
Tanabe, Samantha L Pisani Petrucci, Chen-Tan Lin, Justin M Honce. Originally 
published in JMIR Formative Research (https://formative.jmir.org).

DOI: 10.2196/76097
PMCID: PMC12385610
PMID: 40864905 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


663. Artif Intell Med. 2025 Dec;170:103268. doi: 10.1016/j.artmed.2025.103268. Epub 
2025 Sep 22.

A survey for large language models in biomedicine.

Wang C(1), Li M(2), He J(3), Wang Z(4), Darzi E(5), Chen Z(6), Ye J(7), Li T(3), 
Su Y(3), Ke J(8), Qu K(2), Li S(2), Yu Y(2), Liò P(9), Wang T(10), Wang YG(11), 
Shen Y(12).

Author information:
(1)School of Medical Engineering, Henan Medical University, Xinxiang, China; 
Engineering Technology Research Center of Neurosense and Control of Henan 
Province, Xinxiang, China; Henan International Joint Laboratory of Neural 
Information Analysis and Drug Intelligent Design, Xinxiang, China.
(2)School of Medical Engineering, Henan Medical University, Xinxiang, China.
(3)Shanghai AI Laboratory, Shanghai, China.
(4)Amazon, Palo Alto, CA, USA.
(5)Boston Children's Hospital, MA, USA; Harvard Medical School, Harvard 
University, MA, USA.
(6)Toursun Synbio, Shanghai, China.
(7)Shanghai AI Laboratory, Shanghai, China; Department of Data Science & AI, 
Faculty of IT, Monash University, Melbourne, Australia.
(8)School of Electronic Information and Electrical Engineering, Shanghai Jiao 
Tong University, Shanghai, China; School of Computer Science and Engineering, 
University of New South Wales, Sydney, Australia.
(9)Department of Computer Science and Technology, University of Cambridge, 
Cambridge, UK.
(10)School of Basic Medical Sciences, Henan Medical University, Xinxiang, China. 
Electronic address: wtianyuncn@126.com.
(11)Shanghai AI Laboratory, Shanghai, China; Toursun Synbio, Shanghai, China; 
Institute of Natural Sciences, Shanghai Jiao Tong University, Shanghai, China; 
School of Mathematics and Statistics, University of New South Wales, Sydney, 
Australia. Electronic address: yuguang.wang@sjtu.edu.cn.
(12)Department of Computer Science, Johns Hopkins University, MD, USA. 
Electronic address: yshen92@jhu.edu.

Recent breakthroughs in large language models (LLMs) offer unprecedented natural 
language understanding and generation capabilities. However, existing surveys on 
LLMs in biomedicine often focus on specific applications or model architectures, 
lacking a comprehensive analysis that integrates the latest advancements across 
various biomedical domains. This review, based on an analysis of 484 
publications sourced from databases including PubMed, Web of Science, and arXiv, 
provides an in-depth examination of the current landscape, applications, 
challenges, and prospects of LLMs in biomedicine, distinguishing itself by 
focusing on the practical implications of these models in real-world biomedical 
contexts. Firstly, we explore the capabilities of LLMs in zero-shot learning 
across a broad spectrum of biomedical tasks, including diagnostic assistance, 
drug discovery, and personalized medicine, among others, with insights drawn 
from 137 key studies. Then, we discuss adaptation strategies of LLMs, including 
fine-tuning methods for both uni-modal and multi-modal LLMs to enhance their 
performance in specialized biomedical contexts where zero-shot fails to achieve, 
such as medical question answering and efficient processing of biomedical 
literature. Finally, we discuss the challenges that LLMs face in the biomedicine 
domain including data privacy concerns, limited model interpretability, issues 
with dataset quality, and ethics due to the sensitive nature of biomedical data, 
the need for highly reliable model outputs, and the ethical implications of 
deploying AI in healthcare. To address these challenges, we also identify future 
research directions of LLM in biomedicine including federated learning methods 
to preserve data privacy and integrating explainable AI methodologies to enhance 
the transparency of LLMs. As this field of LLM rapidly evolves, continued 
research and development are essential to fully harness the capabilities of LLMs 
in biomedicine while ensuring their responsible and effective deployment.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.artmed.2025.103268
PMID: 40997586 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that the research was conducted in the absence of any commercial or 
financial relationships that could be construed as a potential conflict of 
interest.


664. NPJ Digit Med. 2025 Dec 22;8(1):793. doi: 10.1038/s41746-025-02273-y.

Streamlining evidence based clinical recommendations with large language models.

Li D(#)(1), Jiang N(#)(2), Huang K(2), Tu R(1), Ouyang S(1), Yu H(1), Qiao L(2), 
Yu C(2), Zhou T(3), Tong D(3), Wang Q(2), Li M(2), Zeng X(2), Tian Y(4), Tian 
X(5), Li J(6).

Author information:
(1)College of Biomedical Engineering and Instrument Science, Zhejiang 
University, Engineering Research Center of EMR and Intelligent Expert System, 
Ministry of Education, Hangzhou, China.
(2)Department of Rheumatology and Clinical Immunology, Peking Union Medical 
College Hospital (PUMCH), Chinese Academy of Medical Sciences and Peking Union 
Medical College; National Clinical Research Center for Dermatologic and 
Immunologic Diseases (NCRC-DID), Ministry of Science and Technology; State Key 
Laboratory of Complex Severe and Rare Diseases; Key Laboratory of Rheumatology 
and Clinical Immunology, Ministry of Education, Beijing, China.
(3)Research Center for Scientific Data Hub, Zhejiang Lab, Hangzhou, China.
(4)College of Biomedical Engineering and Instrument Science, Zhejiang 
University, Engineering Research Center of EMR and Intelligent Expert System, 
Ministry of Education, Hangzhou, China. tyler@zju.edu.cn.
(5)Department of Rheumatology and Clinical Immunology, Peking Union Medical 
College Hospital (PUMCH), Chinese Academy of Medical Sciences and Peking Union 
Medical College; National Clinical Research Center for Dermatologic and 
Immunologic Diseases (NCRC-DID), Ministry of Science and Technology; State Key 
Laboratory of Complex Severe and Rare Diseases; Key Laboratory of Rheumatology 
and Clinical Immunology, Ministry of Education, Beijing, China. tianxp6@126.com.
(6)College of Biomedical Engineering and Instrument Science, Zhejiang 
University, Engineering Research Center of EMR and Intelligent Expert System, 
Ministry of Education, Hangzhou, China. ljs@zju.edu.cn.
(#)Contributed equally

Clinical evidence underpins informed healthcare decisions, yet integrating it 
into real-time practice remains challenging due to intensive workloads, complex 
procedures, and time constraints. This study presents Quicker, an LLM-powered 
system that automates evidence synthesis and generates clinical recommendations 
following standard guideline development workflows. Quicker delivers an 
end-to-end pipeline from clinical questions to recommendations and supports 
customized decision-making through integrated tools and interactive interfaces. 
To evaluate how closely Quicker can reproduce guideline development processes, 
we constructed Q2CRBench-3, a benchmark derived from guideline development 
records for three diseases. Experiments show that Quicker produces precise 
question decomposition, expert-aligned retrieval, and near-comprehensive 
screening. Quicker assistance improved the accuracy of extracted study data, and 
its recommendations were more comprehensive and coherent than clinician-written 
ones. In system-level testing, Quicker working with one participant reduced 
recommendation development to 20-40 min. Overall, the findings demonstrate 
Quicker's potential to enhance the speed and reliability of evidence-based 
clinical decision-making.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-02273-y
PMCID: PMC12749395
PMID: 41423701

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


665. NPJ Digit Med. 2025 Jul 30;8(1):486. doi: 10.1038/s41746-025-01886-7.

A scoping review of artificial intelligence applications in clinical trial risk 
assessment.

Teodoro D(1), Naderi N(2), Yazdani A(3), Zhang B(3), Bornet A(3).

Author information:
(1)Department of Radiology and Medical Informatics, Faculty of Medicine, 
University of Geneva, Geneva, Switzerland. douglas.teodoro@unige.ch.
(2)Laboratoire Interdisciplinaire des Sciences du Numérique, CNRS, Université 
Paris-Saclay, Gif-sur-Yvette, France.
(3)Department of Radiology and Medical Informatics, Faculty of Medicine, 
University of Geneva, Geneva, Switzerland.

Artificial intelligence (AI) is increasingly applied to clinical trial risk 
assessment, aiming to improve safety and efficiency. This scoping review 
analyzed 142 studies published between 2013 and 2024, focusing on safety 
(n = 55), efficacy (n = 46), and operational (n = 45) risk prediction. AI 
techniques, including traditional machine learning, deep learning (e.g., graph 
neural networks, transformers), and causal machine learning, are used for tasks 
like adverse drug event prediction, treatment effect estimation, and phase 
transition prediction. These methods utilize diverse data sources, from 
molecular structures and clinical trial protocols to patient data and scientific 
publications. Recently, large language models (LLMs) have seen a surge in 
applications, featuring in 7 out of 33 studies in 2023. While some models 
achieve high performance (AUROC up to 96%), challenges remain, including 
selection bias, limited prospective studies, and data quality issues. Despite 
these limitations, AI-based risk assessment holds substantial promise for 
transforming clinical trials, particularly through improved risk-based 
monitoring frameworks.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01886-7
PMCID: PMC12307910
PMID: 40731070

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


666. J Trauma Acute Care Surg. 2025 Oct 1;99(4):518-521. doi: 
10.1097/TA.0000000000004725. Epub 2025 Jul 3.

Artificial intelligence in surgery research: Successfully implementing AI 
clinical decision support models.

Choi J(1).

Author information:
(1)From the Division of General Surgery, Department of Surgery, Stanford 
University, Stanford, California.

Artificial intelligence (AI) in surgery literature typically encompasses 
decision support models that aim to help clinicians make better decisions. Many 
studies report developing and validating models, yet few models are implemented 
at the bedside. Exceedingly few models achieve their intended goal upon 
implementation. While the TRIPOD-AI and DECIDE-AI guidelines outline separate 
reporting standards for the development/validation, and staged implementation of 
AI models, respectively, this article outlines how future implementation should 
be considered at the outset before model development. Building on lessons from 
high-performing AI decision support models that faced challenges upon 
implementation, we will discuss study design consideration for building 
trustworthy and actionable AI clinical decision support models that can cross 
the database-to-bedside gap and become successfully implemented.

Copyright © 2025 Wolters Kluwer Health, Inc. All rights reserved.

DOI: 10.1097/TA.0000000000004725
PMID: 40604360 [Indexed for MEDLINE]


667. BMC Med Educ. 2025 Nov 17;25(1):1609. doi: 10.1186/s12909-025-08188-2.

Artificial intelligence in undergraduate medical education: an updated scoping 
review.

Simoni J(1), Urtubia-Fernandez J(2), Mengual E(3), Simoni DA(4)(5), Royo M(6), 
Egaña-Yin D(2), Hertog OLA(2), López-Ortiz L(2), Muñoz-Tomás A(2), 
Santiago-Martínez P(2), Vahamaki A(2), Pereira JL(7)(4).

Author information:
(1)Medical Education Unit, Faculty of Medicine, University of Navarra, Pamplona, 
Spain. jsimoni@unav.es.
(2)Faculty of Medicine, University of Navarra, Pamplona, Spain.
(3)Dept. of Pathology, Anatomy and Physiology, Faculty of Medicine, University 
of Navarra, Pamplona, Spain.
(4)Institute of Culture and Society, University of Navarra, Pamplona, Spain.
(5)Institute of Data Science and Artificial Intelligence (DATAI), University of 
Navarra, Pamplona, Spain.
(6)Library Services, University of Navarra, Pamplona, Spain.
(7)Medical Education Unit, Faculty of Medicine, University of Navarra, Pamplona, 
Spain.

BACKGROUND: The irrevocable alteration of medical education due to widespread 
access to large language models (LLMs) in 2022, and the concomitant surge in 
AI-related literature, has prompted us to update the evolving impact of AI on 
undergraduate medical education (UGME).
METHODS: The scoping review adhered to the framework of Arksey and O'Malley. A 
literature search was conducted in April 2024 on PubMed, Scopus, Web of Science 
Core Collection, ERIC, and Google Scholar using the terms "UGME", "medical 
students", "AI", "NLP", "ML", "ChatGPT", and "LLM", and included publications 
that appeared from January 2020 to April 2024. The inclusion criteria were UGME 
and AI-related topics. The exclusion criteria were postgraduate education, 
continuing medical education, and non-AI technologies.
RESULTS: After screening 3,238 identified publications, 310 were ultimately 
included in the review. One hundred sixty-one publications (52%) related to AI 
use solely in UGME appeared in eight months between the time the last general 
medical education scoping review on AI took place and the current study. The use 
of AI is rapidly increasing in UGME, both in basic and clinical courses, with 
applications ranging from autonomous tutoring, self-assessment, and 
simulation-based learning to assessment generation and grading, clinical 
assessment, procedural skills evaluation, and predictive analytics, among 
others. No publications assessed AI's impact on critical thinking or clinical 
reasoning in medical students. While students strongly demand the acquisition of 
AI literacy during UGME, and some institutions have begun integrating AI into 
their curricula, there is neither a standardized approach for doing so nor a 
consensus on AI competencies or ethical frameworks in UGME.
CONCLUSIONS: This review highlights the dramatic increase in the use of AI in 
UGME, presenting both benefits and challenges. While AI can enhance learning 
experiences, the best evidence for its implementation is unclear and requires, 
as key priorities, the definition of AI competencies, pedagogical methods, and 
ethical guidelines. Further research is needed to assess the impact of AI on 
ethics, empathy, critical thinking, and clinical reasoning. Faculty development 
in AI is vital, as is the need for collaborative and international endeavors.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-08188-2
PMCID: PMC12625756
PMID: 41250109 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


668. NPJ Digit Med. 2025 Mar 1;8(1):134. doi: 10.1038/s41746-025-01528-y.

Developing a named entity framework for thyroid cancer staging and risk level 
classification using large language models.

Fung MMH(#)(1), Tang EHM(#)(2)(3), Wu T(#)(2), Luk Y(1), Au ICH(4), Liu X(1)(2), 
Lee VHF(5), Wong CK(6), Wei Z(2), Cheng WY(2), Tai ICY(7), Ho JWK(2)(8), Wong 
JWH(8), Lang BHH(1), Leung KSM(2)(9)(10)(11), Wong ZSY(12)(13)(14)(4), Wu 
JT(15)(16)(17)(18), Wong CKH(19)(20)(21)(22).

Author information:
(1)Division of Endocrine Surgery, Department of Surgery, School of Clinical 
Medicine, LKS Faculty of Medicine, The University of Hong Kong, Hong Kong SAR, 
China.
(2)Laboratory of Data Discovery for Health (D²4H), Hong Kong Science Park, Hong 
Kong SAR, China.
(3)Department of Family Medicine and Primary Care, School of Clinical Medicine, 
LKS Faculty of Medicine, The University of Hong Kong, Hong Kong SAR, China.
(4)School of Public Health, LKS Faculty of Medicine, The University of Hong 
Kong, Hong Kong SAR, China.
(5)Department of Clinical Oncology, School of Clinical Medicine, LKS Faculty of 
Medicine, The University of Hong Kong, Hong Kong SAR, China.
(6)Department of Medicine, School of Clinical Medicine, LKS Faculty of Medicine, 
The University of Hong Kong, Hong Kong SAR, China.
(7)Department of Orthopaedics and Traumatology, School of Clinical Medicine, LKS 
Faculty of Medicine, The University of Hong Kong, Hong Kong SAR, China.
(8)School of Biomedical Science, LKS Faculty of Medicine, The University of Hong 
Kong, Hong Kong SAR, China.
(9)The Hong Kong Jockey Club Global Health Institute, Hong Kong SAR, China.
(10)WHO Collaborating Centre for Infectious Disease Epidemiology and Control, 
School of Public Health, LKS Faculty of Medicine, The University of Hong Kong, 
Hong Kong SAR, China.
(11)The University of Hong Kong-Shenzhen Hospital, Shenzhen, China.
(12)The Kirby Institute, University of New South Wales, Sydney, Australia.
(13)Biomedical Informatics and Digital Health, School of Medical Sciences, The 
University of Sydney, Sydney, Australia.
(14)Graduate School of Public Health, St. Luke's International University, 
Tokyo, Japan.
(15)Laboratory of Data Discovery for Health (D²4H), Hong Kong Science Park, Hong 
Kong SAR, China. joewu@hku.hk.
(16)The Hong Kong Jockey Club Global Health Institute, Hong Kong SAR, China. 
joewu@hku.hk.
(17)WHO Collaborating Centre for Infectious Disease Epidemiology and Control, 
School of Public Health, LKS Faculty of Medicine, The University of Hong Kong, 
Hong Kong SAR, China. joewu@hku.hk.
(18)The University of Hong Kong-Shenzhen Hospital, Shenzhen, China. 
joewu@hku.hk.
(19)Laboratory of Data Discovery for Health (D²4H), Hong Kong Science Park, Hong 
Kong SAR, China. carlosho@hku.hk.
(20)Department of Family Medicine and Primary Care, School of Clinical Medicine, 
LKS Faculty of Medicine, The University of Hong Kong, Hong Kong SAR, China. 
carlosho@hku.hk.
(21)The Hong Kong Jockey Club Global Health Institute, Hong Kong SAR, China. 
carlosho@hku.hk.
(22)Department of Infectious Disease Epidemiology, London School of Hygiene and 
Tropical Medicine, London, UK. carlosho@hku.hk.
(#)Contributed equally

We developed a named entity (NE) framework for information extraction from 
semi-structured clinical notes retrieved from The Cancer Genome Atlas-Thyroid 
Cancer (TCGA-THCA) database and examined Large Language Models (LLMs) strategies 
to classify the 8th edition of American Joint Committee on Cancer (AJCC) staging 
and American Thyroid Association (ATA) risk category for patients with 
well-differentiated thyroid cancer. The NE framework consisted of annotation 
guidelines development, ground truth labelling, prompting approaches, and 
evaluation codes. Four LLMs (Mistral-7B-Instruct, Llama-3.1-8B-Instruct, 
Gemma-2-9B-Instruct, and Qwen2.5-7B-Instruct) were offline utilised for 
information extraction, comparing with expert-curated ground truth. Our 
framework was developed using 50 TCGA-THCA pathology notes. 289 TCGA-THCA notes 
and 35 pseudo-clinical cases were used for validation. Taking an ensemble-like 
majority-vote strategy achieved satisfactory performance for AJCC and ATA in 
both development and validation sets. Our framework and ensemble classifier 
optimised efficiency and accuracy of classifying stage and risk category in 
thyroid cancer patients.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01528-y
PMCID: PMC11873034
PMID: 40025285

Conflict of interest statement: Competing interests: Z.W. is contributing to npj 
Digital Medicine as an Associate Editor and Guest Editor for the Collection on 
Natural Language Processing in Clinical Medicine. Other authors declared no 
competing interests.


669. JMIR Form Res. 2025 Mar 19;9:e54803. doi: 10.2196/54803.

Synthetic Data-Driven Approaches for Chinese Medical Abstract Sentence 
Classification: Computational Study.

Li J(#)(1)(2)(3), Wang Z(#)(1)(2)(4), Yu L(5), Liu H(6), Song H(1)(2).

Author information:
(1)Shanghai Artificial Intelligence Research Institute Co., Ltd, Shanghai, 
China.
(2)Xiangfu Laboratory, Jiaxing, China.
(3)School of Chemistry and Chemical Engineering, Shanghai Jiao Tong University, 
Shanghai, China.
(4)Inner Mongolia Academy of Science and Technology, Hohhot, China.
(5)University of California San Diego, San Diego, CA, United States.
(6)Shanghai Civil Aviation College, Shanghai, China.
(#)Contributed equally

BACKGROUND: Medical abstract sentence classification is crucial for enhancing 
medical database searches, literature reviews, and generating new abstracts. 
However, Chinese medical abstract classification research is hindered by a lack 
of suitable datasets. Given the vastness of Chinese medical literature and the 
unique value of traditional Chinese medicine, precise classification of these 
abstracts is vital for advancing global medical research.
OBJECTIVE: This study aims to address the data scarcity issue by generating a 
large volume of labeled Chinese abstract sentences without manual annotation, 
thereby creating new training datasets. Additionally, we seek to develop more 
accurate text classification algorithms to improve the precision of Chinese 
medical abstract classification.
METHODS: We developed 3 training datasets (dataset #1, dataset #2, and dataset 
#3) and a test dataset to evaluate our model. Dataset #1 contains 15,000 
abstract sentences translated from the PubMed dataset into Chinese. Datasets #2 
and #3, each with 15,000 sentences, were generated using GPT-3.5 from 40,000 
Chinese medical abstracts in the CSL database. Dataset #2 used titles and 
keywords for pseudolabeling, while dataset #3 aligned abstracts with category 
labels. The test dataset includes 87,000 sentences from 20,000 abstracts. We 
used SBERT embeddings for deeper semantic analysis and evaluated our model using 
clustering (SBERT-DocSCAN) and supervised methods (SBERT-MEC). Extensive 
ablation studies and feature analyses were conducted to validate the model's 
effectiveness and robustness.
RESULTS: Our experiments involved training both clustering and supervised models 
on the 3 datasets, followed by comprehensive evaluation using the test dataset. 
The outcomes demonstrated that our models outperformed the baseline metrics. 
Specifically, when trained on dataset #1, the SBERT-DocSCAN model registered an 
impressive accuracy and F1-score of 89.85% on the test dataset. Concurrently, 
the SBERT-MEC algorithm exhibited comparable performance with an accuracy of 
89.38% and an identical F1-score. Training on dataset #2 yielded similarly 
positive results for the SBERT-DocSCAN model, achieving an accuracy and F1-score 
of 89.83%, while the SBERT-MEC algorithm recorded an accuracy of 86.73% and an 
F1-score of 86.51%. Notably, training with dataset #3 allowed the SBERT-DocSCAN 
model to attain the best with an accuracy and F1-score of 91.30%, whereas the 
SBERT-MEC algorithm also showed robust performance, obtaining an accuracy of 
90.39% and an F1-score of 90.35%. Ablation analysis highlighted the critical 
role of integrated features and methodologies in improving classification 
efficiency.
CONCLUSIONS: Our approach addresses the challenge of limited datasets for 
Chinese medical abstract classification by generating novel datasets. The 
deployment of SBERT-DocSCAN and SBERT-MEC models significantly enhances the 
precision of classifying Chinese medical abstracts, even when using synthetic 
datasets with pseudolabels.

© Zikai Wang, Longxuan Yu, Hui Liu, Haitao Song. Originally published in JMIR 
Formative Research (https://formative.jmir.org).

DOI: 10.2196/54803
PMCID: PMC11939029
PMID: 40106267 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


670. Radiology. 2025 Apr;315(1):e240775. doi: 10.1148/radiol.240775.

Deep Learning Applications in Imaging of Acute Ischemic Stroke: A Systematic 
Review and Narrative Summary.

Jiang B(#)(1), Pham N(#)(1), van Staalduinen EK(1), Liu Y(1), Nazari-Farsani 
S(1), Sanaat A(2), van Voorst H(1), Fettahoglu A(1), Kim D(1), Ouyang J(1), 
Kumar A(1), Srivatsan A(3), Hussein R(1), Lansberg MG(3), Boada F(1), Zaharchuk 
G(1).

Author information:
(1)Department of Radiology, Stanford University School of Medicine, 1201 Welch 
Rd, Stanford, CA 94305.
(2)Division of Nuclear Medicine and Molecular Imaging, Geneva University 
Hospital, Geneva, Switzerland.
(3)Stanford Stroke Center, Department of Neurology and Neurologic Sciences, 
Stanford University School of Medicine, Stanford, Calif.
(#)Contributed equally

Background Acute ischemic stroke (AIS) is a major cause of morbidity and 
mortality, requiring swift and precise clinical decisions based on neuroimaging. 
Recent advances in deep learning-based computer vision and language artificial 
intelligence (AI) models have demonstrated transformative performance for 
several stroke-related applications. Purpose To evaluate deep learning 
applications for imaging in AIS in adult patients, providing a comprehensive 
overview of the current state of the technology and identifying opportunities 
for advancement. Materials and Methods A systematic literature review was 
conducted following Preferred Reporting Items for Systematic Reviews and 
Meta-Analyses guidelines. A comprehensive search of four databases from January 
2016 to January 2024 was performed, targeting deep learning applications for 
imaging of AIS, including automated detection of large vessel occlusion and 
measurement of Alberta Stroke Program Early CT Score. Articles were selected 
based on predefined inclusion and exclusion criteria, focusing on convolutional 
neural networks and transformers. The top-represented areas were addressed, and 
the relevant information was extracted and summarized. Results Of 380 studies 
included, 171 (45.0%) focused on stroke lesion segmentation, 129 (33.9%) on 
classification and triage, 31 (8.2%) on outcome prediction, 15 (3.9%) on 
generative AI and large language models, and 11 (2.9%) on rapid or low-dose 
imaging specific to stroke applications. Detailed data extraction was performed 
for 68 studies. Public AIS datasets are also highlighted, for researchers 
developing AI models for stroke imaging. Conclusion Deep learning applications 
have permeated AIS imaging, particularly for stroke lesion segmentation. 
However, challenges remain, including the need for standardized protocols and 
test sets, larger public datasets, and performance validation in real-world 
settings. © RSNA, 2025 Supplemental material is available for this article.

DOI: 10.1148/radiol.240775
PMID: 40197098 [Indexed for MEDLINE]


671. NPJ Digit Med. 2026 Jan 7;9(1):51. doi: 10.1038/s41746-025-02324-4.

An autonomous agentic workflow for clinical detection of cognitive concerns 
using large language models.

Tian J(1), Fard P(1), Cagan C(1), Rezaii N(2), Rocha RB(2), Wang L(3), Junior 
VM(1), Blacker D(4), Haas JS(1), Patel CJ(5), Murphy SN(2), Moura LMVR(#)(2), 
Estiri H(#)(6).

Author information:
(1)Department of Medicine, Massachusetts General Hospital, Boston, MA, USA.
(2)Department of Neurology, Massachusetts General Hospital, Boston, MA, USA.
(3)Department of Medicine, Brigham and Women's Hospital, Boston, MA, USA.
(4)Department of Psychiatry, Massachusetts General Hospital, Boston, MA, USA.
(5)Department of Biomedical Informatics, Harvard Medical School, Boston, MA, 
USA.
(6)Department of Medicine, Massachusetts General Hospital, Boston, MA, USA. 
hestiri@mgh.harvard.edu.
(#)Contributed equally

Early detection of cognitive impairment is limited by traditional screening 
tools and resource constraints. We developed two large language model workflows 
for identifying cognitive concerns from clinical notes: (1) an expert-driven 
workflow with iterative prompt refinement across three LLMs (LLaMA 3.1 8B, LLaMA 
3.2 3B, Med42 v2 8B), and (2) an autonomous agentic workflow coordinating five 
specialized agents for prompt optimization. Using Llama3.1, we optimized on a 
balanced refinement dataset and validated on an independent dataset reflecting 
real-world prevalence. The agentic workflow achieved comparable validation 
performance (F1 = 0.74 vs. 0.81) and superior refinement results (0.93 vs. 0.87) 
relative to the expert-driven workflow. Sensitivity decreased from 0.91 to 0.62 
between datasets, demonstrating the impact of prevalence shift on 
generalizability. Expert re-adjudication revealed 44% of apparent false 
negatives reflected clinically appropriate reasoning. These findings demonstrate 
that autonomous agentic systems can approach expert-level performance while 
maintaining interpretability, offering scalable clinical decision supports.

© 2026. The Author(s).

DOI: 10.1038/s41746-025-02324-4
PMCID: PMC12808323
PMID: 41501421

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


672. Clin Exp Dermatol. 2025 Nov 25;50(12):2425-2430. doi: 10.1093/ced/llaf323.

Artificial intelligence compared with traditional methods of generating 
dermatology consultation letters: a pilot study comparing accuracy, readability 
and efficiency.

Farooq F(1), Cooper H(1), Shipman A(1), Mitchell CD(1).

Author information:
(1)Department of Dermatology, Portsmouth Hospitals University NHS Trust, 
Portsmouth, UK.

BACKGROUND: In medical practice, clinic letters are essential for accurately 
documenting patient discussions, diagnoses and management plans. Traditionally, 
this involves a clinician dictating a summary after a consultation, which is 
transcribed by a medical secretary and then reviewed.
OBJECTIVES: This pilot study aimed to compare the accuracy, readability and 
efficiency of dermatology clinic letters generated by the traditional 
clinician-secretary method against artificial intelligence (AI)-based platforms 
[ChatGPT (OpenAI, San Francisco, CA, USA) and Heidi (Heidi Health, Melbourne, 
VIC, Australia)] in a simulated clinical setting.
METHODS: Four scripted dermatological cases were enacted by two doctor-patient 
pairs. Clinic letters were produced using three methods: the traditional 
clinician dictation and secretarial transcription or generated by the AI 
platforms ChatGPT or Heidi. Letters were evaluated by two independent senior 
dermatologists. Evaluation focused on accuracy, relevance, satisfaction, 
readability (using the Flesch Reading Ease score) and time efficiency.
RESULTS: ChatGPT achieved the highest accuracy (average 92.6%) and satisfaction 
ratings (4.59/5) but produced lengthy, complex letters with readability at a 
university level. Heidi demonstrated consistent accuracy (85%) and structured 
outputs, with an average satisfaction rating of 4.34/5, although it occasionally 
omitted nuanced details. The traditional method scored lowest in accuracy 
(58.81%) and satisfaction (3.52/5) but was the easiest to read (12-15-year-old 
readability level). Time efficiency was best for Heidi (27 s per letter), 
followed by ChatGPT (54 s per letter), then traditional methods (10 min 54 s per 
letter).
CONCLUSIONS: AI platforms demonstrated superior accuracy and time efficiency 
compared with the traditional method. While ChatGPT achieved marginally higher 
scores, Heidi's consistent performance and reliable operation make it 
potentially more suitable for clinical implementation. Technical reliability, 
data security and readability challenges require further investigation. Future 
research should validate these findings in real clinical settings and explore 
the implications for healthcare workflow and staffing.

© The Author(s) 2025. Published by Oxford University Press on behalf of British 
Association of Dermatologists. All rights reserved. For commercial re-use, 
please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/ced/llaf323
PMID: 40674470 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of interest: The authors declare no 
conflicts of interest.


673. Respir Res. 2025 Feb 12;26(1):52. doi: 10.1186/s12931-025-03130-y.

The large language model diagnoses tuberculous pleural effusion in pleural 
effusion patients through clinical feature landscapes.

Wu C(#)(1), Liu W(#)(2), Mei P(#)(3), Liu Y(1), Cai J(4), Liu L(1), Wang J(3), 
Ling X(1), Wang M(1), Cheng Y(1), He M(1), He Q(1), He Q(1), Yuan X(5), Tong 
J(6).

Author information:
(1)Department of Respiratory Medicine, Affiliated Hospital of Jiujiang 
University, No. 57 East Xunyang Road, Xunyang District, Jiujiang, 332000, China.
(2)Department of Hematology, The Second Affiliated Hospital of Fujian Medical 
University, Quanzhou, 362000, China.
(3)Department of Gastroenterology, Affiliated Hospital of Jiujiang University, 
Jiujiang, 332000, China.
(4)Department of Cardiology, Affiliated Hospital of Jiujiang University, 
Jiujiang, 332000, China.
(5)Department of Respiratory Medicine, First Affiliated Hospital of Gannan 
Medical University, No. 23, Qingnian Road, Zhanggong District, Ganzhou, 341000, 
China. yxlyyxs@126.com.
(6)Department of Respiratory Medicine, Affiliated Hospital of Jiujiang 
University, No. 57 East Xunyang Road, Xunyang District, Jiujiang, 332000, China. 
tjl8880@163.com.
(#)Contributed equally

BACKGROUND: Tuberculous pleural effusion (TPE) is a challenging extrapulmonary 
manifestation of tuberculosis, with traditional diagnostic methods often 
involving invasive surgery and being time-consuming. While various machine 
learning and statistical models have been proposed for TPE diagnosis, these 
methods are typically limited by complexities in data processing and 
difficulties in feature integration. Therefore, this study aims to develop a 
diagnostic model for TPE using ChatGPT-4, a large language model (LLM), and 
compare its performance with traditional logistic regression and machine 
learning models. By highlighting the advantages of LLMs in handling complex 
clinical data, identifying interrelationships between features, and improving 
diagnostic accuracy, this study seeks to provide a more efficient and precise 
solution for the early diagnosis of TPE.
METHODS: We conducted a cross-sectional study, collecting clinical data from 109 
TPE and 54 non-TPE patients for analysis, selecting 73 features from over 600 
initial variables. The performance of the LLM was compared with logistic 
regression and machine learning models (k-Nearest Neighbors, Random Forest, 
Support Vector Machines) using metrics like area under the curve (AUC), F1 
score, sensitivity, and specificity.
RESULTS: The LLM showed comparable performance to machine learning models, 
outperforming logistic regression in sensitivity, specificity, and overall 
diagnostic accuracy. Key features such as adenosine deaminase (ADA) levels and 
monocyte percentage were effectively integrated into the model. We also 
developed a Python package ( https://pypi.org/project/tpeai/ ) for rapid TPE 
diagnosis based on clinical data.
CONCLUSIONS: The LLM-based model offers a non-surgical, accurate, and 
cost-effective method for early TPE diagnosis. The Python package provides a 
user-friendly tool for clinicians, with potential for broader use. Further 
validation in larger datasets is needed to optimize the model for clinical 
application.

© 2025. The Author(s).

DOI: 10.1186/s12931-025-03130-y
PMCID: PMC11823098
PMID: 39939874 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was approved by the Ethics Committee of the Affiliated 
Hospital of Jiujiang University (Approval No.: jjumer-b-2024-0405) and was 
conducted in accordance with the Declaration of Helsinki. As retrospective data 
were used, written informed consent was waived. This article does not include 
any research involving human participants by the authors. All data were analyzed 
anonymously to ensure the privacy of the participants. Consent for publication: 
Not applicable. Competing interests: The authors declare no competing interests.


674. BMC Med Educ. 2025 Nov 24;25(1):1755. doi: 10.1186/s12909-025-08308-y.

Comparing AI chatbot simulation and peer role-play for OSCE preparation: a pilot 
randomized controlled trial.

Lee HY(#)(1), Kim J(#)(2), Choi H(3), Bae H(4), Jeong A(5), Choi S(6), Kim 
JH(7), Kim CE(8).

Author information:
(1)Division of Humanities and Social Medicine, School of Korean Medicine, Pusan 
National University, Yangsan, Korea.
(2)Department of Physiology, College of Korean Medicine, Gachon University, 
Seongnam, Korea.
(3)Department of Sasang Constitutional Medicine, School of Korean Medicine, 
Pusan National University, Yangsan, Korea.
(4)Department of Physiology, College of Korean Medicine, Dongguk University , 
Gyeongju, Korea.
(5)Department of Pediatrics, College of Korean Medicine, Gachon University, 
Seongnam, Korea.
(6)Department of Neuropsychiatry, College of Korean medicine, Gachon University, 
Seongnam, Korea.
(7)Department of Sasang Constitutional Medicine, School of Korean Medicine, 
Pusan National University, Yangsan, Korea. jani77@pusan.ac.kr.
(8)Department of Physiology, College of Korean Medicine, Gachon University, 
Seongnam, Korea. eopchang@gachon.ac.kr.
(#)Contributed equally

BACKGROUND: Artificial intelligence (AI) is increasingly applied in medical 
education, but its role in fostering interactive clinical competencies remains 
underexplored. This pilot study aimed to compare the feasibility and educational 
impact of an AI chatbot-based simulation with traditional peer role-play (PRP) 
for Objective Structured Clinical Examination (OSCE) preparation, and to share 
practical lessons from implementing a novel AI tool in a trial setting.
METHODS: Nineteen final-year Korean medicine students were randomly assigned to 
either an AI chatbot group (n = 9) or a PRP group (n = 10) after a baseline 
knowledge test. Both groups underwent a 30-minute physical examination practice 
session, followed by a one-hour clinical interview training session specific to 
their group. The AI chatbot group practiced with a GPT-4o/Claude 3.5-based 
chatbot providing scenario-driven responses and automated feedback, while the 
PRP group practiced in pairs under tutor supervision. All participants then 
completed two OSCE stations (dizziness and shoulder pain). Performance was 
assessed using a structured checklist covering four domains: history taking, 
physical examination, patient education, and physician-patient interaction. 
Post-study questionnaires evaluated the learning experience.
RESULTS: Although the differences in OSCE scores between the groups did not 
reach statistical significance, several interesting and complementary trends 
were observed. For example, the PRP group tended to score higher in history 
taking (mean 74.4 vs. 66.2 in dizziness scenario; Hedges' g = -0.68, mean 58.6 
vs. 54.5 in shoulder pain scenario; Hedges' g = -0.21), while the AI chatbot 
group showed a tendency towards higher scores in patient education (32.5 vs. 
22.2 in dizziness scenario Hedges' g = 0.44, 85.0 vs. 66.7 in shoulder pain 
scenario; Hedges' g = 0.99). Survey results reflected these following trends. 
The PRP group valued the authenticity of the interaction and the exam-like 
environment. In contrast, the AI chatbot group reported higher satisfaction with 
the autonomy, opportunity for repetitive practice, and structured feedback.
CONCLUSION: In this pilot study, AI chatbot-based training and PRP demonstrated 
complementary strengths for OSCE preparation. While PRP appears effective for 
developing performance-based procedural and communication skills in a realistic 
setting, AI chatbots show potential for fostering clinical reasoning in a 
self-paced, reflective learning environment. These complementary strengths 
suggest a blended learning model, combining both methods, may be optimal for 
holistic clinical skills development. Further research is needed to validate 
these preliminary findings.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-08308-y
PMCID: PMC12750692
PMID: 41286823 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The study was conducted in accordance with the ethical principles 
of the Declaration of Helsinki, and the protocol was approved by Institutional 
Review Board (IRB) of Gil Korean Medicine Hospital, Gachon University (Approval 
No. GIRB-25-101). All participants were provided with a detailed explanation of 
the study objectives and procedures, and written informed consent to participate 
was obtained from all participants prior to their enrollment in the study. 
Consent for publication: Not applicable. Competing interests: The authors 
declare no competing interests.


675. PLoS One. 2025 Jun 2;20(6):e0324830. doi: 10.1371/journal.pone.0324830. 
eCollection 2025.

Does LLM translation align with translation universals? A cross-genre 
simplification study on English-Chinese translation based on dependency grammar.

Jiang Z(1).

Author information:
(1)School of Foreign Languages, Qingdao University, Qingdao, Shandong, China.

This study examines the presence of simplification, a translation universal 
(TU), in English-to-Chinese translation by comparing the Mean Dependency 
Distance (MDD) and Mean Hierarchical Distance (MHD) of Crowdsourcing human 
translations, Large Language Model (LLM) translations, and original Chinese 
texts across fifteen genres. Through analysis of three balanced comparable 
corpora, the research found that: (i) Compared to original Chinese texts, both 
human-translated and LLM-translated Chinese texts demonstrated significant 
syntactic simplification across all genres. (ii) Human translations exhibited a 
more pronounced tendency toward syntactic simplification than LLM translations 
across all genres. These findings not only validate the simplification 
hypothesis at the syntactic level but also highlight the different cognitive and 
processing mechanisms underlying human and LLM translation processes. The 
research indicates that human translators possess an active ability to optimize 
complex syntax that current LLMs lack, providing valuable reference for future 
development of LLMs and methods for LLM-assisted translation. Additionally, by 
adopting MDD and MHD as holistic measures of syntactic complexity, this study 
offers new perspectives for TU research and provides empirical insights into the 
linguistic nature of crowdsourcing translations from an English-to-Chinese 
perspective.

Copyright: © 2025 Zeyuan Jiang. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0324830
PMCID: PMC12129206
PMID: 40455759 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


676. Digit Health. 2025 Oct 8;11:20552076251384143. doi: 10.1177/20552076251384143. 
eCollection 2025 Jan-Dec.

Improving tuberculosis-related knowledge in tuberculosis patients: Protocol for 
the development and validation of an evidence-based Q&A robot powered by large 
language models.

Zhang L(1), He W(2), Wang X(1), Li X(1), Chang J(3), Xu DR(4), Li G(1).

Author information:
(1)Department of the Third Pulmonary Disease, Shenzhen Third People's Hospital, 
Shenzhen, Guangdong Province, China.
(2)Southern Medical University Institute for Global Health, Dermatology Hospital 
of Southern Medical University, Guangzhou, China.
(3)Department of Health Management, School of Health Management, Southern 
Medical University, Guangzhou, China.
(4)SMU Institute for Global Health (SIGHT) and Center for World Health 
Organization Studies, School of Health Management and Dermatology Hospital of 
Southern Medical University, Guangzhou, China.

BACKGROUND: Inadequate health knowledge of tuberculosis patients is one of the 
causes of poor adherence among tuberculosis patients in China's tuberculosis 
control. In this study, we will develop and validate the effectiveness of a 
large language model (LLM) to improve the health knowledge of tuberculosis 
patients.
METHODS: We will design a LLM application tailored to tuberculosis scenarios and 
evaluate its effectiveness in tuberculosis patient health education through a 
single-center, factorial-design randomized controlled trial. The study will 
feature a factorial design with two factors: LLMs-based health education model 
and a peer-intervention health education model, each with two levels (yes/no). A 
total of 148 tuberculosis (TB) patients in the intensive treatment phase will be 
randomly allocated to four groups through simple randomization. The primary 
outcome will be the patients' level of personal health knowledge about 
tuberculosis, measured through questionnaires administered at discharge and 
three months later.
CONCLUSION: We are the first study in China to apply LLMs to tuberculosis health 
education. Tailored specifically for TB, our model uses certified guidelines and 
expert consensus to minimize inaccuracies. Large language models provide access 
to personalized, private health information, and reducing stigma. Instead of 
creating a new platform, we use the popular WeChat platform to deliver education 
via videos, text, and images, enhancing accessibility and engagement. This 
innovative approach aims to improve patient adherence and contribute to better 
TB management and disease control outcomes.

© The Author(s) 2025.

DOI: 10.1177/20552076251384143
PMCID: PMC12511700
PMID: 41078608

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


677. J Med Internet Res. 2025 Aug 25;27:e65523. doi: 10.2196/65523.

Nurse Researchers' Experiences and Perceptions of Generative AI: Qualitative 
Semistructured Interview Study.

Kang R(#)(1), Xuan Z(#)(1), Tong L(1), Wang Y(1), Jin S(1), Xiao Q(1).

Author information:
(1)School of Nursing, Capital Medical University, No.10 Xi-tou-tiao, You-an-men 
Wai, Feng-tai District, Beijing, 100069, China.
(#)Contributed equally

BACKGROUND: With the rapid development and iteration of generative artificial 
intelligence, the growing popularity of such groundbreaking tools among nurse 
researchers, represented by ChatGPT (OpenAI), is receiving passionate debate and 
intrigue. Although there has been qualitative research on generative artificial 
intelligence in other fields, little is known about the experiences and 
perceptions of nurse researchers; this study seeks to report on the topic.
OBJECTIVE: This study aimed to describe the experiences and perceptions of 
generative artificial intelligence among Chinese nurse researchers, as well as 
provide a reference for the application of generative artificial intelligence in 
nursing research in the future.
METHODS: Semistructured interviews were used to collect data in this qualitative 
study. Researchers mainly conducted interviews on the cognition, experience, and 
future expectations of nurse researchers regarding the use of generative 
artificial intelligence. Twenty-seven nurse researchers were included in the 
study. Through purposive sampling and snowball sampling, there were 7 nursing 
faculty researchers, 10 nursing graduate students, and 10 clinical nurse 
researchers. Data were analyzed using inductive content analysis.
RESULTS: Five themes and 12 subthemes were categorized from 27 original 
interview documents as follows: (1) diverse reflections on human-machine 
symbiosis, which includes the interplay between substitution and assistance, 
researchers shaping the potential of generative artificial intelligence, and 
acceptance of generative artificial intelligence with alacrity; (2) multiple 
factors of the usage experience, including individual characteristics and 
various usage scenarios; (3) research paradigm reshaping in the infancy stage, 
which involves full-process groundbreaking assistive tools and emergence of new 
research paths; (4) application risks of generative artificial intelligence, 
including intrinsic limitations of generative artificial intelligence and 
academic integrity and medical ethics; and (5) the co-improvement of technology 
and literacy, which concerns reinforcement needs for generative artificial 
intelligence literacy, development of nursing research generative artificial 
intelligence and urgent need for artificial intelligence-generated content 
detection tools. In this context, the first 4 themes form the rocket of the 
human-machine symbiosis journey. Only when humans fully leverage the advantages 
of machines (generative artificial intelligence) and overcome their shortcomings 
can this human-machine symbiosis journey reach the correct future direction 
(fifth theme).
CONCLUSIONS: This study explored the experiences and perceptions of nurse 
researchers interacting with generative artificial intelligence, which was a 
"symbiotic journey" full of twists and turns, and provides a reference and basis 
for achieving harmonious coexistence between nurse researchers and generative 
artificial intelligence in the future. Nurse researchers, policy makers, and 
application developers can use the conclusions of this study to further promote 
the application of generative artificial intelligence in nursing research, 
policy making, and product development.

© Ruifu Kang, Zehui Xuan, Ling Tong, Yanling Wang, Shuai Jin, Qian Xiao. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/65523
PMCID: PMC12377238
PMID: 40853413 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


678. J Med Internet Res. 2025 Dec 4;27:e78186. doi: 10.2196/78186.

The Efficacy of Rule-Based Versus Large Language Model-Based Chatbots in 
Alleviating Symptoms of Depression and Anxiety: Systematic Review and 
Meta-Analysis.

Du Q(1), Ren Y(#)(1), Meng ZL(#)(2), He H(1), Meng S(1).

Author information:
(1)Beijing Yuxin Technology Co., Ltd, Room 2-5, 13th Floor, Building 2, No. 48, 
Zhichun Road, Haidian District, Beijing, 100086, China, 86 010-81377053.
(2)Department of Psychology, School of Humanities and Social Sciences, Beijing 
Forestry University, Beijing, China.
(#)Contributed equally

BACKGROUND: The global mental health crisis is becoming increasingly severe. Due 
to the shortage of mental health professionals, high treatment costs, and 
insufficient accessibility of services, there is an urgent need for scalable and 
low-cost intervention methods. In recent years, chatbots have shown potential 
for psychological interventions. The efficacy differences between large language 
model (LLM)-based and rule-based chatbots have not been systematically 
evaluated, with few studies directly comparing the two; existing meta-analyses 
have notable limitations: there is high heterogeneity in intervention design 
(eg, dialogue structure, interaction frequency, and duration) across studies, 
and there is a lack of direct comparison of differentiated intervention effects 
on depressive and anxiety symptoms, making it difficult to integrate 
conclusions.
OBJECTIVE: By integrating studies from the past five years, this research 
evaluates the differences in effectiveness between LLM-based and rule-based 
chatbots in alleviating depressive and anxiety symptoms. It also analyzes the 
impacts of control group type, intervention duration, and age on intervention 
outcomes. By analyzing chatbot functionality, the study aims to provide 
evidence-based technological pathway options and optimization recommendations 
for differentiated interventions for depression and anxiety.
METHODS: A systematic search of 7 databases included 15 studies published 
between 2020 and 2025. Robust variance estimation (RVE) was used to account for 
non-independent effect sizes, and standardized mean differences (SMDs) were 
calculated using Hedges g. Based on the expectation of clinical and 
methodological heterogeneity among studies, a random-effects model was 
preselected, and the pooled effect size was estimated using restricted maximum 
likelihood estimation (REML) and interpreted according to Cohen criteria. 
Publication bias was assessed using the RVE-adjusted Egger test, funnel plot 
asymmetry, and a fail-safe N.
RESULTS: For depression, rule-based intervention achieved a small but 
significant effect (g=0.266; 95% CI 0.020-0.512; P=.04), while LLM-based 
intervention showed a nonsignificant effect with wide confidence intervals 
(g=0.407; 95% CI -0.734 to 1.550; P=.17). For anxiety, rule-based intervention 
did not yield a significant effect (g=0.147; 95% CI -0.073 to 0.367; P=.15). 
Similarly, LLM-based intervention showed a higher point estimate but also with 
nonsignificance and wide confidence intervals (g=0.711; 95% CI -0.334 to 1.760; 
P=.13). Subgroup analysis showed that the rule-based chatbot was more effective 
than the blank control for depression, with the greatest effect in the medium 
term (4-8 weeks).
CONCLUSIONS: Rule-based chatbots have a modest effect on improving depressive 
symptoms and are suitable for environments with limited psychological resources; 
4-8 weeks may be a critical intervention window. Intervention duration and 
participant age did not significantly influence intervention effectiveness. 
Limited by the sample size, robust evidence supporting the effectiveness of 
LLM-based chatbot interventions is lacking, and further sample size expansion is 
warranted.

© Qiuxue Du, Yongliang Ren, Ze-long Meng, Han He, Shasha Meng. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/78186
PMCID: PMC12677872
PMID: 41343858 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


679. J Med Internet Res. 2025 Jul 22;27:e73226. doi: 10.2196/73226.

Application of Large Language Models in Stroke Rehabilitation Health Education: 
2-Phase Study.

Qiang S(#)(1), Zhang H(#)(2), Liao Y(#)(3), Zhang Y(4), Gu Y(5), Wang Y(3), Xu 
Z(6), Shi H(7), Han N(8), Yu H(9).

Author information:
(1)Shanghai East Hospital, School of Medicine, Tongji University, Shanghai, 
China.
(2)Department of Emergency and Critical Care, Shanghai East Hospital, School of 
Medicine, Tongji University, Shanghai, China.
(3)Neurological Rehabilitation Center, Shanghai Sunshine Rehabilitation Center, 
School of Medicine, Tongji University, Shanghai, China.
(4)Department of Neurology, Shanghai East Hospital, School of Medicine, Tongji 
University, Shanghai, China.
(5)Department of Gastrointestinal Endoscopy, Shanghai East Hospital, School of 
Medicine, Tongji University, Shanghai, China.
(6)Department of Breast Diseases, Yueyang Hospital of Integrated Traditional 
Chinese and Western Medicine, Shanghai University of Traditional Chinese 
Medicine, Shanghai, China.
(7)Department of Nursing, Zhongshan Hospital, Fudan University, Shanghai, China.
(8)School of Acupuncture-Moxibustion and Tuina, Shanghai University of 
Traditional Chinese Medicine, Shanghai, China.
(9)Department of Nursing, Shanghai East Hospital, School of Medicine, Tongji 
University, No. 1800 Yuntai Road, Shanghai, 200120, China, 86 18964538997
(#)Contributed equally

Erratum in
    J Med Internet Res. 2025 Oct 14;27:e84717. doi: 10.2196/84717.

BACKGROUND: Stroke is a leading cause of disability and death worldwide, with 
home-based rehabilitation playing a crucial role in improving patient prognosis 
and quality of life. Traditional health education often lacks precision, 
personalization, and accessibility. In contrast, large language models (LLMs) 
are gaining attention for their potential in medical health education, owing to 
their advanced natural language processing capabilities. However, the 
effectiveness of LLMs in home-based stroke rehabilitation remains uncertain.
OBJECTIVE: This study evaluates the effectiveness of 4 LLMs-ChatGPT-4, MedGo, 
Qwen, and ERNIE Bot-selected for their diversity in model type, clinical 
relevance, and accessibility at the time of study design in home-based stroke 
rehabilitation. The aim is to offer patients with stroke more precise and secure 
health education pathways while exploring the feasibility of using LLMs to guide 
health education.
METHODS: In the first phase of this study, a literature review and expert 
interviews identified 15 common questions and 2 clinical cases relevant to 
patients with stroke in home-based rehabilitation. These were input into 4 LLMs 
for simulated consultations. Six medical experts (2 clinicians, 2 nursing 
specialists, and 2 rehabilitation therapists) evaluated the LLM-generated 
responses using a Likert 5-point scale, assessing accuracy, completeness, 
readability, safety, and humanity. In the second phase, the top 2 performing 
models from phase 1 were selected. Thirty patients with stroke undergoing 
home-based rehabilitation were recruited. Each patient asked both models 3 
questions, rated the responses using a satisfaction scale, and assessed 
readability, text length, and recommended reading age using a Chinese 
readability analysis tool. Data were analyzed using one-way ANOVA, post hoc 
Tukey Honestly Significant Difference tests, and paired t tests.
RESULTS: The results revealed significant differences across the 4 models in 5 
dimensions: accuracy (P=.002), completeness (P<.001), readability (P=.04), 
safety (P=.007), and humanity (P<.001). ChatGPT-4 outperformed all models in 
each dimension, with scores for accuracy (mean 4.28, SD 0.84), completeness 
(mean 4.35, SD 0.75), readability (mean 4.28, SD 0.85), safety (mean 4.38, 
SD0.81), and user-friendliness (mean 4.65, SD 0.66). MedGo excelled in accuracy 
(mean 4.06, SD 0.78) and completeness (mean 4.06, SD 0.74). Qwen and ERNIE Bot 
scored significantly lower across all 5 dimensions than ChatGPT-4 and MedGo. 
ChatGPT-4 generated the longest responses (mean 1338.35, SD 236.03) and had the 
highest readability score (mean 12.88). In the second phase, ChatGPT-4 performed 
the best overall, while MedGo provided the clearest responses.
CONCLUSIONS: LLMs, particularly ChatGPT-4 and MedGo, demonstrated promising 
performance in home-based stroke rehabilitation education. However, 
discrepancies between expert and patient evaluations highlight the need for 
improved alignment with patient comprehension and expectations. Enhancing 
clinical accuracy, readability, and oversight mechanisms will be essential for 
future real-world integration.

© Shiqi Qiang, Haitao Zhang, Yang Liao, Yue Zhang, Yanfen Gu, Yiyan Wang, Zehui 
Xu, Hui Shi, Nuo Han, Haiping Yu. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org).

DOI: 10.2196/73226
PMCID: PMC12306586
PMID: 40694436 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


680. Int J Med Inform. 2026 Mar 15;208:106231. doi: 10.1016/j.ijmedinf.2025.106231. 
Epub 2025 Dec 22.

Advancing healthcare with large language models: A scoping review of 
applications and future directions.

Zhang Z(1), Momeni Nezhad MJ(2), Bagher Hosseini SM(2), Zolnour A(2), Zonour 
Z(2), Hosseini SM(2), Topaz M(3), Zolnoori M(4).

Author information:
(1)Data Science Institute, Columbia University, New York, NY 10027, United 
States; School of Nursing, Columbia University, New York, NY 10032, United 
States. Electronic address: zz3238@columbia.edu.
(2)Columbia University Irving Medical Center, New York, NY 10032, United States.
(3)Data Science Institute, Columbia University, New York, NY 10027, United 
States; School of Nursing, Columbia University, New York, NY 10032, United 
States.
(4)Data Science Institute, Columbia University, New York, NY 10027, United 
States; Columbia University Irving Medical Center, New York, NY 10032, United 
States.

BACKGROUND: The release of ChatGPT has spurred the widespread adoption of 
generative large language models (LLMs) in healthcare. This scoping review 
systematically examines their use in healthcare.
METHODS: A systematic search was conducted using PubMed, a comprehensive and 
representative database on biomedical and health science, to identify studies 
published between January 1, 2023, and July 30, 2024. Studies were included if 
they assessed the performance of generative LLMs in healthcare applications; 
review or perspective articles were excluded.
RESULTS: A total of 415 studies were included, with a significant increase in 
publications observed after April 2023. Generative LLMs were applied across 
various medical specialties, primarily supporting clinical decision-making 
(26.7%) and providing patient information (23.9%). Smaller proportions were 
focused on professional education and training (18.1%), research (16.1%), and 
workflow support (12.5%). These applications were mainly supported by three key 
NLP tasks: question answering (36.1%), text classification (27.5%), and text 
generation (26.3%). Public datasets appeared in 20% of studies, and 15% used 
clinical patient data. Of the 98 LLMs used, GPT-4 (51.3%), GPT-3.5 (36.6%), and 
ChatGPT (22.4%) were the most common. Direct prompting was the most common 
adaptation method (92.5%), with reinforcement learning rarely utilized (1.4%). 
Accuracy was the most frequently assessed metric, while errors and safety (9.4%) 
and time efficiency (7.0%) were less commonly evaluated.
CONCLUSION: LLMs hold promise across healthcare applications. Expanding their 
use in workflow optimization, trainee education, and research tools could 
enhance healthcare delivery and innovation. Comprehensive evaluation using 
standardized criteria is essential for LLMs integration into healthcare.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106231
PMID: 41443123 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


681. J Med Internet Res. 2025 Jul 14;27:e64452. doi: 10.2196/64452.

Performance of Large Language Models in Numerical Versus Semantic Medical 
Knowledge: Cross-Sectional Benchmarking Study on Evidence-Based Questions and 
Answers.

Avnat E(1)(2), Levy M(3)(4), Herstain D(1), Yanko E(5), Ben Joya D(2)(6), 
Tzuchman Katz M(2), Eshel D(2), Laros S(1)(2), Dagan Y(1)(2), Barami S(1)(2), 
Mermelstein J(2), Ovadia S(2), Shomron N(1), Shalev V(1), Abdulnour RE(7).

Author information:
(1)Faculty of Medicine, Tel Aviv University, Chaim Levanon St 55, Tel Aviv, 
6997801, Israel, 972 545299622.
(2)Kahun Medical Ltd, Givatayim, Israel.
(3)Faculty of Medicine, Hebrew University of Jerusalem, Jerusalem, Israel.
(4)School of Computer Science and Engineering, The Hebrew University of 
Jerusalem, Jerusalem, Israel.
(5)The Azrieli Faculty of Medicine, Bar-Ilan University, Safed, Israel.
(6)Kaplan Medical Center, Rehovot, Israel.
(7)Division of Pulmonary and Critical Care Medicine, Department of Medicine, 
Brigham and Women's Hospital, Harvard Medical School, Boston, MA, United States.

BACKGROUND: Clinical problem-solving requires processing of semantic medical 
knowledge, such as illness scripts, and numerical medical knowledge of 
diagnostic tests for evidence-based decision-making. As large language models 
(LLMs) show promising results in many aspects of language-based clinical 
practice, their ability to generate nonlanguage evidence-based answers to 
clinical questions is inherently limited by tokenization.
OBJECTIVE: This study aimed to evaluate LLMs' performance on two question types: 
numeric (correlating findings) and semantic (differentiating entities), while 
examining differences within and between LLMs in medical aspects and comparing 
their performance to humans.
METHODS: To generate straightforward multichoice questions and answers (Q and 
As) based on evidence-based medicine (EBM), we used a comprehensive medical 
knowledge graph (containing data from more than 50,000 peer-reviewed studies) 
and created the EBM questions and answers (EBMQAs). EBMQA comprises 105,222 Q 
and As, categorized by medical topics (eg, medical disciplines) and nonmedical 
topics (eg, question length), and classified into numerical or semantic types. 
We benchmarked a dataset of 24,000 Q and As on two state-of-the-art LLMs, GPT-4 
(OpenAI) and Claude 3 Opus (Anthropic). We evaluated the LLM's accuracy on 
semantic and numerical question types and according to sublabeled topics. In 
addition, we examined the question-answering rate of LLMs by enabling them to 
choose to abstain from responding to questions. For validation, we compared the 
results for 100 unrelated numerical EBMQA questions between six human medical 
experts and the two language models.
RESULTS: In an analysis of 24,542 Q and As, Claude 3 and GPT-4 performed better 
on semantic Q and As (68.7%, n=1593 and 68.4%, n=1709), respectively. Then on 
numerical Q and As (61.3%, n=8583 and 56.7%, n=12,038), respectively, with 
Claude 3 outperforming GPT-4 in numeric accuracy (P<.001). A median accuracy gap 
of 7% (IQR 5%-10%) was observed between the best and worst sublabels per topic, 
with different LLMs excelling in different sublabels. Focusing on Medical 
Discipline sublabels, Claude 3 performed well in neoplastic disorders but 
struggled with genitourinary disorders (69%, n=676 vs 58%, n=464; P<.0001), 
while GPT-4 excelled in cardiovascular disorders but struggled with neoplastic 
disorders (60%, n=1076 vs 53%, n=704; P=.0002). Furthermore, humans (82.3%, 
n=82.3) surpassed both Claude 3 (64.3%, n=64.3; P<.001) and GPT-4 (55.8%, 
n=55.8; P<.001) in the validation test. Spearman correlation between 
question-answering and accuracy rate in both Claude 3 and GPT-4 was 
insignificant (ρ=0.12, P=.69; ρ=0.43, P=.13).
CONCLUSIONS: Both LLMs excelled more in semantic than numerical Q and As, with 
Claude 3 surpassing GPT-4 in numerical Q and As. However, both LLMs showed 
inter- and intramodel gaps in different medical aspects and remained inferior to 
humans. In addition, their ability to respond or abstain from answering a 
question does not reliably predict how accurately they perform when they do 
attempt to answer questions. Thus, their medical advice should be addressed 
carefully.

© Eden Avnat, Michal Levy, Daniel Herstain, Elia Yanko, Daniel Ben Joya, Michal 
Tzuchman Katz, Dafna Eshel, Sahar Laros, Yael Dagan, Shahar Barami, Joseph 
Mermelstein, Shahar Ovadia, Noam Shomron, Varda Shalev, Raja-Elie E Abdulnour. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/64452
PMCID: PMC12279315
PMID: 40658983 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: The authors EA, ML, DH, 
DBJ, MTK, DE, SL, YD, SB, JM, and SO are paid employees by Kahun Ltd. All other 
authors declare no financial or non-financial competing interests.


682. J Med Internet Res. 2025 Feb 11;27:e56737. doi: 10.2196/56737.

Smart Pharmaceutical Monitoring System With Personalized Medication Schedules 
and Self-Management Programs for Patients With Diabetes: Development and 
Evaluation Study.

Xiao J(#)(1)(2), Li M(#)(1)(3), Cai R(1)(3), Huang H(1)(4), Yu H(1)(2), Huang 
L(1)(2), Li J(1)(2), Yu T(1)(3), Zhang J(5), Cheng S(1)(2).

Author information:
(1)Department of Pharmacy, Xiangya Hospital, Central South University, Changsha, 
China.
(2)The Hunan Institute of Pharmacy Practice and Clinical Research, Changsha, 
China.
(3)Department of Pharmacy, Dali University, Dali, China.
(4)School of Medicine, Wuhan University of Science and Technology, Wuhan, China.
(5)Department of Senile Endocrinology, Xiangya Hospital, Central South 
University, Changsha, China.
(#)Contributed equally

BACKGROUND: With the climbing incidence of type 2 diabetes, the health care 
system is under pressure to manage patients with this condition properly. 
Particularly, pharmacological therapy constitutes the most fundamental means of 
controlling blood glucose levels and preventing the progression of 
complications. However, its effectiveness is often hindered by factors such as 
treatment complexity, polypharmacy, and poor patient adherence. As new 
technologies, artificial intelligence and digital technologies are covering all 
aspects of the medical and health care field, but their application and 
evaluation in the domain of diabetes research remain limited.
OBJECTIVE: This study aims to develop and establish a stand-alone diabetes 
management service system designed to enhance self-management support for 
patients, as well as to assess its performance with experienced health care 
professionals.
METHODS: Diabetes Universal Medication Schedule (DUMS) system is grounded in 
official medicine instructions and evidence-based data to establish medication 
constraints and drug-drug interaction profiles. Individualized medication 
schedules and self-management programs were generated based on patient-specific 
conditions and needs, using an app framework to build patient-side contact 
pathways. The system's ability to provide medication guidance and health 
management was assessed by senior health care professionals using a 5-point 
Likert scale across 3 groups: outputs generated by the system (DUMS group), 
outputs refined by pharmacists (intervention group), and outputs generated by 
ChatGPT-4 (GPT-4 group).
RESULTS: We constructed a cloud-based drug information management system loaded 
with 475 diabetes treatment-related medications; 684 medication constraints; and 
12,351 drug-drug interactions and theoretical supports. The generated 
personalized medication plan and self-management program included recommended 
dosing times, disease education, dietary considerations, and lifestyle 
recommendations to help patients with diabetes achieve correct medication use 
and active disease management. Reliability analysis demonstrated that the DUMS 
group outperformed the GPT-4 group in medication schedule accuracy and safety, 
as well as comprehensiveness and richness of the self-management program 
(P<.001). The intervention group outperformed the DUMS and GPT-4 groups on all 
indicator scores.
CONCLUSIONS: DUMS's treatment monitoring service can provide reliable 
self-management support for patients with diabetes. ChatGPT-4, powered by 
artificial intelligence, can act as a collaborative assistant to health care 
professionals in clinical contexts, although its performance still requires 
further training and optimization.

©Jian Xiao, Mengyao Li, Ruwen Cai, Hangxing Huang, Huimin Yu, Ling Huang, 
Jingyang Li, Ting Yu, Jiani Zhang, Shuqiao Cheng. Originally published in the 
Journal of Medical Internet Research (https://www.jmir.org), 11.02.2025.

DOI: 10.2196/56737
PMCID: PMC11862767
PMID: 39933171 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


683. J Med Internet Res. 2025 Sep 30;27:e74177. doi: 10.2196/74177.

Large Language Models in Lung Cancer: Systematic Review.

Zhong R(1), Chen S(1), Li Z(1), Gao T(1), Su Y(1), Zhang W(1), Liu D(2), Gao 
L(#)(2), Hu K(#)(2).

Author information:
(1)Graduate School, Beijing University of Chinese Medicine, Beijing, China.
(2)Oncology Department, Dongfang Hospital, Beijing University of Chinese 
Medicine, No. 6, Fangxingyuan 1st District, Fengtai District, Beijing, China, 86 
13911650713.
(#)Contributed equally

BACKGROUND: In the era of data and intelligence, artificial intelligence has 
been widely applied in the medical field. As the most cutting-edge technology, 
the large language model (LLM) has gained popularity due to its extraordinary 
ability to handle complex tasks and interactive features.
OBJECTIVE: This study aimed to systematically review current applications of 
LLMs in lung cancer (LC) care and evaluate their potential across the full-cycle 
management spectrum.
METHODS: Following PRISMA (Preferred Reporting Items for Systematic Reviews and 
Meta-Analyses) guidelines, we conducted a comprehensive literature search across 
6 databases up to January 1, 2025. Studies were included if they satisfied the 
following criteria: (1) journal articles, conference papers, and preprints; (2) 
studies that reported the content of LLMs in LC; (3) including original data and 
LC-related data presented separately; and (4) studies published in English. The 
exclusion criteria were as follows: (1) books and book chapters, letters, 
reviews, conference proceedings; (2) studies that did not report the content of 
LLMs in LC; and (3) no original data, and LC-related data that are not presented 
separately. Studies were screened independently by 2 authors (SC and ZL) and 
assessed for quality using Quality Assessment of Diagnostic Accuracy Studies-2, 
Prediction Model Risk of Bias Assessment Tool, and Risk Of Bias in 
Non-randomized Studies - of Interventions tools, selected based on study type. 
Key data items extracted included model type, application scenario, prompt 
method, input and output format, outcome measures, and safety considerations. 
Data analysis was conducted using descriptive statistics.
RESULTS: Out of 706 studies screened, 28 were included (published between 2023 
and 2024). The ability of LLMs to automatically extract medical records, 
popularize general knowledge about LC, and assist clinical diagnosis and 
treatment has been demonstrated through the systematic review, emerging visual 
ability, and multimodal potential. Prompt engineering was a critical component, 
with varying degrees of sophistication from zero-shot to fine-tuned approaches. 
Quality assessments revealed overall acceptable methodological rigor but noted 
limitations in bias control and data security reporting.
CONCLUSIONS: LLMs show considerable potential in improving LC diagnosis, 
communication, and decision-making. However, their responsible use requires 
attention to privacy, interpretability, and human oversight.

©Ruikang Zhong, Siyi Chen, Zexing Li, Tangke Gao, Yisha Su, Wenzheng Zhang, 
Dianna Liu, Lei Gao, Kaiwen Hu. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org).

DOI: 10.2196/74177
PMCID: PMC12483341
PMID: 41026980 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


684. Bioethics. 2026 Jan;40(1):61-72. doi: 10.1111/bioe.70032. Epub 2025 Sep 18.

When Is It Safe to Introduce an AI System Into Healthcare? A Practical Decision 
Algorithm for the Ethical Implementation of Black-Box AI in Medicine.

Allen JW(1)(2), Wilkinson D(1)(2)(3)(4)(5), Savulescu J(2)(4)(5).

Author information:
(1)Department of Paediatrics, Faculty of Medicine, Nursing and Health Sciences, 
Monash University, Clayton, Victoria, Australia.
(2)Uehiro Oxford Insitute, University of Oxford, Oxford, UK.
(3)Newborn Care, Oxford University Hospitals NHS Foundation Trust, Oxford, UK.
(4)Centre for Biomedical Ethics, Yong Loo Lin School of Medicine, National 
University of Singapore, Singapore.
(5)Murdoch Children's Research Institute, Melbourne, Victoria, Australia.

There is mounting global interest in the revolutionary potential of AI tools. 
However, its use in healthcare carries certain risks. Some argue that opaque 
('black box') AI systems in particular undermine patients' informed consent. 
While interpretable models offer an alternative, this approach may be impossible 
with generative AI and large language models (LLMs). Thus, we propose that AI 
tools should be evaluated for clinical use based on their implementation risk, 
rather than interpretability. We introduce a practical decision algorithm for 
the clinical implementation of black-box AI by evaluating its risk of 
implementation. Applied to the case of an LLM for surgical informed consent, we 
assess a system's implementation risk by evaluating: (1) technical robustness, 
(2) implementation feasibility and (3) analysis of harms and benefits. 
Accordingly, the system is categorised as minimal-risk (standard use), 
moderate-risk (innovative use) or high-risk (experimental use). Recommendations 
for implementation are proportional to risk, requiring more oversight for 
higher-risk categories. The algorithm also considers the system's 
cost-effectiveness and patients' informed consent.

© 2025 The Author(s). Bioethics published by John Wiley & Sons Ltd.

DOI: 10.1111/bioe.70032
PMCID: PMC12710678
PMID: 40964933 [Indexed for MEDLINE]


685. J Biomed Inform. 2025 Sep;169:104875. doi: 10.1016/j.jbi.2025.104875. Epub 2025 
Aug 5.

Rapid review: Growing usage of Multimodal Large Language Models in healthcare.

Gupta P(1), Zhang Z(2), Song M(3), Michalowski M(3), Hu X(4), Stiglic G(5), 
Topaz M(6).

Author information:
(1)Columbia University, School of Nursing, NY, United States. Electronic 
address: pg2834@cumc.columbia.edu.
(2)Columbia University, School of Nursing, NY, United States; Columbia 
University, Data Science Institute, NY, United States.
(3)University of Minnesota, School of Nursing, Minneapolis, United States.
(4)Center for Data Science, Emory University, Atlanta, United States; Nell 
Hodgson Woodruff School of Nursing, Emory University, Atlanta, United States; 
Department of Biomedical Informatics, School of Medicine, Emory University, 
Atlanta, United States; Department of Computer Science, College of Arts and 
Sciences, Emory University, Atlanta, United States; Wallace H. Coulter 
Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta, 
United States.
(5)Faculty of Health Sciences, University of Maribor, Maribor, Slovenia; Faculty 
of Electrical Engineering and Computer Science, University of Maribor, Maribor, 
Slovenia; Usher Institute, University of Edinburgh, Edinburgh, United Kingdom.
(6)Columbia University, School of Nursing, NY, United States; Columbia 
University, Data Science Institute, NY, United States; VNS Health, NY, United 
States. Electronic address: mt3315@cumc.columbia.edu.

OBJECTIVE: Recent advancements in large language models (LLMs) have led to 
multimodal LLMs (MLLMs), which integrate multiple data modalities beyond text. 
Although MLLMs show promise, there is a gap in the literature that empirically 
demonstrates their impact in healthcare. This paper summarizes the applications 
of MLLMs in healthcare, highlighting their potential to transform health 
practices.
METHODS: A rapid literature review was conducted in August 2024 using World 
Health Organization (WHO) rapid-review methodology and PRISMA standards, with 
searches across four databases (Scopus, Medline, PubMed and ACM Digital Library) 
and top-tier conferences-including NeurIPS, ICML, AAAI, MICCAI, CVPR, ACL and 
EMNLP. Articles on MLLMs healthcare applications were included for analysis 
based on inclusion and exclusion criteria.
RESULTS: The search yielded 115 articles, 39 included in the final analysis. Of 
these, 77% appeared online (preprints and published) in 2024, reflecting the 
emergence of MLLMs. 80% of studies were from Asia and North America (mainly 
China and US), with Europe lagging. Studies split evenly between pre-built MLLMs 
evaluations (60% focused on GPT versions) and custom MLLMs/frameworks 
development with task-specific customizations. About 81% of studies examined 
MLLMs for diagnosis and reporting in radiology, pathology, and ophthalmology, 
with additional applications in education, surgery, and mental health. Prompting 
strategies, used in 80% of studies, improved performance in nearly half. 
However, evaluation practices were inconsistent with 67% reported accuracy. 
Error analysis was mostly anecdotal, with only 18% categorized failure types. 
Only 13% validated explainability through clinician feedback. Clinical 
deployment was demonstrated in just 3% of studies, and workflow integration, 
governance, and safety were rarely addressed.
DISCUSSION AND CONCLUSION: MLLMs offer substantial potential for healthcare 
transformation through multimodal data integration. Yet, methodological 
inconsistencies, limited validation, and underdeveloped deployment strategies 
highlight the need for standardized evaluation metrics, structured error 
analysis, and human-centered design to support safe, scalable, and trustworthy 
clinical adoption.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104875
PMID: 40754135 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


686. J Clin Epidemiol. 2025 Apr;180:111672. doi: 10.1016/j.jclinepi.2025.111672. Epub 
2025 Jan 17.

Using artificial intelligence to semi-automate trustworthiness assessment of 
randomized controlled trials: a case study.

Au LS(1), Qu L(2), Nielsen J(1), Ge Z(3), Gurrin LC(4), Mol BW(5), Wang R(6).

Author information:
(1)Department of Obstetrics and Gynaecology, Monash University, Melbourne, 
Victoria, Australia.
(2)Department of Data Science & AI, Monash University, Clayton, Victoria, 
Australia.
(3)AIM for Health Lab, Faculty of IT, Monash University, Clayton, Victoria, 
Australia; Monash-Airdoc Research Lab, Faculty of IT, Monash University, 
Clayton, Victoria, Australia.
(4)Centre for Epidemiology and Biostatistics, School of Population and Global 
Health, The University of Melbourne, Parkville, Australia. Electronic address: 
lgurrin@unimelb.edu.au.
(5)Department of Obstetrics and Gynaecology, Monash University, Melbourne, 
Victoria, Australia; Womens and Children's Program Monash Health, Clayton, 
Melbourne, Victoria, Australia; University Medical Centre Amsterdam, Amsterdam, 
The Netherlands.
(6)Department of Obstetrics and Gynaecology, Monash University, Melbourne, 
Victoria, Australia; NHMRC Clinical Trials Centre, University of Sydney, 
Camperdown, New South Wales, Australia.

BACKGROUND AND OBJECTIVE: Randomized controlled trials (RCTs) are the 
cornerstone of evidence-based medicine. Unfortunately, not all RCTs are based on 
real data. This serious breach of research integrity compromises the reliability 
of systematic reviews and meta-analyses, leading to misinformed clinical 
guidelines and posing a risk to both individual and public health. While methods 
to detect problematic RCTs have been proposed, they are time-consuming and 
labor-intensive. The use of artificial intelligence large language models (LLMs) 
has the potential to accelerate the data collection needed to assess the 
trustworthiness of published RCTs.
METHODS: We present a case study using ChatGPT powered by OpenAI's GPT-4o to 
assess an RCT paper. The case study focuses on applying the trustworthiness in 
randomised controlled trials (TRACT checklist) and automating data table 
extraction to accelerate statistical analysis targeting the trustworthiness of 
the data. We provide a detailed step-by-step outline of the process, along with 
considerations for potential improvements.
RESULTS: ChatGPT completed all tasks by processing the PDF of the selected 
publication and responding to specific prompts. ChatGPT addressed items in the 
TRACT checklist effectively, demonstrating an ability to provide precise "yes" 
or "no" answers while quickly synthesizing information from both the paper and 
relevant online resources. A comparison of results generated by ChatGPT and the 
human assessor showed an 84% level of agreement of (16/19) TRACT items. This 
substantially accelerated the qualitative assessment process. Additionally, 
ChatGPT was able to extract efficiently the data tables as Microsoft Excel 
worksheets and reorganize the data, with three out of four extracted tables 
achieving an accuracy score of 100%, facilitating subsequent analysis and data 
verification.
CONCLUSION: ChatGPT demonstrates potential in semiautomating the trustworthiness 
assessment of RCTs, though in our experience this required repeated prompting 
from the user. Further testing and refinement will involve applying ChatGPT to 
collections of RCT papers to improve the accuracy of data capture and lessen the 
role of the user. The ultimate aim is a completely automated process for large 
volumes of papers that seems plausible given our initial experience.

Copyright © 2025 The Author(s). Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jclinepi.2025.111672
PMID: 39828224 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest B.W.M. is 
supported by an NHMRC Investigator grant (GNT1176437); reports consultancy, 
travel support, and research funding from Merck and Guerbet and consultancy for 
Organon and Norgine; and holds stocks from ObsEva. There are no competing 
interests for any other author.


687. J Biomed Inform. 2025 Aug;168:104865. doi: 10.1016/j.jbi.2025.104865. Epub 2025 
Jun 19.

Ontology enrichment using a large language model: Applying lexical, semantic, 
and knowledge network-based similarity for concept placement.

Kollapally NM(1), Geller J(2), Keloth VK(3), He Z(4), Xu J(5).

Author information:
(1)Kean University, United States. Electronic address: nmartink@kean.edu.
(2)New Jersey Institute of Technology, United States.
(3)Yale University, United States.
(4)Florida State University, United States.
(5)JX Consulting, United States.

OBJECTIVE: Ontologies are essential for representing the knowledge of a domain. 
To make ontologies useful, they must encompass a comprehensive domain view. To 
achieve ontology enrichment, there is a need to discover new concepts to be 
added, either because they were missed in the first place, or the 
state-of-the-art has advanced to develop new real-world concepts. Our goal is to 
develop an automatic enrichment pipeline using a seed ontology, a Large Language 
Model (LLM), and source of text. The pipeline is applied to the domain of Social 
Determinants of Health (SDoH), using PubMed as a source of concepts. In this 
work, the applicability and effectiveness of the enrichment pipeline is 
demonstrated by extending the SDoH Ontology called SOHOv1, however our 
methodology could be used in other domains as well.
METHODS: We first retrieved PubMed abstracts of candidate articles with existing 
SOHOv1 concepts as search terms. Next, we used GPT-4-1201 to extract semantic 
triples from the abstracts. We identified concepts from these triples utilizing 
lexical, semantic, and knowledge network-based filtering. We also compared the 
granularity of semantic triples extracted with our method to the triples in the 
SemMedDB (Semantic MEDLINE Database). The results were evaluated by human 
experts and standard ontology tools for checking consistency and semantic 
correctness.
RESULTS: We expanded SOHOv1, which contained 173 concepts and 585 axioms, 
including 207 logical axioms to SOHOv2, which contains 572 concepts, 1,542 
axioms, including 725 logical axioms. Our methods identified more concepts than 
those extracted from SemMedDB for the same task. While we have shown the 
feasibility of our approach for an SDoH ontology, the methodology is 
generalizable to other ontologies with an existing seed ontology and text 
corpus.
CONCLUSIONS: The contributions of this work are: Extracting semantic triples 
from PubMed abstracts using GPT-4-1201 utilizing prompt chaining; showing the 
superiority of triples from GPT-4-1201 over triples from SemMedDB for SDoH; 
using lexical and semantic similarity search techniques with knowledge 
network-based search to identify the concepts to be added to the ontology; 
confirming the quality of the new concepts with human experts.

Copyright © 2025 The Author(s). Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104865
PMCID: PMC12371725
PMID: 40543734 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


688. J Med Internet Res. 2025 Jun 9;27:e72062. doi: 10.2196/72062.

Large Language Models in Medical Diagnostics: Scoping Review With Bibliometric 
Analysis.

Su H(#)(1)(2)(3), Sun Y(#)(1)(2), Li R(4), Zhang A(3), Yang Y(1)(2)(3), Xiao 
F(5), Duan Z(1)(2), Chen J(1)(2), Hu Q(1)(2), Yang T(1)(2), Xu B(1)(2), Zhang 
Q(1)(2), Zhao J(1)(2), Li Y(1)(2), Li H(1)(2).

Author information:
(1)Department of Reproductive Medicine, Xiangya Hospital Central South 
University, Changsha, China.
(2)Clinical Research Center for Women's Reproductive Health in Hunan Province, 
Changsha, China.
(3)Xiangya School of Medicine, Central South University, Changsha, China.
(4)School of Biomedical Sciences and Engineering, South China University of 
Technology, Guangzhou, China.
(5)Department of Metabolism and Endocrinology, Second Xiangya Hospital of 
Central South University, Changsha, China.
(#)Contributed equally

BACKGROUND: The integration of large language models (LLMs) into medical 
diagnostics has garnered substantial attention due to their potential to enhance 
diagnostic accuracy, streamline clinical workflows, and address health care 
disparities. However, the rapid evolution of LLM research necessitates a 
comprehensive synthesis of their applications, challenges, and future 
directions.
OBJECTIVE: This scoping review aimed to provide an overview of the current state 
of research regarding the use of LLMs in medical diagnostics. The study sought 
to answer four primary subquestions, as follows: (1) Which LLMs are commonly 
used? (2) How are LLMs assessed in diagnosis? (3) What is the current 
performance of LLMs in diagnosing diseases? (4) Which medical domains are 
investigating the application of LLMs?
METHODS: This scoping review was conducted according to the Joanna Briggs 
Institute Manual for Evidence Synthesis and adheres to the PRISMA-ScR (Preferred 
Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping 
Reviews). Relevant literature was searched from the Web of Science, PubMed, 
Embase, IEEE Xplore, and ACM Digital Library databases from 2022 to 2025. 
Articles were screened and selected based on predefined inclusion and exclusion 
criteria. Bibliometric analysis was performed using VOSviewer to identify major 
research clusters and trends. Data extraction included details on LLM types, 
application domains, and performance metrics.
RESULTS: The field is rapidly expanding, with a surge in publications after 
2023. GPT-4 and its variants dominated research (70/95, 74% of studies), 
followed by GPT-3.5 (34/95, 36%). Key applications included disease 
classification (text or image-based), medical question answering, and diagnostic 
content generation. LLMs demonstrated high accuracy in specialties like 
radiology, psychiatry, and neurology but exhibited biases in race, gender, and 
cost predictions. Ethical concerns, including privacy risks and model 
hallucination, alongside regulatory fragmentation, were critical barriers to 
clinical adoption.
CONCLUSIONS: LLMs hold transformative potential for medical diagnostics but 
require rigorous validation, bias mitigation, and multimodal integration to 
address real-world complexities. Future research should prioritize explainable 
artificial intelligence frameworks, specialty-specific optimization, and 
international regulatory harmonization to ensure equitable and safe clinical 
deployment.

©Hankun Su, Yuanyuan Sun, Ruiting Li, Aozhe Zhang, Yuemeng Yang, Fen Xiao, 
Zhiying Duan, Jingjing Chen, Qin Hu, Tianli Yang, Bin Xu, Qiong Zhang, Jing 
Zhao, Yanping Li, Hui Li. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org), 09.06.2025.

DOI: 10.2196/72062
PMCID: PMC12186007
PMID: 40489764 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


689. J Med Internet Res. 2026 Jan 17. doi: 10.2196/81387. Online ahead of print.

A Study on the Effectiveness of Al-Assisted Patient Health Education Using Voice 
Cloning and ChatGPT: A Prospective Randomized Controlled Trial.

Sun Y(1), Xu S(1), Jin H(1), Han X(1), Jin K(1), Zhang Y(1), Ma X(1), Wei H(2), 
Ma M(3).

Author information:
(1)Department of Thoracic Surgery, The First Hospital of Lanzhou University, 
Donggang West Road 1#, Lanzhou, CN.
(2)Outpatient Department, The First Hospital of Lanzhou University, Lanzhou, CN.
(3)Gansu International Science and Technology Cooperation Base for Development 
and Application of Thoracic Surgery Key Technologies, The First Clinical Medical 
College of Lanzhou University, Department of Thoracic Surgery, The First 
Hospital of Lanzhou University, Donggang West Road 1#Email: maminjie24@sina.com, 
Lanzhou, CN.

BACKGROUND: Traditional patient education often lacks personalization and 
engagement, potentially limiting knowledge acquisition and treatment 
adherence[1]. Advances in artificial intelligence (AI), including voice cloning 
technology and large language models such as ChatGPT, offer new opportunities to 
deliver personalized, scalable, and interactive health education[2-3]. However, 
evidence regarding the comparative effectiveness of different AI-based voice 
cloning strategies and the reliability of automated AI evaluation tools remains 
limited[4-5].
OBJECTIVE: To evaluate the effectiveness of AI-assisted patient education 
integrating voice cloning and ChatGPT, to compare physician voice cloning with 
patient self-voice cloning, and to assess the reliability of ChatGPT as an 
automated evaluation tool for education outcomes.
METHODS: A prospective, three-arm, parallel-group randomized controlled trial.A 
total of 180 hospitalized patients requiring standardized health education were 
recruited from a tertiary hospital. Inclusion criteria were: age ≥18 years, 
clear diagnosis requiring health education, clear consciousness, and voluntary 
participation with informed consent. Exclusion criteria were: severe hearing 
impairment, severe cognitive impairment, expected hospitalization <3 days, or 
prior participation in similar studies.Participants were randomly assigned 
(1:1:1) to receive (1) traditional education (control), (2) AI-assisted 
education using physician voice cloning, or (3) AI-assisted education using 
patient self-voice cloning. All groups received identical educational content 
with equal duration.The primary outcome was education content compliance, 
evaluated using ChatGPT-4 with validated prompts and verified by expert review. 
Secondary outcomes included knowledge retention, education satisfaction, 
treatment adherence, quality of life (SF-36), and psychological status (Hospital 
Anxiety and Depression Scale).Participants were randomly allocated using a 
computer-generated random sequence. Due to the nature of the intervention, 
participants were not blinded; outcome assessors and data analysts were blinded 
to group allocation.
RESULTS: Of 180 randomized participants, 174 (96.7%) completed the trial. Both 
AI-assisted groups demonstrated significantly higher education content 
compliance immediately after education compared with the control group 
(physician voice: 86.7 ± 7.3; self-voice: 92.5 ± 6.8 vs control: 73.2 ± 8.5; P < 
0.001). The patient self-voice group showed superior knowledge retention before 
discharge, higher education satisfaction, and greater treatment adherence 
compared with both the physician voice and control groups (all P ≤ 0.02). At 
one-month follow-up, the self-voice group maintained improved adherence (Cohen's 
d = 0.74) and exhibited significantly lower anxiety and depression scores (all P 
≤0.02), along with improved SF-36 quality-of-life domains. ChatGPT-based 
evaluations demonstrated high reliability compared with expert assessments 
(weighted κ = 0.87, 95% CI 0.82-0.91).
CONCLUSIONS: This study introduces an innovative patient education model 
integrating AI voice cloning and ChatGPT, representing a novel approach distinct 
from previous studies that primarily relied on standard text-to-speech or 
professionally recorded content. The key innovation lies in utilizing patients' 
own cloned voices for health education delivery, leveraging the self-reference 
effect to enhance learning outcomes. Compared with prior research focusing on 
clinician-narrated content, this study provides the first empirical evidence 
that self-voice education produces superior outcomes across multiple domains 
including compliance, satisfaction, and psychological well-being. These findings 
contribute to the field by establishing a theoretical and practical framework 
for personalized AI-driven patient education. In real-world clinical settings, 
this approach offers a scalable, cost-effective solution to enhance patient 
engagement, particularly valuable in resource-limited environments where 
individualized education is challenging to deliver.
CLINICALTRIAL: Trial Registration: Chinese Clinical Trial Registry 
(ChiCTR2500101882); registration application initiated on January 15, 2025 and 
finalized on April 30, 2025, before participant enrollment began in May 2025.

DOI: 10.2196/81387
PMID: 41548948


690. J Med Internet Res. 2025 Mar 11;27:e67488. doi: 10.2196/67488.

Accuracy of Large Language Models for Literature Screening in Thoracic Surgery: 
Diagnostic Study.

Dai ZY(1), Wang FQ(1), Shen C(1), Ji YL(1), Li ZY(1), Wang Y(1), Pu Q(1).

Author information:
(1)Department of Thoracic Surgery, West China Hospital of Sichuan University, 
Chengdu, China.

BACKGROUND: Systematic reviews and meta-analyses rely on labor-intensive 
literature screening. While machine learning offers potential automation, its 
accuracy remains suboptimal. This raises the question of whether emerging large 
language models (LLMs) can provide a more accurate and efficient approach.
OBJECTIVE: This paper evaluates the sensitivity, specificity, and summary 
receiver operating characteristic (SROC) curve of LLM-assisted literature 
screening.
METHODS: We conducted a diagnostic study comparing the accuracy of LLM-assisted 
screening versus manual literature screening across 6 thoracic surgery 
meta-analyses. Manual screening by 2 investigators served as the reference 
standard. LLM-assisted screening was performed using ChatGPT-4o (OpenAI) and 
Claude-3.5 (Anthropic) sonnet, with discrepancies resolved by Gemini-1.5 pro 
(Google). In addition, 2 open-source, machine learning-based screening tools, 
ASReview (Utrecht University) and Abstrackr (Center for Evidence Synthesis in 
Health, Brown University School of Public Health), were also evaluated. We 
calculated sensitivity, specificity, and 95% CIs for the title and abstract, as 
well as full-text screening, generating pooled estimates and SROC curves. LLM 
prompts were revised based on a post hoc error analysis.
RESULTS: LLM-assisted full-text screening demonstrated high pooled sensitivity 
(0.87, 95% CI 0.77-0.99) and specificity (0.96, 95% CI 0.91-0.98), with the area 
under the curve (AUC) of 0.96 (95% CI 0.94-0.97). Title and abstract screening 
achieved a pooled sensitivity of 0.73 (95% CI 0.57-0.85) and specificity of 0.99 
(95% CI 0.97-0.99), with an AUC of 0.97 (95% CI 0.96-0.99). Post hoc revisions 
improved sensitivity to 0.98 (95% CI 0.74-1.00) while maintaining high 
specificity (0.98, 95% CI 0.94-0.99). In comparison, the pooled sensitivity and 
specificity of ASReview tool-assisted screening were 0.58 (95% CI 0.53-0.64) and 
0.97 (95% CI 0.91-0.99), respectively, with an AUC of 0.66 (95% CI 0.62-0.70). 
The pooled sensitivity and specificity of Abstrackr tool-assisted screening were 
0.48 (95% CI 0.35-0.62) and 0.96 (95% CI 0.88-0.99), respectively, with an AUC 
of 0.78 (95% CI 0.74-0.82). A post hoc meta-analysis revealed comparable effect 
sizes between LLM-assisted and conventional screening.
CONCLUSIONS: LLMs hold significant potential for streamlining literature 
screening in systematic reviews, reducing workload without sacrificing quality. 
Importantly, LLMs outperformed traditional machine learning-based tools 
(ASReview and Abstrackr) in both sensitivity and AUC values, suggesting that 
LLMs offer a more accurate and efficient approach to literature screening.

©Zhang-Yi Dai, Fu-Qiang Wang, Cheng Shen, Yan-Li Ji, Zhi-Yang Li, Yun Wang, 
Qiang Pu. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 11.03.2025.

DOI: 10.2196/67488
PMCID: PMC11937709
PMID: 40068152 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


691. Neurol Clin Pract. 2025 Dec;15(6):e200542. doi: 10.1212/CPJ.0000000000200542. 
Epub 2025 Sep 23.

Accelerating Medical Record Data Abstraction and Analysis in Muscular Dystrophy: 
Large Language Models and International Classification of Diseases Codes.

Zhou H(1)(2), Rajamani G(3), Huang J(2), Jorand-Fletcher M(4), Mohamed Y(4), 
DeGolier KA(5), Xenopoulos-Oddsson A(5), Cui E(6), Zingariello CD(4), Zhang 
R(2), Kang PB(1)(7)(8).

Author information:
(1)Institute for Health Informatics, University of Minnesota, Minneapolis.
(2)Division of Computational Health Sciences, University of Minnesota, 
Minneapolis.
(3)Medical School, University of Minnesota, Minneapolis.
(4)Department of Pediatrics, University of Florida College of Medicine, 
Gainesville.
(5)Masonic Institute for the Developing Brain, University of Minnesota, 
Minneapolis.
(6)Division of Biostatistics and Health Data Science, University of Minnesota, 
Minneapolis.
(7)Greg Marzolf Jr. Muscular Dystrophy Center and Department of Neurology, 
University of Minnesota Medical School, Minneapolis; and.
(8)Institute for Translational Neuroscience, University of Minnesota, 
Minneapolis.

BACKGROUND AND OBJECTIVES: Muscular dystrophies are characterized by progressive 
muscle weakness and degeneration. Identifying cases and abstracting data from 
electronic medical records (EMRs) is helpful for surveillance and research. 
However, manual EMR abstraction is laborious. We studied 2 approaches to 
accelerate EMR abstraction: large language models (LLMs) and International 
Classification of Diseases (ICD) code meta-analysis.
METHODS: In our cross-sectional study, EMRs from 22 individuals with Duchenne 
muscular dystrophy (DMD) and 22 individuals with limb-girdle muscular dystrophy 
(LGMD) were exported into a data shelter and manually annotated using MedTator. 
Annotations were guided by a schema focused on 4 key features of muscular 
dystrophy: first symptoms, ambulatory status, serum creatine kinase (CK) levels, 
and genetic test results. Five LLMs were fed a series of prompts and examples, 
and then, clinic notes from each of the 44 cases were inputted for model 
analysis. Inter-rater agreement (IAA) and F1 scores were calculated for manual 
annotations, and the F1 score for LLMs compared with manual annotations was 
calculated. We then analyzed a separate set of 77 DMD and 59 LGMD cases to 
determine whether the number of health care encounters with a muscular 
dystrophy-related ICD code could predict diagnostic certainty based on MD 
STARnet criteria.
RESULTS: IAA for manual annotations varied between 80% (for annotation of 
symptoms) and 100% (for CK values). The highest performing LLM was Llama 3-8b, 
which yielded the following accuracies: 46.8% for "first symptoms," 56.9% for 
"ambulatory status," 69.2% for "CK values," and 68.4% for "genetic test 
results." Among 77 individuals with DMD, all patients with 20 or more encounters 
linked to relevant ICD codes had definite or probable diagnoses, whereas among 
59 individuals with LGMD, all patients with 25 or more encounters linked to 
relevant ICD codes had definite or probable diagnoses.
DISCUSSION: LLMs promise to accelerate EMR abstraction for rare diseases such as 
muscular dystrophy, but F1 scores for LLMs currently lag manual abstractions for 
unstructured data. Llama 3-8b demonstrated superior performance to the 4 other 
models tested. Metadata such as ICD code counts may help prioritize high-yield 
cases for surveillance and research purposes.

© 2025 American Academy of Neurology.

DOI: 10.1212/CPJ.0000000000200542
PMCID: PMC12456306
PMID: 40994811

Conflict of interest statement: The authors report no relevant disclosures. Full 
disclosure form information provided by the authors is available with the full 
text of this article at Neurology.org/cp.TAKE-HOME POINTS→ Large language models 
(LLMs) show potential to abstract information efficiently from electronic 
medical records.→ The accuracy of LLMs for medical record abstraction needs to 
improve, particularly for unstructured data.→ International Classification of 
Diseases codes and other metadata may help determine which cases are most 
suitable for detailed data abstraction.


692. Adv Med Educ Pract. 2025 Aug 15;16:1439-1453. doi: 10.2147/AMEP.S532951. 
eCollection 2025.

AI Foundations in China's Medical Physiology Education: Pedagogical Practices 
and Systemic Challenges.

Li H(1).

Author information:
(1)Department of Physiology, Medical School, Kunming University of Science and 
Technology,Anning Hospital Affiliated to Kunming University of Science and 
Technology, Kunming, 650500, People's Republic of China.

The integration of artificial intelligence (AI) into China's medical physiology 
education has emerged as a transformative strategy to modernize pedagogical 
practices and address systemic challenges in healthcare training. Current 
initiatives leverage AI-driven tools such as machine learning algorithms for 
physiological simulations and virtual reality (VR) for immersive clinical 
training, aiming to standardize educational outcomes, enhance student 
engagement, and improve readiness for complex clinical scenarios. However, the 
rapid adoption of AI introduces critical challenges, including widening resource 
disparities between urban and rural institutions due to over-reliance on 
technology, risks to data privacy in educational and clinical data management, 
and potential erosion of humanistic care in training environments. These 
challenges are further compounded by concerns over academic integrity-evidenced 
by student use of large language models (LLMs) to substitute critical thinking 
in assessments-and the need to balance AI efficiency with traditional teaching 
methods, particularly in specialized fields like Traditional Chinese Medicine 
(TCM) where master-apprentice models remain foundational. To reconcile 
technological innovation with educational integrity, this review proposes a 
balanced framework encompassing five key strategies: adaptive curriculum design 
that synergizes AI tools with human oversight, ethical governance to ensure 
algorithmic transparency and data security, equitable resource distribution to 
bridge regional gaps, faculty development programs to enhance AI literacy, and 
mentorship ecosystems that integrate AI with personalized guidance. By 
harmonizing AI's potential with core pedagogical values, these strategies aim to 
cultivate a new generation of clinicians equipped with both technical 
proficiency and ethical discernment, ultimately advancing healthcare quality and 
accessibility across China.

© 2025 Li.

DOI: 10.2147/AMEP.S532951
PMCID: PMC12363539
PMID: 40838173

Conflict of interest statement: The author reports no conflicts of interest in 
this work.


693. NPJ Digit Med. 2025 Jun 12;8(1):353. doi: 10.1038/s41746-025-01776-y.

How generative AI voice agents will transform medicine.

Adams SJ(1), Acosta JN(2), Rajpurkar P(3).

Author information:
(1)Department of Medical Imaging, Royal University Hospital, University of 
Saskatchewan, Saskatoon, SK, Canada.
(2)Department of Biomedical Informatics, Harvard Medical School, Boston, MA, 
USA.
(3)Department of Biomedical Informatics, Harvard Medical School, Boston, MA, 
USA. pranav_rajpurkar@hms.harvard.edu.

Generative AI voice agents-conversational systems powered by large language 
models that can understand and produce natural speech in real time-are poised to 
transform how health systems engage with patients. While technical and 
implementation challenges remain, with thoughtful design, rigorous validation, 
and responsible deployment, generative AI voice agents could become a critical 
extension of the care team, increasing the reach of clinicians and health 
systems in ways previously limited by human resources.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01776-y
PMCID: PMC12162835
PMID: 40506466

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


694. J Am Med Inform Assoc. 2026 Jan 11:ocaf230. doi: 10.1093/jamia/ocaf230. Online 
ahead of print.

Structural insights into clinical large language models and their barriers to 
translational readiness.

You J(1), Shin H(1).

Author information:
(1)Department of Medical Informatics and Statistics, Asan Medical Center, Brain 
Korea 21 Project, University of Ulsan College of Medicine, Seoul 05505, Republic 
of Korea.

BACKGROUND: Despite rapid integration into clinical decision-making, clinical 
large language models (LLMs) face substantial translational barriers due to 
insufficient structural characterization and limited external validation.
OBJECTIVE: We systematically map the clinical LLM research landscape to identify 
key structural patterns influencing their readiness for real-world clinical 
deployment.
METHODS: We identified 73 clinical LLM studies published between January 2020 
and March 2025 using a structured evidence-mapping approach. To ensure 
transparency and reproducibility in study selection, we followed key principles 
from the PRISMA 2020 framework. Each study was categorized by clinical task, 
base architecture, alignment strategy, data type, language, study design, 
validation methods, and evaluation metrics.
RESULTS: Studies often addressed multiple early stage clinical tasks-question 
answering (56.2%), knowledge structuring (31.5%), and disease prediction 
(43.8%)-primarily using text data (52.1%) and English-language resources 
(80.8%). GPT models favored retrieval-augmented generation (43.8%), and LLaMA 
models consistently adopted multistage pretraining and fine-tuning strategies. 
Only 6.9% of studies included external validation, and prospective designs were 
observed in just 4.1% of cases, reflecting significant gaps in translational 
reliability. Evaluations were predominantly quantitative only (79.5%), though 
qualitative and mixed-method approaches are increasingly recognized for 
assessing clinical usability and trustworthiness.
CONCLUSION: Clinical LLM research remains exploratory, marked by limited 
generalizability across languages, data types, and clinical environments. To 
bridge this gap, future studies must prioritize multilingual and multimodal 
training, prospective study designs with rigorous external validation, and 
hybrid evaluation frameworks combining quantitative performance with qualitative 
clinical usability metrics.

© The Author(s) 2026. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf230
PMID: 41520192


695. JMIR Form Res. 2025 Jun 30;9:e64986. doi: 10.2196/64986.

Enhancing Diagnostic Accuracy of Ophthalmological Conditions With Complex 
Prompts in GPT-4: Comparative Analysis of Global and Low- and Middle-Income 
Country (LMIC)-Specific Pathologies.

M'gadzah SAT(1), O'Malley A(1).

Author information:
(1)School of Medicine, University of St Andrews, North Haugh, St Andrews, KY16 
9TF, United Kingdom, 44 01382384210.

BACKGROUND: The global incidence of blindness has continued to increase, despite 
the enactment of a Global Eye Health Action Plan by the World Health Assembly. 
This can be attributed, in part, to an aging population, but also to the limited 
diagnostic resources within low- and middle-income countries (LMICs). The advent 
of generative artificial intelligence (AI) within health care could pose a novel 
solution to combating the prevalence of blindness globally.
OBJECTIVE: The objectives of this study are to quantify the effect the addition 
of a complex prompt has on the diagnostic accuracy of a commercially available 
LLM, and to assess whether such LLMs are better or worse at diagnosing 
conditions that are more prevalent in LMICs.
METHODS: Ten clinical vignettes representing globally and LMIC-prevalent 
ophthalmological conditions were presented to GPT-4-0125-preview using simple 
and complex prompts. Diagnostic performance metrics, including sensitivity, 
specificity, positive predictive value (PPV), and negative predictive value 
(NPV), were calculated. Statistical comparison between prompts was conducted 
using a chi-square test of independence.
RESULTS: The complex prompt achieved a higher diagnostic accuracy (90.1%) 
compared to the simple prompt (60.4%), with a statistically significant 
difference (χ2=428.86; P<.001). Sensitivity, specificity, PPV, and NPV were 
consistently improved for most conditions with the complex prompt. The simple 
prompt struggled with LMIC-prevalent conditions, diagnosing only 1 of 5 
accurately, while the complex prompt successfully diagnosed 4 of 5.
CONCLUSIONS: The study established that overall, the inclusion of a complex 
prompt positively affected the diagnostic accuracy of GPT-4-0125-preview, 
particularly for LMIC-prevalent conditions. This highlights the potential for 
LLMs, when appropriately tailored, to support clinicians in diverse health care 
settings. Future research should explore the generalizability of these findings 
across other models and specialties.

© Shona Alex Tapiwa M'gadzah, Andrew O'Malley. Originally published in JMIR 
Formative Research (https://formative.jmir.org).

DOI: 10.2196/64986
PMCID: PMC12261798
PMID: 40626794 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


696. J Clin Epidemiol. 2025 Nov 21;190:112083. doi: 10.1016/j.jclinepi.2025.112083. 
Online ahead of print.

Approaches for reporting and interpreting statistically nonsignificant findings 
in evidence syntheses: a systematic review.

Sharifan A(1), Dobrescu A(2), Harrod C(3), Klerings I(2), Ong AY(4), Ngeh E(5), 
Xiao YT(6), Gartlehner G(7).

Author information:
(1)Department for Evidence-based Medicine and Evaluation, University for 
Continuing Education Krems, Krems, Austria. Electronic address: 
amin.sharifan@donau-uni.ac.at.
(2)Department for Evidence-based Medicine and Evaluation, University for 
Continuing Education Krems, Krems, Austria.
(3)American College of Physicians, Philadelphia, PA, USA.
(4)Institute of Ophthalmology, University College London, London, UK; Oxford Eye 
Hospital, Oxford University Hospitals NHS Foundation Trust, Oxford, UK.
(5)School of Health and Society, University of Salford, Salford, UK.
(6)Center for Cancer Research, Medical University of Vienna and Comprehensive 
Cancer Center, Vienna, Austria.
(7)Department for Evidence-based Medicine and Evaluation, University for 
Continuing Education Krems, Krems, Austria; Center for Public Health Methods, 
RTI International, Research Triangle Park, NC, USA.

OBJECTIVES: To systematically review approaches for reporting and interpreting 
statistically nonsignificant findings with clinical relevance in evidence 
synthesis and to assess their methodological quality and the extent of their 
empirical validation.
STUDY DESIGN AND SETTING: We searched Ovid MEDLINE ALL, Scopus, PsycInfo, 
Library of Guidance for Health Scientists, and MathSciNet for published studies 
in English from January 1, 2000, to January 30, 2025, for (1) best practices in 
guidance documents for evidence synthesis when interpreting clinically relevant 
nonsignificant findings, (2) statistical methods to support the interpretation, 
and (3) reporting practices. To identify relevant reporting guidelines, we also 
searched the Enhancing the QUAlity and Transparency Of health Research Network. 
The quality assessment applied the Mixed Methods Appraisal Tool, Appraisal tool 
for Cross-Sectional Studies, and checklists for expert opinion and systematic 
reviews from the Joanna Briggs Institute. At least two reviewers independently 
conducted all procedures, and a large language model facilitated data extraction 
and quality appraisal.
RESULTS: Of the 5332 records, 37 were eligible for inclusion. Of these, 15 were 
editorials or opinion pieces, nine addressed methods, eight were cross-sectional 
or mixed-methods studies, four were journal guidance documents, and one was a 
systematic review. Twenty-seven records met the quality criteria of the 
appraisal tool relevant to their study design or publication type, while 10 
records, comprising one systematic review, two editorials or opinion pieces, and 
seven cross-sectional studies, did not. Relevant methodological approaches to 
evidence synthesis included utilization of uncertainty intervals and their 
integration with various statistical measures (15 of 37, 41%), Bayes factors 
(six of 37, 16%), likelihood ratios (three of 37, 8%), effect conversion 
measures (two of 37, 5%), equivalence testing (two of 37, 5%), modified Fisher's 
test (one of 37, 3%), and reverse fragility index (one of 37, 3%). Reporting 
practices included problematic "null acceptance" language (14 of 37, 38%), with 
some records discouraging the inappropriate claim of no effect based on 
nonsignificant findings (nine of 37, 24%). None of the proposed methods were 
empirically tested with interest holders.
CONCLUSION: Although various approaches have been proposed to improve the 
presentation and interpretation of statistically nonsignificant findings, a 
widely accepted consensus has not emerged, as these approaches have yet to be 
systematically tested for their practicality and validity. This review provides 
a comprehensive review of available methodological approaches spanning both the 
frequentist and Bayesian statistical frameworks and identifies critical gaps in 
empirical validation of some approaches, namely the lack of thresholds to guide 
the interpretation of results. These findings highlight the need for systematic 
testing of proposed methods with interest holders and the development of 
evidence-based guidance to support appropriate interpretation of nonsignificant 
results in evidence synthesis.
PLAIN LANGUAGE SUMMARY: This review looked at how to best report results that 
are not statistically significant because some of these findings can still be 
important to inform clinical care or health policy. We searched databases for 
studies published between 2000 and 2025. Out of more than 5000 records, 37 
studies were relevant. These studies showed that there is no single best way to 
report nonsignificant findings.

Copyright © 2025 The Author(s). Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jclinepi.2025.112083
PMID: 41276091

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


697. J Med Syst. 2025 Sep 24;49(1):118. doi: 10.1007/s10916-025-02248-2.

Automated Resectability Classification of Pancreatic Cancer CT Reports with 
Privacy-Preserving Open-Weight Large Language Models: A Multicenter Study.

Lee JH(1), Min JH(2), Gu K(1), Han S(1), Hwang JA(1), Choi SY(1)(3), Song KD(1), 
Lee JE(4), Lee J(5), Moon JE(6), Adetyan H(7), Yang JD(7).

Author information:
(1)Department of Radiology and Center for Imaging Science, Samsung Medical 
Center, Sungkyunkwan University School of Medicine, 81 Irwon-ro Gangnam-gu, 
Seoul, 06351, Republic of Korea.
(2)Department of Radiology and Center for Imaging Science, Samsung Medical 
Center, Sungkyunkwan University School of Medicine, 81 Irwon-ro Gangnam-gu, 
Seoul, 06351, Republic of Korea. minjh1123@gmail.com.
(3)Department of Radiology, Soonchunhyang University College of Medicine, 
Bucheon Hospital, Bucheon, Republic of Korea.
(4)Department of Radiology, Chungnam National University Hospital, Chungnam 
National University College of Medicine, Daejeon, Republic of Korea.
(5)Department of Radiology, Chungbuk National University, Chungbuk National 
University Hospital, Cheongju, Republic of Korea.
(6)Department of Radiology, Kangdong Seong-Sim Hospital, Hallym University 
College of Medicine, Seoul, Republic of Korea.
(7)Karsh Division of Gastroenterology and Hepatology, Cedars-Sinai Medical 
Center, Los Angeles, CA, USA.

 To evaluate the effectiveness of open-weight large language models (LLMs) in 
extracting key radiological features and determining National Comprehensive 
Cancer Network (NCCN) resectability status from free-text radiology reports for 
pancreatic ductal adenocarcinoma (PDAC). Methods. Prompts were developed using 
30 fictitious reports, internally validated on 100 additional fictitious 
reports, and tested using 200 real reports from two institutions (January 2022 
to December 2023). Two radiologists established ground truth for 18 key features 
and resectability status. Gemma-2-27b-it and Llama-3-70b-instruct models were 
evaluated using recall, precision, F1-score, extraction accuracy, and overall 
resectability accuracy. Statistical analyses included McNemar's test and 
mixed-effects logistic regression. Results. In internal validation, Llama had 
significantly higher recall than Gemma (99% vs. 95%, p < 0.01) and slightly 
higher extraction accuracy (98% vs. 97%). Llama also demonstrated higher overall 
resectability accuracy (93% vs. 91%). In the internal test set, both models 
achieved 96% recall and 96% extraction accuracy. Overall resectability accuracy 
was 95% for Llama and 93% for Gemma. In the external test set, both models had 
93% recall. Extraction accuracy was 93% for Llama and 95% for Gemma. Gemma 
achieved higher overall resectability accuracy (89% vs. 83%), but the difference 
was not statistically significant (p > 0.05). Conclusion. Open-weight models 
accurately extracted key radiological features and determined NCCN resectability 
status from free-text PDAC reports. While internal dataset performance was 
robust, performance on external data decreased, highlighting the need for 
institution-specific optimization.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02248-2
PMID: 40991110 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics Approval: This study was 
determined to be exempt from IRB review by Samsung Medical Center IRB 
(2024-10-105-001) as it uses only anonymized data from the Clinical Data 
Warehouse with no collection of personally identifiable information. 
Additionally, IRB approval with waiver of informed consent was obtained from 
Cedars-Sinai Medical Center (STUDY00003634). Clinical Trial Number: Not 
applicable. Competing interests: The authors declare no competing interests.


698. J Am Med Inform Assoc. 2025 Oct 15:ocaf176. doi: 10.1093/jamia/ocaf176. Online 
ahead of print.

Using natural language processing to extract information from clinical text in 
electronic medical records for populating clinical registries: a systematic 
review.

Liu L(1), Blake V(1), Barman M(1), Gallego B(1), Churches T(2), Kennedy G(2)(3), 
Ooi SY(4)(5), Delaney GP(2)(3), Jorm L(1).

Author information:
(1)Centre for Big Data Research in Health, University of New South Wales, 
Sydney, NSW 2052, Australia.
(2)Ingham Institute for Applied Medical Research, Liverpool, NSW 2170, 
Australia.
(3)South Western Sydney Clinical School, University of New South Wales, Sydney, 
NSW 2052, Australia.
(4)School of Clinical Medicine, University of New South Wales, Sydney, NSW 2052, 
Australia.
(5)Prince of Wales Hospital, Randwick, NSW 2031, Australia.

OBJECTIVE: Clinical registries advance healthcare by tracking patient outcomes 
and intervention safety. Manually extracting information from clinical text for 
registries is labor- and resource-intensive and often inaccurate. Therefore, 
this systematic review aims to evaluate the use and effectiveness of natural 
language processing (NLP) methods in extracting information from clinical text 
for populating clinical registries.
MATERIALS AND METHODS: PubMed, Embase, Scopus, Web of Science, and ACM Digital 
Library were systematically searched. Studies were included if they used NLP 
techniques to populate clinical registries. The extracted data included details 
of the registry, the clinical text, the registry data elements extracted, the 
NLP methods used, and how their performance was evaluated.
RESULTS: Fifteen articles were included in the review. Since 2020, the use of 
NLP methods for extracting information to populate clinical registries has been 
increasing steadily. Initially, rule-based NLP methods dominated the field, but 
machine learning-based approaches have gradually gained popularity. However, 
only one of the included studies employed generative large language models 
(LLMs). The diversity of clinical text and extracted data elements posed 
challenges to the generalizability of the NLP methods.
CONCLUSION: To date, the application of NLP methods to clinical text for 
populating clinical registries has been limited in both the number of published 
studies and the scope of implementation. The NLP methods used thus far face 
significant challenges in effectively managing the complexity and diversity of 
clinical text and data elements. Moreover, the performance of the NLP methods 
varied significantly. This review underscores the need for a robust and 
adaptable NLP framework. Generative LLMs may provide direction for future 
research, but their use must account for challenges such as accuracy, cost, 
privacy, and limited supporting evidence.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf176
PMID: 41093296


699. Acad Radiol. 2025 May;32(5):2385-2391. doi: 10.1016/j.acra.2024.10.050. Epub 
2024 Dec 9.

Enhancing Oncological Surveillance Through Large Language Model-Assisted 
Analysis: A Comparative Study of GPT-4 and Gemini in Evaluating Oncological 
Issues From Serial Abdominal CT Scan Reports.

Han NY(1), Shin K(2), Kim MJ(3), Park BJ(1), Sim KC(1), Han YE(1), Sung DJ(1), 
Choi JW(4), Yeom SK(5).

Author information:
(1)Department of Radiology, Korea University Anam Hospital, Korea University 
College of Medicine, 73 Goryeodae-ro, Seongbuk-gu, Seoul, Republic of Korea 
(N.Y.H., M.J.K., B.J.P., K.C.S., Y.E.H., D.J.S.).
(2)Center for AI and Digital Healthcare Research, Korea University Anam 
Hospital, Korea University College of Medicine, Seoul, Republic of Korea (K.S.).
(3)Department of Radiology, Korea University Anam Hospital, Korea University 
College of Medicine, 73 Goryeodae-ro, Seongbuk-gu, Seoul, Republic of Korea 
(N.Y.H., M.J.K., B.J.P., K.C.S., Y.E.H., D.J.S.). Electronic address: 
mjkim7@korea.ac.kr.
(4)Department of Radiology, Korea University Guro Hospital, Korea University 
College of Medicine, 148 Gurodong-ro, Guro-gu, Seoul, Republic of 
Korea (J.W.C.).
(5)Department of Radiology, Korea University Ansan Hospital, Korea University 
College of Medicine, 123, Jeokgeum-ro, Danwon-gu, Ansan-si, Gyeonggi-do, 
Republic of Korea (S.K.Y.).

RATIONALE AND OBJECTIVES: We aimed to compare the capabilities of two leading 
large language models (LLMs), GPT-4 and Gemini, in analyzing serial radiology 
reports, to highlight oncological issues that require further clinical 
attention.
MATERIALS AND METHODS: This study included 205 patients, each with two 
consecutive radiological reports. We designed a prompt comprising a three-step 
task to analyze report findings using LLMs. To establish a ground truth, two 
radiologists reached a consensus on a six-level categorization, comprising tumor 
findings (categorized as improved, stable, or aggravated), "benign", "no tumor 
description," and "other malignancy." The performance of GPT-4 and Gemini was 
then compared based on their ability to match corresponding findings between two 
radiological reports and accurately reflect these categories.
RESULTS: In terms of accuracy in matching findings between serial reports, the 
proportion of correctly matched findings was significantly higher for GPT-4 
(96.2%) than for Gemini (91.7%) (P < 0.01). For oncological issue 
identification, the precision for tumor-related finding determinations, recall, 
and F1-scores were 0.68 and 0.63 (P = 0.006), 0.91 and 0.80 (P < 0.001), and 
0.78 and 0.70 for GPT-4 and Gemini, respectively. GPT-4 was more accurate than 
Gemini in determining the correct tumor status for tumor-related findings 
(P < 0.001).
CONCLUSION: This study demonstrated the potential of LLM-assisted analysis of 
serial radiology reports in enhancing oncological surveillance, using a 
carefully engineered prompt. GPT-4 showed superior performance compared to 
Gemini in matching corresponding findings, identifying tumor-related findings, 
and accurately determining tumor status.

Copyright © 2024 The Association of University Radiologists. All rights 
reserved.

DOI: 10.1016/j.acra.2024.10.050
PMID: 39658474 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


700. J Am Med Inform Assoc. 2025 Oct 27:ocaf174. doi: 10.1093/jamia/ocaf174. Online 
ahead of print.

Compliance and factuality of large language models for clinical research 
document generation.

Wang Z(1), Gao J(2), Danek B(1), Theodorou B(1), Shaik R(3), Thati S(4), Won 
S(5), Sun J(1)(3)(4).

Author information:
(1)Keiji AI, Seattle, WA, 98115, United States.
(2)Centre for Medical Informatics, Usher Institute, University of Edinburgh, 
Edinburgh, EH3 9DR, United Kingdom.
(3)Carle Illinois College of Medicine, University of Illinois Urbana-Champaign, 
Urbana, IL, 61801, United States.
(4)Siebel School of Computing and Data Science, University of Illinois 
Urbana-Champaign, Urbana, IL, 61801, United States.
(5)Medical Research Collaborating Center, Seoul National University Bundang 
Hospital, Gyeonggi, 13620, Republic of Korea.

OBJECTIVES: Large language models' (LLMs') performance in high-stakes, 
compliance-driven settings such as drafting clinical research documents remains 
underexplored. This study aims to build a benchmark and an evaluation framework 
for assessing LLMs' compliance and factuality in generating informed consent 
forms (ICFs) from clinical trial protocols.
MATERIALS AND METHODS: We introduce InformBench, a benchmark comprising 900 
clinical trial documents, and propose an evaluation framework grounded in 
regulatory guidelines and site-specific consent templates. We assess LLM 
performance on transforming trial protocols, often hundreds of pages, into 
concise, patient-facing ICFs. Additionally, we design InformGen, a 
retrieval-augmented, human-in-the-loop pipeline aimed at improving generation 
quality.
RESULTS: Baseline LLMs such as GPT-4o achieved only 70%-80% compliance and 
exhibited factual errors in 18%-43% of cases. In contrast, InformGen 
substantially improved outputs, achieving nearly 100% regulatory compliance and 
over 90% factual accuracy, as validated by 5 domain-expert annotators.
DISCUSSION: The study reveals critical limitations in current LLMs for clinical 
research document drafting, particularly in regulatory sensitivity and factual 
grounding. Our results highlight the need for domain-specific benchmarks and 
structured evaluations to support safe deployment in real-world clinical 
research workflows.
CONCLUSION: LLMs offer value in clinical research document generation but must 
be adapted and rigorously evaluated for high-stakes applications. Our benchmark 
and framework provide a foundation for improving and assessing LLM-generated 
outputs in compliance-critical domains.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf174
PMID: 41144289


701. J Med Internet Res. 2025 Feb 7;27:e63550. doi: 10.2196/63550.

ChatGPT for Univariate Statistics: Validation of AI-Assisted Data Analysis in 
Healthcare Research.

Ruta MR(1), Gaidici T(1), Irwin C(1), Lifshitz J(1).

Author information:
(1)University of Arizona College of Medicine - Phoenix, Phoenix, AZ, United 
States.

BACKGROUND: ChatGPT, a conversational artificial intelligence developed by 
OpenAI, has rapidly become an invaluable tool for researchers. With the recent 
integration of Python code interpretation into the ChatGPT environment, there 
has been a significant increase in the potential utility of ChatGPT as a 
research tool, particularly in terms of data analysis applications.
OBJECTIVE: This study aimed to assess ChatGPT as a data analysis tool and 
provide researchers with a framework for applying ChatGPT to data management 
tasks, descriptive statistics, and inferential statistics.
METHODS: A subset of the National Inpatient Sample was extracted. Data analysis 
trials were divided into data processing, categorization, and tabulation, as 
well as descriptive and inferential statistics. For data processing, 
categorization, and tabulation assessments, ChatGPT was prompted to reclassify 
variables, subset variables, and present data, respectively. Descriptive 
statistics assessments included mean, SD, median, and IQR calculations. 
Inferential statistics assessments were conducted at varying levels of prompt 
specificity ("Basic," "Intermediate," and "Advanced"). Specific tests included 
chi-square, Pearson correlation, independent 2-sample t test, 1-way ANOVA, 
Fisher exact, Spearman correlation, Mann-Whitney U test, and Kruskal-Wallis H 
test. Outcomes from consecutive prompt-based trials were assessed against 
expected statistical values calculated in Python (Python Software Foundation), 
SAS (SAS Institute), and RStudio (Posit PBC).
RESULTS: ChatGPT accurately performed data processing, categorization, and 
tabulation across all trials. For descriptive statistics, it provided accurate 
means, SDs, medians, and IQRs across all trials. Inferential statistics accuracy 
against expected statistical values varied with prompt specificity: 32.5% 
accuracy for "Basic" prompts, 81.3% for "Intermediate" prompts, and 92.5% for 
"Advanced" prompts.
CONCLUSIONS: ChatGPT shows promise as a tool for exploratory data analysis, 
particularly for researchers with some statistical knowledge and limited 
programming expertise. However, its application requires careful prompt 
construction and human oversight to ensure accuracy. As a supplementary tool, 
ChatGPT can enhance data analysis efficiency and broaden research accessibility.

©Michael R Ruta, Tony Gaidici, Chase Irwin, Jonathan Lifshitz. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org), 
07.02.2025.

DOI: 10.2196/63550
PMCID: PMC11845875
PMID: 39919289 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


702. NPJ Digit Med. 2025 Mar 25;8(1):178. doi: 10.1038/s41746-025-01566-6.

Accuracy of online symptom assessment applications, large language models, and 
laypeople for self-triage decisions.

Kopka M(1), von Kalckreuth N(2), Feufel MA(2).

Author information:
(1)Division of Ergonomics, Department of Psychology and Ergonomics (IPA), 
Technische Universität Berlin, Berlin, Germany. marvin.kopka@tu-berlin.de.
(2)Division of Ergonomics, Department of Psychology and Ergonomics (IPA), 
Technische Universität Berlin, Berlin, Germany.

Symptom-Assessment Application (SAAs, e.g., NHS 111 online) that assist 
laypeople in deciding if and where to seek care (self-triage) are gaining 
popularity and Large Language Models (LLMs) are increasingly used too. However, 
there is no evidence synthesis on the accuracy of LLMs, and no review has 
contextualized the accuracy of SAAs and LLMs. This systematic review evaluates 
the self-triage accuracy of both SAAs and LLMs and compares them to the accuracy 
of laypeople. A total of 1549 studies were screened and 19 included. The 
self-triage accuracy of SAAs was moderate but highly variable (11.5-90.0%), 
while the accuracy of LLMs (57.8-76.0%) and laypeople (47.3-62.4%) was moderate 
with low variability. Based on the available evidence, the use of SAAs or LLMs 
should neither be universally recommended nor discouraged; rather, we suggest 
that their utility should be assessed based on the specific use case and user 
group under consideration.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01566-6
PMCID: PMC11937345
PMID: 40133390

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


703. J Med Internet Res. 2025 Oct 14;27:e79217. doi: 10.2196/79217.

Evaluation Strategies for Large Language Model-Based Models in Exercise and 
Health Coaching: Scoping Review.

Lai X(1)(2), Lai Y(3), Chen J(1), Huang S(1), Gao Q(2), Huang C(1).

Author information:
(1)Research and Communication Center for Exercise and Health, Xiamen University 
of Technology, 600 Ligong Road, Jimei District, Xiamen, Fujian Province, 361024, 
China, 86 15606951380.
(2)School of Sport Medicine and Rehabilitation, Beijing Sport University, 
Beijing, China.
(3)Department of Mathematics and Digital Science, Chengyi College, Jimei 
University, Xiamen, China.

BACKGROUND: Large language model (LLM)-based artificial intelligence (AI) 
coaches show promise for personalized exercise and health interventions. 
However, the unique demands of ensuring safety and real-time, multimodal 
personalized feedback have created a fragmented evaluation landscape lacking 
standardized frameworks.
OBJECTIVE: This scoping review systematically maps current evaluation strategies 
for LLM-based AI coaches in exercise and health, identifies strengths and 
limitations, and proposes directions for robust, standardized validation.
METHODS: Following PRISMA-ScR (Preferred Reporting Items for Systematic reviews 
and Meta-Analyses extension for Scoping Reviews) guidelines, we conducted a 
systematic search across 6 major databases (eg, PubMed, Web of Science) for 
original research on LLM-based exercise and health coaching. Studies were 
included if they explicitly reported on evaluation methods. We extracted and 
synthesized data on model types, application domains, and evaluation strategies 
and developed a 5-point Evaluation Rigor Score (ERS) to quantitatively assess 
the methodological depth of the evaluation designs.
RESULTS: We included 20 studies, most using proprietary models like ChatGPT 
(75%). Evaluation strategies were highly heterogeneous, mixing human ratings 
(80%) and automated metrics (40%). Crucially, the evidence was limited by low 
methodological rigor: the median ERS was 2.5 out of 5, with 55% of studies 
classified as having low rigor. Key gaps included limited use of real-world data 
(40%) and inconsistent reliability reporting (45%).
CONCLUSIONS: The current evaluation of LLM-based health coaches is fragmented 
and methodologically weak. Future work must establish multidimensional 
validation frameworks that integrate technical benchmarks with human-centered 
methods to ensure safe, effective, and equitable deployment.

© Xiangxun Lai, Yue Lai, JiaCheng Chen, Shengqi Huang, Qi Gao, Caihua Huang. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/79217
PMCID: PMC12520646
PMID: 41086432 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


704. Psychol Med. 2026 Jan 20;56:e27. doi: 10.1017/S0033291725103073.

Autonomous conversational agents for loneliness, social isolation, depression, 
and anxiety in older people without cognitive impairment: Systematic review and 
meta-analysis.

Satake Y(1)(2), Costello H(1), Naran N(3), Ishimaru D(4), Ikeda M(2), Howard 
R(1).

Author information:
(1)Division of Psychiatry, University College London, London, UK.
(2)Department of Psychiatry, The University of Osaka Graduate School of 
Medicine, Suita, Japan.
(3)North London NHS Foundation Trust, London, UK.
(4)Department of Medical Technology, The University of Osaka Hospital, Suita, 
Japan.

Loneliness is a major psychological challenge in older adulthood, contributing 
to increased risks of depression, anxiety, and mortality. Conversational agents 
- technologies that interact with users via natural language - have emerged as 
potential tools to support psychological well-being in later life. This 
systematic review and meta-analysis evaluated the effects of autonomous 
conversational agents, including robotic and nonrobotic systems, on loneliness, 
as well as social isolation, depression, and anxiety in older people without 
cognitive impairment. Seventeen studies with pre-post intervention data were 
included. Nine used physically embodied robots and eight employed nonrobotic 
agents, such as personal voice assistants, chatbots, or screen-based embodied 
agents. Due to the limited number of high-quality comparison studies, all 
meta-analyses were based on within-group pre-post comparisons. Meta-analytic 
results showed mild to moderate improvements in loneliness (standardized mean 
changes using change score [SMCC] = 0.350, 95% confidence interval [CI]: 
0.180-0.520) and depression (SMCC = 0.464, 95% CI: 0.327-0.602), with no study 
reporting symptom worsening. No study included validated measures of social 
isolation, and only one assessed anxiety. These findings indicate that 
conversational agents may offer scalable support for older adults' mental 
health, with potential especially for reducing loneliness and depression. 
Nonetheless, methodological limitations, including lack of blinded outcome 
assessment, inconsistent reporting, and heterogeneous intervention designs, 
underscore the need for more rigorous research. Advances in large language 
models may further enhance the responsiveness and relevance of these 
technologies for supporting psychological well-being in aging populations.

DOI: 10.1017/S0033291725103073
PMID: 41556104 [Indexed for MEDLINE]


705. JAMA Ophthalmol. 2025 Aug 1;143(8):623-632. doi: 
10.1001/jamaophthalmol.2025.1419.

Generation of Fundus Fluorescein Angiography Videos for Health Care Data 
Sharing.

Wu X(1), Wang L(2), Chen R(1), Liu B(1), Zhang W(1), Yang X(3), Feng Y(3), He 
M(1)(4)(5), Shi D(1)(4).

Author information:
(1)School of Optometry, Hong Kong Polytechnic University, Hong Kong SAR, China.
(2)Department of Computing, Hong Kong Polytechnic University, Hong Kong SAR, 
China.
(3)Department of Ophthalmology, Zhongshan Hospital, Fudan University, Shanghai, 
China.
(4)Research Centre for SHARP Vision, Hong Kong Polytechnic University, Hong Kong 
SAR, China.
(5)Centre for Eye and Vision Research, Hong Kong SAR, China.

Comment on
    JAMA Ophthalmol. 2025 Aug 1;143(8):632-633. doi: 
10.1001/jamaophthalmol.2025.2023.

IMPORTANCE: Medical data sharing faces strict restrictions. Text-to-video 
generation shows potential for creating realistic medical data while preserving 
privacy, offering a solution for cross-center data sharing and medical 
education.
OBJECTIVE: To develop and evaluate a text-to-video generative artificial 
intelligence (AI)-driven model that converts the text of reports into dynamic 
fundus fluorescein angiography (FFA) videos, enabling visualization of retinal 
vascular and structural abnormalities.
DESIGN, SETTING, AND PARTICIPANTS: This study retrospectively collected 
anonymized FFA data from a tertiary hospital in China. The dataset included both 
the medical records and FFA examinations of patients assessed between November 
2016 and December 2019. A text-to-video model was developed and evaluated. The 
AI-driven model integrated the wavelet-flow variational autoencoder and the 
diffusion transformer.
MAIN OUTCOMES AND MEASURES: The AI-driven model's performance was assessed 
through objective metrics (Fréchet video distance, learned perceptual image 
patch similarity score, and visual question answering score [VQAScore]). The 
domain-specific evaluation for the generated FFA videos was measured by the 
bidirectional encoder representations from transformers score (BERTScore). Image 
retrieval was evaluated using a Recall@K score. Each video was rated for quality 
by 3 ophthalmologists on a scale of 1 (excellent) to 5 (very poor).
RESULTS: A total of 3625 FFA videos were included (2851 videos [78.6%] for 
training, 387 videos [10.7%] for validation, and 387 videos [10.7%] for 
testing). The AI-generated FFA videos demonstrated retinal abnormalities from 
the input text (Fréchet video distance of 2273, a mean learned perceptual image 
patch similarity score of 0.48 [SD, 0.04], and a mean VQAScore of 0.61 [SD, 
0.08]). The domain-specific evaluations showed alignment between the generated 
videos and textual prompts (mean BERTScore, 0.35 [SD, 0.09]). The Recall@K 
scores were 0.02 for K = 5, 0.04 for K = 10, and 0.16 for K = 50, yielding a 
mean score of 0.073, reflecting disparities between AI-generated and real 
clinical videos and demonstrating privacy-preserving effectiveness. For 
assessment of visual quality of the FFA videos by the 3 ophthalmologists, the 
mean score was 1.57 (SD, 0.44).
CONCLUSIONS AND RELEVANCE: This study demonstrated that an AI-driven 
text-to-video model generated FFA videos from textual descriptions, potentially 
improving visualization for clinical and educational purposes. The 
privacy-preserving nature of the model may address key challenges in data 
sharing while trying to ensure compliance with confidentiality standards.

DOI: 10.1001/jamaophthalmol.2025.1419
PMCID: PMC12203415
PMID: 40569610 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest Disclosures: Dr Shi 
reported having a patent pending on a method for developing artificial 
intelligence models with generated data at Hong Kong Polytechnic University. No 
other disclosures were reported.


706. Med Biol Eng Comput. 2025 Sep;63(9):2679-2691. doi: 10.1007/s11517-025-03353-7. 
Epub 2025 Mar 31.

Automated detection of retinal artery occlusion in fundus photography via 
self-supervised deep learning and multimodal interpretability using a multimodal 
AI chatbot.

Ryu SY(1), Choi JY(2), Yoo TK(3).

Author information:
(1)B&VIIT Eye Center, Seoul, South Korea.
(2)Department of Biomedical Engineering, Yonsei University, Wonju, South Korea.
(3)Department of Ophthalmology, Hangil Eye Hospital, Incheon, South Korea. 
eyetaekeunyoo@gmail.com.

Retinal artery occlusion (RAO) is a sight-threatening condition that requires 
prompt diagnosis to prevent irreversible vision loss. This study presents an 
innovative AI-driven approach for RAO detection from fundus images, marking the 
first application of deep learning for this purpose. Using a self-supervised 
learning (SSL) framework with SimCLR, our model addresses the challenge of 
limited labeled RAO data. The ResNet50 model pretrained with SimCLR demonstrated 
high diagnostic accuracy, achieving areas under the receiver operating 
characteristic curve (AUC) of 0.924 and 0.988 on two external validation 
datasets, highlighting its robustness and generalizability in RAO detection. To 
enhance transparency in clinical AI, we incorporated a multimodal 
interpretability approach using a ChatGPT-4-based AI chatbot. This chatbot, 
combined with Grad-CAM visualizations, provides detailed clinical explanations 
of the model's predictions, emphasizing key RAO features such as retinal 
whitening and cherry-red spots. This multimodal interpretability framework 
improves clinicians' understanding of the model's decision-making process, 
facilitating clinical adoption and trust. By automating RAO detection, this AI 
model serves as a valuable tool for the early identification of ocular and 
systemic vascular risks, enabling timely intervention. These findings highlight 
the potential of fundus imaging for RAO detection and broader cardiovascular 
risk assessment, advancing AI's role in predictive healthcare.

© 2025. International Federation for Medical and Biological Engineering.

DOI: 10.1007/s11517-025-03353-7
PMID: 40163243 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval: All procedures 
complied with the ethical standards of the institutional and national research 
committees, as well as the 1964 Declaration of Helsinki and its later amendments 
or comparable ethical guidelines. As this study utilized publicly accessible, 
de-identified, web-based data, ethics committee approval was not required, and 
no clinical data from human participants were obtained. Consent to participate: 
Not applicable. Conflict of interest: The authors declare no competing 
interests.


707. JAMA Psychiatry. 2026 Jan 14. doi: 10.1001/jamapsychiatry.2025.4116. Online 
ahead of print.

Artificial Intelligence and the Potential Transformation of Mental Health.

Perlis RH(1).

Author information:
(1)Center for Quantitative Health, Massachusetts General Hospital, Boston.

IMPORTANCE: The potential of tools using artificial intelligence (AI) to address 
the many challenges in delivery of mental health care has been widely discussed. 
However, the possible negative consequences of AI for such care have received 
less attention.
OBSERVATIONS: Integrating AI with mental health care has the potential to expand 
access and improve quality of care. It may also contribute to improvements in 
diagnosis, risk stratification, and development of novel therapeutics. At the 
same time, availability of AI chatbots and stratification algorithms may 
diminish access to human-delivered care. Reliance on AI tools may have other 
unanticipated adverse consequences on clinical practice, including diminished 
human clinician skill. The probabilistic nature of many of these tools, 
including large language models, makes their capacity to cause harm difficult to 
determine.
CONCLUSIONS AND RELEVANCE: The likely benefits of AI for psychiatric care 
delivery must be balanced against substantial risks. Strategies to mitigate this 
risk may require regulation to enhance transparency and systematically evaluate 
the impact of AI in practice, as well as clinician training to make optimal use 
of these emerging methods.

DOI: 10.1001/jamapsychiatry.2025.4116
PMID: 41533367


708. Arthroscopy. 2025 Nov;41(11):4607-4614.e4. doi: 10.1016/j.arthro.2025.04.033. 
Epub 2025 Apr 25.

Artificial Intelligence Large Language Models Improve Patient Comprehension of 
Radiologist Magnetic Resonance Imaging Reports.

Berzolla E(1), Gosnell GG(2), Chen L(2), Vonck C(2), Alaia E(2), Meislin R(2).

Author information:
(1)Department of Orthopedic Surgery, NYU Langone Orthopedic Hospital, New York, 
New York, U.S.A.. Electronic address: Emily.Berzolla@nyulangone.org.
(2)Department of Orthopedic Surgery, NYU Langone Orthopedic Hospital, New York, 
New York, U.S.A.

PURPOSE: To assess whether an artificial intelligence (AI) translation of a 
magnetic resonance imaging (MRI) report improved patient understanding of the 
information presented in the radiology report and to evaluate patient 
preferences for AI translations over traditional radiology reports.
METHODS: Patients presenting to an orthopaedic surgery clinic were prospectively 
enrolled and randomized into 2 groups. The standard MRI group received a 
traditional MRI report on a multiligament knee injury written by a radiologist, 
whereas the AI group received an AI-translated version of the same report, 
generated using ChatGPT version 4. All patients completed a standardized quiz to 
assess comprehension of their respective reports. After the quiz, participants 
were provided with both reports and asked to rate their preferences between the 
two. Demographic information including age, sex, race, education level, area 
deprivation index, and orthopaedic history was collected from all patients.
RESULTS: A total of 64 patients (32 in each group) with an average age of 51.9 ± 
15.5 years were enrolled and randomized. No significant differences in 
demographic characteristics were identified between the 2 groups. Patients in 
the AI group scored 20% higher than those in the standard MRI group on the 
comprehension quiz (74.7% vs 54.7%, P < .001). Overall, 87.5% of patients 
preferred the AI translation whereas 4.7% preferred the standard version. 
Patients rated the AI translation as significantly clearer than the standard 
version (4.5 of 5 vs 2.2 of 5, P < .001), although they had less trust in the AI 
translation compared with the standard report (4.1 of 5 vs 4.5 of 5, P = .003). 
A higher education level was predictive of comprehension.
CONCLUSIONS: AI-translated MRI reports significantly improved patient 
comprehension and were preferred for their clarity, despite lower trust levels 
compared with standard radiology reports.
CLINICAL RELEVANCE: AI-translated MRI reports have the potential to enhance 
patient understanding of radiologic findings in orthopaedic care. However, given 
the low level of trust in AI-generated content observed in this study, physician 
oversight remains essential to ensure accuracy and sustain patient confidence.

Copyright © 2025 Arthroscopy Association of North America. Published by Elsevier 
Inc. All rights reserved.

DOI: 10.1016/j.arthro.2025.04.033
PMID: 40288466 [Indexed for MEDLINE]

Conflict of interest statement: Disclosures The authors declare the following 
financial interests/personal relationships which may be considered as potential 
competing interests: E.A. is a paid consultant for Arthrex, Biorez, Joint 
Restoration Foundation, and Mitek. R.M. is a paid consultant for Arthrex, ConMed 
Linvatec, and Mitek. All other authors (E.B., G.G.G., B.S., L.C., C.V.) declare 
that they have no known competing financial interests or personal relationships 
that could have appeared to influence the work reported in this paper.


709. J Am Med Inform Assoc. 2025 Nov 24:ocaf206. doi: 10.1093/jamia/ocaf206. Online 
ahead of print.

Gaps in artificial intelligence research for rural health in the United States: 
a scoping review.

Brown KE(1), Davis SE(1).

Author information:
(1)Department of Biomedical Informatics, Vanderbilt University Medical Center, 
Nashville, TN 37203, United States.

Update of
    medRxiv. 2025 Jun 27:2025.06.26.25330361. doi: 10.1101/2025.06.26.25330361.

OBJECTIVE: Artificial intelligence (AI) has impacted healthcare at urban and 
academic medical centers in the US. There are concerns, however, that the 
promise of AI may not be realized in rural communities. This scoping review aims 
to determine the extent of AI research in the rural US.
MATERIALS AND METHODS: We conducted a scoping review following the PRISMA 
guidelines. We included peer-reviewed, original research studies indexed in 
PubMed, Embase, and WebOfScience after January 1, 2010 and through April 29, 
2025. Studies were required to discuss the development, implementation, or 
evaluation of AI tools in rural US healthcare, including frameworks that help 
facilitate AI development (eg, data warehouses).
RESULTS: Our search strategy found 26 studies meeting inclusion criteria after 
full text screening with 14 papers discussing predictive AI models and 12 papers 
discussing data or research infrastructure. AI models most often targeted 
resource allocation and distribution. Few studies explored model deployment and 
impact. Half noted the lack of data and analytic resources as a limitation. None 
of the studies discussed examples of generative AI being trained, evaluated, or 
deployed in a rural setting.
DISCUSSION: Practical limitations may be influencing and limiting the types of 
AI models evaluated in the rural US. Validation of tools in the rural US was 
underwhelming.
CONCLUSION: With few studies moving beyond AI model design and development 
stages, there are clear gaps in our understanding of how to reliably validate, 
deploy, and sustain AI models in rural settings to advance health in all 
communities.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf206
PMID: 41285144


710. Gerontologist. 2025 May 10;65(6):gnaf113. doi: 10.1093/geront/gnaf113.

A Call for Integrated Approaches in Digital Technology Design for Aging and 
Disability.

Barton HJ(1), Valdez RS(2), Shew A(3), Swenor BK(4), Jolliff A(5), Claypool 
H(6), Czaja SJ(7), Werner NE(5).

Author information:
(1)BerbeeWalsh Department of Emergency Medicine, University of 
Wisconsin-Madison, Madison, Wisconsin, USA.
(2)Department of Systems and Information Engineering, Department of Public 
Health Sciences, University of Virginia, Charlottesville, Virginia, USA.
(3)Department of Science, Technology, and Society (STS), Virginia Tech, 
Blacksburg, Virginia, USA.
(4)Department of Epidemiology at the Johns Hopkins Bloomberg School of Public 
Health, Johns Hopkins Disability Health Research Center, Johns Hopkins School of 
Nursing, Wilmer Eye Institute at the Johns Hopkins School of Medicine, 
Baltimore, Maryland, USA.
(5)Department of Anesthesiology, Vanderbilt University School of Medicine, 
Center for Research and Innovation in Systems Safety, Vanderbilt University 
Medical Center, Nashville, Tennessee, USA.
(6)Lurie Institute for Disability Policy, Heller School for Social Policy and 
Management, Brandeis University, Waltham, Massachusetts, USA.
(7)Division of Geriatrics and Palliative Medicine, Center on Aging and 
Behavioral Research, Weill Cornell Medicine, New York, New York, USA.

The fields of aging and disability often proceed as 2 distinct lines of inquiry 
and action in terms of digital technology design. Guidelines and standards in 
both spaces (e.g., web content accessibility guidelines) have had suboptimal 
impact due to limited comprehensiveness enforcement mechanisms. Standards also 
rarely account for variations within the disability and aging communities and 
the structural power of ageism and ableism. These concerns proliferate in the 
context of contemporary technology discourse (e.g., data privacy, generative 
artificial intelligence). There is an opportunity to bridge both fields given 
that aging and disability can lead to distinct but overlapping experiences and 
technological needs and because of the multiple ways aging and disability may be 
simultaneously experienced. Joint efforts are essential to building the 
political power necessary to address current limitations and associated harms 
and to mitigate the risk of exacerbation associated with increasing 
technological pervasiveness and complexity. Joint efforts can also catalyze a 
paradigm shift from designing to address "deficits" to designs that are 
responsive to assets and the context of older adults' and disabled persons' full 
personhood. This paper reviews best practices for digital technology design 
across aging and disability fields and presents pathways forward toward 
comprehensive, enforceable standards.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
Gerontological Society of America.

DOI: 10.1093/geront/gnaf113
PMCID: PMC12150774
PMID: 40123375 [Indexed for MEDLINE]

Conflict of interest statement: None.


711. BMC Med Educ. 2025 Dec 1;26(1):12. doi: 10.1186/s12909-025-08358-2.

Comparison of ChatGPT-4o and expert evaluation in endodontic education: a 
cross-sectional pilot study.

Alpay S(1), Darafarin Y(2), Dagdelen B(2), Al-Shammari SMM(2), Buyukbayram 
IK(2).

Author information:
(1)Department of Endodontics, Faculty of Dentistry, Istanbul Aydın University, 
Istanbul, Türkiye. salpay@aydin.edu.tr.
(2)Department of Endodontics, Faculty of Dentistry, Istanbul Aydın University, 
Istanbul, Türkiye.

BACKGROUND: Artificial intelligence (AI) has the potential to enhance 
objectivity and scalability in educational assessment, yet its role in 
evaluating technical dental skills remains unclear. This study aimed to compare 
ChatGPT-4o based assessments with expert evaluations in undergraduate endodontic 
training and to explore student perceptions of AI-assisted feedback.
METHODS: This cross-sectional pilot study was conducted during the 2024-2025 
academic year with 32 dental students from a faculty of dentistry, who completed 
root canal treatments. Postoperative radiographs were evaluated by 10 years 
experienced endodontist and ChatGPT-4o was used to evaluate performance based on 
five standardized criteria: canal centering, homogeneity, procedural errors, 
apical shaping, and overall taper, each rated on a 5-point Likert scale. 
Inter-rater reliability was assessed via intraclass correlation coefficients 
(ICC), and Pearson correlation tested linear alignment. Students rated both 
feedback sources via Likert-scale questionnaires and open-ended comments; 
paired-sample t-tests compared the mean scores.
RESULTS: Agreement between AI and expert evaluation was limited, with ICC 
ranging from 0.36 to 0.45, indicating poor to moderate reliability. Pearson r 
values were < 0.3 and not statistically significant, demonstrating weak linear 
correlation. While students rated AI-generated feedback as moderately useful, 
expert feedback scored higher across educational value (mean 4.29 vs. 3.90), 
clinical reasoning support (4.19 vs. 4.06), and reliability (4.00 vs. 3.91); 
differences were not statistically significant. Notably, 53.1% of students 
preferred a combination of AI and expert feedback for optimal learning.
CONCLUSIONS: AI-generated feedback was moderately useful to students, but expert 
feedback consistently scored higher. For complex psychomotor skills and 
radiographic interpretation, AI should serve as an auxiliary tool rather than an 
independent assessor. Further validation of advanced multimodal AI systems and 
development of hybrid frameworks combining algorithmic objectivity with expert 
judgment are recommended.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-08358-2
PMCID: PMC12777446
PMID: 41327262 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was approved by the Ethics Committee of Istanbul Aydın 
University (approval number: 144/2025). All participating students provided 
informed consent, and patient-related data were anonymized in accordance with 
ethical research principles and institutional regulations. All participants 
provided written informed consent prior to participation in the study. This 
study was conducted in accordance with the principles of the Declaration of 
Helsinki. Consent for publication: Not applicable. Competing interests: The 
authors declare no competing interests. Declaration of generative AI and 
AI-assisted technologies in the writing process: During the preparation of this 
manuscript, the authors used ChatGPT-4 to improve the clarity and readability of 
the text. All generated suggestions were carefully reviewed and revised by the 
authors, who take full responsibility for the final content.


712. Eur Radiol. 2025 Apr;35(4):2030-2042. doi: 10.1007/s00330-024-11122-7. Epub 2024 
Oct 15.

ChatGPT as an effective tool for quality evaluation of radiomics research.

Mese I(1), Kocak B(2).

Author information:
(1)Department of Radiology, Erenkoy Mental Health and Neurology Training and 
Research Hospital, University of Health Sciences, Istanbul, Turkey.
(2)Department of Radiology, Basaksehir Cam and Sakura City Hospital, University 
of Health Sciences, Istanbul, Turkey. drburakkocak@gmail.com.

OBJECTIVES: This study aimed to evaluate the effectiveness of ChatGPT-4o in 
assessing the methodological quality of radiomics research using the radiomics 
quality score (RQS) compared to human experts.
METHODS: Published in European Radiology, European Radiology Experimental, and 
Insights into Imaging between 2023 and 2024, open-access and peer-reviewed 
radiomics research articles with creative commons attribution license (CC-BY) 
were included in this study. Pre-prints from MedRxiv were also included to 
evaluate potential peer-review bias. Using the RQS, each study was independently 
assessed twice by ChatGPT-4o and by two radiologists with consensus.
RESULTS: In total, 52 open-access and peer-reviewed articles were included in 
this study. Both ChatGPT-4o evaluation (average of two readings) and human 
experts had a median RQS of 14.5 (40.3% percentage score) (p > 0.05). Pairwise 
comparisons revealed no statistically significant difference between the 
readings of ChatGPT and human experts (corrected p > 0.05). The intraclass 
correlation coefficient for intra-rater reliability of ChatGPT-4o was 0.905 (95% 
CI: 0.840-0.944), and those for inter-rater reliability with human experts for 
each evaluation of ChatGPT-4o were 0.859 (95% CI: 0.756-0.919) and 0.914 (95% 
CI: 0.855-0.949), corresponding to good to excellent reliability for all. The 
evaluation by ChatGPT-4o took less time (2.9-3.5 min per article) compared to 
human experts (13.9 min per article by one reader). Item-wise reliability 
analysis showed ChatGPT-4o maintained consistently high reliability across 
almost all RQS items.
CONCLUSION: ChatGPT-4o provides reliable and efficient assessments of radiomics 
research quality. Its evaluations closely align with those of human experts and 
reduce evaluation time.
KEY POINTS: Question Is ChatGPT effective and reliable in evaluating radiomics 
research quality based on RQS? Findings ChatGPT-4o showed high reliability and 
efficiency, with evaluations closely matching human experts. It can considerably 
reduce the time required for radiomics research quality assessment. Clinical 
relevance ChatGPT-4o offers a quick and reliable automated alternative for 
evaluating the quality of radiomics research, with the potential to assess 
radiomics research at a large scale in the future.

© 2024. The Author(s), under exclusive licence to European Society of Radiology.

DOI: 10.1007/s00330-024-11122-7
PMID: 39406959 [Indexed for MEDLINE]

Conflict of interest statement: Compliance with ethical standards. Guarantor: 
The scientific guarantor of this publication is Burak Kocak, MD. Conflict of 
interest: B.K. is on the editorial board of European Radiology (section editor: 
Imaging Informatics and Artificial Intelligence). He has taken no part in this 
article’s peer review or selection. The authors of this manuscript declare no 
relationships with any companies whose products or services may be related to 
the subject matter of the article. Statistics and biometry: No complex 
statistical methods were necessary for this paper. Informed consent: 
Non-applicable. Ethical approval: Non-applicable. Study subjects or cohorts 
overlap: None. Methodology: Experimental


713. PLoS One. 2025 Aug 29;20(8):e0330160. doi: 10.1371/journal.pone.0330160. 
eCollection 2025.

How does public digital procurement enhance corporate total factor productivity? 
The role of industry-university-research collaboration.

Jia K(1)(2), Qin J(2), Li Y(3).

Author information:
(1)School of Management, Zhejiang University of Technology, Hangzhou, China.
(2)School of Public Administration, Zhejiang University of Technology, Hangzhou, 
China.
(3)School of Cultural Creativity and Management, Communication University of 
Zhejiang, Hangzhou, ChinaThese authors contributed equally to this work.

This study innovatively employs large language model (LLM) technology to 
construct a public digital procurement (PDP) index and, by utilizing microlevel 
data from Chinese A-share listed companies (2015-2023), systematically examines 
the impact of PDP on corporate total factor productivity (TFP) and its 
underlying mechanisms. The results demonstrate that PDP has a significantly 
positive effect on corporate TFP, and this conclusion remains robust after 
endogeneity concerns are addressed and multiple robustness tests are conducted. 
Mechanism analysis reveals that PDP significantly increases corporate TFP 
primarily through three pathways: enhancing digital technology innovation, 
alleviating financial constraints, and improving corporate information 
disclosure. Furthermore, in the context of industry-university-research (IUR) 
collaboration, PDP has a more pronounced positive effect on corporate TFP. 
Additional analysis indicates a synergistic effect between PDP and corporate 
digital transformation, meaning that firms with a higher level of digital 
transformation can more effectively leverage PDP to achieve TFP growth. This 
study provides critical theoretical and empirical evidence for leveraging PDP to 
improve corporate TFP and offers important references for government departments 
in optimizing the design of PDP policies and enhancing their implementation 
effectiveness.

Copyright: © 2025 Jia et al. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0330160
PMCID: PMC12396760
PMID: 40880528 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that they have no known 
competing financial interests or personal relationships that could have appeared 
to influence the work reported in this paper. Besides, we have read and 
understood your journal’s policies, and believe that neither the manuscript nor 
the study violates any of these. There are no conflicts of interest to declare.


714. J Med Internet Res. 2025 Nov 13;27:e79091. doi: 10.2196/79091.

Embracing the Future of Medical Education With Large Language Model-Based 
Virtual Patients: Scoping Review.

Zeng J(#)(1)(2), Qi W(#)(1)(2), Shen S(1), Liu X(1), Li S(1), Wang B(1), Dong 
C(1), Zhu X(1), Shi Y(1), Lou X(1), Wang B(1), Yao J(1), Jiang G(3), Zhang Q(4), 
Cao S(1)(2).

Author information:
(1)School of Public Health and Nursing, Hangzhou Normal University, Hangzhou, 
null, China.
(2)Zhejiang Provincial Research and Evaluation Center for Educational 
Modernization, Hangzhou, null, China.
(3)Department of Psychiatry and Neuropsychology and Alzheimer Center Limburg, 
School for Mental Health and Neuroscience (MHeNS), Maastricht University, 
Maastricht, The Netherlands.
(4)Department of Nursing, Zhejiang Provincial People's Hospital, Hangzhou, 
China.
(#)Contributed equally

BACKGROUND: In recent years, large language models (LLMs) have experienced rapid 
development. LLM-based virtual patients have begun to gain attention, offering 
new opportunities for simulations in medical education.
OBJECTIVE: This study aims to systematically analyze the current applications, 
research trends, and challenges of LLM-based virtual patients in medical 
education and to explore potential future directions for development.
METHODS: This study adheres to the PRISMA-ScR (Preferred Reporting Items for 
Systematic Reviews and Meta-Analyses extension for Scoping Reviews) guidelines. 
Five databases (Web of Science Core Collection, PubMed, IEEE Xplore, Embase, and 
Scopus) were searched from January 1, 2018, to June 24, 2025, to identify 
studies related to the application of LLM-based virtual patients in medical 
education. A comprehensive analysis of LLM-based virtual patients from research 
design to application and evaluation was conducted.
RESULTS: A total of 28 studies were included in this scoping review. Analysis 
revealed that 92.9% (26/28) of the studies were published in the past 2 years, 
indicating that LLM-based virtual patient research is still in its early stages. 
The research primarily focuses on medical training and spans a wide range of 
medical disciplines. When using LLMs, advanced technologies such as social 
robots, virtual reality, and mixed reality are used to present LLM-based virtual 
patients. Combining these technologies with various supplementary tools enhances 
the realism of LLM-based virtual patients and improves user interaction. The 
evaluation of LLM-based virtual patients mainly emphasizes user experience. 
However, evaluation methods lack standardization, and only 13% (3/23) of studies 
used validated tools in assessing LLM-based virtual patients, while only 21.7% 
(5/23) of studies objectively measured learning outcomes facilitated by 
LLM-based virtual patients. All included studies expressed a positive attitude 
toward LLM-based virtual patients; however, they overlook privacy and security 
considerations in practical applications.
CONCLUSIONS: LLM-based virtual patients hold significant innovation potential in 
medical education and are still in the early stages of development. They are 
primarily applied in medical training and show promise in communication skills 
training, although they cannot replace real-world interactions. Moreover, the 
heterogeneity of research designs, the absence of nonverbal cues in 
interactions, and concerns regarding privacy and security limit their broader 
implementation. Future research should focus on improving the reliability, 
realism, safety, and scientific efficacy of LLM-based virtual patients.
TRIAL REGISTRATION: Open Science Framework Registries 10.17605/OSF.IO/DMC9Q; 
https://osf.io/DMC9Q/overview.

©Jianwen Zeng, Wenhao Qi, Shiying Shen, Xin Liu, Sixie Li, Bing Wang, Chaoqun 
Dong, Xiaohong Zhu, Yankai Shi, Xiajing Lou, Bingsheng Wang, Jiani Yao, Guowei 
Jiang, Qiong Zhang, Shihua Cao. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org), 13.11.2025.

DOI: 10.2196/79091
PMCID: PMC12661241
PMID: 41232097 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


715. Gene. 2025 Sep 10;964:149623. doi: 10.1016/j.gene.2025.149623. Epub 2025 Jun 16.

Bridging BioSciences and technology: The impact of AI & GenAI in life sciences 
and agribusiness.

da Cunha NB(1), Fernandes FC(2), Gil-Ley A(3), Franco OL(3), Timakondu N(4), 
Costa FF(5).

Author information:
(1)Genomic Sciences and Biotechnology Program, Catholic University of Brasilia, 
SGAN 916 Modulo B, Bloco C, 70790-160 Brasília, DF, Brazil; Faculty of Agronomy 
and Veterinary Medicine (FAV), Campus Darcy Ribeiro, University of Brasilia 
(UnB), 70910-900 Brasília, DF, Brazil. Electronic address: nicolau.cunha@unb.br.
(2)Genomic Sciences and Biotechnology Program, Catholic University of Brasilia, 
SGAN 916 Modulo B, Bloco C, 70790-160 Brasília, DF, Brazil; Computer Science 
Department, Instituto Federal de Brasília (IFB), Brasília, DF, Brazil.
(3)Genomic Sciences and Biotechnology Program, Catholic University of Brasilia, 
SGAN 916 Modulo B, Bloco C, 70790-160 Brasília, DF, Brazil; S-Inova Biotech, 
Graduate Program in Biotechnology, Catholic University of Dom Bosco, Campo 
Grande, MT, Brazil.
(4)Cancer Biology and Epigenomics Program, Northwestern University's Feinberg 
School of Medicine, Chicago, IL 60611, USA; AIx4ALL, San Francisco Bay Area, CA 
94066, USA.
(5)Genomic Sciences and Biotechnology Program, Catholic University of Brasilia, 
SGAN 916 Modulo B, Bloco C, 70790-160 Brasília, DF, Brazil; Cancer Biology and 
Epigenomics Program, Northwestern University's Feinberg School of Medicine, 
Chicago, IL 60611, USA; AIx4ALL, San Francisco Bay Area, CA 94066, USA. 
Electronic address: fcosta@aix4all.com.

The intersection of biosciences and technology has yielded transformative 
advancements, and Generative Artificial Intelligence (GenAI) started to stand at 
the forefront of this synergy. In the field of life sciences, GenAI is emerging 
as a catalyst, accelerating drug discovery by swiftly generating and predicting 
novel molecules. This expedites the identification of potential drug candidates, 
significantly reducing time and costs compared to traditional methods. Beyond 
drug discovery, GenAI contributes to protein folding predictions, genomics 
research, disease diagnosis and biomarker identification, enhancing our 
understanding of diseases and health conditions, fostering the development of 
personalized medicine. In agribusiness, GenAI proves instrumental in optimizing 
crop breeding and improving agricultural productivity. It can generate new crop 
varieties with desired traits by analyzing vast datasets comprising genomic and 
ecological information, addressing challenges such as disease resistance, 
improved yield, and enhanced nutritional content. Moreover, GenAI transcends 
traditional applications and extends its influence on synthetic biology, 
contributing to the design of novel enzymes and pathways. This opens avenues for 
bio-based manufacturing, renewable energy production, and environmental 
remediation. By harnessing the power of GenAI, the synergies between biosciences 
and technology accelerate innovation, improve efficiency, decrease costs, and 
address critical challenges. Conversely, the ethical considerations surrounding 
GenAI, especially Large Language Model (LLM) utilization in life sciences and 
agribusiness, such as data privacy, algorithmic bias, and the equitable 
distribution of benefits, must be addressed to ensure responsible and fair 
implementation, especially environment sustainability when utilizing this 
technology. This review article discusses the multifaceted impact of GenAI in a 
new era of advancements in life sciences and agribusiness.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.gene.2025.149623
PMID: 40516836 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


716. IEEE J Biomed Health Inform. 2025 Sep;29(9):6238-6250. doi: 
10.1109/JBHI.2025.3542066.

AV-FOS: Transformer-Based Audio-Visual Multimodal Interaction Style Recognition 
for Children With Autism Using the Revised Family Observation Schedule 3rd 
Edition (FOS-R-III).

Zhao Z, Chung E, Chung KM, Park CH.

Challenging behaviors in children with autism is a serious clinical condition, 
oftentimes leading to aggression or self-injurious actions. The Revised Family 
Observation Schedule 3 rd Edition (FOS-R-III) is an intensive and fine-grained 
scale used to observe and analyze the behaviors of individuals with autism, 
which facilitates the diagnosis and monitoring of autism severity. Previous 
AI-based approaches for automated behavior analysis in autism often focused on 
predicting facial expressions and body movements without generating a clinically 
meaningful scale, mostly utilizing visual information. In this study, we propose 
a deep-learning based algorithm with audio-visual multimodal-data clinically 
coded with the FOS-R-III, named AV-FOS model. Our proposed AV-FOS model 
leverages transformer-based structure and self-supervised learning to 
intelligently recognize Interaction Styles (IS) in the FOS-R-III scale from 
subjects' video recordings. This enables the automatic generation of the 
FOS-R-III measures with clinically acceptable accuracy. We explore the IS 
recognition using a multimodal large language model, GPT4V, with prompt 
engineering provided with FOS-R-III measure definitions as the baseline for this 
study and compare with other vision-based deep learning algorithms. We believe 
this research represents a significant advancement in autism research and 
clinical accessibility. The proposed AV-FOS and our FOS-R-III dataset will serve 
as a gateway toward the digital health era for future AI models related to 
autism.

DOI: 10.1109/JBHI.2025.3542066
PMID: 40031833 [Indexed for MEDLINE]


717. J Med Internet Res. 2025 Jul 3;27:e53332. doi: 10.2196/53332.

Exploring Inflammatory Bowel Disease Discourse on Reddit Throughout the COVID-19 
Pandemic Using OpenAI's GPT-3.5 Turbo Model: Classification Model Validation and 
Case Study.

Babinski T(1)(2), Karley S(3), Cooper M(4), Shaik S(#)(2), Wang YK(#)(2).

Author information:
(1)Division of Gastroenterology, Hepatology, and Nutrition, Children's Hospital 
of Philadelphia, Philadelphia, PA, United States.
(2)Division of Management and Education, University of Pittsburgh at Bradford, 
Bradford, PA, United States.
(3)Division of Gastroenterology and Hepatology, University of Pennsylvania, 
Philadelphia, PA, United States.
(4)Department of Child and Adolescent Psychiatry and Behavioral Sciences, 
Children's Hospital of Philadelphia, Philadelphia, PA, United States.
(#)Contributed equally

BACKGROUND: Inflammatory bowel disease (IBD) is a chronic autoimmune disorder 
with an increasing prevalence in the general population. Internet-based 
communities have become vital for communication among patients with IBD, 
especially throughout the COVID-19 pandemic. However, these internet-based 
patient-to-patient communications remain largely underexplored.
OBJECTIVE: This study aims to analyze community posts from 3 of the largest IBD 
support groups on Reddit between March 1, 2020, and December 31, 2022, using a 
pretrained transformer model, and to validate the classification system's 
results via comparison to human scoring.
METHODS: We collected posts (N=53,333) from subreddits r/CrohnsDisease, 
r/UlcerativeColitis, and r/IBD and classified them using OpenAI's GPT-3.5 Turbo 
model to determine sentiment, categorize topics, and identify demographic 
information and mentions of the COVID-19 pandemic. A subset of posts (n=397) was 
manually scored to measure interrater agreement between human raters and the 
GPT-3.5 Turbo model.
RESULTS: Fleiss κ and Gwet AC1 coefficients indicated a high level of agreement 
between raters, with values ranging from 0.53 to 0.91. The raters demonstrated 
almost perfect agreement on the classification of gender, with a Fleiss κ of 
0.91 (P<.001). Medications (14,909/53,333) and symptoms (14,939/53,333) emerged 
as the most discussed topics, and most posts conveyed a neutral sentiment. While 
most users did not disclose their age, those who did primarily belonged to the 
20-29 years (2392/4828) and 30-39 years (859/4828) age groups. Based on 
self-reported gender, we identified 1509 men and 1502 women among our IBD Reddit 
users. When comparing the users on the IBD subreddits to the general IBD 
population, there was a significant difference in gender distribution 
(N=3,090,011; χ22=69.53; P<.001; φ<0.001). After an initial spike in posts 
within the first month, most posts did not reference the COVID-19 pandemic.
CONCLUSIONS: Our study showcases the potential of generative pretrained 
transformer models in processing and extracting insights from medical social 
media data. Future research can benefit from further subanalyses of our 
validated dataset or use OpenAI's model to analyze social media data for other 
conditions, particularly those for which patient experiences are challenging to 
collect.

©Tyler Babinski, Sara Karley, Marita Cooper, Salma Shaik, Y Ken Wang. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org), 
03.07.2025.

DOI: 10.2196/53332
PMCID: PMC12271966
PMID: 40607732 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


718. Diagnostics (Basel). 2025 Aug 11;15(16):2006. doi: 10.3390/diagnostics15162006.

Evaluating ChatGPT's Concordance with Clinical Guidelines of Ménière's Disease 
in Chinese.

Lin MJ(1), Hsieh LC(2)(3)(4), Chen CK(5)(6)(7).

Author information:
(1)Department of Medical Education, Chang Gung Memorial Hospital, Taoyuan 33305, 
Taiwan.
(2)Department of Otolaryngology-Head and Neck Surgery, Mackay Memorial Hospital, 
Taipei City 10449, Taiwan.
(3)Department of Audiology and Speech Language Pathology, Mackay Medical 
University, New Taipei 25245, Taiwan.
(4)Department of Medicine, Mackay Medical University, New Taipei 25245, Taiwan.
(5)Department of Otolaryngology-Head and Neck Surgery, Chang Gung Memorial 
Hospital, Keelung 20401, Taiwan.
(6)School of Traditional Chinese Medicine, College of Medicine, Chang Gung 
University, Taoyuan 33302, Taiwan.
(7)Department of Otolaryngology-Head and Neck Surgery, Chang Gung Memorial 
Hospital, Taoyuan 33305, Taiwan.

Background: Generative AI (GenAI) models like ChatGPT have gained significant 
attention in recent years for their potential applications in healthcare. This 
study evaluates the concordance of responses generated by ChatGPT (versions 3.5 
and 4.0) with the key action statements from the American Academy of 
Otolaryngology-Head and Neck Surgery (AAO-HNS) clinical practice guidelines 
(CPGs) for Ménière's disease translated into Chinese. Methods: Seventeen 
questions derived from the KAS were translated into Chinese and posed to ChatGPT 
versions 3.5 and 4.0. Responses were categorized as correct, partially correct, 
incorrect, or non-answers. Concordance with the guidelines was evaluated, and 
Fisher's exact test assessed statistical differences, with significance set at p 
< 0.05. Comparative analysis between ChatGPT 3.5 and 4.0 was performed. Results: 
ChatGPT 3.5 demonstrated an 82.4% correctness rate (14 correct, 2 partially 
correct, 1 non-answer), while ChatGPT 4.0 achieved 94.1% (16 correct, 1 
partially correct). Overall, 97.1% of responses were correct or partially 
correct. ChatGPT 4.0 offered enhanced citation accuracy and text clarity but 
occasionally included redundant details. No significant difference in 
correctness rates was observed between the models (p = 0.6012). Conclusions: 
Both ChatGPT models showed high concordance with the AAO-HNS CPG for MD, with 
ChatGPT 4.0 exhibiting superior text clarity and citation accuracy. These 
findings highlight ChatGPT's potential as a reliable assistant for better 
healthcare communication and clinical operations. Future research should 
validate these results across broader medical topics and languages to ensure 
robust integration of GenAI in healthcare.

DOI: 10.3390/diagnostics15162006
PMCID: PMC12385325
PMID: 40870857

Conflict of interest statement: The authors declare no conflicts of interest.


719. Best Pract Res Clin Obstet Gynaecol. 2025 Sep;102:102640. doi: 
10.1016/j.bpobgyn.2025.102640. Epub 2025 Jul 18.

Current and emerging data sources for assessment of ovarian toxicity in 
children, adolescents and young adults with cancer.

Strohl HB(1), Do NT(2), Irene Su H(3).

Author information:
(1)Department of Obstetrics, Gynecology, and Reproductive Sciences and Moores 
Cancer Center, University of California, 3855 Health Sciences Dr, Dept 0901, La 
Jolla, San Diego, CA, 92037, USA. Electronic address: hstrohl@health.ucsd.edu.
(2)Department of Obstetrics, Gynecology, and Reproductive Sciences and Moores 
Cancer Center, University of California, 3855 Health Sciences Dr, Dept 0901, La 
Jolla, San Diego, CA, 92037, USA. Electronic address: ntd005@health.ucsd.edu.
(3)Department of Obstetrics, Gynecology, and Reproductive Sciences and Moores 
Cancer Center, University of California, 3855 Health Sciences Dr, Dept 0901, La 
Jolla, San Diego, CA, 92037, USA. Electronic address: hisu@health.ucsd.edu.

One in 20 cancers occurs in children, adolescents, and young adults, with some 
treatments leading to infertility or premature ovarian insufficiency. Cancer 
survivors and clinicians seek to estimate reproductive risks to guide fertility 
preservation and manage ovarian health post-treatment. Available data focus more 
on surrogate outcomes like amenorrhea and ovarian reserve markers than clinical 
outcomes such as ovarian insufficiency. Tools like the Cancer-Related 
Infertility Score Predictor (CRISP) and FDA-recommended data sources, including 
the FDA Adverse Event Reporting System, provide guidance on known ovarian 
toxicity risks. However, many novel and current cancer treatments lack 
comprehensive data. Emerging strategies include using real-world administrative 
data linked with lab results to estimate risks, large language models to 
streamline systematic reviews, regulatory guidance requiring ovarian toxicity 
data in new drug trials, and ex vivo ovary models for testing. This review 
highlights the need for improved methods and consistent assessment to support 
the reproductive health of young cancer survivors.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.bpobgyn.2025.102640
PMID: 40700834 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest None.


720. Med Biol Eng Comput. 2025 Sep 27. doi: 10.1007/s11517-025-03437-4. Online ahead 
of print.

Generation of multimodal realistic computational phantoms as a test-bed for 
validating deep learning-based cross-modality synthesis techniques.

Camagni F(1), Nakas A(2), Parrella G(2), Vai A(3), Molinelli S(3), Vitolo V(4), 
Barcellini A(4)(5), Chalaszczyk A(4), Imparato S(4), Pella A(6), Orlandi 
E(4)(7), Baroni G(2)(6), Riboldi M(8), Paganelli C(2).

Author information:
(1)Department of Electronics, Information, and Bioengineering, Politecnico Di 
Milano, Milan, Italy. francesca.camagni@polimi.it.
(2)Department of Electronics, Information, and Bioengineering, Politecnico Di 
Milano, Milan, Italy.
(3)Medical Physics Unit, Clinical Department, CNAO National Center of 
Oncological Hadrontherapy, Pavia, Italy.
(4)Radiation Oncology Unit, Clinical Department, CNAO National Center of 
Oncological Hadrontherapy, Pavia, Italy.
(5)Department of Internal Medicine and Therapeutics, University of Pavia, Pavia, 
Italy.
(6)Bioengineering Unit, Clinical Department, CNAO National Center of Oncological 
Hadrontherapy, Pavia, Italy.
(7)Department of Clinical, Surgical, Diagnostic,and Pediatric Sciences, 
University of Pavia, Pavia, Italy.
(8)Department of Medical Physics, Ludwig-Maximilians-Universität München, 
Garching, Germany.

The validation of multimodal deep learning models for medical image translation 
is limited by the lack of high-quality, paired datasets. We propose a novel 
framework that leverages computational phantoms to generate realistic CT and MRI 
images, enabling reliable ground-truth datasets for robust validation of 
artificial intelligence (AI) methods that generate synthetic CT (sCT) from MRI, 
specifically for radiotherapy applications. Two CycleGANs (cycle-consistent 
generative adversarial networks) were trained to transfer the imaging style of 
real patients onto CT and MRI phantoms, producing synthetic data with realistic 
textures and continuous intensity distributions. These data were evaluated 
through paired assessments with original phantoms, unpaired comparisons with 
patient scans, and dosimetric analysis using patient-specific radiotherapy 
treatment plans. Additional external validation was performed on public CT 
datasets to assess the generalizability to unseen data. The resulting, paired 
CT/MRI phantoms were used to validate a GAN-based model for sCT generation from 
abdominal MRI in particle therapy, available in the literature. Results showed 
strong anatomical consistency with original phantoms, high histogram correlation 
with patient images (HistCC = 0.998 ± 0.001 for MRI, HistCC = 0.97 ± 0.04 for 
CT), and dosimetric accuracy comparable to real data. The novelty of this work 
lies in using generated phantoms as validation data for deep learning-based 
cross-modality synthesis techniques.

© 2025. The Author(s).

DOI: 10.1007/s11517-025-03437-4
PMID: 41015634

Conflict of interest statement: Declarations. Ethics approval: The study was 
approved by the local ethics committee, and all patients signed informed 
consent. Approval number CNAO 37–2019 4DMRI. Conflict of interest: The authors 
declare no competing interests.


721. J Allergy Clin Immunol. 2025 Sep 17:S0091-6749(25)00950-9. doi: 
10.1016/j.jaci.2025.09.002. Online ahead of print.

Proceedings of the second Artificial Intelligence in Primary Immunodeficiency 
(AIPI) meeting.

Rivière JG(1), Bastarache L(2), Campos LC(3), Carot-Sans G(4), Chin A(5), 
Chunara R(6), Cunningham-Rundles C(7), Erra L(8), Farmer J(9), Garcelon N(10), 
Hsieh E(11), Leavis H(12), Lee S(13), Liu L(14), Kusters M(15), Lloyd BC(16), 
Martinson AK(17), Mester R(18), Moore JB(19), Moshous D(20), Orange JS(21), 
Parrish N(22), Parker SH(23), Pasaniuc B(24), Peng XP(25), Pergent M(26), 
Piera-Jiménez J(4), Quinn J(22), Ramesh S(27), Roberts K(28), Robinson PN(29), 
Savova G(30), Scalchunes C(16), Seidel MG(31), Simoneau R(22), Soler-Palacin 
P(32), Sullivan KE(33), Van Gijn M(34), Wi CI(35), Zhou D(36), Tenembaum V(22), 
Butte MJ(5), Rider NL(37).

Author information:
(1)Universitat Autònoma de Barcelona, Barcelona, Spain; Infection and Immunity 
in Pediatric Patients Research Group, Vall d'Hebron Institut de Recerca, 
Barcelona, Spain; Pediatric Infectious Diseases and Immunodeficiencies Unit, 
Hospital Infantil i de La Dona Vall d'Hebron, Vall d'Hebron Barcelona Hospital 
Campus, Barcelona, Spain; Jeffrey Modell Diagnostic and Research Center for 
Primary Immunodeficiencies, Barcelona, Spain. Electronic address: 
jacques.riviere@uab.cat.
(2)Vanderbilt University Medical Center, Nashville, Tenn.
(3)University College London Institute of Immunity and Transplantation, London, 
United Kingdom.
(4)Catalan Health Service, Barcelona, Spain; Digitalization for the 
Sustainability of the Healthcare System (DS3) Research Group, Barcelona, Spain.
(5)Department of Pediatrics, Division of Immunology, Allergy, and Rheumatology, 
University of California, Los Angeles, Calif.
(6)School of Global Public Health and the Tandon School of Engineering, New York 
University, New York, NY.
(7)Division of Allergy and Immunology, Icahn School of Medicine at Mount Sinai, 
New York, NY.
(8)Departamento de Fisiología, Biología Molecular y Celular, Instituto de 
Biociencias, Biotecnología y Biología Traslacional (IB3) e Instituto de Química 
Biológica (IQUIBICEN), FCEN, UBA, CONICET, Buenos Aires, Argentina.
(9)Program in Clinical Immunodeficiency, Division of Allergy & Immunology, Beth 
Israel Lahey Health, Burlington, Mass.
(10)Data Science Platform, Université Paris Cité, Imagine Institute, INSERM UMR 
1163, Paris, France.
(11)Department of Pediatrics, Section of Allergy and Immunology, and the 
Department of Immunology and Microbiology, University of Colorado School of 
Medicine, Aurora, Colo.
(12)Department of Rheumatology and Clinical Immunology, University Medical 
Center Utrecht and Utrecht University, Utrecht, The Netherlands.
(13)Computational Health Informatics Program, Boston Children's Hospital, 
Boston, Mass.
(14)Fralin Biomedical Research Institute at VTC, Roanoke, Va; Graduate Program 
in Translational Biology, Medicine, and Health, Virginia Tech, Blacksburg, Va.
(15)Paediatric Immunology Department, Great Ormond Street Hospital for Children 
NHS Foundation Trust, London, United Kingdom.
(16)Immune Deficiency Foundation, Hanover, Md.
(17)Department of Pediatrics, Children's National Hospital, Washington, DC.
(18)Department of Computational Medicine, University of California, Los Angeles, 
Calif.
(19)Department of Implementation Science, Division of Public Health Sciences, 
Wake Forest University School of Medicine, Winston-Salem, NC.
(20)Pediatric Immunology, Hematology and Rheumatology, Necker-Enfants Malades 
University Hospital, Assistance Publique-Hôpitaux de Paris, Institut Imagine and 
Université Paris Cité, Paris, France.
(21)Department of Pediatrics, Columbia University Vagelos College of Physicians 
and Surgeons, Columbia University, New York, NY.
(22)Jeffrey Modell Foundation, New York, NY.
(23)Department of Health Systems and Implementation Science, Virginia Tech 
Carilion School of Medicine, Roanoke, Va.
(24)Department of Genetics, Perelman School of Medicine, University of 
Pennsylvania, Philadelphia, Pa.
(25)Division of Pediatric Genetic Medicine, Department of Pediatrics, Children's 
Hospital at Montefiore, Montefiore Medical Center, Albert Einstein College of 
Medicine, Bronx, NY.
(26)The International Patient Organisation for Primary Immunodeficiencies, 
Brussels, Belgium.
(27)Medblocks Inc, Dover, Del.
(28)McWilliams School of Biomedical Informatics, The University of Texas Health 
Science Center at Houston, Houston, Tex.
(29)Berlin Institute of Health, Charité-Universitätsmedizin Berlin, Berlin, 
Germany.
(30)Computational Health Informatics Program, Boston Children's Hospital, 
Harvard Medical School, Boston, Mass.
(31)Division of Pediatric Hematology and Oncology, Department of Pediatric and 
Adolescent Medicine, Styrian Children's Cancer Research Unit for Cancer and 
Inborn Errors of the Blood and Immunity in Children, Medical University of Graz, 
Graz, Austria.
(32)Universitat Autònoma de Barcelona, Barcelona, Spain; Infection and Immunity 
in Pediatric Patients Research Group, Vall d'Hebron Institut de Recerca, 
Barcelona, Spain; Pediatric Infectious Diseases and Immunodeficiencies Unit, 
Hospital Infantil i de La Dona Vall d'Hebron, Vall d'Hebron Barcelona Hospital 
Campus, Barcelona, Spain; Jeffrey Modell Diagnostic and Research Center for 
Primary Immunodeficiencies, Barcelona, Spain.
(33)Division of Allergy Immunology, The Children's Hospital of Philadelphia, 
Philadelphia, Pa.
(34)Department of Human Genetics, Amsterdam University Medical Center, 
Amsterdam, The Netherlands.
(35)Mayo Clinic Children, Mayo Clinic Health System Research, Mayo Clinic, 
Rochester, Minn.
(36)Department of Computer Science, Virginia Tech, Blacksburg, Va.
(37)Department of Health Systems and Implementation Science, Virginia Tech 
Carilion School of Medicine, Section of Allergy-Immunology, The Carilion Clinic, 
Roanoke, Va.

The use of artificial intelligence (AI) in inborn errors of immunity offers 
transformative potential in diagnostics and disease management but faces 
multiple challenges that were discussed at the second Artificial Intelligence in 
Primary Immunodeficiency conference, held in New York City (March 19-22, 2025). 
The conference addressed 7 themes: predictive diagnostic algorithms, health 
equity, industry collaboration, advanced computational tools like large language 
models, patient-led AI initiatives, multiomics integration, and implementation 
science. Discussions highlighted the growing impact of AI on diagnostics, 
genomics, and health systems, emphasizing the need for high-quality, diverse 
datasets and ethical safeguards to ensure equitable application. Participants 
stressed that AI alone cannot resolve systemic inequities or delays in 
diagnosis. Challenges such as the lack of harmonized datasets, the complexity of 
integrating multiomics data, ethical concerns, and the difficulty of adapting 
solutions to low-resource settings were emphasized. Additionally, the use 
implementation science was pointed out as one of the major challenges to ensure 
applicability and scalability in real-world settings. This requires overcoming 
resistance to adoption, addressing infrastructure gaps, and ensuring regulatory 
compliance. Collaboration across academia, clinicians, patients, regulators, and 
industry is essential to ensure AI delivers equitable, lasting benefits for 
individuals with inborn errors of immunity.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jaci.2025.09.002
PMID: 40972982

Conflict of interest statement: Disclosure statement J. G. Rivière reports, 
outside the current report, speaker bureau honoraria and/or funded research from 
Takeda, CSL Behring, Pharming, and Grifols. R. Chunara reports, outside the 
current report, funding from the National Science Foundation (grant 1845487), 
the National Institutes of Health (NIH), and Optum. C. Cunningham-Rundles 
reports, outside the current report, consulting for Guidepost; advisory 
committee roles for Pharming and X4 Pharmaceuticals; participation in the 
Grifols Aspire Program; service on data safety monitoring boards for Grifols and 
Otsuka; and grant review activities for Grifols. J. Farmer reports, outside the 
current report, consulting for Pharming; receipt of investigator-initiated 
research grants from Pfizer, Bristol Myers Squibb, and Pharming; and service as 
associate editor for a Springer publication. E. Hsieh, outside the current 
report, has received funds from the Boettcher Foundation Webb-Waring Biomedical 
Research Grant, Lupus Research Alliance and Lupus Innovation Award, and Global 
Team Science Award; and has received consulting fees from Sumitomo Pharma of 
America Inc. S. Lee reports, outside the current report, funding from the 
National Institute on Minority Health and Health Disparities. J. B. Moore 
reports, outside the current report, consulting services for Medtronic; service 
on the scientific advisory board of Heali AI; and research funding from NIH, de 
Beaumont Foundation, Centers for Disease Control and Prevention (CDC), and 
Childhood Arthritis and Rheumatology Association. J. S. Orange reports, outside 
the current report, consulting for Takeda, Grifols, and ADMA Biologics. X. Peng 
reports, outside the current report, consulting for Genesis Therapeutics and 
MedZown; and participation in the speaker bureau for Pharming. G. Savova 
reports, outside the current report, research funding from NIH (U2CA248010, 
R01LM013486, U24CA253539) and European Molecular Biology Laboratory–European 
Bioinformatics Institute Core Funds. P. Robinson reports, outside the current 
report, funding from the National Institutes of Health (NIH) (NIH/NHGRI 
5U24HG011449). K. Roberts reports, outside the current report, funding from the 
National Institutes of Health (NIH) (NIH R21AI164100 and R01LM014508). M. G. 
Seidel reports, outside the current report, receipt of advisory board honoraria 
from Pharming. P. Soler-Palacín reports, outside the current report, speaker 
bureau honoraria and/or funded research from Takeda, Pharming, CSL Behring, and 
Grifols. M. Butte reports, outside the current report, sponsored research from 
Pharming and X4 Pharmaceuticals; advisory board membership with ADMA Biologics; 
and speaker and advisory roles with Grifols; and is also founder of Rarefied 
Biosciences. N. L. Rider reports, outside the current report, receipt of support 
from National Institute of Allergy and Infectious Diseases, NIH (R21AI164100) 
and the CDC (CD-23-0057) through the Jeffrey Modell Foundation; a grant from 
Takeda (GRN-009472); consultancy for Takeda, Pharming, Grifols, and X4 
Pharmaceuticals; and receipt of royalties from Wolters Kluwer for contributions 
to UpToDate. The rest of the authors declare that they have no relevant 
conflicts of interest.


722. J Med Internet Res. 2025 Aug 18;27:e77893. doi: 10.2196/77893.

Magnitude and Impact of Hallucinations in Tabular Synthetic Health Data on 
Prognostic Machine Learning Models: Validation Study.

Pilgram L(1)(2)(3), El Kababji S(2), Liu D(1)(2), El Emam K(1)(2).

Author information:
(1)School of Epidemiology and Public Health, Faculty of Medicine, University of 
Ottawa, Ottawa, ON, Canada.
(2)CHEO Research Institute, Children's Hospital of Eastern Ontario, Ottawa, ON, 
Canada.
(3)Department of Nephrology and Medical Intensive Care, Charité - 
Universitaetsmedizin Berlin, Berlin, Germany.

BACKGROUND: Generative artificial intelligence (AI) for tabular synthetic data 
generation (SDG) has significant potential to accelerate health care research 
and innovation. A critical limitation of generative AI, however, is 
hallucinations. Although this has been commonly observed in text-generating 
models, it may also occur in tabular SDG.
OBJECTIVE: This study aims to investigate the magnitude of hallucinations in 
tabular synthetic data, whether their frequency increases with training data 
complexity, and the extent to which they impact the utility of synthetic data 
for downstream prognostic machine learning (ML) modeling tasks.
METHODS: On the basis of 12 large and high-dimensional real-world health care 
datasets, 6354 training datasets of different complexity were created by varying 
the subset of variables included in each dataset. Synthetic data were generated 
using 7 different SDG models. Hallucinations were defined as synthetic records 
that did not exist in the population, and the hallucination rate (HR) was the 
proportion of hallucinations in a synthetic dataset. Classification was the 
downstream prognostic modeling task, conducted via an ML approach (light 
gradient boosted machine) and an artificial neural network (multilayer 
perceptron). Mixed-effects models were fitted to examine the relationship 
between training data complexity and the HR and the HR and the predictive 
performance of AI and ML models when trained on the synthetic data.
RESULTS: The HR ranged from 0.3% to 100% (median 99.1%, IQR 98.5%-100.0%) and 
increased with training data complexity. However, in most SDG models, the HR did 
not affect AI and ML prognostic model performance. In the SDG models in which a 
significant association was detected, the estimated effect was very small, with 
a maximum decrease in the area under the receiver operating characteristic curve 
of -0.0002 (95% CI -0.0003 to -0.0002, P<.001) in light gradient boosting 
machine and -0.0001 (95% CI -0.0002 to -0.0001, P=.002) in multilayer 
perceptron.
CONCLUSIONS: These findings suggest that while hallucinations may be very common 
in synthetic tabular health data, they do not necessarily impair its utility for 
prognostic modeling.

©Lisa Pilgram, Samer El Kababji, Dan Liu, Khaled El Emam. Originally published 
in the Journal of Medical Internet Research (https://www.jmir.org), 18.08.2025.

DOI: 10.2196/77893
PMCID: PMC12402739
PMID: 40825542 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: At the time the study was 
conducted KEE was the scholar in residence at the office of the Information and 
Privacy Commissioner of Ontario and held shares in Aetion, which provided the 
sequential synthesis generative model software that was used in this study. At 
the time of publication both of these conflicts are no longer in effect. At the 
time of publication, KEE is the Editor-in-Chief of JMIR AI


723. J Biomed Inform. 2025 Dec;172:104949. doi: 10.1016/j.jbi.2025.104949. Epub 2025 
Nov 1.

Scalable scientific interest profiling using large language models.

Liang Y(1), Zhang G(2), Sun E(3), Idnay B(2), Fang Y(2), Chen F(2), Ta C(2), 
Peng Y(4), Weng C(5).

Author information:
(1)Tandon School of Engineering, New York University, Brooklyn, NY, USA.
(2)Department of Biomedical Informatics, Columbia University, New York, NY, USA.
(3)Henry Samueli School of Engineering and Applied Science, University of 
California, Los Angeles, CA, USA.
(4)Department of Population Health Sciences, Weill Cornell Medicine, New York, 
NY, USA. Electronic address: yip4002@med.cornell.edu.
(5)Department of Biomedical Informatics, Columbia University, New York, NY, USA. 
Electronic address: cw2384@cumc.columbia.edu.

Update of
    ArXiv. 2025 Aug 19:arXiv:2508.15834v1.

OBJECTIVE: Research profiles highlight scientists' research focus, enabling 
talent discovery and fostering collaborations, but they are often outdated. 
Automated, scalable methods are urgently needed to keep these profiles current.
METHODS: In this study, we design and evaluate two Large Language Models 
(LLMs)-based methods to generate scientific interest profiles-one summarizing 
researchers' PubMed abstracts and the other generating a summary using their 
publications' Medical Subject Headings (MeSH) terms-and compare these 
machine-generated profiles with researchers' self-summarized interests. We 
collected the titles, MeSH terms, and abstracts of PubMed publications for 595 
faculty members affiliated with Columbia University Irving Medical Center 
(CUIMC), for 167 of whom we obtained human-written online research profiles. 
Subsequently, GPT-4o-mini, a state-of-the-art LLM, was prompted to summarize 
each researcher's interests. Both manual and automated evaluations were 
conducted to characterize the similarities and differences between the 
machine-generated and self-written research profiles.
RESULTS: The similarity study showed low ROUGE-L, BLEU, and METEOR scores, 
reflecting little overlap between terminologies used in machine-generated and 
self-written profiles. BERTScore analysis revealed moderate semantic similarity 
between machine-generated and reference summaries (F1: 0.542 for MeSH-based, 
0.555 for abstract-based), despite low lexical overlap. In validation, 
paraphrased summaries achieved a higher F1 of 0.851. A further comparison 
between the original and paraphrased manually written summaries indicates the 
limitations of such metrics. Kullback-Leibler (KL) Divergence of term 
frequency-inverse document frequency (TF-IDF) values (8.56 and 8.58 for profiles 
derived from MeSH terms and abstracts, respectively) suggests that 
machine-generated summaries employ different keywords than human-written 
summaries. Manual reviews further showed that 77.78% rated the overall 
impression of MeSH-based profiling as "good" or "excellent," with readability 
receiving favorable ratings in 93.44% of cases, though granularity and factual 
accuracy varied. Overall, panel reviews favored 67.86% of machine-generated 
profiles derived from MeSH terms over those derived from abstracts.
CONCLUSION: LLMs promise to automate scientific interest profiling at scale. 
Profiles derived from MeSH terms have better readability than profiles derived 
from abstracts. Overall, machine-generated summaries differ from human-written 
ones in their choice of concepts, with the latter initiating more novel ideas.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104949
PMCID: PMC12705189
PMID: 41177243 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


724. BMC Med Educ. 2025 Jul 14;25(1):1048. doi: 10.1186/s12909-025-07427-w.

The application of problem-based learning (PBL) guided by ChatGPT in clinical 
education in the Department of Nephrology.

Tong X(1)(2)(3), Hu Y(1)(2)(3), Long Y(1)(2)(3), Zhang Q(1)(2)(3), Yang 
Y(1)(2)(3), Yuan J(1)(2)(3), Zha Y(4)(5)(6).

Author information:
(1)Department of Nephrology, Guizhou Provincial People's Hospital, Guiyang, 
China.
(2)NHC Key Laboratory of Pulmonary Immunological Disease, Guizhou Provincial 
People's Hospital, Guiyang, China.
(3)Medical College, Guizhou University, Guiyang, China.
(4)Department of Nephrology, Guizhou Provincial People's Hospital, Guiyang, 
China. zhayan72@126.com.
(5)NHC Key Laboratory of Pulmonary Immunological Disease, Guizhou Provincial 
People's Hospital, Guiyang, China. zhayan72@126.com.
(6)Medical College, Guizhou University, Guiyang, China. zhayan72@126.com.

BACKGROUND: Nephrology, a complex and specialized medical field, has seen 
significant advancements, yet traditional teaching methods remain outdated and 
less effective. This study explores the integration of Problem-Based Learning 
(PBL) guided by ChatGPT in the Department of Nephrology at Guizhou Provincial 
People's Hospital to enhance medical education.
OBJECTIVE: To assess the impact of ChatGPT-guided Problem-Based Learning (PBL) 
on medical students' education and satisfaction in the Nephrology Department.
METHODS: Fifty-four clinical resident doctors were divided into an experimental 
group, using ChatGPT-guided PBL, and a control group, using traditional methods. 
Both groups were assessed through theoretical and clinical practice exams, 
teaching satisfaction surveys, and self-assessments of teaching effectiveness.
RESULTS: Results indicated the experimental group achieved significantly higher 
scores in both theoretical and clinical assessments. Additionally, they reported 
higher satisfaction and effectiveness in learning. Despite spending more time in 
pre-class preparation, the overall learning time did not increase due to reduced 
post-class review time facilitated by ChatGPT.
CONCLUSIONS: This study demonstrates the potential of ChatGPT-integrated PBL to 
enhance learning outcomes, satisfaction, and efficiency in medical education. 
Future research should include larger samples and longer follow-up to validate 
these findings.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-07427-w
PMCID: PMC12257740
PMID: 40660193 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study (including the experimental protocol and its 
implementation details) was approved by the Ethics Committee of Guizhou 
Provincial People’s Hospital (approval number: 2024–015). The survey was 
administered to medical students at Guizhou Provincial People’s Hospital as part 
of their course. Informed consent was obtained, and participation was entirely 
voluntary. Consent for publication: Not applicable. Competing interests: The 
authors declare no competing interests.


725. Eur J Radiol. 2025 Dec;193:112416. doi: 10.1016/j.ejrad.2025.112416. Epub 2025 
Sep 13.

Chat GPT-4 shows high agreement in MRI protocol selection compared to 
board-certified neuroradiologists.

Bendella Z(1), Wichtmann BD(2), Clauberg R(3), Keil VC(4), Lehnen NC(2), Haase 
R(2), Sáez LC(5), Wiest IC(6), Kather JN(7), Endler C(8), Radbruch A(2), Paech 
D(9), Deike K(10).

Author information:
(1)Clinic of Neuroradiology, University Hospital Bonn, Bonn, Germany; German 
Center for Neurodegenerative Diseases (DZNE), Bonn, Germany. Electronic address: 
zeynep.bendella@ukbonn.de.
(2)Clinic of Neuroradiology, University Hospital Bonn, Bonn, Germany; German 
Center for Neurodegenerative Diseases (DZNE), Bonn, Germany.
(3)Clinic of Neuroradiology, University Hospital Bonn, Bonn, Germany.
(4)Department of Radiology & Nuclear Medicine, Amsterdam UMC, Vrije 
Universiteit, Cancer Center Amsterdam, Amsterdam, the Netherlands.
(5)Department of Radiology & Nuclear Medicine, Amsterdam UMC, Vrije 
Universiteit, Cancer Center Amsterdam, Amsterdam, the Netherlands; Hospital 
Universitario Son Llátzer (HUSLL), Palma, Mallorca, Spain.
(6)Department of Medicine II, Medical Faculty Mannheim, Heidelberg University, 
Mannheim, Germany; Else Kroener Fresenius Center for Digital Health, Medical 
Faculty Carl Gustav Carus, Technical University Dresden, Dresden, Germany.
(7)Else Kroener Fresenius Center for Digital Health, Medical Faculty Carl Gustav 
Carus, Technical University Dresden, Dresden, Germany.
(8)Department of Diagnostic and Interventional Radiology, University Hospital 
Bonn, Bonn, Germany.
(9)Clinic of Neuroradiology, University Hospital Bonn, Bonn, Germany; Department 
of Radiology, Brigham and Womeńs Hospital, Harvard Medical School, Boston, MA, 
USA.
(10)Clinic of Neuroradiology, University Hospital Bonn, Bonn, Germany; Athinoula 
A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, 
Charlestown, MA, USA.

OBJECTIVES: The aim of this study was to determine whether ChatGPT-4 can 
correctly suggest MRI protocols and additional MRI sequences based on real-world 
Radiology Request Forms (RRFs) as well as to investigate the ability of 
ChatGPT-4 to suggest time saving protocols.
MATERIAL & METHODS: Retrospectively, 1,001 RRFs of our Department of 
Neuroradiology (in-house dataset), 200 RRFs of an independent Department of 
General Radiology (independent dataset) and 300 RRFs from an external, foreign 
Department of Neuroradiology (external dataset) were included. Patients' age, 
sex, and clinical information were extracted from the RRFs and used to prompt 
ChatGPT- 4 to choose an adequate MRI protocol from predefined institutional 
lists. Four independent raters then assessed its performance. Additionally, 
ChatGPT-4 was tasked with creating case-specific protocols aimed at saving time.
RESULTS: Two and 7 of 1,001 protocol suggestions of ChatGPT-4 were rated 
"unacceptable" in the in-house dataset for reader 1 and 2, respectively. No 
protocol suggestions were rated "unacceptable" in both the independent and 
external dataset. When assessing the inter-reader agreement, Coheńs weighted ĸ 
ranged from 0.88 to 0.98 (each p < 0.001). ChatGPT-4's freely composed protocols 
were approved in 766/1,001 (76.5 %) and 140/300 (46.67 %) cases of the in-house 
and external dataset with mean time savings (standard deviation) of 3:51 
(minutes:seconds) (±2:40) minutes and 2:59 (±3:42) minutes per adopted in-house 
and external MRI protocol.
CONCLUSION: ChatGPT-4 demonstrated a very high agreement with board-certified 
(neuro-)radiologists in selecting MRI protocols and was able to suggest approved 
time saving protocols from the set of available sequences.

Copyright © 2025 The Author(s). Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ejrad.2025.112416
PMID: 40961911 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: KD, DP, RH and AR are co-founder 
and shareholder of the relios.vision GmbH. BDW has received speaker honoraria 
from Philips Healthcare.


726. J Clin Nurs. 2025 Oct 2. doi: 10.1111/jocn.70127. Online ahead of print.

The Effect of Artificial Intelligence in Promoting Positive Nursing Practice 
Environments: Mixed Methods Systematic Review.

Pereira SCA(1)(2)(3), Ferreira RA(4), Ventura-Silva JM(3)(5), Santos 
EJF(6)(7)(8), Fassarella CS(4), Ribeiro OMPL(2)(3).

Author information:
(1)Abel Salazar Biomedical Sciences Institute, University of Porto, Porto, 
Portugal.
(2)Nursing School of the University of Porto, Porto, Portugal.
(3)RISE-Health, Porto University, Porto, Portugal.
(4)Faculty of Nursing of the State University of Rio de Janeiro, Rio de Janeiro, 
Brazil.
(5)Red Cross Northern School of Health, Oliveira de Azemeis, Portugal.
(6)Polytechnic University of Viseu, School of Health, Viseu, Portugal.
(7)Health Sciences Research Unit: Nursing (UICISA: E), Coimbra, Portugal.
(8)Portugal Centre for Evidence Based Practice (PCEBP): A JBI Centre of 
Excellence, Coimbra, Portugal.

AIM: To synthesise the available evidence on the effect of artificial 
intelligence in promoting positive nursing practice environments, exploring 
outcomes for professionals, clients, and institutions.
BACKGROUND: Artificial intelligence has undergone significant advancements and 
shows great potential to transform nursing practice. However, this technological 
evolution is not without challenges, which must be identified and addressed.
METHODS: A systematic mixed-methods review following the PRISMA 2020 guidelines 
and the methodology proposed by JBI. The search strategy was conducted in 
PubMed, CINAHL, Scopus, and Web of Science, including grey literature. 
Quantitative, qualitative, and mixed-methods studies were included, and the 
selection process involved screening by two independent reviewers, who assessed 
all studies, their methodological quality and extracted their data.
RESULTS: From the conducted search, 11 studies were included, addressing how 
artificial intelligence has transformed nursing practice by optimising 
decision-making, task execution, and patient safety. Artificial intelligence, 
through predictive models and assistants such as ChatGPT, can enhance nursing 
management. However, challenges such as privacy concerns, resistance to change, 
and the need for professional training must be addressed to maximise its 
effectiveness.
CONCLUSION: Artificial intelligence has the potential to positively transform 
the nursing practice environment, optimising decision-making, enhancing patient 
safety, and improving operational efficiency, with clear benefits for 
professionals, patients, and healthcare institutions.
RELEVANCE TO CLINICAL PRACTICE: This study analysed the impact of artificial 
intelligence on nursing, highlighting improvements in clinical decision-making, 
patient safety, and institutional efficiency. Despite the identified benefits, 
the implementation of artificial intelligence in nursing is not without 
challenges and risks, which must be identified and addressed to ensure safe and 
effective adoption.
REPORTING METHOD: The review followed the PRISMA 2020 checklist.
PATIENT OR PUBLIC CONTRIBUTION: No.

© 2025 The Author(s). Journal of Clinical Nursing published by John Wiley & Sons 
Ltd.

DOI: 10.1111/jocn.70127
PMID: 41036770


727. IEEE J Biomed Health Inform. 2025 May;29(5):3642-3653. doi: 
10.1109/JBHI.2025.3529682. Epub 2025 May 6.

Decision Tree Extraction for Clinical Decision Support System With If-Else 
Pseudocode and PlanSelect Strategy.

Hou R, Wang X, Zhang W, Song Z, Wang K, Chen Y, Liu J, Ruan T.

Decision trees, as a structured representation of medical knowledge, are 
critical resources for building clinical decision support systems. Their 
structured decision pathways can be used for retrieval to enhance clinical 
decision making. Currently, mainstream methods mainly utilize large language 
models and in-context learning for decision tree extraction. However, these 
methods often face challenges in understanding the structure of decision trees 
and accurately extracting the complete content of tree nodes, leading to noise 
in the extracted trees and ultimately impacting their effectiveness in clinical 
decision support system. To this end, in this paper, we propose a novel decision 
tree extraction framework, including two stages. In the first stage, we propose 
to use the If-Else pseudocode to represent the decision tree structure and 
design specific constraints on format and content to guide the LLM in generating 
outputs. In the second stage, we introduce a novel node-filling strategy called 
PlanSelect to match the extracted triplets with sub-sentences in the generated 
pseudocode, including four reasoning steps: observation, plan, action, and 
answer. To evaluate the effectiveness of our proposed method, we construct an 
English decision tree extraction dataset (EMDT) and conduct extensive 
experiments on the built and public datasets. Experiments on the Text2DT and 
EMDT datasets demonstrate that our method outperforms the current 
state-of-the-art approaches, achieving improvements of 1.37% and 1.54% on the 
$ER$ metric (which is lower is better), respectively. Furthermore, we use the 
medical decision trees extracted using our framework to improve the model's 
performance on clinical decision making tasks, i.e., CMB-Clin and MedQA.

DOI: 10.1109/JBHI.2025.3529682
PMID: 40030896 [Indexed for MEDLINE]


728. J Crohns Colitis. 2025 Aug 12:jjaf134. doi: 10.1093/ecco-jcc/jjaf134. Online 
ahead of print.

Results of the 9th Scientific Workshop of the European Crohn's and Colitis 
Organisation (ECCO): Artificial Intelligence in medical management and precision 
medicine.

Kopylov U(1)(2), Verstockt B(3), Marigorta UM(4), Noviello D(5), Bossuyt P(6), 
Mookhoek A(7), Sinonque P(3), El-Hussuna A(8), Sahnan K(9), Baumgart DC(10), 
Noor NM(11), Allocca M(12), Carter D(1)(2), Ensari A(13), Iacucci M(14), Pellino 
G(15), Soriano A(16), de Laffolie J(17), Daperno M(18), Raine T(11), Cleynen 
I(19), Sebastian S(20)(21).

Author information:
(1)Department of Gastroenterology, Sheba Medical Center, Tel Hashomer, Israel.
(2)Faculty of Medicine, Tel Aviv University, Israel.
(3)Department of Gastroenterology, KU Leuven, Leuven, Belgium.
(4)Integrative Genomics Lab, Center for Cooperative Research in Biosciences (CIC 
bioGUNE), Basque Research and Technology Alliance (BRTA), Bizkaia Technology 
Park, Derio, Basque Country, and Ikerbasque, Basque Foundation for Science, 
48013 Bilbao, Bizkaia, Spain.
(5)Department of Pathophysiology and Transplantation, University of Milan, 
Milan, Italy.
(6)Department of Gastroenterology, Imelda General Hospital, Bonheiden, Belgium.
(7)Institute of Tissue Medicine and Pathology, University of Bern, Bern, 
Switzerland.
(8)OpenSourceResearch organisation, Aalborg, Denmark.
(9)St Marks Hospital, London, United Kingdom.
(10)Division of Gastroenterology and Hepatology, University of Alberta, 
Edmonton, Alberta, Canada.
(11)Department of Gastroenterology, Cambridge University Hospitals NHS 
Foundation Trust, Cambridge, United Kingdom.
(12)IRCCS Hospital San Raffaele and University Vita-Salute San Raffaele, 
Gastroenterology and Endoscopy, Milan, Italy.
(13)Department of Pathology, Ankara University, Ankara, Turkey.
(14)APC Microbiome Ireland, College of Medicine and Health, University College 
of Cork, Cork, Ireland.
(15)Department of Surgery, University of Campania "Luigi Vanvitelli," Naples, 
Italy.
(16)Department of Internal Medicine, Arcispedale Santa Maria Nuova, IRCCS, 
Reggio Emilia, Italy.
(17)Department General Paediatrics and Neonatology, University Giessen, Giessen, 
Germany.
(18)Gastroenterology Unit, AO Ordine Mauriziano, Turin, Italy.
(19)Department of Human Genetics, KU Leuven, Leuven, Belgium.
(20)IBD Unit, Department of Gastroenterology, Hull University Teaching Hospitals 
NHS Trust, Hull, United Kingdom.
(21)Hull York Medical School, Hull, United Kingdom.

BACKGROUND AND AIMS: Artificial intelligence (AI) is increasingly being applied 
in various fields of medicine, including Inflammatory Bowel Diseases (IBD). This 
systematic review, conducted as part of the ECCO 9th Scientific Workshop on AI 
in IBD, explores AI applications in multiomic precision medicine, large language 
models (LLMs) for textual tasks and utilisation of wearable and remote care 
technologies.
METHODS: A comprehensive systematic analysis of the literature was undertaken, 
emphasising three topics: multiomic predictive models in IBD; natural language 
processing (NLP) and LLMs for clinical practice, research and patient 
communication; and the role of remote monitoring and wearable devices.
RESULTS: Key areas of promise include the implementation of NLP and LLMs for 
case identification and differentiation, tracking disease activity, 
pharmacovigilance, quality assurance and patient support. Multiomic approaches, 
integrating genomics, transcriptomics, proteomics, metabolomics and 
metagenomics, show potential for developing more accurate diagnostic and risk 
prediction models and improving treatment response prediction and detection of 
actionable drug targets for future therapeutics. Wearables and remote monitoring 
technologies can transform IBD management from episodic assessments to 
continuous less biased tracking of patient-reported outcomes and physiological 
biomarkers.
CONCLUSIONS: While AI and multiomic approaches hold substantial promise for 
advancing IBD management and research, further refinement is necessary to ensure 
content validity and address safety concerns, thereby allowing integration of AI 
into clinical workflows and safeguarding of data privacy. Future research should 
prioritise the integration of diverse omic data, conduct of longitudinal studies 
and validation in large and diverse cohorts.

© The Author(s) 2025. Published by Oxford University Press on behalf of European 
Crohn’s and Colitis Organisation. All rights reserved. For commercial re-use, 
please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/ecco-jcc/jjaf134
PMID: 40796214


729. Disabil Rehabil. 2025 Dec 8:1-53. doi: 10.1080/09638288.2025.2593188. Online 
ahead of print.

Which interventions are acceptable to patients for managing fatigue in long-term 
conditions?: A qualitative evidence synthesis.

Booth A(1), Rooney G(1), Sutton A(1), Leaviss J(1), Deary V(2), Dawes H(3), 
Burton C(1)(4).

Author information:
(1)Sheffield Centre for Health and Related Research (SCHARR), University of 
Sheffield, Sheffield, UK.
(2)Department of Psychology, Faculty of Health and Life Sciences, Northumbria 
University, Newcastle, UK.
(3)Medical School, University of Exeter, Exeter, UK.
(4)School of Medicine & Population Health, University of Sheffield, Sheffield, 
UK.

PURPOSE: This qualitative evidence synthesis examined patient experiences of 
fatigue interventions among adults with diverse long-term conditions, 
complementing the EIFFEL systematic review and meta-analysis of intervention 
effectiveness.
MATERIALS AND METHODS: A comprehensive search across MEDLINE, Embase, CINAHL, 
PsycINFO, Web of Science, and Scopus identified relevant studies. Data underwent 
inductive thematic analysis followed by deductive coding using AI-generated 
thematic summaries (Claude 3.7 Sonnet), which were verified by an experienced 
reviewer.
RESULTS: The review identified 40 papers (36 papers from the original search 
plus four from an October 2025 update) covering 35 studies within six 
transdiagnostic themes: Coherence/Understanding, Process of Change, 
Personalisation/Applicability, Barriers to Engagement, Social Support, and 
Delivery Format. These themes applied across both common interventions used for 
different conditions and condition-specific approaches. Personalisation and 
tailoring emerged as essential throughout. Notably, within-condition differences 
proved as significant to patient experience as between-condition comparisons.
CONCLUSIONS: This transdiagnostic synthesis reveals shared patient needs across 
conditions. Acceptable interventions provide coherent explanations, balance 
structure with flexibility, and address knowledge, expectations, and behaviours 
without imposing additional burden. Future interventions should integrate 
transdiagnostic insights and personalisation opportunities to address fatigue 
complexity.

Plain Language Summary: Transdiagnostic approaches to fatigue management may be 
beneficial, as the experience of fatigue appears to share commonalities across 
various long-term conditions regardless of specific diagnoses.Personalisation 
and tailoring of fatigue interventions is essential, as within-condition 
differences can be as important as between-condition comparisons.Rehabilitation 
programs should address the need for coherence and understanding of fatigue as 
part of the process of patient engagement and change.Social support elements 
should be incorporated into fatigue management strategies, as this need was 
identified for both individual and group-based interventions.Consideration of 
delivery format is important for effective implementation of fatigue 
interventions across different long-term conditions.

DOI: 10.1080/09638288.2025.2593188
PMID: 41355528


730. J Biomed Inform. 2025 Nov;171:104923. doi: 10.1016/j.jbi.2025.104923. Epub 2025 
Oct 8.

Multi-feature machine learning for enhanced drug-drug interaction prediction.

Feng Q(1), Huang X(2).

Author information:
(1)Department of Biostatistics, Harvard T.H. Chan School of Public Health, 
Boston, 02115, MA, USA.
(2)Department of Environmental Sciences, Emory University, Atlanta, 30322, GA, 
USA. Electronic address: xiao.huang2@emory.edu.

Drug-drug interactions are a major concern in healthcare, as concurrent drug use 
can cause severe adverse effects. Existing machine learning methods often 
neglect data imbalance and DDI directionality, limiting clinical reliability. To 
overcome these issues, we employed GPT-4o Large Language Model to convert 
free-text DDI descriptions into structured triplets for directionality analysis 
and applied SMOTE to alleviate class imbalance. Using four key drug features 
(molecular fingerprints, enzymes, pathways, targets), our Deep Neural Networks 
(DNN) achieved 88.9% accuracy and showed an average AUPR gain of 0.68 for 
minority classes attributable to SMOTE. By applying attention-based feature 
importance analysis, we demonstrated that the most influential feature in the 
DNN model was supported by pharmacological evidence. These results demonstrate 
the effectiveness of our framework for accurate and robust DDI prediction. The 
source code and data are available at 
https://github.com/FrankFengF/Drug-drug-interaction-prediction.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104923
PMID: 41067296 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


731. Eur J Radiol. 2025 Dec 28;195:112624. doi: 10.1016/j.ejrad.2025.112624. Online 
ahead of print.

ChatGPT-generated informed consent forms in interventional radiology: A 
randomized controlled evaluation of comprehension, attitudinal responses, and 
readability.

Koç U(1), Güneş YC(2), Çolakoğlu MN(3), Coşkun N(4), Dere V(5), Özdemir M(6), 
Özdemir E(4).

Author information:
(1)Department of Radiology, Ankara Bilkent City Hospital, Ankara, Türkiye. 
Electronic address: dr_uralkoc@hotmail.com.
(2)Department of Radiology, Kırıkkale Yüksek İhtisas Hospital, Kırıkkale, 
Türkiye.
(3)Department of Radiology, Ankara Bilkent City Hospital, Ankara, Türkiye.
(4)Ankara Yıldırım Beyazıt University, Ankara, Türkiye; Department of Nuclear 
Medicine, Ankara Bilkent City Hospital, Ankara, Türkiye.
(5)Department of Nuclear Medicine, Ankara Bilkent City Hospital, Ankara, 
Türkiye.
(6)Department of Radiology, Ankara Bilkent City Hospital, Ankara, Türkiye; 
University of Health Sciences, Ankara, Türkiye.

OBJECTIVES: To evaluate whether ChatGPT-generated informed consent forms improve 
comprehension, attitudinal responses, and readability compared with standard 
informed consent forms in two interventional radiology procedures.
MATERIALS AND METHODS: This prospective, single blind, randomized controlled 
study was conducted at a tertiary-care interventional radiology department 
between March and August 2025. A total of 122 participants with no prior 
exposure to the procedures were randomly assigned (1:1) to receive either a 
web-based Chat Generative Pre-trained Transformer (GPT)-4o-generated or a 
standard informed consent form for transarterial radioembolization (TARE) or 
percutaneous hydatid cyst treatment (PHCT). All participants completed a 
ten-item comprehension test and a four-item attitudinal survey. Readability was 
analyzed using multiple indices (Flesch-Kincaid, Gunning Fog, Coleman-Liau, 
SMOG, Automated Readability Index, and Ateşman) and the Patient Education 
Materials Assessment Tool (PEMAT).
RESULTS: Participants who read ChatGPT-generated consent forms achieved higher 
comprehension scores than those who read standard forms (82.9 % vs. 77.3 %, 
p = 0.04). The benefit was consistent across procedures and education levels, 
with the greatest relative improvement among participants with lower educational 
attainment (p = 0.04). Readability analysis showed that ChatGPT-generated forms 
had shorter sentences and lower linguistic complexity (Flesch 39.2; Ateşman 
53.9) than standard forms. Test-retest results indicated better knowledge 
retention for ChatGPT-generated forms, particularly in the TARE subgroup. No 
significant attitudinal differences were observed between groups.
CONCLUSION: ChatGPT-generated informed consent forms demonstrated superior 
comprehension and equivalent attitudinal responses compared with standard 
documents, supporting their potential as communication tools to enhance clarity 
and engagement in interventional radiology.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ejrad.2025.112624
PMID: 41483678

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


732. JMIR Form Res. 2025 Apr 7;9:e64544. doi: 10.2196/64544.

Using Large Language Models to Automate Data Extraction From Surgical Pathology 
Reports: Retrospective Cohort Study.

Lee D(#)(1), Vaid A(#)(2), Menon KM(1), Freeman R(2), Matteson DS(3), Marin 
ML(1), Nadkarni GN(2).

Author information:
(1)Department of Surgery, Icahn School of Medicine at Mount Sinai, 10 Union 
Square East, Suite 2L, New York, NY, 10003, United States, 1 212 241 2891.
(2)Charles Bronfman Institute for Personalized Medicine, Icahn School of 
Medicine at Mount Sinai, New York, NY, United States.
(3)Department of Statistics and Data Science, Cornell University, Ithaca, NY, 
United States.
(#)Contributed equally

BACKGROUND: Popularized by ChatGPT, large language models (LLMs) are poised to 
transform the scalability of clinical natural language processing (NLP) 
downstream tasks such as medical question answering (MQA) and automated data 
extraction from clinical narrative reports. However, the use of LLMs in the 
health care setting is limited by cost, computing power, and patient privacy 
concerns. Specifically, as interest in LLM-based clinical applications grows, 
regulatory safeguards must be established to avoid exposure of patient data 
through the public domain. The use of open-source LLMs deployed behind 
institutional firewalls may ensure the protection of private patient data. In 
this study, we evaluated the extraction performance of a locally deployed LLM 
for automated MQA from surgical pathology reports.
OBJECTIVE: We compared the performance of human reviewers and a locally deployed 
LLM tasked with extracting key histologic and staging information from surgical 
pathology reports.
METHODS: A total of 84 thyroid cancer surgical pathology reports were assessed 
by two independent reviewers and the open-source FastChat-T5 3B-parameter LLM 
using institutional computing resources. Longer text reports were split into 
1200-character-long segments, followed by conversion to embeddings. Three 
segments with the highest similarity scores were integrated to create the final 
context for the LLM. The context was then made part of the question it was 
directed to answer. Twelve medical questions for staging and thyroid cancer 
recurrence risk data extraction were formulated and answered for each report. 
The time to respond and concordance of answers were evaluated. The concordance 
rate for each pairwise comparison (human-LLM and human-human) was calculated as 
the total number of concordant answers divided by the total number of answers 
for each of the 12 questions. The average concordance rate and associated error 
of all questions were tabulated for each pairwise comparison and evaluated with 
two-sided t tests.
RESULTS: Out of a total of 1008 questions answered, reviewers 1 and 2 had an 
average (SD) concordance rate of responses of 99% (1%; 999/1008 responses). The 
LLM was concordant with reviewers 1 and 2 at an overall average (SD) rate of 89% 
(7%; 896/1008 responses) and 89% (7.2%; 903/1008 responses). The overall time to 
review and answer questions for all reports was 170.7, 115, and 19.56 minutes 
for Reviewers 1, 2, and the LLM, respectively.
CONCLUSIONS: The locally deployed LLM can be used for MQA with considerable 
time-saving and acceptable accuracy in responses. Prompt engineering and 
fine-tuning may further augment automated data extraction from clinical 
narratives for the provision of real-time, essential clinical insights.

© Denise Lee, Akhil Vaid, Kartikeya M Menon, Robert Freeman, David S Matteson, 
Michael L Marin, Girish N Nadkarni. Originally published in JMIR Formative 
Research (https://formative.jmir.org).

DOI: 10.2196/64544
PMCID: PMC11996145
PMID: 40194317 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


733. J Med Internet Res. 2025 Oct 29;27:e79379. doi: 10.2196/79379.

Evaluating Web Retrieval-Assisted Large Language Models With and Without 
Whitelisting for Evidence-Based Neurology: Comparative Study.

Masanneck L(1), Epping PZ(1), Meuth SG(1), Pawlitzki M(1).

Author information:
(1)Department of Neurology, Medical Faculty and University Hospital Düsseldorf, 
Heinrich Heine University Düsseldorf, Dusseldorf, Germany.

BACKGROUND: Large language models (LLMs) coupled with real-time web retrieval 
are reshaping how clinicians and patients locate medical evidence, and as major 
search providers fuse LLMs into their interfaces, this hybrid approach might 
become the new "gateway" to the internet. However, open-web retrieval exposes 
models to nonprofessional sources, risking hallucinations and factual errors 
that might jeopardize evidence-based care.
OBJECTIVE: We aimed to quantify the impact of guideline-domain whitelisting on 
the answer quality of 3 publicly available Perplexity web-based 
retrieval-augmented generation (RAG) models and compare their performance using 
a purpose-built, biomedical literature RAG system (OpenEvidence).
METHODS: We applied a validated 130-item question set derived from the American 
Academy of Neurology (AAN) guidelines (65 factual and 65 case based). Perplexity 
Sonar, Sonar-Pro, and Sonar-Reasoning-Pro were each queried 4 times per question 
with open-web retrieval and again with retrieval restricted to aan.com and 
neurology.org ("whitelisted"). OpenEvidence was queried 4 times. Two 
neurologists, blinded to condition, scored each response (0=wrong, 1=inaccurate, 
and 2=correct); any disagreements that arose were resolved by a third 
neurologist. Ordinal logistic models were used to assess the influence of 
question type and source category (AAN or neurology vs nonprofessional) on 
accuracy.
RESULTS: From the 3640 LLM answers that were rated (interrater agreement: 
κ=0.86), correct-answer rates were as follows (open vs whitelisted, 
respectively): Sonar, 60% vs 78%, Sonar-Pro, 80% vs 88%, and 
Sonar-Reasoning-Pro, 81% vs 89%; for OpenEvidence, the correct-answer rate was 
82%. A Friedman test on modal scores across the 7 configurations was significant 
(χ26=73.7; P<.001). Whitelisting improved mean accuracy on the 0 to 2 scale by 
0.23 for Sonar (95% CI 0.12-0.34), 0.08 for Sonar-Pro (95% CI 0.01-0.16), and 
0.08 for Sonar-Reasoning-Pro (95% CI 0.02-0.13). Including ≥1 nonprofessional 
source halved the odds of a higher rating in Sonar (odds ratio [OR] 0.50, 95% CI 
0.37-0.66; P<.001), whereas citing an AAN or neurology document doubled it (OR 
2.18, 95% CI 1.64-2.89; P<.001). Furthermore, factual questions outperformed 
case vignettes across Perplexity models (ORs ranged from 1.95, 95% CI 1.28-2.98 
[Sonar + whitelisting] to 4.28, 95% CI 2.59-7.09 [Sonar-Reasoning-Pro]; all 
P<.01) but not for OpenEvidence (OR 1.44, 95% CI 0.92-2.27; P=.11).
CONCLUSIONS: Restricting retrieval to authoritative neurology domains yielded a 
clinically meaningful 8 to 18 percentage-point gain in correctness and halved 
output variability, upgrading a consumer search assistant to a 
decision-support-level tool that at least performed on par with a specialized 
literature engine. Lightweight source control is therefore a pragmatic safety 
lever for maintaining continuously updated, web-based RAG-augmented LLMs fit for 
evidence-based neurology.

©Lars Masanneck, Paula Zoe Epping, Sven G Meuth, Marc Pawlitzki. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org), 
29.10.2025.

DOI: 10.2196/79379
PMCID: PMC12612646
PMID: 41159599 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


734. J Magn Reson Imaging. 2025 Nov;62(5):1296-1304. doi: 10.1002/jmri.29807. Epub 
2025 May 4.

The Role of Large Language Models (LLMs) in Breast Imaging Today and in the Near 
Future.

Schiaffino S(1)(2), Zhang T(3)(4)(5), Mann RM(3)(4), Pinker K(6).

Author information:
(1)Imaging Institute of Southern Switzerland (IIMSI), Ente Ospedaliero Cantonale 
(EOC), Lugano, Switzerland.
(2)Faculty of Biomedical Sciences, Università della Svizzera Italiana, Lugano, 
Switzerland.
(3)Department of Radiology, Netherlands Cancer Institute, Amsterdam, the 
Netherlands.
(4)Department of Diagnostic Imaging, Radboud University Medical Center, 
Nijmegen, the Netherlands.
(5)GROW School for Oncology and Development Biology, Maastricht University, 
Maastricht, the Netherlands.
(6)Department of Radiology, Columbia University, Vagelos College of Physicians 
and Surgeons, New York, New York, USA.

This narrative review focuses on the integration of large language models 
(LLMs), such as GPT-4 and Gemini, into breast imaging. LLMs excel in 
understanding, processing, and generating human-like text, with potential 
applications ranging widely from decision-making to radiology reporting support. 
LLMs show promise in addressing current critical challenges, including rising 
demands for imaging services concurrent with an increasing shortage in the 
radiologist workforce. Their ability to integrate clinical guidelines and 
generate standardized, evidence-based reports has the potential to improve 
diagnostic consistency and reduce inter-reader variability. Emerging multimodal 
capabilities further extend their utility, enabling the integration of textual 
and visual data for tasks such as tumor classification and decision-making. 
Despite these advancements, significant challenges remain. LLMs often suffer 
from limitations such as hallucinations, biases in training datasets, and 
domain-specific knowledge gaps. These issues can affect their reliability, 
particularly in nuanced tasks like Breast Imaging Reporting and Data System 
categorization and multimodal image assessment. Moreover, ethical concerns about 
data privacy, biased outputs, and regulatory compliance must be addressed before 
effective deployment in the clinical setting. Current studies suggest that while 
LLMs can complement human expertise, their performance still lags behind that of 
radiologists in key areas, particularly in tasks requiring complex medical 
reasoning or direct image analysis. Looking ahead, LLMs are poised to play a 
crucial role in breast imaging by optimizing workflows, supporting 
multidisciplinary meetings, and improving patient education. However, their 
successful integration will depend on proper context training, robust 
validation, and ethical oversight, with human supervision as a crucial 
safeguard. EVIDENCE LEVEL: 5. TECHNICAL EFFICACY: Stage 2.

© 2025 International Society for Magnetic Resonance in Medicine.

DOI: 10.1002/jmri.29807
PMID: 40320742 [Indexed for MEDLINE]


735. BMC Med Educ. 2025 Dec 11;26(1):89. doi: 10.1186/s12909-025-08420-z.

Application and development of artificial intelligence and immersive 
technologies in dental education: a scoping review.

Huang S(#)(1)(2), Qu Y(#)(1)(2), Sun Y(1)(2), Chen S(1)(2), Gu L(3)(4).

Author information:
(1)Hospital of Stomatology, Guanghua School of Stomatology, Sun Yat-sen 
University, Guangzhou, China.
(2)Guangdong Provincial Key Laboratory of Stomatology, Sun Yat-sen University, 
Guangzhou, China.
(3)Hospital of Stomatology, Guanghua School of Stomatology, Sun Yat-sen 
University, Guangzhou, China. gulisha@mail.sysu.edu.cn.
(4)Guangdong Provincial Key Laboratory of Stomatology, Sun Yat-sen University, 
Guangzhou, China. gulisha@mail.sysu.edu.cn.
(#)Contributed equally

BACKGROUND: Artificial intelligence (AI) has been developing rapidly recently, 
with large language models such as Chat Generative Pre-trained Transformer 
(ChatGPT) garnering widespread attention across various fields globally. This 
scoping review examines the applications and advancements of AI, including 
natural language processing and computer simulation, as well as immersive 
technologies such as virtual reality (VR), augmented reality (AR), and mixed 
reality (MR) in dental education. Despite substantial research highlighting the 
benefits of AI in specific dental disciplines, a comprehensive review of the 
latest developments in AI within dental education has yet to be conducted.
METHODS: A scoping review methodology was employed to identify relevant articles 
published between 2022 and 2024. Five databases were used to identify studies 
that utilized AI, VR, AR, and MR within dental education: Web of Science, 
Embase, PubMed, Scopus, and the Educational Resources Information Center (ERIC). 
The search strategy adhered to the guidelines set forth by the Preferred 
Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping 
Reviews (PRISMA-ScR). Research was classified by digital technology type and its 
implementation across dental specialties.
RESULTS: Of the 1401 initial articles identified, 928 full-text articles were 
screened, and 76 ultimately met the selection criteria for the review. The 
selected research focused on AI, VR, AR, or MR applications within dental 
education.
CONCLUSION: This review underscores the transformative impact of AI and 
immersive technologies on dental education. By deepening our understanding and 
expanding the application of these technologies, we can improve educational 
outcomes through better cognitive visualization, standardized clinical training, 
and integrated multidisciplinary learning. Future research should focus on 
optimizing AI integration strategies and developing best practices to leverage 
these advanced technologies in dental education fully.
SUPPLEMENTARY INFORMATION: The online version contains supplementary material 
available at 10.1186/s12909-025-08420-z.

DOI: 10.1186/s12909-025-08420-z
PMCID: PMC12801878
PMID: 41372904

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


736. IEEE J Biomed Health Inform. 2025 May;29(5):3587-3597. doi: 
10.1109/JBHI.2025.3528196. Epub 2025 May 6.

MedFILIP: Medical Fine-Grained Language-Image Pre-Training.

Liang X, Li X, Li F, Jiang J, Dong Q, Wang W, Wang K, Dong S, Luo G, Li S.

Medical vision-language pretraining (VLP) that leverages naturally-paired 
medical image-report data is crucial for medical image analysis. However, 
existing methods struggle to accurately characterize associations between images 
and diseases, leading to inaccurate or incomplete diagnostic results. In this 
work, we propose MedFILIP, a fine-grained VLP model, introduces medical 
image-specific knowledge through contrastive learning, specifically: 1) An 
information extractor based on a large language model is proposed to decouple 
comprehensive disease details from reports, which excels in extracting disease 
deals through flexible prompt engineering, thereby effectively reducing text 
complexity while retaining rich information at a tiny cost. 2) A knowledge 
injector is proposed to construct relationships between categories and visual 
attributes, which help the model to make judgments based on image features, and 
fosters knowledge extrapolation to unfamiliar disease categories. 3) A semantic 
similarity matrix based on fine-grained annotations is proposed, providing 
smoother, information-richer labels, thus allowing fine-grained image-text 
alignment. 4) We validate MedFILIP on numerous datasets, e.g., RSNA-Pneumonia, 
NIH ChestX-ray14, VinBigData, and COVID-19. For single-label, multi-label, and 
fine-grained classification, our model achieves state-of-the-art performance, 
the classification accuracy has increased by a maximum of 6.69%.

DOI: 10.1109/JBHI.2025.3528196
PMID: 40030972 [Indexed for MEDLINE]


737. J Am Med Inform Assoc. 2025 Apr 1;32(4):761-772. doi: 10.1093/jamia/ocaf029.

Deciphering genomic codes using advanced natural language processing techniques: 
a scoping review.

Cheng S(1), Wei Y(1), Zhou Y(1), Xu Z(1), Wright DN(2), Liu J(3), Peng Y(1).

Author information:
(1)Department of Population Health Sciences, Weill Cornell Medicine, New York, 
NY 10065, United States.
(2)Samuel J. Wood Library & C.V. Starr Biomedical Information Center, Weill 
Cornell Medicine, New York, NY 10065, United States.
(3)School of Public Health, Virginia Commonwealth University, Richmond, VA 
23219, United States.

Update of
    ArXiv. 2024 Nov 25:arXiv:2411.16084v1.

OBJECTIVES: The vast and complex nature of human genomic sequencing data 
presents challenges for effective analysis. This review aims to investigate the 
application of natural language processing (NLP) techniques, particularly large 
language models (LLMs) and transformer architectures, in deciphering genomic 
codes, focusing on tokenization, transformer models, and regulatory annotation 
prediction. The goal of this review is to assess data and model accessibility in 
the most recent literature, gaining a better understanding of the existing 
capabilities and constraints of these tools in processing genomic sequencing 
data.
MATERIALS AND METHODS: Following Preferred Reporting Items for Systematic 
Reviews and Meta-Analyses (PRISMA) guidelines, our scoping review was conducted 
across PubMed, Medline, Scopus, Web of Science, Embase, and ACM Digital Library. 
Studies were included if they focused on NLP methodologies applied to genomic 
sequencing data analysis, without restrictions on publication date or article 
type.
RESULTS: A total of 26 studies published between 2021 and April 2024 were 
selected for review. The review highlights that tokenization and transformer 
models enhance the processing and understanding of genomic data, with 
applications in predicting regulatory annotations like transcription-factor 
binding sites and chromatin accessibility.
DISCUSSION: The application of NLP and LLMs to genomic sequencing data 
interpretation is a promising field that can help streamline the processing of 
large-scale genomic data while also providing a better understanding of its 
complex structures. It has the potential to drive advancements in personalized 
medicine by offering more efficient and scalable solutions for genomic analysis. 
Further research is also needed to discuss and overcome current limitations, 
enhancing model transparency and applicability.
CONCLUSION: This review highlights the growing role of NLP, particularly LLMs, 
in genomic sequencing data analysis. While these models improve data processing 
and regulatory annotation prediction, challenges remain in accessibility and 
interpretability. Further research is needed to refine their application in 
genomics.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf029
PMCID: PMC12005631
PMID: 39998912 [Indexed for MEDLINE]

Conflict of interest statement: None declare.


738. Lab Invest. 2025 Dec 16;106(2):104272. doi: 10.1016/j.labinv.2025.104272. Online 
ahead of print.

Using Consensus-Based Reasoning and Large Language Models to Extract Structured 
Data From Surgical Pathology Reports.

Tripathi A(1), Waqas A(2), Venkatesan K(3), Ullah E(4), Khan A(5), Khalil F(6), 
Chen WS(7), Ozturk ZG(8), Saeed-Vafa D(6), Bui MM(6), Schabath MB(9), Rasool 
G(3).

Author information:
(1)Department of Machine Learning, H. Lee Moffitt Cancer Center & Research 
Institute, Tampa, Florida. Electronic address: Aakash.Tripathi@moffitt.org.
(2)Department of Machine Learning, H. Lee Moffitt Cancer Center & Research 
Institute, Tampa, Florida; Department of Cancer Epidemiology, H. Lee Moffitt 
Cancer Center & Research Institute, Tampa, Florida. Electronic address: 
Asim.Waqas@moffitt.org.
(3)Department of Machine Learning, H. Lee Moffitt Cancer Center & Research 
Institute, Tampa, Florida.
(4)Department of Pathology, College of Medicine, The Ohio State University, 
Columbus, Ohio.
(5)Armed Forces Institute of Pathology, Rawalpindi, Pakistan.
(6)Department of Pathology, H. Lee Moffitt Cancer Center & Research Institute, 
Tampa, Florida.
(7)Department of Dermatology & Cutaneous Surgery, University of South Florida, 
Tampa, Florida.
(8)Clinical Science Lab, Basic and Clinical Science Division, H. Lee Moffitt 
Cancer Center & Research Institute, Tampa, Florida.
(9)Department of Cancer Epidemiology, H. Lee Moffitt Cancer Center & Research 
Institute, Tampa, Florida.

Surgical pathology reports provide essential diagnostic information critical for 
cancer staging, treatment planning, and cancer registry documentation. However, 
their writing styles and formats vary widely, reflecting each pathologist's 
stylistic choices, institutional norms, and inherited practices from residency 
training. When performing large-scale data analysis, this unstructured nature 
and variability across tumor types and institutions pose significant hurdles for 
automated data extraction. To overcome these challenges, we present a 
consensus-driven, reasoning-based framework that adapts multiple locally 
deployed large language models (LLMs) to extract both standard diagnostic 
variables (such as site, laterality, histology, stage, grade, and behavior) and 
organ-specific biomarkers. Each LLM generates structured outputs, accompanied by 
justifications, which are subsequently evaluated for accuracy and coherence by 3 
separate reasoning models (DeepSeek-R1-large, Qwen3-32B, and QWQ-32B). Final 
consensus values are determined through aggregation. Board-certified 
pathologists conducted expert validation. This framework was applied to >6100 
pathology reports from The Cancer Genome Atlas (TCGA), spanning 10 organ 
systems, and 510 reports from Moffitt Cancer Center. For TCGA data set, 
automated evaluation demonstrated a mean accuracy of 84.9% ± 7.3%, with 
histology (88%), site (87%), stage (84%), and behavior (84%) showing the highest 
extraction accuracy averaged across all models. Expert review of randomly 
selected 138 reports confirmed high agreement for behavior (100.0%), histology 
(99%), grade (96%), and site (95%) in TCGA data set, with slightly lower 
performance for stage (89%) and laterality (88%). In Moffitt Cancer Center 
reports (brain, breast, and lung), accuracy remained high (88.2% ± 7.2%), with 
behavior (99%), histology (97%), laterality (96%), grade (94%), and site (93%) 
achieving strong agreement. Biomarker extraction achieved 70.6% ± 7.9% overall 
accuracy, with TP53 (84%) on brain tumor, Ki-67 (68%) on breast cancer, and ROS1 
(82%) on lung cancer showing the highest accuracy. Interevaluator agreement 
analysis revealed high concordance (correlation values ≥0.93) across the 3 
evaluation models. Statistical analyses revealed significant main effects of 
model type (F = 1716.82, P < .001), variable (F = 3236.68, P < .001), and organ 
system (F = 1946.43, P < .001), as well as model × variable × organ interactions 
(F = 24.74, P < .001), emphasizing the role of clinical context in model 
performance. These results highlight the potential of stratified, multiorgan 
evaluation frameworks with multievaluator consensus in LLM benchmarking for 
clinical applications. Overall, this consensus-based approach demonstrated that 
locally deployed LLMs can provide a transparent, accurate, and auditable 
solution for integration into real-world pathology workflows, such as synoptic 
reporting and cancer registry abstraction.

Copyright © 2025 United States & Canadian Academy of Pathology. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.labinv.2025.104272
PMID: 41412350


739. J Craniomaxillofac Surg. 2026 Jan 13;54(3):104468. doi: 
10.1016/j.jcms.2026.104468. Online ahead of print.

Can OMFS experts distinguish AI from human manuscripts? A double-blind 
evaluation using ChatGPT-4.

Jain A(1).

Author information:
(1)Department of Oral and Maxillofacial Surgery, Geetanjali Dental and Research 
Institute, Geetanjali University, Eklingpura, 313001, Udaipur, Rajasthan, India; 
Department of Dental Research Cell, Dr. D. Y. Patil Dental College and Hospital, 
Dr. D. Y. Patil Vidyapeeth, Pimpri, 411018, Pune, Maharashtra, India; Department 
of Oral and Maxillofacial Surgery, Narsinhbhai Patel Dental College and 
Hospital, Sankalchand Patel University, Visnagar, 384315, Gujarat, India. 
Electronic address: dranujsinghvi@gmail.com.

OBJECTIVE: As generative AI tools like ChatGPT-4 gain traction in academic 
writing, questions arise regarding their credibility, scientific depth, and 
detectability. This study aimed to evaluate whether experienced oral and 
maxillofacial surgeons (OMFS) can reliably distinguish between AI- and 
human-authored manuscripts, and to compare both in terms of coherence, 
scientific rigor, citation accuracy, and overall quality.
MATERIALS AND METHODS: Three core OMFS topics-impacted third molar surgery, cyst 
enucleation, and TMJ arthroscopy-were selected. For each topic, two manuscripts 
(∼2500 words each) were independently written: one by ChatGPT-4 and one by 
senior OMFS clinicians. Twenty board-certified OMFS reviewers, blinded to 
authorship, evaluated these manuscripts using a validated 25-item questionnaire 
assessing five domains: readability, scientific depth, reference accuracy, 
writing quality, and methodological rigor. Reviewers also attempted to identify 
the authorship source. Citation accuracy was verified through manual PubMed 
cross-checking. Statistical analysis included paired t-tests, chi-square tests, 
and ANOVA.
RESULTS: Human-authored manuscripts outperformed AI-generated ones in scientific 
depth (4.5 ± 0.4 vs. 3.9 ± 0.6, p < 0.01), reference accuracy (4.9 ± 0.1 vs. 
4.4 ± 0.7, p < 0.001), and overall writing quality (4.7 ± 0.4 vs. 4.1 ± 0.5, 
p < 0.01). Coherence and readability scores were comparable (human: 4.8 ± 0.4; 
AI: 4.6 ± 0.5; p = 0.07). Reviewers correctly identified manuscript authorship 
only 54 % of the time (p = 0.68), suggesting AI-generated texts are often 
indistinguishable from human ones in surface fluency.
CONCLUSION: ChatGPT-4 is capable of producing readable and structurally sound 
OMFS manuscripts. However, deficiencies in scientific reasoning and citation 
fidelity underscore the need for expert oversight. As AI tools integrate into 
academic workflows, transparent disclosure and editorial safeguards are 
imperative to uphold scientific integrity.

Copyright © 2026 European Association for Cranio-Maxillo-Facial Surgery. 
Published by Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.jcms.2026.104468
PMID: 41534249

Conflict of interest statement: Conflicts of interest The authors declare no 
relevant conflicts of interest.


740. J Magn Reson Imaging. 2025 Jun;61(6):2376-2390. doi: 10.1002/jmri.29687. Epub 
2025 Jan 9.

Computer-Aided Detection (CADe) and Segmentation Methods for Breast Cancer Using 
Magnetic Resonance Imaging (MRI).

Jannatdoust P(1), Valizadeh P(1), Saeedi N(2), Valizadeh G(3), Salari HM(3), 
Saligheh Rad H(3)(4), Gity M(5).

Author information:
(1)School of Medicine, Tehran University of Medical Science, Tehran, Iran.
(2)Student Research Committee, Islamic Azad University, Mashhad Branch, Mashhad, 
Iran.
(3)Quantitative MR Imaging and Spectroscopy Group (QMISG), Tehran University of 
Medical Sciences, Tehran, Iran.
(4)Department of Medical Physics and Biomedical Engineering, Tehran University 
of Medical Sciences, Tehran, Iran.
(5)Advanced Diagnostic and Interventional Radiology Research Center, Tehran 
University of Medical Sciences, Tehran, Iran.

Breast cancer continues to be a major health concern, and early detection is 
vital for enhancing survival rates. Magnetic resonance imaging (MRI) is a key 
tool due to its substantial sensitivity for invasive breast cancers. 
Computer-aided detection (CADe) systems enhance the effectiveness of MRI by 
identifying potential lesions, aiding radiologists in focusing on areas of 
interest, extracting quantitative features, and integrating with computer-aided 
diagnosis (CADx) pipelines. This review aims to provide a comprehensive overview 
of the current state of CADe systems in breast MRI, focusing on the technical 
details of pipelines and segmentation models including classical intensity-based 
methods, supervised and unsupervised machine learning (ML) approaches, and the 
latest deep learning (DL) architectures. It highlights recent advancements from 
traditional algorithms to sophisticated DL models such as U-Nets, emphasizing 
CADe implementation of multi-parametric MRI acquisitions. Despite these 
advancements, CADe systems face challenges like variable false-positive and 
negative rates, complexity in interpreting extensive imaging data, variability 
in system performance, and lack of large-scale studies and multicentric models, 
limiting the generalizability and suitability for clinical implementation. 
Technical issues, including image artefacts and the need for reproducible and 
explainable detection algorithms, remain significant hurdles. Future directions 
emphasize developing more robust and generalizable algorithms, integrating 
explainable AI to improve transparency and trust among clinicians, developing 
multi-purpose AI systems, and incorporating large language models to enhance 
diagnostic reporting and patient management. Additionally, efforts to 
standardize and streamline MRI protocols aim to increase accessibility and 
reduce costs, optimizing the use of CADe systems in clinical practice. LEVEL OF 
EVIDENCE: NA TECHNICAL EFFICACY: Stage 2.

© 2025 International Society for Magnetic Resonance in Medicine.

DOI: 10.1002/jmri.29687
PMID: 39781684 [Indexed for MEDLINE]


741. Artif Intell Med. 2025 Mar;161:103067. doi: 10.1016/j.artmed.2025.103067. Epub 
2025 Jan 22.

Hybrid approach for drug-target interaction predictions in ischemic stroke 
models.

Peng JJ(1), Zhang YY(2), Li RF(2), Zhu WJ(2), Liu HR(1), Li HY(1), Liu B(3), Cao 
DS(4), Peng J(5), Luo XJ(6).

Author information:
(1)Department of Laboratory Medicine, the third Xiangya Hospital, Central South 
University, Changsha 410013, China.
(2)Department of Pharmacology, Xiangya School of Pharmaceutical Sciences, 
Central South University, Changsha 410003, China.
(3)Department of Pharmacy, Xiangya Hospital, Central South University, Changsha 
410008, China.
(4)Department of Pharmaceutical Chemistry, Xiangya School of Pharmaceutical 
Sciences, Central South University, Changsha 410003, China.
(5)Department of Pharmacology, Xiangya School of Pharmaceutical Sciences, 
Central South University, Changsha 410003, China. Electronic address: 
junpeng@csu.edu.cn.
(6)Department of Laboratory Medicine, the third Xiangya Hospital, Central South 
University, Changsha 410013, China. Electronic address: xjluo22@csu.edu.cn.

Multiple cell death mechanisms are triggered during ischemic stroke and they are 
interconnected in a complex network with extensive crosstalk, complicating the 
development of targeted therapies. We therefore propose a novel framework for 
identifying disease-specific drug-target interaction (DTI), named strokeDTI, to 
extract key nodes within an interconnected graph network of activated pathways 
via leveraging transcriptomic sequencing data. Our findings reveal that the 
drugs a model can predict are highly representative of the characteristics of 
the database the model is trained on. However, models with comparable 
performance yield diametrically opposite predictions in real testing scenarios. 
Our analysis reveals a correlation between the reported literature on 
drug-target pairs and their binding scores. Leveraging this correlation, we 
introduced an additional module to assess the predictive validity of our model 
for each unique target, thereby improving the reliability of the framework's 
predictions. Our framework identified Cerdulatinib as a potential anti-stroke 
drug via targeting multiple cell death pathways, particularly necroptosis and 
apoptosis. Experimental validation in in vitro and in vivo models demonstrated 
that Cerdulatinib significantly attenuated stroke-induced brain injury via 
inhibiting multiple cell death pathways, improving neurological function, and 
reducing infarct volume. This highlights strokeDTI's potential for 
disease-specific drug-target identification and Cerdulatinib's potential as a 
potent anti-stroke drug.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.artmed.2025.103067
PMID: 39956766 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declared no potential conflicts of interest with respect the research, 
authorship, and/or publication of this article.


742. Med Biol Eng Comput. 2025 Sep 25. doi: 10.1007/s11517-025-03446-3. Online ahead 
of print.

Integrating CT image reconstruction, segmentation, and large language models for 
enhanced diagnostic insight.

Abbasi AA(1), Farooqi AH(2).

Author information:
(1)Department of Computer Science, Air University, Islamabad, Pakistan.
(2)Department of Computer Science, Air University, Islamabad, Pakistan. 
ashfaq.hussain@au.edu.pk.

Deep learning has significantly advanced medical imaging, particularly computed 
tomography (CT), which is vital for diagnosing heart and cancer patients, 
evaluating treatments, and tracking disease progression. High-quality CT images 
enhance clinical decision-making, making image reconstruction a key research 
focus. This study develops a framework to improve CT image quality while 
minimizing reconstruction time. The proposed four-step medical image analysis 
framework includes reconstruction, preprocessing, segmentation, and image 
description. Initially, raw projection data undergoes reconstruction via a Radon 
transform to generate a sinogram, which is then used to construct a CT image of 
the pelvis. A convolutional neural network (CNN) ensures high-quality 
reconstruction. A bilateral filter reduces noise while preserving critical 
anatomical features. If required, a medical expert can review the image. The 
K-means clustering algorithm segments the preprocessed image, isolating the 
pelvis and removing irrelevant structures. Finally, the FuseCap model generates 
an automated textual description to assist radiologists. The framework's 
effectiveness is evaluated using peak signal-to-noise ratio (PSNR), normalized 
mean square error (NMSE), and structural similarity index measure (SSIM). The 
achieved values-PSNR 30.784, NMSE 0.032, and SSIM 0.877-demonstrate superior 
performance compared to existing methods. The proposed framework reconstructs 
high-quality CT images from raw projection data, integrating segmentation and 
automated descriptions to provide a decision-support tool for medical experts. 
By enhancing image clarity, segmenting outputs, and providing descriptive 
insights, this research aims to reduce the workload of frontline medical 
professionals and improve diagnostic efficiency.

© 2025. International Federation for Medical and Biological Engineering.

DOI: 10.1007/s11517-025-03446-3
PMID: 40993406

Conflict of interest statement: Declarations. Animal involvement: This research 
does not involve any animals or animal datasets. Conflict of interest: The 
authors declare no competing interests.


743. Front Oncol. 2025 Sep 2;15:1601529. doi: 10.3389/fonc.2025.1601529. eCollection 
2025.

The clinical application potential assessment of the Deepseek-R1 large language 
model in lung cancer.

Xu X(1)(2), Liu Z(2), Zhou S(1), Ji B(2), Fan D(2), Yang Z(1)(2), Chen H(2), 
Yang X(2), Guan M(1)(2).

Author information:
(1)The Graduate School of Qinghai University, Xining, Qinghai, China.
(2)The Department of Oncology, Qinghai Red Cross Hospital, Xining, 
Qinghai, China.

BACKGROUND: This study evaluates the clinical potential of the large language 
model Deepseek-R1 in the diagnosis and treatment of lung cancer, with a specific 
focus on its ability to assist junior oncologists. The research systematically 
assesses the model's performance in terms of diagnostic accuracy, consistency of 
treatment recommendations, and reliability in clinical decision-making.
METHODS: A total of 320 patients newly diagnosed with lung cancer were included 
in this retrospective study. Twenty-six structured clinical questions were 
designed based on international diagnostic and treatment guidelines. These 
questions addressed three key domains: basic medical knowledge, complex clinical 
decision-making, and ethical judgment. All patient data were anonymized before 
being entered into the Deepseek-R1 model. The model's responses, along with 
those generated by five junior oncologists with no more than three years of 
clinical experience, were independently assessed by senior oncologists with over 
ten years of experience. A double-blind evaluation protocol was implemented to 
reduce potential assessment bias. Inter-rater agreement was quantified using 
Cohen's Kappa coefficient.
RESULTS: In the categories of basic knowledge, advanced clinical decisions, and 
ethical questions, Deepseek-R1 achieved average accuracy rates of 92.3%, 87.5%, 
and 85.1%, respectively. These rates were significantly higher than those of 
junior oncologists, whose accuracy rates were 80.4%, 72.8%, and 70.2%, 
respectively (P < 0.05). In a sample of 256 cases evaluated formally, 
Deepseek-R1's overall diagnostic accuracy was 94.6%, compared to 78.9% for 
junior oncologists (P < 0.05). In a longitudinal assessment of 40 cases with 
disease progression, the model demonstrated high consistency in updating its 
recommendations. Logical errors were more frequent among junior oncologists, 
while ethical risks appeared more commonly in the model-generated responses (44% 
vs. 21.9%).
CONCLUSION: Deepseek-R1 significantly outperformed junior oncologists in terms 
of diagnostic accuracy and treatment decision-making, particularly in complex 
and dynamic clinical situations. While limitations remain in its ethical 
reasoning, the model holds substantial potential for supporting junior 
physicians, contributing to multidisciplinary discussions, and optimizing 
treatment pathways.

Copyright © 2025 Xu, Liu, Zhou, Ji, Fan, Yang, Chen, Yang and Guan.

DOI: 10.3389/fonc.2025.1601529
PMCID: PMC12436400
PMID: 40963861

Conflict of interest statement: The remaining authors declares that the research 
was conducted in the absence of any commercial or financial relationships that 
could be construed as a potential conflict of interest.


744. J Med Internet Res. 2025 Apr 30;27:e66098. doi: 10.2196/66098.

Use of Retrieval-Augmented Large Language Model for COVID-19 Fact-Checking: 
Development and Usability Study.

Li H(#)(1), Huang J(#)(1), Ji M(2), Yang Y(3), An R(4).

Author information:
(1)School of Economics and Management, Shanghai University of Sport, Shanghai, 
China.
(2)Department of Surgery, Division of Public Health Sciences, Washington 
University School of Medicine in St. Louis, St. Louis, MO, United States.
(3)Division of Computational and Data Sciences, Washington University in St. 
Louis, St. Louis, MO, United States.
(4)Constance and Martin Silver Center on Data Science and Social Equity, Silver 
School of Social Work, New York University, New York, NY, United States.
(#)Contributed equally

BACKGROUND: The COVID-19 pandemic has been accompanied by an "infodemic," where 
the rapid spread of misinformation has exacerbated public health challenges. 
Traditional fact-checking methods, though effective, are time-consuming and 
resource-intensive, limiting their ability to combat misinformation at scale. 
Large language models (LLMs) such as GPT-4 offer a more scalable solution, but 
their susceptibility to generating hallucinations-plausible yet incorrect 
information-compromises their reliability.
OBJECTIVE: This study aims to enhance the accuracy and reliability of COVID-19 
fact-checking by integrating a retrieval-augmented generation (RAG) system with 
LLMs, specifically addressing the limitations of hallucination and context 
inaccuracy inherent in stand-alone LLMs.
METHODS: We constructed a context dataset comprising approximately 130,000 
peer-reviewed papers related to COVID-19 from PubMed and Scopus. This dataset 
was integrated with GPT-4 to develop multiple RAG-enhanced models: the naïve 
RAG, Lord of the Retrievers (LOTR)-RAG, corrective RAG (CRAG), and self-RAG 
(SRAG). The RAG systems were designed to retrieve relevant external information, 
which was then embedded and indexed in a vector store for similarity searches. 
One real-world dataset and one synthesized dataset, each containing 500 claims, 
were used to evaluate the performance of these models. Each model's accuracy, 
F1-score, precision, and sensitivity were compared to assess their effectiveness 
in reducing hallucination and improving fact-checking accuracy.
RESULTS: The baseline GPT-4 model achieved an accuracy of 0.856 on the 
real-world dataset. The naïve RAG model improved this to 0.946, while the 
LOTR-RAG model further increased accuracy to 0.951. The CRAG and SRAG models 
outperformed all others, achieving accuracies of 0.972 and 0.973, respectively. 
The baseline GPT-4 model reached an accuracy of 0.960 on the synthesized 
dataset. The naïve RAG model increased this to 0.972, and the LOTR-RAG, CRAG, 
and SRAG models achieved an accuracy of 0.978. These findings demonstrate that 
the RAG-enhanced models consistently maintained high accuracy levels, closely 
mirroring ground-truth labels and significantly reducing hallucinations. The 
CRAG and SRAG models also provided more detailed and contextually accurate 
explanations, further establishing the superiority of agentic RAG frameworks in 
delivering reliable and precise fact-checking outputs across diverse datasets.
CONCLUSIONS: The integration of RAG systems with LLMs substantially improves the 
accuracy and contextual relevance of automated fact-checking. By reducing 
hallucinations and enhancing transparency by citing retrieved sources, this 
method holds significant promise for rapid, reliable information verification to 
combat misinformation during public health crises.

©Hai Li, Jingyi Huang, Mengmeng Ji, Yuyi Yang, Ruopeng An. Originally published 
in the Journal of Medical Internet Research (https://www.jmir.org), 30.04.2025.

DOI: 10.2196/66098
PMCID: PMC12079058
PMID: 40306628 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


745. JMIR Form Res. 2025 Dec 4;9:e78138. doi: 10.2196/78138.

Comparing Human and AI Therapists in Behavioral Activation for Depression: 
Cross-Sectional Questionnaire Study.

Napiwotzki I(#)(1), Laue J(#)(2), Caldarone F(1), Idahl M(2), Hadler U(2), 
Amrani H(2), Hildt E(3), Kahl KG(1), Nejdl W(2).

Author information:
(1)Clinic for Psychiatry, Social Psychiatry and Psychotherapy, Department of 
Medizinische Hochschule Hannover, Carl-Neuberg-Str 1, Hannover, 30625, Germany, 
49 511 532 2495.
(2)L3S Research Center, Leibniz University Hannover, Hannover, Germany.
(3)Department of Humanities, Arts, and Social Sciences, Illinois Institute of 
Technology, Chicago, IL, United States.
(#)Contributed equally

BACKGROUND: Large language models (LLMs) have rapidly advanced across numerous 
fields, including mental health care. A shortage of trained therapists and 
mental health care providers has driven informal use of LLMs for therapeutic 
support. However, their clinical utility remains poorly defined.
OBJECTIVE: This study aimed to systematically evaluate and compare the 
therapeutic knowledge and single-turn response capabilities of LLMs versus 
psychotherapists in training in the context of behavioral activation (BA) 
therapy for depression, and to assess how both groups' performance changed when 
provided with structured therapeutic training materials.
METHODS: Six LLMs and 8 human participants completed a questionnaire on 
depression and BA with 20 multiple-choice items and 10 therapy scenarios, each 
with 3 open-ended items, that postulated empathic response, use of validation 
strategies, and theory of mind capabilities. Human participants completed the 
questionnaire before and after a 5-hour workshop and 5-week period with learning 
materials. The LLMs received identical training content as context during the 
second test. All open-ended questions were rated on 5-point scales by 2 experts.
RESULTS: At baseline, the LLMs demonstrated higher knowledge scores than human 
participants (61.0 vs 52.0 out of 100 points) and were rated higher in empathy 
(U=2.0; P=.005; r=0.917), validation quality (U=2.5; P=.006; r=0.896), 
anticipation of cognition (U=0.0; P=.002; r=1.000), and anticipation of emotion 
(U=0.0; P=.002; r=1.000). Following BA training, the LLMs maintained their 
performance advantage across multiple-choice and open-ended items.
CONCLUSIONS: The results suggest that LLMs may generate high-quality therapeutic 
single-turn responses that integrate clinical knowledge with empathetic 
communication. The findings hint at LLMs' potential as valuable tools in mental 
health care, although further clinical trials are needed to evaluate their 
performance in ongoing therapeutic relationships and clinical outcomes.

© Inka Napiwotzki, Julian Laue, Flora Caldarone, Maximilian Idahl, Uwe Hadler, 
Haithem Amrani, Elisabeth Hildt, Kai G Kahl, Wolfgang Nejdl. Originally 
published in JMIR Formative Research (https://formative.jmir.org).

DOI: 10.2196/78138
PMCID: PMC12677730
PMID: 41343763 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


746. BMC Med Inform Decis Mak. 2025 Oct 13;25(1):373. doi: 
10.1186/s12911-025-03224-z.

GPT-4o and the quest for machine learning interpretability in ICU risk of death 
prediction.

E Samadi M(1)(2), Nikulina K(1)(2), Fritsch SJ(3)(4)(5), Schuppert A(6)(7).

Author information:
(1)Institute for Computational Biomedicine, RWTH Aachen University, Aachen, 
Germany.
(2)Center for Computational Life Sciences, RWTH Aachen University, Aachen, 
Germany.
(3)Department of Intensive Care Medicine, University Hospital RWTH Aachen, 
Aachen, Germany.
(4)Jülich Supercomputing Centre, Forschungszentrum Jülich, Jülich, Germany.
(5)Center for Advanced Simulation and Analytics (CASA), Forschungszentrum 
Jülich, Jülich, Germany.
(6)Institute for Computational Biomedicine, RWTH Aachen University, Aachen, 
Germany. aschuppert@ukaachen.de.
(7)Center for Computational Life Sciences, RWTH Aachen University, Aachen, 
Germany. aschuppert@ukaachen.de.

BACKGROUND: Clinical utilization of machine learning is hampered by the lack of 
interpretability inherent in most non-linear black box modeling approaches, 
reducing trust among clinicians and regulators. Advanced large language models 
offer a potential framework for integrating medical knowledge into these models, 
potentially enhancing their interpretability.
METHODS: A hybrid mechanistic/data-driven modeling framework is presented for 
developing an ICU risk of death prediction model for mechanically ventilated 
patients. In the mechanistic modeling part, GPT-4o is used to generate detailed 
medical feature descriptions, which are then aggregated into a comprehensive 
corpus and processed with TF-I DF vectorization. Fuzzy C-means clustering is 
subsequently applied to these vectorized features to identify significant 
mortality cause-specific feature clusters, and a physician reviewed the 
resulting clusters to validate their relevance to actionable insights for 
clinical decision support. In the data-driven part, the identified clusters 
inform the creation of XGBoost-based weak classifiers, whose outcomes are 
combined into a single XGBoost-based strong classifier through a hierarchically 
structured feed-forward network. This process results in a novel GPT hybrid 
model for ICU risk of death prediction.
RESULTS: This study enrolled 16,018 mechanically ventilated ICU patients, 
divided into derivation (12,758) and validation (3,260) cohorts, to develop and 
evaluate a GPT hybrid model for predicting in-ICU death. Leveraging GPT-4o, we 
implemented an automated process for clustering mortality cause-specific 
features, resulting in six feature clusters: Liver Failure, Infection, Renal 
Failure, Hypoxia, Cardiac Failure, and Mechanical Ventilation. This approach 
significantly improved upon previous manual methods, automating the 
reconstruction of structured hybrid models. While the GPT hybrid model showed 
similar predictive accuracy to a Global XGBoost model, it demonstrated superior 
interpretability and clinical relevance by incorporating a wider array of 
features and providing a hierarchical structure of feature importance aligned 
with medical knowledge.
CONCLUSION: We introduce a novel approach to predicting in-ICU risk of death for 
mechanically ventilated patients using a GPT hybrid model. Our methodology 
demonstrates the potential of integrating large language models with traditional 
machine learning techniques to create interpretable and clinically relevant 
predictive models.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-03224-z
PMCID: PMC12516888
PMID: 41083982 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was conducted in accordance with the principles of the 
Declaration of Helsinki. All experimental protocols were approved by the Ethics 
Committee of the RWTH Aachen Faculty of Medicine (local Ethics Committee 
reference number: EK 102/19, date of approval: 26.03.2019). As well, the Ethics 
Committee of the RWTH Aachen Faculty of Medicine (local Ethics Committee 
reference number: EK 102/19, date of approval: 26.03.2019) waived the need to 
obtain Informed consent for the collection and retrospective analysis of the 
de-identified data as well as the publication of the results of the analysis. 
All methods were performed in accordance with the relevant guidelines and 
regulations. Consent for publication: Not applicable. Competing interests: The 
authors declare no competing interests.


747. J Med Internet Res. 2025 Oct 22;27:e74094. doi: 10.2196/74094.

Assessing Large Language Models in Building a Structured Dataset From AskDocs 
Subreddit Data: Methodological Study.

Snell Q(#)(1), Westhoff C(#)(1), Westhoff J(#)(2), Low E(#)(1), Hanson CL(#)(1), 
Tass ESN(#)(1).

Author information:
(1)Brigham Young University, 3361 TMCB, Provo, UT, 84602, United States, 1 
8014225098.
(2)University of Nevada, Reno, Reno, NV, United States.
(#)Contributed equally

BACKGROUND: In an era marked by a growing reliance on digital platforms for 
health care consultation, the subreddit r/AskDocs has emerged as a pivotal 
forum. However, the vast, unstructured nature of forum data presents a 
formidable challenge; the extraction and meaningful analysis of such data 
require advanced tools that can navigate the complexities of language and 
context inherent in user-generated content. The emergence of large language 
models (LLMs) offers new tools for the extraction of health-related content from 
unstructured text found in social media platforms such as Reddit.
OBJECTIVE: This methodological study aimed to evaluate the use of LLMs to 
systematically transform the rich, unstructured textual data from the AskDocs 
subreddit into a structured dataset, an approach that aligns more closely with 
human cognitive processes than traditional data extraction methods.
METHODS: Human annotators and LLMs were used to extract data from 2800 randomly 
sampled r/AskDocs subreddit posts. For human annotation, at least 2 medical 
students extracted demographic information, type of inquiry (diagnosis, symptom, 
or treatment), proxy relationship, chronic condition, health care consultation 
status, and primary focus topic. For LLM data extraction, specially engineered 
prompts were created using JavaScript Object Notation and few-shot prompting. 
Prompts were used to query several state-of-the-art LLMs (eg, Llama 3, Genna, 
and GPT). Cohen κ was calculated across all human annotators, with this dataset 
serving as the gold standard for comparison with LLM data extraction. A high 
degree of human annotator reliability was observed for the coding of demographic 
information. Lower reliability was seen in coding the health-related content of 
the posts.
RESULTS: The highest performance scores compared with the gold standard were 
achieved by Llama 3 70B with 7 few-shot prompt examples (average accuracy=87.4) 
and GPT-4 with 2 few-shot prompt examples (average accuracy=87.4). Llama 3 70B 
excelled in coding health-related content while GPT-4 performed better coding 
demographic content from unstructured posts.
CONCLUSIONS: LLMs performed comparably with human annotators in extracting 
demographic and health-related information from the AskDocs subreddit 
unstructured posts. This study validates the use of LLMs for analyzing digital 
health care communications and highlights their potential as reliable tools for 
understanding online behaviors and interactions, shifting toward more 
sophisticated methodologies in digital research and practice.

© Quinn Snell, Chase Westhoff, John Westhoff, Ethan Low, Carl L Hanson, E 
Shannon Neeley Tass. Originally published in the Journal of Medical Internet 
Research (https://www.jmir.org).

DOI: 10.2196/74094
PMCID: PMC12543290
PMID: 41124662 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


748. Digit Health. 2025 Nov 25;11:20552076251393290. doi: 10.1177/20552076251393290. 
eCollection 2025 Jan-Dec.

LLM-BCgrading: Large language model-based Chinese medical long text 
classification for bladder cancer grade prediction.

Pan X(1), Wen L(2), Li Y(3), Zhang Y(4), Lu M(1).

Author information:
(1)College of Artificial Intelligence, Dalian Maritime University, Dalian, 
China.
(2)Department of Urology, The Second Affiliated Hospital of Dalian Medical 
University, Dalian, China.
(3)School of Computer Science and Informatics, Cardiff University, Cardiff, UK.
(4)School of Information Science and Technology, Dalian Maritime University, 
Dalian, China.

BACKGROUND: Traditional cystoscopic biopsy-based methods for histological 
grading of bladder cancer (BC) are invasive, subject to sampling errors, and 
susceptible to interobserver variability among pathologists. To address these 
challenges, this study explores a large language model (LLM)-based noninvasive 
approach to BC grade prediction using long Chinese medical texts.
METHODS: We retrospectively collected admission records and computed tomography 
urography (CTU) descriptions from 642 patients pathologically diagnosed with BC. 
Each paired text was annotated as low grade or high grade according to 
histopathological results. We developed LLM-BCgrading to leverage HuatuoGPT-7B 
for Chinese medical long-text representation and integrated a gated 
multiplicative attention mechanism (GMAM) to selectively emphasize 
discriminative features. To address class imbalance and clinical risk asymmetry, 
the model was optimized with a cost-sensitive loss function. Performance was 
evaluated on a fixed internal test set with additional evaluation on an 
independent external validation cohort to assess generalizability.
RESULTS: The best-performing configuration combined both admission records and 
CTU descriptions via an attention-based fusion strategy and GMAM, achieving 
balanced accuracy of 0.757, macro F1 score of 0.749, and macro AUC of 0.740. The 
ablation results demonstrated that incorporating both texts significantly 
improved classification performance compared with single-text configurations, 
and the GMAM consistently outperformed conventional attention mechanisms. 
Dimensionality experiments identified 256 as the optimal embedding size, 
balancing computational efficiency and predictive performance.
CONCLUSION: Our findings demonstrate that LLMs can effectively process Chinese 
medical long-texts for accurate preoperative prediction of BC grade. 
Attention-based fusion, cost-sensitive optimization, and interpretability based 
on Shapley additive explanations further support the robustness and clinical 
relevance of this LLM-driven framework.

© The Author(s) 2025.

DOI: 10.1177/20552076251393290
PMCID: PMC12647560
PMID: 41312143

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


749. Clin Exp Dermatol. 2025 Jul 24;50(8):1620-1622. doi: 10.1093/ced/llaf126.

Representations of skin tone and sex in dermatology by generative artificial 
intelligence: a comparative study.

Sakunchotpanit G(1)(2), Nayudu K(1)(3), Chen R(1)(4), Milosavljevic S(1)(5), 
Rohan TZ(1)(6), Ortiz-López L(1)(7), Venkatesh K(1)(5), Nambudiri VE(1)(5).

Author information:
(1)Department of Dermatology, Brigham and Women's Hospital, Boston, MA, USA.
(2)Tufts University School of Medicine, Boston, MA, USA.
(3)Medical College of Georgia, Augusta, GA, USA.
(4)University of Massachusetts Medical School, Worcester, MA, USA.
(5)Harvard Medical School, Boston, MA, USA.
(6)Sidney Kimmel Medical College, Thomas Jefferson University, Philadelphia, PA, 
USA.
(7)Universidad Central del Caribe, Bayamón, PR, USA.

With generative artificial intelligence (AI) demonstrating potential in 
dermatological education, assessment of skin tone diversity is imperative to 
ensure comprehensive patient care. Evaluating DALL·E 3, Midjourney and 
DreamStudio Beta, we generated five images on each platform for eight common 
dermatological conditions designated by the American Academy of Dermatology. The 
Massey-Martin Skin Color Scale was used to evaluate the images, and interrater 
reliability was further assessed by a nonrater. Sex determination was based on 
identifying features. We generated 120 images: 88 (73%) had concordant skin tone 
ratings and 109 (91%) displayed an identifiable sex. Of the 88 images, 85 (97%) 
were rated light-toned, 3 (3%) were rated medium-toned and 0 were rated 
dark-toned. Of the 109 images, 74 (68%) were male and 35 (32%) were female. 
Highlighting substantial biases currently present in common AI platforms, this 
study underscores the need for AI algorithms to address both skin tone and sex 
biases as they continue to skyrocket in popularity.

© The Author(s) 2025. Published by Oxford University Press on behalf of British 
Association of Dermatologists. All rights reserved. For commercial re-use, 
please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/ced/llaf126
PMID: 40080693 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of interest: The authors declare no 
conflicts of interest.


750. NPJ Digit Med. 2025 May 17;8(1):289. doi: 10.1038/s41746-025-01626-x.

The DRAGON benchmark for clinical NLP.

Bosma JS(1)(2)(3), Dercksen K(4), Builtjes L(4), André R(4), Roest C(5), Fransen 
SJ(5), Noordman CR(4)(6), Navarro-Padilla M(4), Lefkes J(7), Alves N(4), de 
Grauw MJJ(4), van Eekelen L(7), Spronck JMA(7), Schuurmans M(4), de Wilde B(4), 
Hendrix W(4), Aswolinskiy W(7), Saha A(4)(6), Twilt JJ(6), Geijs D(7), Veltman 
J(8), Yakar D(9)(5), de Rooij M(10), Ciompi F(7), Hering A(4), Geerdink J(11), 
Huisman H(4); DRAGON consortium.

Collaborators: de Grauw MJJ, van Eekelen L, de Wilde B, van Lohuizen Q, Stegeman 
M, Rutten K, Smit IME, Stultiens G, Overduin CG, Rutten MJCM, Scholten ET, van 
der Post RS, Grünberg K, Vos S, Taken EMG, Nagtegaal ID, Mickan A, Groeneveld M, 
Gerke PK, Meakin JA, Looijen-Salamon MG, de Haas TLM, Hoitsma F, D'Amato M, de 
Rooij M.

Author information:
(1)Diagnostic Image Analysis Group, Department of Medical Imaging, Radboud 
University Medical Center, Nijmegen, The Netherlands. 
Joeran.Bosma@radboudumc.nl.
(2)Department of Health & Information Technology, Ziekenhuisgroep Twente, 
Almelo, The Netherlands. Joeran.Bosma@radboudumc.nl.
(3)Department of Radiology, Netherlands Cancer Institute, Amsterdam, The 
Netherlands. Joeran.Bosma@radboudumc.nl.
(4)Diagnostic Image Analysis Group, Department of Medical Imaging, Radboud 
University Medical Center, Nijmegen, The Netherlands.
(5)Department of Radiology, University Medical Center Groningen, Groningen, The 
Netherlands.
(6)Minimally Invasive Image-Guided Intervention Center, Department of Medical 
Imaging, Radboud University Medical Center, Nijmegen, The Netherlands.
(7)Computational Pathology Group, Department of Pathology, Radboud University 
Medical Center, Nijmegen, The Netherlands.
(8)Department of Radiology, Ziekenhuisgroep Twente, Almelo, The Netherlands.
(9)Department of Radiology, Netherlands Cancer Institute, Amsterdam, The 
Netherlands.
(10)Department of Medical Imaging, Radboud University Medical Center, Nijmegen, 
The Netherlands.
(11)Department of Health & Information Technology, Ziekenhuisgroep Twente, 
Almelo, The Netherlands.

Artificial Intelligence can mitigate the global shortage of medical diagnostic 
personnel but requires large-scale annotated datasets to train clinical 
algorithms. Natural Language Processing (NLP), including Large Language Models 
(LLMs), shows great potential for annotating clinical data to facilitate 
algorithm development but remains underexplored due to a lack of public 
benchmarks. This study introduces the DRAGON challenge, a benchmark for clinical 
NLP with 28 tasks and 28,824 annotated medical reports from five Dutch care 
centers. It facilitates automated, large-scale, cost-effective data annotation. 
Foundational LLMs were pretrained using four million clinical reports from a 
sixth Dutch care center. Evaluations showed the superiority of domain-specific 
pretraining (DRAGON 2025 test score of 0.770) and mixed-domain pretraining 
(0.756), compared to general-domain pretraining (0.734, p < 0.005). While strong 
performance was achieved on 18/28 tasks, performance was subpar on 10/28 tasks, 
uncovering where innovations are needed. Benchmark, code, and foundational LLMs 
are publicly available.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01626-x
PMCID: PMC12084576
PMID: 40379835

Conflict of interest statement: Competing interests: A.S. has received lecture 
honorarium from Guerbet. F.C. has been Chair of the Scientific and Medical 
Advisory Board of TRIBVN Healthcare, received advisory board fees from TRIBVN 
Healthcare, and is shareholder in Aiosyn BV.


751. Aesthet Surg J. 2025 Oct 16;45(11):1206-1212. doi: 10.1093/asj/sjaf120.

Specialized Artificial Intelligence Clinical Decision Support System for 
Aesthetic Surgery: Aesthetic Surgery Using Retrieval Augmentation (AURA).

Ozmen BB, Singh N, Shah K, Berber I, Singh D, Pinsky E, Sinclair NR, Isakov R, 
Schwarz GS.

BACKGROUND: Aesthetic surgery requires integration of specialized knowledge with 
clinical expertise, yet traditional literature search methods are time-intensive 
when addressing specific clinical questions. Although artificial intelligence 
(AI) has transformed various aspects of healthcare delivery, no AI clinical 
decision support systems have been specifically developed to enhance 
evidence-based practice in aesthetic surgery.
OBJECTIVES: The authors aim to develop and evaluate Aesthetic Surgery Using 
Retrieval Augmentation (AURA), a novel AI-powered clinical decision support 
system designed specifically for evidence-based guidance in aesthetic surgery.
METHODS: AURA integrates a comprehensive database of 6546 full-text open-access 
aesthetic surgery publications (January 2001-September 2024) with a commercial 
large language model using retrieval-augmented generation technology. System 
performance was rigorously assessed across 14 complex clinical scenarios 
spanning facial rejuvenation, body contouring, breast procedures, and general 
aesthetic surgery considerations. Evaluation metrics included faithfulness to 
source materials (0-1), answer relevancy (0-1), G-Eval correctness (0-1), 
semantic quality scores (SEM), and SEM confidence ratings.
RESULTS: AURA demonstrated exceptional performance with mean scores of 0.94 for 
faithfulness, 0.86 for answer relevancy, and 0.77 for factual correctness. 
Semantic evaluation revealed strong results with average scores of 0.73 (SEM 
score) and 0.80 (SEM max similarity), predominantly with moderate confidence 
ratings. Performance was notably stronger for established techniques and safety 
considerations compared with emerging procedures requiring comparative analysis.
CONCLUSIONS: In this study, the authors present AURA, the first specialized AI 
clinical decision support system for aesthetic surgery. This novel system 
effectively delivers relevant, accurate information across diverse aesthetic 
surgery domains based on peer-reviewed literature. AURA offers aesthetic 
surgeons an efficient, evidence-based resource for clinical decision support. 
Future development should focus on expanding knowledge sources and prospective 
clinical validation, with implementation emphasizing transparent source 
presentation to complement surgical expertise.

© The Author(s) 2025. Published by Oxford University Press on behalf of The 
Aesthetic Society. All rights reserved. For commercial re-use, please contact 
reprints@oup.com for reprints and translation rights for reprints. All other 
permissions can be obtained through our RightsLink service via the Permissions 
link on the article page on our site—for further information please contact 
journals.permissions@oup.com.

DOI: 10.1093/asj/sjaf120
PMID: 40579186 [Indexed for MEDLINE]


752. IEEE J Biomed Health Inform. 2025 Oct 16;PP. doi: 10.1109/JBHI.2025.3621907. 
Online ahead of print.

EMPOWER: Evolutionary Medical Prompt Optimization With Reinforcement Learning.

Chen Y, He Y, Yang J, Zhang D, Yuan Z, Khan MA, Baili J, Yee PL.

Prompt engineering significantly influences the reliability and clinical utility 
of Large Language Models (LLMs) in medical applications. Current optimization 
approaches inadequately address domain-specific medical knowledge and safety 
requirements. This paper introduces EMPOWER, a novel evolutionary framework that 
enhances medical prompt quality through specialized representation learning, 
multi-dimensional evaluation, and structure-preserving algorithms. Our 
methodology incorporates: (1) a medical terminology attention mechanism, (2) a 
comprehensive assessment architecture evaluating clarity, specificity, clinical 
relevance, and factual accuracy, (3) a component-level evolutionary algorithm 
preserving clinical reasoning integrity, and (4) a semantic verification module 
ensuring adherence to medical knowledge. Evaluation across diagnostic, 
therapeutic, and educational tasks demonstrates significant improvements: 24.7% 
reduction in factually incorrect content, 19.6% enhancement in domain 
specificity, and 15.3% higher clinician preference in blinded evaluations. The 
framework addresses critical challenges in developing clinically appropriate 
prompts, facilitating more responsible integration of LLMs into healthcare 
settings.

DOI: 10.1109/JBHI.2025.3621907
PMID: 41100233


753. J Plast Reconstr Aesthet Surg. 2025 Jun;105:292-304. doi: 
10.1016/j.bjps.2025.04.015. Epub 2025 Apr 17.

Using ChatGPT to write a literature review on autologous fat grafting.

Manley K(1), Salingaros S(1), Fuchsman AC(1), Dong X(1), Spector JA(2).

Author information:
(1)Division of Plastic and Reconstructive Surgery, Weill Cornell Medicine, New 
York, NY, United States of America.
(2)Division of Plastic and Reconstructive Surgery, Weill Cornell Medicine, New 
York, NY, United States of America. Electronic address: jas2037@med.cornell.edu.

BACKGROUND: ChatGPT is a large language model (LLM) that has been proposed as a 
scientific writing tool, though its ethical use remains a highly debated topic 
within the academic community. This article defines the strengths and weaknesses 
of ChatGPT in writing a plastic surgery literature review and describes proper 
methodologies for optimizing GPT-generated output.
METHODS: ChatGPT-4o was prompted to brainstorm topics for a literature review on 
plastic surgery. Autologous fat grafting was chosen and ChatGPT generated each 
section of the literature review with citations, which were subsequently 
evaluated for accuracy. The ability of medical professionals to discriminate 
between a ChatGPT-generated and published fat grafting abstract was assessed.
RESULTS: ChatGPT successfully conceived and performed a literature review on 
autologous fat grafting. The model performed well in outline creation, article 
summarization, and editing content. It generated a professional review of fat 
grafting, though its claims were generalized, not completely factual, and lacked 
accurate citations. ChatGPT provided 21 citations, 5 of which correctly 
referenced a real article. Eight contained errors in their publication details, 
such as publication dates and author lists. The remaining 8 were unable to be 
found in PubMed (hallucinated). Medical professionals were unable to distinguish 
ChatGPT-generated material from a published abstract.
CONCLUSIONS: With appropriate vigilance, ChatGPT may be cautiously used as a 
writing assistant throughout the literature review process; however, authors 
must verify all scientific claims and citations. ChatGPT's greatest limitation 
remains its tendency to hallucinate, which undermines the reliability of a 
generated manuscript and perpetuates inaccurate information.

Copyright © 2025 British Association of Plastic, Reconstructive and Aesthetic 
Surgeons. Published by Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.bjps.2025.04.015
PMID: 40339455 [Indexed for MEDLINE]


754. Aesthetic Plast Surg. 2025 Sep;49(17):5013-5020. doi: 
10.1007/s00266-025-04836-6. Epub 2025 Apr 14.

Man Versus Machine: A Comparative Study of Human and ChatGPT-Generated Abstracts 
in Plastic Surgery Research.

Pressman SM(1), Garcia JP(2), Borna S(1), Gomez-Cabello CA(1), Haider SA(1), 
Haider CR(3), Forte AJ(4)(5).

Author information:
(1)Division of Plastic Surgery, Mayo Clinic, 4500 San Pablo Rd, Jacksonville, 
FL, 32224, USA.
(2)Department of General Surgery, Mayo Clinic, Rochester, MN, USA.
(3)Department of Physiology and Biomedical Engineering, Mayo Clinic, Rochester, 
MN, USA.
(4)Division of Plastic Surgery, Mayo Clinic, 4500 San Pablo Rd, Jacksonville, 
FL, 32224, USA. ajvforte@yahoo.com.br.
(5)Center for Digital Health, Mayo Clinic, Rochester, MN, USA. 
ajvforte@yahoo.com.br.

BACKGROUND: Since its 2022 release, ChatGPT has gained recognition for its 
potential to expedite time-consuming writing tasks like scientific writing. 
Well-written scientific abstracts are essential for clear and efficient 
communication of research findings. This study aims to explore ChatGPT-4's 
capability to produce well-crafted abstracts.
METHODS: Ten abstract-less plastic surgery articles from PubMed were uploaded to 
ChatGPT, each with a prompt to generate one abstract. Flesch-Kincaid Grade Level 
(FKGL) and Flesch Reading Ease Score (FRES) were calculated for all abstracts. 
Additionally, three physician evaluators blindly assessed the ten original and 
ten ChatGPT-generated abstracts using a 5-point Likert scale. Results were 
compared and analyzed using descriptive statistics with mean and standard 
deviation (SD).
RESULTS: The original abstracts averaged an FKGL of 14.1 (SD 2.9) and an FRES of 
25.2 (SD 14.2), while ChatGPT-generated abstracts had scores of 15.6 (SD 2.4) 
and 15.4 (SD 13.1), respectively. Collectively, evaluators identified two-thirds 
of the ChatGPT abstracts, but preferred the ChatGPT abstracts 90% of the time. 
On average, the evaluators found the ChatGPT abstracts to be more "well written" 
(4.23 vs. 3.50, p value < 0.001) and "clear and concise" (4.30 vs. 3.53, p value 
< 0.001) compared to the original abstracts.
CONCLUSIONS: Despite a slightly higher reading level, evaluators generally 
preferred ChatGPT abstracts, which received higher ratings overall. These 
findings suggest ChatGPT holds promise in expediting the creation of 
high-quality scientific abstracts, potentially enhancing efficiency in research 
and scientific writing tasks. However, due to its exploratory nature, this study 
calls for additional research to validate these promising findings.
LEVEL OF EVIDENCE IV: This journal requires that authors assign a level of 
evidence to each article. For a full description of these Evidence-Based 
Medicine ratings, please refer to the Table of Contents or the online 
Instructions to Authors   www.springer.com/00266.

© 2025. Springer Science+Business Media, LLC, part of Springer Nature and 
International Society of Aesthetic Plastic Surgery.

DOI: 10.1007/s00266-025-04836-6
PMID: 40229613 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: None. Human 
and Animal Rights: This study does not contain direct involvement of human or 
animal participants. Informed Consent: Informed consent is not required for this 
study as it does involve direct involvement of human participants.


755. Int J Med Inform. 2026 Jan 17;210:106297. doi: 10.1016/j.ijmedinf.2026.106297. 
Online ahead of print.

Rule-augmented constraint learning for semantic error detection in MIMIC-III 
knowledge graph.

Noben Ö(1), Durukan Kılıç Ö(2), Rienstra T(2), Dumontier M(2), Celebi R(2).

Author information:
(1)Institute of Data Science, Maastricht University, Paul-Henri Spaaklaan 1, 
Maastricht, 6229 GT, Limburg, Netherlands. Electronic address: 
ozge.erten@outlook.com.
(2)Institute of Data Science, Maastricht University, Paul-Henri Spaaklaan 1, 
Maastricht, 6229 GT, Limburg, Netherlands.

High-quality, error-free data is essential for developing reliable data-driven 
models, particularly in clinical decision support systems where inaccurate 
predictions can have serious consequences. While KGs offer a structured and 
semantically rich representation for clinical data, ensuring their consistency 
and correctness remains a challenge. Existing rule mining techniques provide 
solutions for the automatic extraction of logical constraints from KGs, but they 
often produce redundant or clinically irrelevant rules, especially when dealing 
with numeric or categorical literals such as age or lab values. KG 
constraints-rules intended to capture implausible or conflicting facts in the 
KG-can be used to spot semantic errors: facts that might conform to the 
underlying schema but contradict domain knowledge. In this work, we propose a 
novel framework for constraint learning in clinical KGs that identifies and 
transforms high-confidence rules into clinically plausible constraints. We 
propose two approaches, based on class disjointness and literal clustering 
combined with rule mining. We validate the clinical relevance of these generated 
rules using expert-curated constraints and large language models (LLMs). The 
results on the MIMIC-III clinical dataset show that rule filtering based 
constraint learning effectively preserves clinically meaningful rules that align 
with established medical knowledge. For numeric data, we achieve reliable value 
groupings through our clustering-based method, and the rules derived from these 
groupings were validated by LLMs. Their outputs confirm the clinical relevance 
of a portion of those discovered rules. By providing interpretable and scalable 
solutions to semantic inconsistencies in KGs, this study contributes to 
increasing the KG trustworthiness and its clinical usability.

Copyright © 2026 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2026.106297
PMID: 41570511

Conflict of interest statement: Declaration of competing interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: Özge Noben reports that financial 
support was provided by Maastricht University. Özge Noben states a relationship 
with Maastricht University that includes employment. If there are other authors, 
they declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


756. Radiol Med. 2025 Apr;130(4):453-462. doi: 10.1007/s11547-024-01945-1. Epub 2024 
Dec 17.

How Italian radiation oncologists use ChatGPT: a survey by the young group of 
the Italian association of radiotherapy and clinical oncology (yAIRO).

Piras A(1)(2)(3)(4), Mastroleo F(5)(6), Colciago RR(7), Morelli I(8), D'Aviero 
A(9)(10), Longo S(11), Grassi R(12), Iorio GC(13), De Felice F(14), Boldrini 
L(11)(15), Desideri I(8), Salvestrini V(8).

Author information:
(1)UO Radioterapia Oncologica, Villa Santa Teresa, 90011, Bagheria, Palermo, 
Italy.
(2)Ri.Med Foundation, 90133, Palermo, Italy.
(3)Department of Health Promotion, Mother and Child Care, Internal Medicine and 
Medical Specialties, Molecular and Clinical Medicine, University of Palermo, 
90127, Palermo, Italy.
(4)Radiation Oncology, Mater Olbia Hospital, Olbia, Sassari, Italy.
(5)Division of Radiation Oncology, IEO European Institute of Oncology IRCCS, 
20141, Milan, Italy.
(6)Department of Oncology and Hemato-Oncology, University of Milan, 20141, 
Milan, Italy.
(7)School of Medicine and Surgery, University of Milano Bicocca, Piazza 
Dell'Ateneo Nuovo, 1, 20126, Milan, Italy. riccardoraycolciago@gmail.com.
(8)Radiation Oncology Unit, Department of Experimental and Clinical Biomedical 
Sciences, Azienda Ospedaliero-Universitaria Careggi, University of Florence, 
Florence, Italy.
(9)Department of Radiation Oncology, "S.S Annunziata" Chieti Hospital, Chieti, 
Italy.
(10)Department of Medical, Oral and Biotechnogical Sciences, "G.D'Annunzio" 
University of Chieti, Chieti, Italy.
(11)UOC Radioterapia Oncologica, Fondazione Policlinico Universitario "A. 
Gemelli" IRCCS, Rome, Italy.
(12)Department of Precision Medicine, University of Campania "L. Vanvitelli", 
Naples, Italy.
(13)Department of Oncology, Radiation Oncology, University of Turin, Turin, 
Italy.
(14)Radiation Oncology, Policlinico Umberto I, Department of Radiological, 
Oncological and Pathological Sciences, "Sapienza" University of Rome, Rome, 
Italy.
(15)Università Cattolica del Sacro Cuore, Rome, Italy.

PURPOSE: To investigate the awareness and the spread of ChatGPT and its possible 
role in both scientific research and clinical practice among the young radiation 
oncologists (RO).
MATERIAL AND METHODS: An anonymous, online survey via Google Forms (including 24 
questions) was distributed among young (< 40 years old) ROs in Italy through the 
yAIRO network, from March 15, 2024, to 31, 2024. These ROs were officially 
registered with yAIRO in 2023. We particularly focused on the emerging use of 
ChatGPT and its future perspectives in clinical practice.
RESULTS: A total of 76 young physicians answered the survey. Seventy-three 
participants declared to be familiar with ChatGPT, and 71.1% of the surveyed 
physicians have already used ChatGPT. Thirty-one (40.8%) participants strongly 
agreed that AI has the potential to change the medical landscape in the future. 
Additionally, 79.1% of respondents agreed that AI will be mainly successful in 
research processes such as literature review and drafting articles/protocols. 
The belief in ChatGPT's potential results in direct use in daily practice in 
43.4% of the cases, with mainly a fair grade of satisfaction (43.2%). A large 
part of participants (69.7%) believes in the implementation of ChatGPT into 
clinical practice, even though 53.9% fear an overall negative impact.
CONCLUSIONS: The results of the present survey clearly highlight the attitude of 
young Italian ROs toward the implementation of ChatGPT into clinical and 
academic RO practice. ChatGPT is considered a valuable and effective tool that 
can ease current and future workflows.

© 2024. Italian Society of Medical Radiology.

DOI: 10.1007/s11547-024-01945-1
PMID: 39690359 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper. Given their role as Editorial Board Member, Roberta Grassi and Isacco 
Desideri had no involvement in the peer review of this article and has no 
involvement in the peer review of this article and has no access to information 
regarding its peer review. Ethical approval: The study was conducted in 
accordance with the Declaration of Helsinki. Human ethics and consent to 
participate: Human Ethics and Consent to Participate declarations: not 
applicable.


757. IEEE J Biomed Health Inform. 2025 Oct 16;PP. doi: 10.1109/JBHI.2025.3621512. 
Online ahead of print.

MedTsLLM: Medical Time Series Analysis Using Multimodal LLMs.

Chan N, Parker F, Zhang C, Bennett W, Jia MY, Fackler J, Ghobadi K.

Traditional machine learning approaches for biomedical time series analysis face 
fundamental limitations when integrating the heterogeneous data types essential 
for comprehensive clinical understanding. Physiological signals must be 
interpreted within rich clinical contexts that include patient history, current 
medications, and treatment protocols-information typically stored as 
unstructured text that conventional time series models cannot effectively 
utilize. We propose MedTsLLM, a multimodal model that aims to address this 
critical gap by integrating numerical physiological signals with natural 
language clinical information through large language models (LLMs). Our 
framework incorporates patch reprogramming for time series-LLM alignment and 
introduces two key innovations: novel covariate handling strategies that capture 
complex physiological relationships, and contextual prompting mechanisms that 
incorporate patient-specific information. MedTsLLM addresses four clinically 
significant tasks within a unified architecture: semantic segmentation, boundary 
detection, anomaly detection, and classification. Through comprehensive 
evaluation across diverse medical domains, including ECG analysis, respiratory 
monitoring, and cardiac arrhythmia detection, our approach consistently 
outperforms state-of-the-art baselines across all tasks and datasets. These 
results demonstrate the transformative potential of multimodal LLMs for 
biomedical signal analysis, enabling clinicians to extract deeper insights from 
physiological data while leveraging comprehensive clinical context to enhance 
diagnostic accuracy, patient monitoring, and personalized treatment decisions.

DOI: 10.1109/JBHI.2025.3621512
PMID: 41100234


758. J Am Med Inform Assoc. 2025 Aug 1;32(8):1340-1349. doi: 10.1093/jamia/ocaf093.

RAPID: Reliable and efficient Automatic generation of submission rePortIng 
checklists with large language moDels.

Li Z(1), Luo X(2), Yang Z(3), Zhang H(4), Wang B(2), Ge L(4), Bian Z(3)(5), Zou 
J(6)(7)(8), Chen Y(2), Zhang L(1)(9); ADVANCED Working Group.

Collaborators: Wang Y, Li H, Zhu D, Yao Y, Peng D, Lai H, Zhang J, Wang F, Duan 
M, Li Y, Tang S.

Author information:
(1)Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR 
999077, China.
(2)Research Unit of Evidence-Based Evaluation and Guidelines, Chinese Academy of 
Medical Sciences (2021RU017), School of Basic Medical Sciences, Lanzhou 
University, Lanzhou 730000, China.
(3)Vincent V.C. Woo Chinese Medicine Clinical Research Institute, School of 
Chinese Medicine, Hong Kong Baptist University, Hong Kong SAR 999077, China.
(4)School of Public Health, Lanzhou University, Lanzhou 730000, China.
(5)Chinese EQUATOR Centre, Hong Kong Baptist University, Hong Kong SAR 999077, 
China.
(6)Department of Biomedical Data Science, Stanford University, Stanford, CA 
94305, United States.
(7)Department of Electrical Engineering, Stanford University, Stanford, CA 
94305, United States.
(8)Department of Computer Science, Stanford University, Stanford, CA 94305, 
United States.
(9)Institute of Systems Medicine and Health Science, Hong Kong Baptist 
University, Hong Kong SAR 999077, China.

OBJECTIVE: To evaluate an automated reporting checklist generation tool using 
large language models and retrieval augmentation generation technology, called 
RAPID.
MATERIALS AND METHODS: This study utilized large language models to develop a 
retrieval augmentation generation architecture. To assess its performance, a 
total of 91 published journal articles were collected and manually annotated in 
accordance with the CONSORT and CONSORT-AI medical reporting guidelines. These 
articles comprised 50 randomized controlled trials conducted without AI 
intervention and 41 randomized controlled trials that incorporated AI tools.
RESULTS: Fifty RCT articles without the intervention of AI tools and 41 RCT 
articles with the intervention of AI tools were collected as CONSORT and 
CONSORT-AI datasets. All of the CONSORT reporting items (37) were included in 
the tool. RAPID achieved a high average accuracy rate of 92.11% and a content 
consistency score of 81.14% on the CONSORT dataset. Of the CONSORT-AI reporting 
items, 11 items related to the intervention of AI tools were included in the 
tool. RAPID achieved an average accuracy of 83.81% with a content consistency 
score of 72.51% on the CONSORT-AI dataset.
DISCUSSION: RAPID may effectively save time and improve working efficiency for 
different user groups such as medical authors, researchers, editors, and 
reviewers.
CONCLUSION: RAPID has strong scalability, which can be easily adapted to 
different medical reporting guidelines without transfer learning on a large 
dataset. RAPID got state-of-the-art performance on 2 datasets for 2 different 
checklists compared to other methods.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf093
PMCID: PMC12277685
PMID: 40574698 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
declare.


759. J Am Med Inform Assoc. 2025 Jun 1;32(6):1032-1039. doi: 10.1093/jamia/ocaf059.

Detecting emergencies in patient portal messages using large language models and 
knowledge graph-based retrieval-augmented generation.

Liu S(1)(2), Wright AP(1)(3), McCoy AB(1), Huang SS(1)(3), Steitz B(1), Wright 
A(1)(3).

Author information:
(1)Department of Biomedical Informatics, Vanderbilt University Medical Center, 
Nashville, TN 37212, United States.
(2)Department of Computer Science, Vanderbilt University, Nashville, TN 37240, 
United States.
(3)Department of Medicine, Vanderbilt University Medical Center, Nashville, TN 
37232, United States.

OBJECTIVES: This study aims to develop and evaluate an approach using large 
language models (LLMs) and a knowledge graph to triage patient messages that 
need emergency care. The goal is to notify patients when their messages indicate 
an emergency, guiding them to seek immediate help rather than using the patient 
portal, to improve patient safety.
MATERIALS AND METHODS: We selected 1020 messages sent to Vanderbilt University 
Medical Center providers between January 1, 2022 and March 7, 2023. We developed 
four models to triage these messages for emergencies: (1) Prompt-Only: the 
patient message was input with a prompt directly into the LLM; (2) Naïve 
Retrieval Augmented Generation (RAG): provided retrieved information as context 
to the LLM; (3) RAG from Knowledge Graph with Local Search: a knowledge graph 
was used to retrieve locally relevant information based on semantic 
similarities; (4) RAG from Knowledge Graph with Global Search: a knowledge graph 
was used to retrieve globally relevant information through hierarchical 
community detection. The knowledge base was a triage book covering 225 
protocols.
RESULTS: The RAG from Knowledge Graph model with global search outperformed 
other models, achieving an accuracy of 0.99, a sensitivity of 0.98, and a 
specificity of 0.99. It demonstrated significant improvements in triaging 
emergency messages compared to LLM without RAG and naïve RAG.
DISCUSSION: The traditional LLM without any retrieval mechanism underperformed 
compared to models with RAG, which aligns with the expected benefits of 
augmenting LLMs with domain-specific knowledge sources. Our results suggest that 
providing external knowledge, especially in a structured manner and in community 
summaries, can improve LLM performance in triaging patient portal messages.
CONCLUSION: LLMs can effectively assist in triaging emergency patient messages 
after integrating with a knowledge graph about a nurse triage book. Future 
research should focus on expanding the knowledge graph and deploying the system 
to evaluate its impact on patient outcomes.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf059
PMCID: PMC12089757
PMID: 40220286 [Indexed for MEDLINE]

Conflict of interest statement: None declared.


760. World J Gastroenterol. 2025 Oct 7;31(37):111327. doi: 
10.3748/wjg.v31.i37.111327.

Role of artificial intelligence in gastric diseases.

Gong EJ(1), Woo J(2), Lee JJ(2), Bang CS(3).

Author information:
(1)Department of Internal Medicine, Hallym University College of Medicine, 
Chuncheon 24253, Gangwon-do, South Korea.
(2)Institute of New Frontier Research, Hallym University College of Medicine, 
Chuncheon 24253, Gangwon-do, South Korea.
(3)Department of Internal Medicine, Hallym University College of Medicine, 
Chuncheon 24253, Gangwon-do, South Korea. csbang@hallym.ac.kr.

The integration of artificial intelligence (AI) in gastroenterology has evolved 
from basic computer-aided detection to sophisticated multimodal frameworks that 
enable real-time clinical decision support. This study presents AI applications 
in gastric disease diagnosis and management, highlighting the transition from 
domain-specific deep learning to general-purpose large language models. Our 
research reveals a key finding: AI effectiveness demonstrates an inverse 
relationship with user expertise, with moderate-expertise practitioners 
benefiting the most, whereas experts and novices show limited performance gains. 
We developed a clinical decision support system achieving 96% lesion detection 
internally and 82%-87% classification accuracy in external validation. 
Multimodal integration, which combines endoscopic images, clinical histories, 
laboratory results, and genomic data, enables comprehensive disease assessment 
and personalized treatment. The emergence of large language models with 
expanding context windows and multiagent architectures represents a paradigm 
shift in medical AI. Furthermore, emerging technologies are expanding AI's 
potential applications, and feasibility studies on smart glasses in endoscopy 
training suggest opportunities for hands-free assistance, although clinical 
implementation challenges persist. This minireview addresses persistent 
limitations including geographic bias in training data, regulatory hurdles, 
ethical considerations regarding patient privacy and AI accountability, and the 
concentration of AI development among technology giants. Successful integration 
requires balancing innovation with patient safety, while preserving the 
irreplaceable role of human clinical judgment.

©The Author(s) 2025. Published by Baishideng Publishing Group Inc. All rights 
reserved.

DOI: 10.3748/wjg.v31.i37.111327
PMCID: PMC12476687
PMID: 41025012 [Indexed for MEDLINE]

Conflict of interest statement: Conflict-of-interest statement: All the authors 
report no relevant conflicts of interest for this article.


761. Acad Radiol. 2025 Apr;32(4):1785-1790. doi: 10.1016/j.acra.2025.01.017. Epub 
2025 Feb 5.

Evaluating the Efficacy of Perplexity Scores in Distinguishing AI-Generated and 
Human-Written Abstracts.

Elek A(1), Yildiz HS(2), Akca B(3), Oren NC(4), Gundogdu B(5).

Author information:
(1)Ege University Faculty of Medicine, Izmir, Turkey (A.E.). Electronic address: 
alperenelek13@gmail.com.
(2)Adıyaman University Faculty of Medicine, Adıyaman, Turkey (H.S.Y.).
(3)Marmara University, Electrical-Electronics Engineering Department, Istanbul, 
Turkey (B.A.).
(4)Advanced Midwest Radiology Illinois, Oak Brook, Illinois (N.C.O.).
(5)University of Chicago, Department of Radiology, Chicago, Illinois (B.G.).

RATIONALE AND OBJECTIVES: We aimed to evaluate the efficacy of perplexity scores 
in distinguishing between human-written and AI-generated radiology abstracts and 
to assess the relative performance of available AI detection tools in detecting 
AI-generated content.
METHODS: Academic articles were curated from PubMed using the keywords 
"neuroimaging" and "angiography." Filters included English-language, open-access 
articles with abstracts without subheadings, published before 2021, and within 
Chatbot processing word limits. The first 50 qualifying articles were selected, 
and their full texts were used to create AI-generated abstracts. Perplexity 
scores, which estimate sentence predictability, were calculated for both 
AI-generated and human-written abstracts. The performance of three AI tools in 
discriminating human-written from AI-generated abstracts was assessed.
RESULTS: The selected 50 articles consist of 22 review articles (44%), 12 case 
or technical reports (24%), 15 research articles (30%), and one editorial (2%). 
The perplexity scores for human-written abstracts (median; 35.9 IQR; 25.11-51.8) 
were higher than those for AI-generated abstracts (median; 21.2 IQR; 
16.87-28.38), (p=0.057) with an AUC=0.7794. One AI tool performed less than 
chance in identifying human-written from AI-generated abstracts with an accuracy 
of 36% (p>0.05) while another tool yielded an accuracy of 95% with an 
AUC=0.8688.
CONCLUSION: This study underscores the potential of perplexity scores in 
detecting AI-generated and potentially fraudulent abstracts. However, more 
research is needed to further explore these findings and their implications for 
the use of AI in academic writing. Future studies could also investigate other 
metrics or methods for distinguishing between human-written and AI-generated 
texts.

Copyright © 2025 The Association of University Radiologists. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.acra.2025.01.017
PMID: 39915182 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


762. Plast Reconstr Surg Glob Open. 2025 Jun 12;13(6):e6871. doi: 
10.1097/GOX.0000000000006871. eCollection 2025 Jun.

Diagnostic Accuracy of Microsoft's Copilot Artificial Intelligence in Chronic 
Wound Assessment: A Comparative Study.

Tadrousse K(1), Cash CA(1), Kastury MR(1), Thompson N(1), Simman R(2)(3).

Author information:
(1)From the College of Medicine and Life Sciences, University of Toledo, Toledo, 
OH.
(2)Department of Surgery, College of Medicine and Life Sciences, University of 
Toledo, Toledo, OH.
(3)Jobst Vascular Institute, ProMedica Health Network, Toledo, OH.

BACKGROUND: Chronic wounds affect approximately 2.5% of the US population and 
can cause severe complications if not identified and treated promptly. 
Artificial intelligence tools such as Microsoft's Copilot have the potential to 
expedite diagnosis, but their clinical diagnostic accuracy remains 
underexplored.
METHODS: Ten chronic wound cases were selected from the publicly available 
database of the Silesian University of Technology. Images and demographic data 
were entered into Copilot, which generated the top 3 differential diagnoses for 
each case. Diagnostic accuracy was evaluated using a predefined scoring system. 
Statistical analysis included descriptive statistics, the Wilcoxon signed-rank 
test, bootstrapping, the Fisher-Pitman permutation test, Cohen kappa, and Fisher 
exact test.
RESULTS: Copilot correctly identified the primary diagnosis in 30% of cases and 
included the correct diagnosis within its top 3 differentials in 70% of cases. 
The mean diagnostic score was 1.7 (median: 2, SD: 1.25, variance: 1.57). The 
Wilcoxon test indicated no significant deviation from the median reference value 
(P = 0.6364), whereas bootstrapping yielded a 95% confidence interval of 1-4. 
The permutation test demonstrated a significant difference from the null 
hypothesis (P = 0.017), and the Cohen kappa revealed perfect agreement (kappa = 
1, P = 0.00157). The Fisher exact test showed no significant association between 
primary and top 3 diagnostic accuracy (P = 0.20).
CONCLUSIONS: Microsoft Copilot demonstrated limited diagnostic accuracy in 
chronic wound assessment, underscoring the need for cautious integration into 
clinical workflows. Broader datasets and more rigorous validation are crucial 
for enhancing artificial intelligence-supported diagnostics in wound care.

Copyright © 2025 The Authors. Published by Wolters Kluwer Health, Inc. on behalf 
of The American Society of Plastic Surgeons.

DOI: 10.1097/GOX.0000000000006871
PMCID: PMC12160731
PMID: 40510430

Conflict of interest statement: The authors have no financial interest to 
declare in relation to the content of this article.


763. J Med Internet Res. 2025 Jun 11;27:e72638. doi: 10.2196/72638.

Enhancing Pulmonary Disease Prediction Using Large Language Models With Feature 
Summarization and Hybrid Retrieval-Augmented Generation: Multicenter 
Methodological Study Based on Radiology Report.

Li R(#)(1), Mao S(#)(2), Zhu C(1), Yang Y(1), Tan C(3), Li L(4), Mu X(4), Liu 
H(#)(1), Yang Y(#)(2).

Author information:
(1)School of Biomedical Engineering, Capital Medical University, No. 10, 
Xitoutiao, You An Men, Fengtai District, Beijing, 100069, China, 86 
010-83911542.
(2)State Key Laboratory of Networking and Switching Technology, Beijing 
University of Posts and Telecommunications, Beijing, China.
(3)Department of Respiratory Medicine, Beijing Friendship Hospital, Capital 
Medical University, Beijing, China.
(4)Beijing Respiratory and Critical Care Medicine Department, Tsinghua Changgung 
Hospital, School of Clinical Medicine, Tsinghua University, Beijing, China.
(#)Contributed equally

BACKGROUND: The rapid advancements in natural language processing, particularly 
the development of large language models (LLMs), have opened new avenues for 
managing complex clinical text data. However, the inherent complexity and 
specificity of medical texts present significant challenges for the practical 
application of prompt engineering in diagnostic tasks.
OBJECTIVE: This paper explores LLMs with new prompt engineering technology to 
enhance model interpretability and improve the prediction performance of 
pulmonary disease based on a traditional deep learning model.
METHODS: A retrospective dataset including 2965 chest CT radiology reports was 
constructed. The reports were from 4 cohorts, namely, healthy individuals and 
patients with pulmonary tuberculosis, lung cancer, and pneumonia. Then, a novel 
prompt engineering strategy that integrates feature summarization (F-Sum), chain 
of thought (CoT) reasoning, and a hybrid retrieval-augmented generation (RAG) 
framework was proposed. A feature summarization approach, leveraging term 
frequency-inverse document frequency (TF-IDF) and K-means clustering, was used 
to extract and distill key radiological findings related to 3 diseases. 
Simultaneously, the hybrid RAG framework combined dense and sparse vector 
representations to enhance LLMs' comprehension of disease-related text. In 
total, 3 state-of-the-art LLMs, GLM-4-Plus, GLM-4-air (Zhipu AI), and GPT-4o 
(OpenAI), were integrated with the prompt strategy to evaluate the efficiency in 
recognizing pneumonia, tuberculosis, and lung cancer. The traditional deep 
learning model, BERT (Bidirectional Encoder Representations from Transformers), 
was also compared to assess the superiority of LLMs. Finally, the proposed 
method was tested on an external validation dataset consisted of 343 chest 
computed tomography (CT) report from another hospital.
RESULTS: Compared with BERT-based prediction model and various other prompt 
engineering techniques, our method with GLM-4-Plus achieved the best performance 
on test dataset, attaining an F1-score of 0.89 and accuracy of 0.89. On the 
external validation dataset, F1-score (0.86) and accuracy (0.92) of the proposed 
method with GPT-4o were the highest. Compared to the popular strategy with 
manually selected typical samples (few-shot) and CoT designed by doctors 
(F1-score=0.83 and accuracy=0.83), the proposed method that summarized disease 
characteristics (F-Sum) based on LLM and automatically generated CoT performed 
better (F1-score=0.89 and accuracy=0.90). Although the BERT-based model got 
similar results on the test dataset (F1-score=0.85 and accuracy=0.88), its 
predictive performance significantly decreased on the external validation set 
(F1-score=0.48 and accuracy=0.78).
CONCLUSIONS: These findings highlight the potential of LLMs to revolutionize 
pulmonary disease prediction, particularly in resource-constrained settings, by 
surpassing traditional models in both accuracy and flexibility. The proposed 
prompt engineering strategy not only improves predictive performance but also 
enhances the adaptability of LLMs in complex medical contexts, offering a 
promising tool for advancing disease diagnosis and clinical decision-making.

© Ronghao Li, Shuai Mao, Congmin Zhu, Yingliang Yang, Chunting Tan, Li Li, 
Xiangdong Mu, Honglei Liu, Yuqing Yang. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/72638
PMCID: PMC12176309
PMID: 40499132 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


764. IEEE J Biomed Health Inform. 2025 Dec 10;PP. doi: 10.1109/JBHI.2025.3631700. 
Online ahead of print.

Bridging Brain and Machine: A Neurocognitive Framework for Early Alzheimer's 
Detection Through Hierarchical Speech Processing.

Lyu X, Bai J, Li P, Zhang F, Zhang X, Gu Y, Liu N.

Alzheimer's disease progressively disrupts temporal-frontal language networks, 
manifesting through degraded speech production detectable years before clinical 
dementia diagnosis. Traditional automated detection systems rely on handcrafted 
acoustic features, including mel-frequency cepstral coefficients, that may 
discard diagnostically relevant temporal dynamics through fixed spectral 
transformations. Drawing inspiration from cognitive neuroscience models of 
hierarchical speech processing, we propose the neuro residual acoustic learning 
framework, an end-to-end system operating directly on raw waveforms while 
incorporating architectural principles mirroring biological auditory pathways. 
The framework implements acoustic-symbolic transformation bridging continuous 
signals and discrete phonetic-acoustic categories analogous to primary auditory 
cortex processing, hierarchical temporal encoding through residual blocks with 
dilated convolutions capturing multi-scale patterns across phoneme to utterance 
timescales, and squeeze-and-excitation attention mechanisms modeling prefrontal 
modulation of temporal representations. Evaluation on NCMMSC2021 Mandarin speech 
demonstrates 86.55% accuracy on extended utterances and 81.35% on brief 
segments, surpassing contemporary methods, including multimodal architectures 
and large language model hybrids, by 2.88% and 2.43% respectively. 
Cross-linguistic validation on INTERSPEECH2020 ADReSS English speech achieves 
70.83% accuracy, exceeding baselines by 2.08%. Confusion matrix analysis reveals 
93.3% healthy control recall and asymmetric Alzheimer's disease versus mild 
cognitive impairment misclassification patterns consistent with prodromal 
acoustic biomarkers.

DOI: 10.1109/JBHI.2025.3631700
PMID: 41370154


765. Crit Care. 2025 Jun 12;29(1):238. doi: 10.1186/s13054-025-05479-4.

Primer on large language models: an educational overview for intensivists.

Idan D(1), Einav S(2).

Author information:
(1)Ben-Gurion Faculty of Health Sciences, Beer-Sheva, Israel. 
daphnaid@post.bgu.ac.il.
(2)Hebrew University Faculty of Medicine and Regional Medical Director at 
Maccabi Healthcare and Chief Scientist, Medint Medical Intelligence, Hebrew 
University, Jerusalem, Israel.

The integration of artificial intelligence (AI) and machine learning-enabled 
medical technologies into clinical practice is expanding at an unprecedented 
pace. Among these, large language models (LLMs) represent a subset of machine 
learning designed to comprehend linguistic patterns, semantics, and contextual 
meaning by processing vast amounts of textual data. This educational primer aims 
to inform intensivists on the foundational concepts of LLMs and how to approach 
emerging literature in this area. In critical care, LLMs have the potential to 
enhance various aspects of patient management, from triage and clinical 
documentation to diagnostic support and prognostic assessment of patient 
deterioration. They have also demonstrated high appropriateness in addressing 
critical care-related clinical inquiries and are increasingly recognized for 
their role in post-ICU rehabilitation and as educational resources for patients' 
families and caregivers. Despite these promising applications, LLMs still have 
significant limitations, and integrating LLMs into clinical workflows presents 
inherent challenges, particularly concerning bias, reliability, and 
transparency. Given their emerging role as decision-support tools and potential 
collaborative partners in medicine, LLMs must adhere to rigorous validation and 
quality assurance standards. As the trajectory toward AI-driven healthcare 
continues, responsible and evidence-based integration of LLMs into critical care 
practice is imperative to optimize patient outcomes while ensuring ethical and 
equitable deployment.

© 2025. The Author(s).

DOI: 10.1186/s13054-025-05479-4
PMCID: PMC12164094
PMID: 40506762 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: Daphna Idan 
is a medical student and data analyst and has no relevant conflict of interests 
to disclose. Sharon Einav is a Cochrane Editor and is involved in developing an 
LLM for use by clinicians.


766. Arthroscopy. 2025 Nov;41(11):4928-4945.e2. doi: 10.1016/j.arthro.2025.03.066. 
Epub 2025 Apr 8.

Large Language Model Use Cases in Health Care Research Are Redundant and Often 
Lack Appropriate Methodological Conduct: A Scoping Review and Call for Improved 
Practices.

Kunze KN(1), Gerhold C(2), Dave U(2), Abunnur N(2), Mamonov A(2), Nwachukwu 
BU(3), Verma NN(2), Chahla J(2).

Author information:
(1)Department of Orthopaedic Surgery, Hospital for Special Surgery, New York, 
New York, U.S.A.. Electronic address: Kylekunze7@gmail.com.
(2)Midwest Orthopaedics at Rush, Chicago, Illinois, U.S.A.
(3)Department of Orthopaedic Surgery, Hospital for Special Surgery, New York, 
New York, U.S.A.

PURPOSE: To describe the current use cases of large language models (LLMs) in 
musculoskeletal medicine and to evaluate the methodologic conduct of these 
investigations in order to safeguard future implementation of LLMs in clinical 
research and identify key areas for methodological improvement.
METHODS: A comprehensive literature search was performed in accordance with the 
Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines 
using PubMed, Cochrane Library, and Embase databases to identify eligible 
studies. Included studies evaluated the use of LLMs within any realm of 
orthopaedic surgery, regardless of its application in a clinical or educational 
setting. Methodological Index for Non-Randomized Studies criteria was used to 
assess the quality of all included studies.
RESULTS: In total, 114 studies published from 2022 to 2024 were identified. 
Extensive use case redundancy was observed, and 5 main categories of clinical 
applications of LLMs were identified: 48 studies (42.1%) that assessed the 
ability to answer patient questions, 24 studies (21.1%) that evaluated the 
ability to diagnose and manage medical conditions, 21 studies (18.4%) that 
evaluated the ability to take orthopaedic examinations, 11 studies (9.6%) that 
analyzed the ability to develop or evaluate patient educational materials, and 
10 studies (8.8%) concerning other applications, such as generating images, 
generating discharge documents and clinical letters, writing scientific 
abstracts and manuscripts, and enhancing billing efficiency. General 
orthopaedics was the focus of most included studies (n = 39, 34.2%), followed by 
orthopaedic sports medicine (n = 18, 15.8%), and adult reconstructive surgery (n 
= 17, 14.9%). ChatGPT 3.5 was the most common LLM used or evaluated (n = 79, 
69.2%), followed by ChatGPT 4.0 (n = 47, 41.2%). Methodological inconsistency 
was prevalent among studies, with 36 (31.6%) studies failing to disclose the 
exact prompts used, 64 (56.1%) failing to disclose the exact outputs generated 
by the LLM, and only 7 (6.1%) evaluating different prompting strategies to 
elicit desired outputs. No studies attempted to investigate how the influence of 
race or gender influenced model outputs.
CONCLUSIONS: Among studies evaluating LLM health care use cases, the scope of 
clinical investigations was limited, with most studies showing redundant use 
cases. Because of infrequently reported descriptions of prompting strategies, 
incomplete model specifications, failure to disclose exact model outputs, and 
limited attempts to address bias, methodological inconsistency was concerningly 
extensive.
CLINICAL RELEVANCE: A comprehensive understanding of current LLM use cases is 
critical to familiarize providers with the possibilities through which this 
technology may be used in clinical practice. As LLM health care applications 
transition from research to clinical integration, model transparency and 
trustworthiness is critical. The results of the current study suggest that 
guidance is urgently needed, with focus on promoting appropriate methodological 
conduct practices and novel use cases to advance the field.

Copyright © 2025 Arthroscopy Association of North America. Published by Elsevier 
Inc. All rights reserved.

DOI: 10.1016/j.arthro.2025.03.066
PMID: 40209833 [Indexed for MEDLINE]

Conflict of interest statement: Disclosures The authors declare the following 
financial interests/personal relationships which may be considered as potential 
competing interests: J.C. reports board or committee member, American 
Orthopaedic Society for Sports Medicine, Arthroscopy Association of North 
America, and International Society of Arthroscopy, Knee Surgery, and Orthopaedic 
Sports Medicine; paid consultant for Arthrex, CONMED Linvatec, Ossur, and RTI 
Surgical; hospitality payments from Breg, DePuy Synthes Sales, Joint Restoration 
Foundation, Medical Device Business Services, Pacira Pharmaceuticals, and 
SI-Bone; educational support from Midwest Associates; paid consultant and paid 
presenter or speaker for Smith & Nephew; and paid consultant for and hospitality 
payments from Vericel Corporation. K.K. reports consulting with AllaiHealth and 
stocks from BICMD. B.N. reports stocks from BICMD and consulting with Figur8. 
N.N.V. reports hospitality payments from Abbott Laboratories, Axonics, Boston 
Scientific Corporation, Foundation Fusion Solutions LLC, IBSA Pharma, Nalu 
Medical, Nevro, Orthofix Medical, Pacira Pharmaceuticals, Relievant Medsystems, 
Salix Pharmaceuticals, Vericel Corporation, and Vertos Medical; board or 
committee member: American Orthopaedic Society for Sports Medicine, American 
Shoulder and Elbow Surgeons, and Arthroscopy Association of North America; IP 
royalties and research support from Arthrex; research support from Breg and 
Ossur; IP royalties from Graymont Professional Products IP LLC; paid consultant 
with Medacta USA; educational support from Medwest Associates; editorial or 
governing board of SLACK Incorporated; IP royalties and research support from 
Smith & Nephew; travel and lodging from Spinal Simplicity; and IP royalties from 
paid consultant for, and research support from, Stryker. All other authors 
(N.A., U.D., C.G., A.M.) declare that they have no known competing financial 
interests or personal relationships that could have appeared to influence the 
work reported in this paper.


767. Digit Health. 2025 Nov 13;11:20552076251394622. doi: 10.1177/20552076251394622. 
eCollection 2025 Jan-Dec.

Clinical reasoning from real-world oncology reports using large language models.

Park JH(1)(2), Kim S(1), Heo J(1).

Author information:
(1)Department of Radiation Oncology, Ajou University School of Medicine, Suwon, 
Republic of Korea.
(2)Department of Biomedical Sciences, Graduate School of Ajou University, Suwon, 
Republic of Korea.

OBJECTIVE: To evaluate the ability of large language models (LLMs) to perform 
structured information extraction and guideline-based clinical inferences from 
radiology and pathology reports in real-world oncology.
METHODS: We constructed a Question Answering (Q&A) benchmark dataset using 3650 
radiological and 588 pathological reports from 1632 patients. The tasks included 
direct extraction of genomic and histological findings, as well as clinical 
reasoning tasks, such as Response Evaluation Criteria in Solid Tumors 
(RECIST)-based tumor response classification and American Joint Committee on 
Cancer (AJCC)-based tumor-node-metastasis (TNM) staging. We compared the 
performance of the Gemma family of open-source LLMs (Gemma 4B, a lightweight 
4-billion parameter model, and Gemma 12B, a larger 12-billion parameter model) 
with and without structured reasoning prompts designed according to clinical 
guidelines.
RESULTS: The 12B model achieved high performance in direct extraction tasks from 
pathology reports, with F1-score ranging from 92.6 to 93.3 across genomic and 
histological variables. Furthermore, when guided by structured reasoning 
prompts, it also showed substantial improvements in reasoning tasks, achieving 
an F1-score of 81.5 (95% CI: 79.8-83.3) for tumor response, 74.3 (95% CI: 
70.8-77.8) for T-stage, 87.1 (95% CI: 85.1-89.0) for N-stage, and 90.8 (95% CI: 
89.1-92.2) for M-stage. In contrast, the 4B model showed inconsistent 
performance and was sometimes degraded under reasoning prompts.
CONCLUSION: This study shows that LLMs can perform complex guideline-based 
clinical reasoning using real-world radiology reports. By combining the 
RECIST/AJCC criteria with structured prompts, we demonstrated how LLMs can move 
beyond surface-level extraction to support nuanced inference in oncology, with 
implications for future clinical applications.

© The Author(s) 2025.

DOI: 10.1177/20552076251394622
PMCID: PMC12615922
PMID: 41246210

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


768. Int J Dermatol. 2025 Nov 6. doi: 10.1111/ijd.70110. Online ahead of print.

Artificial Intelligence Use in Acne Diagnosis and Management-A Scoping Review.

Frederickson KL(1), Gui H(2), Barbieri JS(3), Daneshjou R(2).

Author information:
(1)Meharry Medical College, Nashville, Tennessee, USA.
(2)Department of Dermatology, Stanford University, Redwood City, California, 
USA.
(3)Department of Dermatology, Brigham and Women's Hospital, Boston, 
Massachusetts, USA.

Artificial intelligence (AI) techniques can allow for early diagnosis and 
treatment of acne. Bias in AI model training remains, leading to various 
challenges in achieving health equity in clinical practice. We aim to assess and 
provide an updated overview of (1) the types of AI-based tools developed for 
acne, (2) the various applications of AI in acne diagnosis and management, (3) 
the performance of these tools, and (4) the current data reported on skin 
diversity in AI model training. [Correction added on 27 December 2025, after 
first online publication: The preceding sentence has been corrected.] We queried 
PubMed, Cochrane and Scopus databases using the terms: "acne", "artificial 
intelligence", "machine learning", "deep learning", "large language model", and 
"ChatGPT". 105 articles were included for analysis. Of the 105 research 
articles, 96.2% (N = 101) were focused on acne diagnosis only, 9.5% (N = 10) on 
acne management only, and 5.7% (N = 6) on both. Most manuscripts used 
image-based models, including deep learning (76.2%, N = 80), classical machine 
learning (9.5%, N = 10), and ensemble models (11.4%, N = 12). The ensemble 
models hold the highest mean accuracy (89.7%), followed by deep learning 
(88.5%), large language models (87.5%), and machine learning models (86.9%). 
Only 13% (N = 14) of studies reported data on patient skin color, while 4 of the 
14 studies included a full spectrum of diverse skin tones. [Correction added on 
27 December 2025, after first online publication: The preceding sentence has 
been corrected.] The application of AI algorithms in healthcare is rapidly 
emerging, providing significant support to providers. With ensemble models 
demonstrating superior performance, AI algorithm use in acne may offer a 
convenient method to consistently diagnose and manage patients remotely. 
Designing systematic guidelines that require a diverse representation of all 
skin colors may improve social justice in healthcare.

© 2025 the International Society of Dermatology.

DOI: 10.1111/ijd.70110
PMID: 41199593


769. Eur J Radiol. 2025 Mar;184:111960. doi: 10.1016/j.ejrad.2025.111960. Epub 2025 
Jan 29.

Large language models in methodological quality evaluation of radiomics research 
based on METRICS: ChatGPT vs NotebookLM vs radiologist.

Mese I(1), Kocak B(2).

Author information:
(1)Department of Radiology, Uskudar State Hospital, Istanbul 34662, Turkey; 
Department of Radiology, University of Health Sciences, Basaksehir Cam and 
Sakura City Hospital, Istanbul 34480, Turkey. Electronic address: 
ismail_mese@yahoo.com.
(2)Department of Radiology, Uskudar State Hospital, Istanbul 34662, Turkey; 
Department of Radiology, University of Health Sciences, Basaksehir Cam and 
Sakura City Hospital, Istanbul 34480, Turkey. Electronic address: 
drburakkocak@gmail.com.

OBJECTIVES: This study aimed to evaluate the effectiveness of large language 
models (LLM) in assessing the methodological quality of radiomics research, 
using METhodological RadiomICs Score (METRICS) tool.
METHODS: This study included open access radiomic research articles published in 
2024 across various journals and a preprint repository, all under the Creative 
Commons Attribution License. Each study was independently evaluated using 
METRICS by two LLMs, ChatGPT-4 and NotebookLM, and a consensus assessment 
performed by two radiologists with expertise in radiomics research.
RESULTS: A total of 48 open access articles were included in this study. 
ChatGPT-4, NotebookLM, and human readers achieved median scores of 79.5 %, 
61.6 %, and 69.0 %, respectively, with a statistically significant difference 
across these evaluations (p < 0.05). Pairwise comparisons indicated no 
statistically significant difference for NotebookLM vs human experts (p > 0.05), 
in contrast to other pairs (p < 0.05). Intraclass correlation coefficient (ICC) 
for ChatGPT-4 and human experts was 0.563 (95 % CI: 0.050---0.795), 
corresponding to poor to good agreement. The ICC for ChatGPT-4 and NotebookLM 
and for human experts and NotebookLM were 0.391 (95 % CI: -0.031---0.665) and 
0.555 (95 % CI: 0.326---0.723), respectively, indicating poor to moderate 
agreement. LLMs completed the tasks in a significantly shorter time (p < 0.05). 
In item-wise reliability analysis, ChatGPT-4 generally demonstrated higher 
consistency than NotebookLM.
CONCLUSION: LLMs hold promise for automatically evaluating the quality of 
radiomics research using METRICS, a new tool that is relatively more complex yet 
comprehensive compared to its counterparts. However, substantial improvements 
are needed for full alignment with human experts.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ejrad.2025.111960
PMID: 39938163 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest Given his 
roles on the editorial board, Burak Kocak, MD had no involvement in the 
peer-review of this article and had no access to information regarding its 
peer-review. Burak Kocak, MD took part in development of METRICS. The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


770. Acad Med. 2025 Sep 1;100(9S Suppl 1):S46-S48. doi: 10.1097/ACM.0000000000006102. 
Epub 2025 May 30.

Effective Engagement With AI Is the Only Path Forward for Clinician-Educators.

Rodman A, James CA.

There is broad agreement that the cognitive abilities of generative artificial 
intelligence (GAI) technologies are existential opportunities for medical 
education, threatening to reshape both the clinical practice environment and the 
nature of learning. Recent improvements in the abilities of these models-most 
notably so-called reasoning models, which can perform many cognitive tasks with 
equal or better performance compared to humans-outpace the abilities of both 
medical schools, professional societies, and consensus organizations to adapt. 
In this commentary, the authors propose that this fact should compel individual 
clinician-educators to become early adopters and actively engage with GAI 
technologies, with their learners, patients, and educational communities, and 
provide practical strategies for engagement.

Copyright © 2025 the Association of American Medical Colleges.

DOI: 10.1097/ACM.0000000000006102
PMID: 40456164 [Indexed for MEDLINE]


771. J Med Internet Res. 2025 Feb 13;27:e48328. doi: 10.2196/48328.

Large Language Models-Supported Thrombectomy Decision-Making in Acute Ischemic 
Stroke Based on Radiology Reports: Feasibility Qualitative Study.

Kottlors J(1), Hahnfeldt R(1), Görtz L(1), Iuga AI(1), Fervers P(1), Bremm J(1), 
Zopfs D(1), Laukamp KR(1), Onur OA(2), Lennartz S(1), Schönfeld M(1), Maintz 
D(1), Kabbasch C(1), Persigehl T(#)(1), Schlamann M(#)(1).

Author information:
(1)Institute for Diagnostic and Interventional Radiology, Faculty of Medicine 
and University Hospital Cologne, University of Cologne, Cologne, Germany.
(2)Department of Neurology, Faculty of Medicine and University Hospital Cologne, 
University of Cologne, Cologne, Germany.
(#)Contributed equally

BACKGROUND: The latest advancement of artificial intelligence (AI) is generative 
pretrained transformer large language models (LLMs). They have been trained on 
massive amounts of text, enabling humanlike and semantical responses to 
text-based inputs and requests. Foreshadowing numerous possible applications in 
various fields, the potential of such tools for medical data integration and 
clinical decision-making is not yet clear.
OBJECTIVE: In this study, we investigate the potential of LLMs in report-based 
medical decision-making on the example of acute ischemic stroke (AIS), where 
clinical and image-based information may indicate an immediate need for 
mechanical thrombectomy (MT). The purpose was to elucidate the feasibility of 
integrating radiology report data and other clinical information in the context 
of therapy decision-making using LLMs.
METHODS: A hundred patients with AIS were retrospectively included, for which 
50% (50/100) was indicated for MT, whereas the other 50% (50/100) was not. The 
LLM was provided with the computed tomography report, information on 
neurological symptoms and onset, and patients' age. The performance of the AI 
decision-making model was compared with an expert consensus regarding the binary 
determination of MT indication, for which sensitivity, specificity, and accuracy 
were calculated.
RESULTS: The AI model had an overall accuracy of 88%, with a specificity of 96% 
and a sensitivity of 80%. The area under the curve for the report-based MT 
decision was 0.92.
CONCLUSIONS: The LLM achieved promising accuracy in determining the eligibility 
of patients with AIS for MT based on radiology reports and clinical information. 
Our results underscore the potential of LLMs for radiological and medical data 
integration. This investigation should serve as a stimulus for further clinical 
applications of LLMs, in which this AI should be used as an augmented supporting 
system for human decision-making.

©Jonathan Kottlors, Robert Hahnfeldt, Lukas Görtz, Andra-Iza Iuga, Philipp 
Fervers, Johannes Bremm, David Zopfs, Kai R Laukamp, Oezguer A Onur, Simon 
Lennartz, Michael Schönfeld, David Maintz, Christoph Kabbasch, Thorsten 
Persigehl, Marc Schlamann. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org), 13.02.2025.

DOI: 10.2196/48328
PMCID: PMC11888093
PMID: 39946168 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: DZ is on the speaker’s 
bureau of Philips and a lecturer for Amboss GmbH. SL received authorship and 
speaker fees from Amboss.


772. J Am Coll Surg. 2025 Nov 1;241(5):766-772. doi: 10.1097/XCS.0000000000001478. 
Epub 2025 Oct 15.

Data Extraction and Curation from Radiology Reports for Pancreatic Cyst 
Surveillance Using Large Language Models.

Choubey AP(1), Eguia E(1), Hollingsworth A(1)(2), Chatterjee S(2), D'Angelica 
MI(1), Jarnagin WR(1), Wei AC(1), Schattner MA(3), Do RK(4), Soares KC(1); MSKCC 
Pancreas Cyst Collaborative.

Collaborators: Alessandris R(5), Armstrong MT(5), Manin E(5), Saadat LV(5), 
Flood J(5), Chatterjee A(6), Balachandran VP(5), Drebin JA(5), Rolston VS(7), 
Kingham TP(5).

Author information:
(1)From the Department of Surgery, Memorial Sloan Kettering Cancer Center, New 
York, NY (Choubey, Eguia, Hollingsworth, D'Angelica, Jarnagin, Wei, Soares).
(2)Artificial Intelligence and Machine Learning, Digital, Informatics and 
Technology Solutions, Memorial Sloan Kettering Cancer Center, New York, NY 
(Hollingsworth, Chatterjee).
(3)Departments of Gastroenterology, Hepatology, and Nutrition (Schattner), 
Memorial Sloan Kettering Cancer Center, New York, NY.
(4)Radiology (Do), Memorial Sloan Kettering Cancer Center, New York, NY.
(5)From the Department of Surgery, Memorial Sloan Kettering Cancer Center, New 
York, NY.
(6)Artificial Intelligence and Machine Learning, Digital, Informatics and 
Technology Solutions, Memorial Sloan Kettering Cancer Center, New York, NY.
(7)Department of Gastroenterology, Hepatology, and Nutrition, Memorial Sloan 
Kettering Cancer Center, New York, NY.

Comment in
    AJR Am J Roentgenol. 2025 Sep 24. doi: 10.2214/AJR.25.33928.

BACKGROUND: Manual curation of radiographic features in pancreatic cyst 
registries for data abstraction and longitudinal evaluation is time-consuming 
and limits widespread implementation. We examined the feasibility and accuracy 
of using large language models (LLMs) to extract clinical variables from 
radiology reports.
STUDY DESIGN: A single-center retrospective study included patients under 
surveillance for pancreatic cysts. Nine radiographic elements used to monitor 
cyst progression were included: cyst size, main pancreatic duct (MPD) size 
(continuous variable), number of lesions, MPD dilation 5 mmol/L or more 
(categorical), branch duct dilation, presence of solid component, calcific 
lesion, pancreatic atrophy, and pancreatitis. LLMs (generative pretrained 
transformer [GPT]) on the OpenAI GPT-4 platform were used to extract elements of 
interest with a zero-shot learning approach using prompting to facilitate 
annotation without any training data. A manually annotated institutional cyst 
database was used as the ground truth for comparison.
RESULTS: Overall, 3,198 longitudinal scans from 991 patients were included. GPT 
successfully extracted the selected radiographic elements with high accuracy. 
Among categorical variables, accuracy ranged from 97% for solid component to 99% 
for calcific lesions. In the continuous variables, accuracy varied from 92% for 
cyst size to 97% for MPD size. However, Cohen's kappa was higher for cyst size 
(0.92) compared with that for MPD size (0.82). Lowest accuracy (81%) was noted 
in the multiclass variable for a number of cysts.
CONCLUSIONS: LLM can accurately extract and curate data from radiology reports 
for pancreatic cyst surveillance and can be reliably used to assemble 
longitudinal databases. Future application of this work can potentiate the 
development of artificial intelligence-based surveillance models.

Copyright © 2025 by the American College of Surgeons. Published by Wolters 
Kluwer Health, Inc. All rights reserved.

DOI: 10.1097/XCS.0000000000001478
PMCID: PMC12252180
PMID: 40637302 [Indexed for MEDLINE]


773. J Med Internet Res. 2025 Jun 19;27:e70315. doi: 10.2196/70315.

Large Language Model Architectures in Health Care: Scoping Review of Research 
Perspectives.

Leiser F(#)(1), Guse R(#)(1), Sunyaev A(2).

Author information:
(1)Research Group Critical Information Infrastructures, Institute of Applied 
Informatics and Formal Description Methods, Karlsruhe Institute of Technology, 
Karlsruhe, Germany.
(2)Chair of Information Infrastructures, School of Computation, Information and 
Technology, Technical University of Munich, Campus Heilbronn, Heilbronn, 
Germany.
(#)Contributed equally

BACKGROUND: Large language models (LLMs) can support health care professionals 
in their daily work, for example, when writing and filing reports or 
communicating diagnoses. With the rise of LLMs, current research investigates 
how LLMs could be applied in medical practice and their benefits for physicians 
in clinical workflows. However, most studies neglect the importance of selecting 
suitable LLM architectures.
OBJECTIVE: In this literature review, we aim to provide insights on the 
different LLM model architecture families (ie, Bidirectional Encoder 
Representations from Transformers [BERT]-based or generative pretrained 
transformer [GPT]-based models) used in previous research. We report on the 
suitability and benefits of different LLM model architecture families for 
various research foci.
METHODS: To this end, we conduct a scoping review to identify which LLMs are 
used in health care. Our search included manuscripts from PubMed, arXiv, and 
medRxiv. We used open and selective coding to assess the 114 identified 
manuscripts regarding 11 dimensions related to usage and technical facets and 
the research focus of the manuscripts.
RESULTS: We identified 4 research foci that emerged previously in manuscripts, 
with LLM performance being the main focus. We found that GPT-based models are 
used for communicative purposes such as examination preparation or patient 
interaction. In contrast, BERT-based models are used for medical tasks such as 
knowledge discovery and model improvements.
CONCLUSIONS: Our study suggests that GPT-based models are better suited for 
communicative purposes such as report generation or patient interaction. 
BERT-based models seem to be better suited for innovative applications such as 
classification or knowledge discovery. This could be due to the architectural 
differences where GPT processes language unidirectionally and BERT 
bidirectionally, allowing more in-depth understanding of the text. In addition, 
BERT-based models seem to allow more straightforward extensions of their models 
for domain-specific tasks that generally lead to better results. In summary, 
health care professionals should consider the benefits and differences of the 
LLM architecture families when selecting a suitable model for their intended 
purpose.

©Florian Leiser, Richard Guse, Ali Sunyaev. Originally published in the Journal 
of Medical Internet Research (https://www.jmir.org), 19.06.2025.

DOI: 10.2196/70315
PMCID: PMC12226782
PMID: 40536801 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


774. Aesthet Surg J. 2025 Jul 15;45(8):860-864. doi: 10.1093/asj/sjaf049.

Initial Proof-of-Concept Study for a Plastic Surgery-Specific Artificial 
Intelligence Large Language Model: PlasticSurgeryGPT.

Ozmen BB, Berber I, Schwarz GS.

BACKGROUND: The advent of general-purpose large language models (LLMs) like 
ChatGPT (OpenAI, San Francisco, CA) has revolutionized natural language 
processing, but their applicability in specialized medical fields, like plastic 
surgery, remains limited because of a lack of domain-specific knowledge.
OBJECTIVES: This study aims to develop and evaluate PlasticSurgeryGPT, a 
dedicated LLM fine-tuned on plastic surgery literature, to enhance performance 
in clinical decision support, surgical education, and research within the field.
METHODS: A comprehensive dataset of 25,389 plastic surgery research abstracts 
published between January 1, 2010, and January 1, 2024, was retrieved from 
PubMed. The abstracts underwent rigorous preprocessing, including text cleaning 
and tokenization. The authors of this study fine-tuned the pretrained GPT-2 
model on this dataset using the PyTorch and HuggingFace frameworks. The 
performance of PlasticSurgeryGPT was evaluated against the default GPT-2 model 
using BLEU, METEOR, and ROUGE-1 metrics.
RESULTS: Using the fine-tuned model, named PlasticSurgeryGPT, the authors 
demonstrated substantial improvements over the generic GPT-2 model in capturing 
the semantic nuances of plastic surgery text. PlasticSurgeryGPT outperformed 
GPT-2 across BLEU, METEOR, and ROUGE-1 metrics, with scores of 0.135519, 
0.583554, and 0.216813, respectively, compared with GPT-2's scores of 0.130179, 
0.550498, and 0.215494.
CONCLUSIONS: PlasticSurgeryGPT represents the first plastic surgery-specific 
LLM, demonstrating enhanced performance in generating relevant and accurate 
content compared with a general-purpose model. This work underscores the 
potential of domain-specific LLMs in improving clinical practice, surgical 
education, and research in plastic surgery. Future studies should focus on 
incorporating full-text articles, multimodal data, and larger models to further 
enhance performance and applicability.

© The Author(s) 2025. Published by Oxford University Press on behalf of The 
Aesthetic Society. All rights reserved. For commercial re-use, please contact 
reprints@oup.com for reprints and translation rights for reprints. All other 
permissions can be obtained through our RightsLink service via the Permissions 
link on the article page on our site—for further information please contact 
journals.permissions@oup.com.

DOI: 10.1093/asj/sjaf049
PMID: 40195018 [Indexed for MEDLINE]


775. Aesthetic Plast Surg. 2025 Mar;49(5):1637-1639. doi: 10.1007/s00266-024-03988-1. 
Epub 2024 Mar 12.

Artificial Intelligence in Plastic Surgery: Analysis of Applications, 
Perspectives, and Psychological Impact.

Barone M(1), De Bernardis R(2), Persichetti P(1).

Author information:
(1)Fondazione Policlinico Universitario Campus Bio-Medico, Via Alvaro del 
Portillo, 200, 00128, Rome, Italy.
(2)Fondazione Policlinico Universitario Campus Bio-Medico, Via Alvaro del 
Portillo, 200, 00128, Rome, Italy. riccardo.debernardis@unicampus.it.

Artificial intelligence (AI) is emerging as a promising tool in the field of 
plastic surgery, offering a wide array of applications that enhance surgical 
outcomes, patient satisfaction, and overall efficiency. This paper explores the 
utilization of AI, highlighting its various advantages and potential drawbacks. 
AI-driven technologies such as computer vision, machine learning algorithms, and 
robotic assistance facilitate preoperative planning, intraoperative guidance, 
and postoperative monitoring. These advancements enable precise anatomical 
measurements, personalized treatment plans, and real-time feedback during 
surgery, leading to improved accuracy and safety. Furthermore, AI-powered image 
analysis aids in facial recognition, skin texture assessment, and simulation of 
surgical outcomes, enabling enhanced patient consultations and predictive 
modeling. However, the integration of AI in plastic surgery also presents 
challenges, including ethical concerns, data privacy, algorithm biases, and the 
need for comprehensive training among healthcare professionals. Additionally, 
the reliance on AI systems may potentially lead to over-reliance or reduced 
surgeon autonomy, necessitating careful validation and continuous refinement of 
these technologies. Despite these challenges, the synergistic collaboration 
between AI and plastic surgery holds great promise in advancing clinical 
practice, fostering innovation, and ultimately benefiting patients through 
optimized esthetic and reconstructive outcomes.Level of Evidence V This journal 
requires that authors assign a level of evidence to each article. For a full 
description of these Evidence-Based Medicine ratings, please refer to the Table 
of Contents or the online Instructions to Authors https://www.springer.com/00266 
.

© 2024. Springer Science+Business Media, LLC, part of Springer Nature and 
International Society of Aesthetic Plastic Surgery.

DOI: 10.1007/s00266-024-03988-1
PMID: 38472350 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare that they have no conflicts of interest to disclose. Ethical Approval: 
All procedures performed in studies involving human participants were in 
accordance with the ethical standards of the institutional and/or national 
research committee and with the 1964 Helsinki declaration and its later 
amendments or comparable ethical standards. Informed Consent: Patients provided 
written consent before their inclusion in this study. Additional consent was 
obtained for the use of their images.


776. Plast Reconstr Surg Glob Open. 2025 Mar 21;13(3):e6625. doi: 
10.1097/GOX.0000000000006625. eCollection 2025 Mar.

Assessment of Appearance-related Questions About Breast Reconstruction Generated 
by Chat Generative Pre-trained Transformer.

Gonzalez XT(1), Roubaud MS(2), Schaverien MV(2), Largo RD(2), Parham CS(2), 
Francis AM(2), Chen TA(3), Hoffman AS(4), Dickey RM(2), Markey MK(4)(5), Reece 
GP(2).

Author information:
(1)From the Department of Electrical and Computer Engineering, The University of 
Texas at Austin, Austin, TX.
(2)Department of Plastic Surgery, The University of Texas MD Anderson Cancer 
Center, Houston, TX.
(3)Department of Psychological, Health, and Learning Sciences, HEALTH Research 
Institute, University of Houston, Houston, TX.
(4)Department of Biomedical Engineering, The University of Texas at Austin, 
Austin, TX.
(5)Department of Imaging Physics, The University of Texas MD Anderson Cancer 
Center, Houston, TX.

BACKGROUND: Previous studies have explored the ability of artificial 
intelligence (AI) tools based on large language models, such as Chat Generative 
Pre-Trained Transformer (ChatGPT), to answer patient questions about breast 
reconstruction. In this study, we assessed the quality of questions generated by 
ChatGPT for breast reconstruction patients to ask their providers.
METHODS: ChatGPT was prompted to generate appearance-related questions 
representative of what patients might ask during breast reconstruction 
consultations. As a benchmark, a comparison group of questions from credible 
online sources was compiled. Blinded to the source, surgeons assessed the 
quality of questions in terms of their acceptability, contribution to the 
informed consent process, and contribution to the shared decision-making 
process. Surgeons were also asked to report their agreement on whether questions 
were generated by an AI tool.
RESULTS: Experienced reconstructive surgeons rated ChatGPT-generated questions 
about appearance-related outcomes of breast reconstruction as acceptable (15 of 
16), likely to positively contribute to the informed consent process (15 of 16), 
and likely to positively contribute to the shared decision-making process (16 of 
16). These ratings were comparable to those for benchmark questions. Surgeons 
did not readily recognize questions as being AI-generated. Differences in 
surgeon assessments were most pronounced regarding the perceived potential for 
the questions to contribute to the informed consent process.
CONCLUSIONS: The quality of ChatGPT-generated questions related to appearance 
concerns is comparable to that of questions sourced from reputable online 
websites. Patients may benefit from discussion with their providers about best 
practices for using AI tools in preparation for consultations.

Copyright © 2025 The Authors. Published by Wolters Kluwer Health, Inc. on behalf 
of The American Society of Plastic Surgeons.

DOI: 10.1097/GOX.0000000000006625
PMCID: PMC11927646
PMID: 40125461

Conflict of interest statement: The authors have no financial interest to 
declare in relation to the content of this article. Gonzalez received funding 
from the National Institutes of Health fellowship (T32 EB007507).


777. Am J Clin Pathol. 2025 May 17;163(5):766-774. doi: 10.1093/ajcp/aqae185.

Bridging the gap: Evaluating ChatGPT-generated, personalized, patient-centered 
prostate biopsy reports.

Proctor ES(1), Nusbaum DJ(2), Lee JM(1)(3), Benirschke RC(1)(3), Freedman A(4), 
Raster G(2), Glaser AP(3)(5), Labbate CV(3)(5), Higgins AM(3)(5), Helfand 
BT(3)(5), Glassy EF(6), Joseph L(7), Edelstein RA(8), Krupinski EA(9), Alnajar 
H(1)(3), Kearns JT(3)(5), Groth JV(1)(3).

Author information:
(1)Department of Pathology and Laboratory Medicine, NorthShore/Endeavor Health, 
Evanston, IL, United States.
(2)Section of Urology, University of Chicago Medicine, Chicago, IL, United 
States.
(3)Pritzker School of Medicine, University of Chicago, Chicago, IL, United 
States.
(4)Department of Preventive Medicine, Northwestern University Feinberg School of 
Medicine, Chicago, IL, United States.
(5)Department of Urology, Endeavor Health, Evanston, IL, United States.
(6)Affiliated Pathologists Medical Group, Inc, Rancho Dominguez, CA, United 
States.
(7)Department of Pathology, Lowell General Hospital, Lowell, MA, United States.
(8)Department of Urology, Tufts Medicine, Boston, MA, United States.
(9)Department of Radiology and Imaging Sciences, Emory University, Atlanta, GA, 
United States.

OBJECTIVE: The highly specialized language used in prostate biopsy pathology 
reports coupled with low rates of health literacy leave some patients unable to 
comprehend their medical information. Patients' use of online search engines can 
lead to misinterpretation of results and emotional distress. Artificial 
intelligence (AI) tools such as ChatGPT (OpenAI) could simplify complex texts 
and help patients. This study evaluates patient-centered prostate biopsy reports 
generated by ChatGPT.
METHODS: Thirty-five self-generated prostate biopsy reports were synthesized 
using National Comprehensive Cancer Network guidelines. Each report was entered 
into ChatGPT, version 4, with the same instructions, and the explanations were 
evaluated by 5 urologists and 5 pathologists.
RESULTS: Respondents rated the AI-generated reports as mostly accurate and 
complete. All but 1 report was rated complete and grammatically correct by the 
majority of physicians. Pathologists did not rate any reports as having severe 
potential for harm, but 1 or more urologists rated severe concern in 20% of the 
reports. For 80% of the reports, all 5 pathologists felt comfortable sharing 
them with a patient or another clinician, but all 5 urologists reached the same 
consensus for only 40% of reports. Although every report required edits, all 
physicians agreed that they could modify the ChatGPT report faster than they 
could write an original report.
CONCLUSIONS: ChatGPT can save physicians substantial time by generating 
patient-centered reports appropriate for patient and physician audiences with 
low potential to cause harm. Surveyed physicians have confidence in the overall 
utility of ChatGPT, supporting further investigation of how AI could be 
integrated into physicians' workflows.

© The Author(s) 2025. Published by Oxford University Press on behalf of American 
Society for Clinical Pathology. All rights reserved. For commercial re-use, 
please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/ajcp/aqae185
PMID: 39838829 [Indexed for MEDLINE]


778. Clin Neurol Neurosurg. 2025 Oct;257:109039. doi: 10.1016/j.clineuro.2025.109039. 
Epub 2025 Jul 2.

Systematic evaluation of AI-based text-to-image models for generating medical 
illustrations in neurosurgery: a multi-stage comparative study.

Davis P(1), Napole A(2), Reddy C(2), Palepu SK(2), Wu AQ(2), Mannam SS(2), 
Gajjar AA(2), Jabarkheel R(2), Ajmera S(2), Ahmad H(2), Scott KW(2), Sioutas 
G(2), Shekhtman O(2), Kandregula S(2), Rahmani R(2), Catapano JS(2), Burkhardt 
JK(2), Srinivasan VM(3).

Author information:
(1)Department of Neurosurgery, Perelman School of Medicine, University of 
Pennsylvania, Philadelphia, PA, USA. Electronic address: 
pierce.davis@pennmedicine.upenn.edu.
(2)Department of Neurosurgery, Perelman School of Medicine, University of 
Pennsylvania, Philadelphia, PA, USA.
(3)Department of Neurosurgery, Perelman School of Medicine, University of 
Pennsylvania, Philadelphia, PA, USA. Electronic address: visishs@upenn.edu.

OBJECTIVE: This study evaluates the effectiveness of artificial intelligence 
(AI) models in generating accurate, high-quality medical illustrations for 
vascular neurosurgery. It aims to develop a systematic framework for producing 
and assessing AI-generated medical images.
METHODS: Four AI models-DALL-E, Copilot, Gemini, and Midjourney-were tested to 
generate illustrations of neurovascular structures and procedures (e.g., 
aneurysms, endovascular techniques). The study had three stages: (1) Proof of 
concept, in which each model generated images for nine neurovascular topics, 
evaluated using a standardized rubric; (2) A focused comparison using simple vs. 
advanced prompt strategies with Gemini; and (3) Validation of the best strategy 
with neurosurgery trainees and attendings. Images were scored from 1 (worst) to 
5 (best) across eight domains: Accuracy, Location, Size/Scale, Color, 
Complexity, Educational Value, Relevance, and Aesthetic Quality.
RESULTS: Gemini consistently outperformed other models in Stage 1, particularly 
in accuracy, color, and educational value. In Stage 2, advanced prompting 
significantly improved image quality across nearly all topics (e.g., fusiform 
aneurysm score rose from 22.4 to 35.0, p = 7E-08). In Stage 3, 85 % of 
respondents indicated they would use the saccular aneurysm image in a manuscript 
without modification. However, complex anatomy like anterior cerebral arteries 
scored lower in accuracy (2.18) and educational value (2.20).
CONCLUSIONS: AI-generated illustrations, especially from Gemini, show strong 
potential in neurosurgical education and communication. While advanced prompting 
improves output quality, challenges remain in consistently rendering complex 
anatomy. This study outlines a reproducible framework for clinical integration 
of AI-generated medical images.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.clineuro.2025.109039
PMID: 40614319 [Indexed for MEDLINE]


779. Eur Arch Otorhinolaryngol. 2025 Dec 29. doi: 10.1007/s00405-025-09925-5. Online 
ahead of print.

Comparative diagnostic accuracy of ChatGPT models in salivary gland disease: a 
multimodal vignette-based evaluation.

Abou-Bakr A(1), Eissa AA(2), Alshikh B(3), Ahmed Y(4), AbuShady EF(5), Tassoker 
M(6), Hassanein FEA(7).

Author information:
(1)Oral Medicine and Periodontology, Faculty of Dentistry, Galala University, 
Suez, Egypt.
(2)Oral and Maxillofacial Surgery, Faculty of Dentistry, Galala University, 
Suez, Egypt.
(3)Oral and Maxillofacial Surgery, Faculty of Dentistry, British University in 
Egypt, El Sherouk City, Egypt.
(4)Prosthodontics Dentistry, Faculty of Dentistry, King Salman International 
University, El-tor, Egypt.
(5)Otorhinolaryngology Department, Faculty of Medicine, Benha University, Benha, 
Qalyubia, Egypt.
(6)Department of Dentomaxillofacial Radiology, Faculty of Dentistry, Necmettin 
Erbakan University, Baglarbasi sk, Meram, Konya, 42050, Türkiye. 
dishekmelek@gmail.com.
(7)Oral Medicine, Periodontology, and Oral Diagnosis, Faculty of Dentistry, King 
Salman International University, El-tor, Egypt.

BACKGROUND: This study evaluated the diagnostic accuracy and consistency of 
ChatGPT-4o in salivary gland disorders compared to experienced clinicians.
METHODS: Eighty anonymized salivary gland cases from peer-reviewed reports were 
evaluated by ChatGPT-4o using standardized multimodal prompts and by three oral 
medicine specialists who provided Top-5 differentials. The primary outcome was 
diagnostic accuracy at the most likely diagnosis (Top-1), within the top three 
(Top-3), and within the top five (Top-5) differential diagnoses, with agreement 
measured by Cohen's kappa and subgroup analyses by gland type, imaging, and case 
difficulty.
RESULTS: At Top-3 and Top-5, ChatGPT showed perfect sensitivity (100%) and Top-1 
86.67%. Experts surpassed ChatGPT at Top-5 (77.5% vs. 67.5%, p < 0.0001), but 
ChatGPT outperformed experts at Top-1 (50.0% vs. 37.5%, p = 0.0309) and Top-3 
(62.5% vs. 62.5%, p = 1.000). At Top-1, Cohen's Kappa indicated moderate 
agreement (0.55). Experts showed notable variation by modality (p = 0.0174) and 
gland (p = 0.053). Although initial subgroup analyses found no notable 
heterogeneity of ChatGPT's performance across imaging modalities, multivariate 
regression identified gland type to be an independent predictor of its Top-1 
accuracy.
CONCLUSIONS: This first study shows ChatGPT can provide expert-level 
differential diagnoses for salivary gland disorders, suggesting promise as a 
supportive tool, though further research is needed to confirm its clinical role.
CLINICAL SIGNIFICANCE: ChatGPT-4o shows promise as a reliable supportive tool 
for differential diagnosis in oral medicine. Compared to experts, it performed 
more consistently across imaging modalities, although the particular salivary 
gland involved had a significant impact on its accuracy. Further validation 
through larger studies is needed for its integration into routine clinical 
practice.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00405-025-09925-5
PMID: 41466068

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was conducted in accordance with the ethical standards 
of the Declaration of Helsinki. Ethical approval was obtained from the Research 
Ethics Committee of Banha University, Egypt (Approval No. BC.20.7/2025). All 
clinical case data were anonymized and sourced from previously published case 
reports in the scientific literature. As no identifiable patient information was 
used and no direct patient involvement occurred, informed consent to participate 
was not required. Consent for publication: Not applicable. Competing interests: 
The authors declare no competing interests.


780. J Med Internet Res. 2025 Jun 2;27:e73233. doi: 10.2196/73233.

Enhancing the Accuracy of Human Phenotype Ontology Identification: Comparative 
Evaluation of Multimodal Large Language Models.

Zhong W(1), Sun M(2), Yao S(3), Liu Y(1), Peng D(4), Liu Y(1), Yang K(1), Gao 
H(1), Yan H(1), Hao W(1), Yan Y(#)(1), Yin C(#)(1).

Author information:
(1)Department of Prenatal Diagnosis, Beijing Obstetrics and Gynecology Hospital, 
Capital Medical University, Beijing Maternal and Child Health Care Hospital, 251 
Yaojiayuan Road, Chaoyang District, Beijing, 100020, China, 86 15572779093.
(2)Department of Reproductive Medicine, Shijiazhuang People's Hospital, Hebei 
Province, Shijiazhuang, China.
(3)Department of Gynecology and Obstetrics, Yijishan Hospital of Wannan Medical 
College, Anhui province, Wuhu, China.
(4)School of Medicine, South China University of Technology, Guangdong Province, 
Guangzhou, China.
(#)Contributed equally

BACKGROUND: Identifying Human Phenotype Ontology (HPO) terms is crucial for 
diagnosing and managing rare diseases. However, clinicians, especially junior 
physicians, often face challenges due to the complexity of describing patient 
phenotypes accurately. Traditional manual search methods using HPO databases are 
time-consuming and prone to errors.
OBJECTIVE: The aim of the study is to investigate whether the use of multimodal 
large language models (MLLMs) can improve the accuracy of junior physicians in 
identifying HPO terms from patient images related to rare diseases.
METHODS: In total, 20 junior physicians from 10 specialties participated. Each 
physician evaluated 27 patient images sourced from publicly available 
literature, with phenotypes relevant to rare diseases listed in the Chinese Rare 
Disease Catalogue. The study was divided into 2 groups: the manual search group 
relied on the Chinese Human Phenotype Ontology website, while the MLLM-assisted 
group used an electronic questionnaire that included HPO terms preidentified by 
ChatGPT-4o as prompts, followed by a search using the Chinese Human Phenotype 
Ontology. The primary outcome was the accuracy of HPO identification, defined as 
the proportion of correctly identified HPO terms compared to a standard set 
determined by an expert panel. Additionally, the accuracy of outputs from 
ChatGPT-4o and 2 open-source MLLMs (Llama3.2:11b and Llama3.2:90b) was evaluated 
using the same criteria, with hallucinations for each model documented 
separately. Furthermore, participating physicians completed an additional 
electronic questionnaire regarding their rare disease background to identify 
factors affecting their ability to accurately describe patient images using 
standardized HPO terms.
RESULTS: A total of 270 descriptions were evaluated per group. The MLLM-assisted 
group achieved a significantly higher accuracy rate of 67.4% (182/270) compared 
to 20.4% (55/270) in the manual group (relative risk 3.31, 95% CI 2.58-4.25; 
P<.001). The MLLM-assisted group demonstrated consistent performance across 
departments, whereas the manual group exhibited greater variability. Among 
standalone MLLMs, ChatGPT-4o achieved an accuracy of 48% (13/27), while the 
open-source models Llama3.2:11b and Llama3.2:90b achieved 15% (4/27) and 18% 
(5/27), respectively. However, MLLMs exhibited a high hallucination rate, 
frequently generating HPO terms with incorrect IDs or entirely fabricated 
content. Specifically, ChatGPT-4o, Llama3.2:11b, and Llama3.2:90b generated 
incorrect IDs in 57.3% (67/117), 98% (62/63), and 82% (46/56) of cases, 
respectively, and fabricated terms in 34.2% (40/117), 41% (26/63), and 32% 
(18/56) of cases, respectively. Additionally, a survey on the rare disease 
knowledge of junior physicians suggests that participation in rare disease and 
genetic disease training may enhance the performance of some physicians.
CONCLUSIONS: The integration of MLLMs into clinical workflows significantly 
enhances the accuracy of HPO identification by junior physicians, offering 
promising potential to improve the diagnosis of rare diseases and standardize 
phenotype descriptions in medical research. However, the notable hallucination 
rate observed in MLLMs underscores the necessity for further refinement and 
rigorous validation before widespread adoption in clinical practice.

© Wei Zhong, Mingyue Sun, Shun Yao, YiFan Liu, Dingchuan Peng, Yan Liu, Kai 
Yang, HuiMin Gao, HuiHui Yan, WenJing Hao, YouSheng Yan, ChengHong Yin. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/73233
PMCID: PMC12148245
PMID: 40456109 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


781. Eur J Surg Oncol. 2026 Jan 14;52(3):111391. doi: 10.1016/j.ejso.2026.111391. 
Online ahead of print.

Virtual patient modeling for generative-AI-assisted treatment decision-making in 
lymphedema care: AI tends to favor more aggressive treatment.

Tsujimoto Y(1), Shiraishi M(2), Yamanaka H(3), Lee H(4), Okazaki M(4), Morimoto 
N(3).

Author information:
(1)Department of Plastic and Reconstructive Surgery, Graduate School of 
Medicine, Kyoto University, Kyoto, Japan. Electronic address: 
tyuki-tora@umin.ac.jp.
(2)Department of Plastic and Reconstructive Surgery, The University of Tokyo 
Hospital, Tokyo, Japan. Electronic address: shiraishi-kyf@umin.ac.jp.
(3)Department of Plastic and Reconstructive Surgery, Graduate School of 
Medicine, Kyoto University, Kyoto, Japan.
(4)Department of Plastic and Reconstructive Surgery, The University of Tokyo 
Hospital, Tokyo, Japan.

BACKGROUND: The rapid advancement of generative artificial intelligence (gen-AI) 
has prompted interest in whether it can recognize and respond to individual 
clinical backgrounds in treatment decision-making. To explore this, we developed 
a virtual patient model for lymphedema and conducted an observational study to 
examine what treatments AI would recommend, whether the recommendations were 
individualized, and what tendencies the AI exhibited.
METHODS: A virtual cohort of 100 patients with secondary upper extremity 
lymphedema following breast cancer surgery was constructed using generative 
pre-trained transformer-4 omni (GPT-4o). For each virtual patient, six clinical 
questions, based on the Japanese Lymphedema Guidelines 2024, were submitted to 
the AI to elicit individualized recommendations. The answers obtained were 
compared with guidelines-defined recommendation levels to assess concordance, 
deviation, and treatment tendencies, analyzed by patient factors.
RESULTS: Multivariate analysis demonstrated that GPT-4o-generated 
recommendations were tailored to individual patient characteristics. They showed 
high concordance with guideline-defined recommendations for conservative care 
but greater variability and bias toward invasive options in surgical contexts.
CONCLUSION: The preference of gen-AI for invasive treatments may reflect an 
overestimation of the benefits of performing treatments rather than withholding 
them, especially in invasive treatments. This bias shows a limitation of current 
gen-AI in complex decisions. Our reproducible simulation framework identified 
this bias and variability, clarifying both strengths and limitations of gen-AI. 
This type of AI-on-AI observational study may help improve the accuracy of AI 
and support its future role in clinical care.

Copyright © 2026 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.ejso.2026.111391
PMID: 41564847

Conflict of interest statement: Declaration of interest statemen The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


782. J Cosmet Dermatol. 2025 Apr;24(4):e70056. doi: 10.1111/jocd.70056.

Treatment of Porphyria Cutanea Tarda Scarring With Combination Laser Treatment 
and a Pilot Use of Artificial Intelligence to Quantify Laser Results.

Kesty CE(1)(2), Kesty KR(1)(2).

Author information:
(1)St. Petersburg Skin and Laser, St. Petersburg, Florida, USA.
(2)Kesty AI, St. Petersburg, Florida, USA.

BACKGROUND: Porphyria cutanea tarda (PCT) is the most common subtype of 
porphyria and results from a deficiency of the enzyme uroporphyrinogen 
decarboxylase. Even after successful treatment, patients can be left with 
significant scarring, and there is little published data on the safety and 
efficacy of light-based or laser-based therapies.
METHODS: This report examines a case of a 47-year-old male with PCT secondary to 
HCV, treated with a combination of fractionated erbium-doped 
yttrium-aluminum-garnet (Er:YAG), intense pulsed light (IPL), and carbon dioxide 
(CO2) lasers to address significant scarring and residual skin damage. An 
artificial intelligence model was used to quantify the results of the laser 
procedures.
RESULTS: After combination laser treatment, the patient exhibited marked 
improvements in skin texture, reduction in scar visibility, and diminished 
hyperpigmentation. The artificial intelligence algorithm quantified the laser 
results and showed improvements in the scores used in the large language model.
CONCLUSION: In this patient, customizing a combination of lasers to target 
different layers of the skin to achieve comprehensive improvement: erbium 
primarily addressed superficial irregularities and pigmentation, while CO2 
promoted deeper collagen remodeling. The use of artificial intelligence to 
quantify the positive results in this case is in line with the clinical 
evaluations and photos.

© 2025 The Author(s). Journal of Cosmetic Dermatology published by Wiley 
Periodicals LLC.

DOI: 10.1111/jocd.70056
PMCID: PMC11960420
PMID: 40167326 [Indexed for MEDLINE]

Conflict of interest statement: K.R.K. is the Founder of Kesty AI, which was 
used in this paper.


783. BMC Med Educ. 2025 Feb 25;25(1):300. doi: 10.1186/s12909-025-06881-w.

Quality assurance and validity of AI-generated single best answer questions.

Ahmed A(1), Kerr E(1), O'Malley A(2).

Author information:
(1)University of St Andrews, St Andrews, UK.
(2)University of St Andrews, St Andrews, UK. aso2@st-andrews.ac.uk.

BACKGROUND: Recent advancements in generative artificial intelligence (AI) have 
opened new avenues in educational methodologies, particularly in medical 
education. This study seeks to assess whether generative AI might be useful in 
addressing the depletion of assessment question banks, a challenge intensified 
during the Covid-era due to the prevalence of open-book examinations, and to 
augment the pool of formative assessment opportunities available to students. 
While many recent publications have sought to ascertain whether AI can achieve a 
passing standard in existing examinations, this study investigates the potential 
for AI to generate the exam itself. This research utilized a commercially 
available AI large language model (LLM), OpenAI GPT-4, to generate 220 single 
best answer (SBA) questions, adhering to Medical Schools Council Assessment 
Alliance guidelines the and a selection of Learning Outcomes (LOs) of the 
Scottish Graduate-Entry Medicine (ScotGEM) program. All questions were assessed 
by an expert panel for accuracy and quality. A total of 50 AI-generated and 50 
human-authored questions were used to create two 50-item formative SBA 
examinations for Year 1 and Year 2 ScotGEM students. Each exam, delivered via 
the Speedwell eSystem, comprised 25 AI-generated and 25 human-authored questions 
presented in random order. Students completed the online, closed-book exams on 
personal devices under exam conditions that reflected summative examinations. 
The performance of both AI-generated and human-authored questions was evaluated, 
focusing on facility and discrimination index as key metrics. The screening 
process revealed that 69% of AI-generated SBAs were fit for inclusion in the 
examinations with little or no modifications required. Modifications, when 
necessary, were predominantly due to reasons such as the inclusion of "all of 
the above" options, usage of American English spellings, and non-alphabetized 
answer choices. 31% of questions were rejected for inclusion in the 
examinations, due to factual inaccuracies and non-alignment with students' 
learning. When included in an examination, post hoc statistical analysis 
indicated no significant difference in performance between the AI- and human- 
authored questions in terms of facility and discrimination index.
DISCUSSION AND CONCLUSION: The outcomes of this study suggest that AI LLMs can 
generate SBA questions that are in line with best-practice guidelines and 
specific LOs. However, a robust quality assurance process is necessary to ensure 
that erroneous questions are identified and rejected. The insights gained from 
this research provide a foundation for further investigation into refining AI 
prompts, aiming for a more reliable generation of curriculum-aligned questions. 
LLMs show significant potential in supplementing traditional methods of question 
generation in medical education. This approach offers a viable solution to 
rapidly replenish and diversify assessment resources in medical curricula, 
marking a step forward in the intersection of AI and education.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-06881-w
PMCID: PMC11854382
PMID: 40001164 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Ethical approval was awarded on 19 Oct 2024 by the School of 
Medicine Ethics Committee at University of St Andrews (Reference number 
MD17293). Since this study does not involve patients a clinical trial number was 
not applicable. All students received information about the study before 
attempting the exam, and a consent form was required to be completed. Students’ 
exam responses were only used in this study if they provided consent. These 
provisions aligned our study with the Declaration of Helsinki. As there was a 
dependent relationship between the researcher (i.e. teachers/assessors on the 
ScotGEM programme) and the students, it was made clear that withholding consent 
would not disadvantage the student and that they would be able to attempt the 
exams as normal. Students’ individual responses were not anonymised to provide 
them with feedback after the conclusion of the exam; however, the identities of 
the students were not presented to the research team for the purposes of the 
post-hoc analysis. Participants were provided with their induvial exam feedback 
(privately) and the general findings of this study (during a whole-class 
briefing session). Students’ responses and were stored securely on University 
cloud storage (OneDrive) and only accessible by the research team. Since this 
study is not a clinical trial we have not reported a clinical trial number. 
Consent for publication: Not applicable. Competing interests: The authors 
declare no competing interests.


784. J Gen Intern Med. 2025 Dec 8. doi: 10.1007/s11606-025-10068-w. Online ahead of 
print.

What is Artificial Intelligence (AI) "Empathy"? A Study Comparing ChatGPT and 
Physician Responses on an Online Forum.

Ruben MA(1), Blanch-Hartigan D(2), Hall JA(3).

Author information:
(1)Department of Psychology, University of Rhode Island, Kingston, RI, USA. 
mollie.ruben@uri.edu.
(2)Center for Health and Business, Department of Natural and Applied Sciences, 
Bentley University, Waltham, MA, USA.
(3)Department of Psychology, Northeastern University, Boston, MA, USA.

BACKGROUND: Artificial intelligence (AI) chatbots may be an asset to 
patient-provider communication, but not enough is known about how patients 
respond and how chatbots answer patients' questions.
OBJECTIVE: How perceptions of empathy, quality, trust, liking, and goodness vary 
by both the actual and perceived source of responses to patient questions 
(chatbot vs. actual physician). We also coded and compared key verbal elements 
in chatbot and physician responses.
DESIGN: This cross-sectional experimental study used chatbot and physician 
responses from Ayers et al. (2023) in a 2 (actual source: chatbot vs. 
physician) × 2 (perceived source: chatbot vs. physician) factorial design.
PARTICIPANTS: U.S.-based, English-speaking participants were recruited online 
(N = 1454).
MAIN MEASURES: Participants rated responses on empathy, quality, trust, liking, 
and goodness. Verbal content of the chatbot and physician responses was 
independently coded by trained research assistants to identify elements 
contributing to higher empathy ratings by participants.
KEY RESULTS: Replicating Ayers et al. (2023), participants rated chatbot 
responses as more empathic than physician responses (Cohen's d = 0.56, 
p < 0.001). Chatbot responses received higher empathy ratings than physician 
responses regardless of what participants were told about authorship (ηp2 = 
0.60, p < 0.001). Empathy ratings were higher when participants thought the 
response was physician-authored, whether it was or not (ηp2 = 0.17, p < 0.001). 
Participant ratings of quality, trust, liking, and goodness followed the same 
pattern as empathy. Chatbot responses contained more coder-rated validation, 
reassurance, and non-judgmental language and were less rushed and more 
structured than physician responses (Cohen's d = 0.32 to 1.82, p's < 0.01).
CONCLUSIONS: AI-generated responses, with human oversight, could enhance 
computer-mediated clinical communication, although patient awareness of AI 
contributions may reduce perceptions of empathy. Identification of the specific 
verbal elements in AI-generated responses could augment communication and 
increase perceptions of empathic care.

© 2025. The Author(s).

DOI: 10.1007/s11606-025-10068-w
PMID: 41359230

Conflict of interest statement: Declarations. Ethical Approval: Ethical approval 
was received by Bentley University’s Institutional Review Board. Consent to 
Participate: All participants provided informed consent to participate in this 
study. Consent to Publish: No identifiable data was collected. All participants 
provided informed consent to participate in this study and to have their data 
published in the aggregate. Competing interests: The authors declare no 
competing interests.


785. JMIR Form Res. 2025 Jun 27;9:e69075. doi: 10.2196/69075.

Analyzing Patient Complaints in Web-Based Reviews of Private Hospitals in 
Selangor, Malaysia, Using Large Language Model-Assisted Content Analysis: Mixed 
Methods Study.

Sulaiman MH(#)(1)(2), Muda N(#)(1), Abdul Razak F(#)(1).

Author information:
(1)Department of Mathematical Sciences, Faculty of Science and Technology, 
National University of Malaysia, Bangi, Malaysia.
(2)Quality Unit, Clinical Management Section, Hospital Sultan Idris Shah, 
Serdang, Malaysia.
(#)Contributed equally

BACKGROUND: Large language model (LLM)-assisted content analysis (LACA) is a 
modification of traditional content analysis, leveraging the LLM to codevelop 
codebooks and automatically assign thematic codes to a web-based reviews 
dataset.
OBJECTIVE: This study aims to develop and validate the use of LACA for analyzing 
hospital web-based reviews and to identify themes of issues from web-based 
reviews using this method.
METHODS: Web-based reviews for 53 private hospitals in Selangor, Malaysia, were 
acquired. Fake reviews were filtered out using natural language processing and 
machine learning algorithms trained on yelp.com validated datasets. GPT-4o mini 
model application programming interface (API) was then applied to filter out 
reviews without any quality issues. In total, 200 of the remaining reviews were 
randomly extracted and fed into the GPT-4o mini model API to produce a codebook 
validated through parallel human-LLM coding to establish interrater reliability. 
The codebook was then used to code (label) all reviews in the dataset. The 
thematic codes were then summarized into themes using factor analysis to 
increase interpretability.
RESULTS: A total of 14,938 web-based reviews were acquired, of which 1121 (9.3%) 
were fake, 1279 (12%) contained negative sentiments, and 9635 (88%) did not 
contain any negative sentiment. GPT-4o mini model subsequently inducted 41 
thematic codes together with their definitions. Average human-GPT interrater 
reliability is perfect (κ=0.81). Factor analysis identified 6 interpretable 
latent factors: "Service and Communication Effectiveness," "Clinical Care and 
Patient Experience," "Facilities and Amenities Quality," "Appointment and 
Patient Flow," "Financial and Insurance Management," and "Patient Rights and 
Accessibility." The cumulative explained variance for the six factors is 0.74, 
and Cronbach α is between 0.88 and 0.97 (good and excellent) for all factors 
except factor 6 (0.61: questionable). The factors identified follow a global 
pattern of issues identified from the literature.
CONCLUSIONS: A data collection and processing pipeline consisting of Python 
Selenium, the GPT-4o mini model API, and a factor analysis module can support 
valid and reliable thematic analysis. Despite the potential for collection and 
information bias in web-based reviews, LACA of web-based reviews is 
cost-effective, time-efficient, and can be performed in real time, helping 
hospital managers develop hypotheses for further investigations promptly.

©Muhammad Hafiz Sulaiman, Nora Muda, Fatimah Abdul Razak. Originally published 
in JMIR Formative Research (https://formative.jmir.org), 27.06.2025.

DOI: 10.2196/69075
PMCID: PMC12254706
PMID: 40577714 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


786. Arthroscopy. 2025 Mar;41(3):588-597. doi: 10.1016/j.arthro.2024.05.025. Epub 
2024 Jun 25.

ChatGPT-4 Performs Clinical Information Retrieval Tasks Using Consistently More 
Trustworthy Resources Than Does Google Search for Queries Concerning the 
Latarjet Procedure.

Oeding JF(1), Lu AZ(2), Mazzucco M(2), Fu MC(3), Taylor SA(3), Dines DM(3), 
Warren RF(3), Gulotta LV(3), Dines JS(3), Kunze KN(4).

Author information:
(1)School of Medicine, Mayo Clinic Alix School of Medicine, Rochester, 
Minnesota, U.S.A.
(2)Weill Cornell College of Medicine, New York, New York, U.S.A.
(3)Sports Medicine and Shoulder Service, Hospital for Special Surgery, New York, 
New York, U.S.A.; Department of Orthopaedic Surgery, Hospital for Special 
Surgery, New York, New York, U.S.A.
(4)Sports Medicine and Shoulder Service, Hospital for Special Surgery, New York, 
New York, U.S.A.; Department of Orthopaedic Surgery, Hospital for Special 
Surgery, New York, New York, U.S.A.. Electronic address: kylekunze7@gmail.com.

PURPOSE: To assess the ability of ChatGPT-4, an automated Chatbot powered by 
artificial intelligence, to answer common patient questions concerning the 
Latarjet procedure for patients with anterior shoulder instability and compare 
this performance with Google Search Engine.
METHODS: Using previously validated methods, a Google search was first performed 
using the query "Latarjet." Subsequently, the top 10 frequently asked questions 
(FAQs) and associated sources were extracted. ChatGPT-4 was then prompted to 
provide the top 10 FAQs and answers concerning the procedure. This process was 
repeated to identify additional FAQs requiring discrete-numeric answers to allow 
for a comparison between ChatGPT-4 and Google. Discrete, numeric answers were 
subsequently assessed for accuracy on the basis of the clinical judgment of 2 
fellowship-trained sports medicine surgeons who were blinded to search platform.
RESULTS: Mean (± standard deviation) accuracy to numeric-based answers was 2.9 ± 
0.9 for ChatGPT-4 versus 2.5 ± 1.4 for Google (P = .65). ChatGPT-4 derived 
information for answers only from academic sources, which was significantly 
different from Google Search Engine (P = .003), which used only 30% academic 
sources and websites from individual surgeons (50%) and larger medical practices 
(20%). For general FAQs, 40% of FAQs were found to be identical when comparing 
ChatGPT-4 and Google Search Engine. In terms of sources used to answer these 
questions, ChatGPT-4 again used 100% academic resources, whereas Google Search 
Engine used 60% academic resources, 20% surgeon personal websites, and 20% 
medical practices (P = .087).
CONCLUSIONS: ChatGPT-4 demonstrated the ability to provide accurate and reliable 
information about the Latarjet procedure in response to patient queries, using 
multiple academic sources in all cases. This was in contrast to Google Search 
Engine, which more frequently used single-surgeon and large medical practice 
websites. Despite differences in the resources accessed to perform information 
retrieval tasks, the clinical relevance and accuracy of information provided did 
not significantly differ between ChatGPT-4 and Google Search Engine.
CLINICAL RELEVANCE: Commercially available large language models (LLMs), such as 
ChatGPT-4, can perform diverse information retrieval tasks on-demand. An 
important medical information retrieval application for LLMs consists of the 
ability to provide comprehensive, relevant, and accurate information for various 
use cases such as investigation about a recently diagnosed medical condition or 
procedure. Understanding the performance and abilities of LLMs for use cases has 
important implications for deployment within health care settings.

Copyright © 2024 Arthroscopy Association of North America. Published by Elsevier 
Inc. All rights reserved.

DOI: 10.1016/j.arthro.2024.05.025
PMID: 38936557 [Indexed for MEDLINE]

Conflict of interest statement: Disclosures The authors declare the following 
financial interests/personal relationships which may be considered as potential 
competing interests: D.M.D. reports board or committee member for AAOS and the 
American Shoulder and Elbow Surgeons; other financial or material support from 
Biomet; editorial or governing board for Springer: publishing royalties, 
financial, or material support from Thieme; and paid consultant for Zimmer. 
J.S.D. reports board or committee member for the American Shoulder and Elbow 
Surgeons; IP royalties, paid consultant, paid presenter or speaker, and research 
support, Arthrex; editorial or governing board, Journal of Shoulder and Elbow 
Surgery; IP royalties from Linvatec; publishing royalties, financial, or 
material support from Thieme; stock or stock options from ViewFi; and publishing 
royalties, financial, or material support from Wolters Kluwer Health - 
Lippincott Williams & Wilkins. M.C.F. reports board or committee member for the 
American Shoulder and Elbow Surgeons; editorial or governing board for 
Arthroscopy and HSS Journal; and paid consultant for Stryker. L.V.G. reports 
paid consultant, paid presenter or speaker, research support from Biomet; IP 
royalties from and paid presenter or speaker for Exactech; editorial or 
governing board for HSS Journal; stock or stock options from Imagen and 
Responsive Arthroscopy; and paid presenter or speaker for Smith & Nephew. R.F.W. 
reports board or committee member for the American Orthopaedic Partners and IP 
royalties from Orthobio Therapeutics. K.N.K. reports editorial board for 
Arthroscopy and HSS Journal. All other authors (S.A.T., A.L., M.M., J.O.) 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


787. IEEE Trans Med Imaging. 2025 Oct;44(10):4116-4127. doi: 
10.1109/TMI.2025.3548872.

ToPoFM: Topology-Guided Pathology Foundation Model for High-Resolution Pathology 
Image Synthesis With Cellular-Level Control.

Li J, Zhu C, Zheng S, Chen P, Sun Y, Li H, Yang L.

Synthetic data generation emerges as a strategy to mitigate data scarcity in 
digital pathology, where complicated tissue and cellular features are correlated 
with cancer diagnosis. The synthesis of such visuals, however, suffers from 
limited inter class diversity and scarcity of cellular annotations. Current 
methodologies struggle with capturing the broad spectrum of pathology features, 
causing unpredictable objects and defected fidelity. Moreover, discrepancies in 
image resolution across developmental and operational phases can amplify the 
distribution shifts, undermining the precision of diagnosis. To address these 
challenges, we introduce TOpology guided PathOlogy Foundation Model (ToPoFM), a 
visual foundation model designed for the synthesis of high-resolution pathology 
images with cellular-level control. Our approach integrates a topology-informed 
cell arrangement generator to steer large language models for crafting synthetic 
cell arrangements. We correlate cell arrangement guidance with diffusion model 
for pathology content generation, then further implement a random sliding 
inference strategy, merging discrete low-resolution samplings into single 
high-resolution representation. Our model requires only small patches for 
training. The efficacy of ToPoFM is demonstrated through extensive experiments, 
complemented by expert validations, showing high fidelity on data synthesis. 
Additionally, we underscore the utility of our generated imagery as an 
augmentation tool, enhancing the performance of downstream tasks, including 
cancer subtype classification and segmentation.

DOI: 10.1109/TMI.2025.3548872
PMID: 40048327 [Indexed for MEDLINE]


788. J Emerg Med. 2025 Nov;78:154-162. doi: 10.1016/j.jemermed.2025.08.014. Epub 2025 
Aug 15.

Evaluating the Accuracy of Artificial Intelligence Chatbots in Triaging 
Emergency Cases: A Comparative Study with Expert Clinicians.

Aalam AA(1).

Author information:
(1)Department of Emergency Medicine, King Abdulaziz University, Jeddah, Saudi 
Arabia. Electronic address: dr.aalam@hotmail.com.

BACKGROUND: Triage is an essential process in every emergency department to 
prioritize patients' presentation based on the level of urgency. Effective and 
accurate triage is associated with better patient outcomes and resource 
allocation. Recently, artificial intelligence (AI) systems, including AI 
chatbots, have shown potential in automating complex clinical tasks such as 
triaging.
OBJECTIVES: This study aims to assess the accuracy of an AI Chatbot (ChatGPT-4o) 
triaging emergency cases and compare its performances with expert emergency 
physicians.
METHODS: This is a cross-sectional observational study in which 60 emergency 
case scenarios were analyzed. First, they were triaged by four expert emergency 
physicians with more than 10 years of experience; cases with three out of four 
physician agreements were included. Second, these cases were triaged by 
ChatGPT-4o and compared with those of expert clinicians. Cohen's kappa was used 
to measure the level.
RESULTS: Clinicians' consensus resulted in the inclusion of 46 cases. The 
overall kappa value for the ChatGPT-4o was 0.695 (95% confidence interval 
0.53036-0.85964), reflecting moderate to substantial agreement with expert 
clinicians. ChatGPT-4o sensitivity and specificity at Triage Level 1 were 100% 
and 97.67%, respectively, and sensitivity of 100% and specificity of 93.02% was 
observed at Triage Level 5, whereas it was found to be least sensitive (50%) at 
Triage Level 4.
CONCLUSION: ChatGPT-4o triaging showed strong agreement with expert physicians 
and high sensitivity for critical patients. Although not yet ready to replace 
clinical professionals, these AI tools could serve as effective decision-support 
resources, allowing health care teams to concentrate on the most urgent cases.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jemermed.2025.08.014
PMID: 40972466 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The author 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


789. Int J Gynaecol Obstet. 2025 Feb;168(2):502-507. doi: 10.1002/ijgo.15861. Epub 
2024 Aug 16.

Analyzing the performance of ChatGPT in answering inquiries about cervical 
cancer.

Yurtcu E(1), Ozvural S(2), Keyif B(1).

Author information:
(1)Department of Obstetrics and Gynecology, Faculty of Medicine, 
Duzce University, Duzce, Türkiye.
(2)Department of Obstetrics and Gynecology, Acıbadem Hospital, 
Biruni University, Istanbul, Türkiye.

OBJECTIVE: To analyze the knowledge of ChatGPT about cervical cancer (CC).
METHODS: Official websites of professional health institutes, and websites 
created by patients and charities underwent strict screening. Using CC-related 
keywords, common inquiries by the public and comments about CC were searched in 
social media applications with these data, a list of frequently asked questions 
(FAQs) was prepared. When preparing question about CC, the European Society of 
Gynecological Oncology (ESGO), European Society for Radiotherapy and Oncology 
(ESTRO), and European Society of Pathology (ESP) guidelines were used. The 
answers given by ChatGPT were scored according to the Global Quality Score 
(GQS).
RESULTS: When all ChatGPT answers to FAQs about CC were evaluated with regard to 
GQS, 68 ChatGPT answers were classified as score 5, and none of ChatGPT answers 
for FAQs were scored as 2 or 1. Moreover, ChatGPT answered 33 of 53 (62.3%) 
CC-related questions based on ESGO, ESTRO, and ESP guidelines with completely 
accurate and satisfactory responses (GQS 5). In addition, eight answers (15.1%), 
seven answers (13.2%), four answers (7.5%), and one answer (1.9%) were 
categorized as GQS 4, GQS 3, GQS 2, and GQS 1, respectively. The reproducibility 
rate of ChatGPT answers about CC-related FAQs and responses about those 
guideline-based questions was 93.2% and 88.7%, respectively.
CONCLUSION: ChatGPT had an accurate and satisfactory response rate for FAQs 
about CC with regards to GQS. However, the accuracy and quality of ChatGPT 
answers significantly decreased for questions based on guidelines.

© 2024 The Author(s). International Journal of Gynecology & Obstetrics published 
by John Wiley & Sons Ltd on behalf of International Federation of Gynecology and 
Obstetrics.

DOI: 10.1002/ijgo.15861
PMCID: PMC11726164
PMID: 39148482 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


790. Arch Pathol Lab Med. 2025 Feb 1;149(2):112-122. doi: 10.5858/arpa.2024-0221-RA.

Introduction to Generative Artificial Intelligence: Contextualizing the Future.

Singh R(1), Kim JY(2), Glassy EF(3), Dash RC(4), Brodsky V(5), Seheult J(6), de 
Baca ME(7), Gu Q(8), Hoekstra S(9), Pritt BS(6).

Author information:
(1)From the Department of Pathology, Summit Health, Woodland Park, New Jersey 
(Singh).
(2)the Department of Pathology, Kaiser Permanente, Los Angeles, California 
(Kim).
(3)Affiliated Pathologists Medical Group, Rancho Dominguez, California (Glassy).
(4)Department of Pathology, Duke Health, Durham, North Carolina (Dash).
(5)the Department of Pathology and Immunology, Washington University, St Louis, 
Missouri (Brodsky).
(6)the Department of Laboratory Medicine and Pathology, Mayo Clinic, Rochester, 
Minnesota (Seheult, Pritt).
(7)Sysmex America, Lincolnshire, Illinois (de Baca).
(8)the Department of Neurology, Neurosurgery, and Critical Care, Mayo Clinic, 
Jacksonville, Florida (Gu).
(9)Information Services, College of American Pathologists, Northfield, Illinois 
(Hoekstra).

CONTEXT.—: Generative artificial intelligence (GAI) is a promising new 
technology with the potential to transform communication and workflows in health 
care and pathology. Although new technologies offer advantages, they also come 
with risks that users, particularly early adopters, must recognize. Given the 
fast pace of GAI developments, pathologists may find it challenging to stay 
current with the terminology, technical underpinnings, and latest advancements. 
Building this knowledge base will enable pathologists to grasp the potential 
risks and impacts that GAI may have on the future practice of pathology.
OBJECTIVE.—: To present key elements of GAI development, evaluation, and 
implementation in a way that is accessible to pathologists and relevant to 
laboratory applications.
DATA SOURCES.—: Information was gathered from recent studies and reviews from 
PubMed and arXiv.
CONCLUSIONS.—: GAI offers many potential benefits for practicing pathologists. 
However, the use of GAI in clinical practice requires rigorous oversight and 
continuous refinement to fully realize its potential and mitigate inherent 
risks. The performance of GAI is highly dependent on the quality and diversity 
of the training and fine-tuning data, which can also propagate biases if not 
carefully managed. Ethical concerns, particularly regarding patient privacy and 
autonomy, must be addressed to ensure responsible use. By harnessing these 
emergent technologies, pathologists will be well placed to continue forward as 
leaders in diagnostic medicine.

© 2025 College of American Pathologists.

DOI: 10.5858/arpa.2024-0221-RA
PMID: 39631430 [Indexed for MEDLINE]

Conflict of interest statement: Hoekstra is an employee of the College of 
American Pathologists. The authors have no relevant financial interest in the 
products or companies described in this article.


791. Europace. 2025 Aug 4;27(8):euaf173. doi: 10.1093/europace/euaf173.

Reliability of large language models for reviewing research with artificial 
intelligence in cardiac electrophysiology using the European Heart Rhythm 
Association artificial intelligence checklist.

Sluha A(1), Hillmann HAK(1), Svennberg E(2), Duncker D(1).

Author information:
(1)Hannover Heart Rhythm Center, Department of Cardiology and Angiology, 
Hannover Medical School, Carl-Neuberg-Str. 1, 30625 Hannover, Germany.
(2)Department of Medicine (Med H), Karolinska University Hospital, Karolinska 
Institute, Stockholm, Sweden.

DOI: 10.1093/europace/euaf173
PMCID: PMC12395547
PMID: 40795818

Conflict of interest statement: Conflict of interest: A.S. declares no conflicts 
of interest. H.A.K.H. received modest lecture honorary, advisory fee, and/or a 
fellowship grant from AstraZeneca, Abbott, Boston Scientific, Cardiomatics, 
Medtronic, and Zoll. E.S. has received institutional lecture fees from Abbott, 
AstraZeneca, Bristol Myers Squibb, Pfizer, and Johnson & Johnson. D.D. received 
modest lecture honorary, travel grants, and/or a fellowship grant from Abbott, 
AstraZeneca, Bayer, Biotronik, Boehringer Ingelheim, Boston Scientific, Bristol 
Myers Squibb, CVRx, Daiichi Sankyo, Medtronic, Microport, Pfizer, Sanofi, and 
Zoll.


792. BMC Nephrol. 2025 Sep 29;26(1):544. doi: 10.1186/s12882-025-04469-6.

The problem of segmenting global glomerulosclerosis in gigapixel 
histopathological images: the borderless glomeruli.

Souza L(1)(2), Silva J(1)(3), Mendonça M(1)(4), Nathan J(5), Duarte A(6), Sarder 
P(7), Dos-Santos WLC(8), Oliveira L(9)(10).

Author information:
(1)IVISION Lab, Universidade Federal da Bahia (UFBA), Salvador, Bahia, Brazil.
(2)Instituto Federal do Maranhão (IFMA), Grajaú, Maranhão, Brazil.
(3)Universidade Federal do Maranhão (UFMA), Balsas, Maranhão, Brazil.
(4)Instituto Federal da Bahia (IFBA), Vitória da Conquista, Bahia, Brazil.
(5)Fundação Oswaldo Cruz (FIOCRUZ), Instituto Gonçalo Moniz, Salvador, Bahia, 
Brazil.
(6)Universidade Estadual de Feira de Santana (UEFS), Feira de Santana, Bahia, 
Brazil.
(7)University of Florida (UF), Miami, FL, USA.
(8)Fundação Oswaldo Cruz (FIOCRUZ), Instituto Gonçalo Moniz, Salvador, Bahia, 
Brazil. washington.santos@fiocruz.br.
(9)IVISION Lab, Universidade Federal da Bahia (UFBA), Salvador, Bahia, Brazil. 
lrebouca@ufba.br.
(10)Fundação Oswaldo Cruz (FIOCRUZ), Instituto Gonçalo Moniz, Salvador, Bahia, 
Brazil. lrebouca@ufba.br.

BACKGROUND: Accurately segmenting glomeruli in kidney whole slide images (WSIs) 
is essential for advancing automation in renal pathology but remains challenging 
in cases of global glomerulosclerosis, where Bowman’s capsule boundaries are 
often unclear. Conventional machine learning (ML) models perform well on normal 
glomeruli but struggle with sclerotic cases due to the lack of distinct 
structural cues. This study investigates the use of the foundation model 
segmentation generative pre-trained transformer (SegGPT) to address this 
limitation.
METHODS: We conducted experiments at both the patch and WSI levels on a private 
dataset to evaluate the performance of SegGPT foundation model against three 
non-foundation architectures, U-Net, U-Net3+, and SwinTransformer + U-Net, 
trained with and without fine-tuning.
RESULTS: The study revealed high segmentation performance for normal glomeruli, 
with non-foundation models achieving mean Dice similarity coefficient (mDice) 
scores of up to 0.94. For segmental sclerosis, performance was moderate, with 
scores reaching up to 0.73. In contrast, the segmentation of globally sclerotic 
glomeruli proved substantially more challenging: Models trained only on normal 
samples yielded mDice scores below 0.03, and even with fine-tuning on mixed 
datasets, WSI-level performance remained limited (mDice < 0.16). With only few 
annotated examples, SegGPT demonstrated markedly superior performance in this 
scenario, achieving up to 0.43 at the WSI level and 0.74 at the patch level. 
However, its performance under idealized conditions also reveals limitations in 
clinical generalization.
CONCLUSION: While conventional models perform well on normal and segmentally 
sclerotic glomeruli, their performance declines sharply in globally sclerotic 
cases, even with fine-tuning. SegGPT showed better generalization in these 
challenging scenarios, particularly at the patch level. However, its limited 
performance at the WSI level underscores the difficulty of translating 
patch-level accuracy to full-slide inference, where contextual ambiguity is 
greater. These results expose a persistent gap between controlled experimental 
setups and real-world conditions, reinforcing the need for more realistic 
evaluation protocols to advance clinical applicability.

DOI: 10.1186/s12882-025-04469-6
PMCID: PMC12482064
PMID: 41023653

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This work was conducted following resolution No. 466/12 of the 
Brazilian National Health Council and in accordance with the Declaration of 
Helsinki. The images (including those shown in this paper) were separated from 
other patients’ data to preserve confidentiality. No data presented herein 
allows patient identification. Informed consent was waived by the Institutional 
Review Board for Research Studies with Human Subjects. All the procedures were 
approved by the Ethics Committee for Research Involving Human Subjects of the 
Gonçalo Moniz Institute from the Oswaldo Cruz Foundation (CPqGM/FIOCRUZ), 
Protocols No. 188/09 and No. 1817574. Consent for publication: All authors agree 
to publish the article. Competing interests: We hereby inform that there is a 
potential conflict of interest related to the work of the authors of the 
scientific article titled “A Deep Learning-Based Approach for Glomeruli Instance 
Segmentation from Multistained Renal Biopsy Pathologic Images” (Lei Jiang, 
Wenkai Chen, Bao Dong, Ke Mei, Chuang Zhu, Jun Liu, Meishun Cai, Yu Yan, Gongwei 
Wang, Li Zuo, and Hongxia Shi, The American Journal of Pathology, 
2021191(8):1431–1441. This conflict arises from our conducting a comparative 
analysis and raising questions regarding the methodological aspects of the 
aforementioned study, which could influence the impartial evaluation of our work 
in case of being reviewed by one of those authors. We emphasize, however, that 
all analyses and critiques were conducted with the utmost objectivity and 
transparency, based on technical and scientific evidence.


793. J Med Internet Res. 2026 Jan 20;28:e70754. doi: 10.2196/70754.

Quantifying Innovation in Stroke: Large Language Model Bibliometric Analysis.

Marcus A(1)(2), Lockwood-Taylor G(3), Rueckert D(1)(4), Bentley P(2).

Author information:
(1)Department of Computing, Imperial College London, London, United Kingdom.
(2)Department of Medicine, Imperial College London, London, United Kingdom.
(3)Department of Psychology and Neuroscience, King's College London, London, 
United Kingdom.
(4)Klinikum rechts der Isar, Technical University of Munich, Munich, Germany.

BACKGROUND: Thrombolysis and mechanical thrombectomy represent the most 
successful stroke innovations over the last 30 years. Quantifying innovation in 
stroke is essential for identifying productive research lines and prioritizing 
funding, but health care lacks validated methods for measuring innovation.
OBJECTIVE: This study aimed to systematically evaluate the relationship between 
stroke-related patents and publications, demonstrate the feasibility of using 
large language models (LLMs) in this process, and identify the most rapidly 
advancing innovations in stroke care by mapping them to a theoretical innovation 
life cycle.
METHODS: The Open Patent Services (European Patent Office) and PubMed databases 
were searched between 1993 and 2023 for "stroke OR cerebrovascular." In this 
bibliometric patent-publication analysis, a 13 billion-parameter Llama LLM was 
trained to identify patents related to stroke disease, as opposed to other 
references to the word "stroke," on a manually labeled subset of 5000 patents 
and assessed using 5-fold cross-validation. The LLM filtered irrelevant results, 
and the resulting patent codes were grouped into innovation clusters. For each 
cluster, annual patent and publication counts were normalized to adjust for 
global trends. Cluster-specific growth curves were plotted to analyze the rates 
and characteristics of growth. The innovation life cycle stage for each 
innovation cluster was estimated by fitting a sigmoid curve to the patent and 
publication data consistent with the diffusion of innovations theory by Rogers.
RESULTS: The cross-validated accuracy of the LLM was 99.2%, with a sensitivity 
of 96.5% and a specificity of 99.6%. An initial bibliometric search retrieved 
237,035 patents and 486,664 research publications. A manual review of a random 
sample of patents before filtering revealed that only 11.2% (56/500) were 
relevant to stroke. After LLM filtering, of the 237,035 patents, 28,225 (11.9%) 
stroke-related patents remained. These were grouped into 7 innovation clusters: 
pharmacological treatment, alternative medicine, rehabilitation devices, medical 
imaging, diagnostic testing, surgical devices, and artificial intelligence (AI) 
methods. Patent and publication counts were strongly correlated across clusters 
(Spearman rs=0.65-0.92; P<.006) except for pharmacological treatment (rs=0.09) 
and alternative medicine (rs=0.55). Pharmacological treatments were the 
top-performing cluster over the last 30 years, accounting for 49.3% 
(36,005/73,094) of all patents, but patent activity in this area has plateaued 
since the late 2000s. AI methods, rehabilitation devices, and medical imaging 
exhibited exponential rates of patent growth, with annual normalized increases 
of 39.2%, 15.9%, and 5.8% compared to 16.9%, 5.3%, and 2.2% for publications, 
respectively.
CONCLUSIONS: Applying an LLM to publicly available patent and publication data 
provides a scalable way to quantify innovation in stroke. Pharmacological 
treatment appears to have entered a saturation phase, whereas AI methods, 
rehabilitation devices, and medical imaging remain in rapid growth, highlighting 
areas of greatest traction for future research and investment.

©Adam Marcus, Georgina Lockwood-Taylor, Daniel Rueckert, Paul Bentley. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 20.01.2026.

DOI: 10.2196/70754
PMID: 41558024 [Indexed for MEDLINE]


794. J Med Internet Res. 2025 Aug 6;27:e74231. doi: 10.2196/74231.

Classifying Patient Complaints Using Artificial Intelligence-Powered Large 
Language Models: Cross-Sectional Study.

Koh SWC(1)(2), Wong ERN(2)(3), Tan JCM(4), van der Lubbe SCC(1), Goh JC(2), 
Ching ESY(1), Chia IWY(1), Low SH(2), Ang PY(2), Quek Q(2), Motani M(4)(5), 
Valderas JM(1)(2)(3)(6).

Author information:
(1)Division of Family Medicine, Department of Medicine, Yong Loo Lin School of 
Medicine, National University of Singapore, NUHS Tower Block Level 9, 1E Kent 
Ridge Road, Singapore, 119228, Singapore, 65 67163185.
(2)National University Polyclinics, National University Health System, 
Singapore, Singapore.
(3)Department of Family Medicine, National University Health System, Singapore, 
Singapore.
(4)Department of Electrical and Computer Engineering, College of Design and 
Engineering, National University of Singapore, Singapore, Singapore.
(5)Institute of Data Science, N.1 Institute for Health, Institute for Digital 
Medicine, National University of Singapore, Singapore, Singapore.
(6)Centre for Research in Health Systems Performance and Department of Medicine, 
Yong Loo Lin School of Medicine, National University of Singapore, Singapore, 
Singapore.

BACKGROUND: Patient complaints provide valuable insights into the performance of 
health care systems, highlighting potential risks not apparent to staff. Patient 
complaints can drive systemic changes that enhance patient safety. However, 
manual categorization and analysis pose a huge logistical challenge, hindering 
the ability to harness the potential of these data.
OBJECTIVE: This study aims to evaluate the accuracy of artificial intelligence 
(AI)-powered categorization of patient complaints in primary care based on the 
Healthcare Complaint Analysis Tool (HCAT) General Practice (GP) taxonomy and 
assess the importance of advanced large language models (LLMs) in complaint 
categorization.
METHODS: This cross-sectional study analyzed 1816 anonymous patient complaints 
from 7 public primary care clinics in Singapore. Complaints were first coded by 
trained human coders using the HCAT (GP) taxonomy through a rigorous process 
involving independent assessment and consensus discussions. LLMs (GPT-3.5 turbo, 
GPT-4o mini, and Claude 3.5 Sonnet) were used to validate manual classification. 
Claude 3.5 Sonnet was further used to identify complaint themes. LLM 
classifications were assessed for accuracy and consistency with human coding 
using accuracy and F1-score. Cohen κ and McNemar test evaluated AI-human 
agreement and compared AI models' concordance, respectively.
RESULTS: The majority of complaints fell under the HCAT (GP) domain of 
management (1079/1816, 59.4%), specifically relating to institutional processes 
(830/1816, 45.7%). Most complaints were of medium severity (994/1816, 54.7%), 
occurred within the practice (627/1816, 34.5%), and resulted in minimal harm 
(75.4%). LLMs achieved moderate to good accuracy (58.4%-95.5%) in HCAT (GP) 
field classifications, with GPT-4o mini generally outperforming GPT-3.5 turbo, 
except in severity classification. All 3 LLMs demonstrated moderate concordance 
rates (average 61.9%-68.8%) in complaints classification with varying levels of 
agreement (κ=0.114-0.623). GPT-4o mini and Claude 3.5 significantly outperformed 
GPT-3.5 turbo in several fields (P<.05), such as domain and stage of care 
classification. Thematic analysis using Claude 3.5 identified long wait times 
(393/1816, 21.6%), staff attitudes (287/1816, 15.8%), and appointment booking 
issues (191/1816, 10.5%) as the top concerns, which accounted for nearly half of 
all complaints.
CONCLUSIONS: Our study highlighted the potential of LLMs in classifying patient 
complaints in primary care using HCAT (GP) taxonomy. While GPT-4o and Claude 3.5 
demonstrated promising results, further fine-tuning and model training are 
required to improve accuracy. Integrating AI into complaint analysis can 
facilitate proactive identification of systemic issues, ultimately enhancing 
quality improvement and patient safety. By leveraging LLMs, health care 
organizations can prioritize complaints and escalate high-risk issues more 
effectively. Theoretically, this could lead to improved patient care and 
experience; further research is needed to confirm this potential benefit.

© Sky Wei Chee Koh, Eunice Rui Ning Wong, John Chong Min Tan, Stephanie C C van 
der Lubbe, Jun Cong Goh, Ethan Sheng Yong Ching, Ian Wen Yih Chia, Si Hui Low, 
Ping Young Ang, Queenie Quek, Mehul Motani, Jose M Valderas. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/74231
PMCID: PMC12327907
PMID: 40768757 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


795. BMC Med Educ. 2025 Jan 31;25(1):159. doi: 10.1186/s12909-025-06753-3.

Empowering medical students with AI writing co-pilots: design and validation of 
AI self-assessment toolkit.

Khojasteh L(1), Kafipour R(2), Pakdel F(3), Mukundan J(4).

Author information:
(1)Department of English Language, School of Paramedical Sciences, Shiraz 
University of Medical Sciences, Shiraz, Iran.
(2)Department of English Language, School of Paramedical Sciences, Shiraz 
University of Medical Sciences, Shiraz, Iran. rezakafipour@gmail.com.
(3)Department of English Language, School of Paramedical Sciences, Shiraz 
University of Medical Sciences, Shiraz, Iran. ffpakdel@yahoo.com.
(4)School of Education, Taylors University, Lakeside Campus, Malaysia.

BACKGROUND AND OBJECTIVES: Assessing and improving academic writing skills is a 
crucial component of higher education. To support students in this endeavor, a 
comprehensive self-assessment toolkit was developed to provide personalized 
feedback and guide their writing improvement. The current study aimed to 
rigorously evaluate the validity and reliability of this academic writing 
self-assessment toolkit.
METHODS: The development and validation of the academic writing self-assessment 
toolkit involved several key steps. First, a thorough review of the literature 
was conducted to identify the essential criteria for authentic assessment. Next, 
an analysis of medical students' reflection papers was undertaken to gain 
insights into their experiences using AI-powered tools for writing feedback. 
Based on these initial steps, a preliminary version of the self-assessment 
toolkit was devised. An expert focus group discussion was then convened to 
refine the questions and content of the toolkit. To assess content validity, the 
toolkit was evaluated by a panel of 22 medical student participants. They were 
asked to review each item and provide feedback on the relevance and 
comprehensiveness of the toolkit for evaluating academic writing skills. Face 
validity was also examined, with the students assessing the clarity, wording, 
and appropriateness of the toolkit items.
RESULTS: The content validity evaluation revealed that 95% of the toolkit items 
were rated as highly relevant, and 88% were deemed comprehensive in assessing 
key aspects of academic writing. Minor wording changes were suggested by the 
students to enhance clarity and interpretability. The face validity assessment 
found that 92% of the items were rated as unambiguous, with 90% considered 
appropriate and relevant for self-assessment. Feedback from the students led to 
the refinement of a few items to improve their clarity in the context of the 
Persian language. The robust reliability testing demonstrated the consistency 
and stability of the academic writing self-assessment toolkit in measuring 
students' writing skills over time.
CONCLUSION: The comprehensive evaluation process has established the academic 
writing self-assessment toolkit as a robust and credible instrument for 
supporting students' writing improvement. The toolkit's strong psychometric 
properties and user-centered design make it a valuable resource for enhancing 
academic writing skills in higher education.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-06753-3
PMCID: PMC11786331
PMID: 39891148 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The study was approved by the local ethics council of Shiraz 
University of Medical Sciences (decree code: IR.SUMS.REC.1403.240). We made 
every effort to adhere to ethical principles in our research design and 
implementation: Informed Consent: We secured written consent from all 
participants, ensuring they were fully informed about the study's objectives, 
methodology, and the intended use of the data collected. Voluntary 
Participation: Participants were informed of their right to withdraw from the 
study at any time without any negative consequences, thereby ensuring their 
autonomy throughout the process. Confidentiality and Anonymity: We maintained 
the anonymity and confidentiality of all participants. All data collected was 
de-identified prior to analysis to further protect participants' identities. 
Data Security: Data was stored securely and was accessible only to the research 
team to guarantee the privacy of participants. Minimal Risk: The study involved 
minimal risk to participants, as it focused exclusively on their feedback 
concerning the self-assessment toolkit, without introducing interventions or 
collecting sensitive personal data. Beneficence: Our research aimed to enhance 
academic writing assessment and instruction, ultimately benefitting future 
students and educators through improved educational practices. Justice: We 
ensured that all eligible participants had equal opportunities to contribute to 
the study, promoting equitable participation. Consent for publication: Not 
applicable. Competing interests: The authors declare no competing interests.


796. Int J Med Inform. 2026 Jan 10;209:106276. doi: 10.1016/j.ijmedinf.2026.106276. 
Online ahead of print.

Automated extraction of fluoropyrimidine treatment and treatment-related 
toxicities from clinical notes using natural language processing.

Wu X(1), Kreider MS(2), Empey PE(2), Li C(3), Wang Y(4).

Author information:
(1)Department of Health Information Management, University of Pittsburgh, 
Pittsburgh, PA, USA.
(2)Department of Pharmacy & Therapeutics, University of Pittsburgh, Pittsburgh, 
PA, USA.
(3)Department of Health Information Management, University of Pittsburgh, 
Pittsburgh, PA, USA; Department of Biomedical Informatics, University of 
Pittsburgh, Pittsburgh, PA, USA.
(4)Department of Health Information Management, University of Pittsburgh, 
Pittsburgh, PA, USA; Department of Biomedical Informatics, University of 
Pittsburgh, Pittsburgh, PA, USA; Intelligent Systems Program, University of 
Pittsburgh, Pittsburgh, PA, USA; Clinical and Translational Science Institute, 
University of Pittsburgh, Pittsburgh, PA, USA. Electronic address: 
yanshan.wang@pitt.edu.

OBJECTIVE: Fluoropyrimidines are widely prescribed for colorectal and breast 
cancers, but are associated with toxicities such as hand-foot syndrome and 
cardiotoxicity. Since toxicity documentation is often embedded in clinical 
notes, we aimed to develop and evaluate natural language processing (NLP) 
methods to extract treatment and toxicity information.
MATERIALS AND METHODS: We constructed a gold-standard dataset of 236 clinical 
notes from 204,165 adult oncology patients. Domain experts annotated categories 
related to treatment regimens and toxicities. We developed rule-based, machine 
learning-based (Random Forest [RF], Support Vector Machine [SVM], Logistic 
Regression [LR]), deep learning-based (BERT, ClinicalBERT), and large language 
models (LLM)-based NLP approaches (zero-shot and error analysis prompting). A 
5-fold cross validation were conducted to validate each model.
RESULTS: Error analysis prompting achieved optimal precision, recall, and F1 
scores for treatment (F1 = 1.000) and toxicities extraction (F1 = 0.965), 
whereas zero-shot perform moderately (treatment F1 = 0.889, toxicities 
extraction F1 = 0.854) Rule-based reached F1 = 1.000 for treatment and 
F1 = 0.904 for toxicities extraction. LR and SVM ranked second and fourth for 
toxicities extraction (LR F1 = 0.914, SVM F1 = 0.903). Deep learning and RF 
underperformed, with performance of BERT reached F1 = 0.792 for treatment and 
F1 = 0.837 for toxicities extraction.,ClinicalBERT reached F1 = 0.797 for 
treatment and F1 = 0.884 for toxicities extraction). RF reached F1 = 0.745 for 
treatment and F1 = 0.853 for toxicities extraction.
DISCUSSION: LMM-based error analysis outperformed all others, followed by 
machine learning methods. Machine learning and deep learning methods were 
limited by small training data and showed limited generalizability, particularly 
for rare categories.
CONCLUSION: LLM-based error analysis most effectively extracted fluoropyrimidine 
treatment and toxicity information from clinical notes, and has strong potential 
to support oncology research and pharmacovigilance.

Copyright © 2026 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2026.106276
PMID: 41534241

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


797. J Med Internet Res. 2025 May 27;27:e68538. doi: 10.2196/68538.

Exploring the Application Capability of ChatGPT as an Instructor in Skills 
Education for Dental Medical Students: Randomized Controlled Trial.

Huang S(#)(1), Wen C(#)(1)(2), Bai X(1), Li S(1), Wang S(1), Wang X(1)(3), Yang 
D(1)(3).

Author information:
(1)State Key Laboratory of Oral & Maxillofacial Reconstruction and Regeneration, 
Key Laboratory of Oral Biomedicine Ministry of Education, Hubei Key Laboratory 
of Stomatology, School & Hospital of Stomatology, Wuhan University, Wuhan, 
China.
(2)Center for Orthodontics and Pediatric Dentistry at Optics Valley Branch, 
School & Hospital of Stomatology, Wuhan University, Wuhan, China.
(3)Department of Periodontology, School & Hospital of Stomatology, Wuhan 
University, Wuhan, China.
(#)Contributed equally

BACKGROUND: Clinical operative skills training is a critical component of 
preclinical education for dental students. Although technology-assisted 
instruction, such as virtual reality and simulators, is increasingly being 
integrated, direct guidance from instructors remains the cornerstone of skill 
development. ChatGPT, an advanced conversational artificial intelligence model 
developed by OpenAI, is gradually being used in medical education.
OBJECTIVE: This study aimed to compare the effects of ChatGPT-assisted skill 
learning on performance, cognitive load, self-efficacy, learning motivation, and 
spatial ability, with the aim of evaluating the potential of ChatGPT in clinical 
operative skills education.
METHODS: In this study, 187 undergraduate dental students recruited from a 
first-class university in China were randomly divided into a ChatGPT group and a 
blank control group. Among them, the control group used videos for skill 
acquisition, and the ChatGPT group used ChatGPT in addition to the videos. After 
1 week of intervention, skills were tested using desktop virtual reality, and 
cognitive load was measured by recording changes in pupil diameter with an eye 
tracker. In addition, a spatial ability test was administered to analyze the 
effect of ChatGPT on those with different spatial abilities. Finally, a 
questionnaire was also used to assess cognitive load and self-efficacy during 
the learning process.
RESULTS: A total of 192 dental undergraduates from a top-tier Chinese university 
were initially recruited for the experiment by October 25, 2024. Following 
eye-tracking calibration procedures, 5 participants were excluded, resulting in 
187 eligible students successfully completing the experimental protocol by 
November 2, 2024. Following a short-term intervention administered through 
randomized allocation, superior performance (ChatGPT group: mean 73.12, SD 
10.06; control group: mean 65.54, SD 12.48; P<.001) was observed among 
participants in the ChatGPT group, along with higher levels of self-efficacy 
(P=.04) and learning motivation (P=.02). In addition, cognitive load was lower 
in the ChatGPT group according to eye-tracking measures (ChatGPT group: mean 
0.137, SD 0.036; control group: mean 0.312, SD 0.032; P<.001). The analysis of 
the learning performance of participants with different spatial abilities in the 
2 modalities showed that compared to the learners with high spatial abilities 
(ChatGPT group: mean 76.58, SD 9.23; control group: mean 73.89, SD 11.75; 
P=.22), those with low spatial abilities (ChatGPT group: mean 70.20, SD 10.71; 
control group: mean 55.41, SD 13.31; P<.001) were more positively influenced by 
ChatGPT.
CONCLUSIONS: ChatGPT has performed outstandingly in assisting dental skill 
learning, and the study supports the integration of ChatGPT into skills teaching 
and provides new ideas for modernizing skill teaching.
TRIAL REGISTRATION: ClinicalTrials.gov 
NCT06942130；https://clinicaltrials.gov/study/NCT06942130.

©Siyu Huang, Chang Wen, Xueying Bai, Sihong Li, Shuining Wang, Xiaoxuan Wang, 
Dong Yang. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 27.05.2025.

DOI: 10.2196/68538
PMCID: PMC12152432
PMID: 40424023 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


798. J Med Internet Res. 2025 Mar 18;27:e70481. doi: 10.2196/70481.

How to Design, Create, and Evaluate an Instruction-Tuning Dataset for Large 
Language Model Training in Health Care: Tutorial From a Clinical Perspective.

Nazar W(1), Nazar G(2), Kamińska A(2), Danilowicz-Szymanowicz L(3).

Author information:
(1)Department of Allergology, Faculty of Medicine, Gdańsk Medical University, 
Gdansk, Poland.
(2)Faculty of Medicine, Gdańsk Medical University, Gdansk, Poland.
(3)Department of Cardiology and Electrotherapy, Faculty of Medicine, Gdańsk 
Medical University, Gdansk, Poland.

High-quality data are critical in health care, forming the cornerstone for 
accurate diagnoses, effective treatment plans, and reliable conclusions. 
Similarly, high-quality datasets underpin the development and performance of 
large language models (LLMs). Among these, instruction-tuning datasets (ITDs) 
used for instruction fine-tuning have been pivotal in enhancing LLM performance 
and generalization capabilities across diverse tasks. This tutorial provides a 
comprehensive guide to designing, creating, and evaluating ITDs for health care 
applications. Written from a clinical perspective, it aims to make the concepts 
accessible to a broad audience, especially medical practitioners. Key topics 
include identifying useful data sources, defining the characteristics of 
well-designed datasets, and crafting high-quality instruction-input-output 
examples. We explore practical approaches to dataset construction, examining the 
advantages and limitations of 3 primary methods: fully manual preparation by 
expert annotators, fully synthetic generation using artificial intelligence 
(AI), and an innovative hybrid approach in which experts draft the initial 
dataset and AI generates additional data. Moreover, we discuss strategies for 
metadata selection and human evaluation to ensure the quality and effectiveness 
of ITDs. By integrating these elements, this tutorial provides a structured 
framework for establishing ITDs. It bridges technical and clinical domains, 
supporting the continued interdisciplinary advancement of AI in medicine. 
Additionally, we address the limitations of current practices and propose future 
directions, emphasizing the need for a global, unified framework for ITDs. We 
also argue that artificial general intelligence (AGI), if realized, will not 
replace empirical research in medicine. AGI will depend on human-curated 
datasets to process and apply medical knowledge. At the same time, ITDs will 
likely remain the most effective method of supplying this knowledge to AGI, 
positioning them as a critical tool in AI-driven health care.

©Wojciech Nazar, Grzegorz Nazar, Aleksandra Kamińska, Ludmila 
Danilowicz-Szymanowicz. Originally published in the Journal of Medical Internet 
Research (https://www.jmir.org), 18.03.2025.

DOI: 10.2196/70481
PMCID: PMC11962319
PMID: 40100270 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


799. Virchows Arch. 2025 Nov 29. doi: 10.1007/s00428-025-04337-6. Online ahead of 
print.

Large language model-based approach for low-cost, rapid and accurate automated 
extraction of predictive biomarker testing results from Dutch pathology reports.

de Jager VD(1), van der Ree M(2), Garcia BC(1), Kuijpers C(3), van der Wekken 
AJ(4), van Kempen LC(1)(5), Schuuring E(1), Willems SM(6).

Author information:
(1)Department of Pathology and Medical Biology, University Medical Center 
Groningen, University of Groningen, Groningen, the Netherlands.
(2)Center for Information Technology, University of Groningen, Groningen, the 
Netherlands.
(3)The Dutch Nationwide Pathology Databank (Palga), Houten, the Netherlands.
(4)Department of Pulmonary Diseases and Tuberculosis, University Medical Center 
Groningen, University of Groningen, Groningen, the Netherlands.
(5)Department of Pathology, University Hospital Antwerp, University of Antwerp, 
Edegem, Belgium.
(6)Department of Pathology and Medical Biology, University Medical Center 
Groningen, University of Groningen, Groningen, the Netherlands. 
s.m.willems@umcg.nl.

National cancer registries and pathology report databases contain valuable 
real-world information that may be used for assessment of healthcare quality or 
retrospective observational studies. As many of these data are currently non- or 
semi-structured, manual information extraction is needed, which is 
labor-intensive, costly and causes significant time delay to data analysis. This 
study aimed to develop a large language model (LLM)-based method to extract 
biomarker testing data for two routinely tested predictive biomarkers - EGFR and 
KRAS - from pathology reports of patients with non-small cell lung cancer 
(NSCLC). Patient cohorts and pathology reports were derived from the Netherlands 
Cancer Registry (NCR) and the Dutch nationwide pathology databank (Palga). 
Manually captured data regarding EGFR and KRAS testing in 3887 patients 
diagnosed with metastatic NSCLC in the Netherlands in 2019 were used for 
training, testing and validation. Annotated data included biomarker testing 
status for EGFR and KRAS, the use of next-generation sequencing (NGS), and test 
results including specific mutation(s). In the test set of the 2019 cohort, the 
model yielded (micro) F1 scores ≥ 0.98 for all variables (overall biomarker 
testing status, KRAS/EGFR testing, KRAS/EGFR test results). The trained model 
was then applied to pathology reports of 4122 patients diagnosed with NSCLC 
between July 2022 and June 2023, with manual annotation for 410 randomly 
selected cases to determine model accuracy. In this test set, (micro) F1 
scores ≥ 0.95 were achieved. None of the manually annotated positive molecular 
test results were missed. Standardized notation of reported mutations was 
correct in 98.7% and 100.0% of KRAS and EGFR mutations, respectively. In the 
entire 2022-2023 cohort, model output revealed overall test rates of 88.1% and 
86.4% for KRAS and EGFR, respectively. Among the tested patients, the model 
described positive KRAS and EGFR test results in 40.5% and 11.5%, respectively. 
This study illustrates the possibility to train and use an LLM-based model to 
accurately extract biomarker testing results from pathology reports of patients 
with lung cancer. This application enables rapid, low-cost assessment of 
biomarker testing, which can be used for evaluation of guideline adherence and 
retrospective biomarker follow-up studies at a nationwide level of the 
Netherlands.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00428-025-04337-6
PMID: 41318702

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was approved by the privacy and scientific committee of 
Palga and the ethical committee of the Netherlands Comprehensive Cancer 
Organization (24–00395). Informed consent was not required due to the study 
design. Conflict of interest: VdJ has received speaker fees received from 
Uitgeverij Jaap (Roche Diagnostics Netherlands B.V., Janssen-Cilag B.V.) (paid 
to UMCG account) and Canadian Anatomic and Molecular Pathology – Pathology 
Oncology Digital Series (Amgen, Bayer, Daiichi-Sankyo, AstraZeneca, Eli Lilly, 
Roche, Pfizer) (paid to UMCG account). AvdW has received grants or contracts 
from AstraZeneca, Boehringer Ingelheim, Pfizer, Roche, and Takeda, has received 
consulting fees from AstraZeneca, Janssen, Eli Lilly, Roche, and Takeda, has 
received payment or honoraria for lectures, presentations, speakers bureaus, 
manuscript writing or educational events from AstraZeneca, Bristol-Myers Squibb, 
Eli Lilly, Pfizer, and Roche, has a leadership or fiduciary role in oncology 
section NVALT, guideline committee NSCLC and CUP, dure geneesmiddelen committee 
NVALT and FMS. LvK has received institutional grants or contracts from Amgen, 
AstraZeneca, Bayer, Janssen-Cilag, Merck, Roche, and Servier, has received 
payments or honoraria for lectures, presentations, speakers bureaus, manuscript 
writing or educational events from AstraZeneca, Bayer, Bristol-Myers Squibb, Eli 
Lilly, Janssen, Novartis, Pfizer, and Roche (all to institution), has received 
support for attending meetings and/or travel from Roche and ThermoFisher (all to 
institution), has participated on a Data Safety Monitoring Board or Advisory 
Board of Cyclomics, Janssen-Cilag, LOGEX, Merck, Menarini, Protyon, and Roche 
(all payments to institution), has a leadership or fiduciary role in the EORTC 
Melanoma Group and the Commission Personalized Medicine – Belgium (all payments 
to institution), has stock or stock options in Cyclomics (personal). ES has 
received (unrestricted) grants from Abbott, Biocartis, AstraZeneca, 
Invitae/Archer, Bayer, Bio-Rad, Roche, Agena Bioscience, CC Diagnositcs, 
MSD/Merck, and SNN/EFRO (all paid to UMCG institution), has received advisory 
board (incidental) and travel expenses (honoraria/grant paid to UMCG 
institution) from MSD/Merck, AstraZeneca, Roche, Novartis, Bayer, BMS, Lilly, 
Amgen, Illumina, Agena Bioscience, CC Diagnostics, and Janssen Cilag 
(Johnson&Johnson), has received advisory board (incidental) (honoraria/grant 
paid to UMCG institution) from Astellas Pharma, GSK, Sinnovisionlab, Sysmex, and 
Protyon, has received payments or honoraria for lectures, presentations, 
speakers bureaus, manuscript writing or educational events from Bio-Rad (travel 
expenses and honoraria paid to UMCG institution), Seracare (honoraria paid to 
UMCG institution), Roche (travel expenses and honoraria paid to UMCG 
institution), Biocartis (travel expenses and honoraria paid to UMCG 
institution), Lilly (travel expenses and honoraria paid to UMCG institution), 
Agena Bioscience (travel expenses and honoraria paid to UMCG institution), and 
Illumina (honoraria paid to UMCG institution, has received support for attending 
meetings and/or travel from BioRad (only travel expenses), Biocartis (only 
travel expenses), Ageno Sciences (only travel expenses), Illumina (travel 
expenses), Roche/Foundation Medicine (travel/hotel/registration expenses 
(partially)), and QCMD (travel/hotel/registration expenses), has a leadership or 
fiduciary role in the Dutch Society of Pathology (board member, unpaid), 
European Society of Pathology (board member, unpaid), European Liquid Biopsy 
Society (board member, unpaid), is chairman/member of the Committee for 
assessment of molecular diagnostics (cieBOD, honoraria paid to UMCG 
institution), is committee member of the National guideline advisory committee 
(honoraria paid to UMCG institution), and is board member of the Committee for 
Clinical Essential Targets (cieKNT) (unpaid). SW has received grants or 
contracts from Roche, Bayer, Eli Lilly, Pfizer, AstraZeneca, Merck Sharp & 
Dohme, Amgen, and Novartis (unrestricted research grants), and has a leadership 
or fiduciary role in the strategic advisory board of Roche. Dr. Garcia declares 
no conflict of interest.


800. Brain Sci. 2025 Apr 11;15(4):391. doi: 10.3390/brainsci15040391.

AI-Driven Information for Relatives of Patients with Malignant Middle Cerebral 
Artery Infarction: A Preliminary Validation Study Using GPT-4o.

Al Barajraji M(1), Barrit S(2), Ben-Hamouda N(3), Harel E(1), Torcida N(4), 
Pizzarotti B(5), Massager N(2), Lechien JR(6)(7)(8).

Author information:
(1)Department of Neurosurgery, University Hospital of Lausanne and University of 
Lausanne, 1005 Lausanne, Switzerland.
(2)Department of Neurosurgery, CHU Tivoli, 7110 La Louvière, Belgium.
(3)Department of Adult Intensive Care, University Hospital of Lausanne (CHUV), 
University of Lausanne, 1005 Lausanne, Switzerland.
(4)Department of Neurology, Hôpital Universitaire de Bruxelles (HUB), 1070 
Brussels, Belgium.
(5)Department of Neurology, University Hospital of Lausanne (CHUV), University 
of Lausanne, 1011 Lausanne, Switzerland.
(6)Department of Surgery, UMONS Research Institute for Health Sciences and 
Technology, University of Mons (UMons), 7000 Mons, Belgium.
(7)Department of Otolaryngology, Elsan Polyclinic of Poitiers, 86000 Poitiers, 
France.
(8)Department of Otolaryngology-Head Neck Surgery, Foch Hospital, School of 
Medicine, UFR Simone Veil, Université Versailles Saint-Quentin-en-Yvelines 
(Paris Saclay University), 78035 Paris, France.

Purpose: This study examines GPT-4o's ability to communicate effectively with 
relatives of patients undergoing decompressive hemicraniectomy (DHC) after 
malignant middle cerebral artery infarction (MMCAI). Methods: GPT-4o was asked 
25 common questions from patients' relatives about DHC for MMCAI, twice over a 
7-day interval. Responses were rated for accuracy, clarity, relevance, 
completeness, sourcing, and usefulness by board-certified intensivist* (one), 
neurologists, and neurosurgeons using the Quality Analysis of Medical AI (QAMAI) 
tool. Interrater reliability and stability were measured using ICC and Pearson's 
correlation. Results: The total QAMAI scores were 22.32 ± 3.08 for the 
intensivist, 24.68 ± 2.8 for the neurologist, 23.36 ± 2.86 and 26.32 ± 2.91 for 
the neurosurgeons, representing moderate-to-high accuracy. The evaluators 
reported moderate ICC (0.631, 95% CI: 0.321-0.821). The highest subscores were 
for the categories of accuracy, clarity, and relevance while the poorest were 
associated with completeness, usefulness, and sourcing. GPT-4o did not 
systematically provide references for their responses. The stability analysis 
reported moderate-to-high stability. The readability assessment revealed an FRE 
score of 7.23, an FKG score of 15.87 and a GF index of 18.15. Conclusions: 
GPT-4o provides moderate-to-high quality information related to DHC for MMCAI, 
with strengths in accuracy, clarity, and relevance. However, limitations in 
completeness, sourcing, and readability may impact its effectiveness in patient 
or their relatives' education.

DOI: 10.3390/brainsci15040391
PMCID: PMC12026103
PMID: 40309831

Conflict of interest statement: The authors declare no conflicts of interest.


801. Eur Arch Otorhinolaryngol. 2025 Nov 1. doi: 10.1007/s00405-025-09785-z. Online 
ahead of print.

Experts V/S AI´s 2.0: Comparative evaluation of AI models and expert consensus 
in obstructive sleep apnea assessment.

Cammaroto G(1)(2), Mira FA(2)(3), Favier V(2)(4), Nunes HDSS(2)(5), de Castro 
JV(2)(6), Carsuzaa F(2)(7), Lechien JR(2)(8), Chiesa Estomba C(2)(9), Iannella 
G(2)(10), Vaira LA(2)(5)(11), Calvo-Henriquez C(2)(12), Cheong RCT(13), de 
Apodaca PMR(2)(14), Lentini M(15)(16), Barillari MR(2)(15)(17), Maniaci 
A(18)(19)(20).

Author information:
(1)Head and Neck Department, ENT & Oral Surgery Unity, G.B. Morgagni, L. 
Pierantoni Hospital, Forlì, Italy.
(2)Young Otolaryngologists-International Federations of Oto-rhinolaryngological 
Societies (YO-IFOS), Paris, France.
(3)ENT Department, Hospital of Linares, Linares, Chile.
(4)ENT Department, University Hospital of Montpellier, Montpellier, France.
(5)ENT and Sleep Medicine Department, Nucleus of Otolaryngology, Head and Neck 
Surgery and Sleep Medicine of São Paulo, São Paulo, Brazil.
(6)ENT Department, Armed Forces Hospital, Lisbon, Portugal.
(7)ENT Department, University Hospital of Poitiers, Poitiers, France.
(8)Division of Laryngology and Broncho-Esophagology, Department of 
Otolaryngology and Head and Neck Surgery, EpiCURA Hospital, UMONS Research 
Institute for Health Sciences and Technology, University of Mons, University of 
Mons, Belgium.
(9)Department of Otorhinolaryngology, Donostia University Hospital, Biodonostia 
Research Institute, Osakidetza, San Sebastian, Spain, Spain.
(10)epartment of 'Organi di Senso', University "Sapienza", Viale dell'Università 
33, Rome, 00185, Italy.
(11)Maxillofacial Surgery Operative Unit, Department of Medicine, Surgery and 
Pharmacy, University of Sassari, Sassari, Italy.
(12)Otolaryngology Department, Hospital Complex of Santiago de Compostela, 
Rhinology and Sleep Apnea Unit, Santiago de Compostela, Spain.
(13)Royal National ENT and Eastman Dental Hospitals, University College London 
Hospitals NHS, London, UK.
(14)ORL, Hospital Universitario Dr. Peset, Valencia, Spain.
(15)Department of medicine and surgery, University of Enna Kore, Enna, Italy.
(16)ASP Ragusa-Hospital Giovanni Paolo II, Ragusa, 97100, Italy.
(17)Division of Phoniatrics and Audiology, Department of Mental and Physical 
Health and Preventive Medicine, University of Campania Luigi Vanvitelli, Naples, 
Italy.
(18)Young Otolaryngologists-International Federations of Oto-rhinolaryngological 
Societies (YO-IFOS), Paris, France. Antonino.maniaci@unikore.it.
(19)Department of medicine and surgery, University of Enna Kore, Enna, Italy. 
Antonino.maniaci@unikore.it.
(20)ASP Ragusa-Hospital Giovanni Paolo II, Ragusa, 97100, Italy. 
Antonino.maniaci@unikore.it.

PURPOSE: This study aims to compare the evaluation of obstructive sleep apnea 
(OSA) by ten super-experts using responses from a 10-question survey answered by 
3 different artificial intelligence chatbots, Chat GPT-3.5, Chat GPT-4.0, 
Gemini, and a panel of 100 otolaryngologists specialized in sleep medicine.
METHODS: A 10-question survey regarding OSA management was answered by Chat 
GPT-3.5, Chat GPT-4.0, Gemini, and a panel of 100 otolaryngologists. The 
responses were assessed by ten super-experts in sleep medicine for their 
agreement with expert consensus, using a Likert scale. Statistical analyses were 
performed to evaluate the level of agreement and significance.
RESULT: Expert consensus had the highest mean score (4.5 ± 0.9), significantly 
outperforming all AI models. ChatGPT-3.5 was the best among AI systems, with a 
score of 4.1 ± 1.2 (p=0.003), followed by ChatGPT-4 with 3.9 ± 1.4 (p<0.001) and 
Gemini with 3.6 ± 1.5 (p<0.001). Perfect agreement with expert consensus was 
achieved in specific scenarios, particularly regarding indications for bariatric 
surgery and lateral pharyngoplasty. However, there were significant differences 
in complex clinical scenarios that required integration of multiple factors, 
particularly in therapeutic management questions where the performance of AI 
models was significantly below that of expert consensus (p<0.01).
CONCLUSIONS: Although AI models are promising in the management of OSA, 
especially for well-defined clinical scenarios, they at present serve best as 
complementary tools rather than replacements for expert clinical judgment. Most 
surprisingly, ChatGPT-3.5 outperformed its newer versions in many aspects, 
indicating that model updates with general capabilities may not always lead to 
better performance in specialized medical domains. These findings emphasize the 
potential of AI as a supportive resource while emphasizing the continuing need 
for human expertise in complex clinical decision-making.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00405-025-09785-z
PMID: 41176557

Conflict of interest statement: Declarations: This study was conducted in 
accordance with the principles of ethical and professional conduct, as outlined 
by the Declaration of Helsinki and relevant international guidelines. The 
following aspects regarding compliance with ethical standards were addressed: 
Research Involving Human Participants and/or Animals: This study did not involve 
human participants or animals. As such, no ethical approval was required. 
Informed consent: Since no human participants were directly involved, informed 
consent was not applicable in this study. However, the otolaryngology 
specialists who participated as evaluators were informed about the purpose of 
the study and provided their consent. Conflicts of interest: The authors declare 
that they have no financial or non-financial conflicts of interest related to 
the content of this article. Competing interests: The author Jerome R. Lechien 
was not involved with the peer review process of this article.


802. J Med Internet Res. 2025 May 23;27:e67365. doi: 10.2196/67365.

Safety and User Experience of a Generative Artificial Intelligence Digital 
Mental Health Intervention: Exploratory Randomized Controlled Trial.

Campellone TR(1), Flom M(1), Montgomery RM(1), Bullard L(1), Pirner MC(1), Pavez 
A(1), Morales M(1), Harper D(1), Oddy C(1), O'Connor T(1), Daniels J(1), Eaneff 
S(1), Forman-Hoffman VL(1), Sackett C(1), Darcy A(1).

Author information:
(1)Woebot Health, San Francisco, CA, United States.

BACKGROUND: General awareness and exposure to generative artificial intelligence 
(AI) have increased recently. This transformative technology has the potential 
to create a more dynamic and engaging user experience in digital mental health 
interventions (DMHIs). However, if not appropriately used and controlled, it can 
introduce risks to users that may result in harm and erode trust. At the time of 
conducting this trial, there had not been a rigorous evaluation of an approach 
to safely implementing generative AI in a DMHI.
OBJECTIVE: This study aims to explore the user relationship, experience, safety, 
and technical guardrails of a DMHI using generative AI compared with a 
rules-based intervention.
METHODS: We conducted a 2-week exploratory randomized controlled trial (RCT) 
with 160 adult participants randomized to receive a generative AI (n=81) or 
rules-based (n=79) version of a conversation-based DMHI. Self-report measures of 
the user relationship (client satisfaction, working alliance bond, and accuracy 
of empathic listening and reflection) and experience (engagement metrics, 
adverse events, and technical guardrail success) were collected. Descriptions 
and validation of technical guardrails for handling user inputs (eg, detecting 
potentially concerning language and off-topic responses) and model outputs (eg, 
not providing medical advice and not providing a diagnosis) are provided, along 
with examples to illustrate how they worked. Safety monitoring was conducted 
throughout the trial for adverse events, and the success of technical guardrails 
created for the generative arm was assessed post trial.
RESULTS: In general, the majority of measures of user relationship and 
experience appeared to be similar in both the generative and rules-based arms. 
The generative arm appeared to be more accurate at detecting and responding to 
user statements with empathy (98% accuracy vs 69%). There were no serious or 
device-related adverse events, and technical guardrails were shown to be 100% 
successful in posttrial review of generated statements. A majority of 
participants in both groups reported an increase in positive sentiment (62% and 
66%) about AI at the end of the trial.
CONCLUSIONS: This trial provides initial evidence that, with the right 
guardrails and process, generative AI can be successfully used in a digital 
mental health intervention (DMHI) while maintaining the user experience and 
relationship. It also provides an initial blueprint for approaches to technical 
and conversational guardrails that can be replicated to build a safe DMHI.
TRIAL REGISTRATION: ClinicalTrials.gov NCT05948670; 
https://clinicaltrials.gov/study/NCT05948670.

©Timothy R Campellone, Megan Flom, Robert M Montgomery, Lauren Bullard, Maddison 
C Pirner, Aaron Pavez, Michelle Morales, Devin Harper, Catherine Oddy, Tom 
O'Connor, Jade Daniels, Stephanie Eaneff, Valerie L Forman-Hoffman, Casey 
Sackett, Alison Darcy. Originally published in the Journal of Medical Internet 
Research (https://www.jmir.org), 23.05.2025.

DOI: 10.2196/67365
PMCID: PMC12144468
PMID: 40408143 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: Authors TC, MF, RM, LB, 
MCP, CS, CO, TO, AP, MM, DH, JD, SE, VFH, and AD were employees of Woebot Health 
at the time the study was conducted.


803. Eur J Radiol. 2026 Jan;194:112558. doi: 10.1016/j.ejrad.2025.112558. Epub 2025 
Nov 19.

Exploring environmental sustainability of artificial intelligence in radiology: 
A scoping review.

Champendal M(1), Lokaj B(2), de Gevigney VD(3), Brulé G(4), Zaghir J(5), Boiko 
P(6), Lovis C(7), Müller H(8), Schmid J(9), Ribeiro RT(10).

Author information:
(1)HESAV School of Health Sciences. Vaud, HES-SO University of Applied Sciences 
and Arts Western, Switzerland; Faculty of Biology and Medicine, University of 
Lausanne, Lausanne, Switzerland. Electronic address: 
melanie.champendal@hesav.ch.
(2)Geneva School of Health Sciences, HES-SO University of Applied Sciences and 
Arts Western Switzerland; Department of Radiology and Medical Informatics, 
University of Geneva, Geneva, Switzerland. Electronic address: 
belinda.lokaj@hesge.ch.
(3)Geneva School of Health Sciences, HES-SO University of Applied Sciences and 
Arts Western Switzerland. Electronic address: 
valentin.durand-de-gevigney@hesge.ch.
(4)Geneva School of Health Sciences, HES-SO University of Applied Sciences and 
Arts Western Switzerland. Electronic address: gael.brule@hesge.ch.
(5)Department of Radiology and Medical Informatics, University of Geneva, 
Geneva, Switzerland; Division of Medical Information Sciences, Geneva University 
Hospitals, Geneva, Switzerland. Electronic address: Jamil.Zaghir@unige.ch.
(6)Geneva School of Health Sciences, HES-SO University of Applied Sciences and 
Arts Western Switzerland. Electronic address: polina.boiko@hesge.ch.
(7)Department of Radiology and Medical Informatics, University of Geneva, 
Geneva, Switzerland; Division of Medical Information Sciences, Geneva University 
Hospitals, Geneva, Switzerland. Electronic address: Christian.Lovis@unige.ch.
(8)Informatics Institute, University of Applied Sciences Western Switzerland 
(HES-SO Valais) Sierre, Switzerland; Medical faculty, University of Geneva, 
Switzerland. Electronic address: henning.mueller@hevs.ch.
(9)Geneva School of Health Sciences, HES-SO University of Applied Sciences and 
Arts Western Switzerland. Electronic address: jerome.schmid@hesge.ch.
(10)HESAV School of Health Sciences. Vaud, HES-SO University of Applied Sciences 
and Arts Western, Switzerland. Electronic address: ricardo.ribeiro@hesav.ch.

OBJECTIVE: Artificial intelligence (AI) is increasingly used in radiology, but 
its environmental implications have not been sufficiently studied, so far. This 
study aims to synthesize existing literature on the environmental sustainability 
of AI in radiology and highlights strategies proposed to mitigate its impact.
METHODS: A scoping review was conducted following the Joanna Briggs Institute 
methodology. Searches across MEDLINE, Embase, CINAHL, and Web of Science focused 
on English and French publications from 2014 to 2024, targeting AI, 
environmental sustainability, and medical imaging. Eligible studies addressed 
environmental sustainability of AI in medical imaging. Conference abstracts, 
non-radiological or non-human studies, and unavailable full texts were excluded. 
Two independent reviewers assessed titles, abstracts, and full texts, while four 
reviewers conducted data extraction and analysis.
RESULTS: The search identified 3,723 results, of which 13 met inclusion 
criteria: nine research articles and four reviews. Four themes emerged: energy 
consumption (n = 10), carbon footprint (n = 6), computational resources (n = 9), 
and water consumption (n = 2). Reported metrics included CO2-equivalent 
emissions, training time, power use effectiveness, equivalent distance travelled 
by car, energy demands, and water consumption. Strategies to enhance 
sustainability included lightweight model architectures, quantization and 
pruning, efficient optimizers, and early stopping. Broader recommendations 
encompassed integrating carbon and energy metrics into AI evaluation, 
transitioning to cloud computing, and developing an eco-label for radiology AI 
systems.
CONCLUSIONS: Research on sustainable AI in radiology remains scarce but is 
rapidly growing. This review highlights key metrics and strategies to guide 
future research and practice toward more transparent, consistent, and 
environmentally responsible AI development in radiology.
ABBREVIATIONS: AI, Artificial intelligence; CNN, Convolutional neural networks; 
CT, Computed tomography; CPU, Central Processing Unit; DL, Deep learning; FLOP, 
Floating-point operation; GHG, Greenhouses gas; GPU, Graphics Processing Unit; 
LCA, Life Cycle Assessment; LLM, Large Language Model; MeSH, Medical Subject 
Headings; ML, Machine learning; MRI, Magnetic resonance imaging; NLP, Natural 
language processing; PUE, Power Usage Effectiveness; TPU, Tensor Processing 
Unit; USA, United States of America; ViT, Vision Transformer; WUE, Water Usage 
Effectiveness.

Copyright © 2025 The Author(s). Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ejrad.2025.112558
PMID: 41275851 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


804. IEEE J Biomed Health Inform. 2025 Oct;29(10):7694-7703. doi: 
10.1109/JBHI.2025.3563289.

When East Meets West: Cross-Domain Drug Interaction Annotations With Large 
Language Models and Bidirectional Neural Networks.

Zhang R, Xie W, Lin Q, Li X, Wong KC.

Drug combination therapy is a promising strategy for managing complex and 
co-existing diseases. However, drug-drug interactions (DDIs) can result in 
unexpected adverse effects, making it crucial to understand such interactions to 
prevent adverse drug reactions and develop new therapeutic strategies. Current 
DDI annotation methods heavily rely on atom-level graph structural features, 
overlooking valuable drug contextual representations within medical literature. 
Additionally, these methods are typically designed for a specific task, limiting 
their scalability to diverse medical scenarios. To address these limitations, we 
propose TEmbed-DDI, a novel framework that leverages contextual representations 
and pre-trained large language model embeddings to enhance feature extraction 
for DDI annotations. Specifically, we retrieve meaningful contextual texts for 
each drug to enrich semantic features and adopt pre-trained large language model 
embeddings to capture rich features from these long-range contextual 
representations. TEmbed-DDI is the first framework to incorporate LLM-powered 
embeddings for medical interaction annotations. Furthermore, a bidirectional 
neural network is integrated into TEmbed-DDI for the integrative Western and 
traditional Chinese medicine DDI annotation tasks. Comparative results 
demonstrate that TEmbed-DDI achieves state-of-the-art performance, with the 
highest AUC scores of 0.992 and 0.95 on the Western CHCH and DEEP interaction 
annotation benchmarks. Even for the newly constructed Traditional Chinese 
Medicine (TCM) DDI annotation benchmark, TEmbed-DDI consistently exhibits 
outstanding generalization capability, achieving an AUC of 0.956. Moreover, case 
studies further validate TEmbed-DDI's capability to annotate previously unknown 
interactions. These findings suggest that TEmbed-DDI can serve as a valuable 
tool in annotating previously unknown drug combinations for real-world 
applications, facilitating the development of efficacious therapies. 
Furthermore, as the first framework combining traditional Chinese medicine into 
DDI annotation tasks, its adaptability highlights the potential in supporting 
cross-domain medical research. TEmbed-DDI's design principles can inspire the 
development of flexible LLM-powered frameworks for drug combination discovery in 
the future.

DOI: 10.1109/JBHI.2025.3563289
PMID: 40261770 [Indexed for MEDLINE]


805. J Med Internet Res. 2025 Dec 17;27:e83219. doi: 10.2196/83219.

Automated Multitier Tagging of Chinese Online Health Education Resources Using a 
Large Language Model: Development and Validation Study.

Meng J(1), Dai R(2), Huang X(3), Gu Y(3), Yan S(2), Wang X(2), Gao J(3), Zhang 
TT(1).

Author information:
(1)School of Public Health, Fudan University, Shanghai, China.
(2)Shanghai Center for Emerging Technologies Governance in Medicine and Public 
Health, Shanghai, China.
(3)Shanghai Municipal Center for Health Promotion, Shanghai, China.

BACKGROUND: Precision health promotion, which aims to tailor health messages to 
individual needs, is hampered by the lack of structured metadata in vast digital 
health resource libraries. This bottleneck prevents scalable, personalized 
content delivery and exacerbates information overload for the public.
OBJECTIVE: This study aimed to develop, deploy, and validate an automated 
tagging system using a large language model (LLM) to create the foundational 
metadata infrastructure required for tailored health communication at scale.
METHODS: We developed a comprehensive, 3-tier health promotion taxonomy (10 
primary, 34 secondary, and 90,562 tertiary tags) using a hybrid Delphi and 
corpus-mining methodology. We then constructed a hybrid inference pipeline by 
fine-tuning a Baichuan2-7B LLM with low-rank adaptation for initial tag 
generation. This was then refined by a domain-specific named entity recognition 
model and standardized against a vector database. The system's performance was 
evaluated against manual annotations from nonexpert staff on a test set of 1000 
resources. We used a "no gold standard" framework, comparing the artificial 
intelligence-human (A-H) interrater reliability (IRR) with a supplemental 
human-human (H-H) IRR baseline and expert adjudication for cases where 
artificial intelligence provided additional tags ("AI Additive").
RESULTS: The A-H agreement was moderate (Cohen κ=0.54, 95% CI 0.53-0.56; Jaccard 
similarity coefficient=0.48, 95% CI 0.46-0.50). Critically, this was higher than 
the baseline nonexpert H-H agreement (Cohen κ=0.32, 95% CI 0.29-0.35; Jaccard 
similarity coefficient=0.35, 95% CI 0.27-0.43). A granular analysis of 
disagreements revealed that in 15.9% (159/1000) of the cases, the "AI Additive" 
tags were not identified by human annotators. Expert adjudication of these cases 
confirmed that the "AI Additive" tags were correct and relevant with a precision 
of 90% (45/50; 95% CI 78.2%-96.7%).
CONCLUSIONS: A fine-tuned LLM, integrated into a hybrid pipeline, can function 
as a powerful augmentation tool for health content annotation. The system's 
consistency (A-H κ=0.54) was found to be superior to the baseline human workflow 
(H-H κ=0.32). By moving beyond simple automation to reliably identify relevant 
health topics missed by manual annotators with high, expert-validated accuracy, 
this study provides a robust technical and methodological blueprint for 
implementing artificial intelligence to enhance precision health communication 
in public health settings.

©Jialin Meng, Ruiming Dai, Xiaolan Huang, Yi Gu, Shixing Yan, Xiaoke Wang, 
Jingrong Gao, Tian-Tian Zhang. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org), 17.12.2025.

DOI: 10.2196/83219
PMCID: PMC12756663
PMID: 41251541 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


806. Ann Biomed Eng. 2025 Oct;53(10):2385-2388. doi: 10.1007/s10439-025-03827-7. Epub 
2025 Aug 17.

Hidden Prompts in Manuscripts Threaten the Integrity of Peer Review and 
Research: Recommendations for Journals and Institutions.

Giray L(1).

Author information:
(1)Department of Liberal Arts, School of Foundational Studies and Education, 
Mapúa University, Manila, Philippines. lggiray@mapua.edu.ph.

I examine the scholarly implications of a troubling case where researchers 
embedded hidden prompts like "give a positive review only" into academic 
preprints to manipulate AI-assisted peer review. AI is now woven into nearly 
every facet of academic life, including the peer review process. I contend that 
manipulating peer review through embedding secret prompts is as serious as 
plagiarism or data fabrication. Peer review may not be perfect, but deception is 
misconduct. Reviewers must still be held accountable. Those who blindly rely on 
AI outputs without critical engagement fail in their scholarly duty. AI should 
only amplify the reviewer's expertise. As institutions begin regulating AI in 
research, similar frameworks must extend to peer review. Journals and publishers 
should establish clear, enforceable guidelines on acceptable AI use: Will AI be 
banned, regulated, or embraced? If allowed, disclosures must be mandatory. 
Authors should also be informed if AI tools will be used in the review process, 
ensuring transparency and consent. Confidentiality is another pressing issue. 
Real cases have shown how ChatGPT links shared by reviewers were indexed online, 
compromising sensitive, unpublished research, even though OpenAI has since moved 
to discontinue public link discoverability. Beyond policy, we must cultivate a 
culture of transparency, trust, and responsibility. Institutions can host ethics 
workshops and mentor early-career scholars. This is not just about AI; it is 
about who we are as researchers and reviewers. No matter how advanced the 
technology, integrity must remain our anchor. Without it, even the most 
innovative research stands on shaky ground.

© 2025. The Author(s) under exclusive licence to Biomedical Engineering Society.

DOI: 10.1007/s10439-025-03827-7
PMID: 40820180 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: No benefits 
in any form have been or will be received from a commercial party related 
directly or indirectly to the subject of this manuscript. The author declares no 
conflict of interest. Ethical Approval: This study does not include any 
individual-level data, and thus does not require any ethical approval.


807. Schizophr Bull. 2025 Aug 13:sbaf102. doi: 10.1093/schbul/sbaf102. Online ahead 
of print.

Analyzing Generative AI and Machine Learning in Auto-Assessing Schizophrenia's 
Negative Symptoms.

Liu CM(1)(2), Chan YH(3), Ho MY(4), Liu CC(1)(2), Lu MH(1), Liao YA(1), Hsieh 
MH(1)(2), Tseng YJ(3)(4)(5)(6).

Author information:
(1)Department of Psychiatry, National Taiwan University Hospital, Taipei 100229, 
Taiwan.
(2)College of Medicine, National Taiwan University, Taipei 100233, Taiwan.
(3)Graduate Institute of Biomedical Electronics and Bioinformatics, National 
Taiwan University, Taipei 106319, Taiwan.
(4)Department of Computer Science and Information Engineering, National Taiwan 
University, Taipei 106319, Taiwan.
(5)The Metabolomics Core Laboratory, Centers of Genomic and Precision Medicine, 
National Taiwan University, Taipei 100025, Taiwan.
(6)School of Pharmacy, College of Medicine, National Taiwan University, Taipei 
100025, Taiwan.

BACKGROUND AND HYPOTHESIS: Traditional assessments of schizophrenia's negative 
symptoms rely on subjective and time-consuming psychiatric interviews. To 
provide more objective and efficient evaluations, this study examines the 
efficacy of an automated system utilizing generative AI (GenAI) and machine 
learning (ML) to assess negative symptoms of schizophrenia, including expression 
(EXP) and motivation and pleasure (MAP) domains.
STUDY DESIGN: A semi-structured interview protocol based on the Clinical 
Assessment Interview for Negative Symptoms was used to conduct interviews with 
schizophrenia patients. An experienced senior psychiatrist carried out these 
interviews, which were audio- and video-recorded, at the National Taiwan 
University Hospital between July 2022 and August 2023. An ML-based system 
analyzed visual and audio data for EXP assessment, while GenAI analyzed 
interview transcripts for MAP assessment.
STUDY RESULTS: The study cohort consisted of 69 males and 91 females with a mean 
age of 41.68 years (SD = 10.46). The ML-based EXP assessment showed moderate to 
substantial reliability, with an intraclass correlation coefficient (3, 1) 
(ICC3,1) of 0.65 and a weighted kappa of 0.62. The GenAI-based MAP assessment 
demonstrated good reliability, with an ICC3,1 of 0.82 and a weighted kappa of 
0.77. The system achieved strong linear correlations with clinician ratings 
(Pearson's correlation coefficient ≥ 0.54) and maintained low error rates (mean 
absolute error ≤ 0.81; root mean square error ≤ 1.16) for each assessment item.
CONCLUSIONS: The study demonstrates the efficacy of GenAI and ML in the 
automated assessment of schizophrenia's negative symptoms, highlighting their 
potential to enhance the consistency and efficiency of clinical evaluations.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
Maryland Psychiatric Research Center. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/schbul/sbaf102
PMID: 40801756


808. J Med Internet Res. 2025 Mar 10;27:e60435. doi: 10.2196/60435.

Generative AI-Enabled Therapy Support Tool for Improved Clinical Outcomes and 
Patient Engagement in Group Therapy: Real-World Observational Study.

Habicht J(#)(1), Dina LM(#)(1)(2), McFadyen J(1), Stylianou M(3), Harper R(1), 
Hauser TU(1)(4)(5)(6), Rollwage M(1).

Author information:
(1)Limbic Ltd, London, United Kingdom.
(2)Department of Psychology, King's College London, London, United Kingdom.
(3)Everyturn Mental Health, Gosforth, United Kingdom.
(4)Max Planck UCL Centre for Computational Psychiatry and Ageing Research, 
University College London, London, United Kingdom.
(5)Department of Psychiatry and Psychotherapy, Medical School and University 
Hospital, Eberhard Karls University of Tubingen, Tübingen, Germany.
(6)German Center for Mental Health, Tübingen, Germany.
(#)Contributed equally

BACKGROUND: Cognitive behavioral therapy (CBT) is a highly effective treatment 
for depression and anxiety disorders. Nonetheless, a substantial proportion of 
patients do not respond to treatment. The lack of engagement with therapeutic 
materials and exercises between sessions, a necessary component of CBT, is a key 
determinant of unsuccessful treatment.
OBJECTIVE: The objective of this study was to test whether the deployment of a 
generative artificial intelligence (AI)-enabled therapy support tool, which 
helps patients to engage with therapeutic materials and exercises in between 
sessions, leads to improved treatment success and patient treatment adherence 
compared with the standard delivery of CBT exercises through static workbooks.
METHODS: We conducted a real-world observational study of 244 patients receiving 
group-based CBT in 5 of the United Kingdom's National Health Service Talking 
Therapies services, comparing 150 (61.5%) patients who used the AI-enabled 
therapy support tool to 94 (38.5%) patients who used the standard delivery of 
CBT exercises. The groups were equivalent with respect to the content of the CBT 
materials and the human-led therapy sessions; however, the intervention group 
received support from the AI-enabled therapy support tool in conducting CBT 
exercises.
RESULTS: Patients using the AI-enabled therapy support tool exhibited greater 
attendance at therapy sessions and fewer dropouts from treatment. Furthermore, 
these patients demonstrated higher reliable improvement, recovery, and reliable 
recovery rates when compared to the control group, which was related to the 
degree of use of the AI-enabled therapy support tool. Moreover, we found that 
engagement with AI-supported CBT interventions, relative to psychoeducational 
materials, predicted better treatment adherence and treatment success, 
highlighting the role of personalization in the intervention's effectiveness. To 
investigate the mechanisms of these effects further, we conducted a separate 
qualitative experiment in a nonclinical sample of users (n=113). Results 
indicated that users perceived the AI-enabled therapy support tool as most 
useful for discussing their problems to gain awareness and clarity of their 
situation as well as learning how to apply coping skills and CBT techniques in 
their daily lives.
CONCLUSIONS: Our results show that an AI-enabled, personalized therapy support 
tool in combination with human-led group therapy is a promising avenue to 
improve the efficacy of and adherence to mental health care.

©Johanna Habicht, Larisa-Maria Dina, Jessica McFadyen, Mona Stylianou, Ross 
Harper, Tobias U Hauser, Max Rollwage. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org), 10.03.2025.

DOI: 10.2196/60435
PMCID: PMC11933774
PMID: 40063074 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: JH, JM, RH, and MR are 
employed by Limbic Limited and hold shares in the company. LMD and TUH work as 
paid consultants for Limbic Limited. Moreover, TUH holds shares in the company.


809. Aesthetic Plast Surg. 2025 Feb;49(3):1011-1012. doi: 10.1007/s00266-023-03805-1. 
Epub 2023 Dec 28.

Dr. GAI: Significance of Generative AI in Plastic Surgery.

Ray PP(1).

Author information:
(1)Department of Computer Applications, Sikkim University, 6th Mile, PO-Tadong, 
Gangtok, Sikkim, 737102, India. ppray@cus.ac.in.

In this letter to the editor, I offer a critique of the article titled 
"Consulting the Digital Doctor: Google Versus ChatGPT as Sources of Information 
on Breast Implant-Associated Anaplastic Large Cell Lymphoma and Breast Implant 
Illness." While acknowledging the authors' pioneering effort to compare 
informational outputs from Google and a generative AI (GAI)-ChatGPT, I raise 
concerns about the methodology, lack of rigorous validation, potential biases, 
and the overstatement of findings. The letter suggests that the authors' 
conclusions about the superiority of ChatGPT in providing high-quality medical 
information may be premature, given the limitations of the study design and the 
evolving nature of artificial intelligence (AI) technology.No Level Assigned 
This journal requires that authors assign a level of evidence to each submission 
to which Evidence-Based Medicine rankings are applicable. This excludes Review 
Articles, Book Reviews, and manuscripts that concern Basic Science, Animal 
Studies, Cadaver Studies, and Experimental Studies. For a full description of 
these Evidence-Based Medicine ratings, please refer to the Table of Contents or 
the online Instructions to Authors www.springer.com/00266.

© 2023. Springer Science+Business Media, LLC, part of Springer Nature and 
International Society of Aesthetic Plastic Surgery.

DOI: 10.1007/s00266-023-03805-1
PMID: 38155290 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The author 
declares that he has no conflicts of interest to disclose. Ethical Approval and 
Consent to Participate: Not applicable. Informed Consent: For this type of study 
informed consent is not required. Statement of Human and Animal Rights: This 
article does not contain any studies with human participants or animals 
performed by any of the authors.


810. Anaesthesia. 2026 Jan;81(1):62-71. doi: 10.1111/anae.16755. Epub 2025 Sep 19.

Real-world deployment and evaluation of PEri-operative AI CHatbot (PEACH): a 
large language model chatbot for peri-operative medicine.

Ke YH(1)(2)(3), Jin L(4), Elangovan K(4), Ong BWX(5), Oh CY(5), Sim J(1), Loh 
KW(1), Soh CR(1)(3), Cheng JMH(1), Lee AKY(1), Ting DSW(3)(4)(6), Liu N(3), 
Abdullah HR(1)(2)(3).

Author information:
(1)Department of Anaesthesiology, Singapore General Hospital, Singapore.
(2)Data Science and Artificial Intelligence Lab, Singapore General Hospital, 
Singapore.
(3)Duke-NUS Medical School, Singapore.
(4)Artificial Intelligence Office Singhealth, Singapore.
(5)Open Government Products, Singapore.
(6)Department of Surgical Retina, Singapore National Eye Centre, Singapore.

INTRODUCTION: Large Language Models are emerging as powerful tools in 
healthcare, particularly for complex, domain-specific tasks. This study 
describes the development and evaluation of PEri-operative AI CHatbot (PEACH). 
It was developed by embedding 35 institutional peri-operative protocols into a 
secure large language model environment, with iterative prompt engineering and 
internal testing to ensure clinical relevance and accuracy.
METHODS: The system was tested with a silent deployment using real-world data. 
Accuracy, safety and usability were assessed. Accuracy was evaluated by 
comparing the responses from PEACH against institutional guidelines and expert 
consensus. Deviations and hallucinations were categorised based on potential 
harm, and user feedback was evaluated using the Davis' Technology Acceptance 
Model. Updates to PEACH were made after the initial silent deployment to make 
minor amendments to one of the protocols.
RESULTS: In total, 240 real-world clinical iterations were evaluated. 
First-generation accuracy was 97.5% (78/80), with an overall accuracy of 96.7% 
(232/240) across three iterations. In the updated PEACH, accuracy improved to 
97.9% (235/240), with a statistically significant difference from the null 
hypothesis of 95% accuracy (p = 0.018). Hallucinations and deviations were 
minimal (1/240 and 2/240, respectively). There was high usability, with 
clinicians noting that PEACH expedited decisions in 95% of cases. The κ 
statistic for inter-rater reliability for PEACH was 0.772 and 0.893 between 
three iterations, compared with 0.610 and 0.784 for experienced peri-operative 
physicians.
DISCUSSION: PEACH is an accurate, adaptable tool that enhances consistency and 
efficiency in peri-operative decision-making. Future research should explore 
scalability across specialties and its impact on clinical outcomes.

Plain Language Summary: Computer programs called large language models (LLMs) 
are becoming helpful in healthcare. This paper talks about how a special 
healthcare chatbot named PEACH was created. PEACH helps doctors make decisions 
before, during and after surgery. It learned from 35 sets of hospital rules to 
give useful and safe advice. The team tested it many times to make sure it gave 
correct answers. PEACH was tested using real hospital cases. The team checked 
how correct, safe and easy it was to use. They compared what PEACH said with 
what the hospital guidelines and expert doctors would say. Any wrong or made‐up 
answers were looked at carefully. The team also asked users what they thought 
about using PEACH. After the first test, they made a few small improvements to 
one of the rules PEACH used. The team tested PEACH on 240 real hospital cases. 
In the first test, it was right 97.5% of the time. After some changes, it got 
even better, being right 97.9% of the time. It almost never made things up or 
gave wrong advice. Doctors said it helped them make decisions faster 95% of the 
time. The tool worked well when different people used it and gave similar 
results. PEACH is a smart and reliable tool that helps doctors make better 
choices during surgeries. The team hopes to test it more and use it in other 
areas of medicine too.

© 2025 Association of Anaesthetists.

DOI: 10.1111/anae.16755
PMID: 40973491 [Indexed for MEDLINE]


811. Aesthetic Plast Surg. 2025 May;49(9):2638-2639. doi: 10.1007/s00266-024-04210-y. 
Epub 2024 Jul 8.

Response to "Letter to the Editor-Exploring the Unknown: Evaluating ChatGPT's 
Performance in Uncovering Novel Aspects of Plastic Surgery and Identifying Areas 
for Future Innovation".

Seth I(1)(2), Lim B(3)(4), Rozen WM(1)(2).

Author information:
(1)Department of Plastic Surgery, Peninsula Health, Melbourne, VIC, 3199, 
Australia.
(2)Central Clinical School, Monash University, Melbourne, Australia.
(3)Department of Plastic Surgery, Peninsula Health, Melbourne, VIC, 3199, 
Australia. lim.bryan58@gmail.com.
(4)Central Clinical School, Monash University, Melbourne, Australia. 
lim.bryan58@gmail.com.

We appreciate Dr. Qi and Dr. Niu for their insightful comments on our study, 
"Exploring the Unknown: Evaluating ChatGPT's Performance in Uncovering Novel 
Aspects of Plastic Surgery and Identifying Areas for Future Innovation." Their 
observations underscore significant considerations in the application of 
artificial intelligence (AI) in plastic surgery. We agree with their concern 
about potential biases in ChatGPT's responses. The AI's frequent attribution of 
the title "parent of plastic surgery" to Sir Harold Delf Gillies, despite 
gender-neutral terminology, highlights underlying biases from training data. 
These biases often reflect historical texts and contemporary writings. 
Addressing them requires refining training datasets for balanced representation 
and developing algorithms that adjust dynamically to diverse inputs. The authors 
also question the criteria ChatGPT uses to identify key contributions to plastic 
surgery. The AI's focus on microsurgery, minimally invasive techniques, and 
tissue engineering, while significant, may prioritize keyword prevalence over a 
holistic evaluation. Enhancing ChatGPT's capabilities through targeted training 
and input from subject matter experts could improve the AI's ability to generate 
more balanced outputs. The identified bias favoring reconstructive over cosmetic 
procedures is another critical point. While reconstructive advancements are 
transformative, cosmetic surgery also has significant innovations. Ensuring 
ChatGPT presents a balanced view of both reconstructive and cosmetic 
advancements is essential. This can be achieved by diversifying training data 
and calibrating the AI to give equitable weight to different subspecialties 
within plastic surgery. AI models like ChatGPT are proficient in processing and 
generating information but lack the human elements of creativity, intuition, and 
emotional depth critical for groundbreaking innovations. AI should complement, 
not replace, the expert judgment and innovative thinking of skilled plastic 
surgeons. Ensuring the accuracy of AI-generated responses is crucial. Clinicians 
must verify AIgenerated information against established medical literature and 
clinical guidelines to maintain accuracy in medical practice. Continuous 
feedback and improvement mechanisms are vital to enhance AI's clinical utility. 
The improvement of AI in plastic surgery will be driven by active involvement 
from surgeons, providing comprehensive and balanced data for training to ensure 
AI systems evolve to support and enhance clinical practice effectively.Level of 
Evidence V This journal requires that authors assign a level of evidence to each 
article. For a full description of these Evidence-Based Medicine ratings, please 
refer to the Table of Contents or the online Instructions to Authors  
www.springer.com/00266 .

© 2024. Springer Science+Business Media, LLC, part of Springer Nature and 
International Society of Aesthetic Plastic Surgery.

DOI: 10.1007/s00266-024-04210-y
PMID: 38977450 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare that they have no conflicts of interest to disclose Human and Animal 
Rights: This article does not contain any studies with human participants or 
animals performed by any of the authors Informed Consent: For this type of 
study, informed consent is not required


812. IEEE J Biomed Health Inform. 2025 Dec 16;PP. doi: 10.1109/JBHI.2025.3607819. 
Online ahead of print.

ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for 
Traditional Chinese Medicine Formula.

Zhang Z, Hao B, Li Y, Yin H.

Traditional Chinese Medicine (TCM) formulas play a significant role in treating 
epidemics and complex diseases. Existing models for TCM utilize traditional 
algorithms or deep learning techniques to analyze formula relationships, yet 
lack comprehensive results, such as complete formula compositions and detailed 
explanations. Although recent efforts have used TCM instruction datasets to 
fine-tune Large Language Models (LLMs) for explainable formula generation, 
existing datasets lack sufficient details, such as the roles of the formula's 
sovereign, minister, assistant, courier; efficacy; contraindications; tongue and 
pulse diagnosis--limiting the depth of model outputs. To address these 
challenges, we propose ZhiFang DanTai, a framework combining Graph-based 
Retrieval Augmented Generation (GraphRAG) with LLM fine-tuning. ZhiFang DanTai 
uses GraphRAG to retrieve and synthesize structured TCM knowledge into concise 
summaries, while also constructing an enhanced instruction dataset to improve 
LLMs' ability to integrate retrieved information. Furthermore, we provide novel 
theoretical proofs demonstrating that integrating GraphRAG with fine-tuning 
techniques can reduce generalization error and hallucination rates in the TCM 
formula task. Experimental re sults on both collected and clinical datasets 
demonstrate that ZhiFang DanTai achieves significant improvements over 
state-of-the-art models. Our model is open-sourced at 
https://huggingface.co/tczzx6/ZhiFangDanTai1.0.

DOI: 10.1109/JBHI.2025.3607819
PMID: 41406278


813. Reg Anesth Pain Med. 2025 Sep 2:rapm-2025-106852. doi: 10.1136/rapm-2025-106852. 
Online ahead of print.

Recommendations for disclosure of artificial intelligence in scientific writing 
and publishing: a regional anesthesia and pain medicine modified Delphi study.

Fettiplace MR(1), Bhatia A(2)(3), Chen Y(4), Orebaugh SL(5), Gofeld M(6), 
Gabriel RA(7), Sessler DI(8)(9), Lonsdale H(10), Bungart B(10), Cheng CP(11), 
Burnett GW(11), Han L(12), Wiles M(13)(14), Coppens S(15)(16), Joseph T(17), 
Schreiber KL(18), Volk T(19)(20), Urman RD(21), Kovacheva VP(18), Wu CL(22)(23), 
Mariano ER(12)(24), Ip VHY(25).

Author information:
(1)Anesthesiology, University of Illinois Chicago, Chicago, Illinois, USA 
fettiplace@gmail.com.
(2)Anesthesia and Pain Management, Toronto Western Hospital and University of 
Toronto, Toronto, Ontario, Canada.
(3)Institute of Health Policy, Management, and Evaluation, University of 
Toronto, Toronto, Ontario, Canada.
(4)Anesthesiology and Pain Medicine, University of Washington, Seattle, 
Washington, USA.
(5)Anesthesiology, University of Pittsburgh Medical Center Health System, 
Pittsburgh, Pennsylvania, USA.
(6)Anaesthesiology, Northern Ontario School of Medicine University, North York, 
Ontario, Canada.
(7)Anesthesiology, University of California San Diego, La Jolla, California, 
USA.
(8)Center for Outcomes Research, UTHealth, Houston, Texas, USA.
(9)Anesthesiology, UTHealth, Houston, Texas, USA.
(10)Anesthesiology, Vanderbilt University Medical Center, Nashville, Tennessee, 
USA.
(11)Anesthesiology, Perioperative & Pain Medicine, Icahn School of Medicine at 
Mount Sinai, New York, New York, USA.
(12)Anesthesiology, Perioperative and Pain Medicine, Stanford University School 
of Medicine, Stanford, California, USA.
(13)Academic Anaesthesia, Sheffield Teaching Hospitals NHS Foundation Trust, 
Sheffield, England, UK.
(14)Centre for Applied Health and Social Care Research, Sheffield Hallam 
University, Sheffield, England, UK.
(15)Anesthesiology, University Hospitals of Leuven, Leuven, Belgium.
(16)Cardiovascular Sciences, Biomedical Sciences Group, University of Leuven, 
Leuven, Belgium.
(17)Anesthesiology and Critical Care, University of Pennsylvania, Philadelphia, 
Pennsylvania, USA.
(18)Anesthesiology, Perioperative, and Pain Medicine, Brigham and Women's 
Hospital, Harvard Medical School, Boston, Massachusetts, USA.
(19)Anaesthesiology, Intensive Care and Pain Therapy, Saarland University 
Hospital and Saarland University Faculty of Medicine, Homburg, Germany.
(20)Outcomes Research Consortium, Houston, Texas, USA.
(21)Anesthesiology, College of Medicine, The Ohio State University, Columbus, 
Ohio, USA.
(22)Anesthesiology, Hospital for Special Surgery, New York, New York, USA.
(23)Anesthesiology, Weill Cornell Medicine, New York, New York, USA.
(24)Anesthesiology and Perioperative Care Service, VA Palo Alto Health Care 
System, Palo Alto, California, USA.
(25)Anesthesia, Perioperative and Pain Medicine, University of Calgary, South 
Health Campus, Calgary, Alberta, Canada.

INTRODUCTION: The use of artificial intelligence (AI) in the scientific process 
is advancing at a remarkable speed, thanks to continued innovations in large 
language models. While AI provides widespread benefits, including editing for 
fluency and clarity, it also has drawbacks, including fabricated content, 
perpetuation of bias, and lack of accountability. The editorial board of 
Regional Anesthesia & Pain Medicine (RAPM) therefore sought to develop best 
practices for AI usage and disclosure.
METHODS: A steering committee from the American Society of Regional Anesthesia 
and Pain Medicine used a modified Delphi process to address definitions, 
disclosure requirements, authorship standards, and editorial oversight for AI 
use in publishing. The committee reviewed existing publication guidelines and 
identified areas of ambiguity, which were translated into questions and 
distributed to an expert workgroup of authors, reviewers, editors, and AI 
researchers.
RESULTS: Two survey rounds, with 91% and 87% response rates, were followed by 
focused discussion and clarification to identify consensus recommendations. The 
workgroup achieved consensus on recommendations to authors about definitions of 
AI, required items to report, disclosure locations, authorship stipulations, and 
AI use during manuscript preparation. The workgroup formulated recommendations 
to reviewers about monitoring and evaluating the responsible use of AI in the 
review process, including the endorsement of AI-detection software, 
identification of concerns about undisclosed AI use, situations where AI use may 
necessitate the rejection of a manuscript, and use of checklists in the review 
process. Finally, there was consensus about AI-driven work, including required 
and optional disclosures and the use of checklists for AI-associated research.
DISCUSSION: Our modified Delphi study identified practical recommendations on AI 
use during the scientific writing and editorial process. The workgroup 
highlighted the need for transparency, human accountability, protection of 
patient confidentiality, editorial oversight, and the need for iterative 
updates. The proposed framework enables authors and editors to harness AI's 
efficiencies while maintaining the fundamental principles of responsible 
scientific communication and may serve as an example for other journals.

© American Society of Regional Anesthesia & Pain Medicine 2025. Re-use permitted 
under CC BY-NC. No commercial re-use. Published by BMJ Group.

DOI: 10.1136/rapm-2025-106852
PMCID: PMC12418700
PMID: 40897450

Conflict of interest statement: Competing interests: None declared.


814. J Med Internet Res. 2025 Jun 3;27:e75052. doi: 10.2196/75052.

Predicting 30-Day Postoperative Mortality and American Society of 
Anesthesiologists Physical Status Using Retrieval-Augmented Large Language 
Models: Development and Validation Study.

Chen YH(1), Ruan SJ(1), Chen PF(2)(3).

Author information:
(1)Department of Electronic and Computer Engineering, National Taiwan University 
of Science and Technology, Taipei, Taiwan.
(2)Department of Anesthesiology, Far Eastern Memorial Hospital, New Taipei City, 
Taiwan.
(3)Department of Electrical Engineering, Yuan Ze University, Taoyuan, Taiwan.

BACKGROUND: Accurately assessing perioperative risk is critical for informed 
surgical planning and patient safety. However, current prediction models often 
rely on structured data and overlook the nuanced clinical reasoning embedded in 
free-text preoperative notes. Recent advances in large language models (LLMs) 
have opened opportunities for harnessing unstructured clinical data, yet their 
application in perioperative prediction remains limited by concerns about 
factual accuracy. Retrieval-augmented generation (RAG) offers a promising 
solution-enhancing LLM performance by grounding outputs in domain-specific 
knowledge sources, potentially improving both predictive accuracy and clinical 
interpretability.
OBJECTIVE: This study aimed to investigate whether integrating LLMs with RAG can 
improve the prediction of 30-day postoperative mortality and American Society of 
Anesthesiologists (ASA) physical status classification using unstructured 
preoperative clinical notes.
METHODS: We conducted a retrospective cohort study using 24,491 medical records 
from a tertiary medical center, including preoperative anesthesia assessments, 
discharge summaries, and surgical information. To extract clinical insights from 
free-text data, we used the LLaMA 3.1-8B language model with RAG, using MedEmbed 
for text embedding and Miller's Anesthesia as the primary retrieval source. We 
evaluated model performance under various configurations, including embedding 
models, chunk sizes, and few-shot prompting. Machine learning (ML) models, 
including random forest, support vector machines (SVM), Extreme Gradient 
Boosting (XGBoost), and logistic regression, were trained on structured features 
as baselines.
RESULTS: A total of 520 (2.1%) patients experienced in-hospital 30-day 
postoperative mortality. The ASA physical status distribution was as follows: 
class I: 535 (2.2%); class II: 15,272 (62.4%); class III: 8024 (32.8%); class 
IV: 606 (2.5%); and class V: 54 (0.22%). For 30-day postoperative mortality 
prediction, the LLaMA‑RAG model achieved an F1-score of 0.4663 (95% CI 
0.4654-0.4672), versus 0.2369 (95% CI 0.2341-0.2397) without few‑shot prompting, 
0.0879 (95% CI 0.0717-0.1041) without RAG, and 0.0436 (95% CI 0.0292-0.0580) 
without either few‑shot prompting or RAG. Among ML models, XGBoost scored 0.4459 
(95% CI 0.4176-0.4742); random forest, 0.3953 (95% CI 0.3791-0.4115); logistic 
regression, 0.2720 (95% CI 0.2647-0.2793); and SVM, 0.2474 (95% CI 
0.2275-0.2673). For ASA classification, LLaMA‑RAG achieved a micro F1-score of 
0.8409 (95% CI 0.8238-0.8551) versus 0.6546 (95% CI 0.6430-0.6796) without 
few-shot prompting, 0.6340 (95% CI 0.6157-0.6535) without RAG, and 0.4238 (95% 
CI 0.3952-0.4490) without either few‑shot prompting or RAG. In comparison, 
XGBoost achieved 0.8273 (95% CI 0.8209-0.8498); logistic regression, 0.7940 (95% 
CI 0.7671-0.7950); random forest, 0.7847 (95% CI 0.7637-0.7868); and SVM, 0.7697 
(95% CI 0.7637-0.7697). Notably, the model demonstrated exceptional sensitivity 
in identifying rare but high-risk cases, such as ASA Class 5 patients and 
postoperative deaths.
CONCLUSIONS: The LLaMA-RAG model significantly improved the prediction of 
postoperative mortality and ASA classification, especially for rare high-risk 
cases. By grounding outputs in domain knowledge, retrieval-augmented generation 
enhanced both accuracy and prompt‑driven interpretability over ML and ablation 
models-highlighting its promise for real-world clinical decision support.

©Ying-Hao Chen, Shanq-Jang Ruan, Pei-fu Chen. Originally published in the 
Journal of Medical Internet Research (https://www.jmir.org), 03.06.2025.

DOI: 10.2196/75052
PMCID: PMC12174870
PMID: 40460423 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


815. Int J Med Inform. 2025 Dec 30;209:106247. doi: 10.1016/j.ijmedinf.2025.106247. 
Online ahead of print.

Leveraging large language models to automate the identification of healthcare 
access barriers for veterans.

Srinivasan S(1), Rizy C(2), Mahbub M(2), Bolme D(2), Peluso A(2), Trafton J(3), 
Danciu I(2).

Author information:
(1)Oak Ridge National Laboratory, Oak Ridge, TN, USA. Electronic address: 
srinivasans@ornl.gov.
(2)Oak Ridge National Laboratory, Oak Ridge, TN, USA.
(3)Program Evaluation and Resource Center, Office of Mental Health and Office of 
Suicide Prevention, Veterans Health Administration, Menlo Park, CA, USA.

OBJECTIVE: To develop and evaluate an automated system for identifying 
healthcare barriers focusing on transportation issues in veterans' clinical 
notes using large language models (LLMs) and to assess the impact of different 
prompting strategies on classification performance and explanation consistency.
METHODS: We developed a hybrid system combining pattern matching for templated 
notes with LLM analysis for free-text notes. Using 2000 manually annotated 
clinical notes, we compared four prompting strategies (dual-role short, 
dual-role long, analysis-first, analysis-only) across Mistral-7B and Llama-3.1 
models. We evaluated classification performance using standard metrics and 
assessed explanation consistency through embedding similarity analysis.
RESULTS: The analysis-first strategy achieved superior performance, with 
Mistral-7B reaching an F1 score of 0.914, outperforming traditional machine 
learning approaches (GBM: 0.786, BERT: 0.811). LLMs demonstrated higher 
explanation consistency within models (mean cosine similarity 0.887-0.908) 
compared to cross-model similarities (0.767-0.872). Pattern matching 
successfully handled 6.7% of templated notes deterministically. Mistral-7B 
showed greater internal consistency but higher abstention rates compared to 
Llama-3.1.
CONCLUSION: Requiring LLMs to analyze evidence before classification improves 
both accuracy and explanation consistency for identifying transportation 
barriers in clinical notes. This approach enables automated barrier detection at 
scale while providing clinically relevant explanations, supporting both 
population-level healthcare planning and individual patient care decisions.

Copyright © 2025. Published by Elsevier B.V.

DOI: 10.1016/j.ijmedinf.2025.106247
PMID: 41506175

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


816. Pediatr Nephrol. 2025 Sep;40(9):2879-2885. doi: 10.1007/s00467-025-06723-3. Epub 
2025 Mar 5.

Assessing the performance of large language models (GPT-3.5 and GPT-4) and 
accurate clinical information for pediatric nephrology.

Sav NM(1).

Author information:
(1)Department of Pediatric Nephrology, Duzce University, Duzce, Turkey. 
savmelike@gmail.com.

BACKGROUND: Artificial intelligence (AI) has emerged as a transformative tool in 
healthcare, offering significant advancements in providing accurate clinical 
information. However, the performance and applicability of AI models in 
specialized fields such as pediatric nephrology remain underexplored. This study 
is aimed at evaluating the ability of two AI-based language models, GPT-3.5 and 
GPT-4, to provide accurate and reliable clinical information in pediatric 
nephrology. The models were evaluated on four criteria: accuracy, scope, patient 
friendliness, and clinical applicability.
METHODS: Forty pediatric nephrology specialists with ≥ 5 years of experience 
rated GPT-3.5 and GPT-4 responses to 10 clinical questions using a 1-5 scale via 
Google Forms. Ethical approval was obtained, and informed consent was secured 
from all participants.
RESULTS: Both GPT-3.5 and GPT-4 demonstrated comparable performance across all 
criteria, with no statistically significant differences observed (p > 0.05). 
GPT-4 exhibited slightly higher mean scores in all parameters, but the 
differences were negligible (Cohen's d < 0.1 for all criteria). Reliability 
analysis revealed low internal consistency for both models (Cronbach's alpha 
ranged between 0.019 and 0.162). Correlation analysis indicated no significant 
relationship between participants' years of professional experience and their 
evaluations of GPT-3.5 (correlation coefficients ranged from - 0.026 to 0.074).
CONCLUSIONS: While GPT-3.5 and GPT-4 provided a foundational level of clinical 
information support, neither model exhibited superior performance in addressing 
the unique challenges of pediatric nephrology. The findings highlight the need 
for domain-specific training and integration of updated clinical guidelines to 
enhance the applicability and reliability of AI models in specialized fields. 
This study underscores the potential of AI in pediatric nephrology while 
emphasizing the importance of human oversight and the need for further 
refinements in AI applications.

© 2025. The Author(s).

DOI: 10.1007/s00467-025-06723-3
PMCID: PMC12296824
PMID: 40045013 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethical approval: All procedures 
performed in studies involving human participants were in accordance with the 
ethical standards of the institutional and national research committee at which 
the studies were conducted and with the 1964 Helsinki Declaration and its later 
amendments or comparable ethical standards. The study protocol was approved by 
the Institutional Ethics Committee of Duzce University School of Medicine 
(2024/218). Consent for publication: The author agrees with the publication of 
this manuscript in Pediatric Nephrology and was fully involved in the study and 
preparation of the manuscript.  Informed consent: Informed consent was obtained 
from all individual participants included in the study. Competing interests: The 
authors declare no competing interests.


817. Eur Arch Otorhinolaryngol. 2025 Nov 23. doi: 10.1007/s00405-025-09856-1. Online 
ahead of print.

Cautionary lessons from real-world testing of GPT-4.1 AI for pediatric foreign 
body aspiration.

Hack S(1), Attal R(2), Elazar D(3), Alon Y(4), Meyuchas R(5)(6), Livne A(7)(8), 
Madgar O(#)(9), Saban M(#)(9).

Author information:
(1)City St. George's University London School of Medicine, Program Delivered by 
University of Nicosia at the Chaim Sheba Medical Center, Ramat Gan, Israel. 
sholemhack1@gmail.com.
(2)City St. George's University London School of Medicine, Program Delivered by 
University of Nicosia at the Chaim Sheba Medical Center, Ramat Gan, Israel.
(3)Touro College of Osteopathic Medicine - Montana, MT, Great Falls, USA.
(4)Nursing Department, The Stanley Steyer School of Health Professions, Faculty 
of Medical and Health Sciences, Tel Aviv University, Tel Aviv, Ramat Aviv, 
69978, Israel.
(5)Department of Otolaryngology, Head and Neck Surgery, Sheba Medical Center, 
Tel Hashomer, Israel.
(6)Faculty of Medicine, Tel Aviv University, Tel Aviv, Israel.
(7)DeepVision Lab, Chaim Sheba Medical Center, Emek Haela St. 1, Ramat Gan, 
52621, Israel.
(8)Department of Diagnostic Imaging, Chaim Sheba Medical Center, Emek Haela St. 
1, Ramat Gan, 52621, Israel.
(9)Gray Faculty of Medical & Health Sciences, Tel-Aviv University, Tel-Aviv, 
Israel.
(#)Contributed equally

PURPOSE: To evaluate the feasibility and diagnostic performance of a multimodal 
large language model (GPT-4.1) in detecting pediatric airway foreign body 
aspiration (FBA) using real-world clinical and radiographic data.
METHODS: This retrospective cohort study included 58 pediatric patients 
evaluated for suspected airway FBA at a tertiary academic hospital between 2015 
and 2024. Each case combined structured clinical data and chest radiographs 
obtained at the time of emergency-department presentation, with bronchoscopy 
serving as the diagnostic reference standard. GPT-4.1, a vision-enabled large 
language model, classified cases as right-bronchus aspiration, left-bronchus 
aspiration, or no aspiration. Model performance was assessed using accuracy, 
precision, recall, and F1-score.
RESULTS: The model achieved an overall accuracy of 62.3%, with precision of 
23.3%, recall of 19.0%, and an F1-score of 0.21. While it correctly identified 
34 of 46 cases without aspiration, it detected only 4 of 12 confirmed 
bronchial-aspiration cases and missed all left-bronchus aspirations.
CONCLUSIONS: This proof-of-concept feasibility study highlights substantial 
limitations of a general-purpose multimodal AI model in pediatric airway triage. 
The low recall and high misclassification rates suggest that vision-enabled 
language models require task-specific training and rigorous validation before 
clinical implementation. Nevertheless, when used as an adjunct rather than a 
replacement for bronchoscopy, such models may eventually support triage 
decisions in resource-limited settings if further optimized and prospectively 
validated.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00405-025-09856-1
PMID: 41276667

Conflict of interest statement: Declarations. Ethical statement: This 
retrospective cohort study was conducted at a tertiary academic hospital and 
approved by the institutional review board (IRB protocol number 1761-24-SMC-D).


818. JMIR Form Res. 2025 Jun 5;9:e69115. doi: 10.2196/69115.

Personalized Support in Hereditary Breast and Ovarian Cancer After Genetic 
Counseling by the Chatbot-Based GENIE Mobile App: Proof-of-Concept Wizard of Oz 
Study.

Wolff D(1), Kupka T(1), Reichert C(1), Ammon N(2), Oeltze-Jafra S(1), Vajen 
B(2).

Author information:
(1)Peter L. Reichertz Institute for Medical Informatics of TU Braunschweig and 
Hannover Medical School, Hannover Medical School, Hanover, Germany.
(2)Department for Human Genetics, Hannover Medical School, Carl-Neuberg-Str. 1, 
Hanover, 30625, Germany, 49 51153280831.

BACKGROUND: The primary aim of genetic counseling at a human genetics center is 
to empower individuals at risk for hereditary diseases to make informed 
decisions regarding their health. In Germany, genetic counseling sessions 
typically last approximately 1 hour and provide highly personalized information 
by a specialist in human genetics. Despite this, many counselees report a need 
for additional support following the counseling session.
OBJECTIVE: This study introduces GENIE, a chatbot-based mobile app designed to 
assist individuals in the postcounseling phase, with a focus on hereditary 
breast and ovarian cancer. GENIE delivers expert-curated, personalized 
information tailored to the user's health and family circumstances. The content 
is presented through predefined dialogs between the user and the mobile 
assistant, aiming to extend the benefits of genetic counseling beyond the 
initial session.
METHODS: A Wizard of Oz study was conducted to evaluate a functional prototype 
of GENIE. A total of 6 patients with breast cancer, at least 2 years 
postdiagnosis, participated in the study. Participants were given access to the 
app for a minimum of 1 week. The evaluation was based on their interaction with 
GENIE, which was personalized using the details of a fictitious patient. Data 
collection included semistructured interviews and a 45-item questionnaire to 
assess usability and content quality.
RESULTS: The analysis of the interview and questionnaire data indicated high 
usability for GENIE, with a mean System Usability Score of 75.33 (SD 4.13). In 
total, 5 of the 6 participants used the app daily; 3 participants were willing 
to pay between US $5 and US $45 as a single purchase, while the other 3 
participants agreed that the app should be free for the user and the costs 
should be directly covered by health insurance. Still, opinions on the app's 
appeal were divided. The layout was seen as moderately professional, a bit 
crowded, and slightly uninspiring. Nevertheless, participants highlighted the 
credibility and relevance of the content, noting its alignment with the 
fictitious patient's scenario. However, areas for improvement were identified, 
particularly concerning the app's design. All participants would recommend the 
app to other affected persons.
CONCLUSIONS: The findings suggest that a mobile app like GENIE can provide 
valuable support to individuals in the postcounseling phase of genetic services. 
GENIE offers distinct advantages over large language models, as the information 
it provides is carefully curated by human experts, minimizing the risk of 
inaccuracies or hallucinations and significantly enhancing the system's 
credibility. This study highlights the need to involve the user group as early 
as possible in the development of a digital health app. Future work will focus 
on the implementation of a comprehensive personalization engine, redesign of the 
user interface, and the execution of a large-scale, 2-arm randomized 
intervention study to validate GENIE's effectiveness.

© Dominik Wolff, Thomas Kupka, Chiara Reichert, Nils Ammon, Steffen 
Oeltze-Jafra, Beate Vajen. Originally published in JMIR Formative Research 
(https://formative.jmir.org).

DOI: 10.2196/69115
PMCID: PMC12161161
PMID: 40471748 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


819. JMIR Form Res. 2025 Apr 2;9:e67782. doi: 10.2196/67782.

AI-Enhanced Virtual Reality Self-Talk for Psychological Counseling: Formative 
Qualitative Study.

Zisquit M(1), Shoa A(2), Oliva R(3), Perry S(1), Spanlang B(4), Brunstein Klomek 
A(1), Slater M(3), Friedman D(2).

Author information:
(1)Baruch Ivcher School of Psychology, Reichman University, Herzliya, Israel.
(2)Sammy Ofer School of Communications, Reichman University, Herzliya, Israel.
(3)Event Lab, University of Barcelona, Barcelona, Spain.
(4)Kiin S.L., Barcelona, Spain.

BACKGROUND: Access to mental health services continues to pose a global 
challenge, with current services often unable to meet the growing demand. This 
has sparked interest in conversational artificial intelligence (AI) agents as 
potential solutions. Despite this, the development of a reliable virtual 
therapist remains challenging, and the feasibility of AI fulfilling this 
sensitive role is still uncertain. One promising approach involves using AI 
agents for psychological self-talk, particularly within virtual reality (VR) 
environments. Self-talk in VR allows externalizing self-conversation by enabling 
individuals to embody avatars representing themselves as both patient and 
counselor, thus enhancing cognitive flexibility and problem-solving abilities. 
However, participants sometimes experience difficulties progressing in sessions, 
which is where AI could offer guidance and support.
OBJECTIVE: This formative study aims to assess the challenges and advantages of 
integrating an AI agent into self-talk in VR for psychological counseling, 
focusing on user experience and the potential role of AI in supporting 
self-reflection, problem-solving, and positive behavioral change.
METHODS: We carried out an iterative design and development of a system and 
protocol integrating large language models (LLMs) within VR self-talk during the 
first two and a half years. The design process addressed user interface, 
speech-to-text functionalities, fine-tuning the LLMs, and prompt engineering. 
Upon completion of the design process, we conducted a 3-month long exploratory 
qualitative study in which 11 healthy participants completed a session that 
included identifying a problem they wanted to address, attempting to address 
this problem using self-talk in VR, and then continuing self-talk in VR but this 
time with the assistance of an LLM-based virtual human. The sessions were 
carried out with a trained clinical psychologist and followed by semistructured 
interviews. We used applied thematic analysis after the interviews to code and 
develop key themes for the participants that addressed our research objective.
RESULTS: In total, 4 themes were identified regarding the quality of advice, the 
potential advantages of human-AI collaboration in self-help, the believability 
of the virtual human, and user preferences for avatars in the scenario. The 
participants rated their desire to engage in additional such sessions at 8.3 out 
of 10, and more than half of the respondents indicated that they preferred using 
VR self-talk with AI rather than without it. On average, the usefulness of the 
session was rated 6.9 (SD 0.54), and the degree to which it helped solve their 
problem was rated 6.1 (SD 1.58). Participants specifically noted that human-AI 
collaboration led to improved outcomes and facilitated more positive thought 
processes, thereby enhancing self-reflection and problem-solving abilities.
CONCLUSIONS: This exploratory study suggests that the VR self-talk paradigm can 
be enhanced by LLM-based agents and presents the ways to achieve this, potential 
pitfalls, and additional insights.

©‪Moreah Zisquit, Alon Shoa, Ramon Oliva, Stav Perry, Bernhard Spanlang, Anat 
Brunstein Klomek, Mel Slater, Doron Friedman. Originally published in JMIR 
Formative Research (https://formative.jmir.org), 02.04.2025.

DOI: 10.2196/67782
PMCID: PMC12004015
PMID: 40173447 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


820. J Med Internet Res. 2025 Sep 8;27:e68291. doi: 10.2196/68291.

Applications of Federated Large Language Model for Adverse Drug Reactions 
Prediction: Scoping Review.

Guo D(1), Choo KR(1).

Author information:
(1)Department of Information Systems and Cybersecurity, The University of Texas 
at San Antonio, 1 UTSA Circle, San Antonio, TX, 78249, United States, 1 (210) 
458-6300.

BACKGROUND: Adverse drug reactions (ADR) present significant challenges in 
health care, where early prevention is vital for effective treatment and patient 
safety. Traditional supervised learning methods struggle to address 
heterogeneous health care data due to their unstructured nature, regulatory 
constraints, and restricted access to sensitive personal identifiable 
information.
OBJECTIVE: This review aims to explore the potential of federated learning (FL) 
combined with natural language processing and large language models (LLMs) to 
enhance ADR prediction. FL enables decentralized training across client clusters 
with limited resources, while LLMs effectively process unstructured health care 
data. By aggregating client-trained models into a global model, FL ensures 
broader data inclusion while maintaining privacy.
METHODS: A scoping review was conducted on peer-reviewed publications retrieved 
from Google Scholar and Semantic Scholar between 2019 and 2024.
RESULTS: Following the PRISMA (Preferred Reporting Items for Systematic Reviews 
and Meta-Analyses) protocol, 145 articles from PubMed, arXiv, IEEE, and ACL 
Anthology met the inclusion criteria. Of these, 12 articles were selected for an 
in-depth review to examine use cases in ADR prediction. We synthesized ADR data 
sources on structured and unstructured data types, use cases of FL integrated 
with natural language processing, and open-source frameworks for ADR 
identifications and predictions. Special attention is given to unstructured ADR 
prediction using federated learning with large language models, including 
development and deployment strategies and evaluation metrics.
CONCLUSIONS: Given the recent emergence of LLM, the integration of FL and LLM 
for ADR prediction remains in its early stage, with limited documented use 
cases. This review explored the potential applications and highlighted the 
advancements of federated learning with large language models in health care 
research, particularly in ADR prediction. Key focus areas include fine-tuning 
and merging algorithms, fairness and unbiasedness, implementation challenges, 
and real-world deployment strategies. By synthesizing current insights, this 
review aims to lay the groundwork for future research in privacy-preserving and 
scalable ADR prediction systems.

©David Guo, Kim-Kwang Raymond Choo. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/68291
PMCID: PMC12516295
PMID: 40921101 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


821. Ewha Med J. 2025 Jul;48(3):e49. doi: 10.12771/emj.2025.00661. Epub 2025 Jul 31.

The TRIPOD-LLM reporting guideline for studies using large language models: a 
Korean translation.

[Article in English]

Gallifant J(1)(2)(3), Afshar M(4)(5), Ameen S(1)(6)(7)(5), Aphinyanaphongs 
Y(8)(5), Chen S(3)(9)(5), Cacciamani G(10)(11)(5), Demner-Fushman D(12)(5), 
Dligach D(13)(5), Daneshjou R(14)(15)(5), Fernandes C(1)(5), Hansen 
LH(1)(16)(5), Landman A(17)(5), Lehmann L(17)(5), McCoy LG(18)(5), Miller 
T(19)(5), Moreno A(20)(5), Munch N(1)(16)(5), Restrepo D(1)(21)(5), Savova 
G(19)(5), Umeton R(22)(5), Gichoya JW(23)(5), Collins GS(24)(25), Moons 
KGM(26)(27), Celi LA(1)(28)(29), Bitterman DS(3)(9).

Author information:
(1)Laboratory for Computational Physiology, Massachusetts Institute of 
Technology, Cambridge, MA, USA.
(2)Department of Critical Care, Guy's and St Thomas' NHS Foundation Trust, 
London, UK.
(3)Artificial Intelligence in Medicine (AIM) Program, Mass General Brigham, 
Harvard Medical School, Boston, MA, USA.
(4)Department of Medicine, University of Wisconsin-Madison, Madison, WI, USA.
(5)See Authors' contributions.
(6)Department of Biomedical Informatics, Harvard Medical School, Boston, MA, 
USA.
(7)Tasmanian School of Medicine, College of Health and Medicine, University of 
Tasmania, Hobart, Tasmania, Australia.
(8)Department of Population Health, NYU Grossman School of Medicine and Langone 
Health, New York, NY, USA.
(9)Department of Radiation Oncology, Brigham and Women's Hospital/Dana-Farber 
Cancer Institute, Boston, MA, USA.
(10)USC Institute of Urology and Catherine and Joseph Aresty Department of 
Urology, Keck School of Medicine, University of Southern California, Los 
Angeles, CA, USA.
(11)Artificial Intelligence Center, USC Institute of Urology, University of 
Southern California, Los Angeles, CA, USA.
(12)National Library of Medicine, NIH, HHS, Bethesda, MD, USA.
(13)Department of Computer Science, Loyola University, Chicago, IL, USA.
(14)Department of Dermatology, Stanford School of Medicine, Redwood City, CA, 
USA.
(15)Department of Biomedical Data Science, Stanford School of Medicine, Redwood 
City, CA, USA.
(16)Cognitive Science, Aarhus University, Jens Chr. Skou 2, Aarhus, Denmark.
(17)Mass General Brigham, Boston, MA, USA.
(18)Faculty of Medicine and Dentistry, University of Alberta, Edmonton, Alberta, 
Canada.
(19)Computational Health Informatics Program, Boston Children's Hospital, 
Harvard Medical School, Boston, MA, USA.
(20)Department of Radiation Oncology, The University of Texas MD Anderson Cancer 
Center, Houston, TX, USA.
(21)Departamento de Telematica, Universidad del Cauca, Popayan, Colombia.
(22)Dana-Farber Cancer Institute, Boston, MA, USA.
(23)Department of Radiology, Emory University School of Medicine, Atlanta, GA, 
USA.
(24)Centre for Statistics in Medicine, Nuffield Department of Orthopaedics, 
Rheumatology, and Musculoskeletal Sciences, University of Oxford, Oxford, UK.
(25)UK EQUATOR Centre, Nuffield Department of Orthopaedics, Rheumatology, and 
Musculoskeletal Sciences, University of Oxford, Oxford, UK.
(26)Julius Center for Health Sciences and Primary Care, UMC Utrecht, Utrecht 
University, Utrecht, the Netherlands.
(27)Health Innovation Netherlands (HINL), Utrecht, the Netherlands.
(28)Division of Pulmonary, Critical Care and Sleep Medicine, Beth Israel 
Deaconess Medical Center, Boston, MA, USA.
(29)Department of Biostatistics, Harvard T.H. Chan School of Public Health, 
Boston, MA, USA.

DOI: 10.12771/emj.2025.00661
PMCID: PMC12362290
PMID: 40739974

Conflict of interest statement: Conflict of interest DSB is an associate editor 
at Radiation Oncology and HemOnc.org, receives research funding from the 
American Association for Cancer Research, and provides advisory and consulting 
services for Mercurial AI. DDF is an associate editor at the Journal of the 
American Medical Informatics Association, is a member of the editorial board of 
Scientific Data, and receives funding from the intramural research program at 
the US National Library of Medicine, NIH. JWG is a member of the editorial board 
of Radiology: Artificial Intelligence, BJR Artificial Intelligence, and NEJM AI. 
All other authors declare no potential conflict of interest relevant to this 
article.


822. Front Oncol. 2025 Dec 11;15:1695468. doi: 10.3389/fonc.2025.1695468. eCollection 
2025.

Benchmarking GPT-5 in radiation oncology: measurable gains, but persistent need 
for expert oversight.

Dinc US(1)(2)(3), Sarkar J(1)(2)(3), Schubert P(1)(2)(3), Semrau S(1)(2)(3), 
Weissmann T(1)(2)(3), Karius A(1)(2)(3), Brand J(1)(2)(3), Axer BN(1)(2)(3), 
Gomaa A(1)(2)(3), Stephan P(1)(2)(3), Sheth I(1)(2)(3), Beirami S(1)(2)(3), 
Schwarz A(1)(2)(3), Gaipl U(1)(2)(3), Frey B(1)(2)(3), Bert C(1)(2)(3), 
Corradini S(3)(4), Fietkau R(1)(2)(3), Putz F(1)(2)(3).

Author information:
(1)Department of Radiation Oncology, University Hospital Erlangen, 
Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany.
(2)Comprehensive Cancer Center Erlangen-Europäische Metropolregion Nürnberg 
(European Metropolitan Region Nuremberg) (CCC ER-EMN), Erlangen, Germany.
(3)Bavarian Cancer Research Center (BZKF), Munich, Germany.
(4)Department of Radiation Oncology, University Hospital, Ludwig Maximilian 
University of Munich, Munich, Germany.

Erratum in
    Front Oncol. 2026 Jan 06;15:1766619. doi: 10.3389/fonc.2025.1766619.

INTRODUCTION: Large language models (LLM) have shown great potential in clinical 
decision support and medical education. GPT-5 is a novel LLM system that has 
been specifically marketed towards oncology use. This study comprehensively 
benchmarks GPT-5 for the field of radiation oncology.
METHODS: Performance was assessed using two complementary benchmarks: (i) the 
American College of Radiology Radiation Oncology In-Training Examination (TXIT, 
2021), comprising 300 multiple-choice items, and (ii) a curated set of 60 
authentic radiation oncologic vignettes representing diverse disease sites and 
treatment indications. For the vignette evaluation, GPT-5 was instructed to 
generate structured therapeutic plans and concise two-line summaries. Four 
board-certified radiation oncologists independently rated outputs for 
correctness, comprehensiveness, and hallucinations. Inter-rater reliability was 
quantified using Fleiss' κ. GPT-5-14 results were compared to published GPT-3.5 
and GPT-4 baselines.
RESULTS: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%, 
outperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were most 
pronounced in dose specification and diagnosis. In the vignette evaluation, 
GPT-5's treatment recommendations were rated highly for correctness (mean 
3.24/4, 95% CI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69). 
Hallucinations were rare, flagged in 10.0% of all individual reviewer 
assessments (24 of 240), and no patient case reached majority consensus for 
their presence. Inter-rater agreement was low (Fleiss' κ 0.083 for correctness), 
reflecting inherent variability in clinical judgment. Errors clustered in 
complex scenarios requiring precise trial knowledge or detailed clinical 
adaptation.
DISCUSSION: GPT-5 clearly outperformed prior model variants on the radiation 
oncology multiple-choice benchmark. Although GPT-5 exhibited favorable 
performance in generating real-world radiation oncology treatment 
recommendations, correctness ratings indicate room for further improvement. 
While hallucinations were infrequent, the presence of substantive errors 
underscores that GPT-5-generated recommendations require rigorous expert 
oversight before clinical implementation. In addition, considerable inter-rater 
variability highlights the challenge of achieving consistent expert evaluation.

Copyright © 2025 Dinc, Sarkar, Schubert, Semrau, Weissmann, Karius, Brand, Axer, 
Gomaa, Stephan, Sheth, Beirami, Schwarz, Gaipl, Frey, Bert, Corradini, Fietkau 
and Putz.

DOI: 10.3389/fonc.2025.1695468
PMCID: PMC12738326
PMID: 41458620

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest. The author(s) declared that 
they were an editorial board member of Frontiers, at the time of submission. 
This had no impact on the peer review process and the final decision.


823. J Palliat Med. 2025 Dec;28(12):1586-1594. doi: 10.1177/10966218251363802. Epub 
2025 Aug 6.

Using Large Language Models to Analyze Symptom Discussions and Recommendations 
in Clinical Encounters.

Fenton ATHR(1), Charewycz N(1), Kanwal Z(1), Durieux BN(1), Pollack KI(2), 
Tulsky JA(1), Wright AA(1), Lindvall CJ(1).

Author information:
(1)Department of Medical Oncology, Dana-Farber Cancer Institute, Boston, 
Massachusetts, USA.
(2)Department of Family Medicine and Community Health, Duke University School of 
Medicine, Durham, North Carolina, USA.

Background: Patient-provider interactions could inform care quality and 
communication but are rarely leveraged because collecting and analyzing them is 
both time-consuming and methodologically complex. The growing availability of 
large language models (LLMs) makes these analyses more feasible, though their 
accuracy remains uncertain. Objectives: Assess an LLM's ability to analyze 
patient-provider interactions. Design: Compare a human's and an LLM's codings of 
clinical encounter transcripts. Setting/Subjects: Two hundred and thirty-six 
potential symptom discussions from transcripts of clinical encounters with 92 
patients living with cancer in the mid-Atlantic United States. Transcripts were 
analyzed by GPT4DFCI in our hospital's Health Insurance Portability and 
Accountability Act compliant infrastructure instance of GPT-4 (OpenAI). 
Measurements: Human and an LLM-coded transcripts to determine whether a 
patient's reported symptom(s) were discussed, who initiated the discussion, and 
any resulting recommendation. We calculated Cohen's κ to assess interrater 
agreement between the LLM and human and qualitatively classified disagreements 
about recommendations. Results: Interrater reliability indicated "strong" and 
"moderate" agreement levels across measures: Agreement was strongest for whether 
the symptom was discussed (k = 0.89), followed by who initiated the discussion 
(k = 0.82), and the recommendation provided (k = 0.78). The human and LLM 
disagreed on the presence and/or content of the recommendation in 16% of 
potential discussions, which we categorized into nine types of disagreements. 
Conclusions: Our results suggest that LLMs' abilities to analyze clinical 
encounters are equivalent to humans. Thus, using LLMs as a research tool may 
make it more feasible to analyze patient-provider interactions, which could have 
broader implications for assessing and improving care quality, care inequities, 
and provider communication.

DOI: 10.1177/10966218251363802
PMID: 40765368 [Indexed for MEDLINE]


824. J Med Internet Res. 2025 Dec 5;27:e73535. doi: 10.2196/73535.

Using Generative AI to Co-Design Digital Mental Health Interventions With 
Adolescents in Rural South Africa: Qualitative Thematic Analysis of 
Participatory Workshops.

Dallison S(1), Moffett B(2), Makhubela P(2), Nkuna T(2), Pozuelo JR(2)(3)(4), 
van Heerden A(5), O'Mahen H(6).

Author information:
(1)Department of Psychology, University of Bath, 10 West, Claverton Down, Bath, 
Somerset, BA2 7AY, United Kingdom, 4401392661000.
(2)MRC/Wits Rural Public Health and Health Transitions Research Unit 
(Agincourt), School of Public Health, Faculty of Health Sciences, University of 
the Witwatersrand, Johannesburg, South Africa.
(3)Department of Global Health and Social Medicine, Harvard Medical School, 
Harvard University, Boston, MA, United States.
(4)Department of Psychiatry, University of Oxford, Oxford, United Kingdom.
(5)Faculty of Health Sciences, University of the Witwatersrand, Johannesburg, 
South Africa.
(6)Mood Disorders Centre, Department of Psychology, University of Exeter, 
Exeter, United Kingdom.

BACKGROUND: Digital mental health interventions (DMHIs) offer a scalable 
approach to address adolescent depression and anxiety. User-centered 
coproduction can optimize acceptability and engagement, but it is often 
resource-intensive. Advances in generative artificial intelligence (GenAI) 
create new opportunities for involving adolescents in co-design, yet research on 
its feasibility and acceptability, particularly in low-resource settings, 
remains underexplored.
OBJECTIVE: This study aimed to explore adolescents' experiences and perspectives 
of using GenAI to co-design stories, images, and music for the Kuamsha app (Sea 
Monster), a gamified DMHI that teaches behavioral activation through interactive 
narratives and peer support.
METHODS: Overall, 2 participatory workshops and focus group discussions were 
conducted with 23 adolescents (aged 15-19 years) in rural Mpumalanga, South 
Africa. Participants were guided to use 3 GenAI tools-ChatGPT (OpenAI), 
text-to-story; MidJourney (MidJourney Inc), text-to-image; and Soundful 
(Soundful Inc), music generation-to create digital content. Data were 
audio-recorded, translated, transcribed, and triangulated with the facilitator's 
observation notes. Thematic analysis was used to explore key themes.
RESULTS: Almost all participants (22/23, 96%) had no prior exposure to GenAI. 
The majority (20/23, 87%) described the creative process as enjoyable and 
engaging, with most (21/23, 91%) reporting that creating music improved their 
mood. Adolescents expressed autonomy and ownership of the process, with more 
than half (14/23, 61%) personalizing outputs to reflect their identities and 
aspirations. All participants (23/23, 100%) preferred artificial intelligence 
(AI)-generated images over the cartoon-like illustrations of the Kuamsha app, 
and most (19/23, 83%) preferred AI-generated music. Story preferences were more 
mixed, with about a quarter of participants (6/23, 26%) recalling that Kuamsha's 
narratives contained embedded lessons that were not integrated into the ChatGPT 
outputs. Most adolescents (18/23, 78%) required support with prompt 
construction, and more than half (13/23, 57%) noted cultural biases in AI 
outputs, particularly in images. Most participants (17/23, 74%) expressed 
interest in using AI for schoolwork and creative projects, while a minority 
(6/23, 26%) preferred to limit use to personal applications. Concerns about 
fairness and the displacement of human creativity were also raised.
CONCLUSIONS: GenAI shows promise for enhancing adolescent engagement in the 
coproduction of DMHIs and enabling culturally relevant and personalized content. 
However, reliance on human support and persistent algorithmic biases remain 
limitations. Further research should explore the integration of therapeutic 
principles into AI-generated media and strategies to mitigate bias.

© Sophie Dallison, Bianca Moffett, Princess Makhubela, Tamera Nkuna, Julia R 
Pozuelo, Alastair van Heerden, Heather O'Mahen. Originally published in the 
Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/73535
PMCID: PMC12680127
PMID: 41348954 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


825. J Med Internet Res. 2025 Dec 5;27:e78132. doi: 10.2196/78132.

Detecting Sociodemographic Biases in the Content and Quality of Large Language 
Model-Generated Nursing Care: Cross-Sectional Simulation Study.

Bai N(#)(1), Yu Y(#)(1), Luo C(#)(1), Zhou SC(1), Wang Q(1), Zou H(1), Liu Q(1), 
Fu G(2), Zhai W(3), Zhao Q(3), Li J(3), Wei X(1), Yang BX(1)(4)(5).

Author information:
(1)Center for Wise Information Technology of Mental Health Nursing Research, 
School of Nursing, Wuhan University, No. 115, Donghu Road, Wuchang District, 
Wuhan, Hubei, China, +86 15902731922.
(2)Sorbonne Université, Institut du Cerveau - Paris Brain Institute - ICM, CNRS, 
Inria, Inserm, AP-HP, Hôpital de la Pitié Salpêtrière, Paris, France.
(3)College of Computer Science, Beijing University of Technology, Beijing, 
China.
(4)Department of Psychiatry, Renmin Hospital of Wuhan University, Wuhan, China.
(5)Research Center for Lifespan Health, Wuhan University, Wuhan, China.
(#)Contributed equally

BACKGROUND: Large language models (LLMs) are increasingly applied in health 
care. However, concerns remain that their nursing care recommendations may 
reflect patients' sociodemographic attributes rather than clinical needs. While 
this risk is acknowledged, there is a lack of empirical evidence evaluating 
sociodemographic bias in LLM-generated nursing care plans.
OBJECTIVE: To investigate potential biases in nursing care plans generated by 
LLMs, we focused on whether outputs differ systematically based on patients' 
sociodemographic characteristics and assessed the implications for equitable 
nursing care.
METHODS: We used a mixed methods simulation study. A standardized clinical 
vignette experiment was used to prompt GPT-4 to generate 9600 nursing care plans 
for 96 patient profiles with varying sociodemographic characteristics (eg, sex, 
age, income, education, and residence). We first conducted a quantitative 
analysis of all plans, assessing variations in thematic content. Subsequently, a 
panel of senior nursing experts evaluated the clinical quality (eg, safety, 
applicability, and completeness) of a stratified subsample of 500 plans.
RESULTS: We analyzed 9600 LLM-generated nursing care plans and identified 8 
consistent themes. Communication and Education (99.98%) and Emotional Support 
(99.97%) were nearly universal, while Nurse Training and Event Analysis were 
least frequent (39.3%). Multivariable analyses revealed systematic 
sociodemographic disparities. Care plans generated for low-income patient 
profiles were less likely to include the theme Environmental Adjustment 
(adjusted relative risk [aRR] 0.90). Profiles with lower education were 
associated with an increased likelihood of including Family Support (aRR 1.10). 
Similarly, plans generated for older patient profiles were more likely to 
contain recommendations for Pain Management (aRR 1.33) and Family Support (aRR 
1.62) but were less likely to mention Nurse Training (aRR 0.78). Sex and 
regional differences were also significant. Expert review of 500 plans showed 
high overall quality (mean 4.47), with strong interrater reliability 
(κ=0.76-0.81). However, urban profiles had higher completeness (β=.22) and 
applicability (β=.14) but lower safety scores (β=-0.09). These findings 
demonstrate that LLM-generated care plans exhibit systematic sociodemographic 
bias, raising important implications for fairness and safe deployment in nursing 
practice.
CONCLUSIONS: This study identified that LLMs systematically reproduce 
sociodemographic biases in the generation of nursing care plans. These biases 
appear in two forms: they shape the thematic content and influence expert-rated 
clinical quality. These findings reveal a substantial risk that such models may 
reinforce existing health inequities. To our knowledge, this is the first 
empirical evidence documenting these nuanced biases in nursing. The study also 
contributes a replicable framework for evaluating LLM-generated care plans. 
Finally, it underscores the critical need for robust human oversight to ensure 
that artificial intelligence serves as a tool for advancing equity rather than 
perpetuating disparities.

© Nan Bai, Yijing Yu, Chunyan Luo, Si Chen Zhou, Qing Wang, Huijing Zou, Qian 
Liu, Guanghui Fu, Wei Zhai, Qing Zhao, Jianqiang Li, Xinni Wei, Bing Xiang Yang. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/78132
PMCID: PMC12683325
PMID: 41355748 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


826. J Med Internet Res. 2026 Jan 19;28:e83188. doi: 10.2196/83188.

Developing a Quality Evaluation Index System for Health Conversational 
Artificial Intelligence: Mixed Methods Study.

Liao W(1), Li M(2), Ma C(#)(1), Han Y(#)(1), Wang D(2), Liu H(1), Wang Y(1), 
Feng Z(1), Wang H(2), Guan Y(2).

Author information:
(1)School of Public Health, Capital Medical University, Beijing, China.
(2)Bytedance Xiaohe Health, Hainan, China.
(#)Contributed equally

BACKGROUND: Effective communication is fundamental to health care; however, 
demographic transitions and a widening global health workforce gap are 
intensifying the imbalance between service demand and resource supply. Health 
conversational artificial intelligence (HCAI) based on large language models 
offers a potential pathway to improve the accessibility and personalization of 
care. Nevertheless, the lack of a rigorous, user-centered evaluation framework 
limits the systematic assessment of HCAI quality, raising concerns regarding 
safety, reliability, and clinical applicability.
OBJECTIVE: This study aims to establish a scientific and systematic quality 
evaluation index system for HCAI, providing both a theoretical foundation and a 
practical tool for the assessment and optimization of HCAI.
METHODS: Based on a literature review, industry standards, and expert group 
discussions, a preliminary framework for the index system was established. Two 
rounds of Delphi expert consultations were then conducted to collect expert 
opinions. The analytic hierarchy process (AHP) was applied to assign weights to 
indicators at each level, and the final content and structure of the index 
system were determined.
RESULTS: Both rounds of expert consultation achieved a 100% response rate. The 
authority coefficient of the experts was 0.84 in both rounds. Kendall W 
coefficient ranged from 0.14 to 0.20 in the first round and from 0.13 to 0.17 in 
the second round, with all values showing statistical significance (round one: 
importance P＜.001, feasibility P＜.001, sensitivity P＜.001; round two: importance 
P=.001, feasibility P＜.001, sensitivity P=.001). The final HCAI quality 
evaluation index system consisted of 3 primary indicators, 7 secondary 
indicators, and 28 tertiary indicators. According to AHP weight calculations, 
the primary indicators were ranked in descending order as follows: ethics and 
compliance (0.4781), health consultation capability (0.4112), and user 
experience (0.1107).
CONCLUSIONS: The evaluation index system constructed in this study demonstrates 
scientific validity and practical relevance. It provides a valuable reference 
for the quality assessment, model optimization, and regulatory oversight of HCAI 
systems.

©Weizhen Liao, Meng Li, Chengyu Ma, Youli Han, Dan Wang, Haopeng Liu, Yi Wang, 
Zijie Feng, Huichao Wang, Yiru Guan. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org), 19.01.2026.

DOI: 10.2196/83188
PMID: 41554116 [Indexed for MEDLINE]


827. Radiology. 2025 Sep;316(3):e250568. doi: 10.1148/radiol.250568.

Clinical Validation of a Generative Artificial Intelligence Model for Chest 
Radiograph Reporting: A Multicohort Study.

Hwang EJ(1), Lee JH(1), Lim WH(1), Jeong WG(2), Hong W(3), Park J(4), Yoo SJ(5), 
Kim H(1)(6).

Author information:
(1)Department of Radiology, Seoul National University Hospital and Seoul 
National University College of Medicine, 101 Daehak-ro, Jongno-gu, Seoul 03080, 
Republic of Korea.
(2)Department of Radiology, Chonnam National University Hwasun Hospital and 
Chonnam National University Medical School, Hwasun, Republic of Korea.
(3)Department of Radiology, Hallym University Sacred Heart Hospital, Anyang, 
Republic of Korea.
(4)Department of Radiology, Yeungnam University Medical Center and Yeungnam 
University College of Medicine, Daegu, Republic of Korea.
(5)Department of Radiology, Hanyang University Medical Center and Hanyang 
University College of Medicine, Seoul, Republic of Korea.
(6)Soombit.ai, Seongnam, Republic of Korea.

Comment in
    Radiology. 2025 Oct;317(1):e252955. doi: 10.1148/radiol.252955.

Background Artificial intelligence (AI)-generated radiology reports have become 
available and require rigorous evaluation. Purpose To evaluate the clinical 
acceptability of chest radiograph reports generated by an AI algorithm and their 
accuracy in identifying referable abnormalities. Materials and Methods Chest 
radiographs from an intensive care unit (ICU), an emergency department, and 
health checkups were retrospectively collected between January 2020 and December 
2022, and outpatient chest radiographs were sourced from a public dataset. An 
automated report-generating AI algorithm was then applied. A panel of seven 
thoracic radiologists evaluated the acceptability of generated reports, and 
acceptability was analyzed using a standard criterion (acceptable without 
revision or with minor revision) and a stringent criterion (acceptable without 
revision). Using chest radiographs from three of the contexts (excluding the 
ICU), AI-generated and radiologist-written reports were compared regarding the 
acceptability of the reports (generalized linear mixed model) and their 
sensitivity and specificity for identifying referable abnormalities (McNemar 
test). The radiologist panel was surveyed to evaluate their perspectives on the 
potential of AI-generated reports to replace radiologist-written reports. 
Results The chest radiographs of 1539 individuals (median age, 55 years; 656 
male patients, 483 female patients, 400 patients of unknown sex) were included. 
There was no evidence of a difference in acceptability between AI-generated and 
radiologist-written reports under the standard criterion (88.4% vs 89.2%; P = 
.36), but AI-generated reports were less acceptable than radiologist-written 
reports under the stringent criterion (66.8% vs 75.7%; P < .001). Compared with 
radiologist-written reports, AI-generated reports identified radiographs with 
referable abnormalities with greater sensitivity (81.2% vs 59.4%; P < .001) and 
lower specificity (81.0% vs 93.6%; P < .001). In the survey, most radiologists 
indicated that AI-generated reports were not yet reliable enough to replace 
radiologist-written reports. Conclusion AI-generated chest radiograph reports 
had similar acceptability to radiologist-written reports, although a substantial 
proportion of AI-generated reports required minor revision. © RSNA, 2025 
Supplemental material is available for this article. See also the editorial by 
Wu and Seo in this issue.

DOI: 10.1148/radiol.250568
PMID: 40985833 [Indexed for MEDLINE]


828. BMC Med Inform Decis Mak. 2025 Aug 19;25(1):312. doi: 
10.1186/s12911-025-03151-z.

Can LLMs effectively assist medical coding? Evaluating GPT performance on DRG 
and targeted clinical tasks.

Feng Y(1).

Author information:
(1)Amplify Health Asia, 21 Collyer Quay, Singapore, 049320, Singapore. 
yeli.feng@amplifyhealth.com.

BACKGROUND: Large language Models (LLM) have demonstrated near human-level 
performance in medical domain, from summarizing clinical notes and passing 
medical licensing examinations, to predictive tasks such as disease diagnoses 
and treatment recommendations. However, currently there is little research on 
their efficacy for medical coding, a pivotal component in health informatics, 
clinical trials, and reimbursement management. This study proposes a prompt 
framework and investigates its effectiveness for medical coding.
METHODS: First, a prompt-based learning framework is proposed. This framework 
aims to improve the performance of GPT for complex medical coding tasks by 
augmenting it with specific prior knowledge, utilizing techniques such as meta 
prompt, many-shot learning, and dynamic in-context learning. Then its 
effectiveness is evaluated with three different medical coding tasks. Finally, 
ablation studies are presented to validate and analyze the contribution of each 
module in the proposed prompt framework.
RESULTS: For the MIMIC-IV dataset, the prediction accuracies of top-1 and top-5 
for the 30 most frequent MS-DRG codes are 68.1% and 90.0%, respectively. The 
clinical trial criteria coding task results in a macro-F1 score of 68.4 on the 
CHIP-CTC test dataset in Chinese. Both results are comparable to the best 
methods in comparison that training deep leaning models or fine-tuning LLMs.
CONCLUSIONS: This study demonstrates that for targeted coding tasks, DRG and 
other simplified scenarios, off-the-shelf GPT models guided by carefully 
designed prompts can achieve performance comparable to state-of-the-art methods. 
While current GPT models serve as helpful assistants to human coding 
specialists,they are not yet equipped to fully replace expert judgment. 
Importantly, the recent and drastic decline of the cost of deploying large 
language models makes their integration into medical coding workflows 
increasingly feasible. As these models continue to evolve, their potential to 
provide reliable support for DRG coding task is likely to grow rapidly over 
time.
TRIAL REGISTRATION: Not applicable.

DOI: 10.1186/s12911-025-03151-z
PMCID: PMC12362839
PMID: 40830470

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
of interests: The authors declare that they have no competing interests.


829. J Am Med Inform Assoc. 2025 Dec 1;32(12):1877-1887. doi: 10.1093/jamia/ocaf149.

A scalable framework for benchmark embedding models in semantic health-care 
tasks.

Soffer S(1)(2), Omar M(3)(4), Gendler M(5), Glicksberg BS(3)(4), Kovatch P(6), 
Efros O(2)(7), Freeman R(3), Charney AW(3), Nadkarni GN(3)(4), Klang E(3)(4).

Author information:
(1)Institute of Hematology, Davidoff Cancer Center, Rabin Medical Center, Petah 
Tikva, 49100, Israel.
(2)Gray Faculty of Medical and Health Sciences, Tel Aviv University, Tel Aviv, 
6997801, Israel.
(3)Windreich Department of AI and Human Health, Icahn School of Medicine at 
Mount Sinai, New York, NY 10029, United States.
(4)Department of Medicine, Icahn School of Medicine at Mount Sinai, The Hasso 
Plattner Institute for Digital Health at Mount Sinai, New York, NY 10019, United 
States.
(5)Azrieli Faculty of Medicine, Bar-Ilan University, Safed, 1311502, Israel.
(6)Department of Genetics and Genomic Sciences, Icahn School of Medicine at 
Mount Sinai, New York, NY 10029, United States.
(7)National Hemophilia Center and Thrombosis Institute, Sheba Medical Center, 
Ramat Gan, 5262100, Israel.

OBJECTIVES: Text embeddings are promising for semantic tasks, such as retrieval 
augmented generation (RAG). However, their application in health care is 
underexplored due to a lack of benchmarking methods. We introduce a scalable 
benchmarking method to test embeddings for health-care semantic tasks.
MATERIALS AND METHODS: We evaluated 39 embedding models across 7 medical 
semantic similarity tasks using diverse datasets. These datasets comprised 
real-world patient data (from the Mount Sinai Health System and MIMIC IV), 
biomedical texts from PubMed, and synthetic data generated with Llama-3-70b. We 
first assessed semantic textual similarity (STS) by correlating the 
model-generated similarity scores with noise levels using Spearman rank 
correlation. We then reframed the same tasks as retrieval problems, evaluated by 
mean reciprocal rank and recall at k.
RESULTS: In total, evaluating 2000 text pairs per 7 tasks for STS and retrieval 
yielded 3.28 million model assessments. Larger models (>7b parameters), such as 
those based on Mistral-7b and Gemma-2-9b, consistently performed well, 
especially in long-context tasks. The NV-Embed-v1 model (7b parameters), 
although top in short tasks, underperformed in long tasks. For short tasks, 
smaller models such as b1ade-embed (335M parameters) performed on-par to the 
larger models. For long retrieval tasks, the larger models significantly 
outperformed the smaller ones.
DISCUSSION: The proposed benchmarking framework demonstrates scalability and 
flexibility, offering a structured approach to guide the selection of embedding 
models for a wide range of health-care tasks.
CONCLUSION: By matching the appropriate model with the task, the framework 
enables more effective deployment of embedding models, enhancing critical 
applications such as semantic search and retrieval-augmented generation (RAG).

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf149
PMCID: PMC12646376
PMID: 40977370 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
declare.


830. J Med Internet Res. 2025 Sep 8;27:e76849. doi: 10.2196/76849.

Effectiveness, Usability, and Acceptability of ChatGPT With Retrieval-Augmented 
Generation (SIV-ChatGPT) in Increasing Seasonal Influenza Vaccination Uptake 
Among Older Adults: Quasi-Experimental Study.

Wang Z(1), Tsang TH(2), Yu FY(1), Fang Y(3), Chen S(1), Sun F(3), Mo PKH(1), 
Wong KC(2).

Author information:
(1)Centre for Health Behaviours Research, JC School of Public Health and Primary 
Care, The Chinese University of Hong Kong, Hong Kong, China (Hong Kong).
(2)School of Governance and Policy Science, The Chinese University of Hong Kong, 
Hong Kong, China (Hong Kong).
(3)Department of Health and Physical Education, The Education University of Hong 
Kong, Hong Kong, China (Hong Kong).

BACKGROUND: Older adults are more vulnerable to severe consequences caused by 
seasonal influenza. Although seasonal influenza vaccination (SIV) is effective 
and free vaccines are available, the SIV uptake rate remained inadequate among 
people aged 65 years or older in Hong Kong, China. There was a lack of studies 
evaluating ChatGPT in promoting vaccination uptake among older adults.
OBJECTIVE: This study aimed to evaluate the effectiveness of ChatGPT with 
retrieval-augmented generation in increasing SIV uptake among older adults over 
a 3-month study period in Hong Kong, China. Participants in an ongoing 
observational cohort study conducted in the same period served as the comparison 
group.
METHODS: A quasi-experimental study was conducted between November 2024 and 
April 2025. Participants were (1) aged ≥65 years, (2) possessed a Hong Kong ID, 
(3) able to speak and comprehend Cantonese, (4) smartphone users, and (5) had no 
SIV uptake for the approaching flu season. Those with a diagnosis of cognitive 
impairment or dementia, blindness or deafness, or known contraindications to the 
SIV were excluded. Participants were recruited through random telephone calls. 
There were 45 and 55 participants in the SIV-ChatGPT group and the comparison 
group, respectively. All participants completed follow-up surveys at T1 (1 month 
after the baseline survey, for the SIV-ChatGPT group only) and T2 (3 months 
after the baseline survey, for both groups). Participants in the SIV-ChatGPT 
group gained access to SIV-ChatGPT in the format of a web-based app after 
completion of the baseline survey. They could use SIV-ChatGPT repeatedly 
throughout a 1-month intervention period and were free to spend as much time as 
they wanted with SIV-ChatGPT. Intention-to-treat analysis was used for outcome 
analyses.
RESULTS: At T2, the SIV uptake rate was higher in the SIV-ChatGPT group than the 
comparison group (15/45, 33% vs 8/55, 14.3%; adjusted odds ratio 2.72, 95% CI 
1.01-7.35, P=.048). All participants were able to provide receipts to validate 
their SIV uptake. In the SIV-ChatGPT group, 40.5% (15/37) of participants who 
used SIV-ChatGPT at least once reported a SIV uptake at T2, which was 
significantly higher than nonusers (0/8, 0%; P=.04). Among the 37 SIV-ChatGPT 
users, the mean score of the System Usability Scale was 67.1 (SD 14.9). Levels 
of subjective behavioral and cognitive engagement with SIV-ChatGPT were 
relatively high, while the affective engagement was moderate.
CONCLUSIONS: SIV-ChatGPT was feasible and acceptable and demonstrated 
preliminary effectiveness in increasing SIV uptake among people aged 65 years or 
older. This study also provided implications to improve the performance of 
SIV-ChatGPT. A full-powered randomized controlled trial should be considered to 
evaluate its efficacy.
TRIAL REGISTRATION: ClinicalTrials.gov NCT06679647; 
https://clinicaltrials.gov/study/NCT06679647.

©Zixin Wang, Tsz Hin Tsang, Fuk-yuen Yu, Yuan Fang, Siyu Chen, Fenghua Sun, 
Phoenix K H Mo, Kwong-Cheong Wong. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org), 08.09.2025.

DOI: 10.2196/76849
PMCID: PMC12455163
PMID: 40921067 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


831. Med Phys. 2025 Oct;52(10):e70034. doi: 10.1002/mp.70034.

Enhancing auto-contouring with large language model in high-dose rate 
brachytherapy for cervical cancers.

Wang J(1), Zhang J(1), Yang K(1), Ghavidel BB(2), Khajetash B(3), Sarikhani 
A(3), Houshyari M(4), Liu T(1), Lei Y(1), Tavakoli M(2).

Author information:
(1)Department of Radiation Oncology, Icahn School of Medicine at Mount Sinai, 
New York, New York, USA.
(2)Department of Radiation Oncology and Winship Cancer Institute, Emory 
University, Atlanta, Georgia, USA.
(3)Department of Medical Physics, School of Medicine, Iran University of Medical 
Sciences, Tehran, IRAN.
(4)Department of Radiation Oncology, Shahid Beheshti University of Medical 
Sciences, Tehran, IRAN.

BACKGROUND: High-dose-rate brachytherapy (HDR-BT) is a cornerstone of cervical 
cancer (CC) treatment, requiring the precise delineation of high-risk clinical 
target volumes (HR-CTV) and organs at risk (OARs) for effective dose delivery 
and toxicity reduction. However, the time-sensitive nature of HDR-BT planning 
and its reliance on expert contouring introduce inter- and intra-observer 
variability, posing challenges for consistent and accurate treatment planning.
PURPOSE: This study proposes a novel deep learning (DL)-based auto-segmentation 
framework, guided by task-specific prompts generated from large language models 
(LLMs), to address these challenges and improve segmentation accuracy and 
efficiency.
METHODS: A retrospective dataset of 32 CC patients, encompassing 124 planning 
computed tomography (pCT) images, was utilized. The framework integrates 
clinical guidelines for organ contouring from the American Brachytherapy Society 
(ABS), the European Society for Radiotherapy and Oncology (ESTRO), and the 
International Commission on Radiation Units and Measurements (ICRU). LLMs, 
particularly Chat-GPT, extracts domain knowledge from these contouring 
guidelines to generate task-specific prompts, which guide a Swin 
transformer-based encoder and a fully convolutional network (FCN) decoder for 
segmentation. The DL pipeline was evaluated on HR-CTV and OARs, including the 
bladder, rectum, and sigmoid. Metrics such as Dice similarity coefficient (DSC), 
Hausdorff distance (HD95%), mean surface distance (MSD), and center-of-mass 
distance (CMD) were used for performance assessment. An ablation study compared 
the prompt-guided approach with a baseline model without prompt guidance. 
Statistical differences were tested with two-tailed paired t-tests, and p-values 
were adjusted using the Benjamini-Hochberg method to address the multiple 
comparisons correction and results with adjusted p < 0.05 were deemed 
significant. Cohen's d values were calculated to quantify effect sizes.
RESULTS: The proposed framework achieved the highest segmentation for the 
bladder (DSC of 0.91 ± 0.07), followed by the HR-CTV (DSC of 0.80 ± 0.08) and 
the rectum (DSC of 0.78 ± 0.07), and a lower accuracy for sigmoid (DSC of 
0.63 ± 0.15) due to its small size and irregular shape. Boundary precision was 
highest for the HR-CTV (HD95%: 6.32 ± 2.31 mm). The ablation study confirmed the 
contribution of prompt guidance, with statistically significant improvements in 
DSC and/or HD95% (p < 0.05) for all OARs. Prompt guidance, however, did not 
improve the accuracy of HR-CTV delineation.
CONCLUSIONS: This study demonstrates the feasibility and effectiveness of 
integrating LLM-generated task-specific prompts with DL-based segmentation for 
HDR-BT in CC. The proposed framework enhances segmentation consistency to 
support accurate treatment planning, addressing critical challenges in HDR-BT 
workflows.

© 2025 American Association of Physicists in Medicine.

DOI: 10.1002/mp.70034
PMID: 40985602 [Indexed for MEDLINE]


832. Cornea. 2025 Dec 9. doi: 10.1097/ICO.0000000000004049. Online ahead of print.

To Evaluate the Efficacy of Zero-Shot Prompting Using Large Language Models in 
the Extraction of Microbial Keratitis Descriptors.

Aruljyothi L(1), Mungle T(2), Woodward MA(3), Prajna V(4), Nuijts RMMA(5), 
Berendschot TTJM(5), Nallasamy N(3)(6).

Author information:
(1)Department of Cornea and Refractive Services, Aravind Eye Hospital, Salem, 
Tamil Nadu, India.
(2)Department of Medicine, Stanford University, Stanford, CA.
(3)Department of Ophthalmology and Visual Sciences, Kellogg Eye Center, 
University of Michigan, Ann Arbor, MI.
(4)Department of Cornea and Refractive Services, Aravind Eye Hospital, Madurai, 
Tamil Nadu, India.
(5)University Eye Clinic Maastricht, Maastricht University, Maastricht, The 
Netherlands; and.
(6)Department of Computational Medicine and Bioinformatics, University of 
Michigan, Ann Arbor, MI.

PURPOSE: To extract microbial keratitis (MK) descriptors from clinician notes in 
electronic health records using Large Language Models (LLMs) with a zero-shot 
prompting approach and compare the descriptors with those identified by expert 
human annotators.
METHODS: Two hundred fifteen patients with culture-proven MK seen between 2019 
and 2023 at Aravind Eye Hospital, Salem, India was gathered. Free-text clinical 
notes from each patient's first encounter corneal examination were obtained. 
Each of the 3 MK descriptors-centrality, infiltrate depth, and thinning-was 
annotated by expert consensus and coded as 1 (present), 0 (absent), or 9 
(details unavailable). GPT-4o and GPT-4o mini were prompted to extract the 3 MK 
descriptors. LLM responses were compared with human annotations using agreement 
measures such as Cohen Kappa scores at 95% confidence interval, in addition to 
sensitivity and specificity.
RESULTS: GPT-4o demonstrated mean sensitivity of 92%, 86%, and 97%, for 
centrality, depth, and thinning, respectively, and 96%, 93%, and 99% mean 
specificity. Cohen Kappa scores calculated for centrality, depth, and thinning 
were 0.88, 0.73, and 0.88, respectively, indicating good agreement. The overall 
sensitivity, specificity, and Cohen Kappa of GPT-4o mini were lower compared 
with GPT-4o, but the difference was not statistically significant.
CONCLUSIONS: Both GPT-4o and GPT-4o mini showed good agreement with human 
annotations in extracting MK descriptors. Detection of MK descriptors was 
influenced by limitations in the quality and consistency of electronic health 
record documentation. The trade-off between efficiency and performance among LLM 
architectures remains a consideration for implementation of LLM for large-scale 
MK data analysis.

Copyright © 2025 Wolters Kluwer Health, Inc. All rights reserved.

DOI: 10.1097/ICO.0000000000004049
PMID: 41379872

Conflict of interest statement: The authors have no conflicts of interest to 
disclose.


833. World J Gastroenterol. 2026 Jan 7;32(1):111428. doi: 10.3748/wjg.v32.i1.111428.

Artificial intelligence and machine learning-driven advancements in 
gastrointestinal cancer: Paving the way for precision medicine.

Suri C(1), Ratre YK(2), Pande B(3), Bhaskar L(4), Verma HK(5).

Author information:
(1)Department of Oncology, University of Alberta, Edmonton T6G 2R3, Alberta, 
Canada.
(2)Department of Biotechnology, Guru Ghasidas Vishwavidyalaya, Bilaspur 495001, 
Chhattisgarh, India.
(3)School of Studies in Life Science, Pt. Ravishankar Shukla University, Raipur 
492010, Chhattisgarh, India.
(4)Department of Zoology, Guru Ghasidas Vishwavidyalaya, Bilaspur 495001, 
Chhattisgarh, India.
(5)Department of Bioscience and Biomedical Engineering Indian Institute of 
Technology, Bhilai 491002, Chhattisgarh, India. henu.verma@yahoo.com.

Gastrointestinal (GI) cancers remain a leading cause of cancer-related morbidity 
and mortality worldwide. Artificial intelligence (AI), particularly machine 
learning and deep learning (DL), has shown promise in enhancing cancer 
detection, diagnosis, and prognostication. A narrative review of literature 
published from January 2015 to march 2025 was conducted using PubMed, Web of 
Science, and Scopus. Search terms included "gastrointestinal cancer", 
"artificial intelligence", "machine learning", "deep learning", "radiomics", 
"multimodal detection" and "predictive modeling". Studies were included if they 
focused on clinically relevant AI applications in GI oncology. AI algorithms for 
GI cancer detection have achieved high performance across imaging modalities, 
with endoscopic DL systems reporting accuracies of 85%-97% for polyp detection 
and segmentation. Radiomics-based models have predicted molecular biomarkers 
such as programmed cell death ligand 2 expression with area under the curves up 
to 0.92. Large language models applied to radiology reports demonstrated 
diagnostic accuracy comparable to junior radiologists (78.9% vs 80.0%), though 
without incremental value when combined with human interpretation. Multimodal AI 
approaches integrating imaging, pathology, and clinical data show emerging 
potential for precision oncology. AI in GI oncology has reached clinically 
relevant accuracy levels in multiple diagnostic tasks, with multimodal 
approaches and predictive biomarker modeling offering new opportunities for 
personalized care. However, broader validation, integration into clinical 
workflows, and attention to ethical, legal, and social implications remain 
critical for widespread adoption.

©The Author(s) 2026. Published by Baishideng Publishing Group Inc. All rights 
reserved.

DOI: 10.3748/wjg.v32.i1.111428
PMCID: PMC12809174
PMID: 41551523 [Indexed for MEDLINE]

Conflict of interest statement: Conflict-of-interest statement: The authors 
declare that they have no conflict of interest.


834. J Med Internet Res. 2025 May 12;27:e65397. doi: 10.2196/65397.

Classifying the Information Needs of Survivors of Domestic Violence in Online 
Health Communities Using Large Language Models: Prediction Model Development and 
Evaluation Study.

Guan S(#)(1), Hui V(#)(2)(3), Stiglic G(4), Constantino RE(3), Lee YJ(3), Wong 
AKC(5).

Author information:
(1)Department of Electrical and Electronic Engineering, The Hong Kong 
Polytechnic University, Hung Hom, China (Hong Kong).
(2)Centre for Smart Health, School of Nursing, The Hong Kong Polytechnic 
University, Hung Hom, China (Hong Kong).
(3)Department of Health and Community Systems, School of Nursing, University of 
Pittsburgh, Pittsburgh, PA, United States.
(4)Faculty of Health Sciences, University of Maribor, Maribor, Slovenia.
(5)School of Nursing, The Hong Kong Polytechnic University, Hung Hom, China 
(Hong Kong).
(#)Contributed equally

BACKGROUND: Domestic violence (DV) is a significant public health concern 
affecting the physical and mental well-being of numerous women, imposing a 
substantial health care burden. However, women facing DV often encounter 
barriers to seeking in-person help due to stigma, shame, and embarrassment. As a 
result, many survivors of DV turn to online health communities as a safe and 
anonymous space to share their experiences and seek support. Understanding the 
information needs of survivors of DV in online health communities through 
multiclass classification is crucial for providing timely and appropriate 
support.
OBJECTIVE: The objective was to develop a fine-tuned large language model (LLM) 
that can provide fast and accurate predictions of the information needs of 
survivors of DV from their online posts, enabling health care professionals to 
offer timely and personalized assistance.
METHODS: We collected 294 posts from Reddit subcommunities focused on DV shared 
by women aged ≥18 years who self-identified as experiencing intimate partner 
violence. We identified 8 types of information needs: shelters/DV 
centers/agencies; legal; childbearing; police; DV report 
procedure/documentation; safety planning; DV knowledge; and communication. Data 
augmentation was applied using GPT-3.5 to expand our dataset to 2216 samples by 
generating 1922 additional posts that imitated the existing data. We adopted a 
progressive training strategy to fine-tune GPT-3.5 for multiclass text 
classification using 2032 posts. We trained the model on 1 class at a time, 
monitoring performance closely. When suboptimal results were observed, we 
generated additional samples of the misclassified ones to give them more 
attention. We reserved 184 posts for internal testing and 74 for external 
validation. Model performance was evaluated using accuracy, recall, precision, 
and F1-score, along with CIs for each metric.
RESULTS: Using 40 real posts and 144 artificial intelligence-generated posts as 
the test dataset, our model achieved an F1-score of 70.49% (95% CI 
60.63%-80.35%) for real posts, outperforming the original GPT-3.5 and GPT-4, 
fine-tuned Llama 2-7B and Llama 3-8B, and long short-term memory. On artificial 
intelligence-generated posts, our model attained an F1-score of 84.58% (95% CI 
80.38%-88.78%), surpassing all baselines. When tested on an external validation 
dataset (n=74), the model achieved an F1-score of 59.67% (95% CI 51.86%-67.49%), 
outperforming other models. Statistical analysis revealed that our model 
significantly outperformed the others in F1-score (P=.047 for real posts; P<.001 
for external validation posts). Furthermore, our model was faster, taking 19.108 
seconds for predictions versus 1150 seconds for manual assessment.
CONCLUSIONS: Our fine-tuned LLM can accurately and efficiently extract and 
identify DV-related information needs through multiclass classification from 
online posts. In addition, we used LLM-based data augmentation techniques to 
overcome the limitations of a relatively small and imbalanced dataset. By 
generating timely and accurate predictions, we can empower health care 
professionals to provide rapid and suitable assistance to survivors of DV.

©Shaowei Guan, Vivian Hui, Gregor Stiglic, Rose Eva Constantino, Young Ji Lee, 
Arkers Kwan Ching Wong. Originally published in the Journal of Medical Internet 
Research (https://www.jmir.org), 12.05.2025.

DOI: 10.2196/65397
PMCID: PMC12107195
PMID: 40354642 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


835. AJR Am J Roentgenol. 2025 Apr;224(4):e2432341. doi: 10.2214/AJR.24.32341. Epub 
2025 Jan 29.

Use of ChatGPT Large Language Models to Extract Details of Recommendations for 
Additional Imaging From Free-Text Impressions of Radiology Reports.

Li KW(1), Lacson R(1), Guenette JP(1), DiPiro PJ(1), Burk KS(1), Kapoor N(1), 
Salah F(1), Khorasani R(1).

Author information:
(1)Center for Evidence-Based Imaging, Department of Radiology, Brigham and 
Women's Hospital, Harvard Medical School, 1620 Tremont St, Boston, MA 02120.

Comment in
    AJR Am J Roentgenol. 2025 Apr;224(4):e2532748. doi: 10.2214/AJR.25.32748.
    AJR Am J Roentgenol. 2025 Apr;224(4):e2532734. doi: 10.2214/AJR.25.32734.

BACKGROUND. Automated extraction of actionable details of recommendations for 
additional imaging (RAIs) from radiology reports could facilitate tracking and 
timely completion of clinically necessary RAIs and thereby potentially reduce 
diagnostic delays. OBJECTIVE. The purpose of the study was to assess the 
performance of large language models (LLMs) in extracting actionable details of 
RAIs from radiology reports. METHODS. This retrospective single-center study 
evaluated reports of diagnostic radiology examinations performed across 
modalities and care settings within five subspecialties (abdominal imaging, 
musculoskeletal imaging, neuroradiology, nuclear medicine, thoracic imaging) in 
August 2023. Of reports identified by a previously validated natural language 
processing algorithm to contain an RAI, 250 were randomly selected; 231 of these 
reports were confirmed to contain an RAI on manual review and formed the study 
sample. Twenty-five reports were used to engineer a prompt instructing an LLM, 
when inputted in a report impression containing an RAI, to extract details about 
the modality, body part, time frame, and rationale of the RAI; the remaining 206 
reports were used for testing the prompt in combination with GPT-3.5 and GPT-4. 
A 4th-year medical student and radiologist from the relevant subspecialty 
independently classified the LLM outputs as correct versus incorrect for 
extracting the four actionable details of RAIs in comparison with the report 
impressions; a third reviewer assisted in resolving discrepancies. Extraction 
accuracy was summarized and compared between LLMs using consensus assessments. 
RESULTS. For GPT-3.5 and GPT-4, the two reviewers agreed about classification of 
LLM output as correct versus incorrect with respect to report impressions for 
95.6% and 94.2% for RAI modality, 89.3% and 88.3% for RAI body part, 96.1% and 
95.1% for RAI time frame, and 89.8% and 88.8% for RAI rationale, respectively. 
GPT-4 was more accurate than GPT-3.5 in extracting RAI modality (94.2% [194/206] 
vs 85.4% [176/206], p < .001), RAI body part (86.9% [179/206] vs 77.2% 
[159/206], p = .004), and RAI time frame (99.0% [204/206] vs 95.6% [197/206], p 
= .02). Both LLMs had accuracy of 91.7% (189/206) for extracting RAI rationale. 
CONCLUSION. LLMs were used to extract actionable details of RAIs from free-text 
impression sections of radiology reports; GPT-4 outperformed GPT-3.5. CLINICAL 
IMPACT. The technique could represent an innovative method to facilitate timely 
completion of clinically necessary radiologist recommendations.

DOI: 10.2214/AJR.24.32341
PMID: 39878409 [Indexed for MEDLINE]


836. Knee Surg Sports Traumatol Arthrosc. 2026 Jan;34(1):370-377. doi: 
10.1002/ksa.70145. Epub 2025 Oct 27.

Is orthopaedics entering the age of generative AI?-A narrative review of current 
applications challenges and future directions.

Oettl FC(1), Pruneski JA(2), Zsidai B(3)(4)(5), Yu Y(6), Cong T(7), Tischer 
T(8)(9), Hirschmann MT(10)(11), Samuelsson K(3)(4)(12).

Author information:
(1)Department of Orthopedic Surgery, Balgrist University Hospital, University of 
Zürich, Zurich, Switzerland.
(2)Department of Orthopaedic Surgery, Tripler Army Medical Center, Honolulu, 
Hawaii, USA.
(3)Sahlgrenska Sports Medicine Center, Gothenburg, Sweden.
(4)Department of Orthopaedics, Institute of Clinical Sciences, Sahlgrenska 
Academy, University of Gothenburg, Gothenburg, Sweden.
(5)Department of Orthopedics, Skåne University Hospital, Malmö/Lund, Sweden.
(6)Department of Computer Science and Engineering, Chalmers University of 
Technology, Gothenburg, Sweden.
(7)Department of Orthopaedic Surgery, University of Pittsburgh, Pittsburgh, 
Pennsylvania, USA.
(8)Department of Orthopaedic Surgery, University Medicine Rostock, Rostock, 
Germany.
(9)Department of Orthopaedic and Trauma Surgery, Malteser Waldkrankenhaus 
Erlangen, Erlangen, Germany.
(10)Department of Orthopaedic Surgery and Traumatology, Kantonsspital Baselland, 
Bruderholz, Switzerland.
(11)University of Basel, Basel, Switzerland.
(12)Department for Orthopaedics, Sahlgrenska University Hospital, Mölndal, 
Sweden.

Artificial intelligence (AI) in medicine is undergoing a pivotal transformation, 
evolving from discriminative models that classify data to generative AI systems 
capable of creating novel content. Generative AI is a type of artificial 
intelligence that can learn from and mimic large amounts of data to create 
content such as text, images, music, videos, code, and more. The generative AI 
paradigm relies on advanced architectures, including large language models 
(LLMs), which are likely to redefine key processes in the practice of clinical 
medicine. The imaging- and procedure-heavy specialty of orthopaedic surgery is 
uniquely positioned to benefit from innovations in spatial reasoning, 
biomechanical analysis, and procedural planning using generative AI. Key 
applications are rapidly emerging, like streamlining clinical workflows through 
automated documentation, the mediation of patient-provider communication and 
enhanced interpretability of complex medical information. While an exciting 
field the current evidence base is quite limited. The continued integration of 
these technologies promises to enhance surgical precision, democratise access to 
advanced planning, and ultimately improve patient outcomes. However, realising 
this potential requires overcoming significant challenges related to the 'black 
box' nature of models, data bias, and evolving regulatory oversight. Rigorous 
clinical validation through prospective trials will be essential to ensure the 
safe, effective, and equitable implementation of generative AI in the future of 
orthopaedic care. LEVEL OF EVIDENCE: Level V.

© 2025 The Author(s). Knee Surgery, Sports Traumatology, Arthroscopy published 
by John Wiley & Sons Ltd on behalf of European Society of Sports Traumatology, 
Knee Surgery and Arthroscopy.

DOI: 10.1002/ksa.70145
PMCID: PMC12747601
PMID: 41144723 [Indexed for MEDLINE]

Conflict of interest statement: Kristian Samuelsson is a member of the Board of 
Directors of Getinge AB (publ) and medtech advisor to Carl Bennet AB. Ting Cong 
is a founder and board member of Sustain Surgical Inc. and Kondral Technology 
Inc. with no conflict to this work.


837. J Med Internet Res. 2025 Jan 30;27:e54601. doi: 10.2196/54601.

Using Large Language Models to Detect and Understand Drug Discontinuation Events 
in Web-Based Forums: Development and Validation Study.

Trevena W(1), Zhong X(1), Alvarado M(1), Semenov A(1), Oktay A(2), Devlin D(3), 
Gohil AY(1), Chittimouju SH(1).

Author information:
(1)Department of Industrial and Systems Engineering, The University of Florida, 
GAINESVILLE, FL, United States.
(2)Department of Industrial and Systems Engineering, The University of San 
Diego, San Diego, CA, United States.
(3)Microsoft, Seattle, WA, United States.

BACKGROUND: The implementation of large language models (LLMs), such as BART 
(Bidirectional and Auto-Regressive Transformers) and GPT-4, has revolutionized 
the extraction of insights from unstructured text. These advancements have 
expanded into health care, allowing analysis of social media for public health 
insights. However, the detection of drug discontinuation events (DDEs) remains 
underexplored. Identifying DDEs is crucial for understanding medication 
adherence and patient outcomes.
OBJECTIVE: The aim of this study is to provide a flexible framework for 
investigating various clinical research questions in data-sparse environments. 
We provide an example of the utility of this framework by identifying DDEs and 
their root causes in an open-source web-based forum, MedHelp, and by releasing 
the first open-source DDE datasets to aid further research in this domain.
METHODS: We used several LLMs, including GPT-4 Turbo, GPT-4o, DeBERTa 
(Decoding-Enhanced Bidirectional Encoder Representations from Transformer with 
Disentangled Attention), and BART, among others, to detect and determine the 
root causes of DDEs in user comments posted on MedHelp. Our study design 
included the use of zero-shot classification, which allows these models to make 
predictions without task-specific training. We split user comments into 
sentences and applied different classification strategies to assess the 
performance of these models in identifying DDEs and their root causes.
RESULTS: Among the selected models, GPT-4o performed the best at determining the 
root causes of DDEs, predicting only 12.9% of root causes incorrectly (hamming 
loss). Among the open-source models tested, BART demonstrated the best 
performance in detecting DDEs, achieving an F1-score of 0.86, a false positive 
rate of 2.8%, and a false negative rate of 6.5%, all without any fine-tuning. 
The dataset included 10.7% (107/1000) DDEs, emphasizing the models' robustness 
in an imbalanced data context.
CONCLUSIONS: This study demonstrated the effectiveness of open- and 
closed-source LLMs, such as GPT-4o and BART, for detecting DDEs and their root 
causes from publicly accessible data through zero-shot classification. The 
robust and scalable framework we propose can aid researchers in addressing 
data-sparse clinical research questions. The launch of open-access DDE datasets 
has the potential to stimulate further research and novel discoveries in this 
field.

©William Trevena, Xiang Zhong, Michelle Alvarado, Alexander Semenov, Alp Oktay, 
Devin Devlin, Aarya Yogesh Gohil, Sai Harsha Chittimouju. Originally published 
in the Journal of Medical Internet Research (https://www.jmir.org), 30.01.2025.

DOI: 10.2196/54601
PMCID: PMC11826943
PMID: 39883487 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


838. Front Psychiatry. 2025 Feb 4;16:1494355. doi: 10.3389/fpsyt.2025.1494355. 
eCollection 2025.

Unleashing the potential of chatbots in mental health: bibliometric analysis.

Han Q(1), Zhao C(2).

Author information:
(1)Department of Information, Zhejiang Chinese Medical University, Hangzhou, 
China.
(2)Department of Humanities, Zhejiang Chinese Medical University, Hangzhou, 
China.

BACKGROUND: The proliferation of chatbots in the digital mental health sector is 
gaining momentum, offering a promising solution to address the pressing shortage 
of mental health professionals. By providing accessible and convenient mental 
health services and support, chatbots are poised to become a primary 
technological intervention in bridging the gap between mental health needs and 
available resources.
OBJECTIVE: This study undertakes a thorough bibliometric analysis and discourse 
on the applications of chatbots in mental health, with the objective of 
elucidating the underlying scientific patterns that emerge at the intersection 
of chatbot technology and mental health care on a global scale.
METHODS: The bibliometric software Biblioshiny and VOSviewer were used to 
conduct a comprehensive analysis of 261 scientific articles published in the Web 
of Science Core Collection between 2015 and 2024. Publications distribution are 
analyzed to measure productivity on countries, institutions, and sources. 
Scientific collaboration networks are generated to analyze the influence as well 
as communications between countries and institutions. Research topics and trends 
are formulated by using a keyword co-occurrence network.
RESULTS: Over the last decade, researches on utilization of chatbots in mental 
health has appeared to be increasing steadily at an annual rate of 46.19%. The 
United States have made significant contributions to the development and 
expansion of publications, accounting for 27.97% of the total research output 
with 2452 citation counts. England came second to the US in terms of 
publications and citations, and followed by Australia, China, and France. 
National Center for Scientific Research in France ranked first among all 
institutions, followed by Imperial College London and University of Zurich. The 
number of articles published in Journal of Medical Internet Research was 
exceptionally high, accounting for 12.26% of the total number of articles, and 
JMIR Mental Health is the most influential publication sources in terms of 
average citations per article. Collaboration among universities in the USA, 
United Kingdom, Switzerland, and Singapore demonstrated a high level. The 
keyword co-occurrence network highlights the prominent techniques in this 
multidisciplinary area and reveals 5 research topics, showing a significant 
overlap between clusters. High-frequency terms such as "ChatGPT", "machine 
learning", and "large language models" underscore the current state of research, 
highlighting the cutting-edge advancements and frontiers in this field.
CONCLUSIONS: This study provides an in-depth analysis of the most prominent 
countries, institutions, publications, collaboration status, and research topics 
associated with utilization of chatbots in mental health over the last decade. 
It offers insights to mental health professionals without an AI background and 
individuals interested in the development of mental health chatbots. The 
findings suggest that chatbots hold a significant role in promoting mental 
health well-being and exhibit considerable potential in demonstrating empathy, 
curiosity, understanding, and collaborative capabilities with users.

Copyright © 2025 Han and Zhao.

DOI: 10.3389/fpsyt.2025.1494355
PMCID: PMC11832554
PMID: 39967582

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


839. Br J Ophthalmol. 2025 Oct 23;109(11):1272-1278. doi: 10.1136/bjo-2024-325861.

Enhancing diabetic retinopathy query responses: assessing large language model 
in ophthalmology.

Wu H(1), Su Z(1), Pan X(1), Shao A(1), Xu Y(1), Wang Y(1), Jin K(2), Ye J(2).

Author information:
(1)Eye Center, Zhejiang University School of Medicine Second Affiliated 
Hospital, Hangzhou, Zhejiang, China.
(2)Eye Center, Zhejiang University School of Medicine Second Affiliated 
Hospital, Hangzhou, Zhejiang, China yejuan@zju.edu.cn jinkai@zju.edu.cn.

BACKGROUND: Diabetic retinopathy (DR) is a leading cause of blindness, with an 
increasing reliance on large language models (LLMs) for health-related 
information. The specificity of LLM-generated responses to DR queries is yet to 
be established, prompting an investigation into their suitability for 
ophthalmological contexts.
METHODS: A cross-sectional study involving six LLMs was conducted to ascertain 
the accuracy and comprehensiveness of responses to 42 DR-related questions from 
1 February 2024 to 31 March 2024. Three consultant-level ophthalmologists 
independently assessed the responses, grading them on accuracy and 
comprehensiveness. Additionally, the self-correction capability and readability 
of the responses were analysed statistically.
RESULTS: An analysis of 252 responses from six LLMs showed an average word count 
ranging from 155.3 to 304.3 and an average character count ranging from 975.3 to 
2043.5. The readability scores showed significant variability, with ChatGPT-3.5 
displaying the lowest readability level. The accuracy of the responses was high, 
with ChatGPT-4.0 receiving 97.6% good ratings and no 'poor' grades for the top 
three models. After introducing a self-correction prompt, the average accuracy 
score demonstrated a significant improvement, increasing from 6.4 to 7.5.
CONCLUSION: LLMs have the potential to provide accurate and comprehensive 
responses to DR-related questions, making them advantageous for ophthalmology 
applications. However, before clinical integration, further refinement is needed 
to address readability, and continuous validation assessments are imperative to 
ensure reliability.

© Author(s) (or their employer(s)) 2025. Re-use permitted under CC BY-NC. No 
commercial re-use. See rights and permissions. Published by BMJ Group.

DOI: 10.1136/bjo-2024-325861
PMCID: PMC12573325
PMID: 40588331 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: None declared.


840. J Am Med Inform Assoc. 2025 Oct 22:ocaf179. doi: 10.1093/jamia/ocaf179. Online 
ahead of print.

AcuKG: a comprehensive knowledge graph for medical acupuncture.

Li Y(1)(2), Peng X(3), Peng S(4), Li J(5), Pei D(6), Zhang Q(7), Lu Y(4), Hu 
Y(8), Li F(5), Zhou L(1)(2), He Y(9), Tao C(5), Xu H(3), Hong N(3).

Author information:
(1)Department of Medicine, Harvard Medical School, Boston, MA 02115, United 
States.
(2)Division of General Internal Medicine and Primary Care, Department of 
Medicine, Brigham and Women's Hospital, Boston, MA 02115, United States.
(3)Department of Biomedical Informatics and Data Science, School of Medicine, 
Yale University, New Haven, CT 06510, United States.
(4)Institute of Information on Traditional Chinese Medicine, China Academy of 
Chinese Medical Sciences, Beijing 100010, China.
(5)Department of Artificial Intelligence and Informatics, Mayo Clinic, 
Jacksonville, FL 32224, United States.
(6)The University of Texas MD Anderson Cancer Center, Houston, TX 77030, United 
States.
(7)National Science Library, Chinese Academy of Sciences, Beijing 100190, China.
(8)McWilliams School of Biomedical Informatics, The University of Texas Health 
Science Center at Houston, Houston, TX 77030, United States.
(9)Unit for Laboratory Animal Medicine, Center for Computational Medicine and 
Bioinformatics, Department of Learning Health Science, University of Michigan 
Medical School, Ann Arbor, MI 48109, United States.

BACKGROUND: Acupuncture, a key modality in traditional Chinese medicine, is 
gaining global recognition as a complementary therapy and a subject of 
increasing scientific interest. However, fragmented and unstructured acupuncture 
knowledge spread across diverse sources poses challenges for semantic retrieval, 
reasoning, and in-depth analysis. To address this gap, we developed AcuKG, a 
comprehensive knowledge graph that systematically organizes acupuncture-related 
knowledge to support sharing, discovery, and artificial intelligence-driven 
innovation in the field.
METHODS: AcuKG integrates data from multiple sources, including online 
resources, guidelines, PubMed literature, ClinicalTrials.gov, and multiple 
ontologies (SNOMED CT, UBERON, and MeSH). We employed entity recognition, 
relation extraction, and ontology mapping to establish AcuKG, with 
human-in-the-loop to ensure data quality. Two cases evaluated AcuKG's usability: 
(1) how AcuKG advances acupuncture research for obesity and (2) how AcuKG 
enhances large language model (LLM) application on acupuncture 
question-answering.
RESULTS: AcuKG comprises 1839 entities and 11 527 relations, mapped to 1836 
standard concepts in 3 ontologies. Two use cases demonstrated AcuKG's 
effectiveness and potential in advancing acupuncture research and supporting LLM 
applications. In the obesity use case, AcuKG identified highly relevant 
acupoints (eg, ST25, ST36) and uncovered novel research insights based on 
evidence from clinical trials and literature. When applied to LLMs in answering 
acupuncture-related questions, integrating AcuKG with GPT-4o and LLaMA 3 
significantly improved accuracy (GPT-4o: 46% → 54%, P = .03; LLaMA 3: 17% → 28%, 
P = .01).
CONCLUSION: AcuKG is an open dataset that provides a structured and 
computational framework for acupuncture applications, bridging traditional 
practices with acupuncture research and cutting-edge LLM technologies.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf179
PMID: 41124298


841. J Med Internet Res. 2025 Mar 27;27:e65537. doi: 10.2196/65537.

Large Language Model-Driven Knowledge Graph Construction in Sepsis Care Using 
Multicenter Clinical Databases: Development and Usability Study.

Yang H(#)(1)(2)(3), Li J(#)(4), Zhang C(1), Sierra AP(5), Shen B(6).

Author information:
(1)Department of Critical Care Medicine, Joint Laboratory of Artificial 
Intelligence for Critical Care Medicine, Frontiers Science Center for 
Disease-related Molecular Network, Institutes for Systems Genetics, Sichuan 
University, West China Hospital, Chengdu, China.
(2)Information Center, Engineering Research Center of Medical Information 
Technology, Ministry of Education, West China Hospital, Sichuan University, 
Chengdu, China.
(3)Department of Computer Science and Information Technologies, Iberian Society 
of Telehealth and Telemedicine, University of A Coruña, A Coruña, Spain.
(4)Department of Clinical Laboratory Medicine, Jinniu Maternity and Child Health 
Hospital of Chengdu, Chengdu, China.
(5)Department of Computer Science and Information Technologies, Iberian Society 
of Telehealth and Telemedicine, Research Center for Information and 
Communications Technologies, Biomedical Research Institute of A Coruña, 
University of A Coruña, A Coruña, Spain.
(6)Department of Critical Care Medicine, Joint Laboratory of Artifcial 
Intelligence for Critical Care Medicine, Frontiers Science Center for 
Disease-related Molecular Network, Institutes for Systems Genetics, Sichuan 
University, West China Hospital, Chengdu, China.
(#)Contributed equally

BACKGROUND: Sepsis is a complex, life-threatening condition characterized by 
significant heterogeneity and vast amounts of unstructured data, posing 
substantial challenges for traditional knowledge graph construction methods. The 
integration of large language models (LLMs) with real-world data offers a 
promising avenue to address these challenges and enhance the understanding and 
management of sepsis.
OBJECTIVE: This study aims to develop a comprehensive sepsis knowledge graph by 
leveraging the capabilities of LLMs, specifically GPT-4.0, in conjunction with 
multicenter clinical databases. The goal is to improve the understanding of 
sepsis and provide actionable insights for clinical decision-making. We also 
established a multicenter sepsis database (MSD) to support this effort.
METHODS: We collected clinical guidelines, public databases, and real-world data 
from 3 major hospitals in Western China, encompassing 10,544 patients diagnosed 
with sepsis. Using GPT-4.0, we used advanced prompt engineering techniques for 
entity recognition and relationship extraction, which facilitated the 
construction of a nuanced sepsis knowledge graph.
RESULTS: We established a sepsis database with 10,544 patient records, including 
8497 from West China Hospital, 690 from Shangjin Hospital, and 357 from Tianfu 
Hospital. The sepsis knowledge graph comprises of 1894 nodes and 2021 distinct 
relationships, encompassing nine entity concepts (diseases, symptoms, 
biomarkers, imaging examinations, etc) and 8 semantic relationships 
(complications, recommended medications, laboratory tests, etc). GPT-4.0 
demonstrated superior performance in entity recognition and relationship 
extraction, achieving an F1-score of 76.76 on a sepsis-specific dataset, 
outperforming other models such as Qwen2 (43.77) and Llama3 (48.39). On the 
CMeEE dataset, GPT-4.0 achieved an F1-score of 65.42 using few-shot learning, 
surpassing traditional models such as BERT-CRF (62.11) and Med-BERT (60.66). 
Building upon this, we compiled a comprehensive sepsis knowledge graph, 
comprising of 1894 nodes and 2021 distinct relationships.
CONCLUSIONS: This study represents a pioneering effort in using LLMs, 
particularly GPT-4.0, to construct a comprehensive sepsis knowledge graph. The 
innovative application of prompt engineering, combined with the integration of 
multicenter real-world data, has significantly enhanced the efficiency and 
accuracy of knowledge graph construction. The resulting knowledge graph provides 
a robust framework for understanding sepsis, supporting clinical 
decision-making, and facilitating further research. The success of this approach 
underscores the potential of LLMs in medical research and sets a new benchmark 
for future studies in sepsis and other complex medical conditions.

©Hao Yang, Jiaxi Li, Chi Zhang, Alejandro Pazos Sierra, Bairong Shen. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org), 
27.03.2025.

DOI: 10.2196/65537
PMCID: PMC11986385
PMID: 40146985 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


842. Br J Ophthalmol. 2025 Dec 15;110(1):1-7. doi: 10.1136/bjo-2024-326097.

From visual question answering to intelligent AI agents in ophthalmology.

Chen X(#)(1), Chen R(#)(1), Xu P(#)(1), Wan X(1), Zhang W(1), Yan B(1), Shang 
X(1), He M(2)(3)(4), Shi D(2)(3).

Author information:
(1)School of Optometry, The Hong Kong Polytechnic University, Kowloon, Hong 
Kong.
(2)School of Optometry, The Hong Kong Polytechnic University, Kowloon, Hong Kong 
danli.shi@polyu.edu.hk mingguang.he@polyu.edu.hk.
(3)Research Centre for SHARP Vision (RCSV), The Hong Kong Polytechnic 
University, Kowloon, Hong Kong.
(4)Centre for Eye and Vision Research (CEVR), 17W Hong Kong Science Park, Hong 
Kong, China.
(#)Contributed equally

Ophthalmic practice involves the integration of diverse clinical data and 
interactive decision-making, posing challenges for traditional artificial 
intelligence (AI) systems. Visual question answering (VQA) addresses this by 
combining computer vision and natural language processing to interpret medical 
images through user-driven queries. Evolving from VQA, multimodal AI agents 
enable continuous dialogue, tool use and context-aware clinical decision 
support. This review explores recent developments in ophthalmic conversational 
AI, spanning theoretical advances and practical implementations. We highlight 
the transformative role of large language models (LLMs) in improving reasoning, 
adaptability and task execution. However, key obstacles remain, including 
limited multimodal datasets, absence of standardised evaluation protocols, and 
challenges in clinical integration. We outline these limitations and propose 
future research directions to support the development of robust, LLM-driven AI 
systems. Realising their full potential will depend on close collaboration 
between AI researchers and the ophthalmic community.

© Author(s) (or their employer(s)) 2026. No commercial re-use. See rights and 
permissions. Published by BMJ Group.

DOI: 10.1136/bjo-2024-326097
PMID: 40866109 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: None declared.


843. J Med Internet Res. 2026 Jan 8;28:e75500. doi: 10.2196/75500.

Intervention in Health Misinformation Using Large Language Models for Automated 
Detection, Thematic Analysis, and Inoculation: Case Study on COVID-19.

Malek S(1), Griffin C(2)(3), Fraleigh RD(2), Lennon R(4), Monga V(5), Shen L(6).

Author information:
(1)Department of Computer Science and Engineering, Pennsylvania State 
University, University Park, PA, United States.
(2)Applied Research Laboratory, Pennsylvania State University, University Park, 
PA, United States.
(3)Department of Mathematics, Pennsylvania State University, University Park, 
PA, United States.
(4)PrimeCare Medical, Harrisburg, PA, United States.
(5)Department of Electrical Engineering, Pennsylvania State University, 
University Park, PA, United States.
(6)Department of Communication Arts and Sciences, Pennsylvania State University, 
211 Sparks Building, University Park, PA, 16802, United States, 1 (814) 
865-1736.

BACKGROUND: The rapid growth of social media as an information channel has 
enabled the swift spread of inaccurate or false health information, 
significantly impacting public health. This widespread dissemination of 
misinformation has caused confusion, eroded trust in health authorities, led to 
noncompliance with health guidelines, and encouraged risky health behaviors. 
Understanding the dynamics of misinformation on social media is essential for 
devising effective public health communication strategies.
OBJECTIVE: This study aims to present a comprehensive and automated approach 
that leverages large language models (LLMs) and machine learning techniques to 
detect misinformation on social media, uncover the underlying causes and themes, 
and generate refutation arguments, facilitating control of its spread and 
promoting public health outcomes by inoculating people against health 
misinformation.
METHODS: We use 2 datasets to train 3 LLMs, namely, BERT, T5, and GPT-2, to 
classify documents into 2 categories: misinformation and nonmisinformation. In 
addition, we use a separate dataset to identify misinformation topics. To 
analyze these topics, we applied 3 topic modeling algorithms-Latent Dirichlet 
Allocation, Top2Vec, and BERTopic-and selected the optimal model based on 
performance evaluated across 3 metrics. Using a prompting approach, we extract 
sentence-level representations for the topics to uncover their underlying 
themes. Finally, we design a prompt text capable of identifying misinformation 
themes effectively.
RESULTS: The trained BERT model demonstrated exceptional performance, achieving 
98% accuracy in classifying misinformation and nonmisinformation, with a 44% 
reduction in false-positive rates for artificial intelligence-generated 
misinformation. Among the 3 topic modeling approaches used, BERTopic 
outperformed the others, achieving the highest metrics with a Coherence Value of 
0.41, Normalized Pointwise Mutual Information of -0.086, and Inverse Rank-Biased 
Overlap of 0.99. To address the issue of unclassified documents, we developed an 
algorithm to assign each document to its closest topic. In addition, we proposed 
a novel method using prompt engineering to generate sentence-level 
representations for each topic, achieving a 99.6% approval rate as "appropriate" 
or "somewhat appropriate" by 3 independent raters. We further designed a prompt 
text to identify themes of misinformation topics and developed another prompt 
capable of detecting misinformation themes with 82% accuracy.
CONCLUSIONS: This study presents a comprehensive and automated approach to 
addressing health misinformation on social media using advanced machine learning 
and natural language processing techniques. By leveraging LLMs and prompt 
engineering, the system effectively detects misinformation, identifies 
underlying themes, and provides explanatory responses to combat its spread. The 
proposed method was tested on an English language COVID-19-related dataset and 
has not been evaluated on real-world online social media data; the experiments 
were conducted offline.

© Samira Malek, Christopher Griffin, Robert D Fraleigh, Robert Lennon, Vishal 
Monga, Lijiang Shen. Originally published in the Journal of Medical Internet 
Research (https://www.jmir.org).

DOI: 10.2196/75500
PMCID: PMC12791202
PMID: 41505350 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


844. Acad Med. 2025 Apr 1;100(4):413-418. doi: 10.1097/ACM.0000000000005963. Epub 
2024 Dec 20.

Integrating Generative Artificial Intelligence Into Medical Education: 
Curriculum, Policy, and Governance Strategies.

Triola MM, Rodman A.

The rapid advancement of generative artificial intelligence (GAI) is poised to 
revolutionize medical education, clinical decision-making, and health care 
workflow. Despite considerable interest and a surfeit of newly available tools, 
medical educators largely lack both competencies and guidance on how to 
incorporate the new and rapidly evolving world of GAI into the core medical 
school curriculum and experiences of undergraduate medical education. This 
Scholarly Perspective highlights the need for medical schools to adapt to this 
new paradigm by implementing policies, governance, and curricula that address 
the ethical, technical, and pedagogical implications of GAI. The authors 
recommend creating policies for appropriate GAI use, designed to protect 
institutional and patient data, and provide students with clarity on the 
appropriate use of AI for education. The authors suggest that implementing GAI 
governance at institutions is crucial to create guiding principles on ethical 
and equitable GAI use and involving students as coinventors of local innovation. 
The authors argue that providing faculty and learners with tools and training 
for safe experimentation with GAI and defining competencies for students and 
faculty are essential. Curricula for GAI should focus on implications of 
clinical uses. The authors propose a set of new competencies for GAI that build 
on those already established for AI in general. Given how dynamic the world of 
GAI is and how quickly new innovations are changing longstanding practices of 
clinical medicine, it is imperative that the medical education community acts 
together to share best practices, gather data to assess the impact of GAI 
education, continuously update the expected competencies of medical students, 
and help students prepare for a career that will be continually changed by GAI.

Copyright © 2024 the Association of American Medical Colleges.

DOI: 10.1097/ACM.0000000000005963
PMID: 39705530 [Indexed for MEDLINE]


845. IEEE Trans Biomed Eng. 2025 Dec 10;PP. doi: 10.1109/TBME.2025.3642706. Online 
ahead of print.

Keeping Medical AI Healthy and Trustworthy: A Review of Detection and Correction 
Methods for System Degradation.

Guan H, Bates D, Zhou L.

Artificial intelligence (AI) is increasingly integrated into modern healthcare, 
offering powerful support for clinical decision-making. However, in real-world 
settings, AI systems may experience performance degradation over time, due to 
factors such as shifting data distributions, changes in patient characteristics, 
evolving clinical protocols, and variations in data quality. These factors can 
compromise model reliability, posing safety concerns and increasing the 
likelihood of inaccurate predictions or adverse outcomes. This review presents a 
forward-looking perspective on monitoring and maintaining the "health" of AI 
systems in healthcare. We highlight the urgent need for continuous performance 
monitoring, early degradation detection, and effective self-correction 
mechanisms. The paper begins by reviewing common causes of performance 
degradation at both data and model levels. We then summarize key techniques for 
detecting data and model drift, followed by an in-depth look at root cause 
analysis. Correction strategies are further reviewed, ranging from model 
retraining to test-time adaptation. Our survey spans both traditional machine 
learning models and state-of-the-art large language models (LLMs), offering 
insights into their strengths and limitations. Finally, we discuss ongoing 
technical challenges and propose future research directions. This work aims to 
guide the development of reliable, robust medical AI systems capable of 
sustaining safe, long-term deployment in dynamic clinical settings.

DOI: 10.1109/TBME.2025.3642706
PMID: 41370148


846. J Med Internet Res. 2026 Jan 23;28:e87969. doi: 10.2196/87969.

Data Poisoning Vulnerabilities Across Health Care Artificial Intelligence 
Architectures: Analytical Security Framework and Defense Strategies.

Abtahi F(1)(2)(3), Seoane F(1)(4)(5)(3), Pau I(6), Vega-Barbas M(6).

Author information:
(1)Department of Clinical Science, Intervention and Technology, Karolinska 
Institutet, Huddinge, Stockholm, Sweden.
(2)Department of Biomedical Engineering and Health Systems, KTH Royal Institute 
of Technology, Huddinge, Stockholm, Sweden.
(3)Department of Clinical Physiology, Karolinska University Hospital, Huddinge, 
Stockholm, Sweden.
(4)Department of Medical Technology, Karolinska University Hospital, Stockholm, 
Sweden.
(5)Department of Textile Technology, University of Borås, Borås, Västra 
Götaland, Sweden.
(6)ETSIS de Telecomunicación, Universidad Politécnica de Madrid, Madrid, Madrid, 
Spain.

BACKGROUND: Health care artificial intelligence (AI) systems are increasingly 
integrated into clinical workflows, yet remain vulnerable to data-poisoning 
attacks. A small number of manipulated training samples can compromise AI models 
used for diagnosis, documentation, and resource allocation. Existing privacy 
regulations, including the Health Insurance Portability and Accountability Act 
and the General Data Protection Regulation, may inadvertently complicate anomaly 
detection and cross-institutional auditing, thereby limiting visibility into 
adversarial activity.
OBJECTIVE: This study provides a comprehensive threat analysis of data poisoning 
vulnerabilities across major health care AI architectures. The goals are to (1) 
identify attack surfaces in clinical AI systems, (2) evaluate the feasibility 
and detectability of poisoning attacks analytically modeled in prior security 
research, and (3) propose a multilayered defense framework appropriate for 
health care settings.
METHODS: We synthesized empirical findings from 41 key security studies 
published between 2019 and 2025 and integrated them into an analytical 
threat-modeling framework specific to health care. We constructed 8 hypothetical 
yet technically grounded attack scenarios across 4 categories: (1) 
architecture-specific attacks on convolutional neural networks, large language 
models, and reinforcement learning agents (scenario A); (2) infrastructure 
exploitation in federated learning and clinical documentation pipelines 
(scenario B); (3) poisoning of critical resource allocation systems (scenario 
C); and (4) supply chain attacks affecting commercial foundation models 
(scenario D). Scenarios were aligned with realistic insider-access threat models 
and current clinical deployment practices.
RESULTS: Multiple empirical studies demonstrate that attackers with access to as 
few as 100-500 poisoned samples can compromise health care AI systems, with 
attack success rates typically ≥60%. Critically, attack success depends on the 
absolute number of poisoned samples rather than their proportion of the training 
corpus, a finding that fundamentally challenges assumptions that larger datasets 
provide inherent protection. We estimate that detection delays commonly range 
from 6 to 12 months and may extend to years in distributed or 
privacy-constrained environments. Analytical scenarios highlight that (1) 
routine insider access creates numerous injection points across health care data 
infrastructure, (2) federated learning amplifies risks by obscuring attribution, 
and (3) supply chain compromises can simultaneously affect dozens to hundreds of 
institutions. Privacy regulations further complicate cross-patient correlation 
and model audit processes, substantially delaying the detection of subtle 
poisoning campaigns.
CONCLUSIONS: Health care AI systems face significant security challenges that 
current regulatory frameworks and validation practices do not adequately 
address. We propose a multilayered defense strategy that combines ensemble 
disagreement monitoring, adversarial testing, privacy-preserving yet auditable 
mechanisms, and strengthened governance requirements. Ensuring patient safety 
may require a shift from opaque, high-performance models toward more 
interpretable and constraint-driven architectures with verifiable robustness 
guarantees.

©Farhad Abtahi, Fernando Seoane, Ivan Pau, Mario Vega-Barbas. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org), 
23.01.2026.

DOI: 10.2196/87969
PMID: 41575020 [Indexed for MEDLINE]


847. J Med Internet Res. 2025 Sep 16;27:e78417. doi: 10.2196/78417.

New Doc on the Block: Scoping Review of AI Systems Delivering Motivational 
Interviewing for Health Behavior Change.

Karve Z(1), Calpey J(1), Machado C(1), Knecht M(1), Mejia MC(1).

Author information:
(1)Schmidt College of Medicine, Florida Atlantic University, Boca Raton, FL, 
United States.

BACKGROUND: Artificial intelligence (AI) is increasingly used in digital health, 
particularly through large language models (LLMs), to support patient engagement 
and behavior change. One novel application is the delivery of motivational 
interviewing (MI), an evidence-based, patient-centered counseling technique 
designed to enhance motivation and resolve ambivalence around health behaviors. 
AI tools, including chatbots, mobile apps, and web-based agents, are being 
developed to simulate MI techniques at scale. While these innovations are 
promising, important questions remain about how faithfully AI systems can 
replicate MI principles or achieve meaningful behavioral impact.
OBJECTIVE: This scoping review aimed to summarize existing empirical studies 
evaluating AI-driven systems that apply MI techniques to support health behavior 
change. Specifically, we examined the feasibility of these systems; their 
fidelity to MI principles; and their reported behavioral, psychological, or 
engagement outcomes.
METHODS: We systematically searched PubMed, Embase, Scopus, Web of Science, and 
Cochrane Library for empirical studies published between January 1, 2018, and 
February 25, 2025. Eligible studies involved AI-driven systems using natural 
language generation, understanding, or computational logic to deliver MI 
techniques to users targeting a specific health behavior. We excluded studies 
using AI solely for training clinicians in MI. Three independent reviewers 
screened and extracted data on study design, AI modality and type, MI 
components, health behavior focus, MI fidelity assessment, and outcome domains.
RESULTS: Of the 1001 records identified, 15 (1.5%) met the inclusion criteria. 
Of these 15 studies, 6 (40%) were exploratory feasibility or pilot studies, and 
3 (20%) were randomized controlled trials. AI modalities included rule-based 
chatbots (9/15, 60%), LLM-based systems (4/15, 27%), and virtual or mobile 
agents (2/15, 13%). Targeted behaviors included smoking cessation (6/15, 40%), 
substance use (3/15, 20%), COVID-19 vaccine hesitancy, type 2 diabetes 
self-management, stress, mental health service use, and opioid use during 
pregnancy. Of the 15 studies, 13 (87%) reported positive findings on feasibility 
or user acceptability, while 6 (40%) assessed MI fidelity using expert review or 
structured coding, with moderate to high alignment reported. Several studies 
found that users perceived the AI systems as judgment free, supportive, and 
easier to engage with than human counselors, particularly in stigmatized 
contexts. However, limitations in empathy, safety transparency, and emotional 
nuance were commonly noted. Only 3 (20%) of the 15 studies reported 
substantially significant behavioral changes.
CONCLUSIONS: AI systems delivering MI show promise for enhancing patient 
engagement and scaling behavior change interventions. Early evidence supports 
their usability and partial fidelity to MI principles, especially in sensitive 
domains. However, most systems remain in early development, and few have been 
rigorously tested. Future research should prioritize randomized evaluations; 
standardized fidelity measures; and safeguards for LLM safety, empathy, and 
accuracy in health-related dialogue.
TRIAL REGISTRATION: OSF Registries 10.17605/OSF.IO/G9N7E; https://osf.io/g9n7e.

©Zev Karve, Jacob Calpey, Christopher Machado, Michelle Knecht, Maria Carmenza 
Mejia. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 16.09.2025.

DOI: 10.2196/78417
PMCID: PMC12485255
PMID: 40957014 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


848. Acad Med. 2025 Sep 1;100(9S Suppl 1):S15-S21. doi: 10.1097/ACM.0000000000006107. 
Epub 2025 Jun 2.

Macy Foundation Innovation Report Part I: Current Landscape of Artificial 
Intelligence in Medical Education.

Boscardin CK, Abdulnour RE, Gin BC.

The rapid emergence of artificial intelligence (AI), including generative large 
language models, offers transformative opportunities in medical education. This 
proliferation has generated numerous speculative discussions about AI's promise 
but has been limited in delivering a comprehensive analysis to distinguish 
evidence-based utility from hype while identifying context-specific 
limitations.In this first part of a 2-part innovation report, commissioned by 
the Josiah Macy Jr. Foundation to inform the discussions at a conference on AI 
in medical education, the authors synthesize the landscape of AI in medical 
education, underscoring both its potential advantages and inherent challenges. 
To map the AI landscape, they reviewed 455 articles that targeted 5 medical 
education domains: (1) admissions, (2) classroom-based learning and teaching, 
(3) workplace-based learning and teaching, (4) assessment, feedback, and 
certification, and (5) program evaluation and research.In admissions, AI-driven 
strategies facilitated holistic applicant reviews through predictive modeling, 
natural language processing, and large language model-based chatbots. 
Preclinical learning benefited from AI-powered virtual patients and curriculum 
design tools that managed expanding medical knowledge and supported robust 
student practice. Within clinical learning, AI aided diagnostic and interpretive 
processes, prompting medical education curricula to demand relevant AI 
competency and literacy frameworks. A few studies reported that assessment and 
feedback processes became more efficient through automated grading and advanced 
analytics, which reduced faculty workload and offered timely, targeted feedback. 
Program evaluation and research gained additional insights using AI on careers, 
diversity, and performance metrics of faculty and learners, improving resource 
allocations and guiding evidence-based approaches.Despite these possibilities, 
bias in AI algorithms, concerns about transparency, inadequate ethical 
guidelines, and risks of over-reliance highlighted the need for cautious, 
informed AI implementation. By mapping AI tasks to medical education 
applications, the authors provide a framework for understanding and leveraging 
AI's potential while addressing technical, ethical, and human-factor 
complexities in this evolving field.

Copyright © 2025 the Association of American Medical Colleges.

DOI: 10.1097/ACM.0000000000006107
PMID: 40456178 [Indexed for MEDLINE]


849. Hum Genet. 2025 Jun;144(6):605-614. doi: 10.1007/s00439-025-02743-z. Epub 2025 
Apr 23.

Harnessing genotype and phenotype data for population-scale variant 
classification using large language models and bayesian inference.

Manders TR(1), Tan CA(2), Kobayashi Y(2), Wahl A(2), Araya C(3)(4), Colavin 
A(3)(5), Facio FM(2)(6), Metz H(2), Reuter J(2), Frésard L(2), Padigepati SR(2), 
Stafford DA(2), Nussbaum RL(3)(7), Nykamp K(2)(6).

Author information:
(1)Labcorp Genetics Inc, 1400 16th Street, San Francisco, CA, 94103, USA. 
toby.manders@labcorp.com.
(2)Labcorp Genetics Inc, 1400 16th Street, San Francisco, CA, 94103, USA.
(3)Invitae Corporation, 1400 16th Street, San Francisco, CA, 94103, USA.
(4)Tapanti.org, PO Box #727, 836 Anacapa St, Santa Barbara, CA, 93102, USA.
(5)Present Address: Threshold Health Inc, 1638 Myrtle Ave, San Diego, CA, 92103, 
USA.
(6)Present Address: GeneDx, 205/207 Perry Parkway, Gaitherburg, MD, 20877, USA.
(7)Present Address: Division of Medical Genetics, Department of Pediatrics, 
University of California San Francisco, 1825 4th St, San Francisco, CA, 94158, 
USA.

Variants of Uncertain Significance (VUS) in genetic testing for hereditary 
diseases burden patients and clinicians, yet clinical data that could reduce VUS 
are underutilized due to a lack of scalable strategies. We assessed whether a 
machine learning approach using genotype and phenotype data could improve 
variant classification and reduce VUS. In this cohort study of a multi-step 
machine learning approach, patient data from test requisition forms were used to 
distinguish patients with molecular diagnoses from controls ("patient score"). A 
generative Bayesian model then used patient scores and variant classifications 
to infer variant pathogenicity ("variant score"). The study included 3.5 million 
patients referred for clinical genetic testing across various conditions. 
Primary outcomes were model- and gene-level discrimination, classification 
performance, probabilistic calibration, and concordance with orthogonal 
pathogenicity measures. Integration into a semi-quantitative classification 
framework was based on posterior pathogenicity probabilities matching 
PPV ≥ 0.99/NPV ≥ 0.95 thresholds, followed by expert review. We generated 1,334 
clinical variant models (CVMs); 595 showed high performance in both machine 
learning steps (AUROCpatient ≥ 0.8 and AUROCvariant ≥ 0.8) on held-out data. 
High-confidence predictions from these CVMs provided evidence for 5,362 VUS 
observed in 200,174 patients, representing 23.4% of all VUS observations in 
these genes. In 17 frequently tested genes, CVMs reclassified over 1,000 unique 
VUS, reducing VUS report rates by 9-49% per condition. In conclusion, a scalable 
machine learning approach using underutilized clinical data improved variant 
classification and reduced VUS.

© 2025. The Author(s).

DOI: 10.1007/s00439-025-02743-z
PMCID: PMC12170740
PMID: 40266329 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval: This study was 
performed in line with the principles of the Declaration of Helsinki. The use of 
de-identified genetic data was approved under the Western Institutional Review 
Board number CR-001-02 (Tracking ID 20161796). Consent to participate: Not 
applicable. Consent for publication: Not applicable. Competing interests: TM, 
CAT, YK, AW, LF, SP and DS are employees of Labcorp and are affiliated with 
Labcorp Genetics Inc. All authors are former employees of Invitae Corporation.


850. BMC Med Educ. 2025 Jul 1;25(1):949. doi: 10.1186/s12909-025-07528-6.

Comparison of AI-generated and clinician-designed multiple-choice questions in 
emergency medicine exam: a psychometric analysis.

Kaya M(1), Sonmez E(2), Halici A(2), Yildirim H(2), Coskun A(2).

Author information:
(1)Emergency Medicine Department, Kutahya Health Sciences University Medical 
School, Kutahya, Turkey. murtaza.kaya@ksbu.edu.tr.
(2)Emergency Medicine Department, Kutahya Health Sciences University Medical 
School, Kutahya, Turkey.

BACKGROUND: Artificial intelligence (AI) has shown promise in generating 
multiple-choice questions (MCQs) for medical education, yet the psychometric 
quality of such items remains underexplored. This study aimed to compare the 
psychometric properties of MCQs created by ChatGPT-4o and those written by 
emergency medicine clinicians.
METHODS: Eighteen emergency medicine residents completed a 100-item examination 
comprising 50 AI-generated and 50 clinician-authored questions across core 
emergency medicine topics. Each item was analyzed for difficulty (P_index), 
discrimination (D_index), and point-biserial correlation (PBCC). Items were also 
categorized based on standardized index classifications.
RESULTS: ChatGPT-4o-generated questions exhibited a higher mean difficulty index 
(P_index: 0.76 ± 0.23) compared to those created by clinicians (0.65 ± 0.24; 
p = 0.02), indicating that the AI-generated items were generally easier. 
Participants achieved significantly higher scores on AI-generated items 
(76.8 ± 8.18) than on clinician-authored questions (67.3 ± 9.65; p = 0.003). The 
mean discrimination index did not differ significantly between AI-generated 
(0.172 ± 0.23) and clinician-generated items (0.196 ± 0.26; p = 0.634). 
Likewise, the mean point-biserial correlation coefficient (PBCC) was nearly 
identical between the two groups (AI: 0.23 ± 0.28; clinicians: 0.23 ± 0.25; 
p = 0.99), suggesting similar internal consistency. Categorical analysis 
revealed that 56% of AI-generated items were classified as "easy," compared to 
36% of clinician-designed items. Furthermore, based on PBCC values, 36% of 
AI-generated items and 24% of clinician items were identified as "problematic" 
(p = 0.015), indicating a higher rate of psychometric concerns among 
AI-generated questions.
CONCLUSION: The findings suggest that AI-generated questions, while generally 
easier and associated with higher participant scores, may pose psychometric 
limitations, as evidenced by a greater proportion of items classified as 
problematic. Although the overall internal consistency and discrimination 
indices were comparable to clinician-authored items, careful quality control and 
validation are essential when integrating AI-generated content into assessment 
frameworks.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-07528-6
PMCID: PMC12210704
PMID: 40597998 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was approved by the local ethics committee (This study 
has approval from Kutahya Health Sciences University Medical Faculty Ethics 
Committee (approval date and number: 11.03.2025, 2025/04–09) Informed consent 
was obtained from the participants. Consent for publication: Not Applicable. 
Human rights: The study protocol conforms to the ethical guidelines of the 1975 
Declaration of Helsinki. Competing interests: The authors declare no competing 
interests.


851. Otolaryngol Head Neck Surg. 2025 Feb;172(2):734-743. doi: 10.1002/ohn.1080. Epub 
2024 Dec 12.

American Academy of Otolaryngology-Head and Neck Surgery (AAO-HNS) Report on 
Artificial Intelligence.

Ayoub NF(1)(2), Rameau A(3), Brenner MJ(4), Bur AM(5), Ator GA(5), Briggs SE(6), 
Takashima M(7), Stankovic KM(2); AAO‐HNS Artificial Intelligence Task Force.

Author information:
(1)Department of Otolaryngology-Head and Neck Surgery, Mass Eye & Ear, Boston, 
Massachusetts, USA.
(2)Department of Otolaryngology-Head and Neck Surgery, Stanford University, Palo 
Alto, California, USA.
(3)Department of Otolaryngology-Head and Neck Surgery, Weill Cornell Medical 
College, Ithaca, New York, USA.
(4)Department of Otolaryngology-Head and Neck Surgery, University of Michigan 
Medical School, Ann Arbor, Michigan, USA.
(5)Department of Otolaryngology-Head and Neck Surgery, University of Kansas 
Medical Center, Kansas City, Kansas, USA.
(6)Department of Otolaryngology-Head and Neck Surgery, MedStar Georgetown 
University Hospital, Washington, District of Columbia, USA.
(7)Department Otolaryngology-Head and Neck Surgery, Houston Methodist, Houston, 
Texas, USA.

This report synthesizes the American Academy of Otolaryngology-Head and Neck 
Surgery (AAO-HNS) Task Force's guidance on the integration of artificial 
intelligence (AI) in otolaryngology-head and neck surgery (OHNS). A 
comprehensive literature review was conducted, focusing on the applications, 
benefits, and challenges of AI in OHNS, alongside ethical, legal, and social 
implications. The Task Force, formulated by otolaryngologist experts in AI, used 
an iterative approach, adapted from the Delphi method, to prioritize topics for 
inclusion and to reach a consensus on guiding principles. The Task Force's 
findings highlight AI's transformative potential for OHNS, offering potential 
advancements in precision medicine, clinical decision support, operational 
efficiency, research, and education. However, challenges such as data quality, 
health equity, privacy concerns, transparency, regulatory gaps, and ethical 
dilemmas necessitate careful navigation. Incorporating AI into otolaryngology 
practice in a safe, equitable, and patient-centered manner requires clinician 
judgment, transparent AI systems, and adherence to ethical and legal standards. 
The Task Force principles underscore the importance of otolaryngologists' 
involvement in AI's ethical development, implementation, and regulation to 
harness benefits while mitigating risks. The proposed principles inform the 
integration of AI in otolaryngology, aiming to enhance patient outcomes, 
clinician well-being, and efficiency of health care delivery.

© 2024 American Academy of Otolaryngology–Head and Neck Surgery Foundation.

DOI: 10.1002/ohn.1080
PMCID: PMC12369755
PMID: 39666770 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: None.


852. J Am Med Inform Assoc. 2025 May 1;32(5):968-970. doi: 10.1093/jamia/ocaf041.

How the National Library of Medicine should evolve in an era of artificial 
intelligence.

Lenert LA(1).

Author information:
(1)Biomedical Informatics Center, Medical University of South Carolina, 
Charleston, SC 29405, United States.

OBJECTIVES: This article describes the challenges faced by the National Library 
of Medicine with the rise of artificial intelligence (AI) and access to human 
knowledge through large language models (LLMs).
BACKGROUND AND SIGNIFICANCE: The rise of AI as a tool for the acceleration and 
falsification of science is impacting every aspect of the transformation of data 
to information, knowledge, and wisdom through the scientific processes.
APPROACH: This perspective discusses the philosophical foundations, threats, and 
opportunities of the AI revolution with a proposal for restructuring the mission 
of the National Library of Medicine (NLM), part of the National Institutes of 
Health, with a central role as the guardian of the integrity of scientific 
knowledge in an era of AI-driven science.
RESULTS: The NLM can rise to new challenges posed by AI by working from its 
foundations in theories of Information Science and embracing new roles. Three 
paths for the NLM are proposed: (1) Become an Authentication Authority For Data, 
Information, and Knowledge through Systems of Scientific Provenance; (2) Become 
An Observatory of the State of Human Health Science supporting living systematic 
reviews; and (3) Become A hub for Culturally Appropriate Bespoke Translation, 
Transformation, and Summarization for different users (patients, the public, as 
well as scientists and clinicians) using AI technologies.
DISCUSSION: Adapting the NLM to the challenges of the Internet revolution by 
developing worldwide-web-accessible resources allowed the NLM to rise to new 
heights. Bold moves are needed to adapt the Library to the AI revolution but 
offer similar prospects of more significant impacts on the advancement of 
science and human health.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf041
PMCID: PMC12012362
PMID: 40063704 [Indexed for MEDLINE]

Conflict of interest statement: None declared.


853. IEEE J Biomed Health Inform. 2025 Aug 15;PP. doi: 10.1109/JBHI.2025.3599643. 
Online ahead of print.

Enhancing Psychological Assessments with Open-Ended Questionnaires and Large 
Language Models: An ASD Case Study.

Altozano A, Minissi ME, Gomez-Zaragoza L, Maddalon L, Alcaniz M, Marin-Morales 
J.

Open-ended questionnaires allow respondents to express freely, capturing richer 
information than close-ended formats, but they are harder to analyze. Recent 
natural language processing advancements enable automatic assessment of 
open-ended responses, yet its use in psychological classification is 
underexplored. This study proposes a methodology using pre-trained large 
language models (LLMs) for automatic classification of open-ended 
questionnaires, applied to autism spectrum disorder (ASD) classification via 
parental reports. We compare multiple training strategies using transcribed 
responses from 51 parents (26 with typically developing children, 25 with ASD), 
exploring variations in model fine-tuning, input representation, and 
specificity. Subject-level predictions are derived by aggregating 12 individual 
question responses. Our best approach achieved 84% subject-wise accuracy and 1.0 
ROC-AUC using an OpenAI embedding model, per-question training, including 
questions in the input, and combining the predictions with a voting system. In 
addition, a zero-shot evaluation using GPT-4o was conducted, yielding comparable 
results, underscoring the potential of both compact, local models and large 
out-of-the-box LLMs. To enhance transparency, we explored interpretability 
methods. Proprietary LLMs like GPT-4o offered no direct explanation, and OpenAI 
embedding models showed limited interpretability. However, locally deployable 
LLMs provided the highest interpretability. This highlights a trade-off between 
proprietary models' performance and local models' explainability. Our findings 
validate LLMs for automatically classifying open-ended questionnaires, offering 
a scalable, cost-effective complement for ASD assessment. These results suggest 
broader applicability for psychological analysis of other conditions, advancing 
LLM use in mental health research.

DOI: 10.1109/JBHI.2025.3599643
PMID: 40815586


854. Front Nutr. 2025 Aug 7;12:1635682. doi: 10.3389/fnut.2025.1635682. eCollection 
2025.

Large language models in clinical nutrition: an overview of its applications, 
capabilities, limitations, and potential future prospects.

Belkhouribchia J(1), Pen JJ(2).

Author information:
(1)Endocrinology Center Hasselt, Hasselt, Belgium.
(2)Department of Nutrition, UZ Brussel, Vrije Universiteit Brussel (VUB), 
Brussels, Belgium.

The integration of large language models (LLMs) into clinical nutrition marks a 
transformative advancement, offering promising solutions for enhancing patient 
care, personalizing dietary recommendations, and supporting evidence-based 
clinical decision-making. Trained on extensive text corpora and powered by 
transformer-based architectures, LLMs demonstrate remarkable capabilities in 
natural language understanding and generation. This review provides an overview 
of their current and potential applications in clinical nutrition, focusing on 
key technologies including prompt engineering, fine-tuning, retrieval-augmented 
generation, and multimodal integration. These enhancements increase domain 
relevance, factual accuracy, and contextual responsiveness, enabling LLMs to 
deliver more reliable outputs in nutrition-related tasks. Recent studies have 
shown LLMs' utility in dietary planning, nutritional education, obesity 
management, and malnutrition risk assessment. Despite these advances, challenges 
remain. Limitations in reasoning, factual accuracy, and domain specificity, 
along with risks of bias and hallucination, underscore the need for rigorous 
validation and human oversight. Furthermore, ethical considerations, 
environmental costs, and infrastructural integration must be addressed before 
widespread adoption. Future directions include combining LLMs with predictive 
analytics, integrating them with electronic health records and wearables, and 
adapting them for multilingual, culturally sensitive dietary guidance. LLMs also 
hold potential as research and educational tools, assisting in literature 
synthesis and patient engagement. Their transformative promise depends on 
cross-disciplinary collaboration, responsible deployment, and clinician 
training. Ultimately, while LLMs are not a replacement for healthcare 
professionals, they offer powerful augmentation tools for delivering scalable, 
personalized, and data-driven nutritional care in an increasingly complex 
healthcare environment.

Copyright © 2025 Belkhouribchia and Pen.

DOI: 10.3389/fnut.2025.1635682
PMCID: PMC12367769
PMID: 40851903

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


855. Am J Clin Nutr. 2025 Nov 27:101127. doi: 10.1016/j.ajcnut.2025.101127. Online 
ahead of print.

Language models in nutrition and dietetics: a scoping review.

O'Hara C(1), Kent G(2), Leydon CL(2), Walsh NM(2), Gibney ER(3), Skoutas D(4), 
Flynn AC(2), Timon CM(2).

Author information:
(1)School of Population Health, Royal College of Surgeons in Ireland, Dublin, 
Ireland. Electronic address: cathalohara@rcsi.ie.
(2)School of Population Health, Royal College of Surgeons in Ireland, Dublin, 
Ireland.
(3)School of Agriculture and Food Science, University College Dublin, Dublin, 
Ireland; UCD Institute of Food and Health, University College Dublin, Ireland.
(4)Information Management Systems Institute, Athena Research Centre, Marousi, 
Greece.

BACKGROUND: Language models are a form of generative artificial intelligence 
that excel at natural language understanding and generation, with possible 
applications for providing evidence-based nutrition information in an 
interactive and personalized manner. However, they also pose risks to public 
health related to inaccuracy, bias, and data protection. It is unclear what 
quantity or type of research has been conducted on these models applied within 
nutrition and dietetics.
OBJECTIVES: To identify the ways in which language models are being applied to 
nutrition and dietetics and the associated opportunities and challenges.
METHODS: The review was conducted in accordance with a predefined protocol and 
included original studies from peer-reviewed journals. The databases Medline, 
EMBASE, PsycINFO, CINAHL, PubMed, Global Health, Web of Science, Scopus, DBLP, 
and IEEE Xplore were searched, and forward and backward citation chasing was 
conducted for included articles.
RESULTS: There were 2118 articles identified, and after screening 40 remained 
for data charting. All studies were published between 2022 and 2024. A total of 
23 different language models were used with 9 studies creating domain-specific 
models using retrieval augmented generation or fine-tuning. A total of 17 
nutrition-related diseases were considered. The tasks assigned to the language 
models included extraction or summarization of data from the scientific 
literature or electronic health records (n = 5), estimation of food composition 
(n = 9), creation of meal plans (n = 7), and provision of dietary advice (n = 
21). Most studies (n = 38) focused on evaluating the models in hypothetical 
situations such as researcher-designed case studies or questions.
CONCLUSION: This is an emerging area with limited evidence for real-world 
implementations or domain-specific models in nutrition and dietetics. Future 
work is required in real-world settings to strengthen this evidence base and to 
broaden it to incorporate the views of nutritionists, dietitians, people with 
medical conditions, and the general public.

Copyright © 2025. Published by Elsevier Inc.

DOI: 10.1016/j.ajcnut.2025.101127
PMID: 41317995

Conflict of interest statement: Conflict of interest CMT, ACF, and DS report 
financial support was provided by European Union. The other authors declare that 
they have no known competing financial interests or personal relationships that 
could have appeared to influence the work reported in this article.


856. IEEE J Biomed Health Inform. 2025 Aug 27;PP. doi: 10.1109/JBHI.2025.3602983. 
Online ahead of print.

MediGuard: Protecting Sensitive Healthcare Data with Privacy-Preserving Language 
Models.

Javed H, Ali F, Shah B, Dilshad N, Kwak D.

The integration of large language models (LLMs) into digital healthcare has the 
potential to significantly improve access to accurate and timely medical advice, 
especially in underserved areas. However, serious privacy concerns hinder the 
widespread adoption of LLM-based medical consultation systems, as they often 
require users to disclose private health information, risking unauthorized 
exposure and non-compliance with regulations. To address these issues, we 
introduce MediGuard, a new privacy-preserving LLM framework that dynamically 
protects sensitive healthcare data throughout the consultation process. 
MediGuard employs adaptive information obfuscation, combined with secure access 
protocols and robust auditing mechanisms, to process only non-sensitive 
information while preserving the necessary semantic integrity for precise 
medical inference and decision-making. Extensive testing across multiple medical 
question-answering datasets demonstrates that MediGuard consistently outperforms 
existing methods in both privacy protection and clinical accuracy, even under 
stringent privacy constraints. Our findings suggest that MediGuard provides 
safe, trustworthy, and clinically reliable medical consultations, setting a new 
standard for privacy-aware healthcare AI.

DOI: 10.1109/JBHI.2025.3602983
PMID: 40864576


857. J Med Internet Res. 2026 Jan 14;28:e86550. doi: 10.2196/86550.

From Agents to Governance: Essential AI Skills for Clinicians in the Large 
Language Model Era.

Cao W(1)(2), Zhang Q(#)(1), Liu J(#)(3)(4), Liu S(5).

Author information:
(1)Department of Cardiology, West China Hospital, Sichuan University, Chengdu, 
Sichuan, China.
(2)Department of Cardiology, The People's Hospital of Leshan, Leshan, Sichuan, 
China.
(3)Information Center, West China Hospital, Sichuan University, Chengdu, China.
(4)Department of Medical Informatics, West China Medical School, Sichuan 
University, Chengdu, China.
(5)Department of Biomedical Informatics, Vanderbilt University Medical Center, 
Nashville, TN, United States.
(#)Contributed equally

Large language models are rapidly transitioning from pilot schemes to routine 
clinical practice. This creates an urgent need for clinicians to develop the 
necessary skills to strike the right balance between seizing opportunities and 
taking accountability. We propose a 3-tier competency framework to support 
clinicians' evolution from cautious users to responsible stewards of artificial 
intelligence (AI). Tier 1 (foundational skills) defines the minimum competencies 
for safe use, including prompt engineering, human-AI agent interaction, security 
and privacy awareness, and the clinician-patient interface (transparency and 
consent). Tier 2 (intermediate skills) emphasizes evaluative expertise, 
including bias detection and mitigation, interpretation of explainability 
outputs, and the effective clinical integration of AI-generated workflows. Tier 
3 (advanced skills) establishes leadership capabilities, mandating competencies 
in ethical governance (delineating accountability and liability boundaries), 
regulatory strategy, and model life cycle management-specifically, the ability 
to govern algorithmic adaptation and change protocols. Integrating this 
framework into continuing medical education programs and role-specific job 
descriptions could enhance clinicians' ability to use AI safely and responsibly. 
This could standardize deployment and support safer clinical practice, with the 
potential to improve patient outcomes.

©Weiping Cao, Qing Zhang, Jialin Liu, Siru Liu. Originally published in the 
Journal of Medical Internet Research (https://www.jmir.org), 14.01.2026.

DOI: 10.2196/86550
PMID: 41534067 [Indexed for MEDLINE]


858. J Med Internet Res. 2025 Aug 18;27:e71113. doi: 10.2196/71113.

Automated Extraction of Mortality Information From Publicly Available Sources 
Using Large Language Models: Development and Evaluation Study.

Al-Garadi M(1), LeNoue-Newton M(1), Matheny ME(1), McPheeters M(2), Whitaker 
JM(1), Deere JA(1), McLemore MF(1), Westerman D(1), Khan MS(1), Hernández-Muñoz 
JJ(3), Wang X(3), Kuzucan A(3), Desai RJ(4), Reeves R(1).

Author information:
(1)Department of Biomedical Informatics, Vanderbilt University Medical Center, 
2525 West End Avenue, Nashville, TN, 37203, United States, 1 2139151696.
(2)Research Triangle Park, NC, United States.
(3)United States Food and Drug Administration, Silver Spring, MD, United States.
(4)Harvard University, Cambridge, MA, United States.

BACKGROUND: Mortality is a critical variable in health care research, especially 
for evaluating medical product safety and effectiveness. However, 
inconsistencies in the availability and timeliness of death date and cause of 
death (CoD) information present significant challenges. Conventional sources 
such as the National Death Index and electronic health records often experience 
data lags, missing fields, or incomplete coverage, limiting their utility in 
time-sensitive or large-scale studies. With the growing use of social media, 
crowdfunding platforms, and web-based memorials, publicly available digital 
content has emerged as a potential supplementary source for mortality 
surveillance. Despite this potential, accurate tools for extracting mortality 
information from such unstructured data sources remain underdeveloped.
OBJECTIVE: The aim of the study is to develop scalable approaches using natural 
language processing (NLP) and large language models (LLMs) for the extraction of 
mortality information from publicly available web-based data sources, including 
social media platforms, crowdfunding websites, and web-based obituaries, and to 
evaluate their performance across various sources.
METHODS: Data were collected from public posts on X (formerly known as Twitter), 
GoFundMe campaigns, memorial websites (EverLoved and TributeArchive), and 
web-based obituaries from 2015 to 2022, focusing on US-based content relevant to 
mortality. We developed an NLP pipeline using transformer-based models to 
extract key mortality information such as decedent names, dates of birth, and 
dates of death. We then used a few-shot learning (FSL) approach with LLMs to 
identify primary and secondary CoDs. Model performance was assessed using 
precision, recall, F1-score, and accuracy metrics, with human-annotated labels 
serving as the reference standard for the transformer-based model and a human 
adjudicator blinded to the labeling source for the FSL model reference standard.
RESULTS: The best-performing model obtained a microaveraged F1-score of 0.88 
(95% CI 0.86-0.90) in extracting mortality information. The FSL-LLM approach 
demonstrated high accuracy in identifying primary CoD across various web-based 
sources. For GoFundMe, the FSL-LLM achieved 95.9% accuracy for primary cause 
identification compared to 97.9% for human annotators. In obituaries, FSL-LLM 
accuracy was 96.5% for primary causes, while human accuracy was 99%. For 
memorial websites, FSL-LLM achieved 98% accuracy for primary causes, with human 
accuracy at 99.5%.
CONCLUSIONS: This study demonstrates the feasibility of using advanced NLP and 
LLM techniques to extract mortality data from publicly available web-based 
sources. These methods can significantly enhance the timeliness, completeness, 
and granularity of mortality surveillance, offering a valuable complement to 
traditional data systems. By enabling earlier detection of mortality signals and 
improving CoD classification across large populations, this approach may support 
more responsive public health monitoring and medical product safety assessments. 
Further work is needed to validate these findings in real-world health care 
settings and facilitate the integration of digital data sources into national 
public health surveillance systems.

© Mohammed Al-Garadi, Michele LeNoue-Newton, Michael E Matheny, Melissa 
McPheeters, Jill M Whitaker, Jessica A Deere, Michael F McLemore, Dax Westerman, 
Mirza S Khan, José J Hernández-Muñoz, Xi Wang, Aida Kuzucan, Rishi J Desai, Ruth 
Reeves. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/71113
PMCID: PMC12359966
PMID: 40824124 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


859. BMC Med Educ. 2025 Feb 11;25(1):225. doi: 10.1186/s12909-024-06344-8.

Determining the influence of an intervention of stress management on medical 
students' levels of psychophysiological stress: the protocol of the 
PROMESS-Stress clinical trial.

Métais A(1)(2), Omarjee M(3)(4), Valero B(5), Gleich A(6), Mekki A(6), Henry 
A(6), Duclos A(7), Lilot M(7)(8)(9), Rode G(6)(10), Schlatter S(7)(6)(8).

Author information:
(1)Research on Healthcare Performance RESHAPE, INSERM U1290, Université Claude 
Bernard Lyon 1, Lyon, France. angele.metais@univ-lyon1.fr.
(2)Faculté de Médecine Lyon Est, Université Claude Bernard Lyon 1, Lyon, France. 
angele.metais@univ-lyon1.fr.
(3)Hospices Civils de Lyon, Unit of Aging Medicine, Edouard Herriot Hospital, 
Lyon, France.
(4)Faculté de Médecine Lyon Sud, Université Claude Bernard Lyon 1, Lyon, France.
(5)Département des Sciences Humaines et Sociales, Centre Léon Bérard, Lyon, 
France.
(6)Faculté de Médecine Lyon Est, Université Claude Bernard Lyon 1, Lyon, France.
(7)Research on Healthcare Performance RESHAPE, INSERM U1290, Université Claude 
Bernard Lyon 1, Lyon, France.
(8)Hospices Civils of Lyon, University Claude Bernard Lyon 1, Healthcare 
Simulation Center (Centre Lyonnais d'Enseignement par Simulation en Santé, 
CLESS), SIMULYON, Lyon, France.
(9)Hospices Civils of Lyon, Pediatric Cardiac Thoracic Vascular Anesthesia and 
Intensive Care Unit 11 Medical-Surgical Department of Congenital Cardiology of 
the Fetus, Child and Adult, Louis Pradel Hospital, Bron, France.
(10)Université Claude Bernard Lyon 1, CNRS, INSERM, Centre de Recherche en 
Neurosciences de Lyon CRNL U1028 UMR5292, TRAJECTOIRES, Bron, F-69500, France.

BACKGROUND: Medical students face highly competitive stressful situations 
throughout their curriculum, which can lead to elevated stress levels and a 
major decline in quality of life, well-being, learning abilities, and health. It 
is crucial to assist medical students in coping with these stressful situations 
during their curriculum.
METHODS: The PROMESS-Stress clinical trial aims to support future healthcare 
professionals by enhancing their abilities to manage stressful situations. The 
support will be provided through a 3-session stress management program. Each 
session will include an individual meeting between a PROMESS-Stress expert and a 
medical student. To reduce stress levels and enhance coping mechanisms, these 
sessions will focus on establishing personalized advice and goals. The present 
protocol is designed to assess the influence of this program on forty-five 
undergraduate medical students (4th and 5th-year) of the Lyon-Est Faculty of 
Medicine (Claude Bernard University Lyon 1, France). Assessments of 
psychological and physiological stress variables will be conducted before and 
during the intervention. At the end of the 3rd session the student's levels of 
satisfaction will be assessed. The primary outcome will be changes in scores on 
the Perceived Stress Scale (PSS), while secondary outcomes will provide a 
detailed characterization of the intervention's effects on stress coping 
behaviors, psychological and physiological stress variables. Exploratory 
outcomes will provide information regarding the student's level of satisfaction 
and will determine the moderators of the program's efficacy. Data will be 
analyzed according to the intention-to-treat principle and presented in 
accordance with the CONSORT Guidelines. Ethical approval has been obtained by 
the Institutional Review Board (IRB: 2023-07-04-02) and all the procedures will 
be performed in adherence to the Helsinki declaration. Results from this study 
will be presented at scientific conferences and in peer-reviewed scientific 
journals.
DISCUSSION: Results will provide valuable insights into the program's efficacy 
in reducing stress and improving coping abilities. If its efficacy is proven, 
PROMESS-Stress could become an integral and sustainable part of medical 
education, fostering a healthier and more resilient future for healthcare 
professionals. This manuscript follows the SPIRIT guidelines (Additional files 1 
& 6).
TRIAL REGISTRATION: ClinicalTrials.gov: NCT06295133; retrospectively registered.

© 2025. The Author(s).

DOI: 10.1186/s12909-024-06344-8
PMCID: PMC11818132
PMID: 39934759 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: All participants will give consent to participate in the study in 
agreement with the ethical approval of the present study (Institutional Review 
Board IRB: 2023-07-04-02) and all the procedures will be performed in adherence 
to the Helsinki declaration. Participants will receive oral and written 
information, and will provide a written consent prior to enrollment in the study 
following a sufficient reflection time. Consent for publication: All authors 
gave their consent to publish the present article. Competing interests: The 
authors declare that they have no competing interests.


860. J Clin Endocrinol Metab. 2026 Jan 21;111(2):316-324. doi: 
10.1210/clinem/dgaf530.

Artificial Intelligence Applications in Thyroid Cancer Care.

Pozdeyev N(1)(2)(3)(4), White SL(1), Bell CC(2), Haugen BR(2)(3), Thomas J(5).

Author information:
(1)Department of Biomedical Informatics, University of Colorado Anschutz, 
Aurora, CO 80045, USA.
(2)Division of Endocrinology, Diabetes and Metabolism, University of Colorado 
Anschutz, Aurora, CO 80045, USA.
(3)University of Colorado Cancer Center, University of Colorado Anschutz, 
Aurora, CO 80045, USA.
(4)Colorado Center for Personalized Medicine, University of Colorado Anschutz, 
Aurora, CO 80045, USA.
(5)Division of Endocrinology, Mercy Hospital, Springfield, MO 65807, USA.

CONTEXT: Artificial intelligence (AI) has created tremendous opportunities to 
improve thyroid cancer care.
EVIDENCE ACQUISITION: We used the "artificial intelligence thyroid cancer" query 
to search the PubMed database until May 31, 2025. We highlight a set of 
high-impact publications selected based on technical innovation, large 
generalizable training datasets, and independent and/or prospective validation 
of AI.
EVIDENCE SYNTHESIS: We review the key applications of AI for diagnosing and 
managing thyroid cancer. Our primary focus is on using computer vision to 
evaluate thyroid nodules on thyroid ultrasound, an area of thyroid AI that has 
gained the most attention from researchers and will likely have a significant 
clinical impact. We also highlight AI for detecting and predicting thyroid 
cancer neck lymph node metastases, digital cyto- and histopathology, large 
language models for unstructured data analysis, patient education, and other 
clinical applications. We discuss how thyroid AI technology has evolved and cite 
the most impactful research studies. Finally, we balance our excitement about 
the potential of AI to improve clinical care for thyroid cancer with current 
limitations, such as the lack of high-quality, independent prospective 
validation of AI in clinical trials, the uncertain added value of AI software, 
unknown performance on non-papillary thyroid cancer types, and the complexity of 
clinical implementation.
CONCLUSION: AI promises to improve thyroid cancer diagnosis, reduce health care 
costs and enable personalized management. High-quality, independent prospective 
validation of AI in clinical trials is lacking and is necessary for the clinical 
community's broad adoption of this technology.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
Endocrine Society. All rights reserved. For commercial re-use, please contact 
reprints@oup.com for reprints and translation rights for reprints. All other 
permissions can be obtained through our RightsLink service via the Permissions 
link on the article page on our site—for further information please contact 
journals.permissions@oup.com. See the journal About page for additional terms.

DOI: 10.1210/clinem/dgaf530
PMID: 40997161 [Indexed for MEDLINE]


861. J Med Internet Res. 2025 Jul 2;27:e65226. doi: 10.2196/65226.

Use of Large Language Models to Classify Epidemiological Characteristics in 
Synthetic and Real-World Social Media Posts About Conjunctivitis Outbreaks: 
Infodemiology Study.

Deiner MS(1)(2), Deiner RY(3), Fathy C(4), Deiner NA(5), Hristidis V(6), McLeod 
SD(1)(2)(7), Bukowski TJ(1), Doan T(1)(2), Seitzman GD(1)(2), Lietman 
TM(1)(2)(8), Porco TC(1)(2)(8).

Author information:
(1)Francis I Proctor Foundation for Research in Ophthalmology, University of 
California, San Francisco, San Francisco, CA, United States.
(2)Department of Ophthalmology, University of California, San Francisco, San 
Francisco, CA, United States.
(3)Abraham Lincoln High School, San Francisco, CA, San Francisco, CA, United 
States.
(4)Document Control Center, Food and Drug Administration, Center for Biologics 
Evaluation and Research, Silver Spring, CA, United States.
(5)College of Letters and Sciences, University of California, Santa Barbara, 
Santa Barbara, CA, United States.
(6)Department of Computer Science and Engineering, University of California, 
Riverside, Riverside, CA, United States.
(7)American Academy of Ophthalmology, San Francisco, CA, United States.
(8)Department of Epidemiology and Biostatistics, University of California, San 
Francisco, San Francisco, CA, United States.

BACKGROUND: The use of web-based search and social media can help identify 
epidemics, potentially earlier than clinical methods or even potentially 
identifying unreported outbreaks. Monitoring for eye-related epidemics, such as 
conjunctivitis outbreaks, can facilitate early public health intervention to 
reduce transmission and ocular comorbidities. However, monitoring social media 
content for conjunctivitis outbreaks is costly and laborious. Large language 
models (LLMs) could overcome these barriers by assessing the likelihood that 
real-world outbreaks are being described. However, public health actions for 
likely outbreaks could benefit more by knowing additional epidemiological 
characteristics, such as outbreak type, size, and severity.
OBJECTIVE: We aimed to assess whether and how well LLMs can classify 
epidemiological features from social media posts beyond conjunctivitis outbreak 
probability, including outbreak type, size, severity, etiology, and community 
setting. We used a validation framework comparing LLM classifications to those 
of other LLMs and human experts.
METHODS: We wrote code to generate synthetic conjunctivitis outbreak social 
media posts, embedded with specific preclassified epidemiological features to 
simulate various infectious eye disease outbreak and control scenarios. We used 
these posts to develop effective LLM prompts and test the capabilities of 
multiple LLMs. For top-performing LLMs, we gauged their practical utility in 
real-world epidemiological surveillance by comparing their assessments of 
Twitter/X, forum, and YouTube conjunctivitis posts. Finally, human raters also 
classified the posts, and we compared their classifications to those of a 
leading LLM for validation. Comparisons entailed correlation or sensitivity and 
specificity statistics.
RESULTS: We assessed 7 LLMs for effectively classifying epidemiological data 
from 1152 synthetic posts, 370 Twitter/X posts, 290 forum posts, and 956 YouTube 
posts. Despite some discrepancies, the LLMs demonstrated a reliable capacity for 
nuanced epidemiological analysis across various data sources and compared to 
humans or between LLMs. Notably, GPT-4 and Mixtral 8x22b exhibited high 
performance, predicting conjunctivitis outbreak characteristics such as 
probability (GPT-4: correlation=0.73), size (Mixtral 8x22b: correlation=0.82), 
and type (infectious, allergic, or environmentally caused); however, there were 
notable exceptions. Assessing synthetic and real-world posts for etiological 
factors, infectious eye disease specialist validations revealed that GPT-4 had 
high specificity (0.83-1.00) but variable sensitivity (0.32-0.71). Interrater 
reliability analyses showed that LLM-expert agreement exceeded expert-expert 
agreement for severity assessment (intraclass correlation coefficient=0.69 vs 
0.38), while agreement varied by condition type (κ=0.37-0.94).
CONCLUSIONS: This investigation into the potential of LLMs for public health 
infoveillance suggests effectiveness in classifying key epidemiological 
characteristics from social media content about conjunctivitis outbreaks. Future 
studies should further explore LLMs' potential to support public health 
monitoring through the automated assessment and classification of potential 
infectious eye disease or other outbreaks. Their optimal role may be to act as a 
first line of documentation, alerting public health organizations for the 
follow-up of LLM-detected and -classified small, early outbreaks, with a focus 
on the most severe ones.

©Michael S Deiner, Russell Y Deiner, Cherie Fathy, Natalie A Deiner, Vagelis 
Hristidis, Stephen D McLeod, Thomas J Bukowski, Thuy Doan, Gerami D Seitzman, 
Thomas M Lietman, Travis C Porco. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org), 02.07.2025.

DOI: 10.2196/65226
PMCID: PMC12268217
PMID: 40601927 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


862. J Med Internet Res. 2025 Mar 10;27:e59792. doi: 10.2196/59792.

Generative AI Models in Time-Varying Biomedical Data: Scoping Review.

He R(1)(2), Sarwal V(1)(2), Qiu X(3), Zhuang Y(4), Zhang L(5), Liu Y(6), Chiang 
J(2)(7).

Author information:
(1)Department of Computer Science, University of California, Los Angeles, Los 
Angeles, CA, United States.
(2)Department of Computational Medicine, University of California, Los Angeles, 
Los Angeles, CA, United States.
(3)Division of Biomedical Sciences, School of Medicine, University of California 
Riverside, Riverside, CA, United States.
(4)Department of Biostatistics, University of Michigan, Ann Arbor, MI, United 
States.
(5)Institute for Integrative Genome Biology, University of California Riverside, 
Riverside, CA, United States.
(6)Institute for Cellular and Molecular Biology, University of Texas at Austin, 
Austin, TX, United States.
(7)Department of Neurosurgery, David Geffen School of Medicine, University of 
California, Los Angeles, Los Angeles, CA, United States.

Erratum in
    J Med Internet Res. 2025 Jul 25;27:e79605. doi: 10.2196/79605.

BACKGROUND: Trajectory modeling is a long-standing challenge in the application 
of computational methods to health care. In the age of big data, traditional 
statistical and machine learning methods do not achieve satisfactory results as 
they often fail to capture the complex underlying distributions of multimodal 
health data and long-term dependencies throughout medical histories. Recent 
advances in generative artificial intelligence (AI) have provided powerful tools 
to represent complex distributions and patterns with minimal underlying 
assumptions, with major impact in fields such as finance and environmental 
sciences, prompting researchers to apply these methods for disease modeling in 
health care.
OBJECTIVE: While AI methods have proven powerful, their application in clinical 
practice remains limited due to their highly complex nature. The proliferation 
of AI algorithms also poses a significant challenge for nondevelopers to track 
and incorporate these advances into clinical research and application. In this 
paper, we introduce basic concepts in generative AI and discuss current 
algorithms and how they can be applied to health care for practitioners with 
little background in computer science.
METHODS: We surveyed peer-reviewed papers on generative AI models with specific 
applications to time-series health data. Our search included single- and 
multimodal generative AI models that operated over structured and unstructured 
data, physiological waveforms, medical imaging, and multi-omics data. We 
introduce current generative AI methods, review their applications, and discuss 
their limitations and future directions in each data modality.
RESULTS: We followed the PRISMA-ScR (Preferred Reporting Items for Systematic 
Reviews and Meta-Analyses extension for Scoping Reviews) guidelines and reviewed 
155 articles on generative AI applications to time-series health care data 
across modalities. Furthermore, we offer a systematic framework for clinicians 
to easily identify suitable AI methods for their data and task at hand.
CONCLUSIONS: We reviewed and critiqued existing applications of generative AI to 
time-series health data with the aim of bridging the gap between computational 
methods and clinical application. We also identified the shortcomings of 
existing approaches and highlighted recent advances in generative AI that 
represent promising directions for health care modeling.

©Rosemary He, Varuni Sarwal, Xinru Qiu, Yongwen Zhuang, Le Zhang, Yue Liu, 
Jeffrey Chiang. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 10.03.2025.

DOI: 10.2196/59792
PMCID: PMC11933772
PMID: 40063929 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


863. J Alzheimers Dis. 2026 Jan 20:13872877261415635. doi: 10.1177/13872877261415635. 
Online ahead of print.

Comparative evaluation of large language models in retrieving known and 
predicting novel drug combinations.

Wang EJ(1)(2), Oğuztüzün Ç(1), Xu R(1), Gao Z(1).

Author information:
(1)Center for Artificial Intelligence in Drug Discovery, School of Medicine, 
Case Western Reserve University, Cleveland, OH, USA.
(2)College of Art and Sciences, Emory University, Atlanta, GA, USA.

BackgroundLarge language models (LLMs) are increasingly used in the biomedical 
field for information retrieval, information extraction and knowledge discovery. 
However, their potential in retrieving and discovering drug combinations for 
diseases remains underexplored.ObjectiveThis study aims to evaluate the 
effectiveness of LLMs in retrieving known drug combinations and to identify 
novel drug combinations for treating Alzheimer's disease (AD).MethodsWe 
developed a series of prompts to guide LLMs in retrieving drug combinations. 
Their performance was evaluated using both FDA-approved combinations and 
combinations identified through PubMed literature mining. We then assessed the 
feasibility of identifying novel drug combination candidates for AD. In 
collaboration with domain experts, we performed pathway enrichment analyses to 
evaluate their potential mechanisms of action within the context of AD.ResultsIn 
a comparative evaluation of multiple LLMs, GPT-5 demonstrated the strongest 
overall performance, achieving an accuracy of 0.95 and a balanced F1 score of 
0.95 in identifying FDA-approved drug combinations. Among the top 10 
drug-combination candidates for AD treatment suggested by GPT-5, the combination 
of donepezil and memantine is already FDA-approved. Three other combinations 
have been tested in AD clinical trials, and three have supporting evidence in 
the literature. We also identified 10 off-label drug combinations, with pathway 
enrichment analyses indicating that several target key AD-related biological 
pathways.ConclusionsLLMs is effective in retrieving drug combinations for a 
given disease and the performance varies among different language models with 
best performance for GPT-5. However, the suggestions from LLM models require 
further validation to be considered reliable.

DOI: 10.1177/13872877261415635
PMID: 41555825


864. J Med Internet Res. 2025 Oct 27;27:e79174. doi: 10.2196/79174.

Carbon Reporting Practices in the NHS: Emissions and Omissions Relating to 
Artificial Intelligence.

Reynolds DJ(1).

Author information:
(1)Wolfson Institute of Population Health, Queen Mary University of London, 
London, United Kingdom.

Artificial intelligence (AI) is being rolled out across the UK National Health 
Service (NHS) to improve efficiency; yet, its carbon footprint is largely 
invisible within mandatory Green Plan reporting. This work shows where NHS 
carbon reporting omits AI-related emissions and proposes feasible accounting and 
procurement measures that allow trusts to assess whether AI adoption advances or 
undermines net zero. A review of NHS sustainability guidance, the Department for 
Environment, Food & Rural Affairs conversion factors, and recent evidence on AI 
energy use shows that current Scopes 1-3 accounting omits substantial emissions 
at 3 points. First, a lack of granularity provides averages that can obscure the 
extreme energy intensity of certain AI workloads. Second, life-cycle emissions 
from specialized hardware (eg, graphics processing units) are often excluded 
unless trusts own the equipment, ignoring upstream manufacturing impacts. Third, 
widespread use of unprocured generative AI tools is unmeasured; extrapolating 
general practice survey data suggests that ChatGPT queries alone could release ≈ 
349t CO₂e per year in primary care. To close these gaps, we propose three 
potential ways to help reduce these reporting gaps: (1) AI-specific carbon 
disclosure clauses in vendor contracts, (2) inclusion of cradle-to-grave 
emission factors for AI hardware in Scope 3 reporting, and (3) lightweight 
monitoring of external AI traffic (while recognizing potential ethical issues 
with this). Implementing these measures would give health care leaders a more 
accurate baseline against which to judge whether AI supports or undermines the 
NHS net-zero target.

©Duncan J Reynolds. Originally published in the Journal of Medical Internet 
Research (https://www.jmir.org), 27.10.2025.

DOI: 10.2196/79174
PMCID: PMC12603583
PMID: 41144961 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


865. IEEE J Biomed Health Inform. 2025 Dec;29(12):8767-8774. doi: 
10.1109/JBHI.2025.3624144.

Personalized Causal Graph Reasoning for LLMs: An Implementation for Dietary 
Recommendations.

Yang Z, Rahmani AM.

Large Language Models (LLMs) excel at general-purpose reasoning by leveraging 
broad commonsense knowledge, but they remain limited in tasks requiring 
personalized reasoning over multifactorial personal data. This limitation 
constrains their applicability in domains such as healthcare, where decisions 
must adapt to individual contexts. We introduce Personalized Causal Graph 
Reasoning, a framework that enables LLMs to reason over individual-specific 
causal graphs constructed from longitudinal data. Each graph encodes how 
user-specific factors influence targeted outcomes. In response to a query, the 
LLM traverses the graph to identify relevant causal pathways, rank them by 
estimated impact, simulate potential outcomes, and generate tailored responses. 
We implement this framework in the context of nutrient-oriented dietary 
recommendations, where variability in metabolic responses demands personalized 
reasoning. Using counterfactual evaluation, we assess the effectiveness of 
LLM-generated food suggestions for glucose control. Our method reduces 
postprandial glucose iAUC across three time windows compared to prior 
approaches. Additional LLM-as-a-judge evaluations further confirm improvements 
in personalization quality.

DOI: 10.1109/JBHI.2025.3624144
PMID: 41359706 [Indexed for MEDLINE]


866. BMC Med Educ. 2025 Feb 19;25(1):267. doi: 10.1186/s12909-024-06422-x.

Determining the influence of a sleep improvement intervention on medical 
students' sleep and fatigue: protocol of the PROMESS-Sleep clinical trial.

Ruet A(#)(1), Ndiki Mayi EF(#)(2), Métais A(1)(3), Valero B(4), Henry A(1), 
Duclos A(3), Lilot M(1)(3)(5)(6), Rode G(1)(7), Schlatter S(8)(9)(10).

Author information:
(1)Faculté de Médecine Lyon Est, Université Claude Bernard Lyon 1, Lyon, France.
(2)Faculté de Médecine Lyon Sud, Université Claude Bernard Lyon 1, Lyon, France.
(3)Research on Healthcare Performance (RESHAPE), INSERM U1290, Université Claude 
Bernard Lyon 1, Lyon, France.
(4)Département des Sciences Humaines et Sociales, Centre Léon Bérard, Lyon, 
France.
(5)Hospices Civils of Lyon, University Claude Bernard Lyon 1, Healthcare 
Simulation Center (Centre Lyonnais d'Enseignement par Simulation en Santé, 
CLESS), SIMULYON, Lyon, France.
(6)Hospices Civils of Lyon, Pediatric Cardiac Thoracic Vascular Anesthesia and 
Intensive Care Unit 11, Medical-Surgical Department of Congenital Cardiology of 
the Fetus, Child and Adult, Louis Pradel Hospital, Lyon, France.
(7)Université Claude Bernard Lyon 1, CNRS, INSERM, Centre de Recherche en 
Neurosciences de Lyon, CRNL U1028 UMR5292, TRAJECTOIRES, Bron, F-69500, France.
(8)Faculté de Médecine Lyon Est, Université Claude Bernard Lyon 1, Lyon, France. 
sophie.schlatter@univ-lyon1.fr.
(9)Research on Healthcare Performance (RESHAPE), INSERM U1290, Université Claude 
Bernard Lyon 1, Lyon, France. sophie.schlatter@univ-lyon1.fr.
(10)Hospices Civils of Lyon, University Claude Bernard Lyon 1, Healthcare 
Simulation Center (Centre Lyonnais d'Enseignement par Simulation en Santé, 
CLESS), SIMULYON, Lyon, France. sophie.schlatter@univ-lyon1.fr.
(#)Contributed equally

BACKGROUND: Medical students face a demanding workload, stressful situations, 
and irregular sleep patterns, which can lead to elevated sleep disturbances and 
high fatigue levels. These difficulties may be further associated with a major 
decline in well-being, quality of life, performance, and health. Thus, these 
struggles must be addressed to reduce these students' sleep disturbances and 
fatigue during their curriculum.
METHODS: The PROMESS-Sleep clinical trial aims to support future healthcare 
professionals by enhancing their abilities to manage their sleep. The support 
will be provided through a three-session sleep management program. Each session 
will include an individual meeting between a PROMESS-Sleep expert and a medical 
student, during which self-care education, advice, and personalized goals will 
be established. The present protocol is designed to assess the influence of this 
program on 45 undergraduate medical students (fourth- and fifth-year) of the 
Lyon-Est Faculty of Medicine (Claude Bernard University Lyon 1, France). 
Assessments of sleep and fatigue will be conducted before and during the 
intervention using self-reported questionnaires and actigraphy. At the end of 
the third session, the student's satisfaction levels regarding the program will 
be assessed. The primary outcome will be changes in scores on the Pittsburgh 
Sleep Quality Index (PSQI) during the program. Secondary outcomes will provide a 
detailed characterization of changes in various aspects of sleep disturbances, 
fatigue, sleep habits, and sleep-wake rhythms. Exploratory outcomes will provide 
information regarding the students' satisfaction levels and will determine the 
moderators of the program's efficacy. Data will be analyzed according to the 
intention-to-treat principle and presented in accordance with the CONSORT 
Guidelines. Ethical approval has been obtained by the Institutional Review Board 
(IRB: 2023-07-04-03), and all procedures will be performed in adherence to the 
Helsinki Declaration. The results from this study will be presented at 
scientific conferences and in peer-reviewed scientific journals.
DISCUSSION: The results will provide valuable insights into the program's 
efficacy in reducing sleep disturbances and fatigue. If its efficacy is proven, 
PROMESS-Sleep could become an integral and sustainable part of medical education 
due to fostering a healthier and more resilient future for healthcare 
professionals. This manuscript follows the SPIRIT guidelines (Additional files 1 
& 8).
TRIAL REGISTRATION: ClinicalTrials.gov: NCT06297330; retrospectively registered.

© 2025. The Author(s).

DOI: 10.1186/s12909-024-06422-x
PMCID: PMC11841311
PMID: 39972313 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: All participants will consent to participate in agreement with the 
ethical approval of the present study (Institutional Review Board IRB), and all 
the procedures will be performed in adherence to the Helsinki Declaration. 
Participants will receive oral and written information and provide written 
consent before enrollment in the study following sufficient reflection time. 
Consent for publication: All authors gave their consent to publish the present 
article. Competing interests: The authors declare no competing interests.


867. Br J Anaesth. 2025 May;134(5):1333-1340. doi: 10.1016/j.bja.2024.08.038. Epub 
2024 Nov 14.

Generation of preoperative anaesthetic plans by ChatGPT-4.0: a mixed-method 
study.

Abdel Malek M(1), van Velzen M(2), Dahan A(2), Martini C(2), Sitsen E(2), Sarton 
E(2), Boon M(2).

Author information:
(1)Department of Anaesthesiology, Leiden University Medical Centre, Leiden, The 
Netherlands. Electronic address: m.abdel_malek@lumc.nl.
(2)Department of Anaesthesiology, Leiden University Medical Centre, Leiden, The 
Netherlands.

BACKGROUND: Recent advances in artificial intelligence (AI) have enabled 
development of natural language algorithms capable of generating coherent texts. 
We evaluated the quality, validity, and safety of this generative AI in 
preoperative anaesthetic planning.
METHODS: In this exploratory, single-centre, convergent mixed-method study, 10 
clinical vignettes were randomly selected, and ChatGPT (OpenAI, 4.0) was 
prompted to create anaesthetic plans, including cardiopulmonary risk assessment, 
intraoperative anaesthesia technique, and postoperative management. A 
quantitative assessment compared these plans with those made by eight senior 
anaesthesia consultants. A qualitative assessment was performed by an 
adjudication committee through focus group discussion and thematic analysis. 
Agreement on cardiopulmonary risk assessment was calculated using weighted 
Kappa, with descriptive data representation for other outcomes.
RESULTS: ChatGPT anaesthetic plans showed variable agreement with consultants' 
plans. ChatGPT, the survey panel, and adjudication committee frequently 
disagreed on cardiopulmonary risk estimation. The ChatGPT answers were 
repetitive and lacked variety, evidenced by the strong preference for general 
anaesthesia and absence of locoregional techniques. It also showed inconsistent 
choices regarding airway management, postoperative analgesia, and medication 
use. While some differences were not deemed clinically significant, subpar 
postoperative pain management advice and failure to recommend tracheal 
intubation for patients at high risk for pulmonary aspiration were considered 
inappropriate recommendations.
CONCLUSIONS: Preoperative anaesthetic plans generated by ChatGPT did not 
consistently meet minimum clinical standards and were unlikely the result of 
clinical reasoning. Therefore, ChatGPT is currently not recommended for 
preoperative planning. Future large language models trained on 
anaesthesia-specific datasets might improve performance but should undergo 
vigorous evaluation before use in clinical practice.

Copyright © 2024 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.bja.2024.08.038
PMID: 39547871 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of interest MAM is founder of 
medical data platform Delphyr, which is unrelated to this work. All other 
authors declare that they have no conflicts of interest.


868. J Med Internet Res. 2025 Dec 11;27:e84918. doi: 10.2196/84918.

Factors Influencing Adoption of Large Language Models in Health Care: 
Multicenter Cross-Sectional Mixed Methods Observational Study.

Yang X(#)(1)(2), Xiao Y(3), Liu D(1)(2), Deng H(4), Huang J(5), Zhou Y(6), Liang 
M(2)(7), Dong L(8), Yuan Z(8), Yao J(1)(2), Guo W(1)(2), Xu C(#)(1)(2).

Author information:
(1)Department of Thoracic Surgery, Guizhou Provincial People's Hospital, No. 83, 
Zhongshan East Road, Guiyang, Guizhou, 550000, China, 86 18620726507.
(2)NHC Key Laboratory of Pulmonary Immunological Diseases, Guizhou Provincial 
People's Hospital, Guiyang, Guizhou, China, 1 18620726507.
(3)Department of Cardio-Thoracic Surgery, Third Affiliated Hospital of Sun 
Yat-sen University, Guangzhou, Guangdong, China.
(4)Department of Anesthesiology, Third Xiangya Hospital, Central South 
University, Changsha, Hunan, China.
(5)Department of Thoracic Surgery, Jiangxi Provincial Cancer Hospital, Nanchang, 
Jiangxi, China.
(6)Department of Dermatology, University of Hong Kong-Shenzhen Hospital, 
Shenzhen, Guangdong, Guizhou, China.
(7)Department of Respiratory Medicine, Guizhou Provincial People's Hospital, 
Guiyang, Guizhou, China.
(8)The Second Clinical Medical College, Guangdong Medical University, Dongguan, 
Guangdong, China.
(#)Contributed equally

BACKGROUND: Large language models (LLMs) such as ChatGPT are transforming how 
health information is accessed, communicated, and applied. However, their 
adoption in health care remains limited by uncertainties surrounding trust, 
privacy, and digital readiness, particularly in low- and middle-income contexts.
OBJECTIVE: This study aimed to examine how trust, information behavior, and 
sociotechnical readiness influence the willingness of health care professionals 
(HCPs) and patients or caregivers (PCs) in China to adopt LLMs for medical 
information and decision support.
METHODS: We conducted a multicenter, cross-sectional mixed methods observational 
study across five tertiary hospitals, combining surveys of 240 HCPs and 480 PCs 
with semistructured interviews (n=30). Quantitative analyses included logistic 
regression (LR), random forest (RF), and extreme gradient boosting models with 
Shapley additive explanations-based interpretability. Qualitative data were 
thematically analyzed to capture role-specific concerns and expectations.
Among HCPs, mean age 39.9 (SD 6.5 years; 159/240, 66.2% physicians), 69.2% 
(166/240) were aware of LLMs and 36.7% (88/240) had previous experience. Among 
PCs (mean age 50.1, SD 12.6 years; 242/480, 50.4% male), only 26% (125/480) had 
previous exposure. Trust, perceived usefulness, and digital readiness were the 
strongest facilitators of adoption. Multivariable models identified trust as the 
dominant predictor for both groups (HCPs: odds ratio [OR] 3.78, 95% CI 
2.15-6.63; PCs: OR 36.34, 95% CI 18.41-71.74; P<.001). For HCPs, previous use 
(OR 5.61, 95% CI 3.02-10.44; P<.001) and legal clarity (OR 1.56, 95% CI 
1.07-2.27; P=.02) increased willingness, while privacy concerns reduced it (OR 
0.72, 95% CI 0.53-0.97; P=.03). Among PCs, perceived usefulness (OR 2.01, 95% CI 
1.52-2.67; P<.001), education, and digital tool use were positive predictors. 
Model performance was high (area under the receiver operating characteristic 
curve [AUC] 0.83-0.85 for HCPs and 0.93-0.96 for PCs). Qualitative findings 
identified 11 themes: HCPs stressed workflow integration and accountability, 
while PCs emphasized comprehensibility, reassurance, and equitable access; trust 
consistently linked technical credibility with social legitimacy.
CONCLUSIONS: Adoption of LLMs in health care depends less on algorithmic 
performance than on the management of trust, literacy, and institutional 
readiness. Trust functions as a multidimensional construct rooted in 
transparency, reliability, and contextual validation. Theoretically, this study 
extends technology adoption frameworks by embedding ethical trust, digital 
literacy, and institutional support within a unified sociotechnical readiness 
model, advancing information management theory beyond performance-centric 
paradigms. Empirically, trust and perceived usefulness outweighed demographic or 
structural factors, with predictive accuracy exceeding 0.9 across user groups. 
Practically, these findings offer actionable guidance for the design and 
governance of artificial intelligence systems, emphasizing role-sensitive 
interfaces, plain-language communication, and transparent accountability 
mechanisms to promote equitable and trustworthy adoption.

© Xiongwen Yang, Yi Xiao, Di Liu, Huiyin Deng, Jian Huang, Yubin Zhou, Maoli 
Liang, Longyan Dong, Zihao Yuan, Jing Yao, Wankai Guo, Chuan Xu. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/84918
PMCID: PMC12697921
PMID: 41380031 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


869. Arthroscopy. 2025 Feb;41(2):276-278. doi: 10.1016/j.arthro.2024.09.047. Epub 
2024 Oct 3.

Editorial Commentary: Off-the-Shelf Large Language Models Are of Insufficient 
Quality to Provide Medical Treatment Recommendations, While Customization of 
Large Language Models Results in Quality Recommendations.

Ramkumar PN(1), Masotto AF(1), Woo JJ(1).

Author information:
(1)Commons Clinic (A.F.M., J.J.W.); The Warren Alpert Medical School of Brown 
University (J.J.W.).

The content accuracy of off-the-shelf large language models (LLMs) mirrors the 
content accuracy of the unregulated Internet from which these generative 
artificial intelligence models are supplied. With error rates approximating 30% 
in terms of treatment recommendations for the management of common 
musculoskeletal conditions, seeking expert opinion remains paramount. However, 
custom LLMs represent an excellent opportunity to infuse niche, bespoke 
expertise from the many specialties and subspecialties within medicine. Methods 
of customizing these generative models broadly fall under the categories of 
prompt engineering; "retrieval-augmented generation" prioritizing retrieval of 
relevant information from a specific domain of data; "fine-tuning" of a basic 
pretrained model into one that is refined for health care-related vernacular and 
acronyms; and "agentic augmentation" including software that breaks down complex 
tasks into smaller ones, recruiting multiple LLMs (with or without 
retrieval-augmented generation), optimizing the output, internally deciding 
whether the response is appropriate or sufficient, and even passing on an unmet 
outcome to a human for supervision ("phone a friend"). Custom LLMs offer 
physicians and their associated organizations the rare opportunity to regain 
control of our profession by re-establishing authority in our increasingly 
digital landscape.

Copyright © 2024 Arthroscopy Association of North America. Published by Elsevier 
Inc. All rights reserved.

DOI: 10.1016/j.arthro.2024.09.047
PMID: 39368620 [Indexed for MEDLINE]

Conflict of interest statement: Disclosures The authors declare the following 
financial interests/personal relationships which may be considered as potential 
competing interests: P.N.R. receives consulting fees from Stryker; owns stock or 
stock options in Intelligent Health Analytics; is Associate Editor of 
Arthroscopy; and is on the editorial board of Journal of Arthroplasty. J.J.W. 
owns stock or stock options in Intelligent Health Analytics. A.F.M. declares 
that he has no known competing financial interests or personal relationships 
that could have appeared to influence the work reported in this letter.


870. J Med Internet Res. 2025 Oct 21;27:e80557. doi: 10.2196/80557.

Improving Large Language Model Applications in the Medical and Nursing Domains 
With Retrieval-Augmented Generation: Scoping Review.

Miao Y(#)(1), Zhao Y(#)(1), Luo Y(1), Wang H(1), Wu Y(1)(2).

Author information:
(1)School of Nursing, Capital Medical University, Bejing, China.
(2)The Chinese Institutes for Medical Research, Bejing, China.
(#)Contributed equally

BACKGROUND: Retrieval-augmented generation (RAG) is increasingly used to improve 
large language models in the medical and nursing domains. However, a 
comprehensive understanding of its specific architecture and applications in 
medical and nursing reasoning remains limited.
OBJECTIVE: We aimed to summarize the current state, existing limitations, and 
future development directions of RAG in the medical and nursing domains.
METHODS: The PubMed, Web of Science, IEEE Xplore, and arXiv databases were 
searched for relevant articles using queries that combined terms related to RAG, 
medical, and nursing domains, covering the period from November 1, 2022, to May 
31, 2025. This review was conducted following the PRISMA-ScR (Preferred 
Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping 
Reviews) guidelines.
RESULTS: A total of 917 articles were retrieved, of which 67 met the inclusion 
criteria. Most studies focused on the medical domain (63/67, 94%), while only a 
few addressed nursing applications (4/67, 6%). The RAG frameworks included in 
this review were categorized into 5 functional types: text-based RAG (36/67, 
54%), knowledge graph-enhanced RAG (17/67, 25%), agentic RAG (6/67, 9%), 
multimodal RAG (2/67, 3%), and plug-and-play RAG (6/67, 9%). On the basis of the 
Simon decision-making process theory, we divided the RAG workflow into 4 stages: 
intent recognition, knowledge retrieval, knowledge integration, and generation. 
Only 26 studies included explicit reasoning support, and few were aligned with 
real-world clinical workflows. Only 12 studies attempted to address ethical 
considerations related to RAG.
CONCLUSIONS: We identified 4 key shifts in recent RAG development: shifting from 
surface-level matching toward contextualized intent recognition, from vague 
semantics toward logic-driven dynamic retrieval, from passive toward active 
knowledge retrieval, and from simple aggregation toward coherent context 
construction. However, most RAG systems in the medical and nursing domains have 
not yet introduced reasoning methods, and those that have are still 
predominantly reliant on data‑driven associations without causal modeling. This 
highlights the need to integrate causal mechanisms for more effective and 
domain-relevant reasoning in health care.
TRIAL REGISTRATION: OSF Registries 10.17605/OSF.IO/WBSV5; https://osf.io/wbsv5.

©Yiqun Miao, Yuhan Zhao, Yuan Luo, Huiying Wang, Ying Wu. Originally published 
in the Journal of Medical Internet Research (https://www.jmir.org), 21.10.2025.

DOI: 10.2196/80557
PMCID: PMC12587015
PMID: 41118646 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


871. J Med Internet Res. 2025 Apr 1;27:e70789. doi: 10.2196/70789.

Adoption of Large Language Model AI Tools in Everyday Tasks: Multisite 
Cross-Sectional Qualitative Study of Chinese Hospital Administrators.

Chen J(1), Liu Y(2), Liu P(1), Zhao Y(1), Zuo Y(3)(4)(5), Duan H(6).

Author information:
(1)Medical Services Management Department, Peking University People's Hospital, 
Beijing, China.
(2)Department of Obstetrics and Gynecology, Peking University Shenzhen Hospital, 
Shenzhen, China.
(3)Department of Gynecology and Obstetrics Nursing, West China Second University 
Hospital, Sichuan University, Chengdu, China.
(4)West China School of Nursing, Sichuan University, Chengdu, China.
(5)Key laboratory of Birth Defects and Related Disease of Women and Children 
(Sichuan University), Ministry of Education, Chengdu, China.
(6)School of Public Administration and Policy, Renmin University of China, 
Beijing, China.

BACKGROUND: Large language model (LLM) artificial intelligence (AI) tools have 
the potential to streamline health care administration by enhancing efficiency 
in document drafting, resource allocation, and communication tasks. Despite this 
potential, the adoption of such tools among hospital administrators remains 
understudied, particularly at the individual level.
OBJECTIVE: This study aims to explore factors influencing the adoption and use 
of LLM AI tools among hospital administrators in China, focusing on enablers, 
barriers, and practical applications in daily administrative tasks.
METHODS: A multicenter, cross-sectional, descriptive qualitative design was 
used. Data were collected through semistructured face-to-face interviews with 31 
hospital administrators across 3 tertiary hospitals in Beijing, Shenzhen, and 
Chengdu from June 2024 to August 2024. The Colaizzi method was used for thematic 
analysis to identify patterns in participants' experiences and perspectives.
RESULTS: Adoption of LLM AI tools was generally low, with significant 
site-specific variations. Participants with higher technological familiarity and 
positive early experiences reported more frequent use, while barriers such as 
mistrust in tool accuracy, limited prompting skills, and insufficient training 
hindered broader adoption. Tools were primarily used for document drafting, with 
limited exploration of advanced functionalities. Participants strongly 
emphasized the need for structured training programs and institutional support 
to enhance usability and confidence.
CONCLUSIONS: Familiarity with technology, positive early experiences, and 
openness to innovation may facilitate adoption, while barriers such as limited 
knowledge, mistrust in tool accuracy, and insufficient prompting skills can 
hinder broader use. LLM AI tools are now primarily used for basic tasks such as 
document drafting, with limited application to more advanced functionalities due 
to a lack of training and confidence. Structured tutorials and institutional 
support are needed to enhance usability and integration. Targeted training 
programs, combined with organizational strategies to build trust and improve 
accessibility, could enhance adoption rates and broaden tool use. Future 
quantitative investigations should validate the adoption rate and influencing 
factors.

©Jun Chen, Yu Liu, Peng Liu, Yiming Zhao, Yan Zuo, Hui Duan. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org), 
01.04.2025.

DOI: 10.2196/70789
PMCID: PMC12000786
PMID: 40116330 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


872. J Med Ethics. 2025 Jul 7;51(7):493-496. doi: 10.1136/jme-2023-109737.

Can large language models help solve the cost problem for the right to 
explanation?

Munch L(1), Bjerring JC(2).

Author information:
(1)Philosophy and History of Ideas, Aarhus Universitet, Aarhus, Denmark 
lauritzmunch@gmail.com.
(2)Philosophy and History of Ideas, Aarhus Universitet, Aarhus, Denmark.

By now a consensus has emerged that people, when subjected to high-stakes 
decisions through automated decision systems, have a moral right to have these 
decisions explained to them. However, furnishing such explanations can be 
costly. So the right to an explanation creates what we call the cost problem: 
providing subjects of automated decisions with appropriate explanations of the 
grounds of these decisions can be costly for the companies and organisations 
that use these automated decision systems. In this paper, we explore whether 
large language models could prove significant in overcoming the cost problem. We 
provide an initial case for believing that they can but only with serious 
ethical costs.

© Author(s) (or their employer(s)) 2025. No commercial re-use. See rights and 
permissions. Published by BMJ Group.

DOI: 10.1136/jme-2023-109737
PMID: 39266048 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: None declared.


873. Br J Nutr. 2025 Dec 22:1-11. doi: 10.1017/S000711452510593X. Online ahead of 
print.

Artificial intelligence applications for assessing ultra-processed food 
consumption: a scoping review.

Campbell JL(1), Schofield G(1), Tiedt HR(2), Zinn C(1).

Author information:
(1)Human Potential Centre, Faculty of Health and Environmental Sciences, 
https://ror.org/01zvqw119Auckland University of Technology, Auckland1142, New 
Zealand.
(2)Sports Performance Research Institute New Zealand, Auckland University of 
Technology, Auckland1142, New Zealand.

Ultra-processed foods (UPF), defined using frameworks such as NOVA, are 
increasingly linked to adverse health outcomes, driving interest in ways to 
identify and monitor their consumption. Artificial intelligence (AI) offers 
potential, yet its application in classifying UPF remains underexamined. To 
address this gap, we conducted a scoping review mapping how AI has been used, 
focusing on techniques, input data, classification frameworks, accuracy and 
application. Studies were eligible if peer-reviewed, published in English 
(2015-2025), and they applied AI approaches to assess or classify UPF using 
recognised or study-specific frameworks. A systematic search in May 2025 across 
PubMed, Scopus, Medline and CINAHL identified 954 unique records with eight 
ultimately meeting the inclusion criteria; one additional study was added in 
October following an updated search after peer review. Records were 
independently screened and extracted by two reviewers. Extracted data covered AI 
methods, input types, frameworks, outputs, validation and context. Studies used 
diverse techniques, including random forest classifiers, large language models 
and rule-based systems, applied across various contexts. Four studies explored 
practical settings: two assessed consumption or purchasing behaviours, and two 
developed substitution tools for healthier options. All relied on NOVA or 
modified versions to categorise processing. Several studies reported predictive 
accuracy, with F1 scores from 0·86 to 0·98, while another showed alignment 
between clusters and NOVA categories. Findings highlight the potential of AI 
tools to improve dietary monitoring and the need for further development of 
real-time methods and validation to support public health.

DOI: 10.1017/S000711452510593X
PMID: 41423345


874. Eur Radiol. 2025 Sep 3. doi: 10.1007/s00330-025-11989-0. Online ahead of print.

Evaluating large language model-generated brain MRI protocols: performance of 
GPT4o, o3-mini, DeepSeek-R1 and Qwen2.5-72B.

Kim SH(1)(2), Schramm S(3), Schmitzer L(3), Serguen K(3), Ziegelmayer S(4), 
Busch F(4), Komenda A(4), Makowski MR(4), Adams LC(4), Bressem KK(4)(5), Zimmer 
C(3), Kirschke J(3), Wiestler B(6), Hedderich D(3), Finck T(#)(3), Bodden 
J(#)(3).

Author information:
(1)Institute of Diagnostic and Interventional Radiology, TUM University 
Hospital, School of Medicine and Health, Technical University of Munich, Munich, 
Germany. suhwan.kim@tum.de.
(2)Institute of Diagnostic and Interventional Neuroradiology, TUM University 
Hospital, School of Medicine and Health, Technical University of Munich, Munich, 
Germany. suhwan.kim@tum.de.
(3)Institute of Diagnostic and Interventional Neuroradiology, TUM University 
Hospital, School of Medicine and Health, Technical University of Munich, Munich, 
Germany.
(4)Institute of Diagnostic and Interventional Radiology, TUM University 
Hospital, School of Medicine and Health, Technical University of Munich, Munich, 
Germany.
(5)Department of Cardiovascular Radiology and Nuclear Medicine, German Heart 
Center Munich, School of Medicine and Health, Technical University of Munich, 
Munich, Germany.
(6)AI for Image-Guided Diagnosis and Therapy, School of Medicine and Health, 
Technical University of Munich, Munich, Germany.
(#)Contributed equally

OBJECTIVES: To evaluate the potential of LLMs to generate sequence-level brain 
MRI protocols.
MATERIALS AND METHODS: This retrospective study employed a dataset of 150 brain 
MRI cases derived from local imaging request forms. Reference protocols were 
established by two neuroradiologists. GPT-4o, o3-mini, DeepSeek-R1 and 
Qwen2.5-72B were employed to generate brain MRI protocols based on the case 
descriptions. Protocol generation was conducted (1) with additional in-context 
learning involving local standard protocols (enhanced) and (2) without 
additional information (base). Additionally, two radiology residents 
independently defined MRI protocols. The sum of redundant and missing sequences 
(accuracy index) was defined as performance metric. Accuracy indices were 
compared between groups using paired t-tests.
RESULTS: The two neuroradiologists achieved substantial inter-rater agreement 
(Cohen's κ = 0.74). o3-mini demonstrated superior performance (base: 
2.65 ± 1.61; enhanced: 1.94 ± 1.25), followed by GPT-4o (base: 3.11 ± 1.83; 
enhanced: 2.23 ± 1.48), DeepSeek-R1 (base: 3.42 ± 1.84; enhanced: 2.37 ± 1.42) 
and Qwen2.5-72B (base: 5.95 ± 2.78; enhanced: 2.75 ± 1.54). o3-mini consistently 
outperformed the other models with a significant margin. All four models showed 
highly significant performance improvements under the enhanced condition (adj. 
p < 0.001 for all models). The highest-performing LLM (o3-mini [enhanced]) 
yielded an accuracy index comparable to residents (o3-mini [enhanced]: 
1.94 ± 1.25, resident 1: 1.77 ± 1.29, resident 2: 1.77 ± 1.28).
CONCLUSION: Our findings demonstrate the promising potential of LLMs in 
automating brain MRI protocoling, especially when augmented through in-context 
learning. o3-mini exhibited superior performance, followed by GPT-4o.
KEY POINTS: QuestionBrain MRI protocoling is a time-consuming, 
non-interpretative task, exacerbating radiologist workload. Findingso3-mini 
demonstrated superior brain MRI protocoling performance. All models showed 
notable improvements when augmented with local standard protocols. Clinical 
relevanceMRI protocoling is a time-intensive, non-interpretative task that adds 
to radiologist workload; large language models offer potential for 
(semi-)automation of this process.

© 2025. The Author(s).

DOI: 10.1007/s00330-025-11989-0
PMID: 40900267

Conflict of interest statement: Compliance with ethical standards. Guarantor: 
The scientific guarantor of this publication is S.H.K. Conflict of interest: 
L.C.A. is a member of the Scientific Editorial Board of European Radiology 
(section: Urogenital) and, as such, did not participate in the selection or 
review processes for this article. The remaining authors of this manuscript 
declare no relationships with any companies, whose products or services may be 
related to the subject matter of the article. Statistics and biometry: No 
complex statistical methods were necessary for this paper. Informed consent: 
Written informed consent was waived by the Institutional Review Board. Ethical 
approval: Institutional Review Board approval was obtained. This article is 
available as a preprint at: 
https://www.medrxiv.org/content/10.1101/2025.04.08.25325433 . Study subjects or 
cohorts overlap: Not applicable. Methodology: Retrospective Performed at one 
institution


875. J Med Internet Res. 2025 Apr 4;27:e68486. doi: 10.2196/68486.

Virtual Patients Using Large Language Models: Scalable, Contextualized 
Simulation of Clinician-Patient Dialogue With Feedback.

Cook DA(1)(2), Overgaard J(1), Pankratz VS(3), Del Fiol G(4), Aakre CA(1).

Author information:
(1)Division of General Internal Medicine, Mayo Clinic College of Medicine and 
Science, Rochester, MN, United States.
(2)Multidisciplinary Simulation Center, Mayo Clinic College of Medicine and 
Science, Rochester, MN, United States.
(3)Health Sciences Center, University of New Mexico, Albuquerque, NM, United 
States.
(4)Department of Biomedical Informatics, University of Utah School of Medicine, 
Salt Lake City, UT, United States.

BACKGROUND: Virtual patients (VPs) are computer screen-based simulations of 
patient-clinician encounters. VP use is limited by cost and low scalability.
OBJECTIVE: We aimed to show that VPs powered by large language models (LLMs) can 
generate authentic dialogues, accurately represent patient preferences, and 
provide personalized feedback on clinical performance. We also explored using 
LLMs to rate the quality of dialogues and feedback.
METHODS: We conducted an intrinsic evaluation study rating 60 VP-clinician 
conversations. We used carefully engineered prompts to direct OpenAI's 
generative pretrained transformer (GPT) to emulate a patient and provide 
feedback. Using 2 outpatient medicine topics (chronic cough diagnosis and 
diabetes management), each with permutations representing different patient 
preferences, we created 60 conversations (dialogues plus feedback): 48 with a 
human clinician and 12 "self-chat" dialogues with GPT role-playing both the VP 
and clinician. Primary outcomes were dialogue authenticity and feedback quality, 
rated using novel instruments for which we conducted a validation study 
collecting evidence of content, internal structure (reproducibility), relations 
with other variables, and response process. Each conversation was rated by 3 
physicians and by GPT. Secondary outcomes included user experience, bias, 
patient preferences represented in the dialogues, and conversation features that 
influenced authenticity.
RESULTS: The average cost per conversation was US $0.51 for GPT-4.0-Turbo and US 
$0.02 for GPT-3.5-Turbo. Mean (SD) conversation ratings, maximum 6, were overall 
dialogue authenticity 4.7 (0.7), overall user experience 4.9 (0.7), and average 
feedback quality 4.7 (0.6). For dialogues created using GPT-4.0-Turbo, physician 
ratings of patient preferences aligned with intended preferences in 20 to 47 of 
48 dialogues (42%-98%). Subgroup comparisons revealed higher ratings for 
dialogues using GPT-4.0-Turbo versus GPT-3.5-Turbo and for human-generated 
versus self-chat dialogues. Feedback ratings were similar for human-generated 
versus GPT-generated ratings, whereas authenticity ratings were lower. We did 
not perceive bias in any conversation. Dialogue features that detracted from 
authenticity included that GPT was verbose or used atypical vocabulary (93/180, 
51.7% of conversations), was overly agreeable (n=56, 31%), repeated the question 
as part of the response (n=47, 26%), was easily convinced by clinician 
suggestions (n=35, 19%), or was not disaffected by poor clinician performance 
(n=32, 18%). For feedback, detractors included excessively positive feedback 
(n=42, 23%), failure to mention important weaknesses or strengths (n=41, 23%), 
or factual inaccuracies (n=39, 22%). Regarding validation of dialogue and 
feedback scores, items were meticulously developed (content evidence), and we 
confirmed expected relations with other variables (higher ratings for advanced 
LLMs and human-generated dialogues). Reproducibility was suboptimal, due largely 
to variation in LLM performance rather than rater idiosyncrasies.
CONCLUSIONS: LLM-powered VPs can simulate patient-clinician dialogues, 
demonstrably represent patient preferences, and provide personalized performance 
feedback. This approach is scalable, globally accessible, and inexpensive. 
LLM-generated ratings of feedback quality are similar to human ratings.

©David A Cook, Joshua Overgaard, V Shane Pankratz, Guilherme Del Fiol, Chris A 
Aakre. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 04.04.2025.

DOI: 10.2196/68486
PMCID: PMC12008702
PMID: 39854611 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


876. Dermatology. 2025 Dec 18:1-17. doi: 10.1159/000550153. Online ahead of print.

When jack of all trades is a master of none: Comparing the performance of GPT-4 
Omni against specialised neural networks in identifying malignant dermatological 
lesions from smartphone images and structured clinical data.

Deng J, Zhao HJ, Hwang J, Alsefaou A, Guo E, Heybati K, Moskalyk M.

INTRODUCTION: Artificial intelligence (AI) can potentially assist in triaging 
suspicious skin lesions as malignant or benign. General-purpose multimodal large 
language models (LLMs), such as GPT-4o, have not been rigorously evaluated for 
this task. This study assessed GPT-4o's ability to triage skin lesions and 
compared its performance to specialised neural networks.
METHODS: We evaluated GPT-4o using 1000 random cases from the PAD-UFES-20 
dataset with 50 repeated trials. GPT-4o was tested using clinical data-only, 
image-only, and multimodal inputs. GPT-4o's performance, consistency, and 
fairness across different demographic subgroups was evaluated. Its performance 
metrics were compared against specialised unimodal and multimodal neural 
networks trained on a separate subset of the PAD-UFES-20 dataset.
RESULTS: GPT-4o exhibited poor diagnostic performance across all modalities, 
with average balanced accuracies of 0.571, 0.602, and 0.622 for clinical data, 
image, and multimodal inputs, respectively. Sensitivity was consistently high 
(>0.95) with the tradeoff of very low specificity. Mean agreement rates were 
high (>0.90), however Fleiss' κ indicated only moderate consistency due to a 
strong bias toward malignant classifications. Fairness evaluations showed poorer 
discriminative performance in younger patients compared to middle-aged and 
elderly patients, but no notable differences between different sex and skin tone 
subgroups. Specialised neural networks significantly outperformed GPT-4o on most 
pairwise comparisons. Multimodal inputs significantly improved GPT-4o 
performance over unimodal inputs.
CONCLUSION: Although GPT-4o consistently triaged skin lesions with high 
sensitivity, its very low specificity limits clinical utility. Thus, 
general-purpose LLMs like GPT-4o are currently unsuitable for clinical 
dermatological diagnostics without significant field-specific developments and 
validation.

The Author(s). Published by S. Karger AG, Basel.

DOI: 10.1159/000550153
PMID: 41411218


877. BMC Med Educ. 2025 Aug 5;25(1):1146. doi: 10.1186/s12909-025-07718-2.

Artificial intelligence assisted automated short answer question scoring tool 
shows high correlation with human examiner markings.

Seneviratne HMTW(1), Manathunga SS(2).

Author information:
(1)Department of Pharmacology, Faculty of Medicine, University of Peradeniya, 
Peradeniya, Sri Lanka. thilanka.medi@gmail.com.
(2)Department of Pharmacology, Faculty of Medicine, University of Peradeniya, 
Peradeniya, Sri Lanka.

BACKGROUND: Optimizing the skill of answering Short answer questions (SAQ) in 
medical undergraduates with personalized feedback is challenging. With the 
increasing number of students and staff shortages this task is becoming 
practically difficult. Hence, we aimed to develop automated SAQ scoring tool 
(ASST) using artificial intelligence (AI) to evaluate written answers with 
feedback.
METHODS: This study investigated the use of large language models (LLMs) for 
automated SAQ scoring, specifically following rubrics. Rubric is a set of 
guidelines or criteria used to evaluate or grade assignments, which is provided 
by instructors. We focused on short answer questions from the Systematic 
Pharmacology course, along with model responses and rubrics shared with the LLM. 
The LLM analyzed student answers by extracting key parts, scoring them based on 
the rubric criteria, and providing feedback. The evaluation process relied on 
GPT-4, where the final score was determined by averaging results across five 
sampled runs. To validate the method, human examiners also graded the same 
answers, and the marks were compared to measure correlation.
RESULTS: Across 30 student answers, AES scores showed high correlation with 
independent human examiner markings (correlation coefficients of 0.93 and 0.96). 
An intra-class correlation coefficient of 0.94 indicate excellent inter-rater 
reliability between the LLM and human examiners.
CONCLUSION: AI assisted automated SAQ scoring tool markings highly correlate 
with the human examiner markings which shows promise for transparent, flexible 
automated grading with granular feedback. This approach enabling criteria-based 
assessment could have valuable applications in education, reducing grading 
burdens on instructors while providing students with more granular, actionable 
feedback on their written work.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-07718-2
PMCID: PMC12323204
PMID: 40764994 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethical approval and consent to 
participate: Not applicable, since participation of human subjects was not 
entertained. Consent for publication: Not applicable. Competing interests: The 
authors declare no competing interests.


878. J Med Internet Res. 2025 Nov 7;27:e80289. doi: 10.2196/80289.

Nurses' Perspectives on Evidence Dissemination Barriers and Large Language 
Model-Based Support: Qualitative Study Using Focus Groups and Nominal Group 
Technique.

Ruan J(1), Tang Y(1), Wei Z(2)(3), Xing W(1)(4), Hu Y(1)(4).

Author information:
(1)School of Nursing, Fudan University, Shanghai, China.
(2)School of Data Science, Fudan University, Shanghai, China.
(3)Shanghai Innovation Institute, Shanghai, China.
(4)JBI Fudan University Centre for Evidence-Based Nursing, Shanghai, China.

BACKGROUND: Current evidence dissemination methods fall short of meeting 
clinical nurses' needs, hindering the implementation of evidence-based nursing 
practice. Large language models (LLMs), with their advanced natural language 
processing capabilities, offer potential as innovative tools to facilitate 
evidence dissemination. However, general-purpose LLMs typically lack 
domain-specific knowledge, are insufficient to support effective evidence 
dissemination in clinical contexts. It is essential to develop artificial 
intelligence tools tailored to nurses' needs and preferences to enhance evidence 
dissemination.
OBJECTIVE: The aim of this study is to identify the challenges and barriers 
clinical nurses face in disseminating evidence, examine their perspectives on 
the use of existing LLMs to support evidence dissemination, and explore their 
needs and preferences regarding an LLM-based nursing evidence question-answering 
system.
METHODS: This qualitative study used a combined method of focus group 
discussions and the nominal group technique (NGT). Using purposive sampling, 
nurses with diverse specialties, professional titles, and years of experience 
were recruited, resulting in a total of 22 clinical nurses who completed the 
entire study. A total of 2 focus group discussions were conducted online via 
Tencent Meeting between November and December 2024 to explore the challenges and 
barriers nurses face in disseminating evidence, as well as their perspectives on 
using existing LLMs to support evidence dissemination. The data were analyzed 
using qualitative content analysis following the approach of Graneheim and 
Lundman. Subsequently, the NGT was used between March and April 2025 to identify 
nurses' needs and preferences for the system to be developed. To overcome 
geographical constraints and participants' busy schedules, the NGT was conducted 
entirely online, using online questionnaires and WeChat groups. Overall, 2 
rounds of voting were conducted to determine the priority ranking of the 
functionalities.
RESULTS: The focus group yielded 3 main themes and 7 subthemes. Three main 
themes were identified as (1) pathways for evidence dissemination among nurses, 
(2) barriers that hinder the effective dissemination of evidence, and (3) 
advantages and limitations of using LLMs to support evidence dissemination. The 
limitations of current LLMs served as the foundation for nurses' subsequent 
reflections in the nominal group discussions on the desired functions of a newly 
developed LLM. The NGT sessions ultimately identified 9 desired functions. After 
prioritization, the top 3 ranked functions were evidence-based, high-quality 
question-answering, evidence source provision, and personalized evidence 
recommendation.
CONCLUSIONS: The current evidence dissemination process faces multiple barriers. 
LLMs hold promise as innovative tools to support evidence dissemination, but 
require further refinement. Clinical nurses have identified key functional 
needs, guiding the development of LLMs specifically tailored to clinical nursing 
practice.

©Junyi Ruan, Yimin Tang, Zhongyu Wei, Weijie Xing, Yan Hu. Originally published 
in the Journal of Medical Internet Research (https://www.jmir.org), 07.11.2025.

DOI: 10.2196/80289
PMCID: PMC12639338
PMID: 41202291 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


879. World J Surg. 2025 Dec;49(12):3402-3409. doi: 10.1002/wjs.70165. Epub 2025 Nov 
21.

Beyond the Hype: Mapping the Evolution of Artificial Intelligence in General 
Surgery Through Two Decades of Bibliometrics.

Erdem O(1), Canbak T(1), Acar A(1), Başak F(1).

Author information:
(1)Department of General Surgery, University of Health Sciences, Umraniye 
Training and Research Hospital, Istanbul, Türkiye.

BACKGROUND: Artificial intelligence (AI) has transformed many facets of general 
surgery. A quantitative bibliometric overview can map publication trends, 
research fronts, and collaborative patterns to guide future work. Our study 
provides a comprehensive analysis of the literature on AI in general surgery, 
identifying key trends and influential contributors.
METHODS: We retrieved 536 "Article" and "Review" records from Scopus and Web of 
Science from January 2005 through June 2025. After a rigorous deduplication 
process, 536 unique publications remained. We analyzed annual scientific 
production, top journals, authors, keyword co-occurrence, and highly cited 
papers using descriptive and relational bibliometric analyses.
RESULTS: Annual publications grew exponentially, accelerating significantly 
after 2019 and peaking at 160 publications in 2024. Annals of Surgery (n = 28), 
Surgical Endoscopy (n = 25), and Journal of Medical Internet Research (n = 20) 
were the most productive journals. Palenzuela DL (n = 7), Dayan D (n = 6), and 
Liu J (n = 6) were the most prolific authors. The most frequent keywords were 
"Artificial intelligence" (64), "General surgery" (43), and "Surgery" (31). 
Keyword co-occurrence analysis revealed five thematic clusters: AI language 
models, clinical outcomes/risk prediction, surgical education, 
socio-professional themes, and core surgical practice. The most cited articles 
focused on surgical phase recognition, medical education, and large-language 
models.
CONCLUSIONS: AI in general surgery has seen a period of exponential growth, 
moving from exploratory discourse to applied research. While research is 
concentrated among a few authors and journals, its thematic diversity suggests a 
nascent, fragmented field without a dominant intellectual core. Future work 
should prioritize prospective validation, data-sharing infrastructures, and 
ethical frameworks to ensure responsible clinical translation. We propose an 
ethical-educational-technological (EET) framework to guide the responsible 
integration of AI into surgical practice and training.

© 2025 International Society of Surgery/Société Internationale de Chirurgie 
(ISS/SIC).

DOI: 10.1002/wjs.70165
PMID: 41269235 [Indexed for MEDLINE]


880. Br J Anaesth. 2025 May;134(5):1308-1317. doi: 10.1016/j.bja.2025.01.028. Epub 
2025 Mar 3.

Automated structured data extraction from intraoperative echocardiography 
reports using large language models.

MacKay EJ(1), Goldfinger S(2), Chan TJ(3), Grasfield RH(4), Eswar VJ(5), Li 
K(6), Cao Q(7), Pouch AM(3).

Author information:
(1)Department of Anaesthesiology and Critical Care, Perelman School of Medicine, 
University of Pennsylvania, Philadelphia, PA, USA; Penn Center for Perioperative 
Outcomes Research and Transformation (CPORT), Perelman School of Medicine, 
University of Pennsylvania, Philadelphia, PA, USA; Penn's Cardiovascular 
Outcomes, Quality and Evaluative Research Center (CAVOQER), Perelman School of 
Medicine, University of Pennsylvania, Philadelphia, PA, USA. Electronic address: 
emily.mackay@pennmedicine.upenn.edu.
(2)Department of Radiology, Perelman School of Medicine, University of 
Pennsylvania, Philadelphia, PA, USA; School of Arts and Sciences, University of 
Pennsylvania, Philadelphia, PA, USA.
(3)Department of Radiology, Perelman School of Medicine, University of 
Pennsylvania, Philadelphia, PA, USA; Department of Bioengineering, University of 
Pennsylvania, Philadelphia, PA, USA.
(4)School of Medicine and Health Sciences, Des Moines University, Des Moines, 
IA, USA.
(5)Department of Anaesthesiology and Critical Care, Perelman School of Medicine, 
University of Pennsylvania, Philadelphia, PA, USA.
(6)Harvard Medical School, Harvard University, Boston, MA, USA.
(7)Department of Biostatistics, Epidemiology, and Informatics, Perelman School 
of Medicine, University of Pennsylvania, Philadelphia, PA, USA.

BACKGROUND: Consensus-based large language model (LLM) ensembles might provide 
an automated solution for extracting structured data from unstructured text in 
echocardiography reports.
METHODS: This cross-sectional study utilised 600 intraoperative transoesophageal 
reports (100 for prompt engineering; 500 for testing) randomly sampled from 7106 
adult patients undergoing cardiac surgery at two hospitals within the University 
of Pennsylvania Healthcare System. Three echocardiographic parameters (left 
ventricular ejection fraction, right ventricular systolic function, and 
tricuspid regurgitation) were extracted from both the presurgical and 
postsurgical sections of the reports. LLM ensembles were generated using five 
open-source LLMs and four voting strategies: (1) unanimous (five out of five in 
agreement); (2) supermajority (four or more of five in agreement); (3) majority 
(three or more of five in agreement); and (4) plurality (two or more of five in 
agreement). Returned LLM ensemble responses were compared with the reference 
standard dataset to calculate raw accuracy, consensus accuracy, error rate, and 
yield.
RESULTS: Of the four LLM ensembles, the unanimous LLM ensemble achieved the 
highest consensus accuracies (99.4% presurgical; 97.9% postsurgical) and the 
lowest error rates (0.6% presurgical; 2.1% postsurgical) but had the lowest data 
extraction yields (81.7% presurgical; 80.5% postsurgical) and the lowest raw 
accuracies (81.2% presurgical; 78.9% postsurgical). In contrast, the plurality 
LLM ensemble achieved the highest raw accuracies (96.1% presurgical; 93.7% 
postsurgical) and the highest data extraction yields (99.4% presurgical; 98.9% 
postsurgical) but had the lowest consensus accuracies (96.7% presurgical; 94.7% 
postsurgical) and highest error rates (3.3% presurgical; 5.3% postsurgical).
CONCLUSIONS: A consensus-based LLM ensemble successfully generated structured 
data from unstructured text contained in intraoperative transoesophageal 
reports.

Copyright © 2025 British Journal of Anaesthesia. Published by Elsevier Ltd. All 
rights reserved.

DOI: 10.1016/j.bja.2025.01.028
PMCID: PMC12106877
PMID: 40037947 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of interest The authors declare that 
they have no conflicts of interest.


881. Laryngoscope. 2025 Nov;135(11):4053-4063. doi: 10.1002/lary.32308. Epub 2025 Jun 
7.

Preparing for AI in Resident Selection: A Scoping Review of Current Applications 
and Limitations.

Michaelson G(1)(2), Karasik D(2)(3), Polen E(4), Rangarajan SV(1)(2), Zhao 
NW(1)(2).

Author information:
(1)Case Western Reserve University School of Medicine, Cleveland, Ohio, USA.
(2)Department of Otolaryngology-Head and Neck Surgery, University Hospitals 
Cleveland Medical Center, Cleveland, Ohio, USA.
(3)Technion-Israel Institute of Technology, Ruth and Bruce Rappaport Faculty of 
Medicine, Haifa, Israel.
(4)Core Library, University Hospitals Cleveland Medical Center, Cleveland, Ohio, 
USA.

OBJECTIVES: Residency programs face a large number of applications with limited 
time to review them. Artificial intelligence (AI) tools could potentially 
streamline this process. This scoping review examines how AI has been applied in 
the resident selection process, the impacts described, and the limitations 
identified.
DATA SOURCES: PubMed, OVID, Cochrane, EMBASE, Google Scholar, and Web of 
Science.
REVIEW METHODS: A comprehensive search strategy was developed using keywords 
related to artificial intelligence, natural language processing, machine 
learning, medical residency, and personnel selection. Studies were included if 
they were published in English in the last 10 years, involved real or simulated 
medical or surgical residency applicants or applications, and used AI in any 
part of the selection process. Articles were identified and screened 
independently by two reviewers, with conflicts resolved by consensus.
RESULTS: Of 115 identified articles, 11 met inclusion criteria. Applications of 
AI fell into three thematic areas: assessing bias (n = 4), predicting selection 
outcomes (n = 5), and comparing AI to human reviewers (n = 2). AI models 
included natural language processing, machine learning, and large language 
models. While several studies demonstrated AI's potential to detect bias, 
improve reviewer consistency, and identify overlooked applicants, concerns 
remain about bias replication, limited generalizability, and the 
interpretability of AI-driven decisions.
CONCLUSIONS: Artificial intelligence holds promise for a more efficient and 
equitable resident selection process. Further investigation is needed to guide 
responsible development, validation, and oversight of these tools.

© 2025 The Author(s). The Laryngoscope published by Wiley Periodicals LLC on 
behalf of The American Laryngological, Rhinological and Otological Society, Inc.

DOI: 10.1002/lary.32308
PMCID: PMC12645350
PMID: 40481757 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


882. Brain Sci. 2025 Jun 20;15(7):667. doi: 10.3390/brainsci15070667.

From Narratives to Diagnosis: A Machine Learning Framework for Classifying Sleep 
Disorders in Aging Populations: The sleepCare Platform.

Frantzidis CA(1).

Author information:
(1)School of Engineering and Physical Sciences, University of Lincoln, Lincoln 
LN6 7TS, UK.

Background/Objectives: Sleep disorders are prevalent among aging populations and 
are often linked to cognitive decline, chronic conditions, and reduced quality 
of life. Traditional diagnostic methods, such as polysomnography, are 
resource-intensive and limited in accessibility. Meanwhile, individuals 
frequently describe their sleep experiences through unstructured narratives in 
clinical notes, online forums, and telehealth platforms. This study proposes a 
machine learning pipeline (sleepCare) that classifies sleep-related narratives 
into clinically meaningful categories, including stress-related, 
neurodegenerative, and breathing-related disorders. The proposed framework 
employs natural language processing (NLP) and machine learning techniques to 
support remote applications and real-time patient monitoring, offering a 
scalable solution for the early identification of sleep disturbances. Methods: 
The sleepCare consists of a three-tiered classification pipeline to analyze 
narrative sleep reports. First, a baseline model used a Multinomial Naïve Bayes 
classifier with n-gram features from a Bag-of-Words representation. Next, a 
Support Vector Machine (SVM) was trained on GloVe-based word embeddings to 
capture semantic context. Finally, a transformer-based model (BERT) was 
fine-tuned to extract contextual embeddings, using the [CLS] token as input for 
SVM classification. Each model was evaluated using stratified train-test splits 
and 10-fold cross-validation. Hyperparameter tuning via GridSearchCV optimized 
performance. The dataset contained 475 labeled sleep narratives, classified into 
five etiological categories relevant for clinical interpretation. Results: The 
transformer-based model utilizing BERT embeddings and an optimized Support 
Vector Machine classifier achieved an overall accuracy of 81% on the test set. 
Class-wise F1-scores ranged from 0.72 to 0.91, with the highest performance 
observed in classifying normal or improved sleep (F1 = 0.91). The macro average 
F1-score was 0.78, indicating balanced performance across all categories. 
GridSearchCV identified the optimal SVM parameters (C = 4, kernel = 'rbf', gamma 
= 0.01, degree = 2, class_weight = 'balanced'). The confusion matrix revealed 
robust classification with limited misclassifications, particularly between 
overlapping symptom categories such as stress-related and neurodegenerative 
sleep disturbances. Conclusions: Unlike generic large language model 
applications, our approach emphasizes the personalized identification of sleep 
symptomatology through targeted classification of the narrative input. By 
integrating structured learning with contextual embeddings, the framework offers 
a clinically meaningful, scalable solution for early detection and 
differentiation of sleep disorders in diverse, real-world, and remote settings.

DOI: 10.3390/brainsci15070667
PMCID: PMC12293951
PMID: 40722260

Conflict of interest statement: The author declares no conflict of interest.


883. Front Oncol. 2025 Aug 14;15:1605916. doi: 10.3389/fonc.2025.1605916. eCollection 
2025.

Clinical decision-making for uveal melanoma radiotherapy: comparative 
performance of experienced radiation oncologists and leading generative AI 
models.

Wang X(1), Wang P(1).

Author information:
(1)Department of Ophthalmology, The First Affiliated Hospital of Chongqing 
Medical University, Chongqing Key Laboratory for the Prevention and Treatment of 
Major Blinding Eye Diseases, Chongqing, China.

BACKGROUND: Uveal melanoma is the most common primary intraocular malignancy in 
adults, yet radiotherapy decision-making for this disease often remains complex 
and variable. Although emerging generative AI models have shown promise in 
synthesizing vast clinical information, few studies have systematically compared 
their performance against experienced radiation oncologists in this specialized 
domain. This study examined the comparative accuracy of three leading generative 
AI models and experienced radiation oncologists in guideline-based clinical 
decision-making for uveal melanoma.
METHODS: A structured, 20-question examination reflecting standard radiotherapy 
guidelines was developed. Fifty radiation oncologists, each with 10-15 years of 
experience, completed an open-book exam following a 15-day standardized review. 
Meanwhile, Grok 3 (Think), Gemini 2.0 Flash Thinking, and Open ai o1 pro were 
each tested through 10 independent chat sessions. Twelve recognized experts in 
uveal melanoma, blinded to the source of each submission, scored all answer 
sets. Kruskal-Wallis tests with post hoc comparisons were conducted to evaluate 
group-level differences in total and domain-specific performance.
RESULTS: Of the 80 total sets (50 from oncologists, 30 from AI), Open ai o1 pro 
achieved the highest mean total score (98.0 ± 1.9), followed by oncologists 
(91.5 ± 3.2), Grok 3 (82.3 ± 2.1), and Gemini 2.0 (74.2 ± 3.4). Statistically 
significant differences emerged across all domains, with human experts 
particularly excelling in treatment selection but still trailing Open ai o1 pro 
overall. Completion time was significantly shorter for the AI models compared 
with oncologists.
CONCLUSION: These findings suggest that advanced generative AI can exceed 
expert-level performance in certain aspects of radiotherapy decision-making for 
uveal melanoma. Although AI may expedite clinical workflows and offer highly 
accurate guidance, human judgment remains indispensable for nuanced patient 
care.

Copyright © 2025 Wang and Wang.

DOI: 10.3389/fonc.2025.1605916
PMCID: PMC12392105
PMID: 40896429

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


884. J Med Internet Res. 2025 Jul 8;27:e75347. doi: 10.2196/75347.

Psychometric Evaluation of Large Language Model Embeddings for Personality Trait 
Prediction.

Maharjan J(1), Jin R(1), Zhu J(1), Kenne D(2).

Author information:
(1)Department of Computer Science, Kent State University, 800 East Summit 
Street, Kent, OH, 44242, United States, 1 3305931365.
(2)Department of Public Health, Kent State University, Kent, OH, United States.

BACKGROUND: Recent advancements in large language models (LLMs) have generated 
significant interest in their potential for assessing psychological constructs, 
particularly personality traits. While prior research has explored LLMs' 
capabilities in zero-shot or few-shot personality inference, few studies have 
systematically evaluated LLM embeddings within a psychometric validity framework 
or examined their correlations with linguistic and emotional markers. 
Additionally, the comparative efficacy of LLM embeddings against traditional 
feature engineering methods remains underexplored, leaving gaps in understanding 
their scalability and interpretability for computational personality assessment.
OBJECTIVE: This study evaluates LLM embeddings for personality trait prediction 
through four key analyses: (1) performance comparison with zero-shot methods on 
PANDORA Reddit data, (2) psychometric validation and correlation with LIWC 
(Linguistic Inquiry and Word Count) and emotion features, (3) benchmarking 
against traditional feature engineering approaches, and (4) assessment of model 
size effects (OpenAI vs BERT vs RoBERTa). We aim to establish LLM embeddings as 
a psychometrically valid and efficient alternative for personality assessment.
METHODS: We conducted a multistage analysis using 1 million Reddit posts from 
the PANDORA Big Five personality dataset. First, we generated text embeddings 
using 3 LLM architectures (RoBERTa, BERT, and OpenAI) and trained a custom 
bidirectional long short-term memory model for personality prediction. We 
compared this approach against zero-shot inference using prompt-based methods. 
Second, we extracted psycholinguistic features (LIWC categories and National 
Research Council emotions) and performed feature engineering to evaluate 
potential performance enhancements. Third, we assessed the psychometric validity 
of LLM embeddings: reliability validity using Cronbach α and convergent validity 
analysis by examining correlations between embeddings and established linguistic 
markers. Finally, we performed traditional feature engineering on static 
psycholinguistic features to assess performance under different settings.
RESULTS: LLM embeddings trained using simple deep learning techniques 
significantly outperform zero-shot approaches on average by 45% across all 
personality traits. Although psychometric validation tests indicate moderate 
reliability, with an average Cronbach α of 0.63, correlation analyses spark a 
strong association with key linguistic or emotional markers; openness correlates 
highly with social (r=0.53), conscientiousness with linguistic (r=0.46), 
extraversion with social (r=0.41), agreeableness with pronoun usage (r=0.40), 
and neuroticism with politics-related text (r=0.63). Despite adding advanced 
feature engineering on linguistic features, the performance did not improve, 
suggesting that LLM embeddings inherently capture key linguistic features. 
Furthermore, our analyses demonstrated efficacy on larger model size with a 
computational cost trade-off.
CONCLUSIONS: Our findings demonstrate that LLM embeddings offer a robust 
alternative to zero-shot methods in personality trait analysis, capturing key 
linguistic patterns without requiring extensive feature engineering. The 
correlation between established psycholinguistic markers and the performance 
trade-off with computational cost provides a hint for future computational 
linguistic work targeting LLM for personality assessment. Further research 
should explore fine-tuning strategies to enhance psychometric validity.

© Julina Maharjan, Ruoming Jin, Jianfeng Zhu, Deric Kenne. Originally published 
in the Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/75347
PMCID: PMC12262148
PMID: 40627556 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


885. Eur Radiol. 2025 Aug 26. doi: 10.1007/s00330-025-11954-x. Online ahead of print.

ESR Essentials: artificial intelligence in breast imaging-practice 
recommendations by the European Society of Breast Imaging.

Schiaffino S(1)(2), Bernardi D(3)(4), Healy N(5)(6), Marino MA(7), Romeo V(8), 
Sechopoulos I(9), Mann RM(9)(10), Pinker K(11).

Author information:
(1)Imaging Institute of Southern Switzerland (IIMSI), Ente Ospedaliero Cantonale 
(EOC), Lugano, Switzerland. schiaffino.simone@gmail.com.
(2)Faculty of Biomedical Sciences, Università della Svizzera Italiana, Lugano, 
Switzerland. schiaffino.simone@gmail.com.
(3)IRCCS Humanitas Research Hospital, Milan, Italy.
(4)Department of Biomedical Sciences, Humanitas University, Milan, Italy.
(5)Beaumont Breast Centre, Beaumont Hospital, Dublin, Ireland.
(6)Department of Radiology, Royal College of Surgeons, Dublin, Ireland.
(7)Department of Biomedical Sciences and Morphologic and Functional Imaging, 
University of Messina, Messina, Italy.
(8)Department of Advanced Biomedical Sciences, University of Naples Federico II, 
Naples, Italy.
(9)Department of Diagnostic Imaging, Radboud University Medical Center, 
Nijmegen, The Netherlands.
(10)Department of Radiology, Netherlands Cancer Institute, Amsterdam, The 
Netherlands.
(11)Department of Radiology, Columbia University, Vagelos College of Physicians 
and Surgeons, New York, NY, US.

Artificial intelligence (AI) can enhance the diagnostic performance of breast 
cancer imaging and improve workflow optimization, potentially mitigating 
excessive radiologist workload and suboptimal diagnostic accuracy. AI can also 
boost imaging capabilities through individual risk prediction, molecular 
subtyping, and neoadjuvant therapy response predictions. Evidence demonstrates 
AI's potential across multiple modalities. The most robust data come from 
mammographic screening, where AI models improve diagnostic accuracy and optimize 
workflow, but rigorous post-market surveillance is required before any 
implementation strategy in this field. Commercial tools for digital breast 
tomosynthesis and ultrasound, potentially able to reduce interpretation time and 
improve accuracy, are also available, but post-implementation evaluation studies 
are likewise lacking. Besides basic tools for breast MRI with limited proven 
clinical benefit, AI applications for other modalities are not yet commercially 
available. Applications in contrast-enhanced mammography are still in the 
research stage, especially for radiomics-based molecular subtype classification. 
Applications of Large Language Models (LLMs) are in their infancy, and there are 
currently no clinical applications. Consequently, and despite their promise, all 
commercially available AI tools for breast imaging should currently still be 
regarded as techniques that, at best, aid radiologists in image evaluation. 
Their use is therefore optional, and the findings may always be overruled. KEY 
POINTS: AI systems improve diagnostic accuracy and efficiency of mammography 
screening, but long-term outcomes data are lacking. Commercial tools for digital 
breast tomosynthesis and ultrasound are available, but post-implementation 
evaluation studies are lacking. AI tools for breast imaging should still be 
regarded as a non-obligatory aid to radiologists for image interpretation.

© 2025. The Author(s).

DOI: 10.1007/s00330-025-11954-x
PMID: 40856739

Conflict of interest statement: Compliance with ethical standards. Guarantor: 
The scientific guarantor of this publication is Katja Pinker. Conflict of 
interest: The authors of this manuscript declare relationships with the 
following companies: Simone Schiaffino Consulting fees from Tempus; payment or 
honoraria for lectures, presentations, speakers bureaus, manuscript writing, or 
educational events from GE HealthCare; and support for attending meetings and/or 
travel from Bracco. Member of the European Society of Radiology Research 
Committee; member of the editorial board for European Journal of Radiology and 
European Radiology Experimental. Ritse Mann Grants or contracts from the Dutch 
Cancer Society, Europees Fonds voor Regionale Ontwikkeling Programma 
Oost-Nederland, Horizon Europe, European Research Council, Dutch Research 
Council, Health Holland, Siemens Healthineers, Bayer, ScreenPoint Medical, 
Beckton Dickinson, PA Imaging, Lunit, and Koning Health; royalties or licenses 
from Elsevier; consulting fees from Siemens Healthineers, Bayer, ScreenPoint 
Medical, Beckton Dickinson, PA Imaging, Lunit, Koning Health, and Guerbet; 
participation on a data and safety monitoring board or advisory board for the 
SMALL trial; member of the European Society of Breast Imaging executive board; 
member of the European Society of Radiology Research Committee; member of the 
editorial board for European Journal of Radiology; member of the Dutch Breast 
Cancer Research Group; and associate editor for Radiology. Katja Pinker Grants 
or contracts from the Research and Innovation Framework Programme, FET Open, 
Anniversary Fund of the Oesterreichische Nationalbank, Vienna Science and 
Technology Fund, Memorial Sloan Kettering Cancer Center, and Breast Cancer 
Research Foundation; unpaid consultant for Genentech; consulting fees from 
Merantix, AURA Health Technologies, and Guerbet; payment or honoraria for 
lectures, presentations, speakers bureaus, manuscript writing, or educational 
events from the European Society of Breast Imaging, Bayer, Siemens Healthineers, 
International Diagnostic Course Davos, Olea Medical, and Roche; support for 
attending meetings and/or travel from the European Society of Breast Imaging; 
participation on a data and safety monitoring board or advisory board for Bayer 
and Guerbet; and institution (Memorial Sloan Kettering Cancer Center) has 
institutional financial interests relative to Grail. The other authors of this 
manuscript declare no relationships with any companies, whose products or 
services may be related to the subject matter of the article. Statistics and 
biometry: No complex statistical methods were necessary. Informed consent: 
Written informed consent was not required. Ethical approval: Institutional 
Review Board approval was not required. Study subjects or cohorts overlap: Not 
applicable. Methodology: Practice recommendations


886. J Med Internet Res. 2025 May 5;27:e69284. doi: 10.2196/69284.

The Applications of Large Language Models in Mental Health: Scoping Review.

Jin Y(#)(1), Liu J(#)(1), Li P(#)(1), Wang B(#)(1), Yan Y(2), Zhang H(3), Ni 
C(3), Wang J(4), Li Y(5), Bu Y(5), Wang Y(2).

Author information:
(1)Department of Statistics, Faculty of Arts and Sciences, Beijing Normal 
University, Beijing, China.
(2)School of Psychology, Center for Studies of Psychological Application, and 
Guangdong Key Laboratory of Mental Health and Cognitive Science, Key Laboratory 
of Brain, Cognition and Education Sciences, Ministry of Education, South China 
Normal University, Guangzhou, Guangdong, China.
(3)School of Statistics, Beijing Normal University, Beijing, China.
(4)Faculty of Computer Science, Guangdong Polytechnic Normal University, 
Guangzhou, Guangdong, China.
(5)The People's Hospital of Pingbian County, Honghe, Yunnan, China.
(#)Contributed equally

BACKGROUND: Mental health is emerging as an increasingly prevalent public issue 
globally. There is an urgent need in mental health for efficient detection 
methods, effective treatments, affordable privacy-focused health care solutions, 
and increased access to specialized psychiatrists. The emergence and rapid 
development of large language models (LLMs) have shown the potential to address 
these mental health demands. However, a comprehensive review summarizing the 
application areas, processes, and performance comparisons of LLMs in mental 
health has been lacking until now.
OBJECTIVE: This review aimed to summarize the applications of LLMs in mental 
health, including trends, application areas, performance comparisons, 
challenges, and prospective future directions.
METHODS: A scoping review was conducted to map the landscape of LLMs' 
applications in mental health, including trends, application areas, comparative 
performance, and future trajectories. We searched 7 electronic databases, 
including Web of Science, PubMed, Cochrane Library, IEEE Xplore, Weipu, CNKI, 
and Wanfang, from January 1, 2019, to August 31, 2024. Studies eligible for 
inclusion were peer-reviewed articles focused on LLMs' applications in mental 
health. Studies were excluded if they (1) were not peer-reviewed or did not 
focus on mental health or mental disorders or (2) did not use LLMs; studies that 
used only natural language processing or long short-term memory models were also 
excluded. Relevant information on application details and performance metrics 
was extracted during the data charting of eligible articles.
RESULTS: A total of 95 articles were drawn from 4859 studies using LLMs for 
mental health tasks. The applications were categorized into 3 key areas: 
screening or detection of mental disorders (67/95, 71%), supporting clinical 
treatments and interventions (31/95, 33%), and assisting in mental health 
counseling and education (11/95, 12%). Most studies used LLMs for depression 
detection and classification (33/95, 35%), clinical treatment support and 
intervention (14/95, 15%), and suicide risk prediction (12/95, 13%). Compared 
with nontransformer models and humans, LLMs demonstrate higher capabilities in 
information acquisition and analysis and efficiently generating natural language 
responses. Various series of LLMs also have different advantages and 
disadvantages in addressing mental health tasks.
CONCLUSIONS: This scoping review synthesizes the applications, processes, 
performance, and challenges of LLMs in the mental health field. These findings 
highlight the substantial potential of LLMs to augment mental health research, 
diagnostics, and intervention strategies, underscoring the imperative for 
ongoing development and ethical deliberation in clinical settings.

©Yu Jin, Jiayi Liu, Pan Li, Baosen Wang, Yangxinyu Yan, Huilin Zhang, Chenhao 
Ni, Jing Wang, Yi Li, Yajun Bu, Yuanyuan Wang. Originally published in the 
Journal of Medical Internet Research (https://www.jmir.org), 05.05.2025.

DOI: 10.2196/69284
PMCID: PMC12089884
PMID: 40324177 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


887. Gut. 2025 Sep 22:gutjnl-2025-335946. doi: 10.1136/gutjnl-2025-335946. Online 
ahead of print.

AI-empowered human microbiome research.

Zhou T(1)(2), Zhao F(3)(2)(4).

Author information:
(1)Institute of Zoology, Chinese Academy of Sciences, Beijing, China.
(2)University of Chinese Academy of Sciences, Beijing, People's Republic of 
China.
(3)Institute of Zoology, Chinese Academy of Sciences, Beijing, China 
zhfq@ioz.ac.cn.
(4)Key Laboratory of Systems Biology, Hangzhou Institute for Advanced Study, 
University of Chinese Academy of Sciences, Hangzhou, People's Republic of China.

Recent advances in high-throughput microbiome profiling have generated expansive 
data sets that offer unprecedented opportunities to investigate the role of 
microbes in human health. However, the complexity and high dimensionality of 
these data present significant analytical challenges that often exceed the 
capabilities of traditional computational methods. Artificial intelligence (AI), 
encompassing both classical machine learning and modern deep learning 
approaches, has emerged as a powerful solution to these challenges. In this 
review, we systematically explore AI-driven methodologies in microbiome 
research, including clustering algorithms, dimensionality reduction techniques, 
convolutional and recurrent neural networks, and emerging large language models. 
We assess how these approaches enable the extraction of meaningful biological 
patterns from complex microbial data from a multiscale perspective, facilitating 
insights into community dynamics, host-microbe interactions and functional 
genomics. Additionally, we explore the transformative impact of AI on 
translational applications across both academic research and real-world clinical 
settings, including disease diagnostics, therapeutic development and precision 
microbiome engineering. By critically evaluating the current capabilities and 
limitations of AI in this context, this review aims to chart a path forward for 
the integration of AI into microbiome research, ultimately accelerating 
innovations in personalised medicine and deepening our understanding of 
host-microbiome relationships.

© Author(s) (or their employer(s)) 2025. Re-use permitted under CC BY-NC. No 
commercial re-use. See rights and permissions. Published by BMJ Group.

DOI: 10.1136/gutjnl-2025-335946
PMID: 40983504

Conflict of interest statement: Competing interests: None declared.


888. J Med Internet Res. 2025 Nov 17;27:e68573. doi: 10.2196/68573.

Application of Narrative and AI-Assisted Follow-Up After Voluntary Medical Male 
Circumcision: Multicenter, Double-Blind, Prospective, Randomized Controlled 
Trial.

Wang L(#)(1), Peng Y(#)(2), Zhang G(#)(1), Huang Y(#)(1), Yang G(1), Jiang Y(3), 
Xu Q(4), Yu J(5), Jin J(6), Qiao H(7), Wu Q(7), Wei Z(8), Tang W(1), Liu J(1).

Author information:
(1)Department of Urology, First Affiliated Hospital of Chongqing Medical 
University, No. 1 Youyi Road, Chongqing, Yuzhong District, 400016, China, 86 
18696682178.
(2)Department of Urology, Peking Union Medical College Hospital, Beijing, China.
(3)Department of Urology, First Hospital of Jilin University, Changchun, China.
(4)Department of Urology, Yongchuan Hospital of Chongqing Medical University, 
Chongqing, China.
(5)School of Nursing, Xiangya Medical School, Sichuan, China.
(6)Department of Psychiatry, Chongqing Cancer Hospital, Chongqing, China.
(7)School of Nursing, Chongqing Medical University, Chongqing, China.
(8)School of the First Clinical Medicine, Chongqing Medical University, 
Chongqing, China.
(#)Contributed equally

BACKGROUND: Postoperative anxiety following voluntary medical male circumcision 
(VMMC) poses a significant health challenge, with limited telemedicine access 
and inadequate communication compromising recovery and adherence. 
Narrative-based interventions have shown promise in reducing psychological 
distress in other contexts, and large language models may enable automated 
follow-up, but their role in VMMC care remains underexplored.
OBJECTIVE: We evaluated the effect of a narrative-enhanced tool (NET) on 
anxiety, sleep quality, quality of life, and pain management and identified risk 
factors for postoperative anxiety. We also assessed the feasibility of an 
artificial intelligence-assisted consultation (AAC) system in improving 
follow-up efficiency.
METHODS: From October 1, 2023, to April 29, 2024, patients aged ≥15 years 
undergoing VMMC were recruited and randomized 1:1 to a standardized risk tool 
(SRT) or NET group. In addition to the routine postoperative communication, the 
NET group received a narrative video highlighting positive recovery experiences. 
Both groups accessed an AAC chatbot for automated follow-ups. Primary outcomes 
were anxiety levels measured by the 7-item Generalized Anxiety Disorder scale 
(GAD-7), sleep quality measured by Pittsburgh Sleep Quality Index, quality of 
life measured by 3-level EuroQoL 5D questionnaire, and pain levels measured by 
Numerical Rating Scale. Secondary outcomes included analgesic use, satisfaction, 
and health care worker efficiency. Repeated measures ANOVA assessed trends and 
regression identified risk factors for anxiety.
RESULTS: Between October 1, 2023, and April 29, 2024, 671 eligible participants 
were enrolled, with 388 completing the 30-day follow-up (SRT group: n=189, mean 
age 26.21, SD 3.69 years; NET group: n=199, mean age 26.41, SD 3.56 years; 
P=.60). Both groups exhibited increased anxiety levels, diminished quality of 
life, and poorer sleep quality during the 30-day postoperative period. However, 
compared to SRT, the NET group demonstrated lower GAD-7 scores (7.06, SD 2.73 
vs. 9.95, SD 3.50; P<.001), improved sleep quality (12.29, SD 3.57 vs 13.20, SD 
3.54; P=.01), higher quality of life scores (0.87, SD 0.07 vs 0.84, SD 0.09; 
P<.001), more regular analgesic use (154/173, 89.02% vs 100/169, 59.17%; 
P<.001), reduced opioid consumption (5/173, 2.89% vs 25/169, 14.79%; P<.001), 
and higher pain medication satisfaction (4.21, SD 0.69 vs 3.76, SD 0.97; 
P<.001). Multivariate analysis identified SRT assignment, inability to recall 
opioid risk levels, hematoma, swelling, and pain as independent risk factors for 
elevated GAD-7 scores. Implementation of the AAC substantially reduced health 
care worker follow-up time (2.34, SD 1.95 min vs 7.85, SD 2.65 min; P<.001).
CONCLUSIONS: The study demonstrates that narrative is effective in reducing 
anxiety, improving quality of life, and improving pain management post-VMMC. The 
integration of artificial intelligence into clinical follow-up protocols has the 
potential to enhance health care worker efficiency without compromising patient 
satisfaction.

© Linfeng Wang, Yueqiang Peng, Gaojie Zhang, Yong Huang, Guo Yang, Yu Jiang, 
Qiao Xu, Jiang Yu, Jiajia Jin, Hong Qiao, Qingyu Wu, Ziling Wei, Wei Tang, Jiayu 
Liu. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/68573
PMCID: PMC12622909
PMID: 41248462 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


889. Ann Neurol. 2025 Sep;98(3):431-447. doi: 10.1002/ana.27229. Epub 2025 May 27.

AI in Neurology: Everything, Everywhere, all at Once PART 2: Speech, Sentience, 
Scruples, and Service.

Rizzo M(1).

Author information:
(1)Department of Neurological Sciences, University of Nebraska Medical Center, 
Omaha, NE.

Artificial intelligence (AI) applications are finding use in real-world 
neurological settings. Whereas part 1 of this 3-part review series focused on 
the birth of AI and its foundational principles, this part 2 review shifts gears 
to explore more practical aspects of neurological care. The review details how 
large language models, generative AI, and robotics are supporting diagnostic 
accuracy, patient interaction, and treatment personalization. Special attention 
is given to ethical and philosophical facets of AI that nonetheless impact 
practical aspects of care and patient safety, such as accountability for 
AI-driven decisions and the "black box" nature of many algorithms. We will 
discuss whether AI systems can develop sentience, and the implications for 
human-AI collaboration. By examining human-robot interactions in neurology, this 
part 2 review highlights the profound impact AI could have on patient care and, 
as covered in the ensuing part 3, on global health care delivery and data 
analytics, while maintaining ethical oversight and human control. ANN NEUROL 
2025;98:431-447.

© 2025 The Author(s). Annals of Neurology published by Wiley Periodicals LLC on 
behalf of American Neurological Association.

DOI: 10.1002/ana.27229
PMCID: PMC12392058
PMID: 40421866 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no conflicts of interest to 
declare.


890. J Med Internet Res. 2025 Jul 23;27:e71418. doi: 10.2196/71418.

Perceptions and Attitudes of Chinese Oncologists Toward Endorsing AI-Driven 
Chatbots for Health Information Seeking Among Patients with Cancer: 
Phenomenological Qualitative Study.

Zeng L(1), Li Q(1), Zuo Y(2)(3)(4), Zhang Y(5), Li Z(6).

Author information:
(1)Division of Abdominal Tumor Multimodality Treatment, Cancer Center, West 
China Hospital, Sichuan University, Chengdu, China.
(2)Department of Gynecology and Obstetrics Nursing, West China Second University 
Hospital, Sichuan University, Chengdu, China.
(3)West China School of Nursing, Sichuan University, Chengdu, China.
(4)Key Laboratory of Birth Defects and Related Disease of Women and Children, 
Sichuan University, Chengdu, China.
(5)Division of Internal Medicine, Institute of Integrated Traditional Chinese 
and Western Medicine, West China Hospital, Sichuan University, Chengdu, China.
(6)Department of Radiation Oncology, Hainan Affiliated Hospital of Hainan 
Medical University (Hainan General Hospital), #19 Xiuhua Road, Xiuying District, 
Haikou, 570311, China, 86 18898963083.

BACKGROUND: Chatbots driven by large language model artificial intelligence (AI) 
have emerged as potential tools to enhance health information access for 
patients with cancer. However, their integration into patient education raises 
concerns among oncologists. Limited literature has examined the perceptions and 
attitudes of oncologists in terms of endorsing AI-driven chatbots for health 
information.
OBJECTIVE: This study aims to explore the perceptions and attitudes of Chinese 
oncologists toward endorsing AI-driven chatbots to patients with cancer.
METHODS: In this phenomenological qualitative study, we purposively sampled 
oncologists from 4 hospitals in Southwest and East China and conducted 
semistructured interviews with 24 participants between November 19, 2024, and 
December 21, 2024. The data saturation principle was observed to determine the 
end point of data collection. Data were analyzed using the Colaizzi method.
RESULTS: The participants were aged 42.0 (range 29-53) years on average, 
including 9 (37%) female and 15 (62%) male participants. The participants had an 
average of 8.8 (range 1-25) years in oncology. Of the participants, 7 (29%) had 
recommended AI chatbots to patients. Three key themes were revealed from 
analysis of interview transcriptions, including perceived benefits, significant 
concerns, and impacts on doctor-patient dynamics. Benefits included enhanced 
accessibility and potential support for chronic condition management. Concerns 
centered on liability, misinformation, lack of personalization, privacy and data 
security risks, and patient readiness and education. Oncologists stressed a dual 
impact of AI chatbots on doctor-patient dynamics, recognizing the potential for 
improved communication and risks of trust erosion due to overreliance on AI.
CONCLUSIONS: While recognizing the potential of AI-driven chatbots to enhance 
accessibility of health information and chronic disease management, Chinese 
oncologists report significant concerns, including liability, misinformation, 
lack of personalization, privacy and data security risks, and patient readiness. 
Addressing the challenges requires comprehensive solutions, such as clear 
policies and guidelines, rigorous testing and validation, institutional 
endorsement, and robust patient and provider education. Future efforts should 
focus on resolving the barriers while leveraging the strengths of AI technology 
to support patient-centered care in a safe, effective, and ethical manner.

© Lijuan Zeng, Qiaoqi Li, Qiaoqi Li, Ying Zhang, Zhaojun Li. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/71418
PMCID: PMC12309621
PMID: 40699917 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


891. BMC Med Educ. 2025 Dec 5;25(1):1678. doi: 10.1186/s12909-025-08234-z.

Factors influencing medical students' adoption of AI educational agents: an 
extended UTAUT model.

Zhao X(1), Liu W(1), Yue S(1), Chen J(1), Xia D(1), Bing K(2), Xia X(3), Wang 
K(4).

Author information:
(1)Faculty of Artificial Intelligence in Education, Central China Normal 
University, Wuhan, 430079, Hubei, China.
(2)Department of Surgical Education Research, Union Hospital, Tongji Medical 
College, Huazhong University of Science and Technology, 430022, Wuhan, China.
(3)Department of Nuclear Medicine, Union Hospital, Tongji Medical College, 
Huazhong University of Science and Technology, No.1277 Jiefang Avenue, Wuhan, 
430022, Hubei, China. xiaotian_xia@hust.edu.cn.
(4)Department of Urology, Tongji Medical College, Union Hospital, Huazhong 
University of Science and Technology, Wuhan, 430022, China. wks4869@hust.edu.cn.

BACKGROUND: Artificial intelligence (AI) is reshaping the landscape of medical 
education with unprecedented depth and breadth. As technologies like large 
language models and natural language processing advance, AI agents with 
multimodal interaction capabilities-such as intelligent teaching assistants and 
virtual simulation labs-are demonstrating immense potential. Concurrently, 
medical students face challenges including a disconnect between theoretical 
knowledge and clinical practice, excessive cognitive load, and a lack of 
personalized practical opportunities. Medical education AI agents are poised to 
address these issues, but their successful integration hinges on student 
acceptance and adoption. This study aims to fill a gap in the current empirical 
research by investigating the key psychological mechanisms and behavioral 
factors that influence medical students' adoption of AI educational agents.
METHODS: This study constructed an extended Unified Theory of Acceptance and Use 
of Technology (UTAUT) model by integrating four key variables tailored to the 
medical education context: AI Trust, Perceived Risk, Hedonic Motivation, and 
Trialability. A cross-sectional survey was conducted with an initial sample of 
200 clinical medicine students following their interaction with a 
custom-developed interactive medical education AI agent. After excluding invalid 
responses, a final valid sample of 155 participants was retained. Partial Least 
Squares Structural Equation Modeling (PLS-SEM) was employed to validate the 
theoretical model and test the research hypotheses.
RESULTS: The constructed model demonstrated strong explanatory power, 
successfully accounting for 85.3% of the variance in students' behavioral 
intention (R² = 0.853). Effort Expectancy (β = 0.362, p < 0.001) and Performance 
Expectancy (β = 0.297, p < 0.001) were the strongest direct positive predictors 
of behavioral intention, with Facilitating Conditions (β = 0.258, p = 0.002) 
also showing a significant impact. A noteworthy finding was that Social 
Influence had no significant effect on behavioral intention (β = 0.038, 
p = 0.633). Furthermore, Hedonic Motivation had a significant positive influence 
on both Effort Expectancy (β = 0.818, p < 0.001) and Performance Expectancy 
(β = 0.237, p < 0.001). AI Trust, Trialability, and lower Perceived Risk also 
significantly enhanced students' Performance Expectancy.
CONCLUSIONS: The findings indicate that for medical students, who are highly 
autonomous professional learners, the intrinsic value of an AI educational tool 
(i.e., its utility and ease of use) is the dominant factor in their adoption 
decisions, far outweighing the social influence of peers or authorities. 
Therefore, the key to successfully promoting such technologies lies in building 
users' intrinsic trust, reducing their perceived risks, and providing an 
engaging, immersive learning experience. These findings provide a solid 
empirical basis for the optimal design of medical AI educational agents and for 
strategies to effectively integrate them into the curriculum.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-08234-z
PMCID: PMC12681116
PMID: 41350656 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was reviewed and approved by the Ethics Committee of 
Union Hospital, Tongji Medical College, Huazhong University of Science and 
Technology (No.: 20250985). The Ethics Committee of Union Hospital, Tongji 
Medical College, Huazhong University of Science and Technology waived the need 
of informed consent. All procedures were conducted in accordance with the 
ethical standards of the institutional and/or national research committee and 
with the 1964 Helsinki declaration and its later amendments. Consent for 
publication: Not applicable. Competing interests: The authors declare no 
competing interests.


892. J Med Internet Res. 2025 Jun 19;27:e73222. doi: 10.2196/73222.

Large Language Model-Assisted Surgical Consent Forms in Non-English Language: 
Content Analysis and Readability Evaluation.

Oh N(1), Kim J(1), Park S(1), An S(1), Lee E(1), Do H(1), Baik J(1), Gwon SM(1), 
Rhu J(1), Choi GS(1), Park S(1), Cho JY(2), Lee HW(2), Lee B(2), Jeong ES(3), 
Lee JM(4), Choi Y(4), Kwon J(5), Kim KD(6), Kim SH(7), Chun GS(7).

Author information:
(1)Department of Surgery, Samsung Medical Center, 81 Ilwonro, Seoul, Republic of 
Korea, 82 1093650277.
(2)Department of Surgery, Seoul National University Bundang Hospital, 
Seongnam-si, Republic of Korea.
(3)Department of Surgery, Dongguk University Ilsan Hospital, Goyang-si, Republic 
of Korea.
(4)Department of Surgery, Seoul National University Hospital, Seoul, Republic of 
Korea.
(5)Department of Surgery, Soon Chun Hyang University Cheonan Hospital, Cheonan, 
Republic of Korea.
(6)Department of Surgery, Inha University Hospital, Incheon, Republic of Korea.
(7)Department of Surgery, Chungnam National University, Daejeon, Republic of 
Korea.

BACKGROUND: Surgical consent forms convey critical information; yet, their 
complex language can limit patient comprehension. Large language models (LLMs) 
can simplify complex information and improve readability, but evidence of the 
impact of LLM-generated modifications on content preservation in non-English 
consent forms is lacking.
OBJECTIVE: This study evaluates the impact of LLM-assisted editing on the 
readability and content quality of surgical consent forms in Korean-particularly 
consent documents for standardized liver resection-across multiple institutions.
METHODS: Standardized liver resection consent forms were collected from 7 South 
Korean medical institutions, and these forms were simplified using ChatGPT-4o. 
Thereafter, readability was assessed using KReaD and Natmal indices, while text 
structure was evaluated based on character count, word count, sentence count, 
words per sentence, and difficult word ratio. Content quality was analyzed 
across 4 domains-risk, benefit, alternative, and overall impression-using 
evaluations from 7 liver resection specialists. Statistical comparisons were 
conducted using paired 2-sided t tests, and a linear mixed-effects model was 
applied to account for institutional and evaluator variability.
RESULTS: Artificial intelligence-assisted editing significantly improved 
readability, reducing the KReaD score from 1777 (SD 28.47) to 1335.6 (SD 59.95) 
(P<.001) and the Natmal score from 1452.3 (SD 88.67) to 1245.3 (SD 96.96) 
(P=.007). Sentence length and difficult word ratio decreased significantly, 
contributing to increased accessibility (P<.05). However, content quality 
analysis showed a decline in the risk description scores (before: 2.29, SD 0.47 
vs after: 1.92, SD 0.32; P=.06) and overall impression scores (before: 2.21, SD 
0.49 vs after: 1.71, SD 0.64; P=.13). The linear mixed-effects model confirmed 
significant reductions in risk descriptions (β₁=-0.371; P=.01) and overall 
impression (β₁=-0.500; P=.03), suggesting potential omissions in critical safety 
information. Despite this, qualitative analysis indicated that evaluators did 
not find explicit omissions but perceived the text as overly simplified and less 
professional.
CONCLUSIONS: Although LLM-assisted surgical consent forms significantly enhance 
readability, they may compromise certain aspects of content completeness, 
particularly in risk disclosure. These findings highlight the need for a 
balanced approach that maintains accessibility while ensuring medical and legal 
accuracy. Future research should include patient-centered evaluations to assess 
comprehension and informed decision-making as well as broader multilingual 
validation to determine LLM applicability across diverse health care settings.

© Namkee Oh, Jongman Kim, Sunghae Park, Sunghyo An, Eunjin Lee, Hayeon Do, 
Jiyoung Baik, Suk Min Gwon, Jinsoo Rhu, Gyu-Seong Choi, Seonmin Park, Jai Young 
Cho, Hae Won Lee, Boram Lee, Eun Sung Jeong, Jeong-Moo Lee, YoungRok Choi, Jieun 
Kwon, Kyeong Deok Kim, Seok-Hwan Kim, Gwang-Sik Chun. Originally published in 
the Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/73222
PMCID: PMC12200805
PMID: 40537063 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


893. J Med Internet Res. 2025 Sep 29;27:e67640. doi: 10.2196/67640.

Preferences, Perceptions, and Use of Online Nutrition Content Among Young 
Australian Adults: Qualitative Study.

Lam BT(1), Szymlek-Gay EA(1), Larsson C(2), Margerison C(1).

Author information:
(1)School of Exercise and Nutrition Sciences, Institute for Physical Activity 
and Nutrition, Deakin University, Geelong, Australia.
(2)Department of Food and Nutrition, and Sport Science, University of 
Gothenburg, Gothenburg, Sweden.

BACKGROUND: Nutrition misinformation is pervasive on frequently accessed online 
sources such as social media platforms and websites. Young adults are at a high 
risk of viewing or engaging with this content due to their high internet and 
social media usage.
OBJECTIVE: This study aimed to understand young adults' preferences, 
perceptions, and use of online nutrition content.
METHODS: Young Australian adults (aged 18-25 years) were recruited and 
interviewed individually via video calling (Zoom; Zoom Video Communications) 
between December 2023 and February 2024. Participants were recruited via 
convenience sampling using Facebook advertising. The interviewer followed a 
semistructured format, and questions were guided using a piloted template. 
Reflexive thematic analysis was conducted using NVivo (Lumivero) to explore the 
preferences, perceptions, and use of online nutrition content among the sample.
RESULTS: The sample (N=20; mean age 22.9 y, SD 2.3 y) was predominantly female 
(n=13, 65%) and had, or was studying toward, a tertiary qualification (16/17, 
94%). Most participants used social media (19/20, 95%) and internet websites 
(16/20, 80%) to access nutrition content. Other platforms used included 
generative artificial intelligence (n=1), apps (n=1), eBooks (n=1), newsletters 
(n=1), and podcasts (n=1). When exploring perceptions, most participants agreed 
that online nutrition content was quick and easy to find and informative. 
Furthermore, perceived reliability and engagement depended on several factors 
such as the creator's credentials, length and format of content, consensus on 
topics, and sponsorships. Short-form content was not considered reliable, 
despite its engaging nature. Content containing sponsorships or product 
endorsements was met with skepticism. However, participants were more likely to 
trust content reportedly created by health professionals, but it was unknown 
whether they were accessing verified professionals. The oversaturation of 
content demotivated participants from evaluating the reliability of content. 
When asked about preferences, participants valued both short- and long-form 
content, and evidence-based content such as statistics and references and 
preferred casual and entertaining content that incorporated high-quality and 
dynamic editing techniques such as voiceovers.
CONCLUSIONS: The study identified the online nutrition content sources and 
topics young Australian adults access and the key factors that influence their 
perceptions and preferences. Young Australian adults acknowledge that 
misinformation is not exclusive to certain platforms. The accessibility and 
engagement of content and the ambiguity of professional "credentials" may lead 
them to trust information that is potentially of low quality and accuracy. 
Findings also show that there needs to be a balance between engaging formats and 
presenting evidence-based information when designing online nutrition content to 
engage these audiences while combatting nutrition misinformation. Future 
research should explore how these factors impact usage of online nutrition 
content and dietary behaviors among young Australian adults. Further 
consultation with this cohort can inform tailored interventions that aim to 
enhance their food and nutrition literacy and diet quality.

©Bill Tiger Lam, Ewa A Szymlek-Gay, Christel Larsson, Claire Margerison. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 29.09.2025.

DOI: 10.2196/67640
PMCID: PMC12519026
PMID: 41021916 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


894. J Med Internet Res. 2025 Nov 18;27:e80497. doi: 10.2196/80497.

Online Health-Seeking Behaviors and Information Needs Among Patients With 
Lymphoma in China: Study of Regional and Temporal Trends.

Ning K(#)(1), Gu H(#)(2), Franklin M(#)(3), Yang X(#)(4), Wei R(#)(5), Song 
Z(#)(1), Xu H(1), Leng LL(6), Liu M(7), Dai J(1), Zhang J(2), Zeng R(1), Hou 
Y(1), Wang R(1), Liu Z(1), Huang C(1), Cai R(1), Liu H(4), Xia LC(4).

Author information:
(1)Department of Network Intelligence, Peng Cheng Laboratory, Shenzhen, 
Guangdong, China.
(2)Department of Patient Services, HOUSE086, Beijing, China.
(3)Department of Statistical Sciences, University of Toronto, Toronto, ON, 
Canada.
(4)Department of Statistics and Mathematical Finance, South China University of 
Technology, Guangzhou, Guangdong, China.
(5)Institute of Hematology, Peking University People's Hospital, Beijing, China.
(6)Department of Sociology, Zhejiang University, Hangzhou, Zhejiang, China.
(7)School of Biomedical Engineering, Sun Yat-sen University, Shenzhen, 
Guangdong, China.
(#)Contributed equally

BACKGROUND: Health disparities are closely associated with socioeconomic 
inequalities. Although this relationship is well recognized in the context of 
traditional health care access, its influence on online health-seeking behaviors 
such as posting questions on patient forums and seeking peer responses remains 
poorly understood, particularly in the context of resource-limited regions. 
Furthermore, it is unclear what types of questions are most frequently asked 
online and to what extent these questions receive helpful responses.
OBJECTIVE: This study aims to examine how socioeconomic status influences online 
health-seeking behavior by analyzing regional disparities in forum participation 
and their correlation with economic development. In addition, it aims to 
identify unmet informational needs among patients with lymphoma through large 
language model (LLM)-based forum thread classification and expert evaluation of 
forum responses by using data from the largest online blood cancer forum in 
China.
METHODS: We analyzed over 110,000 patient-initiated forum threads posted between 
2012 and 2023, covering all the provinces of mainland China. Regional trends in 
forum participation rates were examined and correlated with economic 
development, as measured by gross regional product per capita. Second, an LLM 
was used to classify the threads into 6 predefined topics based on their 
semantic content, thereby providing an overview of the topics that users cared 
about. Additionally, an expert manual review was conducted based on relevance, 
accuracy, and comprehensiveness to assess whether users' questions were 
adequately addressed within the forum discussions.
RESULTS: Regional forum participation rates were significantly associated with 
levels of regional economic development (Wilcoxon rank-sum test; P<.001), with 
the highest participation rates in the East Coast regions. Participation rates 
in less-developed regions steadily increased, reflecting the growing public 
demand for accessible health information. LLM-based analysis revealed that most 
discussions centered on medical concerns such as interpreting reports and 
selecting treatment plans across all regions. However, only 37% (117/316) of the 
user questions received useful responses, underscoring persistent gaps in access 
to reliable information.
CONCLUSIONS: To our knowledge, this study represents the most comprehensive 
real-world investigation to date of spontaneous online forum participation and 
information needs among patients with cancer. Our findings highlight the 
necessity for government and health care providers to implement initiatives such 
as artificial intelligence-driven information platforms and region-specific 
health education campaigns to bridge information gaps, reduce regional 
disparities, and improve patient outcomes across China.

©Kaida Ning, Hongfei Gu, Meredith Franklin, Xiaoying Yang, Rong Wei, Zhen Song, 
Hong Xu, Ling Li Leng, Mengting Liu, Ju Dai, Jin Zhang, Rui Zeng, Yongshuai Hou, 
Rongjie Wang, Zirong Liu, Chenyang Huang, Runfa Cai, Huiling Liu, Li Charlie 
Xia. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 18.11.2025.

DOI: 10.2196/80497
PMCID: PMC12673299
PMID: 41252192 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


895. AJR Am J Roentgenol. 2025 Jul;225(1):e2532729. doi: 10.2214/AJR.25.32729. Epub 
2025 Jul 16.

Multimodal Large Language Model With Knowledge Retrieval Using Flowchart 
Embedding for Forming Follow-Up Recommendations for Pancreatic Cystic Lesions.

Zhu Z(1), Liu J(1), Hong CW(1), Houshmand S(1), Wang K(1), Yang Y(1).

Author information:
(1)Department of Radiology and Biomedical Imaging, University of California, San 
Francisco, 505 Parnassus Ave, San Francisco, CA 94143.

Comment in
    AJR Am J Roentgenol. 2025 Jul;225(1):e2533102. doi: 10.2214/AJR.25.33102.
    AJR Am J Roentgenol. 2025 Jul;225(1):e2533129. doi: 10.2214/AJR.25.33129.

BACKGROUND. The American College of Radiology (ACR) Incidental Findings 
Committee (IFC) algorithm provides guidance for pancreatic cystic lesion (PCL) 
management. Its implementation using plain-text large language model (LLM) 
solutions is challenging given that key components include multimodal data 
(e.g., figures and tables). OBJECTIVE. The purpose of the study is to evaluate a 
multimodal LLM approach incorporating knowledge retrieval using flowchart 
embedding for forming follow-up recommendations for PCL management. METHODS. 
This retrospective study included patients who underwent abdominal CT or MRI 
from September 1, 2023, to September 1, 2024, and whose report mentioned a PCL. 
The reports' Findings sections were inputted to a multimodal LLM (GPT-4o). For 
task 1 (198 patients: mean age, 69.0 ± 13.0 [SD] years; 110 women, 88 men), the 
LLM assessed PCL features (presence of PCL, PCL size and location, presence of 
main pancreatic duct communication, presence of worrisome features or high-risk 
stigmata) and formed a follow-up recommendation using three knowledge retrieval 
methods (default knowledge, plain-text retrieval-augmented generation [RAG] from 
the ACR IFC algorithm PDF document, and flowchart embedding using the LLM's 
image-to-text conversion for in-context integration of the document's flowcharts 
and tables). For task 2 (85 patients: mean initial age, 69.2 ± 10.8 years; 48 
women, 37 men), an additional relevant prior report was inputted; the LLM 
assessed for interval PCL change and provided an adjusted follow-up schedule 
accounting for prior imaging using flowchart embedding. Three radiologists 
assessed LLM accuracy in task 1 for PCL findings in consensus and follow-up 
recommendations independently; one radiologist assessed accuracy in task 2. 
RESULTS. For task 1, the LLM with flowchart embedding had accuracy for PCL 
features of 98.0-99.0%. The accuracy of the LLM follow-up recommendations based 
on default knowledge, plain-text RAG, and flowchart embedding for radiologist 1 
was 42.4%, 23.7%, and 89.9% (p < .001), respectively; radiologist 2 was 39.9%, 
24.2%, and 91.9% (p < .001); and radiologist 3 was 40.9%, 25.3%, and 91.9% (p < 
.001). For task 2, the LLM using flowchart embedding showed an accuracy for 
interval PCL change of 96.5% and for adjusted follow-up schedules of 81.2%. 
CONCLUSION. Multimodal flowchart embedding aided the LLM's automated provision 
of follow-up recommendations adherent to a clinical guidance document. CLINICAL 
IMPACT. The framework could be extended to other incidental findings through the 
use of other clinical guidance documents as the model input.

DOI: 10.2214/AJR.25.32729
PMID: 40237430 [Indexed for MEDLINE]


896. IEEE Trans Biomed Eng. 2025 Aug 11;PP. doi: 10.1109/TBME.2025.3597325. Online 
ahead of print.

ChatRadio-Valuer: A Chat Large Language Model for Generalizable Radiology 
Impression Generation on Multi-institution and Multi-system Data.

Zhong T, Zhao W, Zhang Y, Pan Y, Dong P, Jiang Z, Jiang H, Zhou Y, Kui X, Shang 
Y, Zhao L, Yang L, Wei Y, Li Z, Zhang J, Yang L, Chen H, Zhao H, Liu Y, Zhu N, 
Li Y, Wang Y, Yao J, Wang J, Zeng Y, He L, Zheng C, Zhang Z, Li M, Liu Z, Dai H, 
Wu Z, Zhang L, Zhang S, Cai X, Hu X, Zhao S, Jiang X, Zhang X, Liu W, Li X, Zhu 
D, Guo L, Shen D, Han J, Liu T, Liu J, Zhang T.

Achieving clinical level performance and widespread deployment for generating 
radiology impressions encounters a giant challenge for conventional artificial 
intelligence models tailored to specific diseases and organs. Concurrent with 
the increasing accessibility of radiology reports and advancements in modern 
general AI techniques, the emergence and potential of deployable radiology AI 
exploration have been bolstered. Here, we present ChatRadio-Valuer, the first 
general radiology diagnosis large language model for localized deployment within 
hospitals and being close to clinical use for multi-institution and multi-system 
diseases. ChatRadio-Valuer achieved 15 state-of-the-art results across five 
human systems and six institutions in clinical-level events (n=332,673) through 
rigorous and full-spectrum assessment, including engineering metrics, clinical 
validation, and efficiency evaluation. Notably, it exceeded OpenAI's GPT-3.5 and 
GPT-4 models, achieving superior performance in comprehensive disease diagnosis 
compared to the average level of radiology experts. Besides, ChatRadio-Valuer 
supports zero-shot transfer learning, greatly boosting its effectiveness as a 
radiology assistant, while ensuring adherence to privacy standards and being 
readily utilized for large-scale patient populations. Our expeditions suggest 
the development of localized LLMs would become an imperative avenue in hospital 
applications.

DOI: 10.1109/TBME.2025.3597325
PMID: 40788800


897. Med Phys. 2025 May;52(5):3243-3257. doi: 10.1002/mp.17639. Epub 2025 Jan 27.

Development of a CT radiomics prognostic model for post renal tumor resection 
overall survival based on transformer enhanced K-means clustering.

Wang Y(1)(2), Li Y(3), Chen S(1)(2), Wen Z(1)(2), Hu Y(4)(5), Zhang H(6), Zhou 
P(2)(5)(7), Pang H(3).

Author information:
(1)School of Nursing, Southwest Medical University, Luzhou, China.
(2)Wound Healing Basic Research and Clinical Application Key Laboratory of 
Luzhou, Luzhou, China.
(3)Department of Oncology, The Affiliated Hospital of Southwest Medical 
University, Luzhou, China.
(4)Department of Medical Imaging, Southwest Medical University, Luzhou, China.
(5)Department of Radiology, The Affiliated Hospital of Southwest Medical 
University, Luzhou, China.
(6)Department of Radiotherapy, Jiangxi Cancer Hospital, The Second Affiliated 
Hospital of Nanchang Medical College, Jiangxi Clinical Research Center for 
Cancer, Nanchang, China.
(7)Department of Nursing, The Affiliated Hospital of Southwest Medical 
University, Luzhou, China.

BACKGROUND: Kidney tumors, common in the urinary system, have widely varying 
survival rates post-surgery. Current prognostic methods rely on invasive 
biopsies, highlighting the need for non-invasive, accurate prediction models to 
assist in clinical decision-making.
PURPOSE: This study aimed to construct a K-means clustering algorithm enhanced 
by Transformer-based feature transformation to predict the overall survival rate 
of patients after kidney tumor resection and provide an interpretability 
analysis of the model to assist in clinical decision-making.
METHODS: This study was based on a publicly available C4KC-KiTS-2019 dataset 
from the TCIA database, including preoperative computed tomography (CT) images 
and survival time data of 210 patients. Initially, the radiomics features of the 
kidney tumor area were extracted using the 3D slicer software. Feature selection 
was then conducted using ICC, mRMR algorithms, and LASSO regression to calculate 
radiomics scores. Subsequently, the selected features were input into a 
pre-trained Transformer model for feature transformation to obtain a 
higher-dimensional feature set. Then, K-means clustering was performed using 
this feature set, and the model was evaluated using receiver operating 
characteristic (ROC) and Kaplan-Meier curves. Finally, the SHAP interpretability 
algorithm was used for the feature importance analysis of the K-means clustering 
results.
RESULTS: Eleven important features were selected from 851 radiomics features. 
The K-means clustering model after Transformer feature transformation showed 
AUCs of 0.889, 0.841, and 0.926 for predicting 1-, 3-, and 5-year overall 
survival rates, respectively, thereby outperforming both the K-means model with 
original feature inputs and the radiomics score method. A clustering analysis 
revealed survival prognosis differences among different patient groups, and a 
SHAP analysis provided insights into the features that had the most significant 
impacts on the model predictions.
CONCLUSIONS: The K-means clustering algorithm enhanced by the Transformer 
feature transformation proposed in this study demonstrates promising accuracy 
and interpretability in predicting the overall survival rate after kidney tumor 
resection. This method provides a valuable tool for clinical decision-making and 
contributes to improved management and treatment strategies for patients with 
kidney tumors.

© 2025 American Association of Physicists in Medicine.

DOI: 10.1002/mp.17639
PMCID: PMC12082754
PMID: 39871101 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


898. Am J Ophthalmol. 2025 Dec 3;283:129-137. doi: 10.1016/j.ajo.2025.11.046. Online 
ahead of print.

ChatGPT-Assisted Glaucoma Diagnosis: A Health-Equitable Multi-Ancestry Analysis 
Using Visual Field and Optical Coherence Tomography Data.

Huang AS(1), Fam A(2), Zhao H(2), Paulescu N(2), Fabczak-Kubicka A(2), Wiggs 
JL(3), Zebardast N(3), Friedman DS(3), DO R(4), Aziz K(3), Kang JH(5), Elze 
T(3), Wang M(3), Harris A(2), Tai TYT(2), Tsai JC(2), Pasquale LR(2).

Author information:
(1)From the Department of Ophthalmology, Icahn School of Medicine at Mount Sinai 
(A.S.H., A.F., H.Z., N.P., A.F.-K., A.H., T.Y.T.T., J.C.T., L.R.P.), New York 
Eye and Ear Infirmary of Mount Sinai, New York, New York, USA. Electronic 
address: andyshihuang@gmail.com.
(2)From the Department of Ophthalmology, Icahn School of Medicine at Mount Sinai 
(A.S.H., A.F., H.Z., N.P., A.F.-K., A.H., T.Y.T.T., J.C.T., L.R.P.), New York 
Eye and Ear Infirmary of Mount Sinai, New York, New York, USA.
(3)Department of Ophthalmology, Massachusetts Eye and Ear (J.L.W., N.Z., D.S.F., 
K.A., T.E., M.W.), Harvard Medical School, Boston, Massachusetts, USA.
(4)The Charles Bronfman Institute for Personalized Medicine (R.D.), Icahn School 
of Medicine at Mount Sinai, New York, New York, USA.
(5)Channing Division of Network Medicine, Department of Medicine (J.H.K.), 
Brigham and Women's Hospital and Harvard Medical School, Boston, Massachusetts, 
USA.

PURPOSE: Early glaucoma detection is challenging due to variable ocular anatomy, 
non-glaucomatous optic neuropathy impacting optical coherence tomography (OCT) 
results, and the subjective nature of visual field (VF) tests. Multimodal large 
language models may overcome these challenges to provide equitable and accurate 
screening diagnoses across ancestries and glaucoma genetic predispositions. We 
evaluated ChatGPT o1 Pro's accuracy in identifying glaucoma using 
circumpapillary retinal nerve fiber layer (RNFL) OCT and VF data, and its 
consistency across ancestries and glaucoma polygenic risk scores (PRS).
DESIGN: Cross-sectional diagnostic accuracy study.
SETTINGS AND PARTICIPANTS: We enrolled 204 participants from the Mount Sinai 
BioMe Biobank for a comprehensive ophthalmic examination from November 2022 to 
March 2025. This cross-sectional diagnostic accuracy study included 38% European 
(EUR) and 62% non-European (non-EUR) participants stratified by low/intermediate 
(n = 107) and high-risk glaucoma PRS (n = 97). Two glaucoma specialists masked 
to PRS status provided a consensus reference diagnosis. ChatGPT received only 
de-identified VFs and OCT-RNFL numerical outputs to determine glaucoma status. 
Performance metrics were compared with the reference diagnosis. Subgroup 
comparisons by ancestry (EUR versus non-EUR) and PRS (high versus 
low/intermediate) were conducted. We used logistic regression models to assess 
the impacts of ancestry, PRS and ocular parameters on classification accuracy.
MAIN OUTCOME MEASURES: ChatGPT o1 Pro's diagnostic performance in detecting 
glaucoma compared to consensus specialist diagnoses, stratified by ancestry and 
genetic risk.
RESULTS: ChatGPT o1 Pro exhibited 96.0% sensitivity (95% confidence interval 
(CI): 88.3%-100%), 83.7% specificity (95% CI: 78.3%-89.1%), 85.2% accuracy (95% 
CI: 80.3%-90.1%), an area under the receiver operator curve (AUC) of 0.899, a 
positive predictive value (PPV) of 45.3% (95% CI: 31.9%-58.7%), and a negative 
predictive value (NPV) of 99.3% (95% CI: 98.0%-100%); κ for agreement with the 
consensus reference was 0.538. No significant differences were observed between 
EUR and non-EUR subgroups (AUC: 0.894 vs 0.906, P = .79; accuracy: 88.3% vs 
83.3%, P = .44) or high and low/intermediate-PRS subgroups (AUC: 0.889 vs 0.922, 
P = .45; accuracy: 85.4% vs 85.0%, P = .50). Global RNFL was the only 
determinant of reference disease classification (OR = 1.1 per micron, P < .001).
CONCLUSION: ChatGPT o1 Pro diagnosed glaucoma similarly to specialists using 
only VF and OCT data. The model performance was similar across ancestral groups 
and genetic predispositions to glaucoma.

Copyright © 2025 The Author(s). Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.ajo.2025.11.046
PMCID: PMC12698116
PMID: 41349854

Conflict of interest statement: Declaration of interests The authors declare the 
following financial interests/personal relationships which may be considered as 
potential competing interests: Andy S Huang reports administrative support and 
equipment, drugs, or supplies were provided by Northwell Manhattan Eye Ear and 
Throat Institute. Louis R Pasquale reports administrative support was provided 
by Research to Prevent Blindness. Louis R Pasquale reports administrative 
support was provided by National Eye Institute. Given Dr. Louis R Pasquale’s 
role on Editorial Board for AJO, he had no involvement in the peer review of 
this article and had no access to information regarding its peer review. Full 
responsibility for the editorial process for this article was delegated to 
another journal editor. If there are other authors, they declare that they have 
no known competing financial interests or personal relationships that could have 
appeared to influence the work reported in this paper.


899. Knee Surg Sports Traumatol Arthrosc. 2025 Oct;33(10):3478-3488. doi: 
10.1002/ksa.12571. Epub 2024 Dec 26.

High specificity of an AI-powered framework in cross-checking male professional 
football anterior cruciate ligament tear reports in public databases.

Diniz P(1)(2)(3)(4), Grimm B(3), Mouton C(1)(2), Ley C(5), Andersen TE(6), Seil 
R(1)(2)(3).

Author information:
(1)Department of Orthopaedic Surgery, Centre Hospitalier de Luxembourg - 
Clinique d'Eich, Luxembourg, Luxembourg.
(2)Luxembourg Institute of Research in Orthopaedics, Sports Medicine and Science 
(LIROMS), Luxembourg, Luxembourg.
(3)Luxembourg Institute of Health (LIH), Luxembourg, Luxembourg.
(4)Department of Bioengineering, iBB - Institute for Bioengineering and 
Biosciences, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, 
Portugal.
(5)Department of Mathematics, University of Luxembourg, Esch-sur-Alzette, 
Luxembourg.
(6)Oslo Sports Trauma Research Center, Department of Sports Medicine, Norwegian 
School of Sport Sciences, Oslo, Norway.

PURPOSE: While public databases like Transfermarkt provide valuable data for 
assessing the impact of anterior cruciate ligament (ACL) injuries in 
professional footballers, they require robust verification methods due to 
accuracy concerns. We hypothesised that an artificial intelligence (AI)-powered 
framework could cross-check ACL tear-related information from large publicly 
available data sets with high specificity.
METHODS: The AI-powered framework uses Google Programmable Search Engine to 
search a curated, multilingual list of websites and OpenAI's GPT to translate 
search queries, appraise search results and analyse injury-related information 
in search result items (SRIs). Specificity was the chosen performance metric-the 
AI-powered framework's ability to accurately identify texts that do not mention 
an athlete suffering an ACL tear-with SRI as the evaluation unit. A database of 
ACL tears in male professional footballers from first- and second-tier leagues 
worldwide (1999-2024) was collected from Transfermarkt.com, and players were 
randomly selected for appraisal until enough SRIs were obtained to validate the 
framework's specificity. Player age at injury and time until return-to-play 
(RTP) were recorded and compared with Union of European Football Associations 
(UEFA) Elite Club Injury Study data.
RESULTS: Verification of 231 athletes yielded 1546 SRIs. Human analysis of the 
SRIs showed that 335 mentioned an ACL tear, corresponding to 83 athletes with 
ACL tears. Specificity and sensitivity of GPT in identifying mentions of ACL 
tears in a player were 99.3% and 88.4%, respectively. Mean age at rupture was 
26.6 years (standard deviation: 4.6, 95% confidence interval [CI]: 25.6-27.6). 
Median RTP time was 225 days (interquartile range: 96, 95% CI: 209-251), which 
is comparable to reports using data from the UEFA Elite Club Injury Study.
CONCLUSION: This study shows that an AI-powered framework can achieve high 
specificity in cross-checking ACL tear reports in male professional football 
from public databases, markedly reducing manual workload and enhancing the 
reliability of media-based sports medicine research.
LEVEL OF EVIDENCE: Level III.

© 2024 European Society of Sports Traumatology, Knee Surgery and Arthroscopy.

DOI: 10.1002/ksa.12571
PMID: 39724452 [Indexed for MEDLINE]


900. Front Oncol. 2025 Aug 1;15:1613462. doi: 10.3389/fonc.2025.1613462. eCollection 
2025.

Pre-operative T-stage discrimination in gallbladder cancer using machine 
learning and DeepSeek-R1.

Chae J(1), Wang Z(1), Wu D(2), Zhang L(3), Tuzikov A(4), Madiyevich MT(5), Xu 
M(6), Yu D(6), Qin P(1).

Author information:
(1)Institute of Biopharmaceutical and Health Engineering, Shenzhen International 
Graduate School, Tsinghua University, Shenzhen, Guangdong, China.
(2)School of Communication Engineering and the Artificial Intelligence 
Institute, Hangzhou Dianzi University, Hangzhou, Zhejiang, China.
(3)The First Hospital of Hebei Medical University, Shijiazhuang, Hebei, China.
(4)United Institute of Informatics Problems, National Academy of Sciences of 
Belarus, Minsk, Belarus.
(5)Department of Biomedical Engineering & Tashkent State Technical University, 
Tashkent, Uzbekistan.
(6)Affiliated Fifth Hospital, Wenzhou Medical University, Wenzhou, 
Zhejiang, China.

BACKGROUND: Gallbladder cancer (GBC) frequently exhibits non-specific early 
symptoms, delaying diagnosis. This study (i) assessed whether routine blood 
biomarkers can distinguish early T stages via machine learning and (ii) compared 
the T-stage discrimination performance of a large language model (DeepSeek-R1) 
when supplied with (a) radiology-report text alone versus (b) radiology-report 
text plus blood-biomarker values.
METHODS: We retrospectively analyzed 232 pathologically confirmed GBC patients 
treated at Lishui Central Hospital between 2023 and 2024 (T1, n = 51; T2, n = 
181). Seven blood variables-neutrophil-to-lymphocyte ratio (NLR), 
monocyte-to-lymphocyte ratio (MLR), platelet-tolymphocyte ratio (PLR), 
carcino-embryonic antigen (CEA), carbohydrate antigen 19-9 (CA19-9), 
carbohydrate antigen 125 (CA125), and alpha-fetoprotein (AFP)-were used to train 
Random forest, Support Vector Machine (SVC), XGBoost, and LightGBM models. 
Synthetic Minority Over-sampling Technique (SMOTE) was applied only to the 
training folds in one setting and omitted in another. Model performance was 
evaluated on an independent test set (N = 47) by the area under the 
receiver-operating-characteristic curve (AUROC, 95% CI by 1 000-sample bootstrap 
confidence interval, CI); cross-validation (CV) accuracy served as a 
supplementary metric. DeepSeek-R1 was prompted in a zero-shot, chain-of-thought 
manner to classify T1 versus T2 using (a) the radiology report alone or (b) the 
report plus the patient's biomarker profile.
RESULTS: Biomarker-based machine-learning models yielded uniformly poor T-stage 
discrimination. Without SMOTE, individual models such as XGBoost achieved an 
AUROC of 0.508 on the independent test set, while recall for the T1 class 
remained low (e.g., 14.3% for some models), indicating performance near random 
chance. Applying SMOTE to the training data produced statistically significant 
gains in cross-validation (CV) accuracy for several models (e.g., XGBoost CV 
Acc. 0.71 → 0.80, p = 0.005; LGBM CV Acc. [No-SMOTE] → [SMOTE], p = 0.004). 
However, these improvements did not translate to better discrimination on the 
independent test set; for instance, XGBoost's AUROC decreased from 0.508 to 
0.473 after SMOTE application. Overall, the biomarker models failed to provide 
clinically meaningful T-stage differentiation. DeepSeek-R1 analyzing radiology 
text alone reached 89.6% accuracy on the full 232-patient cohort dataset, and 
consistently flagged T2 cases on phrases such as "gallbladder wall thickening." 
Supplying biomarker values did not change accuracy (89.6%).
CONCLUSIONS: The evaluated blood biomarkers did not independently aid early 
T-stage discrimination, and SMOTE offered no meaningful performance gain. 
Conversely, a radiologytext-driven large language model delivered high accuracy 
with interpretable rationale, highlighting its potential to guide surgical 
strategy in GBC. Prospective multi-center studies with larger cohorts are 
warranted to confirm these findings.

Copyright © 2025 Chae, Wang, Wu, Zhang, Tuzikov, Madiyevich, Xu, Yu and Qin.

DOI: 10.3389/fonc.2025.1613462
PMCID: PMC12355213
PMID: 40823085

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


901. Eur J Obstet Gynecol Reprod Biol. 2025 Sep;313:114625. doi: 
10.1016/j.ejogrb.2025.114625. Epub 2025 Aug 5.

Artificial intelligence in endometriosis care: A comparative analysis of large 
language model and human specialist responses to endometriosis-related queries.

Burla L(1), Metzler JM(2), Kalaitzopoulos DR(3), Kamm S(4), Ormos M(5), Passweg 
D(6), Schraag S(7), Samartzis EP(2), Samartzis N(3), Witzel I(2), Imesch P(8).

Author information:
(1)Department of Gynecology, University Hospital of Zurich, Zurich, Switzerland. 
Electronic address: laurinburla@hotmail.com.
(2)Department of Gynecology, University Hospital of Zurich, Zurich, Switzerland.
(3)Department of Gynecology and Obstetrics, Hospital Schaffhausen, Schaffhausen, 
Switzerland.
(4)Department of Gynecology and Obstetrics, Hospital Limmattal, Schlieren, 
Switzerland.
(5)Department of Gynecology and Obstetrics, Hospital Baden, Baden, Switzerland.
(6)Department of Gynecology and Obstetrics, City Hospital Zurich, Triemli, 
Zurich, Switzerland.
(7)Women's Health Clinic, Frau im Zentrum, Zurich, Switzerland.
(8)Department of Gynecology, University Hospital of Zurich, Zurich, Switzerland; 
Clinic for Gynecology, Bethanien Hospital, Zurich, Switzerland.

INTRODUCTION: Many women with endometriosis turn to digital sources for 
information. Meanwhile, large language models (LLMs) appear capable of offering 
health advice. This study aimed to evaluate the potential in answering 
endometriosis-related queries.
METHOD: This comparative study used 150 anonymized endometriosis Q&As from 
online forums (2021-2023), answered by human specialists. Another 150 responses 
were generated using an LLM (ChatGPT-4o). Eight expert reviewers, split into two 
groups, blindly evaluated either the human or artificial intelligence (AI) 
response for each question. The primary endpoint was whether responses could be 
correctly identified as human or AI. Secondary endpoints included incorrect 
information, harm, and suitability for patient communication.
RESULT: Each reviewer group assessed 600 responses; human and AI responses were 
differentiated significantly (χ2 = 246.162, p < 0.001, slight interrater 
agreement). Most responses were accurate (84.8 %), harmless (87.2 %), and 
patient-suitable (73.8 %). There were no significant differences regarding 
incorrect information (p = 0.308), harm likelihood (p = 0.944), harm extent 
(p = 0.892), medical consensus alignment (p = 0.235), or suitability for 
patients (p = 0.544).
CONCLUSION: This study found that AI was not inferior to human specialists in 
answering endometriosis-related queries. While reviewers were able to 
distinguish AI- from human-generated responses significantly, interrater 
agreement was only slight. No substantial differences were observed in medical 
content. As AI continues to evolve, patients increasingly turn to it for medical 
guidance, highlighting the need for greater specialization in endometriosis 
care. Future research should further investigate the risks, benefits, and 
patient acceptance of AI in clinical practice.

Copyright © 2025 The Author(s). Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ejogrb.2025.114625
PMID: 40829501 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


902. JAMA Netw Open. 2025 Apr 1;8(4):e256359. doi: 10.1001/jamanetworkopen.2025.6359.

Semantic Clinical Artificial Intelligence vs Native Large Language Model 
Performance on the USMLE.

Elkin PL(1)(2), Mehta G(1), LeHouillier F(1)(2), Resnick M(1)(2), Mullin S(3), 
Tomlin C(1), Resendez S(1), Liu J(1), Nebeker JR(2)(4), Brown SH(2)(5).

Author information:
(1)Department of Biomedical Informatics, Jacobs School of Medicine and 
Biomedical Sciences, University at Buffalo, Buffalo, New York.
(2)Clinical Informatics, Department of Veterans Affairs, Washington, DC.
(3)Roswell Park Comprehensive Cancer Center, Buffalo, New York.
(4)Department of Medicine, University of Utah, Salt Lake City.
(5)Department of Biomedical Informatics, Vanderbilt University Medical Center, 
Nashville, Tennessee.

IMPORTANCE: Large language models (LLMs) are being implemented in health care. 
Enhanced accuracy and methods to maintain accuracy over time are needed to 
maximize LLM benefits.
OBJECTIVE: To evaluate whether LLM performance on the US Medical Licensing 
Examination (USMLE) can be improved by including formally represented semantic 
clinical knowledge.
DESIGN, SETTING, AND PARTICIPANTS: This comparative effectiveness research study 
was conducted between June 2024 and February 2025 at the Department of 
Biomedical Informatics, Jacobs School of Medicine and Biomedical Sciences, 
University at Buffalo, Buffalo, New York, using sample questions from the USMLE 
Steps 1, 2, and 3.
INTERVENTION: Semantic clinical artificial intelligence (SCAI) was developed to 
insert formally represented semantic clinical knowledge into LLMs using 
retrieval augmented generation (RAG).
MAIN OUTCOMES AND MEASURES: The SCAI method was evaluated by comparing the 
performance of 3 Llama LLMs (13B, 70B, and 405B; Meta) with and without SCAI RAG 
on text-based questions from the USMLE Steps 1, 2, and 3. LLM accuracy for 
answering questions was determined by comparing the LLM output with the USMLE 
answer key.
RESULTS: The LLMs were tested on 87 questions in the USMLE Step 1, 103 in Step 
2, and 123 in Step 3. The 13B LLM enhanced by SCAI RAG was associated with 
significantly improved performance on Steps 1 and 3 but only met the 60% passing 
threshold on Step 3 (74 questions correct [60.2%]). The 70B and 405B LLMs passed 
all the USMLE steps with and without SCAI RAG. The SCAI RAG 70B model scored 80 
questions (92.0%) correctly on Step 1, 82 (79.6%) on Step 2, and 112 (91.1%) on 
Step 3. The SCAI RAG 405B model scored 79 (90.8%) correctly on Step 1, 87 
(84.5%) on Step 2, and 117 (95.1%) on Step 3. Significant improvements 
associated with SCAI RAG were found for the 13B model on Steps 1 and 3, the 70B 
model on Step 2, and the 405B parameter model on Step 3. The 70B model was 
significantly better than the 13B model, and the 405B model was not 
significantly better than the 70B model.
CONCLUSIONS AND RELEVANCE: In this comparative effectiveness research study, 
SCAI RAG was associated with significantly improved scores on the USMLE Steps 1, 
2, and 3. The 13B model passed Step 3 with RAG, and the 70B and 405B models 
passed and scored well on Steps 1, 2, and 3 with or without augmentation. New 
forms of reasoning by LLMs, like semantic reasoning, have potential to improve 
the accuracy of LLM performance on important medical questions. Improving LLM 
performance in health care with targeted, up-to-date clinical knowledge is an 
important step in LLM implementation and acceptance.

DOI: 10.1001/jamanetworkopen.2025.6359
PMCID: PMC12015668
PMID: 40261653 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest Disclosures: Prof Elkin 
reported receiving grants from the National Institutes of Health (NIH) during 
the conduct of the study. Mr Mehta reported receiving grants from the NIH during 
the conduct of the study. Mr LeHouillier reported receiving grants from the 
National Library of Medicine (NLM) during the conduct of the study. Dr Mullin 
reported receiving a T15 postdoctoral training grant fellowship from the NLM 
during the conduct of the study. No other disclosures were reported.


903. IEEE Trans Biomed Eng. 2025 Dec 12;PP. doi: 10.1109/TBME.2025.3643900. Online 
ahead of print.

T2AgeNet: A Text-Guided Framework with Tissue Features for Brain Age Estimation.

Chen S, Wu W, Zhang X, Xu J, Xiong T, Yang C, Xu X.

Brain age estimation from structural MRI is an effective approach for detecting 
abnormal neurodevel opment and neurodegeneration. However, most existing methods 
produce global biomarkers that lack tissue-level specificity and fail to 
leverage medical prior knowledge. To address these limitations, we propose 
T2AgeNet, a dual-path image-text framework for tissue-level brain age estimation 
that integrates anatomical features with clinical semantics. The framework first 
segments brain MRI to generate tissue-specific masks, forming the basis for 
localized age prediction. To further incorporate medical prior knowledge, the 
model first aligns visual features with personalized clinical descriptions to 
guide semantic understanding of tissue-level variation. In parallel, it 
transforms handcrafted aging-related features into textual representations 
through an auxiliary branch using a large language model, enabling enriched 
interpretation and representation. We evaluate T2AgeNet on five datasets 
spanning fetal development, preterm infants, Alzheimer's disease, and autism 
spectrum disorder. Results demonstrate accurate age estimation across diverse 
populations. On the OASIS-3 and ABIDE-I datasets, the model further identifies 
tissue specific structural abnormalities consistent with known neurological 
patterns.

DOI: 10.1109/TBME.2025.3643900
PMID: 41385426


904. J Med Internet Res. 2025 Dec 17;27:e78854. doi: 10.2196/78854.

An Exploratory Typology of Tobacco-Related Misleading Content on Social Media: 
Qualitative Analysis of Instagram and TikTok.

Han E(1)(2), Lyu JC(3)(4), Ling PM(1).

Author information:
(1)Division of General Internal Medicine, School of Medicine, University of 
California, San Francisco, San Francisco, CA, United States.
(2)Fondren Library, Rice University, Houston, TX, United States.
(3)TSET Health Promotion Research Center, Stephenson Cancer Center, University 
of Oklahoma Health Sciences Center, University of Oklahoma, Oklahoma City, OK, 
United States.
(4)Department of Family and Preventive Medicine, College of Medicine, University 
of Oklahoma Health Sciences Center, University of Oklahoma, Oklahoma City, OK, 
United States.

BACKGROUND: Tobacco-related misinformation on social media platforms presents 
growing challenges to digital health communication and public health. Although 
prior studies have focused on platform-specific patterns, a unified framework 
for categorizing and comparing misinformation across platforms is lacking. Such 
a framework is essential for improving infodemiological surveillance and 
designing targeted digital interventions.
OBJECTIVE: This study was an exploratory analysis aimed to build a 
cross-platform typology to categorize tobacco-related misinformation.
METHODS: Data from Instagram and TikTok between January 2020 and August 2023 
were collected using a third-party data collection platform (CrowdTangle) and 
the TikTok Research application programming interface (API). We reviewed a total 
of 4850 Instagram posts using a combination of generative artificial 
intelligence (AI) and human validation by two independent reviewers. In 
addition, 719 TikTok videos were reviewed manually using qualitative analysis. 
We iteratively developed and refined the exploratory typology informed by the 
literature integrating our prior analysis of Twitter data and these new 
datasets.
RESULTS: Of the 22 (71%) Instagram posts and 9 (29%) TikTok videos we analyzed 
closely to classify misinformation, 2 (6.5%) were about cigarettes, 22 (71%) 
were about electronic cigarettes (e-cigarettes), 1 (3.2%) was about heated 
tobacco products (HTPs), 2 (6.5%) were about nicotine (not mentioning specific 
products), and 3 (9.7%) were about cannabidiol (CBD) products. 1 (3.2%) post did 
not mention any type of products. These categories could overlap in a single 
post. The resulting typology consisted of five core narrative archetypes: false 
or misleading health claims (A1), wellness and lifestyle appeal (A2), 
conspiracy-driven policy agenda (A3), undermining trust in science and medicine 
(A4), and recreational nicotine use normalization (A5). Each archetype has 
attributes of false claim types and sources. Among the posts we analyzed, A1 and 
A2 were most likely to be found on Instagram. A3 was most frequently found on 
Twitter. A4 was commonly seen on both Twitter and TikTok, and A5 was most 
frequently found on TikTok. Two additional dimensions-type of falsehood and 
source-were also added to characterize a given misinformation post. This 
exploratory typology paved the way for a structured lens to view how 
misinformation is tailored to digital environments and target audiences.
CONCLUSIONS: This cross-platform typology building supports digital health 
research by integrating AI and qualitative methods to categorize tobacco-related 
misinformation. It can inform the development of automated misinformation 
detection models, enhance real-time infodemiological monitoring, and guide 
digital public health campaigns to build tailored countermessaging.

©Eileen Han, Joanne Chen Lyu, Pamela M Ling. Originally published in the Journal 
of Medical Internet Research (https://www.jmir.org), 17.12.2025.

DOI: 10.2196/78854
PMCID: PMC12756659
PMID: 41406469 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


905. Eur Radiol. 2025 Apr;35(4):2018-2029. doi: 10.1007/s00330-024-11074-y. Epub 2024 
Oct 10.

Automatic structuring of radiology reports with on-premise open-source large 
language models.

Woźnicki P(1), Laqua C(2), Fiku I(2), Hekalo A(2), Truhn D(3), Engelhardt 
S(4)(5), Kather J(6)(7)(8), Foersch S(9), D'Antonoli TA(10), Pinto Dos Santos 
D(11)(12), Baeßler B(2), Laqua FC(2).

Author information:
(1)Department of Diagnostic and Interventional Radiology, University Hospital 
Würzburg, Würzburg, Germany. woznicki_p@ukw.de.
(2)Department of Diagnostic and Interventional Radiology, University Hospital 
Würzburg, Würzburg, Germany.
(3)Department of Diagnostic and Interventional Radiology, University Hospital 
Aachen, Aachen, Germany.
(4)Department of Internal Medicine III, Heidelberg University Hospital, 
Heidelberg, Germany.
(5)DZHK (German Centre for Cardiovascular Research), Partner Site 
Heidelberg/Mannheim, Heidelberg, Germany.
(6)Department of Internal Medicine I, University Hospital Carl Gustav Carus, 
Technical University Dresden, Dresden, Germany.
(7)Else Kroener Fresenius Center for Digital Health, Medical Faculty Carl Gustav 
Carus, TUD Dresden University of Technology, Dresden, Germany.
(8)Medical Oncology, National Center for Tumor Diseases (NCT), University 
Hospital Heidelberg, Heidelberg, Germany.
(9)Institute of Pathology, University Medical Center Mainz, Mainz, Germany.
(10)Institute of Radiology and Nuclear Medicine, Cantonal Hospital Baselland, 
Liestal, Switzerland.
(11)Department of Diagnostic and Interventional Radiology, University of 
Cologne, Cologne, Germany.
(12)Department of Radiology, University Hospital of Frankfurt, Frankfurt, 
Germany.

OBJECTIVES: Structured reporting enhances comparability, readability, and 
content detail. Large language models (LLMs) could convert free text into 
structured data without disrupting radiologists' reporting workflow. This study 
evaluated an on-premise, privacy-preserving LLM for automatically structuring 
free-text radiology reports.
MATERIALS AND METHODS: We developed an approach to controlling the LLM output, 
ensuring the validity and completeness of structured reports produced by a 
locally hosted Llama-2-70B-chat model. A dataset with de-identified narrative 
chest radiograph (CXR) reports was compiled retrospectively. It included 202 
English reports from a publicly available MIMIC-CXR dataset and 197 German 
reports from our university hospital. Senior radiologist prepared a detailed, 
fully structured reporting template with 48 question-answer pairs. All reports 
were independently structured by the LLM and two human readers. Bayesian 
inference (Markov chain Monte Carlo sampling) was used to estimate the 
distributions of Matthews correlation coefficient (MCC), with [-0.05, 0.05] as 
the region of practical equivalence (ROPE).
RESULTS: The LLM generated valid structured reports in all cases, achieving an 
average MCC of 0.75 (94% HDI: 0.70-0.80) and F1 score of 0.70 (0.70-0.80) for 
English, and 0.66 (0.62-0.70) and 0.68 (0.64-0.72) for German reports, 
respectively. The MCC differences between LLM and humans were within ROPE for 
both languages: 0.01 (-0.05 to 0.07), 0.01 (-0.05 to 0.07) for English, and 
-0.01 (-0.07 to 0.05), 0.00 (-0.06 to 0.06) for German, indicating approximately 
comparable performance.
CONCLUSION: Locally hosted, open-source LLMs can automatically structure 
free-text radiology reports with approximately human accuracy. However, the 
understanding of semantics varied across languages and imaging findings.
KEY POINTS: Question Why has structured reporting not been widely adopted in 
radiology despite clear benefits and how can we improve this? Findings A locally 
hosted large language model successfully structured narrative reports, showing 
variation between languages and findings. Critical relevance Structured 
reporting provides many benefits, but its integration into the clinical routine 
is limited. Automating the extraction of structured information from radiology 
reports enables the capture of structured data while allowing the radiologist to 
maintain their reporting workflow.

© 2024. The Author(s).

DOI: 10.1007/s00330-024-11074-y
PMCID: PMC11913902
PMID: 39390261 [Indexed for MEDLINE]

Conflict of interest statement: Compliance with ethical standards. Guarantor: 
The scientific guarantor of this publication is P.W. and F.C.L. Conflict of 
interest: The authors of this manuscript declare relationships with the 
following companies: P.W. is a consultant at Smart Reporting GmbH. D.T. holds 
shares in StratifAI GmbH and has received honoraria for lectures by Bayer AG. 
B.B. is Founder and CEO of LernRad GmbH and has received speaker honoraria from 
Bayer Vital GmbH. T.A.D. is a Scientific Editorial Board member of European 
Radiology, and D.P.D.S. is a Deputy Editor of European Radiology; they have not 
taken part in this paper’s review and decision process. Statistics and biometry: 
One of the authors (F.C.L.) has significant statistical expertise. Informed 
consent: Written informed consent was waived by the Institutional Review Board. 
Ethical approval: Institutional Review Board approval was obtained (nr: 
20221004-02). Study subjects or cohorts overlap: The MIMIC chest X-ray 
(MIMIC-CXR) cohort was published in 2019 ( 
https://doi.org/10.1038/s41597-019-0322-0 ). It is publicly available. 
Methodology: Retrospective Experimental Performed at one institution


906. Eur Respir J. 2025 Dec 4;66(6):2500894. doi: 10.1183/13993003.00894-2025. Print 
2025 Dec.

Characterising research trends in bronchiectasis through AI-powered analytics.

Narayana JK(1), Koo Wei Ling Y(1), Mac Aogáin M(2)(3), Chotirmall SH(4)(5).

Author information:
(1)Lee Kong Chian School of Medicine, Nanyang Technological University, 
Singapore, Singapore.
(2)Department of Biochemistry, St. James's Hospital, Dublin, Ireland.
(3)Clinical Biochemistry Unit, School of Medicine, Trinity College Dublin, 
Dublin, Ireland.
(4)Lee Kong Chian School of Medicine, Nanyang Technological University, 
Singapore, Singapore schotirmall@ntu.edu.sg.
(5)Department of Respiratory and Critical Care Medicine, Tan Tock Seng Hospital, 
Singapore, Singapore.

Comment in
    Eur Respir J. 2025 Dec 4;66(6):2501687. doi: 10.1183/13993003.01687-2025.
    Eur Respir J. 2026 Jan 22;67(1):2501843. doi: 10.1183/13993003.01843-2025.
    Eur Respir J. 2026 Jan 22;67(1):2501968. doi: 10.1183/13993003.01968-2025.

BACKGROUND: Interest in bronchiectasis is increasing and no prior study has used 
artificial intelligence (AI) to interrogate its rich, multidimensional 
literature to characterise research trends, themes and knowledge gaps.
METHODS: We reviewed original bronchiectasis research between 1949 and 2024 (a 
75-year period) to identify, characterise and assess research trends and 
trajectories using two AI-powered approaches: 1) Atlas, an AI topic-modelling 
tool; and 2) a custom model, leveraging ChatGPT embedding and text generation 
models.
RESULTS: AI-powered analytics revealed a nine-fold increase in bronchiectasis 
research speed since 2000, typified by enhanced richness with four new research 
topics emerging every 5 years. Publication trends mirror clinical and 
technological advances, exemplified by significant rises in computed tomography, 
microbiome and clinical studies following the adoption of high-resolution 
computed tomography (1970s), next-generation sequencing (2005) and the first 
clinical guidelines (2008-2010), respectively. Topics with sustained growth 
(i.e. popular topics) include bronchiectasis-COPD overlap, microbiome infection, 
cardiovascular health and exacerbations. Those with sudden, short-term increased 
interest (i.e. trending topics) have focused on microbial pathogens and primary 
ciliary dyskinesia genetics. Mortality represents a nascent topic, demonstrating 
the highest year-on-year interest. Growth of research within the "vicious 
vortex" demonstrates thematic imbalance, with few studies overlapping with 
non-vortex components. Evolving research focus towards inflammation is evident, 
with increased work on comorbidities and quality of life demonstrating a shift 
from disease-centric to patient-centric research.
CONCLUSION: AI captures bronchiectasis as a dynamic and interdisciplinary field 
in continuing growth. Emerging research topics extend beyond the vicious vortex 
framework, indicating a transition from disease-centric to patient-centric 
approaches to optimise clinical care.

Copyright ©The authors 2025.

DOI: 10.1183/13993003.00894-2025
PMCID: PMC12675959
PMID: 40876962 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of interest: S.H. Chotirmall has served 
on advisory boards for CSL Behring, Pneumagen Ltd., Zaccha Pte Ltd, Boehringer 
Ingelheim, GSK and Sanofi; on data monitoring boards for Inovio Pharmaceuticals 
and Imam Abdulrahman Bin Faisal University; and has received personal fees from 
AstraZeneca and Chiesi Farmaceutici, all unrelated to this work. The remaining 
authors have no potential conflicts of interest to disclose.


907. J Med Internet Res. 2025 Nov 21;27:e76571. doi: 10.2196/76571.

Considerations for Patient Privacy of Large Language Models in Health Care: 
Scoping Review.

Zhong X(#)(1)(2), Li S(#)(1)(2), Chen Z(3), Ge L(4)(5)(6), Yu D(7), Wang 
S(1)(2), You L(#)(1)(2), Shang H(#)(2)(8).

Author information:
(1)Dongzhimen Hospital, Beijing University of Chinese Medicine, Beijing, null, 
China.
(2)Key Laboratory of Chinese Internal Medicine of the Ministry of Education, 
Beijing University of Chinese Medicine, Beijing, null, China.
(3)Institute of Basic Research in Clinical Medicine, China Academy of Chinese 
Medical Sciences, Beijing, China.
(4)Department of Health Policy and Health Management, School of Public Health, 
Lanzhou University, Lanzhou, China.
(5)Evidence-Based Social Science Research Center, School of Public Health, 
Lanzhou University, Lanzhou, China.
(6)WHO Collaborating Center for Guideline Implementation and Knowledge 
Translation, Lanzhou, China.
(7)Second Clinical College of Guangzhou University of Chinese Medicine, 
Guangzhou, China.
(8)Dongfang Hospital, Beijing University of Chinese Medicine, Beijing, China.
(#)Contributed equally

BACKGROUND: The application of large language models (LLMs) in health care holds 
significant potential for enhancing patient care and advancing medical research. 
However, the protection of patient privacy remains a critical issue, especially 
when handling patient health information (PHI).
OBJECTIVE: This scoping review aims to evaluate the adequacy of current 
approaches and identify areas in need of improvement to ensure robust patient 
privacy protection in the existing studies about PHI-LLMs within the health care 
domain.
METHODS: A search of the literature published from January 1, 2022, to July 20, 
2025, was performed on July 20, 2025, using 2 databases (PubMed and Embase). 
This scoping review focused on the following three research questions: (1) What 
studies on the development and application of LLMs using PHI currently exist 
within the health care domain? (2) What patient privacy considerations are 
addressed in existing PHI-LLMs research, and are these measures sufficient? (3) 
How can future research on the development and application of LLMs using PHI 
better protect patient privacy? Studies were included if they focused on the 
development and application of LLMs within health care using PHI, encompassing 
activities such as model construction, fine-tuning, optimization, testing, and 
performance comparison. Eligible literature comprised original research articles 
written in English. Conversely, studies were excluded if they used publicly 
available datasets, under the assumption that such data have been adequately 
deidentified. Additionally, non-English publications, reviews, abstracts, 
incomplete reports, and preprints were excluded from the review due to the lack 
of rigorous peer review.
RESULTS: This study systematically identified 9823 studies on PHI-LLM and 
included 464 studies published between 2022 and 2025. Among the 464 studies, (1) 
a small number of studies neglected ethical review (n=45, 9.7%) and patient 
informed consent (n=148, 31.9%) during the research process, (2) more than a 
third of the studies (n=178, 38.4%) failed to report whether to implement 
effective measures to protect PHI, and (3) there was a significant lack of 
transparency and comprehensive detail in anonymization and deidentification 
methods.
CONCLUSIONS: We propose comprehensive recommendations across 3 phases-study 
design, implementation, and reporting-to strengthen patient privacy protection 
and transparency in PHI-LLM. This study emphasizes the urgent need for the 
development of stricter regulatory frameworks and the adoption of advanced 
privacy protection technologies to effectively safeguard PHI. It is anticipated 
that future applications of LLMs in the health care field will achieve a balance 
between innovation and robust patient privacy protection, thereby enhancing 
ethical standards and scientific credibility.

©Xiaoying Zhong, Siyi Li, Zhao Chen, Long Ge, Dongdong Yu, Shijia Wang, 
Liangzhen You, Hongcai Shang. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org), 21.11.2025.

DOI: 10.2196/76571
PMCID: PMC12680930
PMID: 41269747 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


908. Acad Med. 2025 Jul 1;100(7):769-775. doi: 10.1097/ACM.0000000000006047. Epub 
2025 Mar 24.

Using Generative Artificial Intelligence When Writing Letters of Recommendation.

Fried JC, Johnson NR, Pelletier A, Landman A, Bartz D.

Artificial intelligence (AI) provides an opportunity to streamline tasks within 
academic medicine. Generative AI (genAI) models, specifically, have the capacity 
to generate new written content, follow detailed instructions for product 
improvement, and incorporate content from supplemental data sources. While a 
part of the professional responsibility of faculty in academic medicine, writing 
letters of recommendation (LORs) is often time consuming and repetitive 
candidate to candidate. Yet, crafting these letters well is paramount to convey 
an applicant's unique attributes in a time when pass/fail grading and remote 
interviews are increasingly common.In this article, the authors provide an 
approachable framework for the ethical use of genAI to assist with writing LORs 
in academic medicine. They briefly discuss the fundamental structure of genAI, 
the advantages between several genAI models specifically for the task of letter 
writing, privacy concerns that can develop when using genAI, iterative methods 
to develop effective prompts to craft letter drafts, personalization of 
finalized content, genAI use to identify bias, and appropriate documentation of 
AI usage.Once practiced, this process can prevent the need for shortcuts, such 
as copying and pasting from CVs or reusing previously written letters between 
candidates, that currently sacrifice letter quality to reduce writing time. 
Ethical use, privacy, and disclosure necessitate a deliberate framework for the 
use of genAI in letter writing. Future research is needed to inform the 
development of a specific AI model to generate LORs. The framework presented 
here provides faculty with the steps needed to begin incorporating genAI into 
their letter writing practice.

Copyright © 2025 the Association of American Medical Colleges.

DOI: 10.1097/ACM.0000000000006047
PMCID: PMC12197835
PMID: 40127307 [Indexed for MEDLINE]


909. Clin Cancer Res. 2025 Aug 1;31(15):3229-3238. doi: 
10.1158/1078-0432.CCR-25-0449.

Detection of Early-Stage Colorectal Cancer Using Cell-Free oncRNA Biomarkers and 
Artificial Intelligence.

Momen-Roknabadi A(#)(1), Karimzadeh M(#)(1), Chen NC(#)(1), Cavazos TB(#)(1), 
Wang J(1), Ku J(1), Degtiar A(1), Krishnan A(1), Hernandez M(1), Gebala M(1), 
Huang A(1), Chen S(1), Nguyen D(1), Lam T(1), Hanna R(1), Fish L(1), Smith 
AJ(1), Sekhon S(1), Yen J(1), Gregg J(2), Li H(1), Hormozdiari F(3), Behsaz 
B(1), Hartwig A(1), Goodarzi H(4)(5), Schwartzberg L(1)(6), Alipanahi B(1).

Author information:
(1)Exai Bio, Palo Alto, California.
(2)University of Nevada School of Medicine, Reno, Nevada.
(3)University of California, Davis, California.
(4)Arc Institute, Palo Alto, California.
(5)University of California, San Francisco, California.
(6)University of Nevada, Reno, Nevada.
(#)Contributed equally

PURPOSE: Colorectal cancer is the second leading cause of cancer-related deaths 
worldwide, and early detection significantly improves treatment outcomes, but 
existing blood-based tests often have limited sensitivity in early-stage 
disease. We developed a blood-based test combining orphan noncoding RNAs 
(oncRNA), a group of small cell-free RNAs, with generative artificial 
intelligence to detect colorectal cancer.
EXPERIMENTAL DESIGN: We leveraged a cohort of 613 colorectal cancer cases and 
controls to train a model that demonstrated both high clinical performance and 
minimal technical variability in robustness testing. We further validated our 
model in an independent, single-source cohort of 192 colorectal cancer cases and 
controls. Model performance was assessed by sensitivity, specificity, and area 
under the ROC curve, with attention to early-stage detection.
RESULTS: In our independent validation set, we achieved an overall sensitivity 
of 89% at 90% specificity, with an 80% sensitivity for stage I-an important 
milestone, as early-stage colorectal cancer detection remains a challenge for 
other blood-based technologies. Performance was consistent across demographic 
subgroups.
CONCLUSIONS: Our oncRNA-based blood test, powered by artificial intelligence, 
offers strong performance for early colorectal cancer detection, including in 
stage I disease for which existing blood-based assays are limited. These 
findings support further development toward a minimally invasive colorectal 
cancer screening tool.

©2025 The Authors; Published by the American Association for Cancer Research.

DOI: 10.1158/1078-0432.CCR-25-0449
PMCID: PMC12314518
PMID: 40366744 [Indexed for MEDLINE]

Conflict of interest statement: A. Momen-Roknabadi reports a patent for 
Detection of Cancer via Nucleic Acids pending. M. Karimzadeh reports a patent 
for Detection of Cancer via Nucleic Acids pending. N.-C. Chen reports a patent 
for Detection of Cancer via Nucleic Acids pending. T.B. Cavazos reports a patent 
for Detection of Cancer via Nucleic Acids pending. J. Wang reports a patent for 
Detection of Cancer via Nucleic Acids pending. J. Ku reports a patent for 
Detection of Cancer via Nucleic Acids pending. A. Degtiar reports a patent for 
Detection of Cancer via Nucleic Acids pending. A. Krishnan reports a patent for 
Detection of Cancer via Nucleic Acids pending. M. Hernandez reports a patent for 
Detection of Cancer via Nucleic Acids pending. M. Gebala reports a patent for 
Detection of Cancer via Nucleic Acids pending. A. Huang reports a patent for 
Detection of Cancer via Nucleic Acids pending. S. Chen reports a patent for 
Detection of Cancer via Nucleic Acids pending. D. Nguyen reports a patent for 
Detection of Cancer via Nucleic Acids pending. T. Lam reports a patent for 
Detection of Cancer via Nucleic Acids pending. R. Hanna reports a patent for 
Detection of Cancer via Nucleic Acids pending. L. Fish reports other support 
from Exai Bio outside the submitted work, as well as a patent for 63/750,189 
pending. A.J. Smith reports a patent for Detection of Cancer via Nucleic Acids 
pending. S. Sekhon reports a patent for Detection of Cancer via Nucleic Acids 
pending. J. Yen reports a patent for Detection of Cancer via Nucleic Acids 
pending. J. Gregg reports personal fees from Exai Bio during the conduct of the 
study, as well as a patent for 63/750,189 pending. H. Li reports a patent for 
Detection of Cancer via Nucleic Acids pending. F. Hormozdiari reports other 
support from the University of California at Davis outside the submitted work, 
as well as a patent for Detection of Cancer via Nucleic Acids pending. B. Behsaz 
reports a patent for Detection of Cancer via Nucleic Acids pending. A. Hartwig 
reports a patent for Detection of Cancer via Nucleic Acids pending, as well as 
employment with Exai Bio during the conduct of the study. H. Goodarzi reports 
grants and personal fees from Exai Bio and other support from Vevo Therapeutics 
outside the submitted work, as well as a patent for Detection of Cancer via 
Nucleic Acids pending. L. Schwartzberg reports personal fees from Exai Bio 
during the conduct of the study and personal fees from Foundation Medicine, 
Agendia, and Caris Life Sciences outside the submitted work as well as a patent 
for Detection of Cancer via Nucleic Acids pending to Exai Bio. B. Alipanahi 
reports a patent for Detection of Cancer via Nucleic Acids pending.


910. Protein Sci. 2025 Sep;34(9):e70251. doi: 10.1002/pro.70251.

Finding the dark matter: Large language model-based enzyme kinetic data 
extractor and its validation.

Wei G(1), Ran X(1), Ai-Abssi R(1), Yang Z(1)(2)(3)(4)(5).

Author information:
(1)Department of Chemistry, Vanderbilt University, Nashville, Tennessee, USA.
(2)Center for Structural Biology, Vanderbilt University, Nashville, Tennessee, 
USA.
(3)Vanderbilt Institute of Chemical Biology, Vanderbilt University, Nashville, 
Tennessee, USA.
(4)Data Science Institute, Vanderbilt University, Nashville, Tennessee, USA.
(5)Department of Chemical and Biomolecular Engineering, Vanderbilt University, 
Nashville, Tennessee, USA.

Despite the vast number of enzymatic kinetic measurements reported across 
decades of biochemical literature, the majority of relational enzyme kinetic 
data-linking amino acid sequence, substrate identity, kinetic parameters, and 
assay conditions-remains uncollected and inaccessible in structured form. This 
constitutes a significant portion of the "dark matter" of enzymology. Unlocking 
these hidden data through automated extraction offers an opportunity to expand 
enzyme dataset diversity and size, critical for building accurate, generalizable 
models that drive predictive enzyme engineering. To address this limitation, we 
built EnzyExtract, a large language model-powered pipeline that automates the 
extraction, verification, and structuring of enzyme kinetics data from 
scientific literature. By processing 137,892 full-text publications (PDF/XML), 
EnzyExtract collected more than 218,095 enzyme-substrate-kinetics entries, 
including 218,095 kcat and 167,794 Km values. These entries are mapped to 
enzymes spanning 3569 unique four-digit EC numbers, with a total of 84,464 
entries assigned at least a first-digit EC number. EnzyExtract identified 89,544 
unique kinetic entries (kcat and Km combined) absent from BRENDA, significantly 
expanding the known enzymology dataset. The newly curated dataset was compiled 
into a database named EnzyExtractDB. EnzyExtract demonstrates high accuracy when 
benchmarked against manually curated datasets and strong consistency with 
BRENDA-derived data. To create model-ready datasets, enzyme and substrate 
sequences were aligned to UniProt and PubChem, yielding 92,286 high-confidence, 
sequence-mapped kinetic entries. To assess the practical utility of our dataset, 
we retrained several state-of-the-art kcat predictors (including MESI, DLKcat, 
and TurNuP) using EnzyExtractDB. Across held-out test sets, all models 
demonstrate improved predictive performance in terms of RMSE, MAE, and R2, 
highlighting the value of high-quality, large-scale, literature-derived 
EnzyExtractDB for enhancing predictive modeling of enzyme kinetics. The 
EnzyExtract source code and the database are openly available at 
https://github.com/ChemBioHTP/EnzyExtract, and an interactive demo can be 
accessed via Google Colab at 
https://colab.research.google.com/drive/1MwKSEZzLPNOseksRshbzkkFoO_cgJhva.

© 2025 The Author(s). Protein Science published by Wiley Periodicals LLC on 
behalf of The Protein Society.

DOI: 10.1002/pro.70251
PMCID: PMC12355964
PMID: 40815276 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no competing financial 
interest.


911. J Med Internet Res. 2025 Sep 24;27:e81769. doi: 10.2196/81769.

Critical Limitations in Systematic Reviews of Large Language Models in Health 
Care.

Weizman Z(1).

Author information:
(1)Faculty of Health Sciences, Ben-Gurion University, 8 Balfour Street, 
Tel-Aviv, 6521120, Israel, 972 544888686.

Comment on
    J Med Internet Res. e82729.
    J Med Internet Res. 27:e71916.

DOI: 10.2196/81769
PMCID: PMC12459740
PMID: 40991747

Conflict of interest statement: Conflicts of Interest: None declared.


912. J Med Internet Res. 2025 Sep 24;27:e82729. doi: 10.2196/82729.

Author's Reply: Critical Limitations in Systematic Reviews of Large Language 
Models in Health Care.

Python A(1)(2)(3), Li H(1)(4), Fu JF(5)(6)(7).

Author information:
(1)Center for Data Science, Zhejiang University, Hangzhou, China.
(2)School of Medicine, Zhejiang University, Hangzhou, China.
(3)Centre for Human Genetics, Nuffield Department of Medicine, University of 
Oxford, Roosevelt Drive, Oxford, OX3 7BN, United Kingdom, 44 01865 287500.
(4)School of Mathematical Sciences, Zhejiang University, Hangzhou, China.
(5)School of Medicine, Children's Hospital of Zhejiang University, Hangzhou, 
China.
(6)National Clinical Research Center for Child Health, Hangzhou, China.
(7)National Regional Center for Children's Health, Hangzhou, China.

Comment on
    doi: 10.2196/81769.
    doi: 10.2196/71916.

DOI: 10.2196/82729
PMCID: PMC12459737
PMID: 40991734

Conflict of interest statement: Conflicts of Interest: None declared.


913. J Med Internet Res. 2025 Jul 31;27:e72984. doi: 10.2196/72984.

Large Language Model Symptom Identification From Clinical Text: Multicenter 
Study.

McMurry AJ(1)(2), Phelan D(1), Dixon BE(3)(4), Geva A(1)(5), Gottlieb D(1)(6), 
Jones JR(1), Terry M(1), Taylor DE(4), Callaway H(4), Manoharan S(4), Miller 
T(1)(2), Olson KL(1)(2), Mandl KD(1)(2).

Author information:
(1)Computational Health Informatics Program, Boston Children's Hospital, 401 
Park Drive, LM5506, Mail Stop BCH3187, Boston, MA, 02215, United States, 1 
617-355-4145.
(2)Department of Pediatrics, Harvard Medical School, Boston, MA, United States.
(3)Department of Health Policy and Management, Fairbanks School of Public 
Health, Indiana University, Indianapolis, IN, United States.
(4)Center for Biomedical Informatics, Regenstrief Institute, Indianapolis, IN, 
United States.
(5)Department of Anesthesia, Harvard Medical School, Boston, MA, United States.
(6)Department of Biomedical Informatics, Harvard Medical School, Boston, MA, 
United States.

BACKGROUND: Recognizing patient symptoms is fundamental to medicine, research, 
and public health. However, symptoms are often underreported in coded formats 
even though they are routinely documented in physician notes. Large language 
models (LLMs), noted for their generalizability, could help bridge this gap by 
mimicking the role of human expert chart reviewers for symptom identification.
OBJECTIVE: The primary objective of this multisite study was to measure the 
accurate identification of infectious respiratory disease symptoms using LLMs 
instructed to follow chart review guidelines. The secondary objective was to 
evaluate LLM generalizability in multisite settings without the need for 
site-specific training, fine-tuning, or customization.
METHODS: Four LLMs were evaluated: GPT-4, GPT-3.5, Llama2 70B, and Mixtral 8×7B. 
LLM prompts were instructed to take on the role of chart reviewers and follow 
symptom annotation guidelines when assessing physician notes. Ground truth 
labels for each note were annotated by subject matter experts. Optimal LLM 
prompting strategies were selected using a development corpus of 103 notes from 
the emergency department at Boston Children's Hospital. The performance of each 
LLM was measured using a test corpus with 202 notes from Boston Children's 
Hospital. The performance of an International Classification of Diseases, Tenth 
Revision (ICD-10)-based method was also measured as a baseline. Generalizability 
of the most performant LLM was then measured in a validation corpus of 308 notes 
from 21 emergency departments in the Indiana Health Information Exchange.
RESULTS: Symptom identification accuracy was superior for every LLM tested for 
each infectious disease symptom compared to an ICD-10-based method 
(F1-score=45.1%). GPT-4 was the highest scoring (F1-score=91.4%; P<.001) and was 
significantly better than the ICD-10-based method, followed by GPT-3.5 
(F1-score=90.0%; P<.001), Llama2 (F1-score=81.7%; P<.001), and Mixtral 
(F1-score=83.5%; P<.001). For the validation corpus, performance of the 
ICD-10-based method decreased (F1-score=26.9%), while GPT-4 increased 
(F1-score=94.0%), demonstrating better generalizability using GPT-4 (P<.001).
CONCLUSIONS: LLMs significantly outperformed an ICD-10-based method for 
respiratory symptom identification in emergency department electronic health 
records. GPT-4 demonstrated the highest accuracy and generalizability, 
suggesting that LLMs may augment or replace traditional approaches. LLMs can be 
instructed to mimic human chart reviewers with high accuracy. Future work should 
assess broader symptom types and health care settings.

© Andrew J McMurry, Dylan Phelan, Brian E Dixon, Alon Geva, Daniel Gottlieb, 
James R Jones, Michael Terry, David E Taylor, Hannah Callaway, Sneha Manoharan, 
Timothy Miller, Karen L Olson, Kenneth D Mandl. Originally published in the 
Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/72984
PMCID: PMC12313083
PMID: 40743494 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


914. J Infect Dis. 2025 Feb 20;231(2):e345-e354. doi: 10.1093/infdis/jiae403.

Breaking Boundaries in Spinal Surgery: GPT-4's Quest to Revolutionize Surgical 
Site Infection Management.

Zhao B(1), Liu H(2), Liu Q(1), Qi W(3), Zhang W(1), Du J(1), Jin Y(4), Weng 
X(1).

Author information:
(1)Department of Anesthesiology and SICU, Xinhua Hospital, School of Medicine, 
Shanghai Jiao Tong University, Shanghai, China.
(2)Department of Anesthesiology, Shanghai Ninth People's Hospital, Shanghai Jiao 
Tong University, Shanghai, China.
(3)Department of Psychogeriatric, Shanghai Mental Health Center, School of 
Medicine, Shanghai Jiao Tong University, Shanghai, China.
(4)Department of Dermatology, Second Affiliated Hospital of Naval Medical 
University, Shanghai Key Laboratory of Medical Mycology, Shanghai, China.

BACKGROUND: Surgical site infection (SSI) is a common and costly complication in 
spinal surgery. Identifying risk factors and preventive strategies is crucial 
for reducing SSIs. Generative Pre-trained Transformer 4 (GPT-4) has evolved from 
a simple text-based tool to a sophisticated multimodal data expert, invaluable 
for clinicians. This study explored GPT-4's applications in SSI management 
across various clinical scenarios.
METHODS: GPT-4 was employed in clinical scenarios related to SSIs in spinal 
surgery. Researchers designed specific questions for GPT-4 to generate tailored 
responses. Six evaluators assessed the responses for logic and accuracy using a 
5-point Likert scale. Interrater consistency was measured with Fleiss' kappa, 
and radar charts visualized GPT-4's performance.
RESULTS: Interrater consistency, measured by Fleiss' kappa, ranged from 0.62 to 
0.83. The average scores for logic and accuracy were 24.27 ± 0.4 and 24.46 ± 
0.25. Radar charts indicated consistently high performance across criteria. 
GPT-4 demonstrated proficiency in creating personalized treatment plans, 
improving SSI management strategies, and identified emerging research trends.
CONCLUSIONS: GPT-4 shows a significant potential in SSI management in spinal 
surgery, promoting patient-centered care and precision medicine. Despite 
limitations in antibiotics and patient education, GPT-4's continuous learning, 
data privacy focus, and professional collaboration indicate its potential to 
revolutionize SSI management, requiring further development.

© The Author(s) 2024. Published by Oxford University Press on behalf of 
Infectious Diseases Society of America.

DOI: 10.1093/infdis/jiae403
PMCID: PMC11841635
PMID: 39136574 [Indexed for MEDLINE]

Conflict of interest statement: Potential conflicts of interest. The authors: No 
reported conflicts of interest. All authors have submitted the ICMJE Form for 
Disclosure of Potential Conflicts of Interest. Conflicts that the editors 
consider relevant to the content of the manuscript have been disclosed.


915. Am J Gastroenterol. 2025 Nov 24. doi: 10.14309/ajg.0000000000003864. Online 
ahead of print.

Use of Large Language Models to Determine the Surveillance Colonoscopy Interval: 
A Bi-Institutional Validation Study.

Acharya V(1), Mehta SJ(2), Sussman DA(3), Kumaresan V(4), England J(5), Cook 
TS(1), Issenberg SB(6), Deshpande AR(3).

Author information:
(1)Department of Radiology, University of Pennsylvania Perelman School of 
Medicine, Philadelphia, Pennsylvania, USA.
(2)Division of Gastroenterology, Department of Medicine, University of 
Pennsylvania Perelman School of Medicine, Philadelphia, Pennsylvania, USA.
(3)Division of Digestive Health and Liver Diseases, Department of Medicine, 
University of Miami Miller School of Medicine, Miami, Florida, USA.
(4)V Labs, Sunnyvale, California, USA.
(5)Gastromed, Miami, Florida, USA.
(6)Michael S Gordon Center for Simulation and Innovation in Medical Education, 
University of Miami Miller School of Medicine, Miami, Florida, USA.

INTRODUCTION: To determine the appropriate postpolypectomy colonoscopy 
surveillance interval, endoscopists synthesize information from colonoscopy and 
pathology report impressions and subsequently apply guideline-recommended 
interval algorithms, such as those developed by the United States Multi-Society 
Task Force. Given the complexity of these guidelines, this manual process is 
error-prone, necessitating automated tools, including large language models 
(LLMs), to improve guideline adherence. The primary aim of this study was to 
identify the LLM performance in determining the guideline-concordant 
postpolypectomy surveillance interval on a cohort of 1,000 real-world 
colonoscopy and pathology report impressions.
METHODS: The data of patients who underwent a screening or surveillance 
colonoscopy in 2023-2024 at 2 academic health centers were included. Using a 
custom prompt outlining the US Multi-Society Task Force postpolypectomy 
surveillance algorithm, the LLM (GPT-4o) was asked to determine the appropriate 
surveillance interval for all 1,000 examples in the data set. This experiment, 
using the same model, prompt, and data set, was repeated 10 times; all 
experiments were conducted between January 27, 2025, and February 3, 2025.
RESULTS: Across 10 experiments, the average accuracy was 94.6%. There was no 
significant difference in accuracy based on the institution from which the data 
originated or the presence of synchronous upper gastrointestinal endoscopy data 
within the pathology report impression. Examples with 1-3 colon polyps had an 
average accuracy of 95.8% whereas examples with 4 or more colon polyps had an 
average accuracy of 88.2%, combined P value < 0.001.
DISCUSSION: LLMs with a custom prompt achieve consistently high accuracy in 
determining the guideline-based surveillance colonoscopy interval.

Copyright © 2025 by The American College of Gastroenterology.

DOI: 10.14309/ajg.0000000000003864
PMID: 41351229


916. Eur Radiol. 2025 Aug 20. doi: 10.1007/s00330-025-11933-2. Online ahead of print.

Applying large language model for automated quality scoring of radiology 
requisitions using a standardized criteria.

Büyüktoka RE(1), Surucu M(2), Erekli Derinkaya PB(3), Adibelli ZH(4), Salbas 
A(5), Koc AM(6), Buyuktoka AD(3), Isler Y(7), Ugur MA(8), Isiklar E(9).

Author information:
(1)Department of Radiology, Izmir Foça State Hospital, Izmir, Türkiye. 
rasiterenbuyuktoka@hotmail.com.
(2)Bucak Computer and Informatics Faculty, Burdur Mehmet Akif Ersoy University, 
Burdur, Türkiye.
(3)Department of Radiology, Izmir City Hospital, Izmir, Türkiye.
(4)Department of Radiology, Izmir City Hospital & Health Sciences University, 
Izmir, Türkiye.
(5)Department of Radiology, Izmir Ataturk Education and Research Hospital, 
Izmir, Türkiye.
(6)Department of Radiology, Izmir Katip Celebi University & Izmir Ataturk 
Education and Research Hospital, Izmir, Türkiye.
(7)Department of Electrical and Electronics Engineering, Alanya Alaaddin 
Keykubat University, Antalya, Türkiye.
(8)Department of General Surgery, Izmir City Hospital, Izmir, Türkiye.
(9)Department of Internal Medicine, Izmir City Hospital, Izmir, Türkiye.

OBJECTIVES: To create and test a locally adapted large language model (LLM) for 
automated scoring of radiology requisitions based on the reason for exam imaging 
reporting and data system (RI-RADS), and to evaluate its performance based on 
reference standards.
MATERIALS AND METHODS: This retrospective, double-center study included 131,683 
radiology requisitions from two institutions. A bidirectional encoder 
representation from a transformer (BERT)-based model was trained using 101,563 
requisitions from Center 1 (including 1500 synthetic examples) and externally 
tested on 18,887 requisitions from Center 2. The model's performance for two 
different classification strategies was evaluated by the reference standard 
created by three different radiologists. Model performance was assessed using 
Cohen's Kappa, accuracy, F1-score, sensitivity, and specificity with 95% 
confidence intervals.
RESULTS: A total of 18,887 requisitions were evaluated for the external test 
set. External testing yielded a performance with an F1-score of 0.93 (95% CI: 
0.912-0.943); κ = 0.88 (95% CI: 0.871-0.884). Performance was highest in common 
categories RI-RADS D and X (F1 ≥ 0.96) and lowest for rare categories RI-RADS A 
and B (F1 ≤ 0.49). When grouped into three categories (adequate, inadequate, and 
unacceptable), overall model performance improved [F1-score = 0.97; (95% CI: 
0.96-0.97)].
CONCLUSION: The locally adapted BERT-based model demonstrated high performance 
and almost perfect agreement with radiologists in automated RI-RADS scoring, 
showing promise for integration into radiology workflows to improve requisition 
completeness and communication.
KEY POINTS: Question Can an LLM accurately and automatically score radiology 
requisitions based on standardized criteria to address the challenges of 
incomplete information in radiological practice? Findings A locally adapted 
BERT-based model demonstrated high performance (F1-score 0.93) and almost 
perfect agreement with radiologists in automated RI-RADS scoring across a large, 
multi-institutional dataset. Clinical relevance LLMs offer a scalable solution 
for automated scoring of radiology requisitions, with the potential to improve 
workflow in radiology. Further improvement and integration into clinical 
practice could enhance communication, contributing to better diagnoses and 
patient care.

© 2025. The Author(s), under exclusive licence to European Society of Radiology.

DOI: 10.1007/s00330-025-11933-2
PMID: 40836020

Conflict of interest statement: Compliance with ethical standards. Guarantor: 
Rasit Eren Buyuktoka. Conflict of interest: The authors of this manuscript 
declare no relationships with any companies, whose products or services may be 
related to the subject matter of the article. Statistics and biometry: Murat 
Surucu, PhD, kindly provided statistical advice for this manuscript. Informed 
consent: Written informed consent was waived by the Institutional Review Board. 
Ethical approval: Institutional Review Board approval was obtained. Study 
subjects or cohorts overlap: We declare that there is no subject overlap with 
previously published work or manuscripts currently under review elsewhere. The 
study cohort, data, analyses, and results presented in this manuscript have not 
been previously reported or published. Methodology: Retrospective Observational 
Multicenter study


917. Int J Environ Res Public Health. 2025 Oct 5;22(10):1527. doi: 
10.3390/ijerph22101527.

Rapid Agrichemical Inventory via Video Documentation and Large Language Model 
Identification.

Anastario M(1), Armendáriz-Arnez C(2), Shakespeare Largo L(1), Gordon T(3), 
Roberts EFS(3).

Author information:
(1)Department of Health Sciences, Northern Arizona University, Flagstaff, AZ 
86011, USA.
(2)Escuela Nacional de Estudios Superiores Unidad Morelia, Universidad Nacional 
Autónoma de México, Morelia 58190, Mexico.
(3)Department of Anthropology, University of Michigan, Ann Arbor, MI 48109, USA.

BACKGROUND: This technical note presents a methodological approach to 
agrichemical inventory documentation. It complements exposure assessments in 
field settings with time-restricted observational periods. Conducted in 
Michoacán, Mexico, this method leverages large language model (LLM) capabilities 
for categorizing agrichemicals from brief video footage.
METHOD: Given time-limited access to a storage shed housing various 
agrichemicals, a short video was recorded and processed into 31 screenshots. 
Using OpenAI's ChatGPT (model: GPT-4o®), agrichemicals in each image were 
identified and categorized as fertilizers, herbicides, insecticides, fungicides, 
or other substances.
RESULTS: Human validation revealed that the LLM accurately identified 75% of 
agrichemicals, with human verification correcting entries.
CONCLUSIONS: This rapid identification method builds upon behavioral methods of 
exposure assessment, facilitating initial data collection in contexts where 
researcher access to hazardous materials may be time limited and would benefit 
from the efficiency and cross-validation offered by this method. Further 
refinement of this LLM-assisted approach could optimize accuracy in the 
identification of agrichemical products and expand its application to complement 
exposure assessments in field-based research, particularly as LLM technologies 
rapidly evolve. Most importantly, this Technical Note illustrates how field 
researchers can strategically harness LLMs under real-world time constraints, 
opening new possibilities for rapid observational approaches to exposure 
assessment.

DOI: 10.3390/ijerph22101527
PMCID: PMC12563168
PMID: 41154931 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


918. J Cataract Refract Surg. 2025 Apr 25. doi: 10.1097/j.jcrs.0000000000001680. 
Online ahead of print.

Cataract Surgery Registries: History, Utility, Barriers and Future.

Pietris J(1)(2)(3), Bahrami B(4)(3), LaHood B(4)(3), Goggin M(4)(3), Chan 
WO(2)(3).

Author information:
(1)School of Medicine, University of Adelaide, Adelaide SA 5000, Australia.
(2)Department of Ophthalmology, Royal Adelaide Hospital, Adelaide SA 5000, 
Australia.
(3)South Australian Institute of Ophthalmology, University of Adelaide, Adelaide 
SA 5000, Australia.
(4)Department of Ophthalmology, Queen Elizabeth Hospital, Woodville SA 5011, 
Australia.

Cataract surgery databases have become indispensable tools in ophthalmology, 
providing extensive data that enhance surgical practices and patient care. This 
narrative review traces the development of these databases, and summarises some 
of the significant contributions of these databases, such as improved surgical 
outcomes, informed clinical guidelines, and enhanced quality assurance. There 
are significant barriers to establishing and maintaining cataract surgery 
databases, including data protection and management challenges, economic 
constraints, technological hurdles, and ethical considerations. These obstacles 
complicate efforts to ensure data accuracy, standardisation, and 
interoperability across diverse healthcare settings. Large language models, and 
artificial intelligence has potential in streamlining data collection and 
analysis for the future of these databases. Innovations like blockchain for data 
security and cloud computing for scalability are examined as solutions to 
current limitations. Addressing the existing challenges and leveraging 
technological advancements will be crucial for the continued evolution and 
utility of these databases, ensuring they remain pivotal in advancing cataract 
surgery and patient care.

Copyright © 2025 Published by Wolters Kluwer on behalf of ASCRS and ESCRS.

DOI: 10.1097/j.jcrs.0000000000001680
PMID: 40277407


919. Front Oncol. 2025 May 15;15:1508455. doi: 10.3389/fonc.2025.1508455. eCollection 
2025.

Early prediction of colorectal adenoma risk: leveraging large-language model for 
clinical electronic medical record data.

Yang X(1), Xu J(1), Ji H(1), Li J(2), Yang B(3), Wang L(3).

Author information:
(1)Information Management and Big Data Center, Peking University Third Hospital, 
Beijing, China.
(2)Department of Gastroenterology, Peking University Third Hospital, Beijing, 
China.
(3)Goodwill Hessian Health Technology Co. Ltd, Beijing, China.

OBJECTIVE: To develop a non-invasive, radiation-free model for early colorectal 
adenoma prediction using clinical electronic medical record (EMR) data, 
addressing limitations in current diagnostic approaches for large-scale 
screening.
DESIGN: Retrospective analysis utilized 92,681 cases with EMR, spanning from 
2012 to 2022, as the training cohort. Testing was performed on an independent 
test cohort of 19,265 cases from 2023. Several classical machine learning 
algorithms were applied in combination with the BGE-M3 large-language model 
(LLM) for enhanced semantic feature extraction. Area under the receiver 
operating characteristic curve (AUC) is the major metric for evaluating model 
performance. The Shapley additive explanations (SHAP) method was employed to 
identify the most influential risk factors.
RESULTS: XGBoost algorithm, integrated with BGE-M3, demonstrated superior 
performance (AUC = 0.9847) in the validation cohort. Notably, when applied to 
the independent test cohort, XGBoost maintained its strong predictive ability 
with an AUC of 0.9839 and an average advance prediction time of 6.88 hours, 
underscoring the effectiveness of the BGE-M3 model. The SHAP analysis further 
identified 16 high-impact risk factors, highlighting the interplay of genetic, 
lifestyle, and environmental influences on colorectal adenoma risk.
CONCLUSION: This study developed a robust machine learning-based model for 
colorectal adenoma risk prediction, leveraging clinical EMR and LLM. The 
proposed model demonstrates high predictive accuracy and has the potential to 
enhance early detection, making it well-suited for large-scale screening 
programs. By facilitating early identification of individuals at risk, this 
approach may contribute to reducing the incidence and mortality associated with 
colorectal cancer.

Copyright © 2025 Yang, Xu, Ji, Li, Yang and Wang.

DOI: 10.3389/fonc.2025.1508455
PMCID: PMC12119310
PMID: 40444092

Conflict of interest statement: Authors BY and LW were employed by the company 
Goodwill Hessian Health Technology Co. Ltd. The remaining authors declare that 
the research was conducted in the absence of any commercial or financial 
relationships that could be construed as a potential conflict of interest.


920. Eur Radiol. 2025 Aug 8. doi: 10.1007/s00330-025-11888-4. Online ahead of print.

GPT-4 for automated sequence-level determination of MRI protocols based on 
radiology request forms from clinical routine.

Terzis R(#)(1), Kaya K(#)(2), Schömig T(2), Janssen JP(2), Iuga AI(2), Kottlors 
J(2), Lennartz S(2), Gietzen C(2), Gözdas C(2), Müller L(3), Hahnfeldt R(2), 
Maintz D(2), Dratsch T(2), Pennig L(2).

Author information:
(1)Institute for Diagnostic and Interventional Radiology, Faculty of Medicine 
and University Hospital Cologne, University of Cologne, Cologne, Germany. 
robert.terzis@uk-koeln.de.
(2)Institute for Diagnostic and Interventional Radiology, Faculty of Medicine 
and University Hospital Cologne, University of Cologne, Cologne, Germany.
(3)Department of Diagnostic and Interventional Radiology, University Medical 
Center of the Johannes Gutenberg-University, Mainz, Germany.
(#)Contributed equally

OBJECTIVES: This study evaluated GPT-4's accuracy in MRI sequence selection 
based on radiology request forms (RRFs), comparing its performance to radiology 
residents.
MATERIALS AND METHODS: This retrospective study included 100 RRFs across four 
subspecialties (cardiac imaging, neuroradiology, musculoskeletal, and oncology). 
GPT-4 and two radiology residents (R1: 2 years, R2: 5 years MRI experience) 
selected sequences based on each patient's medical history and clinical 
questions. Considering imaging society guidelines, five board-certified 
specialized radiologists assessed protocols based on completeness, quality, and 
utility in consensus, using 5-point Likert scales. Clinical applicability was 
rated binarily by the institution's lead radiographer.
RESULTS: GPT-4 achieved median scores of 3 (1-5) for completeness, 4 (1-5) for 
quality, and 4 (1-5) for utility, comparable to R1 (3 (1-5), 4 (1-5), 4 (1-5); 
each p > 0.05) but inferior to R2 (4 (1-5), 5 (1-5); p < 0.01, respectively, and 
5 (1-5); p < 0.001). Subspecialty protocol quality varied: GPT-4 matched R1 (4 
(2-4) vs. 4 (2-5), p = 0.20) and R2 (4 (2-5); p = 0.47) in cardiac imaging; 
showed no differences in neuroradiology (all 5 (1-5), p > 0.05); scored lower 
than R1 and R2 in musculoskeletal imaging (3 (2-5) vs. 4 (3-5); p < 0.01, and 5 
(3-5); p < 0.001); and matched R1 (4 (1-5) vs. 2 (1-4), p = 0.12) as well as R2 
(5 (2-5); p = 0.20) in oncology. GPT-4-based protocols were clinically 
applicable in 95% of cases, comparable to R1 (95%) and R2 (96%).
CONCLUSION: GPT-4 generated MRI protocols with notable completeness, quality, 
utility, and clinical applicability, excelling in standardized subspecialties 
like cardiac and neuroradiology imaging while yielding lower accuracy in 
musculoskeletal examinations.
KEY POINTS: Question Long MRI acquisition times limit patient access, making 
accurate protocol selection crucial for efficient diagnostics, though it's 
time-consuming and error-prone, especially for inexperienced residents. Findings 
GPT-4 generated MRI protocols of remarkable yet inconsistent quality, performing 
on par with an experienced resident in standardized fields, but moderately in 
musculoskeletal examinations. Clinical relevance The large language model can 
assist less experienced radiologists in determining detailed MRI protocols and 
counteract increasing workloads. The model could function as a semi-automatic 
tool, generating MRI protocols for radiologists' confirmation, optimizing 
resource allocation, and improving diagnostics and cost-effectiveness.

© 2025. The Author(s).

DOI: 10.1007/s00330-025-11888-4
PMID: 40779162

Conflict of interest statement: Compliance with ethical standards. Guarantor: 
The scientific guarantor of this publication is Lenhard Pennig. Conflict of 
interest: S.L. is a member of the Scientific Editorial Board of European 
Radiology (Section: computer tomography). As such, they did not participate in 
the selection or review processes for this article. The remaining authors of 
this manuscript declare no relationships with any companies, whose products or 
services may be related to the subject matter of the article. Conflicts of 
interest not related to this article: David Maintz and Lenhard Pennig received 
speaker’s honoraria from Philips Healthcare. Statistics and biometry: Two 
authors have significant statistical expertise: Jonathan Kottlors, Institute for 
Diagnostic and Interventional Radiology, University Hospital Cologne, Cologne, 
Germany Thomas Dratsch, Institute for Diagnostic and Interventional Radiology, 
University Hospital Cologne, Cologne, Germany Informed consent: Written informed 
consent was waived by the Institutional Review Board. Ethical approval: 
Institutional Review Board approval was obtained. Study subjects or cohorts 
overlap: None. Methodology: Retrospective Experimental Performed at one 
institution


921. J Allergy Clin Immunol. 2025 Sep 7:S0091-6749(25)00939-X. doi: 
10.1016/j.jaci.2025.08.022. Online ahead of print.

Artificial intelligence in allergy and immunology: Recent developments, 
implementation challenges, and the road toward clinical impact.

van Breugel M(1), Greenhawt M(2), Eguiluz-Gracia I(3), Torres Jaén MJ(3), 
Anagnostou A(4), Koppelman GH(5).

Author information:
(1)Department of Pediatric Pulmonology and Pediatric Allergology, Beatrix 
Children's Hospital, University Medical Center Groningen, University of 
Groningen, Groningen, The Netherlands; Groningen Research Institute for Asthma 
and COPD (GRIAC), University Medical Center Groningen, University of Groningen, 
Groningen, The Netherlands; Ditto Care, Rotterdam, The Netherlands. Electronic 
address: m.van.breugel@umcg.nl.
(2)Department of Pediatrics, Section of Allergy and Immunology, Children's 
Hospital Colorado, University of Colorado School of Medicine, Aurora, Colo.
(3)Allergy Unit, IBIMA-BIONAND, Regional University Hospital of Malaga, Malaga 
University, Malaga, Spain.
(4)Division of Allergy, Immunology and Retrovirology, Baylor College of 
Medicine, Houston, Tex.
(5)Department of Pediatric Pulmonology and Pediatric Allergology, Beatrix 
Children's Hospital, University Medical Center Groningen, University of 
Groningen, Groningen, The Netherlands; Groningen Research Institute for Asthma 
and COPD (GRIAC), University Medical Center Groningen, University of Groningen, 
Groningen, The Netherlands.

Artificial intelligence (AI) is increasingly recognized for its capacity to 
transform medicine. While publications applying AI in allergy and immunology 
have increased in number, clinical implementation substantially lags behind 
other specialties. By mid-2024, over 1,000 US Food and Drug 
Administration-approved AI-enabled medical devices existed, but none 
specifically addressed allergy and immunology. This gap partly reflects the 
field's limited reliance on imaging, which facilitated early AI breakthroughs in 
radiology and pathology. This narrative review examines recent AI developments, 
including large language models and AI agents, evaluating their applicability to 
allergy and immunology practice. We analyze current and potential applications, 
emphasizing those demonstrating clinical value while identifying implementation 
barriers amplified by allergic diseases' unique complexities, including data 
privacy concerns, bias, reliability constraints, and evolving regulatory 
frameworks. To bridge the persistent research-to-implementation gap, we propose 
a 6-point road map: (1) prioritize impactful applications, (2) define clinically 
relevant benchmarks, (3) enforce rigorous governance, (4) transition to 
operationalization, (5) promote clinical adoption through trustworthy AI, and 
(6) establish life cycle management. This road map builds on established 
implementation frameworks while incorporating critical field-specific 
considerations unique to allergy and immunology. Through this approach, we 
provide a perspective for advancing AI in allergy and immunology from academic 
promise to tangible clinical benefit.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jaci.2025.08.022
PMID: 40925528

Conflict of interest statement: Disclosure statement Disclosure of potential 
conflict of interest: M. van Breugel is founder of and owns stock in Ditto Care. 
M. Greenhawt is a consultant for Aquestive; is a member of physician/medical 
advisory boards for DBV Technologies, Takeda, Griffols, Nutricia, Novartis, 
Aquestive, Allergy Therapeutics, AstraZeneca, ALK-Abello, Bryn, Genentech, and 
Prota; is a speaker for Genentech and ARS; receives research funding from the 
Asthma and Allergy Foundation of America; is an unpaid member of the scientific 
advisory council for the National Peanut Board and medical advisory board of the 
International Food Protein Induced Enterocolitis Syndrome Association; is a 
member of the Brighton Collaboration Criteria Vaccine Anaphylaxis 2.0 working 
group; is senior associate editor for Annals of Allergy, Asthma & Immunology; is 
a member of the Joint Taskforce on Allergy Practice Parameters; and has received 
honoraria for lectures from Red Nucleus, Medscape, Paradigm Medical 
Communications, RMEI, PSL, Kaplan, Food Allergy Research and Education, as well 
as multiple state/local allergy societies. M. J. Torres Jaén reports, outside 
the submitted work, personal fees from Diater, Aimmune Therapeutics, and Leti 
laboratories; and grants from European Commission, MINECO and ISCIII of the 
Spanish government and SEAIC. A. Anagnostou reports institutional funding from 
Novartis, AAFA, Aquestive, ALK; advisory board member of Novartis, Bryn, 
Aquestive, and ALK; and consultation/speaker fees from ALK, EPG Health, MJH, 
Adelphi, Aimmune Therapeutics, Genentech, Medscape, ARS, FARE, and Stallergens. 
G. H. Koppelman reports institutional grant funding from the Dutch Lung 
Foundation, ZON-MW (VICI grant), Ubbo Emmius Foundation, European Union 
(Prominent grant), Vertex, TEVA The Netherlands, and GSK; fees to his 
institution for consultancy to AZ, Pure IMS, and GSK; and invited speaker for 
Sanofi, AstraZeneca, and Boehringer-Ingelheim, all outside the submitted work; 
and is cofounder and chair of the exquAIro Foundation (www.exquAiro.com), aimed 
at educating biomedical professionals to support medical breakthroughs with AI 
in medicine. The other author declares no relevant conflicts of interest.


922. Radiology. 2025 Apr;315(1):e241554. doi: 10.1148/radiol.241554.

Accuracy of Large Language Model-based Automatic Calculation of Ovarian-Adnexal 
Reporting and Data System MRI Scores from Pelvic MRI Reports.

Bhayana R(1), Jajodia A(1), Chawla T(1), Deng Y(2), Bouchard-Fortier G(3)(4), 
Haider M(2), Krishna S(2).

Author information:
(1)University Medical Imaging Toronto, Joint Department of Medical Imaging, 
University Health Network, Mount Sinai Hospital and Women's College Hospital, 
Department of Medical Imaging, University of Toronto, Toronto General Hospital, 
200 Elizabeth St, Peter Munk Building, 1st Fl, Toronto, ON, Canada M5G 24C.
(2)Department of Biostatistics, University Health Network, Toronto, Canada.
(3)Department of Obstetrics and Gynecology, University of Toronto, Toronto, 
Canada.
(4)Division of Gynecologic Oncology, Princess Margaret Cancer Centre, University 
Health Network and Sinai Health System, Toronto, Canada.

Background Ovarian-Adnexal Reporting and Data System (O-RADS) for MRI helps 
assign malignancy risk, but radiologist adoption is inconsistent. Automatic 
assignment of O-RADS scores from reports could increase adoption and accuracy. 
Purpose To evaluate the accuracy of large language models (LLMs), after 
strategic optimization, for automatically calculating O-RADS scores from 
reports. Materials and Methods This retrospective single-center study from a 
large quaternary care cancer center included consecutive gadolinium 
chelate-enhanced pelvic MRI reports with at least one assigned O-RADS score from 
July 2021 to October 2023. Reports from January 2018 to October 2019 (before 
O-RADS MRI implementation) were randomly selected for additional testing. 
Reference standard O-RADS scores were determined by radiologists interpreting 
reports. After prompt optimization using a subset of reports, two LLM-based 
strategies were evaluated: few-shot learning with GPT-4 (version 0613; OpenAI) 
prompted with O-RADS rules ("LLM only") and a hybrid strategy leveraging GPT-4 
to classify features fed into a deterministic formula ("hybrid"). Accuracy of 
each model and originally reported scores were calculated and compared using the 
McNemar test. Results A total of 284 reports from 284 female patients (mean age, 
53.2 years ± 16.3 [SD]) with 372 adnexal lesions were included: 10 reports in 
the training set (16 lesions), 134 reports in the internal test set 1 (173 
lesions; 158 O-RADS assigned), and 140 reports in internal test set 2 (183 
lesions). For assigning O-RADS MRI scores, the hybrid model accuracy (97%; 168 
of 173) outperformed LLM-only model (90%; 155 of 173; P = .006). For lesions 
with an originally reported O-RADS score, hybrid model accuracy exceeded that of 
reporting radiologists (97% [153 of 158] vs 88% [139 of 158]; P = .004). Hybrid 
model also outperformed LLM-only model for 183 lesions from before O-RADS 
implementation (95% [173 of 183] vs 87% [159 of 183], respectively; P = .01). 
Conclusion A hybrid LLM-based application, combining LLM feature classification 
with deterministic elements, accurately assigned O-RADS MRI scores from report 
descriptions, exceeding both an LLM-only strategy and the original reporting 
radiologist. © RSNA, 2025 Supplemental material is available for this article.

DOI: 10.1148/radiol.241554
PMID: 40167432 [Indexed for MEDLINE]


923. Am J Clin Pathol. 2025 Nov 19;164(5):721-729. doi: 10.1093/ajcp/aqaf091.

Systematic comparison of GPT models for the analysis of pathology reports in a 
low-resource language: A case study for Turkish.

Dilbaz OF(1), Ozates MN(2)(3), Bolat B(4), Gunduz-Demir C(2)(3), Kulac 
I(3)(5)(6).

Author information:
(1)Department of Pathology, Sisli Hamidiye Etfal Health Application and Research 
Center, University of Health Sciences, Istanbul, Turkey.
(2)Department of Computer Engineering, Koc University, Istanbul, Turkey.
(3)KUIS AI Center, Koc University, Istanbul, Turkey.
(4)School of Medicine, Koc University, Istanbul, Turkey.
(5)Department of Pathology, School of Medicine, Koc University, Istanbul, 
Turkey.
(6)Research Center for Translational Medicine, Koc University, Istanbul, Turkey.

OBJECTIVE: Large language models (LLMs) can process text for various 
applications, including surgical pathology reports, but studies primarily focus 
on English. Their performance has not been systematically studied for a 
low-resource language. To analyze the performance of various LLMs, 759 Turkish 
pathology reports from 5 different procedures were selected.
METHODS: We used 10 examples from every procedure to optimize prompts for 
OpenAI's GPT-3.5 Turbo, GPT-4o mini, and GPT-4o. The rest was used to test 
generalizability.
RESULTS: The GPT-4o model performed superior in processing Turkish reports 
(12%-25% over GPT-3.5 Turbo, 3%-16% over GPT-4o mini). English-translated 
versions of the reports have been demonstrated to enhance accuracy, especially 
for GPT-3.5 Turbo and GPT-4o mini. GPT4-o showed comparable results for Turkish 
and English. A 12% to 22% performance gap was observed between GPT-4o and 
GPT-3.5 Turbo for English-translated reports. Domain-related tips in prompts 
increased accuracy. Results of larger test sets were parallel for all models 
with the validation set. The GPT-4o model yielded the most accurate results, 
while the GPT-4o mini model demonstrated intermediate performance. The GPT-3.5 
Turbo model exhibited the least accuracy.
CONCLUSIONS: To our knowledge, for the first time in the literature, we have 
demonstrated the performance of GPT models in Turkish surgical pathology 
reports, and results indicate that data extracted by GPT-4o are almost ready for 
direct application.

© American Society for Clinical Pathology, 2025.

DOI: 10.1093/ajcp/aqaf091
PMCID: PMC12629874
PMID: 40971916 [Indexed for MEDLINE]

Conflict of interest statement: None declared.


924. Br J Anaesth. 2025 Jul;135(1):21-25. doi: 10.1016/j.bja.2025.04.004. Epub 2025 
May 21.

Generative AI models: the next anaesthetic agent?

Julius A(1), Bowness JS(2).

Author information:
(1)Department of Anaesthesia, Royal Free London NHS Foundation Trust, London, 
UK; Department of Targeted Intervention, University College London, London, UK. 
Electronic address: https://twitter.com/@anaesthetic_AI.
(2)Department of Targeted Intervention, University College London, London, UK; 
Department of Anaesthesia, University College London Hospitals NHS Foundation 
Trust, London, UK. Electronic address: https://twitter.com/@bowness_james.

Comment on
    doi: 10.1016/j.bja.2025.01.028.

A study by MacKay and colleagues addresses a pressing need in cardiac 
anaesthesia by demonstrating an innovative method to extract structured data 
from free-text intraoperative transoesophageal echocardiography reports. 
Narrative descriptions of echocardiographic findings are often unstructured, 
making manual extraction labour-intensive and susceptible to error. By deploying 
an ensemble of large language models in a consensus-based framework, the authors 
show that key echocardiographic parameters can be extracted with a high degree 
of accuracy and manageable error rates. This work presents a technical solution 
to a specific data-handling challenge and points towards broader applications of 
artificial intelligence (AI) in streamlining perioperative care.

Copyright © 2025 British Journal of Anaesthesia. Published by Elsevier Ltd. All 
rights reserved.

DOI: 10.1016/j.bja.2025.04.004
PMCID: PMC12715424
PMID: 40537197 [Indexed for MEDLINE]

Conflict of interest statement: Declarations of interest JSB is an employee of 
GE Healthcare, a healthcare technology company producing AI-based medical 
devices. He has previously served as a senior clinical advisor for Intelligent 
Ultrasound, receiving research funding and honoraria, acts as a consultant for 
AutonomUS, and has received speaker fees from the Belgian Association for 
Regional Anaesthesia. AJ is a co-founder of clinicalguidelines.ai, a health tech 
company focused on generative AI solutions for health.


925. Eur Radiol. 2025 Sep;35(9):5299-5313. doi: 10.1007/s00330-025-11500-9. Epub 2025 
Mar 17.

Impact of hospital-specific domain adaptation on BERT-based models to classify 
neuroradiology reports.

Agarwal S(1), Wood D(1), Murray BAK(1), Wei Y(1), Busaidi AA(2), Kafiabadi S(2), 
Guilhem E(2), Lynch J(2), Townend M(1), Mazumder A(3), Barker GJ(4), Cole JH(5), 
Sasieni P(6), Ourselin S(1), Modat M(1), Booth TC(7)(8).

Author information:
(1)School of Biomedical Engineering & Imaging Sciences, King's College London, 
Becket House, London, UK.
(2)Department of Neuroradiology, Ruskin Wing, King's College Hospital NHS 
Foundation Trust, London, UK.
(3)Guy's and St Thomas' NHS Foundation Trust, Great Maze Pond, London, UK.
(4)Department of Neuroimaging, Institute of Psychiatry, Psychology, & 
Neuroscience, King's College London, London, UK.
(5)Centre for Medical Image Computing, Department of Computer Science, 
University College London, London, UK.
(6)Clinical Trials Unit, King's College London, Guy's Campus, Great Maze Pond, 
London, UK.
(7)School of Biomedical Engineering & Imaging Sciences, King's College London, 
Becket House, London, UK. thomas.booth@kcl.ac.uk.
(8)Department of Neuroradiology, Ruskin Wing, King's College Hospital NHS 
Foundation Trust, London, UK. thomas.booth@kcl.ac.uk.

OBJECTIVES: To determine the effectiveness of hospital-specific domain 
adaptation through masked language modelling (MLM) on BERT-based models' 
performance in classifying neuroradiology reports, and to compare these models 
with open-source large language models (LLMs).
MATERIALS AND METHODS: This retrospective study (2008-2019) utilised 126,556 and 
86,032 MRI brain reports from two tertiary hospitals-King's College Hospital 
(KCH) and Guys and St Thomas' Trust (GSTT). Various BERT-based models, including 
RoBERTa, BioBERT and RadBERT, underwent MLM on unlabelled reports from these 
centres. The downstream tasks were binary abnormality classification and 
multi-label classification. Performances of models with and without 
hospital-specific domain adaptation were compared against each other and LLMs on 
internal (KCH) and external (GSTT) hold-out test sets. Model performances for 
binary classification were compared using 2-way and 1-way ANOVA.
RESULTS: All models that underwent hospital-specific domain adaptation performed 
better than their baseline counterparts (all p-values < 0.001). For binary 
classification, MLM on all available unlabelled reports (194,467 reports) 
yielded the highest balanced accuracies (KCH: mean 97.0 ± 0.4% (standard 
deviation), GSTT: 95.5 ± 1.0%), after which no differences between BERT-based 
models remained (1-way ANOVA, p-values > 0.05). There was a log-linear 
relationship between the number of reports and performance. LLama-3.0 70B was 
the best-performing LLM (KCH: 97.1%, GSTT: 94.0%). Multi-label classification 
demonstrated consistent performance improvements from MLM for all abnormality 
categories.
CONCLUSION: Hospital-specific domain adaptation should be considered best 
practice when deploying BERT-based models in new clinical settings. When 
labelled data is scarce or unavailable, LLMs can serve as a viable alternative, 
assuming adequate computational power is accessible.
KEY POINTS: Question BERT-based models can classify radiology reports, but it is 
unclear if there is any incremental benefit from additional hospital-specific 
domain adaptation. Findings Hospital-specific domain adaptation resulted in the 
highest BERT-based model accuracies and performance scaled log-linearly with the 
number of reports. Clinical relevance BERT-based models after hospital-specific 
domain adaptation achieve the best classification results provided sufficient 
high-quality training labels. When labelled data is scarce, LLMs such as 
Llama-3.0 70B are a viable alternative provided there are sufficient 
computational resources.

© 2025. The Author(s).

DOI: 10.1007/s00330-025-11500-9
PMCID: PMC12350531
PMID: 40097844 [Indexed for MEDLINE]

Conflict of interest statement: Compliance with ethical standards. Guarantor: 
The scientific guarantor of this publication is Thomas C. Booth. Conflict of 
interest: The authors of this manuscript declare no relationships with any 
companies, whose products or services may be related to the subject matter of 
the article. Statistics and biometry: One of the authors has significant 
statistical expertise. Informed consent: Written informed consent was not 
required for this study because reports contained no patient-identifiable data. 
Ethical approval: Institutional Review Board approval was obtained. The UK 
National Health Research Authority/REC approved this retrospective 
methodological study (IRAS ID 235,658, REC ID 18/YH/0458). Patient consent was 
not obtained as this retrospective study used radiology reports which contained 
no patient-identifiable information. No animal subjects were used. Study 
subjects or cohorts overlap: Some study subjects or cohorts have been previously 
reported in our previous submission to European Radiology [4]. Methodology: 
Retrospective Experimental Multicenter study


926. Front Neurol. 2026 Jan 7;16:1737297. doi: 10.3389/fneur.2025.1737297. 
eCollection 2025.

Protocol for a randomized controlled trial evaluating the artificial 
intelligence health education accurately linking system in patients with 
mild-to-moderate stroke.

Liu Z(#)(1), Li YH(#)(2), Fang Y(3), Wang H(4), Wu T(4), Liu S(1), Yang Y(1), 
Qin Y(1), Tao X(1), Mao J(1), Wang L(5), Li X(6), Wang X(5), Yang R(5), Liu 
Y(6), Chen M(6), Shi D(5), Li N(3), Wang Y(3), Hu Y(#)(7), Zhang S(#)(8).

Author information:
(1)Nursing Department, the First Affiliated Hospital of Henan University of 
Chinese Medicine, Zhengzhou, China.
(2)School of Education, Chengdu College of Arts and Sciences, Chengdu, China.
(3)School of Nursing, Henan University of Science and Technology, Luoyang, 
China.
(4)The Brain Disease Regional Diagnosis and Treatment Center, the First 
Affiliated Hospital of Henan University of Chinese Medicine, Zhengzhou, China.
(5)Department of Neurology, the First Affiliated Hospital of Henan University of 
Science and Technology, Luoyang, China.
(6)Intensive Care Unit, the First Affiliated Hospital of Henan University of 
Science and Technology, Luoyang, China.
(7)School of Nursing, Shandong Second Medical University, Weifan, China.
(8)Nursing Department, the First Affiliated Hospital of Henan University of 
Science and Technology, Luoyang, China.
(#)Contributed equally

BACKGROUND: Stroke is a leading cause of death and disability worldwide. 
Although survival rates from mild-to-moderate stroke are high, long-term 
functional impairment remains common, requiring sustained self-management beyond 
traditional rehabilitation. Conventional models depend on institutional medical 
care, which not only drives up costs but also disrupts continuity of care. 
Meanwhile, psychological, risk-related, and behavioral factors are often 
overlooked. Advances in artificial intelligence (AI) and mobile health provide 
opportunities for individualized, long-term support. Based on this, we developed 
the AI Health Education Accurately Linking System (AI-HEALS) to evaluate its 
potential to improve physiological parameters, risk perception, and 
self-management in patients with mild-to-moderate stroke.
METHODS: This single-blind randomized controlled trial evaluates AI-HEALS, 
delivered via WeChat (China's most widely used social media app), to improve the 
monitoring of key physiological indicators in patients with mild-to-moderate 
stroke. Eligible participants are randomly allocated either standard care as a 
control or standard care plus a three-month regimen of AI-HEALS. It features an 
AI-powered interactive Q&A system that supports both voice and text 
communication, real-time monitoring of physiological and behavioral indicators, 
personalized health reminders, and specially designed educational content. These 
are all offered through the official WeChat account "Stroke Health Management 
Expert." The primary outcomes are changes in blood pressure, glucose, and blood 
lipids. Secondary outcomes include risk perception of recurrence of stroke, 
self-management behaviors, and psychological state of mind. Follow-up 
assessments are conducted at 3, 6, and 9 months after completion of the 
intervention to evaluate both short-term and sustained effects.
DISCUSSION: This protocol presents a new AI-mHealth approach to delivering 
stroke care. If proven feasible and effective, AI-HEALS could offer a scalable 
and sustainable model for improving long-term health outcomes, reducing the risk 
of recurrence, and optimizing the use of healthcare resources for stroke and 
other chronic conditions.
CLINICAL TRIAL REGISTRATION: 
https://www.chictr.org.cn/showproj.html?proj=251515, Identifier, 
ChiCTR2500096422.

Copyright © 2026 Liu, Li, Fang, Wang, Wu, Liu, Yang, Qin, Tao, Mao, Wang, Li, 
Wang, Yang, Liu, Chen, Shi, Li, Wang, Hu and Zhang.

DOI: 10.3389/fneur.2025.1737297
PMCID: PMC12819302
PMID: 41573403

Conflict of interest statement: The author(s) declared that this work was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


927. Front Oncol. 2026 Jan 6;15:1766619. doi: 10.3389/fonc.2025.1766619. eCollection 
2025.

Correction: Benchmarking GPT-5 in radiation oncology: measurable gains, but 
persistent need for expert oversight.

Dinç U(1)(2)(3), Sarkar J(1)(2)(3), Schubert P(1)(2)(3), Semrau S(1)(2)(3), 
Weissmann T(1)(2)(3), Karius A(1)(2)(3), Brand J(1)(2)(3), Axer BN(1)(2)(3), 
Gomaa A(1)(2)(3), Stephan P(1)(2)(3), Sheth I(1)(2)(3), Beirami S(1)(2)(3), 
Schwarz A(1)(2)(3), Gaipl U(1)(2)(3), Frey B(1)(2)(3), Bert C(1)(2)(3), 
Corradini S(3)(4), Fietkau R(1)(2)(3), Putz F(1)(2)(3).

Author information:
(1)Department of Radiation Oncology, University Hospital Erlangen, 
Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany.
(2)Comprehensive Cancer Center Erlangen-Europäische Metropolregion Nürnberg 
(European Metropolitan Region Nuremberg) (CCC ER-EMN), Erlangen, Germany.
(3)Bavarian Cancer Research Center (BZKF), Munich, Germany.
(4)Department of Radiation Oncology, University Hospital, Ludwig Maximilian 
University of Munich, Munich, Germany.

Erratum for
    Front Oncol. 2025 Dec 11;15:1695468. doi: 10.3389/fonc.2025.1695468.

[This corrects the article DOI: 10.3389/fonc.2025.1695468.].

Copyright © 2026 Dinç, Sarkar, Schubert, Semrau, Weissmann, Karius, Brand, Axer, 
Gomaa, Stephan, Sheth, Beirami, Schwarz, Gaipl, Frey, Bert, Corradini, Fietkau 
and Putz.

DOI: 10.3389/fonc.2025.1766619
PMCID: PMC12817473
PMID: 41568373


928. Eur Radiol. 2025 Nov 17. doi: 10.1007/s00330-025-12122-x. Online ahead of print.

Comparing performance of seven fine-tuned open-source large language models in 
summarizing and predicting outcome-relevant information from mechanical 
thrombectomy reports in patients with acute ischemic stroke.

Eminovic S(1), Lindholz M(2)(3), Dell'Orco A(4), Meddeb A(3)(4), Baumgärtner 
GL(2), Schulze-Weddige S(2), Siebert E(4), Wattjes MP(4), Penzkofer T(#)(2)(3), 
Nawabi J(#)(3)(4).

Author information:
(1)Department of Radiology, Charité - Universitätsmedizin Berlin, 
Humboldt-Universität zu Berlin, Freie Universität Berlin, Berlin Institute of 
Health, Berlin, Germany. semil.eminovic@charite.de.
(2)Department of Radiology, Charité - Universitätsmedizin Berlin, 
Humboldt-Universität zu Berlin, Freie Universität Berlin, Berlin Institute of 
Health, Berlin, Germany.
(3)Berlin Institute of Health at Charité - Universitätsmedizin Berlin, Berlin, 
Germany.
(4)Department of Neuroradiology, Charité - Universitätsmedizin Berlin, 
Humboldt-Universität zu Berlin, Freie Universität Berlin, Berlin Institute of 
Health, Berlin, Germany.
(#)Contributed equally

PURPOSE: This study evaluates seven open-source Large Language Models (LLMs) in 
summarizing radiology reports of acute ischemic stroke patients treated with 
mechanical thrombectomy and predicting angiography-based outcome measures 
relevant to post-thrombectomy reperfusion.
MATERIALS AND METHODS: 2000 mechanical thrombectomy reports (findings and 
summarizing impression section as gold standard) were split into training set 
(N = 1900) for model fine-tuning and test set (N = 100). A two-step evaluation 
was performed: (1) Quantitative analyses of seven LLMs with metrics ROUGE-1, -2, 
-L, METEOR, BERTScore (F1) and BLEU comparing LLM-generated summaries against 
gold-standard impressions. (2) Qualitative manual evaluation of the four 
best-performing models by two radiologists, assessing correctness and 
completeness across key parameters: outcome-relevant scores, vessel information, 
occlusion side, number of passes, relevant additional information, 
hallucinations, and grammar quality. Statistical significance was assessed via a 
two-tailed, four-sample χ² test, followed by post hoc pairwise χ² comparisons.
RESULTS: BioMistral-7b scored highest across most quantitative metrics (ROUGE-1: 
0.47, ROUGE-2: 0.30, ROUGE-L: 0.43, METEOR: 0.46, BERTScore (F1): 0.82). Manual 
evaluation revealed gemma-2-9b most frequently documented pass counts (56 out of 
100 cases (56%); p < 0.02 vs. Llama-3.1-8b/mistral-7b-instruct), while 
mistral-7b-instruct described them most often correctly (29 out of 38 mentioned 
passes (76.32%); p < 0.02 vs. BioMistral-7b and p < 0.01 vs. gemma-2-9b). All 
four manually evaluated LLMs performed moderately well in predicting 
"Thrombolysis-In-Cerebral-Ischemia (TICI)" Score (correctness rate ranging from 
66 to 71%; p = 0.89).
CONCLUSION: All four manually evaluated LLMs effectively summarized thrombectomy 
reports and demonstrated moderate accuracy predicting TICI scores. Their 
integration into radiology workflows could enhance efficiency, warranting 
further clinical validation.
KEY POINTS: Question Specifically fine-tuned Large Language Models (LLMs) can 
improve radiology workflow by automatically summarizing thrombectomy reports and 
inferring angiographic classifications from textual descriptions. Findings 
Fine-tuned LLMs achieve similar performance in summarizing thrombectomy reports, 
with each model performing best in specific categories and showing moderate 
accuracy in correct "Thrombolysis-In-Cerebral-Ischemia (TICI)" Score prediction 
(66-71%). Clinical relevance Integrating fine-tuned LLMs into radiology 
workflows may accelerate decision-making and improve patient outcomes by 
automatically summarizing reports and assessing recanalization success, while 
future work should enhance contextual understanding, address ambiguous inputs, 
and limit hallucinations.

© 2025. The Author(s).

DOI: 10.1007/s00330-025-12122-x
PMID: 41249547

Conflict of interest statement: Compliance with ethical standards. Guarantor: 
The scientific guarantor of this publication is Jawed Nawabi. Conflict of 
interest: T.P. reports research agreements (no personal payments, outside of the 
submitted work) with AGO Research GmbH, Aravive, Inc., ARCAGY-GINECO, Astellas 
Pharma Global Development Inc., AstraZeneca AB, AstraZeneca GmbH, Clovis 
Oncology, EQRx International, Inc., F. Hoffmann-La Roche Ltd, GlaxoSmithKline 
Research & Development Limited, Grupo Español de Investigación en Cáncer de 
Ovario (GEICO), ImmunoGen Inc, Incyte Corporation, Karyopharm Therapeutics, 
Mario Negri Gynecology Oncology Group (MaNGO) (111), Merck KGaA, Merck Sharp & 
Dohme Corp., NOGGO e.V., Nordic Society of Gynaecological Oncology – Clinical 
Trial Unit (NSGO-CTU), Novartis Pharma GmbH, NovoCure GmbH, Sutro Biopharma, 
Inc., TESARO Inc., TORL Biotherapeutics, LLC, Tubulis GmbH, 
Universitätspoliklinik A. Gemelli, and Verastem Inc, as well as fees for a book 
translation (Elsevier) and speaking engagements (Bayer Healthcare). J.N. reports 
research agreements (no personal payments, outside of submitted work) with Briya 
Lab Ltd. Statistics and biometry: No complex statistical methods were necessary 
for this paper. Informed consent: Written informed consent was waived by the 
Institutional Review Board. Ethical approval: Institutional Review Board 
approval was obtained. Study subjects or cohorts overlap: Some study subjects or 
cohorts have been previously reported in Meddeb et al [10]. Methodology: 
Retrospective Observational Performed at one institution


929. Front Endocrinol (Lausanne). 2025 Sep 29;16:1667809. doi: 
10.3389/fendo.2025.1667809. eCollection 2025.

LLM evaluation for thyroid nodule assessment: comparing ACR-TIRADS, C-TIRADS, 
and clinician-AI trust gap.

Dai X(1)(2), Xi Y(3), Hu Y(4), Ding Q(2), Zhang Y(2), Liu H(2), Chen P(2), Wang 
X(2), Wang W(2), Zhang C(1).

Author information:
(1)Department of Ultrasound, The First Affiliated Hospital of Anhui Medical 
University, Hefei, Anhui, China.
(2)Department of Ultrasound, Huangshan City People's Hospital, Huangshan, 
Anhui, China.
(3)Department of Endocrinology, Huangshan City People's Hospital, Huangshan, 
Anhui, China.
(4)Department of Thyroid, Breast and Hernia Surgery, Huangshan City People's 
Hospital, Huangshan, Anhui, China.

OBJECTIVE: To evaluate the diagnostic performance and clinical utility of 
advanced large language models (LLMs) -GPT-4o, GPT-o3-mini, and DeepSeek-R1- in 
stratifying thyroid nodule malignancy risk and generating guideline-aligned 
management recommendations based on structured narrative ultrasound 
descriptions.
METHODS: This diagnostic modeling study evaluated three LLMs-GPT-4o, 
GPT-o3-mini, and DeepSeek-R1-using standardized narrative ultrasound 
descriptors. These descriptors were annotated by consensus among three senior 
board-certified sonologists and processed independently in a stateless manner to 
ensure unbiased outputs. LLM outputs were assessed under both ACR-TIRADS and 
C-TIRADS frameworks. Two experienced clinicians (a thyroid surgeon and an 
endocrinologist) independently rated the outputs across five clinical dimensions 
using 5-point Likert scales. Primary outcomes included the area under the 
receiver operating characteristic curve (AUC) for malignancy prediction, and 
clinician ratings of guideline adherence, patient safety, operational 
feasibility, clinical applicability, and overall performance.
RESULTS: GPT-4o achieved the highest predictive AUC (0.898) under C-TIRADS, 
approaching expert-level accuracy. DeepSeek-R1, particularly with C-TIRADS, 
received the highest clinician ratings (mean Likert: surgeon 4.65, 
endocrinologist 4.63), reflecting greater trust in its practical 
recommendations. Clinicians consistently favored the C-TIRADS framework across 
all models. GPT-4o and GPT-o3-mini received lower ratings in trustworthiness and 
recommendation quality, especially from the endocrinologist.
CONCLUSION: While GPT-4o demonstrated superior diagnostic accuracy, clinicians 
most trusted DeepSeek-R1 combined with the C-TIRADS framework for generating 
practical, guideline-consistent recommendations. The findings highlight the 
critical need for alignment between AI-generated outputs and clinician 
expectations, and the importance of incorporating region-specific clinical 
guidelines (like C-TIRADS) for the effective real-world implementation of LLMs 
in thyroid nodule management decision support.

Copyright © 2025 Dai, Xi, Hu, Ding, Zhang, Liu, Chen, Wang, Wang and Zhang.

DOI: 10.3389/fendo.2025.1667809
PMCID: PMC12515656
PMID: 41089291 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


930. Eur Radiol. 2025 Nov 14. doi: 10.1007/s00330-025-12102-1. Online ahead of print.

Large language model-based uncertainty-adjusted label extraction for artificial 
intelligence model development in upper extremity radiography.

Kreutzer H(1)(2), Caselitz AS(3)(4), Dratsch T(5), Pinto Dos Santos D(6), Kuhl 
C(3), Truhn D(3)(4), Nebelung S(3)(4).

Author information:
(1)Department of Diagnostic and Interventional Radiology, University Hospital 
Aachen, Aachen, Germany. hannkreutzer@ukaachen.de.
(2)Lab for Artificial Intelligence in Medicine, Department of Diagnostic and 
Interventional Radiology, University Hospital Aachen, Aachen, Germany. 
hannkreutzer@ukaachen.de.
(3)Department of Diagnostic and Interventional Radiology, University Hospital 
Aachen, Aachen, Germany.
(4)Lab for Artificial Intelligence in Medicine, Department of Diagnostic and 
Interventional Radiology, University Hospital Aachen, Aachen, Germany.
(5)Institute for Diagnostic and Interventional Radiology, Faculty of Medicine 
and University Hospital Cologne, University of Cologne, Cologne, Germany.
(6)Department of Diagnostic and Interventional Radiology, University Medical 
Center Mainz, Mainz, Germany.

OBJECTIVES: To evaluate GPT-4o's zero-shot ability to extract structured 
diagnostic labels (with uncertainty) from free-text radiology reports and to 
test how these labels affect multi-label image classification of musculoskeletal 
radiographs.
MATERIALS AND METHODS: This retrospective study included radiography series of 
the clavicle (n = 1170), elbow (n = 3755), and thumb (n = 1978). After 
anonymization, GPT-4o filled out structured templates by indicating imaging 
findings as present ("true"), absent ("false"), or "uncertain." To assess the 
impact of label uncertainty, "uncertain" labels of the training and validation 
sets were automatically reassigned to "true" (inclusive) or "false" (exclusive). 
Label-image pairs were used for multi-label classification using the ResNet50 
architecture. Label extraction accuracy was manually verified on internal 
(clavicle: n = 233, elbow: n = 745, thumb: n = 393) and external test sets 
(n = 300 for each). Performance was assessed using macro-averaged receiver 
operating characteristic (ROC) area under the curve (AUC), precision, recall 
curves, sensitivity, specificity, and accuracy. AUCs were compared with the 
DeLong test.
RESULTS: Automatic extraction was correct in 98.6% (60,618 of 61,488) of labels 
in the test sets. Across anatomic regions, label-based model training yielded 
competitive performance measured by macro-averaged AUC values for inclusive 
(e.g., elbow: AUC = 0.80 (range, 0.62-0.87)) and exclusive models (elbow: 
AUC = 0.80 (range, 0.61-0.88)). Models generalized well on external datasets 
(elbow (inclusive): AUC = 0.79 (range, 0.61-0.87); elbow (exclusive): AUC = 0.79 
(range, 0.63-0.89)). No significant differences were observed across labeling 
strategies or datasets (p ≥ 0.15).
CONCLUSION: GPT-4o extracted labels from radiologic reports to train competitive 
multi-label classification models with high accuracy. Detected uncertainty in 
the radiologic reports did not influence the performance of these models.
KEY POINTS: Question Can GPT-4o automatically extract high-accuracy, 
uncertainty-aware diagnostic labels from routine radiologic reports of the 
clavicle, elbow, and thumb for use in training multi-label image classifiers? 
Findings GPT-4o extracted labels with > 98% accuracy, and multi-label 
classifiers for clavicle, elbow, and thumb radiographs performed consistently 
regardless of how uncertainty was handled. Clinical relevance Automated 
GPT-4o-based labeling of routine clavicle, elbow, and thumb radiologic reports 
enables the rapid conversion of radiologic reports into structured multi-label 
training datasets, supporting scalable development of dedicated image 
classification models.

© 2025. The Author(s).

DOI: 10.1007/s00330-025-12102-1
PMID: 41236656

Conflict of interest statement: Compliance with ethical standards. Guarantor: 
The scientific guarantor of this publication is Sven Nebelung. Conflict of 
interest: D.T. received honoraria for lectures by Bayer, GE, and Philips and 
holds shares in StratifAI GmbH, Germany and in Synagen GmbH, Germany, neither of 
whom have supported or influenced this study. D.P.d.S. is a Deputy Editor for 
European Radiology and, as such, did not participate in the selection or review 
processes for this article. All ethical standards have been strictly adhered to. 
Statistics and biometry: One of the authors has significant statistical 
expertise. Informed consent: Written informed consent was waived by the 
Institutional Review Board. Ethical approval: Institutional Review Board 
approval was obtained. Study subjects or cohorts overlap: No overlap to be 
reported. Methodology: Retrospective Diagnostic/observational Multicenter study


931. Crit Care Med. 2025 Nov 21. doi: 10.1097/CCM.0000000000006969. Online ahead of 
print.

Extending the Observational Medical Outcomes Partnership (OMOP) Common Data 
Model for Critical Care Medicine: A Framework for Standardizing Complex ICU Data 
Using the Society of Critical Care Medicine's Critical Care Data Dictionary 
(C2D2).

Adams MCB(1), Hurley RW(2)(3), Bartels K(4), Perkins ML(5), Hudson C(5), 
Topaloglu U(6)(7), Cobb JP(8), Reuter-Rice K(9), Stocking JC(10), Khanna 
AK(11)(12).

Author information:
(1)Departments of Anesthesiology, Artificial Intelligence, Translational 
Neuroscience, and Public Health Sciences, Wake Forest University School of 
Medicine, Winston-Salem, NC.
(2)Department of Anesthesiology, Translational Neuroscience, and Public Health 
Sciences, Wake Forest University School of Medicine, Winston-Salem, NC.
(3)Pain Outcomes Lab, Wake Forest University Health Sciences, Winston-Salem, NC.
(4)Anesthesiology, Psychiatry, and Learning Health Sciences, University of 
Michigan, Ann Arbor, MI.
(5)Department of Comprehensive Cancer Center, Wake Forest University School of 
Medicine, Winston-Salem, NC.
(6)Department of Cancer Biology, Wake Forest University School of Medicine, 
Winston-Salem, NC.
(7)National Cancer Institute, Bethesda, MD.
(8)Departments of Surgery and of Anesthesiology, Keck School of Medicine of USC, 
Los Angeles, CA.
(9)Departments of Pediatrics and Neurosurgery, Division of Critical Care 
Medicine, Duke University, School of Nursing, School of Medicine, Durham, NC.
(10)Department of Internal Medicine, Division of Pulmonary, Critical Care, and 
Sleep Medicine, University of California Davis Health, Sacramento, CA.
(11)Department of Anesthesiology, Division of Critical Care Medicine, Wake 
Forest School of Medicine, Atrium Health Wake Forest Baptist Medical Center, 
Winston-Salem, NC.
(12)Outcomes Research Consortium, Houston, TX.

OBJECTIVES: To evaluate the compatibility of the Society of Critical Care 
Medicine's (SCCM) Critical Care Data Dictionary (C2D2) with the Observational 
Medical Outcomes Partnership (OMOP) Common Data Model (CDM) and initiate a set 
of steps extending OMOP to accommodate specialized critical care data elements.
DESIGN: Systematic analysis and mapping study using a three-tiered semantic 
matching approach to demonstrate technical feasibility and identify fundamental 
challenges in critical care data standardization.
SETTING: Critical care medicine informatics research environment.
SUBJECTS: The SCCM's C2D2 elements.
INTERVENTIONS: None.
MEASUREMENTS AND MAIN RESULTS: We evaluated the compatibility of C2D2 clinical 
variables with the OMOP CDM using a three-tier classification system (full 
match, partial match, and no match). Our analysis of 226 C2D2 elements revealed 
that 49.6% of concepts had full OMOP equivalents, 46.4% required modification, 
and 4.0% had no suitable mapping. Key incompatibilities were identified in 
ventilator parameters, composite scoring systems, and advanced organ support 
documentation. A large language model-based semantic matching system yielded a 
precision of 59.5%, recall of 87.0%, and F1 score of 70.7% at an optimized 
similarity threshold of 0.90. These findings highlight the need to harmonize 
data standardization approaches within the field of critical care, including how 
to handle concept stacking within single variables, age-specific criteria, and 
specialized constructs that were curated through the SCCM Delphi process, but 
reveal an OMOP mapping incompatibility or missing variables.
CONCLUSIONS: Extending the OMOP CDM for critical care is technically feasible 
and requires targeted modifications to accommodate composite scores, temporal 
precision, and specialized critical care concepts as well as the resources 
needed to support this build. The community acutely faces crucial decisions 
about whether to pursue OMOP integration, adapt the C2D2 for version 2.0 
compatibility, work toward OMOP vocabulary inclusion through Observational 
Health Data Sciences and Informatics processes, or collaborate with electronic 
health record vendors for native critical care standards support. These 
decisions require balancing technical feasibility with long-term sustainability 
and maintenance considerations.

DOI: 10.1097/CCM.0000000000006969
PMID: 41269063

Conflict of interest statement: Dr. Adams’ institution received funding from the 
National Institute on Drug Abuse. Drs. Adams, Hurley, Perkins, Topaloglu, and 
Stocking received support for article research from the National Institutes of 
Health (NIH). Drs. Bartels’ and Perkins’ institutions received funding from the 
NIH. Dr. Hurley received funding from Nevro (research funds to the institution, 
topic-painful diabetic neuropathy and not related to this work) and State Farm 
(expert/consulting not related to this work). Dr. Bartels’ institution received 
funding from the Agency for Healthcare Research and Quality. Dr. Topaloglu 
disclosed government work. Dr. Cobb received funding from Akido Labs and 
Bauhealth; he is a past Workgroup Co-Chair of the Society of Critical Care 
Medicine (SCCM) Discovery Data Science Campaign. Dr. Reuter-Rice’s institution 
received funding from the National Institute of Neurologic Disorders and Stroke 
and Elsevier; she disclosed that she is co-chair of the SCCM Data Science 
Campaign. Dr. Stocking’s institution received funding from the National Heart, 
Lung, and Blood Institute under award number K01HL168222. Dr. Khanna received 
funding from Medtronic, Edwards Lifesciences, GE Healthcare, Philips Research 
North America, Sentinel Medical, Bayer Corporation, AOP, Innoviva Therapeutics, 
and Pharmazz; he disclosed that he is a past chair of the SCCM Discovery Network 
was involved in the Delphi process for the development of the Critical Care Data 
Dictionary; and he is currently a member of the SCCM council. Mr. Hudson has 
disclosed that he does not have any potential conflicts of interest.


932. AJR Am J Roentgenol. 2025 Oct 29. doi: 10.2214/AJR.25.33759. Online ahead of 
print.

Artificial Intelligence for CT and MRI Protocoling: A Meta-Analysis of 
Traditional Machine Learning, BERT, and Large Language Models.

Sacoransky E(1), Chauhan Y(2), Adams SJ(1).

Author information:
(1)Department of Medical Imaging, Royal University Hospital, College of 
Medicine, University of Saskatchewan, Saskatoon, Saskatchewan, Canada.
(2)College of Medicine, University of Saskatchewan, Saskatoon, Saskatchewan, 
Canada.

Comment in
    AJR Am J Roentgenol. 2025 Nov 12. doi: 10.2214/AJR.25.34195.

Background: Examination protocoling is a resource-intensive task. Various 
artificial intelligence (AI) approaches have been investigated to automate this 
process. Objective: To evaluate performance of traditional machine-learning (ML) 
models, bidirectional encoder representations from transformers (BERT) models, 
and large language models (LLMs) for automated CT and MRI protocoling. Evidence 
Acquisition: MEDLINE, Embase, Scopus, Web of Science, IEEE Xplore, and Google 
Scholar databases were searched through July 2025 for studies reporting 
performance of an AI-based technique in assigning protocols for CT or MRI 
requisitions. Accuracy results were separately extracted for all models tested 
in each study and pooled using random-effects meta-analysis. AI approaches were 
compared using Welch t tests. Common sources of error were qualitatively 
summarized. Evidence Synthesis: The final analysis included 23 studies, 
comprising 1,196,259 imaging requisitions. Requisition subspecialties included 
body imaging (n=4), musculoskeletal imaging (n=3), neuroradiology (n=6), 
thoracic imaging (n=1), and multiple subspecialties (n=9). Sixteen studies 
evaluated traditional ML models, eight evaluated BERT models, and five evaluated 
LLMs. Task-specific model fine-tuning was performed in three studies for 
traditional ML models, all studies for BERT models, and one study for LLMs. 
Overall pooled protocoling accuracy was 85% (95% CI: 83-87%). Pooled accuracy 
was 83% (95% CI: 80-85%) for traditional ML models, 87% (95% CI: 85-89%) for 
BERT models, and 86% (95% CI: 83-89%) for LLMs; these pooled accuracies were not 
significantly different between any pairwise combination of the three AI 
approaches (all p > .05). Among 30 distinct models (14 traditional ML models, 
nine BERT models, seven LLMs), the top ten performing models comprised two 
traditional ML models, six BERT models (including the top performing model 
[BioBert; accuracy, 93%]), and two LLMs. Common sources of error included 
ambiguous requisition text, data imbalance yielding incorrect protocol 
assignments for low-volume protocols, presence of multiple clinically reasonable 
protocols for given requisitions, and difficulty handling requisitions 
containing terms strongly associated with disparate protocols. Conclusion: The 
top performing AI models for automated CT and MRI protocoling included 
predominantly finetuned BERT models. Clinical Impact: AI tools show strong 
potential to help streamline radiologist workflows, possibly through hybrid 
AI-radiologist approaches. Fine-tuned LLMs warrant further exploration.

DOI: 10.2214/AJR.25.33759
PMID: 41159787


933. Radiology. 2026 Jan;318(1):e252105. doi: 10.1148/radiol.252105.

Optimizing Large Language Models for Automated Protocoling of Abdominal and 
Pelvic CT Scans: The Power of Context.

Buckley BW(#)(1), Dias AB(#)(1), Deng Y(2), Schmidt H(1), Kielar A(1), Krishna 
S(#)(1), Bhayana R(#)(1).

Author information:
(1)University Medical Imaging Toronto, Joint Department of Medical Imaging, 
University Health Network, Mount Sinai Hospital and Women's College Hospital, 
Department of Medical Imaging, University of Toronto, Toronto General Hospital, 
200 Elizabeth St, Peter Munk Building, 1st Fl, Toronto, ON, Canada M5G 24C.
(2)Department of Biostatistics, University Health Network, Toronto, Canada.
(#)Contributed equally

Background Accurate protocoling is critical for imaging accuracy. Manual 
protocoling is time-consuming and error prone. Purpose To evaluate the 
performance of large language models (LLMs) in automatically assigning protocols 
for abdominal and pelvic CT scans after optimization with context engineering 
and fine-tuning and to compare performance with that of radiologists in 
practice. Materials and Methods This retrospective study included patients with 
abdominal or pelvic CT scans obtained between January 2024 and June 2024. 
Requisition data, human-selected protocol, and training level (resident, fellow, 
or radiologist) were extracted. Reference standard protocols were defined by 
radiologists in consultation with institutional guidelines. Context engineering 
involved detailed prompt instructions using a prompt set with GPT-4o (version 
2024-08-06; Open AI). A subset of patients was reserved for fine-tuning 
(training set and validation set) and another for testing (internal test set). 
Two models were tested (prompting-only and fine-tuned). Model-selected protocols 
and original human-selected protocols were categorized compared with the 
reference standard after review by blinded radiologists as follows: exact match, 
equal alternative, reasonable but inferior, or inappropriate. Exact match and 
equal alternative were considered optimal. Performance of models and 
radiologists were compared using the McNemar test. Results This study included 
1448 patients (mean age, 61 years ± 17 [SD]; 728 female patients). GPT-4o with 
prompting only selected optimal protocols more frequently than humans (96.2% 
[527 of 548 patients] vs 88.3% [484 of 548 patients]; P < .001), but there was 
no evidence of a difference in inappropriate protocols (1.3% [seven of 548 
patients] vs 2.4% [13 of 548 patients]; P = .21). Fine-tuning GPT-4o did not 
improve the proportion of optimal protocols over prompting only (96.2% [527 of 
548 patients] vs 96.2% [527 of 548 patients]; P > .99). In subgroup analyses, 
the proportion of protocols matching the reference standard was similar among 
radiologists (79.4% [173 of 218 patients]), fellows (74.9% [164 of 219 
patients]), and residents (72.1% [80 of 111 patients]; P = .30). Conclusion For 
protocoling abdominal and pelvic CT scans, the LLM, GPT-4o, selected optimal 
protocols more frequently than radiologists when optimized with detailed 
prompting, and fine-tuning of the model did not further improve performance. © 
RSNA, 2026 Supplemental material is available for this article.

DOI: 10.1148/radiol.252105
PMID: 41493275 [Indexed for MEDLINE]


934. JAMA Netw Open. 2025 Oct 1;8(10):e2539267. doi: 
10.1001/jamanetworkopen.2025.39267.

Syndromic Analysis of Sepsis Cohorts Using Large Language Models.

Pak TR(1)(2), Kanjilal S(1)(3), McKenna CS(1), Hoffner-Heinike A(4), Rhee 
C(1)(3), Klompas M(1)(3).

Author information:
(1)Department of Population Medicine, Harvard Medical School and Harvard Pilgrim 
Health Care Institute, Boston, Massachusetts.
(2)Division of Infectious Diseases, Department of Medicine, Massachusetts 
General Hospital, Boston.
(3)Division of Infectious Diseases, Department of Medicine, Brigham and Women's 
Hospital, Boston, Massachusetts.
(4)Department of Medicine, Massachusetts General Hospital, Boston.

Comment in
    doi: 10.1001/jamanetworkopen.2025.39275.

IMPORTANCE: Presenting signs and symptoms affect the care of patients with 
possible sepsis. However, signs and symptoms are not incorporated into most 
large observational studies because they are difficult to extract from clinical 
notes at scale.
OBJECTIVE: To assess the use of large language models (LLMs) to extract 
presenting signs and symptoms from admission notes and characterize their 
associations with infectious diagnoses, multidrug-resistant infections, and 
mortality.
DESIGN, SETTING, AND PARTICIPANTS: This retrospective cohort study obtained data 
from 5 Massachusetts hospitals within 1 health care system between June 1, 2015, 
and August 1, 2022. Participants were hospitalized adult patients with possible 
infection (determined by blood culture drawn and intravenous antibiotics 
administered within 24 hours of arrival). An LLM (LLaMA 3 8B; Meta) was used to 
extract up to 10 presenting signs and symptoms from each patient's 
history-and-physical admission notes. LLM-generated labels were validated by 
blinded review of 303 random admission notes. Data analyses were performed from 
July 2023 to August 2025.
EXPOSURES: Thirty most common signs and symptoms were retained as exposures, and 
unsupervised clustering was used to create syndromes, which were compared with 
infection sources derived from the International Statistical Classification of 
Diseases, Tenth Revision, Clinical Modification discharge codes.
MAIN OUTCOMES AND MEASURES: Outcomes included positive cultures for 
methicillin-resistant Staphylococcus aureus (MRSA), positive cultures for 
multidrug-resistant gram-negative (MDRGN) organisms, and in-hospital mortality. 
Multivariable logistic regression was used to adjust for demographics, 
comorbidities, physiologic markers of severity of illness, and time to 
antibiotics.
RESULTS: Among the 104 248 patients (median [IQR] age, 66 [52-78] years; 54 137 
males [51.9%]) included, 23 619 (22.7%) had sepsis without shock, 25 990 (24.9%) 
had septic shock, and 94 913 (91.0%) had 1 or more admission note within 24 
hours. The LLM labeled the notes of 93 674 of 94 913 patients (98.7%). On manual 
validation, LLM labels had an accuracy of 99.3% (95% CI, 99.2%-99.3%), balanced 
accuracy of 84.6% (95% CI, 83.5%-85.8%), positive predictive value of 68.4% (95% 
CI, 66.0%-70.7%), sensitivity of 69.7% (95% CI, 67.3%-72.0%), and specificity of 
99.6% (95% CI, 99.6%-99.6%) compared with the physician medical record reviewer. 
The 30 most common signs and symptoms were clustered into syndromes that 
correlated with infection sources. Presence of skin and soft tissue symptoms 
(adjusted odds ratio [AOR], 1.73; 95% CI, 1.49-2.00) and absence of 
gastrointestinal (AOR, 0.63; 95% CI, 0.54-0.73) or urinary tract symptoms (AOR, 
0.34; 95% CI, 0.22-0.50) were associated with MRSA culture positivity; inverse 
associations were seen for MDRGN organisms. Cardiopulmonary symptoms were 
associated with increased mortality (AOR, 1.30; 95% CI, 1.17-1.45).
CONCLUSIONS AND RELEVANCE: This cohort study found that an LLM accurately 
extracted presenting signs and symptoms from admission notes that clustered into 
syndromes differentially correlated with infection sources, multidrug-resistant 
infections, and mortality. Further research is warranted to evaluate the value 
of large-scale sign-and-symptom data in models of antibiotic choice, 
effectiveness, and outcomes in patients with possible sepsis.

DOI: 10.1001/jamanetworkopen.2025.39267
PMCID: PMC12552932
PMID: 41134571 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest Disclosures: Dr Pak 
reported receiving grants from the National Institute of Allergy and Infectious 
Diseases (NIAID) and the Agency for Healthcare Research and Quality (AHRQ) 
during the conduct of the study. Dr Rhee reported receiving grants from the 
Centers for Disease Control and Prevention (CDC) and the AHRQ during the conduct 
of the study and personal fees from UpToDate Inc outside the submitted work. Dr 
Klompas reported receiving grants from the AHRQ during the conduct of the study, 
grants from the CDC and the AHRQ outside the submitted work, and personal fees 
from UpToDate outside the submitted work. No other disclosures were reported.


935. J Clin Endocrinol Metab. 2025 Feb 18;110(3):e841-e850. doi: 
10.1210/clinem/dgae235.

Comparing ChatGPT's and Surgeon's Responses to Thyroid-related Questions From 
Patients.

Guo S(1), Li R(2), Li G(1), Chen W(1), Huang J(1), He L(1), Ma Y(1), Wang L(1), 
Zheng H(3), Tian C(4), Zhao Y(5), Pan X(6), Wan H(7), Liu D(8), Li Z(1), Lei 
J(1).

Author information:
(1)Division of Thyroid Surgery, Department of General Surgery, West China 
Hospital, Sichuan University, Chengdu, Sichuan 610041, China.
(2)Health Management Center, General Practice Medical Center, West China 
Hospital, Sichuan University, Chengdu, Sichuan 610041, China.
(3)Department of Thyroid Surgery, General Surgery Ward 7, The First Hospital of 
Lanzhou University, Lanzhou, Gansu 730000, China.
(4)Chengdu Women's and Children's Central Hospital, School of Medicine, 
University of Electronic Science and Technology of China, Chengdu, Sichuan 
610031, China.
(5)Thyroid Surgery, Zhengzhou Central Hospital Affiliated of Zhengzhou 
University, Zhengzhou, Henan 450007, China.
(6)Department of Thyroid Surgery, General Surgery III, Gansu Provincial 
Hospital, Lanzhou, Gansu 730000, China.
(7)Department of Oncology, Sanya People's Hospital, Sanya, Hainan 572000, China.
(8)Department of Vascular Thyroid Surgery, The Second Affiliated Hospital of 
Guangzhou University of Chinese Medicine, Guangzhou, Guangdong 510120, China.

CONTEXT: For some common thyroid-related conditions with high prevalence and 
long follow-up times, ChatGPT can be used to respond to common thyroid-related 
questions.
OBJECTIVE: In this cross-sectional study, we assessed the ability of ChatGPT 
(version GPT-4.0) to provide accurate, comprehensive, compassionate, and 
satisfactory responses to common thyroid-related questions.
METHODS: First, we obtained 28 thyroid-related questions from the Huayitong app, 
which together with the 2 interfering questions eventually formed 30 questions. 
Then, these questions were responded to by ChatGPT (on July 19, 2023), a junior 
specialist, and a senior specialist (on July 20, 2023) separately. Finally, 26 
patients and 11 thyroid surgeons evaluated those responses on 4 dimensions: 
accuracy, comprehensiveness, compassion, and satisfaction.
RESULTS: Among the 30 questions and responses, ChatGPT's speed of response was 
faster than that of the junior specialist (8.69 [7.53-9.48] vs 4.33 [4.05-4.60]; 
P < .001) and the senior specialist (8.69 [7.53-9.48] vs 4.22 [3.36-4.76]; P < 
.001). The word count of the ChatGPT's responses was greater than that of both 
the junior specialist (341.50 [301.00-384.25] vs 74.50 [51.75-84.75]; P < .001) 
and senior specialist (341.50 [301.00-384.25] vs 104.00 [63.75-177.75]; P < 
.001). ChatGPT received higher scores than the junior specialist and senior 
specialist in terms of accuracy, comprehensiveness, compassion, and satisfaction 
in responding to common thyroid-related questions.
CONCLUSION: ChatGPT performed better than a junior specialist and senior 
specialist in answering common thyroid-related questions, but further research 
is needed to validate the logical ability of the ChatGPT for complex thyroid 
questions.

© The Author(s) 2024. Published by Oxford University Press on behalf of the 
Endocrine Society. All rights reserved. For commercial re-use, please contact 
reprints@oup.com for reprints and translation rights for reprints. All other 
permissions can be obtained through our RightsLink service via the Permissions 
link on the article page on our site—for further information please contact 
journals.permissions@oup.com.

DOI: 10.1210/clinem/dgae235
PMID: 38597169 [Indexed for MEDLINE]


936. Radiology. 2025 Mar;314(3):e241646. doi: 10.1148/radiol.241646.

Value of Using a Generative AI Model in Chest Radiography Reporting: A Reader 
Study.

Hong EK(1)(2), Roh B(3), Park B(3), Jo JB(4), Bae W(4), Soung Park J(5), Sung 
DW(6).

Author information:
(1)Department of Radiology, Mass General Brigham, Boston, Mass.
(2)Department of Radiology, Brigham & Women's Hospital, 75 Francis St, Boston, 
MA 02115.
(3)Kakaocorp, Seoul, South Korea.
(4)Soombit.ai, Seoul, South Korea.
(5)Department of Radiology, Soonchunhyang University College of Medicine, 
Cheonan, South Korea.
(6)Department of Radiology, Kyung Hee University School of Medicine, Seoul, 
South Korea.

Background Multimodal generative artificial intelligence (AI) technologies can 
produce preliminary radiology reports, and validation with reader studies is 
crucial for understanding the clinical value of these technologies. Purpose To 
assess the clinical value of the use of a domain-specific multimodal generative 
AI tool for chest radiograph interpretation by means of a reader study. 
Materials and Methods A retrospective, sequential, multireader, multicase reader 
study was conducted using 758 chest radiographs from a publicly available 
dataset from 2009 to 2017. Five radiologists interpreted the chest radiographs 
in two sessions: without AI-generated reports and with AI-generated reports as 
preliminary reports. Reading times, reporting agreement (RADPEER), and quality 
scores (five-point scale) were evaluated by two experienced thoracic 
radiologists and compared between the first and second sessions from October to 
December 2023. Reading times, report agreement, and quality scores were analyzed 
using a generalized linear mixed model. Additionally, a subset of 258 chest 
radiographs was used to assess the factual correctness of the reports, and 
sensitivities and specificities were compared between the reports from the first 
and second sessions with use of the McNemar test. Results The introduction of 
AI-generated reports significantly reduced average reading times from 34.2 
seconds ± 20.4 to 19.8 seconds ± 12.5 (P < .001). Report agreement scores 
shifted from a median of 5.0 (IQR, 4.0-5.0) without AI reports to 5.0 (IQR, 
4.5-5.0) with AI reports (P < .001). Report quality scores changed from 4.5 
(IQR, 4.0-5.0) without AI reports to 4.5 (IQR, 4.5-5.0) with AI reports (P < 
.001). From the subset analysis of factual correctness, the sensitivity for 
detecting various abnormalities increased significantly, including widened 
mediastinal silhouettes (84.3% to 90.8%; P < .001) and pleural lesions (77.7% to 
87.4%; P < .001). While the overall diagnostic performance improved, variability 
among individual radiologists was noted. Conclusion The use of a domain-specific 
multimodal generative AI model increased the efficiency and quality of radiology 
report generation. © RSNA, 2025 Supplemental material is available for this 
article. See also the editorial by Babyn and Adams in this issue.

DOI: 10.1148/radiol.241646
PMID: 40067108 [Indexed for MEDLINE]


937. World J Gastroenterol. 2025 Oct 21;31(39):111495. doi: 
10.3748/wjg.v31.i39.111495.

Emerging role of artificial intelligence in gastroenterology and hepatology.

Shrestha UK(1).

Author information:
(1)Department of Gastroenterology and Hepatology, Nepal Mediciti Hospital, 
Lalitpur 44700, Bagmati, Nepal. umidshrestha@gmail.com.

Artificial intelligence (AI) has emerged as a transformative tool in the 
diagnosis and management of gastrointestinal (GI) and liver diseases. In 
clinical practice AI consists of overlapping technologies such as machine 
learning (ML), deep learning, natural language processing, computer vision, and 
generative AI. ML is a computer learning system that can provide insight into 
disease risk factors and phenotypes. Deep learning is an advanced and complex 
form of ML, structured with different levels of specific algorithms known as 
convolutional neural networks that can rapidly and accurately process 
unstructured, high-dimensional data, such as texts, images, and waveforms. 
Natural language processing is dedicated to facilitating interactions between 
computers and humans using natural language and helps to analyze, understand, 
and derive actionable information from unstructured healthcare data, including 
electronic health records, clinical notes, medical literature, and 
patient-generated content. Computer vision focuses on enabling computers to see 
and interpret images and videos and serves as an augmentation tool for 
endoscopists, improving accuracy and decreasing procedural time. Generative AI 
is capable of creating new forms of content by learning from a large body of 
data in the form of text, audio, images, or video and includes large language 
models. AI has been used in several GI diseases such as esophageal neoplasia, 
gastric cancer, Helicobacter pylori infection, gastritis, GI stromal tumors, 
colorectal polyps, inflammatory bowel disease, irritable bowel syndrome, GI 
bleeding, and pancreatobiliary diseases. The potential applications of AI in 
liver diseases encompass a variety of conditions such as liver masses, metabolic 
dysfunction-associated steatotic liver disease, viral hepatitis, cirrhosis, and 
liver transplantation. This review discussed the common terminologies and the 
current status of AI in gastroenterology and hepatology, exploring its 
applications and ethical issues.

©The Author(s) 2025. Published by Baishideng Publishing Group Inc. All rights 
reserved.

DOI: 10.3748/wjg.v31.i39.111495
PMCID: PMC12576571
PMID: 41180786 [Indexed for MEDLINE]

Conflict of interest statement: Conflict-of-interest statement: The authors 
declare that they have no conflicts of interest.


938. Surgery. 2026 Feb;190:109849. doi: 10.1016/j.surg.2025.109849. Epub 2025 Nov 3.

Position statement from the society of University surgeons, surgical education 
committee: Artificial intelligence in surgical training for medical students, 
residents, and fellows.

Kewalramani D(1), Jawa RS(2), Martin CA(3), Gosain A(4), Wan D(5), Varghese TK 
Jr(6), Boland GM(7), Dream SY(8), Sifri Z(9), Mollen K(10), Chu D(10), Narayan 
M(11).

Author information:
(1)Department of Surgery, Rutgers Robert Wood Johnson Medical School, New 
Brunswick, NJ. Electronic address: https://twitter.com/divvya20k.
(2)Department of Surgery, Stony Brook Medicine, Stony Brook, NY.
(3)Department of Surgery, Washington University School of Medicine, St. Louis, 
MO.
(4)Department of Surgery, University of Virginia School of Medicine, 
Charlottesville, VA.
(5)Department of Surgery, Stanford University School of Medicine, Stanford, CA.
(6)Department of Surgery, University of Utah, Salt Lake City, UT.
(7)Department of Surgery, Mass General Brigham, Boston, MA.
(8)Department of Surgery, University of Alabama at Birmingham, Birmingham, AL.
(9)Department of Surgery, New Jersey Medical School, Newark, NJ.
(10)Department of Surgery, University of Pittsburgh, Pittsburgh, PA.
(11)Department of Surgery, Rutgers Robert Wood Johnson Medical School, New 
Brunswick, NJ. Electronic address: mayur.narayan@rutgers.edu.

OBJECTIVE: To provide guidance from the Society of University of Surgeons' 
Surgical Education Committee on the responsible integration of artificial 
intelligence into surgical training.
METHODS: Members of the Education Committee reviewed current applications of 
artificial intelligence in surgical training and convened expert discussions to 
frame opportunities, risks, and implementation strategies. The Committee 
emphasized narrow artificial intelligence (eg, computer vision, predictive 
analytics) and generative artificial intelligence (eg, large language models), 
clarifying their definitions and describing their applications to surgical 
education. Stakeholder-specific recommendations were developed for accrediting 
bodies, professional societies, institutions, and individual educators.
RESULTS: Although artificial intelligence offers the potential to enhance 
learner engagement, reduce training variability, and improve global access to 
high-quality education, its integration raises critical challenges related to 
academic integrity, redefining faculty roles, adapting assessment methods, and 
ensuring patient safety. This position statement from the Society of University 
of Surgeons provides a framework for responsibly incorporating artificial 
intelligence from policy-level decisions and national standard-setting, 
institutional infrastructure and ethical oversight, down to practical day-to-day 
mentorship by surgeon educators. We emphasize foundational competencies such as 
prompt engineering and artificial intelligence literacy, clinical and 
educational applications, ethical considerations in research, robust skill 
evaluation methods, institutional accountability, and global equity, 
particularly in low-resource settings.
CONCLUSION: Artificial intelligence has the potential to be a powerful adjunct 
to surgical education if deployed thoughtfully and ethically. This position 
statement outlines a phased, stakeholder-specific framework to ensure artificial 
intelligence complements, rather than replaces the vital educator-learner 
relationship, while fostering safer, more reflective, and globally connected 
surgeons.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.surg.2025.109849
PMID: 41187419 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest/Disclosure The authors have 
no relevant financial disclosures.


939. Front Neurol. 2025 Jul 24;16:1603536. doi: 10.3389/fneur.2025.1603536. 
eCollection 2025.

GPT-based prediction of short-term survival following decompressive 
hemicraniectomy in malignant middle cerebral artery infarction.

Lehmann S(1), Vychopen M(1), Güresir E(1), Wach J(1).

Author information:
(1)Department of Neurosurgery, University Hospital Leipzig, Leipzig, Germany.

INTRODUCTION: An analysis of the prognostic ability of the large language model 
(LLM) Generative Pre-trained Transformer (GPT) to predict short-term survival 
and functional outcomes in patients with malignant middle cerebral artery (MCA) 
infarction following decompressive hemicraniectomy.
METHODS: This retrospective study included 100 patients with malignant MCA 
infarction who underwent decompressive craniectomy (DC). GPT-4 and GPT-4 Omni 
were used to predict patient outcomes based on 20 patient-specific factors. Each 
version of GPT was tested with and without context enrichment (CE). CE versions 
were provided with the current AHA/ASA 2019 guidelines and meta-analyses of RCTs 
to inform decision-making. The real-life outcome of the patients, measured by 
the modified Rankin Scale (mRS), served as a reference. The following endpoints 
were evaluated: survival during inpatient stay, achievement of a functional 
status of mRS 0-4 at discharge, and at 3-, 6-, and 12-months post-discharge. We 
analyzed the prognostic prediction of GPT by calculating the area under the 
curve (AUC) and determining the optimal cutoff using the Youden index for 
divergent prediction outcomes. After dichotomization according to the cutoff 
set, a chi-squared test (two-sided) was performed.
RESULTS: GPT-4 and GPT-4 Omni demonstrated the ability to estimate survival 
during in-hospital stay. In both versions, the CE GPT outperformed the non-CE 
versions. GPT-4 Omni (CE) achieved an AUC of 0.67 (95% CI: 0.54-0.79; p = 
0.002), while GPT-4 (CE) reached an AUC of 0.70 (95% CI: 0.57-0.82; p = 0.018). 
GPT-4 also achieved statistical significance even without CE (AUC of 0.66; 95% 
CI: 0.53-0.78; p = 0.018). In contrast, the non-CE version of GPT-4 Omni did not 
reach significance in predicting the survival of hospitalization (AUC of 0.60; 
95% CI: 0.48-0.73; p = 0.07). For questions regarding the functional outcome of 
patients, neither version of GPT was able to make a sufficient prognostic 
prediction. However, when provided with the pre-stroke mRS, GPT-4 Omni was able 
to predict the mRS at discharge (p = 0.01; Pearson's correlation coefficient = 
0.696).
CONCLUSION: The study shows the already existing high potential of AI in 
predicting short-term outcomes. It also shows the existing limitations for the 
evaluation of more complex questions, such as functional outcomes.

Copyright © 2025 Lehmann, Vychopen, Güresir and Wach.

DOI: 10.3389/fneur.2025.1603536
PMCID: PMC12329377
PMID: 40777857

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


940. Phys Med Biol. 2025 Jul 24;70(15). doi: 10.1088/1361-6560/adf02c.

Automated radiotherapy treatment planning guided by GPT-4Vision.

Liu S(1)(2), Pastor-Serrano O(1), Chen Y(1), Gopaulchan M(1), Liang W(3), 
Buyyounouski M(1), Pollom E(1), Le QT(1), Gensheimer M(1), Dong P(1), Yang Y(1), 
Zou J(2)(3), Xing L(1).

Author information:
(1)Department of Radiation Oncology, Stanford University, Stanford, CA, United 
States of America.
(2)Department of Biomedical Data Science, Stanford University, Stanford, CA, 
United States of America.
(3)Department of Computer Science, Stanford University, Stanford, CA, United 
States of America.

Update of
    ArXiv. 2025 Apr 8:arXiv:2406.15609v3.

Objective. Radiotherapy treatment planning is a time-consuming and potentially 
subjective process that requires iterative adjustment of model parameters to 
balance multiple conflicting objectives. Recent advancements in frontier 
artificial intelligence (AI) models offer promising avenues for addressing the 
challenges in planning and clinical decision-making. This study introduces 
GPT-RadPlan, an automated treatment planning framework that integrates radiation 
oncology knowledge with the reasoning capabilities of large multi-modal models, 
such as GPT-4Vision (GPT-4 V) from OpenAI.Approach. Via in-context learning, we 
incorporate clinical requirements and three approved clinical plans along with 
their optimization settings, enabling GPT-4 V to acquire treatment planning 
domain knowledge. The resulting GPT-RadPlan system is integrated into our 
in-house inverse treatment planning system through an application programming 
interface. For a given patient, GPT-RadPlan acts as both plan evaluator and 
planner, first assessing dose distributions and dose-volume histograms, and then 
providing 'textual feedback' on how to improve the plan to match the physician's 
requirements. In this manner, GPT-RadPlan iteratively refines the plan by 
adjusting planning parameters, such as weights and dose objectives, based on its 
suggestions.Main results. The efficacy of the automated planning system is 
showcased across 17 prostate cancer and 13 head & neck cancer VMAT plans with 
prescribed doses of 70.2 Gy and 72 Gy, respectively, where we compared 
GPT-RadPlan results to clinical plans produced by human experts. In all cases, 
GPT-RadPlan either outperformed or matched the clinical plans, demonstrating 
superior target coverage and reducing organ-at-risk doses by 5 Gy on average 
(15% for prostate and 10%-15% for head & neck).Significance. Consistently 
satisfying the the dose-volume objectives in the clinical protocol, GPT-RadPlan 
represents the first multimodal large language model agent that mimics the 
behaviors of human planners in radiation oncology clinics, achieving promising 
results in automating the treatment planning process without the need for 
additional training.

© 2025 Institute of Physics and Engineering in Medicine. All rights, including 
for text and data mining, AI training, and similar technologies, are reserved.

DOI: 10.1088/1361-6560/adf02c
PMID: 40664228 [Indexed for MEDLINE]


941. Front Endocrinol (Lausanne). 2025 Nov 19;16:1620132. doi: 
10.3389/fendo.2025.1620132. eCollection 2025.

Artificial intelligence in diabetes care: from predictive analytics to 
generative AI and implementation challenges.

Deng M(#)(1), Yang R(#)(1), Zheng X(2), Deng Y(3), Jiang J(4).

Author information:
(1)Department of Gynecological Oncology, Beijing Obstetrics and Gynecology 
Hospital, Capital Medical University, Beijing Maternal and Child Health Care 
Hospital, Beijing, China.
(2)Department of Gynecology and Obstetrics, Handan Fukang Hospital, Handan, 
Hebei, China.
(3)Department of Educational Management, Nanchang University, Nanchang, 
Jiangxi, China.
(4)State Key Laboratory of Medical Proteomics, National Center for Protein 
Sciences, Institute of Lifeomics, Beijing, China.
(#)Contributed equally

Generative artificial intelligence (GenAI) is transforming public health and 
medicine as well, in the form of disease surveillance, resource allocation and 
clinical decision making. Interventions to improve efficiency - multimodal 
predictive algorithms, federated learning platforms - reveal the internal 
contradictions of the system between algorithmic efficiency and fairness: speed 
of technical innovation and regulatory deficit, data flows without borders vs. 
ethical values of places. We present a three-dimensional governance structure 
for the topic covering the technical, institutional and ethical domains. From a 
technology point of view, explainability solutions and culturally-aware design 
align transparency with cultural sensibility. From an institution point of view, 
privacy-protecting data platforms and risk-based regulation align innovation 
with accountability. From an ethical point of view, incorporating local values 
and disbursing AI dividends sustain equitable health outcomes. There are still 
challenges that demand the utmost priority, including the algorithmic prejudice, 
the data imperialism and the opacity in medical AI decision making. Future 
priorities include the development of broader measurement tools that integrate 
clinical impact, equity, and societal impact; the development of transnational 
governance institutions to mitigate concerns relating to data sovereignty; and 
the development of forms of participatory design between designers, 
practitioners, and populations. A balance between technical creativity, 
visionary policy-making, and caring leadership to advocate for human-centered 
healthcare will provide us with trusted AI ecosystems. Technical excellence 
alone cannot guarantee success unless fairness and accessibility, social 
responsiveness, and justice for future global health is guaranteed.

Copyright © 2025 Deng, Yang, Zheng, Deng and Jiang.

DOI: 10.3389/fendo.2025.1620132
PMCID: PMC12672236
PMID: 41347125 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


942. Front Psychiatry. 2025 Oct 2;16:1675098. doi: 10.3389/fpsyt.2025.1675098. 
eCollection 2025.

The use of the social robot NAO in medical settings: how to facilitate 
interactions between healthcare professionals and patients with autism spectrum 
disorder.

Biagi F(1), Iani C(2), Biagiotti L(1).

Author information:
(1)Department of Engineering "Enzo Ferrari", University of Modena and Reggio 
Emilia, Modena, Italy.
(2)Department of Surgery, Medicine, Dentistry and Morphological Sciences, 
University of Modena and Reggio Emilia, Reggio Emilia, Italy.

OBJECTIVES: This study investigates how to facilitate the use of the social 
robot NAO in medical settings to support interactions with children diagnosed 
with Autism Spectrum Disorder (ASD). The objective was to develop intuitive 
control methods that enable healthcare professionals to easily integrate the 
robot into clinical practice.
METHODS: Two control modes were designed: Puppet mode, where clinicians manually 
operate the robot via a graphical console, and Assistant mode, where a Large 
Language Model translates clinicians' spoken requests into robot actions and 
dialogue. Twenty-three doctors evaluated both modes through video demonstrations 
and completed questionnaires assessing usability, usefulness, and ethical 
acceptability.
RESULTS: Both modes were considered effective and user-friendly. Assistant mode 
was perceived as more intuitive and adaptable, facilitating seamless 
interaction, whereas Puppet mode was judged slightly more reassuring for 
patients and somewhat more appropriate in terms of robot actions.
CONCLUSION: Overall, both approaches were positively received, with Assistant 
mode emerging as the preferred option for integration into clinical workflows 
due to its perceived simplicity and flexibility. These findings highlight 
clinicians' positive perceptions of two novel control modes and emphasize NAO's 
potential to enhance patient engagement and reduce stress. Further empirical 
validation with children in real clinical trials is warranted to confirm these 
benefits and optimize robot-assisted interventions in ASD care.

Copyright © 2025 Biagi, Iani and Biagiotti.

DOI: 10.3389/fpsyt.2025.1675098
PMCID: PMC12528083
PMID: 41113200

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest. The reviewer EL declared a 
shared parent affiliation with the authors to the handling editor at the time of 
review.


943. Radiology. 2025 Feb;314(2):e240650. doi: 10.1148/radiol.240650.

AI Applications for Thoracic Imaging: Considerations for Best Practice.

Hwang EJ(1), Goo JM(1), Park CM(1).

Author information:
(1)Department of Radiology, Seoul National University Hospital and Seoul 
National University College of Medicine, 101 Daehak-ro, Jongno-gu, Seoul 03080, 
Korea.

Artificial intelligence (AI) technology is rapidly being introduced into 
thoracic radiology practice. Current representative use cases for AI in thoracic 
imaging show cumulative evidence of effectiveness. These include AI assistance 
for reading chest radiographs and low-dose (1.5-mSv) chest CT scans for lung 
cancer screening and triaging pulmonary embolism on chest CT scans. Other 
potential use cases are also under investigation, including filtering out normal 
chest radiographs, monitoring reading errors, and automated opportunistic 
screening of nontarget diseases. However, implementing AI tools in daily 
practice requires establishing practical strategies. Practical AI implementation 
will require objective on-site performance evaluation, institutional information 
technology infrastructure integration, and postdeployment monitoring. Meanwhile, 
the remaining challenges of adopting AI technology need to be addressed. These 
challenges include educating radiologists and radiology trainees, alleviating 
liability risk, and addressing potential disparities due to the uneven 
distribution of data and AI technology. Finally, next-generation AI technology 
represented by large language models (LLMs), including multimodal models, which 
can interpret both text and images, is expected to innovate the current 
landscape of AI in thoracic radiology practice. These LLMs offer opportunities 
ranging from generating text reports from images to explaining examination 
results to patients. However, these models require more research into their 
feasibility and efficacy.

© RSNA, 2025 See also the editorial by van Beek in this issue.

DOI: 10.1148/radiol.240650
PMID: 39998373 [Indexed for MEDLINE]


944. Radiology. 2025 Jul;316(1):e243378. doi: 10.1148/radiol.243378.

Drafting the Future: The Dawn of AI Report Generation in Radiology.

Seah JCY(1)(2)(3), Tang JSN(3)(4), Tran A(3).

Author information:
(1)Division of Neuroradiology, Massachusetts General Hospital, Boston, Mass.
(2)Department of Radiology, Alfred Health, Melbourne, Victoria, Australia.
(3)Harrison.ai, Level 1, 24 Campbell St, Sydney, NSW 2000, Australia.
(4)Division of Thoracic and Cardiovascular Imaging, Massachusetts General 
Hospital, Boston, Mass.

Radiology faces a global shortage of radiologists. This shortage, particularly 
in the United States, affects timely diagnosis and exacerbates burnout due to 
escalating workloads. Because training new radiologists and developing the 
workforce takes a substantial amount of time, there is growing interest in using 
artificial intelligence (AI) as a key solution to help meet demand and improve 
efficiency. Recent years have seen the development of comprehensive AI, where a 
single algorithm is trained with multitask learning to classify and detect 
multiple abnormalities on images from high-volume modalities such as radiography 
and CT. Further, the parallel and rapid development of capability in large 
language models (LLMs) intersects with comprehensive AI, enabling the production 
of accurate and human-like draft reports. Multimodal LLMs are capable of 
accepting images as well as text as inputs, giving them the ability to generate 
a draft report directly from medical images; this marks a major step forward in 
AI application in radiology. These AI-produced reports can dramatically enhance 
efficiency, scaling the net capacity of the current workforce. The authors 
envision a future in which AI is used to fully automate reporting for 
high-volume modalities in routine clinical settings, and they discuss evaluation 
frameworks for safe and effective integration of these AI report generation 
tools into clinical workflows.

© RSNA, 2025.

DOI: 10.1148/radiol.243378
PMID: 40728403 [Indexed for MEDLINE]


945. J Thorac Cardiovasc Surg. 2025 Sep;170(3):793-801. doi: 
10.1016/j.jtcvs.2024.09.014. Epub 2024 Sep 14.

SciScribe: Automating and contextualizing literature reviews in cardiac surgery.

Mahboubi R(1), Dinkla K(2), Weiss A(1), Acierto A(3), Staar P(2), Robinson J(1), 
Hammoud MS(1), Karamlou T(4).

Author information:
(1)Department of Thoracic and Cardiovascular Surgery, Heart and Vascular 
Institute, Cleveland Clinic, Cleveland, Ohio.
(2)IBM Research, Zürich, Switzerland.
(3)Syracuse University, Syracuse, NY.
(4)Department of Thoracic and Cardiovascular Surgery, Heart and Vascular 
Institute, Cleveland Clinic, Cleveland, Ohio. Electronic address: 
karamlt@ccf.org.

BACKGROUND: The task of writing structured content reviews and guidelines has 
grown stronger and more complex. We propose to go beyond search tools and toward 
curation tools by automating time-consuming and repetitive steps of extracting 
and organizing information.
METHODS: SciScribe is built as an extension of IBM's Deep Search platform, which 
provides document processing and search capabilities. This platform was used to 
ingest and search full-content publications from PubMed Central (PMC) and 
official, structured records from the ClinicalTrials and OpenPayments databases. 
Author names and NCT numbers, mentioned within the publications, were used to 
link publications to these official records as context. Search strategies 
involve traditional keyword-based search as well as natural language question 
and answering via large language models (LLMs).
RESULTS: SciScribe is a web-based tool that helps accelerate literature reviews 
through key features: (1) accumulating a personal collection from publication 
sources, such as PMC or other sources; (2) incorporating contextual information 
from external databases into the presented papers, promoting a more informed 
assessment by readers; (3) semantic questioning and answering of documents to 
quickly assess relevance and hierarchical organization; and (4) semantic 
question answering for each document within a collection, collated into tables.
CONCLUSIONS: Emergent language processing techniques are opening new avenues to 
accelerate and enhance the literature review process, for which we have 
demonstrated a use case implementation in cardiac surgery. SciScribe automates 
and accelerates this process, mitigates errors associated with repetition and 
fatigue, and contextualizes results by linking relevant external data sources 
instantaneously.

Copyright © 2024 The American Association for Thoracic Surgery. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jtcvs.2024.09.014
PMID: 39278616 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest Statement The authors 
reported no conflicts of interest. The Journal policy requires editors and 
reviewers to disclose conflicts of interest and to decline handling or reviewing 
manuscripts for which they may have a conflict of interest. The editors and 
reviewers of this article have no conflicts of interest.


946. AJR Am J Roentgenol. 2025 Aug;225(2):e2532956. doi: 10.2214/AJR.25.32956. Epub 
2025 May 7.

Prompt Engineering for Large Language Models in Interventional Radiology.

Dietrich N(1), Bradbury NC(2), Loh C(2).

Author information:
(1)Temerty Faculty of Medicine, University of Toronto, 1 King's College Cir, 
Toronto, ON M5S 1A8, Canada.
(2)Department of Radiology, University of North Dakota School of Medicine and 
Health Sciences, Grand Forks, ND.

Comment in
    AJR Am J Roentgenol. 2025 Aug;225(2):e2533212. doi: 10.2214/AJR.25.33212.
    AJR Am J Roentgenol. 2025 Aug;225(2):e2533266. doi: 10.2214/AJR.25.33266.

Prompt engineering plays a crucial role in optimizing artificial intelligence 
(AI) and large language model (LLM) outputs by refining input structure, a key 
factor in medical applications where precision and reliability are paramount. 
This Clinical Perspective provides an overview of prompt-engineering techniques 
and their relevance to interventional radiology (IR). It explores key 
strategies, including zero-shot, one-or few-shot, chain-of-thought, 
tree-of-thought, self-consistency, and directional stimulus prompting, showing 
their application in IR-specific contexts. Practical examples illustrate how 
these techniques can be effectively structured for workplace and clinical use. 
Additionally, this article discusses best practices for designing effective 
prompts and addresses challenges in the clinical use of generative AI, including 
data privacy and regulatory concerns. It concludes with an outlook on the future 
of generative AI in IR, highlighting advances including retrieval-augmented 
generation, domain-specific LLMs, and multimodal models.

DOI: 10.2214/AJR.25.32956
PMID: 40334089 [Indexed for MEDLINE]


947. Radiology. 2025 Apr;315(1):e240528. doi: 10.1148/radiol.240528.

Best Practices for Large Language Models in Radiology.

Bluethgen C(1), Van Veen D(1), Zakka C(1), Link KE(1), Fanous AH(1), Daneshjou 
R(1), Frauenfelder T(1), Langlotz CP(1), Gatidis S(1), Chaudhari A(1).

Author information:
(1)From the Stanford Center for Artificial Intelligence in Medicine and Imaging, 
Palo Alto, Calif (C.B., D.V.V., C.P.L., S.G., A.C.); Institute for Diagnostic 
and Interventional Radiology, University Hospital Zurich, University of Zurich, 
Rämistrasse 100, 8005 Zurich, Switzerland (C.B., T.F.); Department of Electrical 
Engineering, Stanford University, Stanford, Calif (D.V.V.); Department of 
Cardiothoracic Surgery, Stanford Medicine, Stanford, Calif (C.Z.); Department of 
Medical Education, Icahn School of Medicine at Mount Sinai, New York, NY 
(K.E.L.); NVIDIA, New York, NY (K.E.L.); UT Health San Antonio, San Antonio, Tex 
(A.H.F.); Department of Biomedical Data Science, Stanford Medicine, Stanford, 
Calif (A.H.F., R.D., C.P.L., A.C.); Department of Dermatology, Stanford 
Medicine, Redwood City, Calif (R.D.); Department of Medicine, Stanford Medicine, 
Stanford, Calif (C.P.L., A.C.); and Department of Radiology, Stanford 
University, Stanford, Calif (C.P.L., S.G., A.C.).

Radiologists must integrate complex imaging data with clinical information to 
produce actionable insights. This task requires a nuanced application of 
language across many activities, including managing clinical requests, analyzing 
imaging findings in the context of clinical data, interpreting these through the 
radiologist's lens, and effectively documenting and communicating the outcomes. 
Radiology practices must ensure reliable communication among numerous systems 
and stakeholders critical for medical decision-making. Large language models 
(LLMs) offer an opportunity to improve the management and interpretation of the 
vast amounts of text data in radiology. Despite being developed as 
general-purpose tools, these advanced computational models demonstrate 
impressive capabilities in specialized tasks, even without specific training. 
Unlocking the potential of LLMs for radiology requires an understanding of their 
foundations and a strategic approach to navigate their idiosyncrasies. This 
review, drawing from practical radiology and machine learning expertise, 
provides general and technically adept radiologists insight into the potential 
of LLMs in radiology. It also equips those interested in implementing applicable 
best practices that have so far stood the test of time in the rapidly evolving 
landscape of LLMs. The review provides practical advice for optimizing LLM 
characteristics for radiology practices, including advice on limitations, 
effective prompting, and fine-tuning strategies.

© RSNA, 2025.

DOI: 10.1148/radiol.240528
PMID: 40298602 [Indexed for MEDLINE]


948. Nucleic Acids Res. 2026 Jan 14;54(2):gkaf1480. doi: 10.1093/nar/gkaf1480.

RaptScore: a large language model-based algorithm for versatile aptamer 
evaluation.

Kimura-Yamazaki A(1), Adachi T(2), Nakamura S(2), Nakamura Y(2), Hamada 
M(1)(3)(4).

Author information:
(1)Graduate School of Advanced Science and Engineering, Waseda University, 
Shinjuku-ku Okubo 3-4-1, 169-0072 Tokyo, Japan.
(2)RIBOMIC, Minato-ku Shirokanedai 3-16-13, 108-0071 Tokyo, Japan.
(3)Cellular and Molecular Biotechnology (CMB) Research Institute, National 
Institute of Advanced Industrial Science and Technology (AIST), Koto-ku Aomi 
2-3-26, 135-0064 Tokyo, Japan.
(4)Graduate School of Medicine, Nippon Medical School, Bunkyo-ku Sendagi1-1-5, 
113-8602 Tokyo, Japan.

RNA aptamers are a high-potency tool in the life sciences, offering promising 
applications in drug discovery and beyond. They are typically obtained through 
systematic evolution of ligands by exponential enrichment (SELEX), which imposes 
constraints on sequence length and diversity. Several metrics, such as frequency 
and enrichment, have been developed to identify high-activity aptamers from 
SELEX. However, existing evaluation metrics are limited to sequences that appear 
within SELEX and cannot assess sequences of varying lengths, limiting their 
utility in optimizing aptamer design. To overcome these limitations, we 
developed RaptScore, a novel binding activity evaluation metric leveraging large 
language models. RaptScore enables the assessment of arbitrary sequences, 
including those absent from SELEX, and accommodates variations in sequence 
length. RaptScore exhibited a strong correlation with binding activity, allowing 
the identification of shorter aptamers with enhanced binding properties. By 
integrating RaptScore with in silico maturation, we achieved a 10-nucleotide 
truncation while maintaining binding efficiency. Furthermore, we demonstrated 
improved aptamer discovery efficiency by combining RaptScore with RaptGen, a 
variational autoencoder-based aptamer discovery tool. By enabling efficient 
sequence evaluation and optimization, RaptScore provides a powerful tool for 
aptamer research, facilitating the discovery of high-activity candidates while 
reducing experimental effort.

© The Author(s) 2026. Published by Oxford University Press.

DOI: 10.1093/nar/gkaf1480
PMCID: PMC12802916
PMID: 41533571 [Indexed for MEDLINE]

Conflict of interest statement: None declared.


949. Radiology. 2025 Nov;317(2):e242167. doi: 10.1148/radiol.242167.

Large-Scale Evaluation of Machine Learning Models in Identifying Follow-Up 
Recommendations in Radiology Reports.

Xiao P(#)(1), Yu X(#)(1), Ha SM(1), Bani A(1), Mintz A(1), Wang J(1), Elbanan 
M(2), Mokkarala M(1), Mattay G(1), Nazeri A(1), Kannampallil T(3)(4), Lai 
AM(4)(5), Narra VR(1), Marcus DS(4)(5), Bierhals AJ(1), Sotiras A(1)(4).

Author information:
(1)Mallinckrodt Institute of Radiology, Washington University School of 
Medicine, 4525 Scott Ave, MSC 8225-0082-03, St Louis, MO 63110.
(2)Department of Radiology, University of Missouri, Columbia, Mo.
(3)Department of Anesthesiology, Washington University School of Medicine, St 
Louis, Mo.
(4)Institute for Informatics, Data Science & Biostatistics, Washington 
University School of Medicine, St Louis, Mo.
(5)Department of Medicine, Washington University School of Medicine, St Louis, 
Mo.
(#)Contributed equally

Background Radiology reports often contain follow-up recommendations vital for 
optimal patient care, prevention of complications, and mitigation of legal risk. 
However, there is a lack of comprehensive comparison methods for identifying 
these recommendations across a large volume of reports from various modalities, 
including open-source large language models. Purpose To evaluate the performance 
of machine learning (ML) models, including Meta's open-source LLAMA3 and 
OpenAI's Health Insurance Portability and Accountability Act-compliant 
Generative Pre-trained Transformer, in identifying follow-up recommendations in 
radiology reports. Materials and Methods In this retrospective study, three sets 
of radiology reports were analyzed across multiple imaging modalities from a 
large urban academic medical center: an expert annotated dataset (n = 11 901) 
from January 1 to January 10, 2015; a dataset (n = 32 959) extracted through 
regular expressions (ie, sequences of characters that define search patterns in 
text) from January 11, 2015, to January 1, 2017; and a dataset (n = 4909) 
annotated during dictation from September 8, 2018, to February 23, 2021. To 
assess generalization on impressions, two expertly annotated datasets were used: 
2000 chest radiography reports from the publicly available MIMIC-CXR database 
for external testing and 100 institutional CT reports from January 1 to January 
15, 2024, for temporal testing. Thirty-two text classification methods were 
evaluated separately based on the findings and impression sections of these 
reports. Performance metrics included precision, recall, accuracy, and F1 score; 
with 95% bootstrapped CIs and areas under the precision-recall curve. 
Statistical comparisons were performed by using the McNemar test. Results The 
study included 49 769 reports from 35 509 patients (mean age, 52.2 years ± 22.0 
[SD]; 18 477 female patients) for training (n = 37 140), validation (n = 2584), 
and internal testing (n = 10 045). For the findings section, a 
generative-discriminative model initialized with Google's Word2vec embeddings 
(Hybrid-google) achieved the highest F1 score (0.835; 95% CI: 0.825, 0.845). For 
the impression section, an attention-based bidirectional long short-term memory 
(LSTM) with random initialization (AttBiLSTM-random) performed best, with an F1 
score of 0.979 (95% CI: 0.976, 0.982). Prefixed prompting with GPT-4 
demonstrated superior external and temporal generalization performance on the 
MIMIC-CXR and institutional CT datasets, achieving F1 scores of 0.969 (95% CI: 
0.961, 0.977) and 0.973 (95% CI: 0.937, 1.000), respectively. Conclusion ML 
models showed promise for automating the classification of follow-up 
recommendations in radiology reports. © RSNA, 2025 Supplemental material is 
available for this article.

DOI: 10.1148/radiol.242167
PMCID: PMC12661374
PMID: 41217283 [Indexed for MEDLINE]

Conflict of interest statement: Disclosures of conflicts of interest: Please see 
ICMJE form(s) for author conflicts of interest. These have been provided as 
supplemental materials.


950. Front Endocrinol (Lausanne). 2025 Nov 3;16:1689233. doi: 
10.3389/fendo.2025.1689233. eCollection 2025.

Comparative efficacy and safety of anti-osteoporotic therapies for kidney 
transplant recipients: a systematic review and network meta-analysis.

Liu X(#)(1), Li X(#)(1), Zhao Y(1), Gao Q(1), Xue Y(1), Wu Z(1), Shi X(2), Wu 
X(1).

Author information:
(1)Department of Integrated Traditional Chinese and Western Medicine, Second 
Affiliated Hospital of Xi'an Jiaotong University, Xi'an, Shaanxi, China.
(2)Key Laboratory for Disease Prevention and Control and Health Promotion of 
Shaanxi Province, School of Public Health, Medical Science Center, Xi'an 
Jiaotong University, Xi'an, China.
(#)Contributed equally

BACKGROUND: Kidney transplant recipients (KTRs) are at an increased risk of 
osteoporosis, which negatively impacts their quality of life and transplant 
outcomes. However, the efficacy and safety of anti-osteoporosis treatments in 
this group remain uncertain.
METHODS: We conducted a systematic search of PubMed, Embase, Web of Science, and 
the Cochrane Central Register of Controlled Trials up to August 1, 2024. 
Randomized controlled trials (RCTs) examining anti-osteoporotic medications in 
KTRs were included. Primary outcomes were changes in bone mineral density (BMD) 
at femoral neck and lumbar spine, and adverse events. We performed a frequentist 
network meta-analysis using random-effects models. Evidence certainty was 
assessed using the GRADE approach.
RESULTS: Twenty-one RCTs involving 1,066 participants were included, published 
between 2000 and 2021. For femoral neck BMD, bisphosphonates significantly 
improved BMD compared to control (MD = 0.04, 95%CI=0.00-0.09, p<0.05) based on 
low certainty evidence, while calcitonin was significantly superior to calcium 
(MD=-0.14, 95%CI=-0.28 to -0.01). Most other comparisons showed no statistically 
significant differences based on very low to moderate certainty evidence. For 
lumbar spine BMD, bisphosphonates, calcitonin, and calcium demonstrated 
statistically significant inferiority compared to denosumab, with 
bisphosphonates showing MD=-4.98 (95%CI=-6.84 to -3.13), calcitonin showing 
MD=-4.35 (95%CI=-6.24 to -2.47), and calcium showing MD=-5.85 (95%CI=-7.72 to 
-3.98), while denosumab was superior to control (MD = 5.10, 95%CI=3.25-6.95), 
based on low to very low certainty evidence from one RCT. Calcitonin was also 
significantly superior to calcium (MD = 0.60, 95%CI=0.07-1.12). For safety 
outcomes, no statistically significant differences were observed between 
interventions based on low to moderate certainty evidence.
CONCLUSION: Denosumab appears most effective for improving lumbar spine BMD in 
KTRs, while calcitonin shows promise for femoral neck BMD improvement. However, 
the low to moderate certainty of evidence necessitates individualized treatment 
approaches considering patient-specific factors including renal function and 
safety profiles. These findings suggest current guidelines emphasizing 
bisphosphonates as first-line therapy may require revision, though larger 
long-term studies with fracture endpoints are needed to confirm these results.
SYSTEMATIC REVIEW REGISTRATION: 
https://www.crd.york.ac.uk/prospero/?utm_source=chatgpt.com, identifier PROSPERO 
CRD42024587203.

Copyright © 2025 Liu, Li, Zhao, Gao, Xue, Wu, Shi and Wu.

DOI: 10.3389/fendo.2025.1689233
PMCID: PMC12620273
PMID: 41255525 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


951. Diagnostics (Basel). 2025 Oct 20;15(20):2646. doi: 10.3390/diagnostics15202646.

From Prompts to Practice: Evaluating ChatGPT, Gemini, and Grok Against Plastic 
Surgeons in Local Flap Decision-Making.

Marcaccini G(1), Corradini L(1), Shadid O(2), Seth I(2)(3), Rozen WM(2), 
Grimaldi L(1), Cuomo R(1).

Author information:
(1)Plastic Surgery Unit, Department of Medicine, Surgery and Neuroscience, 
University of Siena, 53100 Siena, Italy.
(2)Faculty of Medicine and Surgery, Peninsula Clinical School, Monash 
University, Melbourne, VIC 3199, Australia.
(3)Department of Plastic and Reconstructive Surgery, Frankston Hospital, 
Peninsula Health, Frankston, VIC 3199, Australia.

Background: Local flaps are a cornerstone of reconstructive plastic surgery for 
oncological skin defects, ensuring functional recovery and aesthetic 
integration. Their selection, however, varies with surgeon experience. 
Generative artificial intelligence has emerged as a potential decision-support 
tool, although its clinical role remains uncertain. Methods: We evaluated three 
generative AI platforms (ChatGPT-5 by OpenAI, Grok by xAI, and Gemini by Google 
DeepMind) in their free-access versions available in September 2025. Ten 
preoperative photographs of suspected cutaneous neoplastic lesions from diverse 
facial and limb sites were submitted to each platform in a two-step task: 
concise description of site, size, and tissue involvement, followed by the 
single most suitable local flap for reconstruction. Outputs were compared with 
the unanimous consensus of experienced plastic surgeons. Results: Performance 
differed across models. ChatGPT-5 consistently described lesion size accurately 
and achieved complete concordance with surgeons in flap selection. Grok showed 
intermediate performance, tending to recognise tissue planes better than lesion 
size and proposing flaps that were often acceptable but not always the preferred 
choice. Gemini estimated size well, yet was inconsistent for anatomical site, 
tissue involvement, and flap recommendation. When partially correct answers were 
considered acceptable, differences narrowed but the overall ranking remained 
unchanged. Conclusion: Generative AI can support reconstructive reasoning from 
clinical images with variable reliability. In this series, ChatGPT-5 was the 
most dependable for local flap planning, suggesting a potential role in 
education and preliminary decision-making. Larger studies using standardised 
image acquisition and explicit uncertainty reporting are needed to confirm 
clinical applicability and safety.

DOI: 10.3390/diagnostics15202646
PMCID: PMC12564864
PMID: 41153319

Conflict of interest statement: The authors declare no conflicts of interest.


952. Eye (Lond). 2025 Apr;39(6):1132-1137. doi: 10.1038/s41433-024-03545-9. Epub 2024 
Dec 17.

Benchmarking the performance of large language models in uveitis: a comparative 
analysis of ChatGPT-3.5, ChatGPT-4.0, Google Gemini, and Anthropic Claude3.

Zhao FF(#)(1), He HJ(#)(1)(2), Liang JJ(#)(1), Cen J(3), Wang Y(1), Lin H(1), 
Chen F(1), Li TP(1), Yang JF(1), Chen L(1), Cen LP(4).

Author information:
(1)Joint Shantou International Eye Center of Shantou University and The Chinese 
University of Hong Kong, Shantou, Guangdong, China.
(2)Shantou University Medical College, Shantou, Guangdong, China.
(3)Shaoguan University Medical college, Shaoguan, China.
(4)Guangdong Provincial Key Laboratory of Medical Immunology and Molecular 
Diagnostics, School of Medical Technology, Guangdong Medical University, 
Zhanjiang, China. cenlp@hotmail.com.
(#)Contributed equally

BACKGROUND/OBJECTIVE: This study aimed to evaluate the accuracy, 
comprehensiveness, and readability of responses generated by various Large 
Language Models (LLMs) (ChatGPT-3.5, Gemini, Claude 3, and GPT-4.0) in the 
clinical context of uveitis, utilizing a meticulous grading methodology.
METHODS: Twenty-seven clinical uveitis questions were presented individually to 
four Large Language Models (LLMs): ChatGPT (versions GPT-3.5 and GPT-4.0), 
Google Gemini, and Claude. Three experienced uveitis specialists independently 
assessed the responses for accuracy using a three-point scale across three 
rounds with a 48-hour wash-out interval. The final accuracy rating for each LLM 
response ('Excellent', 'Marginal', or 'Deficient') was determined through a 
majority consensus approach. Comprehensiveness was evaluated using a three-point 
scale for responses rated 'Excellent' in the final accuracy assessment. 
Readability was determined using the Flesch-Kincaid Grade Level formula. 
Statistical analyses were conducted to discern significant differences among 
LLMs, employing a significance threshold of p < 0.05.
RESULTS: Claude 3 and ChatGPT 4 demonstrated significantly higher accuracy 
compared to Gemini (p < 0.001). Claude 3 also showed the highest proportion of 
'Excellent' ratings (96.3%), followed by ChatGPT 4 (88.9%). ChatGPT 3.5, Claude 
3, and ChatGPT 4 had no responses rated as 'Deficient', unlike Gemini (14.8%) 
(p = 0.014). ChatGPT 4 exhibited greater comprehensiveness compared to Gemini 
(p = 0.008), and Claude 3 showed higher comprehensiveness compared to Gemini 
(p = 0.042). Gemini showed significantly better readability compared to ChatGPT 
3.5, Claude 3, and ChatGPT 4 (p < 0.001). Gemini also had fewer words, letter 
characters, and sentences compared to ChatGPT 3.5 and Claude 3.
CONCLUSIONS: Our study highlights the outstanding performance of Claude 3 and 
ChatGPT 4 in providing precise and thorough information regarding uveitis, 
surpassing Gemini. ChatGPT 4 and Claude 3 emerge as pivotal tools in improving 
patient understanding and involvement in their uveitis healthcare journey.

© 2024. The Author(s), under exclusive licence to The Royal College of 
Ophthalmologists.

DOI: 10.1038/s41433-024-03545-9
PMCID: PMC11978972
PMID: 39690303 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


953. Diagnostics (Basel). 2025 Jun 6;15(12):1451. doi: 10.3390/diagnostics15121451.

Risk of Bias Assessment of Diagnostic Accuracy Studies Using QUADAS 2 by Large 
Language Models.

Leucuța DC(1), Urda-Cîmpean AE(1), Istrate D(1), Drugan T(1).

Author information:
(1)Department of Medical Informatics and Biostatistics, Iuliu Hațieganu 
University of Medicine and Pharmacy, 400349 Cluj-Napoca, Romania.

Background/Objectives: Diagnostic accuracy studies are essential for the 
evaluation of the performance of medical tests. The risk of bias (RoB) for these 
studies is commonly assessed using the Quality Assessment of Diagnostic Accuracy 
Studies (QUADAS) tool. This study aimed to assess the capabilities and reasoning 
accuracy of large language models (LLMs) in evaluating the RoB in diagnostic 
accuracy studies, using QUADAS 2, compared to human experts. Methods: Four LLMs 
were used for the AI assessment: ChatGPT 4o model, X.AI Grok 3 model, Gemini 2.0 
flash model, and DeepSeek V3 model. Ten recent open-access diagnostic accuracy 
studies were selected. Each article was independently assessed by human experts 
and by LLMs using QUADAS 2. Results: Out of 110 signaling questions assessments 
(11 questions for each of the 10 articles) by the four AI models, and the mean 
percentage of correct assessments of all the models was 72.95%. The most 
accurate model was Grok 3, followed by ChatGPT 4o, DeepSeek V3, and Gemini 2.0 
Flash, with accuracies ranging from 74.45% to 67.27%. When analyzed by domain, 
the most accurate responses were for "flow and timing", followed by "index 
test", and then similarly for "patient selection" and "reference standard". An 
extensive list of reasoning errors was documented. Conclusions: This study 
demonstrates that LLMs can achieve a moderate level of accuracy in evaluating 
the RoB in diagnostic accuracy studies. However, they are not yet a substitute 
for expert clinical and methodological judgment. LLMs may serve as complementary 
tools in systematic reviews, with compulsory human supervision.

DOI: 10.3390/diagnostics15121451
PMCID: PMC12191753
PMID: 40564772

Conflict of interest statement: The authors declare no conflicts of interest.


954. Diagnostics (Basel). 2025 May 13;15(10):1221. doi: 10.3390/diagnostics15101221.

Diagnostic Performance of Publicly Available Large Language Models in Corneal 
Diseases: A Comparison with Human Specialists.

Jiao C(1), Rosas E(1), Asadigandomani H(2), Delsoz M(3), Madadi Y(3), Raja H(3), 
Munir WM(4), Tamm B(4), Mehravaran S(5), Djalilian AR(6), Yousefi S(3)(7), 
Soleimani M(1).

Author information:
(1)Department of Ophthalmology, University of North Carolina at Chapel Hill, 
Chapel Hill, NC 27599, USA.
(2)Department of Ophthalmology, University of California San Francisco, San 
Francisco, CA 94143, USA.
(3)Department of Ophthalmology, Hamilton Eye Institute, University of Tennessee 
Health Science Center, Memphis, TN 38103, USA.
(4)Department of Ophthalmology and Visual Sciences, University of Maryland 
School of Medicine, Baltimore, MD 21201, USA.
(5)Department of Biology, School of Computer, Mathematical, and Natural 
Sciences, Morgan State University, Baltimore, MD 21251, USA.
(6)Department of Ophthalmology and Visual Sciences, University of Illinois at 
Chicago, Chicago, IL 60612, USA.
(7)Department of Genetics, Genomics, and Informatics, University of Tennessee 
Health Science Center, Memphis, TN 38136, USA.

Background/Objectives: This study evaluated the diagnostic accuracy of seven 
publicly available large language models (LLMs)-GPT-3.5, GPT-4.o Mini, GPT-4.o, 
Gemini 1.5 Flash, Claude 3.5 Sonnet, Grok3, and DeepSeek R1-in diagnosing 
corneal diseases, comparing their performance to human specialists. Methods: 
Twenty corneal disease cases from the University of Iowa's EyeRounds were 
presented to each LLM. Diagnostic accuracy was determined by comparing 
LLM-generated diagnoses to the confirmed case diagnoses. Four human cornea 
specialists evaluated the same cases to establish a benchmark and assess 
interobserver agreement. Results: Diagnostic accuracy varied significantly among 
LLMs (p = 0.001). GPT-4.o achieved the highest accuracy (80.0%), followed by 
Claude 3.5 Sonnet and Grok3 (70.0%), DeepSeek R1 (65.0%), GPT-3.5 (60.0%), 
GPT-4.o Mini (55.0%), and Gemini 1.5 Flash (30.0%). Human experts averaged 92.5% 
accuracy, outperforming all LLMs (p < 0.001, Cohen's d = -1.314). GPT-4.o showed 
no significant difference from human consensus (p = 0.250, κ = 0.348), while 
Claude and Grok3 showed fair agreement (κ = 0.219). DeepSeek R1 also performed 
reasonably (κ = 0.178), although not significantly. Conclusions: Among the 
evaluated LLMs, GPT-4.o, Claude 3.5 Sonnet, Grok3, and DeepSeek R1 demonstrated 
promising diagnostic accuracy, with GPT-4.o most closely matching human 
performance. However, performance remained inconsistent, especially in complex 
cases. LLMs may offer value as diagnostic support tools, but human expertise 
remains indispensable for clinical decision-making.

DOI: 10.3390/diagnostics15101221
PMCID: PMC12110359
PMID: 40428214

Conflict of interest statement: The authors declare that they have no conflicts 
of interest related to this study.


955. Diagnostics (Basel). 2025 Feb 28;15(5):587. doi: 10.3390/diagnostics15050587.

Management of Dupuytren's Disease: A Multi-Centric Comparative Analysis Between 
Experienced Hand Surgeons Versus Artificial Intelligence.

Seth I(1)(2)(3), Marcaccini G(1)(4), Lim K(1), Castrechini M(5), Cuomo R(4), Ng 
SK(3), Ross RJ(1)(2), Rozen WM(1)(2).

Author information:
(1)Department of Plastic and Reconstructive Surgery, Peninsula Health, 
Frankston, VIC 3199, Australia.
(2)Faculty of Medicine and Surgery, Peninsula Clinical School, Monash 
University, Frankston, VIC 3199, Australia.
(3)Department of Plastic and Reconstructive Surgery, Austin Health, Heidelberg, 
VIC 3199, Australia.
(4)Plastic Surgery Unit, Department of Medicine, Surgery and Neuroscience, 
University of Siena, 53100 Siena, Italy.
(5)Plastic Surgery Unit, Department of Surgery "P. Valdoni", "Sapienza" 
University of Rome, 00185 Rome, Italy.

Background: Dupuytren's fibroproliferative disease affecting the hand's palmar 
fascia leads to progressive finger contractures and functional limitations. 
Management of this condition relies heavily on the expertise of hand surgeons, 
who tailor interventions based on clinical assessment. With the growing interest 
in artificial intelligence (AI) in medical decision-making, this study aims to 
evaluate the feasibility of integrating AI into the clinical management of 
Dupuytren's disease by comparing AI-generated recommendations with those of 
expert hand surgeons. Methods: This multicentric comparative study involved 
three experienced hand surgeons and five AI systems (ChatGPT, Gemini, 
Perplexity, DeepSeek, and Copilot). Twenty-two standardized clinical prompts 
representing various Dupuytren's disease scenarios were used to assess 
decision-making. Surgeons and AI systems provided management recommendations, 
which were analyzed for concordance, rationale, and predicted outcomes. Key 
metrics included union accuracy, surgeon agreement, precision, recall, and F1 
scores. The study also evaluated AI performance in unanimous versus 
non-unanimous cases and inter-AI agreements. Results: Gemini and ChatGPT 
demonstrated the highest union accuracy (86.4% and 81.8%, respectively), while 
Copilot showed the lowest (40.9%). Surgeon agreement was highest for Gemini 
(45.5%) and ChatGPT (42.4%). AI systems performed better in unanimous cases 
(accuracy up to 92.0%) than in non-unanimous cases (accuracy as low as 35.0%). 
Inter-AI agreements ranged from 75.0% (ChatGPT-Gemini) to 48.0% 
(DeepSeek-Copilot). Precision, recall, and F1 scores were consistently higher 
for ChatGPT and Gemini than for other systems. Conclusions: AI systems, 
particularly Gemini and ChatGPT, show promise in aligning with expert surgical 
recommendations, especially in straightforward cases. However, significant 
variability exists, particularly in complex scenarios. AI should be viewed as 
complementary to clinical judgment, requiring further refinement and validation 
for integration into clinical practice.

DOI: 10.3390/diagnostics15050587
PMCID: PMC11898831
PMID: 40075834

Conflict of interest statement: The authors declare no conflicts of interest. 
Ishith Seth serves as the Guest Editor for the Special Issue in which this 
manuscript is published. However, the editorial process was conducted 
independently to ensure transparency and integrity.


956. Front Endocrinol (Lausanne). 2025 Sep 19;16:1627919. doi: 
10.3389/fendo.2025.1627919. eCollection 2025.

Causal insights into gestational diabetes mellitus.

Zahoor S(1), Constantinou AC(2), O'Halloran F(3), O'Mahony L(4), O'Riordan M(4), 
Kgosidialwa O(4), Culliney L(4), Said Alhajri M(1), Hasanuzzaman M(5).

Author information:
(1)Department of Computer Science, Munster Technological University, 
Cork, Ireland.
(2)Bayesian Artificial Intelligence Research Lab, Machine Intelligence and 
Decision Systems (MInDS) Group, Queen Mary University of London (QMUL), 
London, United Kingdom.
(3)Department of Biological Sciences, Munster Technological University, 
Cork, Ireland.
(4)Cork University Hospital, Cork, Ireland.
(5)Queen's University Belfast, School of Electronics, Electrical Engineering and 
Computer Science (EEECS), Belfast, United Kingdom.

INTRODUCTION: Gestational diabetes mellitus (GDM), defined by the onset of 
hyperglycaemia during pregnancy, remains the most prevalent metabolic 
complication in pregnancy. It is associated with increased risks of adverse 
maternal, neonatal, and long-term metabolic outcomes. This study aimed to 
identify potential causal relationships within clinical data on GDM that could 
support more targeted and effective interventions.
METHODS: A clinically curated dataset of patients diagnosed with GDM at a major 
Irish maternity hospital was analysed, covering the study periods 2014-2016 and 
2020. A knowledge graph was constructed by integrating clinical expertise, 
established literature, and insights generated using the GPT-4 large language 
model. To complement this, 20 structure learning algorithms were applied to 
independently infer Causal Bayesian Networks (CBNs). A model-averaging approach 
was then used to generate a consensus-based causal structure to account for 
variability across individual models.
RESULTS: The integrative model produced a more stable representation of 
underlying relationships and yielded quantifiable insights to support clinical 
decision-making. Clinicians involved in the study reported improved confidence 
in patient care strategies due to the ability to quantify these relationships, 
facilitating more personalised, evidence-based practice. Key findings from the 
model-averaged CBN highlighted critical pathways in GDM management, such as the 
influence of birth weight on neonatal intensive care unit (NICU) admissions and 
the impact of dietary intervention on maternal glucose regulation. Sensitivity 
analysis confirmed birth weight, gestational age at delivery, and mode of 
delivery as major determinants of maternal and neonatal outcomes. Non-modifiable 
factors, including a history of multiple pregnancies and prior GDM, also 
contributed to risk stratification.
DISCUSSION: This study applied structure learning techniques to observational 
clinical data to identify clinically relevant relationships. The resulting 
insights provide a basis for generating hypotheses that could refine 
intervention strategies and improve patient outcomes in GDM care.

Copyright © 2025 Zahoor, Constantinou, O’Halloran, O’Mahony, O’Riordan, 
Kgosidialwa, Culliney, Said Alhajri and Hasanuzzaman.

DOI: 10.3389/fendo.2025.1627919
PMCID: PMC12490993
PMID: 41048423 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


957. Ann Med Surg (Lond). 2025 May 20;87(7):4679-4681. doi: 
10.1097/MS9.0000000000003368. eCollection 2025 Jul.

Can generative AI-enabled ChatGPT or large language models (LLMs) assist 
orthopedic surgeons?

Chakraborty C(1), Chatterjee S(2), Bhattacharya M(3), Islam MA(4)(5).

Author information:
(1)Department of Biotechnology, School of Life Science and Biotechnology, Adamas 
University, Kolkata, India.
(2)Institute for Skeletal Aging & Orthopedic Surgery, Hallym 
University-Chuncheon Sacred Heart Hospital, Chuncheon-si, Republic of Korea.
(3)Department of Zoology, Fakir Mohan University, Vyasa Vihar, Balasore, India.
(4)Department of Microbiology, COVID-19 Diagnostic Lab, Noakhali Science and 
Technology University, Noakhali, Bangladesh.
(5)Department of Microbiology, Advanced Molecular Lab, President Abdul Hamid 
Medical College, Karimganj, Kishoreganj, Bangladesh.

The incorporation of generative artificial intelligence (AI) models, like 
ChatGPT, into orthopedic surgery, has generated considerable interest among 
medical professionals. This paper provides an overview of the latest research on 
using ChatGPT in orthopedics. It explores the potential applications, 
advantages, obstacles, and ethical concerns of implementing this technology. 
Based on various studies, this paper delves into how ChatGPT contributes to 
patient communication, treatment planning, surgical outcomes, and diagnostic 
processes in orthopedic care. Although there is great potential for the 
technology to transform orthopedic practice, there are ongoing concerns about 
data security, ethical considerations, and the reliability of self-diagnosis 
tools. Effective collaboration among stakeholders, thorough validation 
procedures, and continuous research efforts are crucial in unlocking the 
complete potential of AI in orthopedic surgery while minimizing any risks 
involved. Despite the obstacles, the growing use of ChatGPT and other AI 
technologies in orthopedics indicates a significant change in healthcare 
delivery.

Copyright © 2025 The Author(s). Published by Wolters Kluwer Health, Inc.

DOI: 10.1097/MS9.0000000000003368
PMCID: PMC12369820
PMID: 40851947

Conflict of interest statement: No conflicts of interest.


958. Genes (Basel). 2025 Jun 28;16(7):762. doi: 10.3390/genes16070762.

The Artificial Intelligence-Assisted Diagnosis of Skeletal Dysplasias in 
Pediatric Patients: A Comparative Benchmark Study of Large Language Models and a 
Clinical Expert Group.

Ilić N(1), Marić N(2)(3), Cvetković D(4), Bogosavljević M(5), Bukara-Radujković 
G(2)(3), Krstić J(1), Paunović Z(5)(6), Begović N(5)(6), Panić Zarić S(4), 
Todorović S(4), Mitrović K(4)(7), Vlahović A(6)(8), Sarajlija A(1)(7)(9).

Author information:
(1)Clinical Genetics Outpatient Clinic, Mother and Child Health Care Institute 
of Serbia "Dr Vukan Cupic", 11070 Belgrade, Serbia.
(2)Clinic for Children Diseases, University Clinical Center of the Republic of 
Srpska, 78000 Banja Luka, Bosnia and Herzegovina.
(3)Medical Faculty, University of Banjaluka, 78000 Banja Luka, Bosnia and 
Herzegovina.
(4)Department of Endocrinology, Mother and Child Health Care Institute of Serbia 
"Dr Vukan Cupic", 11070 Belgrade, Serbia.
(5)Department of Orthopedic Surgery and Traumatology, Mother and Child Health 
Care Institute of Serbia "Dr Vukan Cupic", 11070 Belgrade, Serbia.
(6)Department of Surgery, Division of Pediatric Surgery, University of Belgrade, 
11000 Belgrade, Serbia.
(7)Department of Pediatrics, Faculty of Medicine, University of Belgrade, 11000 
Belgrade, Serbia.
(8)Department of Plastic and Reconstructive Surgery and Burns, Mother and Child 
Health Care Institute of Serbia "Dr Vukan Cupic", 11070 Belgrade, Serbia.
(9)Faculty of Medicine, University of Eastern Sarajevo, 73300 Foča, Republic of 
Srpska, Bosnia and Herzegovina.

BACKGROUND/OBJECTIVES: Skeletal dysplasias are a heterogeneous group of rare 
genetic disorders with diverse and overlapping clinical presentations, posing 
diagnostic challenges even for experienced clinicians. With the increasing 
availability of artificial intelligence (AI) in healthcare, large language 
models (LLMs) offer a novel opportunity to assist in rare disease diagnostics. 
This study aimed to compare the diagnostic accuracy of two advanced LLMs, 
ChatGPT (version GPT-4) and DeepSeek, with that of a clinical expert panel in a 
cohort of pediatric patients with genetically confirmed skeletal dysplasias.
METHODS: We designed a prospective vignette-based diagnostic benchmarking study 
including 45 children with confirmed skeletal dysplasias from two tertiary 
centers. Both LLMs were prompted to provide primary and differential diagnoses 
based on standardized clinical case vignettes. Their outputs were compared with 
those of two human experts (a pediatric endocrinologist and a pediatric 
orthopedic surgeon), using molecular diagnosis as the gold standard.
RESULTS: ChatGPT and DeepSeek achieved a comparable diagnostic top-3 accuracy 
(62.2% and 64.4%, respectively), with a high intermodel agreement (Cohen's κ = 
0.95). The expert panel outperformed both models (82.2%). While LLMs performed 
well on more common disorders, they struggled with ultra-rare and multisystemic 
conditions. In one complex case missed by experts, the DeepSeek model 
successfully proposed the correct diagnosis.
CONCLUSIONS: LLMs offer a complementary diagnostic value in skeletal dysplasias, 
especially in under-resourced medical settings. Their integration as a 
supportive tool in multidisciplinary diagnostic workflows may enhance early 
recognition and reduce diagnostic delays in rare disease care.

DOI: 10.3390/genes16070762
PMCID: PMC12294814
PMID: 40725418 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


959. Eye (Lond). 2025 Nov;39(16):2860-2871. doi: 10.1038/s41433-025-04006-7. Epub 
2025 Sep 26.

Generative artificial intelligence in ophthalmology: a scoping review of current 
applications, opportunities, and challenges.

Rahat Qureshi A(1), Micieli JA(2)(3)(4), Wong JCY(5).

Author information:
(1)Temerty Faculty of Medicine, University of Toronto, Toronto, ON, Canada.
(2)Department of Ophthalmology and Vision Sciences, University of Toronto, 
Toronto, ON, Canada.
(3)Kensington Vision and Research Centre and Kensington Research Institute, 
Toronto, ON, Canada.
(4)Department of Ophthalmology, St. Michael's Hospital, St. Michael's Hospital 
Unity Health, Toronto, ON, Canada.
(5)Department of Ophthalmology and Vision Sciences, University of Toronto, 
Toronto, ON, Canada. jovi.wong@mail.utoronto.ca.

Generative artificial intelligence (AI) has the potential for enhancing 
diagnostic accuracy, augmenting image datasets, and simulating disease 
progression in ophthalmology. There is a paucity in the literature regarding a 
comprehensive mapping of the broad range of generative AI models. A scoping 
review was conducted to (1) identify and categorise generative AI models applied 
in ophthalmology, (2) examine their primary clinical and research applications, 
(3) evaluate the feasibility and challenges of deploying these models in 
clinical practice, and (4) outline future research directions. Primary research 
articles published in English over the past 10 years, explicitly referencing a 
generative AI method for ophthalmic imaging modalities were included. Searches 
were conducted on, MEDLINE, Embase, and Web of Science up to December 24, 2024. 
One reviewer independently screened titles, abstracts, and full texts and 
subsequently charted data using a calibrated extraction form. Data regarding 
study design, generative AI type, and clinical outcomes or applications were 
included. 40 studies were included, with most studies focused on data 
augmentation (n = 11) and predictive modelling (n = 11), followed by image 
enhancement (n = 8), segmentation (n = 7), and education/interpretability 
(n = 3). GAN-based approaches predominated, but diffusion models, VAEs, and 
flow-based models have recently gained traction. Implementation barriers 
included data availability, regulatory considerations, and model stability. This 
scoping review mapped the current applications of generative AI in ophthalmology 
and identified emerging trends, such as diffusion models. Although these methods 
hold promise, robust real-world validation and clearer regulatory pathways are 
needed to fully integrate generative AI into clinical practice.

© 2025. The Author(s), under exclusive licence to The Royal College of 
Ophthalmologists.

DOI: 10.1038/s41433-025-04006-7
PMCID: PMC12583551
PMID: 41006683 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


960. Diagnostics (Basel). 2025 Jun 26;15(13):1635. doi: 10.3390/diagnostics15131635.

Chatbots in Radiology: Current Applications, Limitations and Future Directions 
of ChatGPT in Medical Imaging.

Lanzafame LRM(1), Gulli C(1), Mazziotti S(1), Ascenti G(1), Gaeta M(1), Vogl 
TJ(2), Yel I(2)(3), Koch V(2)(3), Grünewald LD(2)(3), Muscogiuri G(4), Booz 
C(2)(3), D'Angelo T(1).

Author information:
(1)Diagnostic and Interventional Radiology Unit, BIOMORF Department, University 
Hospital "Policlinico G. Martino", Via Consolare Valeria 1, 98100 Messina, 
Italy.
(2)Department of Diagnostic and Interventional Radiology, University Hospital 
Frankfurt, Theodor-Stern-Kai 7, 60590 Frankfurt am Main, Germany.
(3)Division of Experimental Imaging, Department of Diagnostic and Interventional 
Radiology, University Hospital Frankfurt, Theodor-Stern-Kai 7, 60590 Frankfurt 
am Main, Germany.
(4)Department of Radiology, ASST Papa Giovanni XXIII Hospital, 24127 Bergamo, 
Italy.

Artificial intelligence (AI) is reshaping radiological practice, with recent 
advancements in natural language processing (NLP), large language models (LLMs), 
and chatbot technologies opening new avenues for clinical integration. These 
AI-driven conversational agents have demonstrated potential in streamlining 
patient triage, optimizing imaging protocol selection, supporting image 
interpretation, automating radiology report generation, and improving 
communication among radiologists, referring physicians, and patients. Emerging 
evidence also highlights their role in decision-making, clinical data 
extraction, and structured reporting. While the clinical adoption of chatbots 
remains limited by concerns related to data privacy, model robustness, and 
ethical oversight, ongoing developments and regulatory efforts are paving the 
way for responsible implementation. This review provides a critical overview of 
the current and emerging applications of chatbots in radiology, evaluating their 
capabilities, limitations, and future directions for clinical and research 
integration.

DOI: 10.3390/diagnostics15131635
PMCID: PMC12248470
PMID: 40647634

Conflict of interest statement: The authors have no conflicts of interest to 
declare that are relevant to the content of this article.


961. Diagnostics (Basel). 2025 Jul 30;15(15):1919. doi: 10.3390/diagnostics15151919.

Performance of Large Language Models in Recognizing Brain MRI Sequences: A 
Comparative Analysis of ChatGPT-4o, Claude 4 Opus, and Gemini 2.5 Pro.

Salbas A(1), Buyuktoka RE(2).

Author information:
(1)Department of Radiology, Izmir Katip Celebi University, Ataturk Training and 
Research Hospital, Izmir 35150, Turkey.
(2)Department of Radiology, Foca State Hospital, Izmir 35680, Turkey.

Background/Objectives: Multimodal large language models (LLMs) are increasingly 
used in radiology. However, their ability to recognize fundamental imaging 
features, including modality, anatomical region, imaging plane, 
contrast-enhancement status, and particularly specific magnetic resonance 
imaging (MRI) sequences, remains underexplored. This study aims to evaluate and 
compare the performance of three advanced multimodal LLMs (ChatGPT-4o, Claude 4 
Opus, and Gemini 2.5 Pro) in classifying brain MRI sequences. Methods: A total 
of 130 brain MRI images from adult patients without pathological findings were 
used, representing 13 standard MRI series. Models were tested using zero-shot 
prompts for identifying modality, anatomical region, imaging plane, 
contrast-enhancement status, and MRI sequence. Accuracy was calculated, and 
differences among models were analyzed using Cochran's Q test and McNemar test 
with Bonferroni correction. Results: ChatGPT-4o and Gemini 2.5 Pro achieved 100% 
accuracy in identifying the imaging plane and 98.46% in identifying 
contrast-enhancement status. MRI sequence classification accuracy was 97.7% for 
ChatGPT-4o, 93.1% for Gemini 2.5 Pro, and 73.1% for Claude 4 Opus (p < 0.001). 
The most frequent misclassifications involved fluid-attenuated inversion 
recovery (FLAIR) sequences, often misclassified as T1-weighted or 
diffusion-weighted sequences. Claude 4 Opus showed lower accuracy in 
susceptibility-weighted imaging (SWI) and apparent diffusion coefficient (ADC) 
sequences. Gemini 2.5 Pro exhibited occasional hallucinations, including 
irrelevant clinical details such as "hypoglycemia" and "Susac syndrome." 
Conclusions: Multimodal LLMs demonstrate high accuracy in basic MRI recognition 
tasks but vary significantly in specific sequence classification tasks. 
Hallucinations emphasize caution in clinical use, underlining the need for 
validation, transparency, and expert oversight.

DOI: 10.3390/diagnostics15151919
PMCID: PMC12345967
PMID: 40804881

Conflict of interest statement: The authors declare no conflicts of interest.


962. Diagnostics (Basel). 2025 Nov 6;15(21):2808. doi: 10.3390/diagnostics15212808.

Validation of a Dermatology-Focused Multimodal Large Language Model in 
Classification of Pigmented Skin Lesions.

Mijares J(1), Jairath N(1)(2), Zhang A(1), Que SKT(1).

Author information:
(1)Department of Dermatology, Indiana University School of Medicine, 
Indianapolis, IN 46202, USA.
(2)Ronald O. Perelman Department of Dermatology, New York University Grossman 
School of Medicine, New York, NY 10016, USA.

Background: Artificial intelligence (AI) has shown significant promise in 
augmenting diagnostic capabilities across medical specialties. Recent 
advancements in generative AI allow for synthesis and interpretation of complex 
clinical data including imaging and patient history to assess disease risk. 
Objective: To evaluate the diagnostic performance of a dermatology-trained 
multimodal large language model (DermFlow, Delaware, USA) in assessing 
malignancy risk of pigmented skin lesions. Methods: This retrospective study 
utilized data from 59 patients with 68 biopsy-proven pigmented skin lesions seen 
at Indiana University clinics from February 2023 to May 2025. De-identified 
patient histories and clinical images were input into DermFlow, and clinical 
images only were input into Claude Sonnet 4 (Claude) to generate differential 
diagnoses. Clinician pre-operative diagnoses were extracted from the clinical 
note. Assessments were compared to histopathologic diagnoses (gold standard). 
Results: Among 68 clinically concerning pigmented lesions, DermFlow achieved 
47.1% top diagnosis accuracy and 92.6% any-diagnosis accuracy, with F1 = 0.948, 
sensitivity 93.9%, and specificity 89.5% (balanced accuracy 91.7%). Claude had 
8.8% top diagnosis and 73.5% any-diagnosis accuracy, F1 = 0.816, sensitivity 
81.6%, specificity 52.6% (balanced accuracy 67.1%). Clinicians achieved 38.2% 
top diagnosis and 72.1% any-diagnosis accuracy, F1 = 0.776, sensitivity 67.3%, 
specificity 84.2% (balanced accuracy 75.8%). DermFlow recommended biopsy in 
95.6% of cases vs. 82.4% for Claude, with multiple pairwise differences favoring 
DermFlow (p < 0.05). Conclusions: DermFlow demonstrated comparable or superior 
diagnostic performance to clinicians and superior performance to Claude in 
evaluating pigmented skin lesions. Although additional data must be gathered to 
further validate the model in real clinical settings, these initial findings 
suggest potential utility for dermatology-trained AI models in clinical 
practice, particularly in settings with limited dermatologist availability.

DOI: 10.3390/diagnostics15212808
PMCID: PMC12608998
PMID: 41226100

Conflict of interest statement: Author N.J. provided clinical input to guide 
development of DermFlow. The remaining authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


963. JACC Adv. 2025 Mar;4(3):101593. doi: 10.1016/j.jacadv.2025.101593. Epub 2025 Feb 
8.

Embracing Generative Artificial Intelligence in Clinical Research and Beyond: 
Opportunities, Challenges, and Solutions.

Foote HP(1), Hong C(2), Anwar M(3), Borentain M(4), Bugin K(5), Dreyer N(6), 
Fessel J(7), Goyal N(8), Hanger M(9), Hernandez AF(10), Hornik CP(10), Jackman 
JG(10), Lindsay AC(11), Matheny ME(12), Ozer K(13), Seidel J(14), Stockbridge 
N(5), Embi PJ(12), Lindsell CJ(15).

Author information:
(1)Department of Pediatrics, Duke University, Durham, North Carolina, USA.
(2)Department of Biostatistics and Bioinformatics, Duke University, Durham, 
North Carolina, USA; Duke Clinical Research Institute, Durham, North Carolina, 
USA.
(3)National Institute of Biomedical Imaging and Bioengineering, National 
Institutes of Health, Bethesda, Maryland, USA.
(4)Bayer Corporation, Whippany, New Jersey, USA.
(5)United States Food and Drug Administration, Silver Spring, Maryland, USA.
(6)Dreyer Strategies, Newton, Massachusetts, USA.
(7)National Center for Advancing Translational Sciences, National Institutes of 
Health, Bethesda, Maryland, USA.
(8)Alnylam Pharmaceuticals, Cambridge, Massachusetts, USA.
(9)Clinical Trials Transformation Initiative Duke Clinical Research Institute, 
North Carolina, USA.
(10)Duke Clinical Research Institute, Durham, North Carolina, USA.
(11)Janssen Pharmaceuticals, Horsham, Pennsylvania, USA.
(12)Vanderbilt University Medical Center, Nashville, Tennessee, USA.
(13)Novo Nordisk, Plainsboro, New Jersey, USA.
(14)Boehringer Ingelheim, Plainsboro, New Jersey, USA.
(15)Duke Clinical Research Institute, Durham, North Carolina, USA. Electronic 
address: chris.lindsell@duke.edu.

To explore threats and opportunities and to chart a path for safely navigating 
the rapid changes that generative artificial intelligence (AI) will bring to 
clinical research, the Duke Clinical Research Institute convened a 
multidisciplinary think tank in January 2024. Leading experts from academia, 
industry, nonprofits, and government agencies highlighted the potential 
opportunities of generative AI in automation of documentation, strengthening of 
participant and community engagement, and improvement of trial accuracy and 
efficiency. Challenges include technical hurdles, ethical dilemmas, and 
regulatory uncertainties. Success is expected to require establishing rigorous 
data management and security protocols, fostering integrity and trust among 
stakeholders, and sharing information about the safety and effectiveness of AI 
applications. Meeting insights point towards a future where, through 
collaboration and transparency, generative AI will help to shorten the 
translational pipeline and increase the inclusivity and equitability of clinical 
research.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jacadv.2025.101593
PMCID: PMC11850149
PMID: 39923329

Conflict of interest statement: Funding support and author disclosures Funding 
support for the meeting from which this consensus document was generated was 
provided through registration fees from the following industry sponsors: Alnylam 
Pharmaceuticals, Inc (Cambridge, MA, USA); Amgen, Inc (Apex, NC, USA); 
AstraZeneca (Durham, NC, USA); Bayer Pharma AG (Berlin, Germany); Boehringer 
Ingelheim Pharma GmbH & Co KG (Ingelheim am Rhein, Germany); Bristol Myers 
Squibb Company (New York, NY, USA); Intellia Therapeutics, Inc (Cambridge, MA, 
USA); Janssen Pharmaceuticals (Beerse, Belgium); Companies of Johnson & Johnson 
(New Brunswick, NJ, USA); Novartis AG (Basel, Switzerland); and Novo Nordisk A/S 
(Bagsværd, Denmark). No government funds were used for this meeting. The think 
tank was funded by industry sponsors with representation at the meeting. Dr 
Foote is supported by the Eunice Kennedy Shriver National Institute of Child 
Health and Human Development under awards T32HD094671 and T32HD104576. Dr 
Borentain has stock or stock options with Bayer and is a Bayer employee. Dr 
Dreyer received travel support for attendance at a meeting by Picnic Health and 
has stock options from Picnic Health. Dr Fessel received payment for a lecture 
given at the University for Continuing Education, Kremes, Austria, and received 
travel support from Boehringer Ingelheim Pharma GmbH & Co KG, Department of 
Global Biostatistics & Data Sciences. Dr Goyal received travel support from 
Alnylam Pharmaceuticals for attendance at the think tank meeting and receives 
stock and stock options for Alnylam Pharmaceuticals as an employee. NG is also 
the co-founder of the patient recruitment platforms iTrials. Hanger works for 
the Clinical Trials Transformation Initiative, which is supported primarily by 
an award from the Food and Drug Administration (FDA) of the U.S. Department of 
Health and Human Services (HHS), with an additional 15% from nongovernmental 
entities (member organizations); also received travel support from the National 
Academies of Sciences, Engineering, Medicine Forum on Drug Discovery, 
Development, and Translation. Dr Hernandez has received grants or contracts from 
Pfizer Inc and Merck and consulting fees from Merck. Dr Lindsay has stock or 
stock options with GSK and Johnson & Johnson. Dr Hornik has received grants from 
the National Institutes of Health (award numbers: 5R01-HD106588-02, 
1RL1-HD107784-03, 5R33-HL147833-05, 5T32HD104576-03, OT2OD034481, and 
1U01-MD018294-01) and Burroughs Wellcome (1020016) and has received consulting 
fees from Lightship Inc, Cytokinetics, and UCB Pharma; and also has participated 
on an advisory board for The Emmes Corporation, LLC. Dr Matheny has received 
consulting fees from the Patient-Centered Outcomes Research Institute (PCORI) 
trust fund. Dr Ozer has stock with Novo Nordisk and is a Novo Nordisk employee. 
Dr Lindsell has received grants or contracts from the following, with all 
payments made to the institution: the National Institutes of Health, the 
Department of Defense, the U.S. Centers for Disease Control and Prevention, 
Novartis, AstraZeneca, Cytokinetics, Biomeme, Entegrion Inc, Endpoint Health, 
and bioMerieux; In addition, patents for risk stratification in sepsis and 
septic shock were issued to Cincinnati Children's Hospital Medical Center; has 
also participated in data safety monitoring boards unrelated to the current work 
and has participated on Clinical and Translational Science Awards (CTSA) 
external advisory boards; is also an ex officio member of Association for 
Clinical and Translational Science (ACTS) board, Clinical Research Forum (CRF) 
board, ACTS Advisory Committee, and the Center for Clinical and Translational 
Science (CCTS) Executive Committee; and has stock options with Bioscape Digital. 
In addition, CJL is the Editor-in-Chief of the Journal of Clinical and 
Translational Science. CH, JJ, KB, MA, NS, and PJE report no conflicts of 
interest. The first draft of the manuscript was written independently of sponsor 
input. Industry representatives were able to provide comments on subsequent 
drafts of the manuscript. The ultimate decision to include these comments rested 
with the primary and senior authors.


964. Diagnostics (Basel). 2025 Jun 7;15(12):1455. doi: 10.3390/diagnostics15121455.

Evaluation of the Performance of Large Language Models in the Management of 
Axial Spondyloarthropathy: Analysis of EULAR 2022 Recommendations.

Usen A(1), Kuculmez O(2).

Author information:
(1)Department of Physical Medicine and Rehabilitation, Medipol University, 
Istanbul 34810, Turkey.
(2)Department of Physical Medicine and Rehabilitation, Baskent University Alanya 
Hospital, Antalya 07400, Turkey.

Introduction: Guidelines have great importance in revealing complex and chronic 
conditions such as axial spondyloarthropathy. The aim of this study is to 
compare the answers given by various large language models to open-ended 
questions created from ASAS-EULAR 2022 guidance. Materials and Methods: This was 
a cross-sectional and comparative study. A total of 15 recommendations in the 
ASAS-EULAR 2022 guideline were derived directly from their content into 
open-ended questions in a clinical context. Each question was asked to the 
ChatGPT-3.5, GPT-4o, and Gemini 2.0 Flash models, and the answers were evaluated 
with a seven-point Likert system in terms of usability, reliability, 
Flesch-Kincaid Reading Ease (FKRE) and Flesch-Kincaid Grade Level (FKGL) metrics 
for readability, Universal Sentence Encoder (USE) and ROUGE-L for semantic and 
surface-level similarity. The results of different large language models were 
statistically compared, and p < 0.05 was revealed as statistically significant. 
Results: Better FKRE and FKGL scores were obtained in the Google Gemini 2.0 
program (p < 0.05). Reliability and usefulness scores were significantly higher 
for ChatGPT-4o and Gemini 2.0 (p < 0.05). ChatGPT-4o yielded significantly 
higher semantic similarity scores compared to ChatGPT-3.5 (p < 0.05). There was 
a negative correlation between FKRE and FKGL scores and a positive correlation 
between reliability and usefulness scores (p < 0.05). Conclusions: It was 
determined that ChatGPT-4o and Gemini 2.0 programs gave more reliable, useful, 
and readable answers to open-ended questions derived from the ASAS-EULAR 2022 
guidelines. These programs may potentially assist in supporting treatment 
decision-making in rheumatology in the future.

DOI: 10.3390/diagnostics15121455
PMCID: PMC12192445
PMID: 40564776

Conflict of interest statement: The authors declare that they have no conflicts 
of interest.


965. Vision (Basel). 2025 Apr 11;9(2):33. doi: 10.3390/vision9020033.

Coherent Interpretation of Entire Visual Field Test Reports Using a Multimodal 
Large Language Model (ChatGPT).

Tan JCK(1)(2)(3).

Author information:
(1)Faculty of Medicine, University of New South Wales, Kensington, NSW 2033, 
Australia.
(2)Save Sight Institute, University of Sydney, Sydney, NSW 2000, Australia.
(3)Prince of Wales Hospital Eye Clinic, High Street, Level 4, The Prince of 
Wales Hospital, High Street Building, Randwick, NSW 2031, Australia.

This study assesses the accuracy and consistency of a commercially available 
large language model (LLM) in extracting and interpreting sensitivity and 
reliability data from entire visual field (VF) test reports for the evaluation 
of glaucomatous defects. Single-page anonymised VF test reports from 60 eyes of 
60 subjects were analysed by an LLM (ChatGPT 4o) across four domains-test 
reliability, defect type, defect severity and overall diagnosis. The main 
outcome measures were accuracy of data extraction, interpretation of 
glaucomatous field defects and diagnostic classification. The LLM displayed 100% 
accuracy in the extraction of global sensitivity and reliability metrics and in 
classifying test reliability. It also demonstrated high accuracy (96.7%) in 
diagnosing whether the VF defect was consistent with a healthy, suspect or 
glaucomatous eye. The accuracy in correctly defining the type of defect was 
moderate (73.3%), which only partially improved when provided with a more 
defined region of interest. The causes of incorrect defect type were mostly 
attributed to the wrong location, particularly confusing the superior and 
inferior hemifields. Numerical/text-based data extraction and interpretation was 
overall notably superior to image-based interpretation of VF defects. This study 
demonstrates the potential and also limitations of multimodal LLMs in processing 
multimodal medical investigation data such as VF reports.

DOI: 10.3390/vision9020033
PMCID: PMC12015771
PMID: 40265401

Conflict of interest statement: The author declares no conflicts of interest.


966. Dent J (Basel). 2025 Oct 23;13(11):489. doi: 10.3390/dj13110489.

Artificial Intelligence in Endodontic Education: A Systematic Review with 
Frequentist and Bayesian Meta-Analysis of Student-Based Evidence.

Ardila CM(1)(2), Pineda-Vélez E(2)(3), Vivares-Builes AM(2)(3).

Author information:
(1)Department of Periodontics, Saveetha Dental College, and Hospitals, Saveetha 
Institute of Medical and Technical Sciences, Saveetha University, Saveetha 
600077, India.
(2)Biomedical Stomatology Research Group, Basic Sciences Department, Faculty of 
Dentistry, Universidad de Antioquia U de A, Medellín 050010, Colombia.
(3)Faculty of Dentistry, Institución Universitaria Visión de las Américas, 
Medellín 050040, Colombia.

BACKGROUND/OBJECTIVES: Artificial intelligence (AI) is entering dental 
curricula, yet its educational value in endodontics remains unclear. This review 
synthesized student-based evidence on AI in endodontics, primarily comparing AI 
vs. students on diagnostic tasks as an educational endpoint and secondarily 
considering assessment tasks relevant to training.
METHODS: PubMed/MEDLINE, Embase, Scopus, and Web of Science were searched in 
July 2025. Eligible studies involved dental students using AI in endodontic 
tasks or applied AI to student-generated outputs. For diagnostic comparisons we 
performed random-effects meta-analysis and a complementary Bayesian 
random-effects model with weakly informative priors. Risk of bias used QUADAS-2; 
certainty used GRADE.
RESULTS: Five studies met inclusion. Two provided complete mean-SD data for the 
primary meta-analysis and one contributed to a sensitivity model after SD 
imputation; two were summarized narratively (AUC/F1 only). Pooled effects 
favored AI: Hedges g = 1.48 (95% CI 0.60-2.36; I2 ≈ 84%); sensitivity (k = 3) g 
= 1.45 (95% CI 0.77-2.14; I2 ≈ 77%). Across the two LLM studies with analyzable 
means/SDs, the pooled mean difference in accuracy was approximately +20 
percentage points (AI - students). Bayesian analyses yielded posterior means 
near 1.5 with 95% credible intervals excluding 0 and P (μ > 0) ≈ 1.00. 
Educational outcomes were sparsely and non-standardly reported.
CONCLUSIONS: Student-based evidence indicates that AI likely outperforms dental 
students on endodontic diagnostic tasks, supporting its use as an adjunct for 
formative tutoring, objective feedback, and more consistent assessment.

DOI: 10.3390/dj13110489
PMCID: PMC12650973
PMID: 41294470

Conflict of interest statement: The authors declare no conflicts of interest.


967. Sensors (Basel). 2025 Oct 20;25(20):6484. doi: 10.3390/s25206484.

A Robust Framework for Coffee Bean Package Label Recognition: Integrating Image 
Enhancement with Vision-Language OCR Models.

Le TT(1), Hwang Y(2), Kadiptya AY(2), Son J(2), Kim H(2).

Author information:
(1)Blockchain Platform Research Center, Pusan National University, Busan 609735, 
Republic of Korea.
(2)School of Computer Science and Engineering, Pusan National University, Busan 
609735, Republic of Korea.

Text recognition on coffee bean package labels is of great importance for 
product tracking and brand verification, but it poses a challenge due to 
variations in image quality, packaging materials, and environmental conditions. 
In this paper, we propose a pipeline that combines several image enhancement 
techniques and is followed by an Optical Character Recognition (OCR) model based 
on vision-language (VL) Qwen VL variants, conditioned by structured prompts. To 
facilitate the evaluation, we construct a coffee bean package image set 
containing two subsets, namely low-resolution (LRCB) and high-resolution coffee 
bean image sets (HRCB), enclosing multiple real-world challenges. These cases 
involve various packaging types (bottles and bags), label sides (front and 
back), rotation, and different illumination. To address the image quality 
problem, we design a dedicated preprocessing pipeline for package label 
situations. We develop and evaluate four Qwen-VL OCR variants with prompt 
engineering, which are compared against four baselines: DocTR, PaddleOCR, 
EasyOCR, and Tesseract. Extensive comparison using various metrics, including 
the Levenshtein distance, Cosine similarity, Jaccard index, Exact Match, BLEU 
score, and ROUGE scores (ROUGE-1, ROUGE-2, and ROUGE-L), proves significant 
improvements upon the baselines. In addition, the public POIE dataset validation 
test proves how well the framework can generalize, thus demonstrating its 
practicality and reliability for label recognition.

DOI: 10.3390/s25206484
PMCID: PMC12568198
PMID: 41157537 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


968. Sensors (Basel). 2025 Aug 24;25(17):5272. doi: 10.3390/s25175272.

Digital Cardiovascular Twins, AI Agents, and Sensor Data: A Narrative Review 
from System Architecture to Proactive Heart Health.

Tasmurzayev N(1)(2), Amangeldy B(1)(2), Imanbek B(1), Baigarayeva Z(1)(3), 
Imankulov T(1)(2), Dikhanbayeva G(1)(2), Amangeldi I(1), Sharipova S(1).

Author information:
(1)Faculty of Information Technology, Al-Farabi Kazakh National University, 
Almaty 050040, Kazakhstan.
(2)LLP "DigitAlem", Almaty 050042, Kazakhstan.
(3)LLP "Kazakhstan R&D Solutions", Almaty 050040, Kazakhstan.

Cardiovascular disease remains the world's leading cause of mortality, yet 
everyday care still relies on episodic, symptom-driven interventions that detect 
ischemia, arrhythmias, and remodeling only after tissue damage has begun, 
limiting the effectiveness of therapy. A narrative review synthesized 183 
studies published between 2016 and 2025 that were located through PubMed, MDPI, 
Scopus, IEEE Xplore, and Web of Science. This review examines CVD diagnostics 
using innovative technologies such as digital cardiovascular twins, which 
involve the collection of data from wearable IoT devices (electrocardiography 
(ECG), photoplethysmography (PPG), and mechanocardiography), clinical records, 
laboratory biomarkers, and genetic markers, as well as their integration with 
artificial intelligence (AI), including machine learning and deep learning, 
graph and transformer networks for interpreting multi-dimensional data streams 
and creating prognostic models, as well as generative AI, medical large language 
models (LLMs), and autonomous agents for decision support, personalized alerts, 
and treatment scenario modeling, and with cloud and edge computing for data 
processing. This multi-layered architecture enables the detection of silent 
pathologies long before clinical manifestations, transforming continuous 
observations into actionable recommendations and shifting cardiology from 
reactive treatment to predictive and preventive care. Evidence converges on four 
layers: sensors streaming multimodal clinical and environmental data; hybrid 
analytics that integrate hemodynamic models with deep-, graph- and transformer 
learning while Bayesian and Kalman filters manage uncertainty; decision support 
delivered by domain-tuned medical LLMs and autonomous agents; and prospective 
simulations that trial pacing or pharmacotherapy before bedside use, closing the 
prediction-intervention loop. This stack flags silent pathology weeks in advance 
and steers proactive personalized prevention. It also lays the groundwork for 
software-as-a-medical-device ecosystems and new regulatory guidance for 
trustworthy AI-enabled cardiovascular care.

DOI: 10.3390/s25175272
PMCID: PMC12431230
PMID: 40942702 [Indexed for MEDLINE]

Conflict of interest statement: Authors Nurdaulet Tasmurzayev, Bibars Amangeldy, 
Timur Imankulov and Gulmira Dikhanbayeva were employed by the company LLP 
“DigitAlem”. Author Zhanel Baigarayeva was employed by the company LLP 
“Kazakhstan R&D Solutions”. The remaining authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


969. Healthcare (Basel). 2025 Aug 4;13(15):1898. doi: 10.3390/healthcare13151898.

Leveraging and Harnessing Generative Artificial Intelligence to Mitigate the 
Burden of Neurodevelopmental Disorders (NDDs) in Children.

Oleribe OO(1)(2).

Author information:
(1)Department of Health Sciences, School of Public Health and Health Sciences, 
California State University, Dominguez Hills, Carson, CA 90747, USA.
(2)Center for Family Health Initiative (CFHI), Orange, CA 92865, USA.

Neurodevelopmental disorders (NDDs) significantly impact children's health and 
development. They pose a substantial burden to families and the healthcare 
system. Challenges in early identification, accurate and timely diagnosis, and 
effective treatment persist due to overlapping symptoms, lack of appropriate 
diagnostic biomarkers, significant stigma and discrimination, and systemic 
barriers. Generative Artificial Intelligence (GenAI) offers promising solutions 
to these challenges by enhancing screening, diagnosis, personalized treatment, 
and research. Although GenAI is already in use in some aspects of NDD 
management, effective and strategic leveraging of evolving AI tools and 
resources will enhance early identification and screening, reduce diagnostic 
processing by up to 90%, and improve clinical decision support. Proper use of 
GenAI will ensure individualized therapy regimens with demonstrated 36% 
improvement in at least one objective attention measure compared to baseline and 
81-84% accuracy relative to clinician-generated plans, customize learning 
materials, and deliver better treatment monitoring. GenAI will also accelerate 
NDD-specific research and innovation with significant time savings, as well as 
provide tailored family support systems. Finally, it will significantly reduce 
the mortality and morbidity associated with NDDs. This article explores the 
potential of GenAI in transforming NDD management and calls for policy 
initiatives to integrate GenAI into NDD management systems.

DOI: 10.3390/healthcare13151898
PMCID: PMC12346636
PMID: 40805930

Conflict of interest statement: The author declares no conflicts of interest.


970. Diagnostics (Basel). 2025 Aug 20;15(16):2094. doi: 10.3390/diagnostics15162094.

From Description to Diagnostics: Assessing AI's Capabilities in Forensic Gunshot 
Wound Classification.

Sessa F(1), Guardo E(1), Esposito M(2), Chisari M(2), Di Mauro L(1), Salerno 
M(1), Pomara C(1).

Author information:
(1)Department of Medical, Surgical and Advanced Technologies "G.F. Ingrassia", 
University of Catania, 95121 Catania, Italy.
(2)Faculty of Medicine and Surgery, "Kore" University of Enna, 94100 Enna, 
Italy.

Background/Objectives: The integration of artificial intelligence (AI) into 
forensic science is expanding, yet its application in firearm injury diagnostics 
remains underexplored. This study investigates the diagnostic capabilities of 
ChatGPT-4 (February 2024 update) in classifying gunshot wounds, specifically 
distinguishing entrance from exit wounds, and evaluates its potential, 
limitations, and forensic applicability. Methods: ChatGPT-4 was tested using 
three datasets: (1) 36 firearm injury images from an external database, (2) 40 
images of intact skin from the forensic archive of the University of Catania 
(negative control), and (3) 40 real-case firearm injury images from the same 
archive. The AI's performance was assessed before and after machine learning 
(ML) training, with classification accuracy evaluated through descriptive and 
inferential statistics. Results: ChatGPT-4 demonstrated a statistically 
significant improvement in identifying entrance wounds post-ML training, with 
enhanced descriptive accuracy of morphological features. However, its 
performance in classifying exit wounds remained limited, reflecting challenges 
noted in forensic literature. The AI showed high accuracy (95%) in 
distinguishing intact skin from injuries in the negative control analysis. A 
lack of standardized datasets and contextual forensic information contributed to 
misclassification, particularly for exit wounds. Conclusions: While ChatGPT-4 is 
not yet a substitute for specialized forensic deep learning models, its 
iterative learning capacity and descriptive improvements suggest potential as a 
supplementary diagnostic tool in forensic pathology. However, risks such as 
overconfident misclassifications and AI-generated hallucinations highlight the 
need for expert oversight and cautious integration in forensic workflows. Future 
research should prioritize dataset expansion, contextual data integration, and 
standardized validation protocols to enhance AI reliability in medico-legal 
diagnostics.

DOI: 10.3390/diagnostics15162094
PMCID: PMC12385544
PMID: 40870946

Conflict of interest statement: The authors declare no conflicts of interest.


971. Cancers (Basel). 2025 Jun 20;17(13):2073. doi: 10.3390/cancers17132073.

Evaluating the Accuracy of Privacy-Preserving Large Language Models in 
Calculating the Spinal Instability Neoplastic Score (SINS).

Chan LYT(1), Chan DZM(1), Tan YL(1), Yap QV(2), Ong W(1), Lee A(1), Ge S(1), 
Leow WN(3), Makmur A(1), Ting Y(1), Teo EC(1), Jiong Hao T(4), Kumar N(4), 
Hallinan JTPD(1).

Author information:
(1)Department of Diagnostic Imaging, National University Hospital, Singapore 
119074, Singapore.
(2)Biostatistics Unit, Yong Loo Lin School of Medicine, Singapore 117597, 
Singapore.
(3)AIO Innovation Office, National University Health System, Singapore 119228, 
Singapore.
(4)University Spine Centre, University Orthopaedics, Hand and Reconstructive 
Microsurgery, National University Health System, Singapore 119074, Singapore.

Background: Large language models (LLMs) have emerged as powerful tools in 
healthcare. In diagnostic radiology, LLMs can assist in the computation of the 
Spine Instability Neoplastic Score (SINS), which is a critical tool for 
assessing spinal metastases. However, the accuracy of LLMs in calculating the 
SINS based on radiological reports remains underexplored. Objective: This study 
evaluates the accuracy of two institutional privacy-preserving LLMs-Claude 3.5 
and Llama 3.1-in computing the SINS from radiology reports and electronic 
medical records, comparing their performance against clinician readers. Methods: 
A retrospective analysis was conducted on 124 radiology reports from patients 
with spinal metastases. Three expert readers established a reference standard 
for the SINS calculation. Two orthopaedic surgery residents and two LLMs (Claude 
3.5 and Llama 3.1) independently calculated the SINS. The intraclass correlation 
coefficient (ICC) was used to measure the inter-rater agreement for the total 
SINS, while Gwet's Kappa was used to measure the inter-rater agreement for the 
individual SINS components. Results: Both LLMs and clinicians demonstrated 
almost perfect agreement with the reference standard for the total SINS. Between 
the two LLMs, Claude 3.5 (ICC = 0.984) outperformed Llama 3.1 (ICC = 0.829). 
Claude 3.5 was also comparable to the clinician readers with ICCs of 0.926 and 
0.986, exhibiting a near-perfect agreement across all individual SINS components 
[0.919-0.990]. Conclusions: Claude 3.5 demonstrated high accuracy in calculating 
the SINS and may serve as a valuable adjunct in clinical workflows, potentially 
reducing clinician workload while maintaining diagnostic reliability. However, 
variations in LLM performance highlight the need for further validation and 
optimisation before clinical integration.

DOI: 10.3390/cancers17132073
PMCID: PMC12248813
PMID: 40647373

Conflict of interest statement: The authors declare that they have no known 
competing financial interests or personal relationships that could have appeared 
to influence the work reported in this paper.


972. Diagnostics (Basel). 2025 Dec 16;15(24):3219. doi: 10.3390/diagnostics15243219.

Reliability of Large Language Model-Based Artificial Intelligence in AIS 
Assessment: Lenke Classification and Fusion-Level Suggestion.

Aktan C(1), Koşar A(1), Ünal M(1), Korkmaz M(2), Kaya Ö(3), Akgül T(2), Güler 
F(1).

Author information:
(1)Department of Orthopedics and Traumatology, Antalya Training and Research 
Hospital, Antalya 07100, Turkey.
(2)Department of Orthopedics and Traumatology, Faculty of Medicine, Istanbul 
University, Istanbul 34093, Turkey.
(3)Department of Orthopedics and Traumatology, İstinye University Medical Park 
TEM Hospital, Istanbul 34250, Turkey.

Background: Accurate deformity classification and fusion-level planning are 
essential in adolescent idiopathic scoliosis (AIS) surgery and are traditionally 
guided by Cobb angle measurement and the Lenke system. Multimodal large language 
models (LLMs) (e.g., ChatGPT-4.0; Claude 3.7 Sonnet, Gemini 2.5 Pro, 
DeepSeek-R1-0528 Chat) are increasingly used for image interpretation despite 
limited validation for radiographic decision-making. This study evaluated the 
agreement and reproducibility of contemporary multimodal LLMs for AIS assessment 
compared with expert spine surgeons. Methods: This single-center retrospective 
study included 125 AIS patients (94 females, 31 males; mean age 14.8 ± 1.9 
years) who underwent posterior instrumentation (2020-2024). Two experienced 
spine surgeons independently performed Lenke classification (including lumbar 
and sagittal modifiers) and selected fusion levels (UIV-LIV) on standing AP, 
lateral, and side-bending radiographs; discrepancies were resolved by consensus 
to establish the reference standard. The same radiographs were analyzed by four 
paid multimodal LLMs using standardized zero-shot prompts. Because LLMs showed 
inconsistent end-vertebra selection, LLM-derived Cobb angles lacked a common 
anatomical reference frame and were excluded from quantitative analysis. 
Agreement with expert consensus and test-retest reproducibility (repeat analyses 
one week apart) were assessed using Cohen's κ. Evaluation times were recorded. 
Results: Surgeon agreement was high for Lenke classification (92.0%, κ = 0.913) 
and fusion-level selection (88.8%, κ = 0.879). All LLMs demonstrated 
chance-level test-retest reproducibility and very low agreement with expert 
consensus (Lenke: 1.6-10.2%, κ = 0.001-0.036; fusion: 0.8-12.0%, κ = 
0.003-0.053). Claude produced missing outputs in 17 Lenke and 29 fusion-level 
cases. Although LLMs completed assessments far faster than surgeons (seconds vs. 
~11-12 min), speed did not translate into clinically acceptable reliability. 
Conclusions: Current general-purpose multimodal LLMs do not provide reliable 
Lenke classification or fusion-level planning in AIS. Their poor agreement with 
expert surgeons and marked internal inconsistency indicate that LLM-generated 
interpretations should not be used for surgical decision-making or patient 
self-assessment without task-specific validation.

DOI: 10.3390/diagnostics15243219
PMCID: PMC12731402
PMID: 41464219

Conflict of interest statement: The authors declare no conflict of interest.


973. Sensors (Basel). 2025 Apr 4;25(7):2295. doi: 10.3390/s25072295.

An End-to-End General Language Model (GLM)-4-Based Milling Cutter Fault 
Diagnosis Framework for Intelligent Manufacturing.

He J(1), Liu X(2), Lei Y(3), Cao A(3), Xiong J(1).

Author information:
(1)School of Management Science and Engineering, Southwestern University of 
Finance and Economics, Chengdu 611130, China.
(2)Institute of Chinese Financial Studies, Southwestern University of Finance 
and Economics, Chengdu 611130, China.
(3)School of Mechanical Engineering, Southwest Jiaotong University, Chengdu 
610031, China.

CNC machine and cutting tools are an indispensable part of the cutting process. 
Their life default diagnosis is related to the efficiency of the entire 
production process, which ultimately impacts economic performance. Many methods 
provided by deep learning articles have been verified for use on large cutting 
datasets and can help in diagnosing tools' lifetime well; however, on small 
samples, the challenge of learning difficulties still emerges. The rise in large 
language models (LLMs) has brought changes to tool life diagnosis. This study 
proposes a fault diagnosis algorithm based on GLM-4, and the experimental 
validation on the PHM 2010 dataset and a proprietary milling cutter dataset 
demonstrates the superiority of the proposed model, achieving diagnostic 
accuracies of 93.8% and 93.3%, respectively, outperforming traditional models 
(SVM, CNN, RNN) and baseline LLMs (ChatGLM2-6B variants). Further robustness and 
noise-resistance analyses confirm its stability under varying SNR levels (10 dB 
to -10 dB) and limited training samples. This work highlights the potential of 
integrating domain-specific feature engineering with LLMs to advance intelligent 
manufacturing diagnostics.

DOI: 10.3390/s25072295
PMCID: PMC11991448
PMID: 40218806

Conflict of interest statement: The authors declare no competing interests.


974. Diagnostics (Basel). 2025 Nov 12;15(22):2871. doi: 10.3390/diagnostics15222871.

StressSpeak: A Speech-Driven Framework for Real-Time Personalized Stress 
Detection and Adaptive Psychological Support.

Umer L(1)(2)(3), Iqbal J(1), Ayaz Y(1)(2), Imam H(2), Ahmad A(2), Asgher 
U(2)(4)(5).

Author information:
(1)School of Mechanical and Manufacturing Engineering, National University of 
Sciences and Technology, Islamabad 44000, Pakistan.
(2)National Center of Artificial Intelligence, National University of Sciences 
and Technology, Islamabad 44000, Pakistan.
(3)Department of Optometry and Orthoptics, Faculty of Rehabilitation and Allied 
Health Sciences, Riphah International University, Islamabad 44000, Pakistan.
(4)School of Interdisciplinary Engineering and Sciences (SINES), National 
University of Sciences and Technology (NUST), Islamabad 44000, Pakistan.
(5)Laboratory of Human Factors and Automation in Aviation, Department of Air 
Transport, Faculty of Transportation Sciences, Czech Technical University in 
Prague (CTU), 128 00 Prague, Czech Republic.

Background: Stress is a critical determinant of mental health, yet conventional 
monitoring approaches often rely on subjective self-reports or physiological 
signals that lack real-time responsiveness. Recent advances in large language 
models (LLMs) offer opportunities for speech-driven, adaptive stress detection, 
but existing systems are limited to retrospective text analysis, monolingual 
settings, or detection-only outputs. Methods: We developed a real-time, 
speech-driven stress detection framework that integrates audio recording, 
speech-to-text conversion, and linguistic analysis using transformer-based LLMs. 
The system provides multimodal outputs, delivering recommendations in both text 
and synthesized speech. Nine LLM variants were evaluated on five benchmark 
datasets under zero-shot and few-shot learning conditions. Performance was 
assessed using accuracy, precision, recall, F1-score, and misclassification 
trends (false-negatives and false-positives). Real-time feasibility was analyzed 
through latency modeling, and user-centered validation was conducted across 
cross-domains. Results: Few-shot fine-tuning improved model performance across 
all datasets, with Large Language Model Meta AI (LLaMA) and Robustly Optimized 
BERT Pretraining Approach (RoBERTa) achieving the highest F1-scores and reduced 
false-negatives, particularly for suicide risk detection. Latency analysis 
revealed a trade-off between responsiveness and accuracy, with delays ranging 
from ~2 s for smaller models to ~7.6 s for LLaMA-7B on 30 s audio inputs. 
Multilingual input support and multimodal output enhanced inclusivity. User 
feedback confirmed strong usability, accessibility, and adoption potential in 
real-world settings. Conclusions: This study demonstrates that real-time, 
LLM-powered stress detection is both technically robust and practically 
feasible. By combining speech-based input, multimodal feedback, and 
user-centered validation, the framework advances beyond traditional detection 
only models toward scalable, inclusive, and deployment-ready digital mental 
health solutions.

DOI: 10.3390/diagnostics15222871
PMCID: PMC12651002
PMID: 41300895

Conflict of interest statement: The authors declare that there is no conflict of 
interest regarding the publication of this paper.


975. Healthcare (Basel). 2025 Dec 24;14(1):45. doi: 10.3390/healthcare14010045.

Large Language Models for Cardiovascular Disease, Cancer, and Mental Disorders: 
A Review of Systematic Reviews.

Triantafyllidis A(1), Segkouli S(1), Kokkas S(1), Alexiadis A(1), Lithoxoidou 
EE(1), Manias G(2), Antoniades A(3), Votis K(1), Tzovaras D(1).

Author information:
(1)Information Technologies Institute, Centre for Research and Technology 
Hellas, 57001 Thessaloniki, Greece.
(2)Department of Digital Systems, University of Piraeus, 18534 Piraeus, Greece.
(3)Stremble Ventures Ltd., 4042 Limassol, Cyprus.

Background/Objective: The use of Large Language Models (LLMs) has recently 
gained significant interest from the research community toward the development 
and adoption of Generative Artificial Intelligence (GenAI) solutions for 
healthcare. The present work introduces the first meta-review (i.e., review of 
systematic reviews) in the field of LLMs for chronic diseases, focusing 
particularly on cardiovascular, cancer, and mental diseases, to identify their 
value in patient care, and challenges for their implementation and clinical 
application. Methods: A literature search in the bibliographic databases of 
PubMed and Scopus was conducted following the Preferred Reporting Items for 
Systematic Reviews and Meta-Analyses (PRISMA) guidelines, to identify systematic 
reviews incorporating LLMs. The original studies included in the reviews were 
synthesized according to their target disease, specific application, LLMs used, 
data sources, accuracy, and key outcomes. Results: The literature search 
identified 5 systematic reviews respecting our inclusion and exclusion criteria, 
which examined 81 unique LLM-based solutions. The highest percentage of the 
solutions targeted mental disease (86%), followed by cancer (7%) and 
cardiovascular disease (6%), implying a large research focus in mental health. 
Generative Pre-trained Transformer (GPT)-family models were used most frequently 
(~55%), followed by Bidirectional Encoder Representations from Transformers 
(BERT) variants (~40%). Key application areas included depression detection and 
classification (38%), suicidal ideation detection (7%), question answering based 
on treatment guidelines and recommendations (7%), and emotion classification 
(5%). Study aims and designs were highly heterogeneous, and methodological 
quality was generally moderate with frequent risk-of-bias concerns. Reported 
performance varied widely across domains and datasets, and many evaluations 
relied on fictional vignettes or non-representative data, limiting 
generalisability. The most significant found challenges in the development and 
evaluation of LLMs include inconsistent accuracy, bias detection and mitigation, 
model transparency, data privacy, need for continual human oversight, ethical 
concerns and guidelines, as well as the design and conduction of high-quality 
studies. Conclusions: While LLMs show promise for screening, triage, decision 
support, and patient education-particularly in mental health-the current 
literature is descriptive and constrained by data, transparency, and safety 
gaps. We recommend prioritizing rigorous real-world evaluations, diverse 
benchmark datasets, bias-auditing, and governance frameworks before LLM clinical 
deployment and large adoption.

DOI: 10.3390/healthcare14010045
PMCID: PMC12785959
PMID: 41516976

Conflict of interest statement: Author Athos Antoniades was employed by the 
company Stremble Ventures Ltd. The remaining authors declare that the research 
was conducted in the absence of any commercial or financial relationships that 
could be construed as a potential conflict of interest. The funders had no role 
in the design of the study; in the collection, analyses, or interpretation of 
data; in the writing of the manuscript; or in the decision to publish the 
results.


976. Diagnostics (Basel). 2025 Apr 30;15(9):1146. doi: 10.3390/diagnostics15091146.

Enhancing Radiologist Productivity with Artificial Intelligence in Magnetic 
Resonance Imaging (MRI): A Narrative Review.

Nair A(1), Ong W(1), Lee A(1), Leow NW(2), Makmur A(1)(3), Ting YH(1)(3), Lee 
YJ(1), Ong SJ(1)(3), Tan JJH(4), Kumar N(4), Hallinan JTPD(1)(3).

Author information:
(1)Department of Diagnostic Imaging, National University Hospital, 5 Lower Kent 
Ridge Rd, Singapore 119074, Singapore.
(2)AIO Innovation Office, National University Health System, 3 Research Link, 
#02-04 Innovation 4.0, Singapore 117602, Singapore.
(3)Department of Diagnostic Radiology, Yong Loo Lin School of Medicine, National 
University of Singapore, 10 Medical Drive, Singapore 117597, Singapore.
(4)National University Spine Institute, Department of Orthopaedic Surgery, 
National University Health System, 1E Lower Kent Ridge Road, Singapore 119228, 
Singapore.

Artificial intelligence (AI) shows promise in streamlining MRI workflows by 
reducing radiologists' workload and improving diagnostic accuracy. Despite MRI's 
extensive clinical use, systematic evaluation of AI-driven productivity gains in 
MRI remains limited. This review addresses that gap by synthesizing evidence on 
how AI can shorten scanning and reading times, optimize worklist triage, and 
automate segmentation. On 15 November 2024, we searched PubMed, EMBASE, MEDLINE, 
Web of Science, Google Scholar, and Cochrane Library for English-language 
studies published between 2000 and 15 November 2024, focusing on AI applications 
in MRI. Additional searches of grey literature were conducted. After screening 
for relevance and full-text review, 67 studies met inclusion criteria. Extracted 
data included study design, AI techniques, and productivity-related outcomes 
such as time savings and diagnostic accuracy. The included studies were 
categorized into five themes: reducing scan times, automating segmentation, 
optimizing workflow, decreasing reading times, and general time-saving or 
workload reduction. Convolutional neural networks (CNNs), especially 
architectures like ResNet and U-Net, were commonly used for tasks ranging from 
segmentation to automated reporting. A few studies also explored machine 
learning-based automation software and, more recently, large language models. 
Although most demonstrated gains in efficiency and accuracy, limited external 
validation and dataset heterogeneity could reduce broader adoption. AI 
applications in MRI offer potential to enhance radiologist productivity, mainly 
through accelerated scans, automated segmentation, and streamlined workflows. 
Further research, including prospective validation and standardized metrics, is 
needed to enable safe, efficient, and equitable deployment of AI tools in 
clinical MRI practice.

DOI: 10.3390/diagnostics15091146
PMCID: PMC12071790
PMID: 40361962

Conflict of interest statement: The authors declare no conflicts of interest.


977. Healthcare (Basel). 2025 Mar 10;13(6):603. doi: 10.3390/healthcare13060603.

A Review of Large Language Models in Medical Education, Clinical Decision 
Support, and Healthcare Administration.

Vrdoljak J(1), Boban Z(2), Vilović M(1), Kumrić M(1), Božić J(1).

Author information:
(1)Department for Pathophysiology, School of Medicine, University of Split, 
21000 Split, Croatia.
(2)Department for Medical Physics, School of Medicine, University of Split, 
21000 Split, Croatia.

Background/Objectives: Large language models (LLMs) have shown significant 
potential to transform various aspects of healthcare. This review aims to 
explore the current applications, challenges, and future prospects of LLMs in 
medical education, clinical decision support, and healthcare administration. 
Methods: A comprehensive literature review was conducted, examining the 
applications of LLMs across the three key domains. The analysis included their 
performance, challenges, and advancements, with a focus on techniques like 
retrieval-augmented generation (RAG). Results: In medical education, LLMs show 
promise as virtual patients, personalized tutors, and tools for generating study 
materials. Some models have outperformed junior trainees in specific medical 
knowledge assessments. Concerning clinical decision support, LLMs exhibit 
potential in diagnostic assistance, treatment recommendations, and medical 
knowledge retrieval, though performance varies across specialties and tasks. In 
healthcare administration, LLMs effectively automate tasks like clinical note 
summarization, data extraction, and report generation, potentially reducing 
administrative burdens on healthcare professionals. Despite their promise, 
challenges persist, including hallucination mitigation, addressing biases, and 
ensuring patient privacy and data security. Conclusions: LLMs have 
transformative potential in medicine but require careful integration into 
healthcare settings. Ethical considerations, regulatory challenges, and 
interdisciplinary collaboration between AI developers and healthcare 
professionals are essential. Future advancements in LLM performance and 
reliability through techniques such as RAG, fine-tuning, and reinforcement 
learning will be critical to ensuring patient safety and improving healthcare 
delivery.

DOI: 10.3390/healthcare13060603
PMCID: PMC11942098
PMID: 40150453

Conflict of interest statement: The authors declare no conflicts of interest.


978. Sensors (Basel). 2025 Oct 24;25(21):6560. doi: 10.3390/s25216560.

Context-Aware Alerting in Elderly Care Facilities: A Hybrid Framework 
Integrating LLM Reasoning with Rule-Based Logic.

Nahid N(1), Ahad MAR(2), Inoue S(1).

Author information:
(1)Graduate School of Life Science and Systems Engineering, Kyushu Institute of 
Technology, 2-4 Hibikino, Wakamatsu Ward, Kitakyushu 808-0135, Japan.
(2)Department of Engineering & Computing, School of Architecture Computing and 
Engineering, University of East London, London E16 2RD, UK.

The rising demand for elderly care amid ongoing nursing shortages has 
highlighted the limitations of conventional alert systems, which frequently 
generate excessive alerts and contribute to alarm fatigue. The objective of this 
study is to develop a hybrid, context-aware nurse alerting framework for 
long-term care (LTC) facilities that minimizes redundant alarms, reduces alarm 
fatigue, and enhances patient safety and caregiving balance during multi-person 
care scenarios such as mealtimes. To do so, we aimed to intelligently suppress, 
delay, and validate alerts by integrating rule-based logic with Large Language 
Model (LLM)-driven semantic reasoning. We conducted an experimental study in a 
real-world LTC environment involving 28 elderly residents (6 high, 8 medium, and 
14 low care levels) and four nurses across three rooms over seven days. The 
proposed system utilizes video-derived skeletal motion, care-level annotations, 
and dynamic nurse-elderly proximity for decision making. Statistical analyses 
were performed using F1 score, accuracy, false positive rate (FPR), and false 
negative rate (FNR) to evaluate performance improvements. Compared to the 
baseline where all nurses were notified (100% alarm load), the proposed method 
reduced average alarm load to 27.5%, achieving a 72.5% reduction, with 
suppression rates reaching 100% in some rooms for some nurses. Performance 
metrics further validate the system's effectiveness: the macro F1 score improved 
from 0.18 (baseline) to 0.97, while accuracy rose from 0.21 (baseline) to 0.98. 
Compared to the baseline error rates (FPR 0.20, FNR 0.79), the proposed method 
achieved drastically lower values (FPR 0.005, FNR 0.023). Across both spatial 
(room-level) and temporal (day-level) validations, the proposed approach 
consistently outperformed baseline and purely rule-based methods. These findings 
demonstrate that the proposed approach effectively minimizes false alarms while 
maintaining strong operational efficiency. By integrating rule-based mechanisms 
with LLM-based contextual reasoning, the framework significantly enhances alert 
accuracy, mitigates alarm fatigue, and promotes safer, more sustainable, and 
human-centered care practices, making it suitable for practical deployment 
within real-world long-term care environments.

DOI: 10.3390/s25216560
PMCID: PMC12608296
PMID: 41228783 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that they have no known 
competing financial interests or personal relationships that could have 
influenced the work reported in this study.


979. Children (Basel). 2025 Nov 26;12(12):1608. doi: 10.3390/children12121608.

Seeing Beyond the Microscope: Artificial Intelligence and Fluorescence Confocal 
Digital Imaging in Pediatric Surgical Pathology.

Di Fabrizio D(1), Daziani G(2), Qose I(3), Bindi E(1)(4), Ilari M(1), Filosa 
A(3), Busardò FP(2), Goteri G(3), Cobellis G(1)(4).

Author information:
(1)Pediatric Surgery Unit, Salesi Children's Hospital, Polytechnic University of 
Marche, 60123 Ancona, Italy.
(2)Section of Legal Medicine, Department of Biomedical Sciences and Public 
Health, Polytechnic University of Marche, 60126 Ancona, Italy.
(3)Department of Biomedical Sciences and Public Health, Institute of 
Pathological Anatomy, Polytechnic University of Marche, 60126 Ancona, Italy.
(4)Department of Specialized Clinical and Odontostomatological Sciences, 
Polytechnic University of Marche, 60126 Ancona, Italy.

Background: Digital pathology (DP) combined with fluorescence confocal 
microscopy (FCM) allows rapid tissue assessment while preserving specimens. 
Artificial intelligence (AI) and large language models (LLMs) may enhance 
diagnostic workflows, but their role in pediatric surgical pathology is largely 
unexplored. Methods: We conducted a prospective, single-center study including 
20 pediatric surgical cases with ex vivo FCM images acquired intraoperatively. 
Two commercially available LLMs, GPT-4V (AnPathology-Gpt) and Claude 3.7 Sonnet 
(AnPathology Project), were tested using structured prompts to generate 
diagnostic reports with and without immunohistochemistry (IHC) data, when 
available. Outputs were compared against the gold standard diagnosis by an 
experienced pediatric pathologist. Diagnostic performance was evaluated through 
accuracy, sensitivity, specificity, and Cohen's kappa. A paired sub-analysis was 
performed for cases with IHC (n = 5), and a sensitivity analysis excluding IHC 
cases (n = 15) was conducted. Results: Across all 20 cases, AnPathology-Gpt 
achieved 85% accuracy, 100% sensitivity, 86% specificity, and κ = 0.78, while 
AnPathology Project reached 80% accuracy, 100% sensitivity, 57% specificity, and 
κ = 0.63. Both models correctly identified all 13 neoplastic cases, with errors 
limited to non-neoplastic lesions mimicking tumors. In the IHC sub-analysis, 
accuracy improved from 40% to 80% and sensitivity from 50% to 100% for both 
models, resolving two false negatives observed in the FCM-only evaluation. 
Sensitivity analysis excluding IHC confirmed consistency of the results. 
Conclusions: This pilot study demonstrates that multimodal LLMs can support 
accurate and rapid diagnosis in pediatric digital pathology. The addition of IHC 
improves performance in diagnostically complex cases. Larger multicenter studies 
are needed to validate these findings and to define the role of AI-assisted 
workflows in pediatric surgical pathology.

DOI: 10.3390/children12121608
PMCID: PMC12731743
PMID: 41462749

Conflict of interest statement: The authors declare no conflict of interest.


980. Front Endocrinol (Lausanne). 2025 Oct 29;16:1684173. doi: 
10.3389/fendo.2025.1684173. eCollection 2025.

Clinical efficacy evaluation of washed microbiota transplantation treatment for 
metabolic related fatty liver disease and its impact on tongue coating 
microorganisms.

Huang L(#)(1), Wang S(#)(1)(2), Zhang H(#)(1), Feng S(1), Zhong H(1)(3), Chen 
J(1), Xie W(1), Wu L(1), Zhang T(3), He X(1), Yang J(1)(3).

Author information:
(1)Department of Gastroenterology, The First Affiliated Hospital of Guangdong 
Pharmaceutical University, Guangzhou, Guangdong,, China.
(2)Sichuan Integrative Medicine Hospital, Chengdu, Sichuan, China.
(3)Guangdong Provincial Key Laboratory of Microbial Safety and Health, State Key 
Laboratory of Applied Microbiology Southern China, Institute of Microbiology, 
Guangdong Academy of Sciences, Guangzhou, China.
(#)Contributed equally

OBJECTIVE: The present study aims to explore the impact of washed microbiota 
transplantation (WMT) on the tongue microbiota composition of individuals with 
metabolic-associated fatty liver disease (MAFLD) and elucidate its biological 
correlations.
METHODS: We conducted a comprehensive analysis of hepatic fat deposition and 
characterized the tongue coating microbiota using 16S rRNA gene sequencing in 
MAFLD patients before and after undergoing WMT treatment. Furthermore, a MAFLD 
mouse model was established for additional validation.
RESULTS: At the genus level, significant differences in tongue coating 
microbiota structure were observed between MAFLD patients and HC. Specifically, 
Neisseria positively correlated with the BARD score, Porphyromonas and 
Rhodococcus positively correlated with fat decay, and Petostreptococcus, a 
conditionally pathogenic bacterium, exhibited a significantly higher relative 
abundance in MAFLD patients compared to HC. Conversely, Actinomyces positively 
correlated with the FIB-4 score, Megasphaera negatively correlated with the APRI 
score, and Subdoligulum negatively correlated with low-density lipoprotein 
levels. Notably, following effective WMT treatment, patients exhibited improved 
symptoms, with a significant reduction in the relative abundance of 
Petostreptococcus and an increase in potential probiotics such as 
Lachnospiraceae and Bifidobacterium in their tongue coating microbiota. 
Additionally, structural differences in the tongue coating microbiota were 
identified at the genus level between MAFLD model mice and HC mice. After WMT 
treatment, the relative abundance of conditionally pathogenic bacteria like 
Enterococcus was significantly decreased in MAFLD model mice.
CONCLUSIONS: WMT not only significantly ameliorates liver fat deposition in 
MAFLD patients but also alters the tongue coating microbial structure associated 
with disease severity, thereby potentially mitigating adverse patient outcomes.

Copyright © 2025 Huang, Wang, Zhang, Feng, Zhong, Chen, Xie, Wu, Zhang, He and 
Yang.

DOI: 10.3389/fendo.2025.1684173
PMCID: PMC12605323
PMID: 41234231 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


981. Sensors (Basel). 2025 Jun 26;25(13):3987. doi: 10.3390/s25133987.

Towards Predictive Communication: The Fusion of Large Language Models and 
Brain-Computer Interface.

Carìa A(1).

Author information:
(1)Department of Psychology and Cognitive Science, University of Trento, 38068 
Rovereto, Italy.

Integration of advanced artificial intelligence with neurotechnology offers 
transformative potential for assistive communication. This perspective article 
examines the emerging convergence between non-invasive brain-computer interface 
(BCI) spellers and large language models (LLMs), with a focus on predictive 
communication for individuals with motor or language impairments. First, I will 
review the evolution of language models-from early rule-based systems to 
contemporary deep learning architectures-and their role in enhancing predictive 
writing. Second, I will survey existing implementations of BCI spellers that 
incorporate language modeling and highlight recent pilot studies exploring the 
integration of LLMs into BCI. Third, I will examine how, despite advancements in 
typing speed, accuracy, and user adaptability, the fusion of LLMs and BCI 
spellers still faces key challenges such as real-time processing, robustness to 
noise, and the integration of neural decoding outputs with probabilistic 
language generation frameworks. Finally, I will discuss how fully integrating 
LLMs with BCI technology could substantially improve the speed and usability of 
BCI-mediated communication, offering a path toward more intuitive, adaptive, and 
effective neurotechnological solutions for both clinical and non-clinical users.

DOI: 10.3390/s25133987
PMCID: PMC12252171
PMID: 40648241 [Indexed for MEDLINE]

Conflict of interest statement: The author declares no conflict of interest.


982. Sci Rep. 2025 Nov 10;15(1):39231. doi: 10.1038/s41598-025-22402-7.

Large language model performance versus human expert ratings in automated 
suicide risk assessment.

Thomas J(1)(2)(3), Elyoseph Z(4)(5), Kuchinke L(6), Meinlschmidt G(7)(8)(9)(10).

Author information:
(1)krisenchat gGmbH, Berlin, Germany.
(2)Division of Clinical Psychology and Epidemiology, Department of Psychology, 
University of Basel, Basel, Switzerland.
(3)Division of Clinical Psychology and Cognitive Behavioural Therapy, 
International Psychoanalytic University (IPU) Berlin, Berlin, Germany.
(4)Department of Counseling and Human Development, Faculty of Education, 
University of Haifa, Haifa, Israel.
(5)Imperial College London, London, UK.
(6)Psychological Methods and Evaluation, International Psychoanalytic University 
(IPU) Berlin, Berlin, Germany.
(7)Division of Clinical Psychology and Cognitive Behavioural Therapy, 
International Psychoanalytic University (IPU) Berlin, Berlin, Germany. 
meinlschmidt@uni-trier.de.
(8)Department of Clinical Psychology and Psychotherapy - Methods and Approaches, 
Trier University, 54286, Trier, Germany. meinlschmidt@uni-trier.de.
(9)Department of Digital and Blended Psychosomatics and Psychotherapy, 
Psychosomatic Medicine, University Hospital and University of Basel, Basel, 
Switzerland. meinlschmidt@uni-trier.de.
(10)Department of Psychosomatic Medicine, University Hospital and University of 
Basel, Basel, Switzerland. meinlschmidt@uni-trier.de.

Large Language Models' (LLMs) potential for psychological diagnostics requires 
systematic evaluation. We aimed to investigate conditions for reliable and valid 
psychological assessments, focusing on suicide risk evaluation in clinical data 
by comparing LLM-generated ratings with human expert ratings across model 
configurations., analyzing 100 youth crisis text line conversation transcripts 
rated by four experts using the Nurses' Global Assessment of Suicide Scale 
(NGASR). Using Mixtral-8x7B-Instruct, we generated ratings across three 
temperature settings and prompting styles (zero-shot, few-shot, 
chain-of-thought). Across configurations we compared (a) 
inter-rating-reliability for AI-generated NGASR risk and sum scores, (b) 
LLM-to-human observer agreement regarding sum score, risk category, and item, 
using Krippendorff's α, (c) classification metrics of risk categories and 
individual items against human ratings. LLM configuration strongly influenced 
assessment reliability. Zero-shot prompting at temperature 0 yielded perfect 
inter-rating reliability (α = 1.00, 95% CI: [1-1] for high & very high risk), 
while few-shot prompting showed best human-AI agreement for very high risk 
(α = 0.78, 95% CI: [0.67-0.89]) and strongest classification performance 
(balanced accuracy 0.54-0.71). Lower temperatures consistently improved 
reliability and accuracy. However, critical clinical items showed poor validity. 
Our findings establish optimal conditions (zero temperature, task-specific 
prompting) for LLM-based psychological assessment. However, inconsistent 
clinical item performance and only moderate LLM-to-human observer agreement 
limit LLMs to initial screening rather than detailed assessment, requiring 
careful parameter control and validation.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-22402-7
PMCID: PMC12603332
PMID: 41213985 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: J.T is 
employed and receives a salary from krisenchat.This needs to move to the 
competing interests section. krisenchat had no impact on the design of this 
study and did not influence the collection, execution, analyses, interpretation 
of the data, or the decision to submit the article/contribution for 
publication. L.K. and Z.E. have no competing interests. G.M. received funding 
from the Stanley Thomas Johnson Stiftung & Gottfried und Julia 
Bangerter-Rhyner-Stiftung under projects no. PC 28/17 and PC 05/18, from 
Gesundheitsförderung Schweiz under project no. 18.191/K50001, from the Swiss 
Heart Foundation under project no. FF21101, from the Research Foundation of the 
International Psychoanalytic University (IPU) Berlin under projects no. 5087 and 
5217, from the Swiss National Science Foundation (SNSF) under project no. 
100014_135328, from the German Federal Ministry of Education and Research under 
budget item 68606 in the context of an evaluation project conducted amongst 
others in collaboration with krisenchat, from the Hasler Foundation under 
project No. 23004, in the context of a Horizon Europe project from the Swiss 
State Secretariat for Education, Research and lnnovation (SERI) under contract 
number 22.00094, and from Wings Health in the context of a proof-of-concept 
study. G.M. is a co-founder, and shareholder of Therayou AG, active in digital 
and blended mental healthcare. GM receives royalties from publishing companies 
as author, including a book published by Springer, and an honorarium from 
Lundbeck for speaking at a symposium. Furthermore, GM is compensated for 
providing psychotherapy to patients, acting as a supervisor, serving as a 
self-experience facilitator (‘Selbsterfahrungsleiter’), and for postgraduate 
training of psychotherapists and supervisors. Ethical approval: All methods in 
this study were carried out in accordance with relevant guidelines and 
regulations. All experimental protocols were approved by the Ethics Committee of 
the International Psychoanalytic University (IPU) Berlin (approval number: 
2023_08). Informed consent was obtained from all subjects through krisenchat’s 
terms of service, which explicitly state that user data may be used for research 
purposes without direct identification of individuals. All personally 
identifiable information was removed from chat transcripts during preprocessing. 
The study utilized existing data from the crisis helpline, and participants were 
not compensated as this was a secondary analysis of routine service data. 
Research was performed in accordance with the Declaration of Helsinki. Use of 
artificial intelligence toolsWe used artificial intelligence (AI)-based tools, 
including Claude and ChatGPT to support manuscript preparation. Further, we used 
publicly available search technologies, which we recognize likely utilise AI 
capabilities. We confirm that the contributions of AI were strictly in an 
assistive capacity. AI was not involved in conceptual tasks. Human oversight was 
continuously employed to ensure the accuracy of content and address any ethical 
concerns. Consent for publication: Krisenchat had no impact on the design of 
this study and did not influence the collection, execution, analyses, 
interpretation of the data, or the decision to submit the article/contribution 
for publication.


983. Clin Neuroradiol. 2025 Dec;35(4):827-835. doi: 10.1007/s00062-025-01538-z. Epub 
2025 Jul 31.

Information Extraction and Summarization for Neurovascular Consultations with 
GPT-4o: A Clinical Case Study.

Indrakanti AK(1)(2), Heierle JE(1)(2), Münger H(1)(2), Koch AT(1)(2), Kaiser 
P(2), Bach M(2), Fiehler J(3), Tsogkas I(1)(2), Guzman R(4), Mutke MA(5)(6), 
Psychogios M(1)(2).

Author information:
(1)Department of Diagnostic and Interventional Neuroradiology, Clinic of 
Radiology and Nuclear Medicine, University Hospital Basel, Petersgraben 4, 4031, 
Basel, Switzerland.
(2)Clinic of Radiology and Nuclear Medicine, University Hospital Basel, 
Petersgraben 4, 4031, Basel, Switzerland.
(3)Department of Neuroradiology, University Medical Center Hamburg-Eppendorf, 
Hamburg, Germany.
(4)Department of Neurosurgery, University Hospital Basel, Petersgraben 4, 4031, 
Basel, Switzerland.
(5)Department of Diagnostic and Interventional Neuroradiology, Clinic of 
Radiology and Nuclear Medicine, University Hospital Basel, Petersgraben 4, 4031, 
Basel, Switzerland. matthias.mutke@usb.ch.
(6)Clinic of Radiology and Nuclear Medicine, University Hospital Basel, 
Petersgraben 4, 4031, Basel, Switzerland. matthias.mutke@usb.ch.

PURPOSE: In outpatient settings, extensive patient records must frequently be 
reviewed under time constraints, making efficient extraction and summarization 
of key clinical information essential. Large language models (LLMs) are 
potentially useful for this task but require validation for clinical 
reliability. This study assesses OpenAI's GPT-4o for generating structured 
summaries to assist in neurovascular consultation preparation, aiming to 
increase efficiency by automating critical data extraction.
METHODS: A prospective study was conducted from May to August 2024 at a tertiary 
care hospital, involving a total of 70 patients. Structured summaries were 
generated by GPT-4o using a predefined template. Extracted data were categorized 
into aneurysm-specific details, imaging summaries, and patient-specific clinical 
factors. Accuracy and completeness were assessed by clinicians, with performance 
measured using precision, recall, specificity, and accuracy.
RESULTS: High accuracy (≥ 0.96) was measured across most categories. In 
aneurysm-and patient-specific data, extraction performance varied based on 
stability over time. Aneurysm location and other stable details were extracted 
consistently, while changes in aneurysm size and medication lists showed 
variations. In rare cases, aneurysm details were misattributed to a different 
aneurysm within the same patient. Imaging summaries were generally concise and 
clinically useful, though their effectiveness declined when summarizing multiple 
prior studies.
CONCLUSION: Neurovascular patient data was effectively structured by GPT-4o, 
demonstrating high accuracy with minimal errors. While occasional 
misattributions like outdated information were observed, reliable citation of 
sources facilitated easy verification. These findings support integrating 
LLM-generated summaries into neurovascular consultations, with further 
optimization needed for temporal data tracking and on-premise implementation to 
address privacy concerns.

© 2025. The Author(s).

DOI: 10.1007/s00062-025-01538-z
PMCID: PMC12552373
PMID: 40742451 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: 
A.K. Indrakanti, J.E. Heierle, H. Münger, A.T. Koch, P. Kaiser, M. Bach, 
J. Fiehler, I. Tsogkas, R. Guzman, M.A. MutkeX and M. Psychogios declare that 
they haves no competing interests. Ethical standards: This prospective study 
received an ethics waiver from the local institutional Review Board. (BASEC) All 
patient-related text was fully anonymized prior to use.


984. Diagnostics (Basel). 2025 Oct 9;15(19):2547. doi: 10.3390/diagnostics15192547.

Exploring AI's Potential in Papilledema Diagnosis to Support Dermatological 
Treatment Decisions in Rural Healthcare.

Shapiro J(1), Atlas M(2), Fridman N(3), Cohen I(4), Khamaysi Z(5)(6), Awwad 
M(7), Silverstein N(8), Kozlovsky T(9)(10), Maharshak I(9)(10).

Author information:
(1)Maccabi Healthcare Services, Tel Aviv 6817110, Israel.
(2)Business Administration Faculty, Ono Academic College, Kiryat Ono 5510701, 
Israel.
(3)NF Algorithms & AI, Tel Aviv 6248906, Israel.
(4)Rutgers School of Public Health, Rutgers University, Piscataway, NJ 08854, 
USA.
(5)Department of Dermatology, Rambam Health Care Campus, Haifa 3109601, Israel.
(6)The Bruce and Ruth Rappaport Faculty of Medicine, Technion-Israel Institute 
of Technology, Haifa 3525433, Israel.
(7)Tzafon Medical Center, Ophthalmology Unit, Tiberias 1521000, Israel.
(8)Technion Institute of Technology, Haifa 3200003, Israel.
(9)Department of Ophthalmology, Edith Wolfson Medical Center, Holon 5822012, 
Israel.
(10)Gray School of Medicine, Tel-Aviv University, Tel Aviv 6997801, Israel.

Background: Papilledema, an ophthalmic finding associated with increased 
intracranial pressure, is often induced by dermatological medications, including 
corticosteroids, isotretinoin, and tetracyclines. Early detection is crucial for 
preventing irreversible optic nerve damage, but access to ophthalmologic 
expertise is often limited in rural settings. Artificial intelligence (AI) may 
enable the automated and accurate detection of papilledema from fundus images, 
thereby supporting timely diagnosis and management. Objective: The primary 
objective of this study was to explore the diagnostic capability of ChatGPT-4o, 
a general large language model with multimodal input, in identifying papilledema 
from fundus photographs. For context, its performance was compared with a 
ResNet-based convolutional neural network (CNN) specifically fine-tuned for 
ophthalmic imaging, as well as with the assessments of two human 
ophthalmologists. The focus was on applications relevant to dermatological care 
in resource-limited environments. Methods: A dataset of 1094 fundus images (295 
papilledema, 799 normal) was preprocessed and partitioned into a training set 
and a test set. The ResNet model was fine-tuned using discriminative learning 
rates and a one-cycle learning rate policy. GPT-4o and two human evaluators (a 
senior ophthalmologist and an ophthalmology resident) independently assessed the 
test images. Diagnostic metrics including sensitivity, specificity, positive 
predictive value (PPV), negative predictive value (NPV), accuracy, and Cohen's 
Kappa, were calculated for each evaluator. Results: GPT-4o, when applied to 
papilledema detection, achieved an overall accuracy of 85.9% with substantial 
agreement beyond chance (Cohen's Kappa = 0.72), but lower specificity (78.9%) 
and positive predictive value (73.7%) compared to benchmark models. For context, 
the ResNet model, fine-tuned for ophthalmic imaging, reached near-perfect 
accuracy (99.5%, Kappa = 0.99), while two human ophthalmologists achieved 
accuracies of 96.0% (Kappa ≈ 0.92). Conclusions: This study explored the 
capability of GPT-4o, a large language model with multimodal input, for 
detecting papilledema from fundus photographs. GPT-4o achieved moderate 
diagnostic accuracy and substantial agreement with the ground truth, but it 
underperformed compared to both a domain-specific ResNet model and human 
ophthalmologists. These findings underscore the distinction between generalist 
large language models and specialized diagnostic AI: while GPT-4o is not 
optimized for ophthalmic imaging, its accessibility, adaptability, and rapid 
evolution highlight its potential as a future adjunct in clinical screening, 
particularly in underserved settings. These findings also underscore the need 
for validation on external datasets and real-world clinical environments before 
such tools can be broadly implemented.

DOI: 10.3390/diagnostics15192547
PMCID: PMC12523928
PMID: 41095766

Conflict of interest statement: Author Naomi Fridman was employed by the company 
NF Algorithms & AI. The remaining authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


985. Diagnostics (Basel). 2025 Jul 23;15(15):1851. doi: 10.3390/diagnostics15151851.

Advances in Periodontal Diagnostics: Application of MultiModal Language Models 
in Visual Interpretation of Panoramic Radiographs.

Camlet A(1), Kusiak A(1), Ossowska A(1), Świetlik D(2).

Author information:
(1)Department of Periodontology and Oral Mucosa Diseases, Medical University of 
Gdansk, Orzeszkowej 18 St., 80-208 Gdansk, Poland.
(2)Division of Biostatistics and Neural Networks, Medical University of Gdansk, 
Debinki 1 St., 80-211 Gdansk, Poland.

Background: Periodontitis is a multifactorial disease leading to the loss of 
clinical attachment and alveolar bone. The diagnosis of periodontitis involves a 
clinical examination and radiographic evaluation, including panoramic images. 
Panoramic radiographs are cost-effective methods widely used in periodontitis 
classification. The remaining bone height (RBH) is a parameter used to assess 
the alveolar bone level. Large language models are widely utilized in the 
medical sciences. ChatGPT, the leading conversational model, has recently been 
extended to process visual data. The aim of this study was to assess the 
effectiveness of the ChatGPT models 4.5, o1, o3 and o4-mini-high in RBH 
measurement and tooth counts in relation to dental professionals' evaluations. 
Methods: The analysis was based on 10 panoramic images, from which 252, 251, 246 
and 271 approximal sites were qualified for the RBH measurement (using the 
models 4.5, o1, o3 and o4-mini-high, respectively). Three examiners were asked 
to independently evaluate the RBH in approximal sites, while the tooth count was 
achieved by consensus. Subsequently, the results were compared with the ChatGPT 
outputs. Results: ChatGPT 4.5, ChatGPT o3 and ChatGPT o4-mini-high achieved 
substantial agreement with clinicians in the assessment of tooth counts (κ = 
0.65, κ = 0.66, κ = 0.69, respectively), while ChatGPT o1 achieved moderate 
agreement (κ = 0.52). In the context of RBH values, the ChatGPT models 
consistently exhibited a positive mean bias compared with the clinicians. 
ChatGPT 4.5 was reported to provide the lowest bias (+12 percentage points (pp) 
for the distal surfaces, width of the 95% CI for limits of agreement (LoAs) ~60 
pp; +11 pp for the mesial surfaces, LoA width ~54 pp). Conclusions: ChatGPT 4.5 
and ChatGPT o3 show potential in the assessment of tooth counts on a panoramic 
radiograph; however, their present level of accuracy is insufficient for 
clinical use. In the current stage of development, the ChatGPT models 
substantially overestimated the RBH values; therefore, they are not applicable 
for classifying periodontal disease.

DOI: 10.3390/diagnostics15151851
PMCID: PMC12345756
PMID: 40804816

Conflict of interest statement: The authors declare no conflicts of interest.


986. Life (Basel). 2025 Sep 1;15(9):1387. doi: 10.3390/life15091387.

From Bedside to Bot-Side: Artificial Intelligence in Emergency Appendicitis 
Management.

Ersahin K(1), Sanduleanu S(2), Thulasi Seetha S(3), Bremm J(4), Abbasli C(5), 
Zimmer C(6), Damer T(1), Kottlors J(4), Goertz L(4), Bruns C(7), Maintz D(4), 
Abdullayev N(2)(5).

Author information:
(1)Department of General and Visceral Surgery, GFO Clinics Troisdorf, Academic 
Hospital of the Friedrich-Wilhelms-University Bonn, 50937 Troisdorf, Germany.
(2)Department of Radiology and Neuroradiology, GFO Clinics Troisdorf, Academic 
Hospital of the Friedrich-Wilhelms-University Bonn, 53840 Troisdorf, Germany.
(3)National Center for Oncological Hadrontherapy (CNAO), 27100 Pavia, Italy.
(4)Institute for Diagnostic and Interventional Radiology, Faculty of Medicine 
and University Hospital Cologne, University of Cologne, 50937 Cologne, Germany.
(5)Institute for Biomedical Engineering, Azerbaijan Technical University, AZ1073 
Baku, Azerbaijan.
(6)Faculty of Medicine, University of Cologne, 50391 Cologne, Germany.
(7)Department of General, Visceral, Tumor and Transplantation Surgery, 
University Hospital of Cologne, Kerpener Straße 62, 50937 Cologne, Germany.

Introduction: Acute appendicitis (AA) is a common cause of abdominal pain that 
can lead to complications like perforation and intra-abdominal abscesses, 
increasing morbidity and mortality, often requiring emergency surgery. 
Nevertheless, appendectomy is performed in up to 95% of uncomplicated cases, 
while complications like perforation and intra-abdominal abscesses increase 
morbidity and mortality. The current study compares the accuracy of GPT-4.5, 
DeepSeek R1, and machine learning in assisting with surgical decision-making for 
patients presenting with lower abdominal pain at the Emergency Department. 
Methods: In this multicenter retrospective study, 63 histopathologically 
confirmed appendicitis patients and 50 control patients with right abdominal 
pain presenting at the Emergency Department at two German hospitals between 
October 2022 and October 2023 were included. Using each patient's clinical, 
laboratory, and radiological findings, DeepSeek (with and without 
Retrieval-Augmented Generation using 2020 Jerusalem guidelines) was compared in 
terms of accuracy with GPT-4.5 and a random forest-based machine-learning model, 
with a board-certified surgeon (reference standard) to determine the optimal 
treatment approach (laparoscopic exploration/appendectomy versus conservative 
antibiotic therapy). Results: Accuracy of agreement with board-certified 
surgeons in the decision-making of appendectomy versus conservative therapy 
increased non-significantly from 80.5% to 83.2% with DeepSeek and from 70.8 to 
76.1% when GPT-4.5 was provided with the World Journal of Emergency Surgery 2020 
Jerusalem guidelines on the diagnosis and treatment of acute appendicitis. The 
estimated machine-learning model training accuracy was 84.3%, while the 
validation accuracy for the model was 85.0%. Discussion: GPT-4.5 and DeepSeek 
R1, as well as the machine-learning model, demonstrate promise in aiding 
surgical decision-making for appendicitis, particularly in resource-constrained 
settings. Ongoing training and validation are required to optimize the 
performance of such models.

DOI: 10.3390/life15091387
PMCID: PMC12470868
PMID: 41010329

Conflict of interest statement: The authors declare no financial, consultant, 
institutional, or other relationships that might lead to bias or a conflict of 
interest in relation to this article.


987. Pharmacy (Basel). 2025 Dec 3;13(6):176. doi: 10.3390/pharmacy13060176.

Large Language Models for Drug-Related Adverse Events in Oncology Pharmacy: 
Detection, Grading, and Actioning.

Zitu MM(1), Manne A(2), Zhu Y(3), Rahat WB(4), Binkheder S(5).

Author information:
(1)Department of Machine Learning, Moffitt Cancer Center and Research Institute, 
Tampa, FL 33612, USA.
(2)Department of Medicine, College of Medicine, The Ohio State University, 
Columbus, OH 43210, USA.
(3)Department of Pediatrics, University Hospitals Rainbow Babies & Children's 
Hospital, Cleveland, OH 44106, USA.
(4)Department of Computer Science and Engineering, East West University, 
Aftabnagar, Dhaka 1212, Bangladesh.
(5)Medical Informatics and E-Learning Unit, Medical Education Department, 
College of Medicine, King Saud University, Riyadh 12372, Saudi Arabia.

Preventable medication harm in oncology is often driven by drug-related adverse 
events (AEs) that trigger order changes such as holds, dose reductions, delays, 
rechallenges, and enhanced monitoring. Much of the evidence needed to make these 
decisions lives in unstructured clinical texts, where large language models 
(LLMs), a type of artificial intelligence (AI), now offer extraction and 
reasoning capabilities. In this narrative review, we synthesize empirical 
studies evaluating LLMs and related NLP systems applied to clinical text for 
oncology AEs, focusing on three decision-linked tasks: (i) AE detection from 
clinical documentation, (ii) Common Terminology Criteria for Adverse Events 
(CTCAE) grade assignment, and (iii) grade-aligned actions. We also consider how 
these findings can inform pharmacist-facing recommendations for order-level 
safety. We conducted a narrative review of English-language studies indexed in 
PubMed, Ovid MEDLINE, and Embase. Eligible studies used LLMs on clinical 
narratives and/or authoritative guidance as model inputs or reference standards; 
non-text modalities and non-empirical articles were excluded. Nineteen studies 
met inclusion criteria. LLMs showed the potential to detect oncology AEs from 
routine notes and often outperformed diagnosis codes for surveillance and cohort 
construction. CTCAE grading was feasible but less stable than detection; 
performance improved when outputs were constrained to CTCAE terms/grades, 
temporally anchored, and aggregated at the patient level. Direct evaluation of 
grade-aligned actions was uncommon; most studies reported proxies (e.g., steroid 
initiation or drug discontinuation) rather than formal grade-to-action 
correctness. While prospective, real-world impact reporting remained sparse, 
several studies quantified scale advantages and time savings, supporting an 
initial role as high-recall triage with pharmacist adjudication. Overall, the 
evidence supports near-term, pharmacist-in-the-loop use of AI for AE 
surveillance and review, with CTCAE-structured, citation-backed outputs 
delivered into the pharmacist's electronic health record order-verification 
workspace as reviewable artifacts. Future work must standardize reporting and 
CTCAE/version usage, and measure grade-to-action correctness prospectively, to 
advance toward order-level decision support.

DOI: 10.3390/pharmacy13060176
PMCID: PMC12736875
PMID: 41441324

Conflict of interest statement: The authors declare no conflicts of interest.


988. Sensors (Basel). 2025 Sep 18;25(18):5831. doi: 10.3390/s25185831.

Feasibility of an AI-Enabled Smart Mirror Integrating MA-rPPG, Facial Affect, 
and Conversational Guidance in Realtime.

Kasno MA(1)(2), Jung JW(1).

Author information:
(1)Department of Computer Science and Artificial Intelligence, College of 
Advanced Convergence Engineering, Dongguk University, Seoul 04620, Republic of 
Korea.
(2)Faculty of Electrical Technology and Engineering, Universiti Teknikal 
Malaysia Melaka, Melaka 76100, Malaysia.

This paper presents a real-time smart mirror system combining multiple AI 
modules for multimodal health monitoring. The proposed platform integrates three 
core components: facial expression analysis, remote photoplethysmography (rPPG), 
and conversational AI. A key innovation lies in transforming the Moving Average 
rPPG (MA-rPPG) model-originally developed for offline batch processing-into a 
real-time, continuously streaming setup, enabling seamless heart rate and 
peripheral oxygen saturation (SpO2) monitoring using standard webcams. The 
system also incorporates the DeepFace facial analysis library for live emotion, 
age detection, and a Generative Pre-trained Transformer 4o (GPT-4o)-based mental 
health chatbot with bilingual (English/Korean) support and voice synthesis. 
Embedded into a touchscreen mirror with Graphical User Interface (GUI), this 
solution delivers ambient, low-interruption interaction and real-time user 
feedback. By unifying these AI modules within an interactive smart mirror, our 
findings demonstrate the feasibility of integrating multimodal sensing (rPPG, 
affect detection) and conversational AI into a real-time smart mirror platform. 
This system is presented as a feasibility-stage prototype to promote real-time 
health awareness and empathetic feedback. The physiological validation was 
limited to a single subject, and the user evaluation constituted only a small 
formative assessment; therefore, results should be interpreted strictly as 
preliminary feasibility evidence. The system is not intended to provide clinical 
diagnosis or generalizable accuracy at this stage.

DOI: 10.3390/s25185831
PMCID: PMC12473532
PMID: 41013066 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


989. Sensors (Basel). 2025 Aug 31;25(17):5372. doi: 10.3390/s25175372.

LLM-Powered Prediction of Hyperglycemia and Discovery of Behavioral Treatment 
Pathways from Wearables and Diet.

Mamun A(1)(2), Arefeen A(1), Racette SB(1), Sears DD(1), Whisner CM(1), Buman 
MP(1), Ghasemzadeh H(1).

Author information:
(1)College of Health Solutions, Arizona State University, Phoenix, AZ 85054, 
USA.
(2)School of Computing and Augmented Intelligence, Arizona State University, 
Tempe, AZ 85281, USA.

Postprandial hyperglycemia, marked by the blood glucose level exceeding the 
normal range after consuming a meal, is a critical indicator of progression 
toward type 2 diabetes in people with prediabetes and in healthy individuals. A 
key metric for understanding blood glucose dynamics after eating is the 
postprandial Area Under the Curve (AUC). Predicting postprandial AUC in advance 
based on a person's lifestyle factors, such as diet and physical activity level, 
and explaining the factors that affect postprandial blood glucose could allow an 
individual to adjust their behavioral choices accordingly to maintain normal 
glucose levels. In this work, we develop an explainable machine learning 
solution, GlucoLens, that takes sensor-driven inputs and utilizes advanced data 
processing, large language models, and trainable machine learning models to 
estimate postprandial AUC and predict hyperglycemia from diet, physical 
activity, and recent glucose patterns. We use data obtained using wearables in a 
five-week clinical trial of 10 adults who worked full-time to develop and 
evaluate the proposed computational model that integrates wearable sensing, 
multimodal data, and machine learning. Our machine learning model takes 
multimodal data from wearable activity and glucose monitoring sensors, along 
with food and work logs, and provides an interpretable prediction of the 
postprandial glucose patterns. GlucoLens achieves a normalized root mean squared 
error (NRMSE) of 0.123 in its best configuration. On average, the proposed 
technology provides a 16% better predictive performance compared to the 
comparison models. Additionally, our technique predicts hyperglycemia with an 
accuracy of 79% and an F1 score of 0.749 and recommends different treatment 
options to help avoid hyperglycemia through diverse counterfactual explanations. 
With systematic experiments and discussion supported by established prior 
research, we show that our method is generalizable and consistent with clinical 
understanding.

DOI: 10.3390/s25175372
PMCID: PMC12431146
PMID: 40942802 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest. 
The funders had no role in the design of the study; in the collection, analyses, 
or interpretation of data; in the writing of the manuscript; or in the decision 
to publish the results.


990. Diagnostics (Basel). 2025 Dec 11;15(24):3160. doi: 10.3390/diagnostics15243160.

Evaluation of ChatGPT-5 for Automated ASPECTS Assessment on Non-Contrast CT in 
Acute Ischemic Stroke.

Genez S(1), Özer H(1), Buz Yaşar A(1), Yılmazsoy Y(1), Soydan T(1), Sarıoğlu 
AE(1), Ersoy S(2).

Author information:
(1)Department of Radiology, Faculty of Medicine, Bolu Abant Izzet Baysal 
University, Bolu 14030, Turkey.
(2)Department of Neurology, Faculty of Medicine, Bolu Abant Izzet Baysal 
University, Bolu 14030, Turkey.

Background/Objectives: This study aimed to evaluate the ability of ChatGPT-5, a 
multimodal large language model, to perform automated ASPECTS assessment on 
non-contrast CT (NCCT) in patients with acute ischemic stroke. Methods: This 
retrospective, single-center study included 199 patients with anterior 
circulation AIS who underwent baseline NCCT before reperfusion therapy between 
November 2020 and February 2025. Each NCCT was evaluated by two human readers 
and by ChatGPT-5 using four representative images (two ganglionic and two 
supraganglionic). Interobserver agreement was measured with the intraclass 
correlation coefficient (ICC), and prognostic performance was analyzed using 
multivariable logistic regression and receiver operating characteristic (ROC) 
analysis for 3-month functional independence (mRS ≤ 2). Results: ChatGPT-5 
demonstrated good-to-excellent agreement with expert consensus (ICC = 0.845; 95% 
CI, 0.792-0.884; κ = 0.79). ChatGPT-ASPECTS were independently associated with 
3-month functional independence (OR = 1.28 per point; p = 0.004), comparable to 
consensus-ASPECTS (OR = 1.31; p = 0.003). Prognostic discrimination was similar 
between ChatGPT-5 and consensus scoring (AUC = 0.78 vs. 0.80; p = 0.41). 
Conclusions: ChatGPT-5 achieved high reliability and strong prognostic validity 
in automated ASPECTS assessment without task-specific training. These findings 
highlight the emerging potential of large language models for quantitative image 
interpretation, though clinical implementation will require multicenter 
validation and regulatory approval.

DOI: 10.3390/diagnostics15243160
PMCID: PMC12732079
PMID: 41464159

Conflict of interest statement: The authors declare that they have no conflicts 
of interest.


991. Cancers (Basel). 2025 Sep 30;17(19):3198. doi: 10.3390/cancers17193198.

Large Language Model (LLM)-Predicted and LLM-Assisted Calculation of the Spinal 
Instability Neoplastic Score (SINS) Improves Clinician Accuracy and Efficiency.

Chan MDZ(1), Tjio CKE(1), Chan TLY(1), Tan YL(1), Chua AXY(1), Loh SKY(1), Leow 
GZH(1), Gan MY(1), Lim X(1), Choo AK(1), Liu Y(1), Tan JWP(1), Teo EC(1), Yap 
QV(2), Yonghan T(1), Makmur A(1)(3), Kumar N(4), Tan JH(4), Hallinan JTPD(1)(3).

Author information:
(1)Department of Diagnostic Imaging, National University Hospital, Singapore 
119074, Singapore.
(2)Biostatistics Unit, Yong Loo Lin School of Medicine, Singapore 117597, 
Singapore.
(3)Department of Diagnostic Radiology, Yong Loo Lin School of Medicine, 
Singapore 117597, Singapore.
(4)University Spine Centre, University Orthopaedics, Hand and Reconstructive 
Microsurgery, National University Health System, Singapore 119074, Singapore.

Background: The Spinal Instability Neoplastic Score (SINS) guides treatment for 
patients with spinal tumors, but issues arise with complexity, interobserver 
variability, and time demands. Large language models (LLMs) may help overcome 
these limitations. Objectives: This study evaluates the accuracy and efficiency 
of a privacy-preserving LLM (PP-LLM) for SINS calculation, with and without 
clinician involvement, to assess its feasibility as a clinical decision-support 
tool. Methods: This retrospective observational study was granted a 
Domain-Specific Review Board waiver owing to minimal risk. Patients from 2020 to 
2022 were included. A PP-LLM was employed to maintain secure handling of patient 
data. A consensus SINS reference standard was established by musculoskeletal 
radiologists and an orthopedic surgeon. Eight orthopedic and oncology trainees 
were divided into two groups to calculate SINS, with and without PP-LLM 
assistance. LLM-predicted scores were also generated independently of any human 
input. Results: The main outcomes were agreement with the reference standard 
(measured by intraclass correlation coefficients [ICCs]) and time required for 
SINS calculation. The LLM-assisted method achieved excellent agreement (ICC = 
0.993, 95%CI = 0.991-0.994), closely followed by the LLM-predicted approach (ICC 
= 0.990, 95%CI = 0.984-0.993). Clinicians working without LLM support showed a 
significantly lower ICC compared to both LLM methods (0.968, 95%CI = 
0.960-0.975) (both p < 0.001). The LLM alone produced scores in approximately 5 
s, while the median scoring time for LLM-assisted clinicians was 60.0 s (IQR = 
46.0-80.0), notably shorter than the 83.0 s (IQR = 58.0-124.0) required without 
LLM assistance. Conclusions: An LLM-based approach, whether used autonomously or 
in conjunction with clinical expertise, enhances both accuracy and efficiency in 
SINS calculation. Adopting this technology may streamline oncologic workflows 
and facilitate more timely interventions for patients with spinal metastases.

DOI: 10.3390/cancers17193198
PMCID: PMC12523502
PMID: 41097723

Conflict of interest statement: The authors declare that they have no known 
competing financial interests or personal relationships that could have appeared 
to influence the work reported in this paper.


992. Diagnostics (Basel). 2025 Oct 8;15(19):2536. doi: 10.3390/diagnostics15192536.

Performance of ChatGPT-4o in Determining Radiology-Pathology Concordance and 
Management Recommendations Following Image-Guided Breast Biopsies.

Lee A(1)(2), Curpen B(1)(2), Alikhassi A(1)(2).

Author information:
(1)Temerty Faculty of Medicine, University of Toronto, Toronto, ON M5S 3K3, 
Canada.
(2)Medical Imaging, Sunnybrook Health Science Centre, Toronto, ON M4N 3M5, 
Canada.

Background: Determining radiology-pathology concordance after breast biopsies is 
critical to ensuring appropriate patient management. However, expertise and 
multidisciplinary input are not universally accessible. Purpose: To evaluate the 
performance of a large language model, ChatGPT-4o, in determining the 
radiology-pathology concordance of breast biopsies and suggesting subsequent 
management steps. Methods: A retrospective single-center study analyzed 244 
cases of image-guided breast biopsies of women. ChatGPT-4o assessed 
de-identified radiology and pathology reports for concordance and recommended 
management. Radiologist assessments served as the reference standard with final 
surgical pathology and 2-year imaging follow-up serving as gold standards when 
applicable. Concordance rates, management recommendations, and diagnostic 
agreement with the gold standard were compared using statistical tests, 
including McNemar's, chi-square, Fisher-Freeman-Halton, and Cohen's kappa. 
Results: ChatGPT-4o achieved a concordance rate of 98.8% vs. 98.0% for 
radiologists (p = 0.625) and demonstrated high diagnostic agreement with the 
gold standard (kappa = 0.947, p < 0.001). ChatGPT-4o favored imaging follow-up 
more than radiologists (49.2% vs. 41.8%, p < 0.001) and surgical management less 
frequently (41.8% vs. 46.7%). Conclusions: ChatGPT-4o demonstrated diagnostic 
performance comparable to radiologists with breast imaging subspecialities in 
evaluating breast biopsy concordance. Its slightly more conservative management 
approach may enhance shared decision-making in resource-limited settings.

DOI: 10.3390/diagnostics15192536
PMCID: PMC12523907
PMID: 41095755

Conflict of interest statement: The authors declare no conflicts of interest.


993. Diagnostics (Basel). 2025 Sep 27;15(19):2474. doi: 10.3390/diagnostics15192474.

Large Language Model and Knowledge Graph-Driven AJCC Staging of Prostate Cancer 
Using Pathology Reports.

Jo E(1), Noh TI(2), Joo HJ(1)(3)(4).

Author information:
(1)Department of Biomedical Informatics, Korea University College of Medicine, 
Seoul 02841, Republic of Korea.
(2)Department of Urology, Korea University School of Medicine, Seoul 02841, 
Republic of Korea.
(3)Korea University Research Institute for Medical Bigdata Science, Korea 
University, Seoul 02841, Republic of Korea.
(4)Department of Cardiology, Cardiovascular Center, Korea University College of 
Medicine, Seoul 02841, Republic of Korea.

Background/Objectives: To develop an automated American Joint Committee on 
Cancer (AJCC) staging system for radical prostatectomy pathology reports using 
large language model-based information extraction and knowledge graph 
validation. Methods: Pathology reports from 152 radical prostatectomy patients 
were used. Five additional parameters (Prostate-specific antigen (PSA) level, 
metastasis stage (M-stage), extraprostatic extension, seminal vesicle invasion, 
and perineural invasion) were extracted using GPT-4.1 with zero-shot prompting. 
A knowledge graph was constructed to model pathological relationships and 
implement rule-based AJCC staging with consistency validation. Information 
extraction performance was evaluated using a local open-source large language 
model (LLM) (Mistral-Small-3.2-24B-Instruct) across 16 parameters. The 
LLM-extracted information was integrated into the knowledge graph for automated 
AJCC staging classification and data consistency validation. The developed 
system was further validated using pathology reports from 88 radical 
prostatectomy patients in The Cancer Genome Atlas (TCGA) dataset. Results: 
Information extraction achieved an accuracy of 0.973 and an F1-score of 0.986 on 
the internal dataset, and 0.938 and 0.968, respectively, on external validation. 
AJCC staging classification showed macro-averaged F1-scores of 0.930 and 0.833 
for the internal and external datasets, respectively. Knowledge graph-based 
validation detected data inconsistencies in 5 of 150 cases (3.3%). Conclusions: 
This study demonstrates the feasibility of automated AJCC staging through the 
integration of large language model information extraction and knowledge 
graph-based validation. The resulting system enables privacy-protected clinical 
decision support for cancer staging applications with extensibility to broader 
oncologic domains.

DOI: 10.3390/diagnostics15192474
PMCID: PMC12523256
PMID: 41095693

Conflict of interest statement: The authors declare no conflicts of interest.


994. Diagn Interv Radiol. 2025 Jul 8. doi: 10.4274/dir.2025.253445. Online ahead of 
print.

Foundation models for radiology: fundamentals, applications, opportunities, 
challenges, risks, and prospects.

Akinci D'Antonoli T(1)(2), Bluethgen C(3), Cuocolo R(4), Klontzas ME(5)(6)(7), 
Ponsiglione A(8), Kocak B(9).

Author information:
(1)University Hospital Basel, Department of Diagnostic and Interventional 
Neuroradiology, Basel, Switzerland.
(2)University Children's Hospital Basel, Department of Pediatric Radiology, 
Basel, Switzerland.
(3)University Hospital Zurich, Institute for Diagnostic and Interventional 
Radiology, Zurich, Switzerland.
(4)Department of Medicine, Surgery and Dentistry, University of Salerno, 
Baronissi, Italy.
(5)University of Crete School of Medicine, Artificial Intelligence and 
Translational Imaging (ATI) Lab, Department of Radiology, Crete, Greece.
(6)Foundation for Research and Technology (ICS-FORTH), Institute of Computer 
Science, Computational Biomedicine Lab, Crete, Greece.
(7)Karolinska Institute, Department of Clinical Science, Intervention and 
Technology (CLINTEC), Division of Radiology, Stockholm, Sweden.
(8)University of Naples Federico II, Department of Advanced Biomedical Sciences, 
Naples, Italy.
(9)Başakşehir Çam and Sakura City Hospital, Department of Radiology, İstanbul, 
Türkiye.

Foundation models (FMs) represent a significant evolution in artificial 
intelligence (AI), impacting diverse fields. Within radiology, this evolution 
offers greater adaptability, multimodal integration, and improved 
generalizability compared with traditional narrow AI. Utilizing large-scale 
pre-training and efficient fine-tuning, FMs can support diverse applications, 
including image interpretation, report generation, integrative diagnostics 
combining imaging with clinical/laboratory data, and synthetic data creation, 
holding significant promise for advancements in precision medicine. However, 
clinical translation of FMs faces several substantial challenges. Key concerns 
include the inherent opacity of model decision-making processes, environmental 
and social sustainability issues, risks to data privacy, complex ethical 
considerations, such as bias and fairness, and navigating the uncertainty of 
regulatory frameworks. Moreover, rigorous validation is essential to address 
inherent stochasticity and the risk of hallucination. This international 
collaborative effort provides a comprehensive overview of the fundamentals, 
applications, opportunities, challenges, and prospects of FMs, aiming to guide 
their responsible and effective adoption in radiology and healthcare.

DOI: 10.4274/dir.2025.253445
PMID: 40626693


995. Diagnostics (Basel). 2025 Mar 15;15(6):735. doi: 10.3390/diagnostics15060735.

Artificial Intelligence in Relation to Accurate Information and Tasks in 
Gynecologic Oncology and Clinical Medicine-Dunning-Kruger Effects and 
Ultracrepidarianism.

Pavlik EJ(1), Land Woodward J(2), Lawton F(3), Swiecki-Sikora AL(1), Ramaiah 
DD(2), Rives TA(1).

Author information:
(1)Division of Gynecologic Oncology, Department of Obstetrics and Gynecology, 
Chandler Medical Center-Markey Cancer Center, University of Kentucky College of 
Medicine, Lexington, KY 40536-0293, USA.
(2)University of Kentucky College of Medicine, Lexington, KY 40536-0293, USA.
(3)SE London Gynecological Cancer Centre, Emeritus Surgeon, London SE5 9RS, UK.

Publications on the application of artificial intelligence (AI) to many 
situations, including those in clinical medicine, created in 2023-2024 are 
reviewed here. Because of the short time frame covered, here, it is not possible 
to conduct exhaustive analysis as would be the case in meta-analyses or 
systematic reviews. Consequently, this literature review presents an examination 
of narrative AI's application in relation to contemporary topics related to 
clinical medicine. The landscape of the findings reviewed here span 254 papers 
published in 2024 topically reporting on AI in medicine, of which 83 articles 
are considered in the present review because they contain evidence-based 
findings. In particular, the types of cases considered deal with AI accuracy in 
initial differential diagnoses, cancer treatment recommendations, board-style 
exams, and performance in various clinical tasks, including clinical imaging. 
Importantly, summaries of the validation techniques used to evaluate AI findings 
are presented. This review focuses on AIs that have a clinical relevancy 
evidenced by application and evaluation in clinical publications. This relevancy 
speaks to both what has been promised and what has been delivered by various AI 
systems. Readers will be able to understand when generative AI may be expressing 
views without having the necessary information (ultracrepidarianism) or is 
responding as if the generative AI had expert knowledge when it does not. A lack 
of awareness that AIs may deliver inadequate or confabulated information can 
result in incorrect medical decisions and inappropriate clinical applications 
(Dunning-Kruger effect). As a result, in certain cases, a generative AI system 
might underperform and provide results which greatly overestimate any medical or 
clinical validity.

DOI: 10.3390/diagnostics15060735
PMCID: PMC11941301
PMID: 40150078

Conflict of interest statement: The authors declare no conflicts of interest.


996. Diagnostics (Basel). 2025 Dec 25;16(1):72. doi: 10.3390/diagnostics16010072.

Enhancing Patient Understanding of Perianal Fistula MRI Findings Using ChatGPT: 
A Randomized, Single Centre Study.

Anand E(1)(2), Ghersin I(1), Lingam G(1)(2), Devlin K(1), Pelly T(1)(2), Singer 
D(3), Tomlinson C(4), Munro REJ(5), Capstick R(5), Antoniou A(5), Hart AL(1)(2), 
Tozer P(1)(2), Sahnan K(1)(2), Lung P(1)(2).

Author information:
(1)Robin Phillips' Fistula Research Unit, St Mark's The National Bowel Hospital, 
London NW10 7NS, UK.
(2)Department of Surgery & Cancer, Imperial College London, London SW7 2AZ, UK.
(3)Tenrec Analytics, St Albans AL1 4TJ, UK.
(4)Institute of Health Informatics, University College London, London WC1E 6BT, 
UK.
(5)St Mark's The National Bowel Hospital, London NW10 7NS, UK.

Background/Objectives: Large Language Models (LLMs) may help translate complex 
Magnetic Resonance Imaging (MRI) fistula reports into accessible, 
patient-friendly summaries. This study evaluated the clinical utility, safety, 
and patient acceptability of Generative Pre-trained Transformer (GPT-4o) in 
generating such reports. Methods: A three-phase study was conducted at a single 
centre. Phase I involved prompt engineering and pilot testing of GPT-4o outputs 
for feasibility. Phase II assessed 250 consecutive MRI fistula reports from 
September 2024 to November 2024, each reviewed by a multi-disciplinary panel to 
determine hallucinations and thematic content. Phase III randomised patients to 
review either a simple or complex fistula case, each containing an original 
report and an Artificial Intelligence (AI)-generated summary (order randomised, 
origin blinded), and rate readability, trustworthiness, usefulness and 
comprehension. Results: Sixteen patients participated in Phase I pilot testing. 
In Phase II, hallucinations occurred in 11% of outputs, with unverified 
recommendations also identified. In Phase III, 61 patients (mean age 48, 41% 
female) evaluated paired original and AI-generated summaries. AI summaries 
scored significantly higher for readability, comprehension, and usefulness than 
original reports (all p < 0.001), with equivalent trust ratings. Mean 
Flesch-Kincaid scores were markedly higher for AI-generated summaries (66 vs. 
26; p < 0.001). Clinicians highlighted improved anatomical structuring and 
accessible language, but emphasised risks of inaccuracies. A revised template 
incorporating Multi-Disciplinary Team (MDT)-focused action points and a lay 
summary section was co-developed. Conclusions: LLMs can enhance the readability 
and patient understanding of complex MRI reports but remain limited by 
hallucinations and inconsistent terminology. Safe implementation requires 
structured oversight, domain-specific refinement, and clinician validation. 
Future development should prioritise standardised reporting templates 
incorporating clinician-approved lay summaries.

DOI: 10.3390/diagnostics16010072
PMCID: PMC12785849
PMID: 41515567

Conflict of interest statement: Author Daniel Singer was employed by the company 
Tenrec Analytics. Author Robin E. J. Munro reports ownership of stock in IQVIA 
and is an employee of IQVIA. Author Anna Antoniou was employed by Novartis UK. 
The remaining authors declare that the research was conducted in the absence of 
any commercial or financial relationships that could be construed as a potential 
conflict of interest.


997. Healthcare (Basel). 2025 Mar 17;13(6):657. doi: 10.3390/healthcare13060657.

Advancements in Digital Cytopathology Since COVID-19: Insights from a Narrative 
Review of Review Articles.

Giansanti D(1).

Author information:
(1)Centro TISP, ISS Via Regina Elena 299, 00161 Rome, Italy.

Background/Objectives: The integration of digitalization in cytopathology is an 
emerging field with transformative potential, aiming to enhance diagnostic 
precision and operational efficiency. This narrative review of reviews (NRR) 
seeks to identify prevailing themes, opportunities, challenges, and 
recommendations related to the process of digitalization in cytopathology. 
Methods: Utilizing a standardized checklist and quality control procedures, this 
review examines recent advancements and future implications in this domain. 
Twenty-one review studies were selected through a systematic process. Results: 
The results highlight key emerging trends, themes, opportunities, challenges, 
and recommendations in digital cytopathology. First, the study identifies 
pivotal themes that reflect the ongoing technological transformation, guiding 
future focus areas in the field. A major trend is the integration of artificial 
intelligence (AI), which is increasingly critical in improving diagnostic 
accuracy, streamlining workflows, and assisting decision making. Notably, 
emerging AI technologies like large language models (LLMs) and chatbots are 
expected to provide real-time support and automate tasks, though concerns around 
ethics and privacy must be addressed. The reviews also emphasize the need for 
standardized protocols, comprehensive training, and rigorous validation to 
ensure AI tools are reliable and effective across clinical settings. Lastly, 
digital cytopathology holds significant potential to improve healthcare 
accessibility, especially in remote areas, by enabling faster, more efficient 
diagnoses and fostering global collaboration through telepathology. Conclusions: 
Overall, this study highlights the transformative impact of digitalization in 
cytopathology, improving diagnostic accuracy, efficiency, and global 
accessibility through tools like whole-slide imaging and telepathology. While 
artificial intelligence plays a significant role, the broader focus is on 
integrating digital solutions to enhance workflows and collaboration. Addressing 
challenges such as standardization, training, and ethical considerations is 
crucial to fully realize the potential of these advancements.

DOI: 10.3390/healthcare13060657
PMCID: PMC11942033
PMID: 40150507

Conflict of interest statement: The author declares no conflict of interest.


998. Diagnostics (Basel). 2025 Sep 22;15(18):2409. doi: 10.3390/diagnostics15182409.

Criteria and Protocol: Assessing Generative AI Efficacy in Perceiving EULAR 2019 
Lupus Classification.

Lushington GH(1), Nair S(1), Jupe ER(1), Rubin B(1), Purushothaman M(1).

Author information:
(1)Progentec Diagnostics Inc., Oklahoma City, OK 73104, USA.

Background/Objectives: In clinical informatics, the term 'information overload' 
is increasingly used to describe the operational impediments of excessive 
documentation. While electronic health records (EHRs) are growing in abundance, 
many medical records (MRs) remain in legacy formats that impede efficient, 
systematic processing, contributing to the extenuating challenges of care 
fragmentation. Thus, there is a growing interest in using generative AI (genAI) 
for automated MR summarization and characterization. Methods: MRs for a set of 
78 individuals were digitized. Some were known systemic lupus erythematosus 
(SLE) cases, while others were under evaluation for possible SLE classification. 
A two-pass genAI assessment strategy was implemented using the Claude 3.5 large 
language model (LLM) to mine MRs for information relevant to classifying SLE vs. 
undifferentiated connective tissue disorder (UCTD) vs. neither via the 
22-criteria EULAR 2019 model. Results: Compared to clinical determination, the 
antinuclear antibody (ANA) criterion (whose results are crucial for classifying 
SLE-negative cases) exhibited favorable sensitivity 0.78 ± 0.09 (95% confidence 
interval) and a positive predictive value 0.85 ± 0.08 but a marginal performance 
for specificity 0.60 ± 0.11 and uncertain predictivity for the negative 
predictive value 0.48 ± 0.11. Averaged over the remaining 21 criteria, these 
four performance metrics were 0.69 ± 0.11, 0.87 ± 0.04, 0.54 ± 0.10, and 0.93 ± 
0.03. Conclusions: ANA performance statistics imply that genAI yields confident 
assessments of SLE negativity (per high sensitivity) but weaker positivity. The 
remaining genAI criterial determinations support (per specificity) confident 
assertions of SLE-positivity but tend to misclassify a significant fraction of 
clinical positives as UCTD.

DOI: 10.3390/diagnostics15182409
PMCID: PMC12468409
PMID: 41008780

Conflict of interest statement: Authors Gerald H. Lushington, Sandeep Nair, 
Eldon R. Jupe, Bernard Rubin and Mohan Purushothaman were employed by the 
company Progentec Diagnostics, Inc. All authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


999. Bioengineering (Basel). 2025 Oct 28;12(11):1174. doi: 
10.3390/bioengineering12111174.

An Explainable AI Framework for Corneal Imaging Interpretation and Refractive 
Surgery Decision Support.

Wang MH(1)(2)(3).

Author information:
(1)Zhuhai People's Hospital (The Affiliated Hospital of Beijing Institute of 
Technology, Zhuhai Clinical Medical College of Jinan University), Zhuhai 519000, 
China.
(2)The Department of Ophthalmology and Visual Sciences, The Chinese University 
of Hong Kong, Hong Kong 999077, China.
(3)Zhuhai Institute of Advanced, Technology Chinese Academy of Sciences, Zhuhai 
519000, China.

This study introduces an explainable neuro-symbolic and large language model 
(LLM)-driven framework for intelligent interpretation of corneal topography and 
precision surgical decision support. In a prospective cohort of 20 eyes, 
comprehensive IOLMaster 700 reports were analyzed through a four-stage pipeline: 
(1) automated extraction of key parameters-including corneal curvature, 
pachymetry, and axial biometry; (2) mapping of these quantitative features onto 
a curated corneal disease and refractive-surgery knowledge graph; (3) Bayesian 
probabilistic inference to evaluate early keratoconus and surgical eligibility; 
and (4) explainable multi-model LLM reporting, employing DeepSeek and GPT-4.0, 
to generate bilingual physician- and patient-facing narratives. By transforming 
complex imaging data into transparent reasoning chains, the pipeline delivered 
case-level outputs within ~95 ± 12 s. When benchmarked against independent 
evaluations by two senior corneal specialists, the framework achieved 92 ± 4% 
sensitivity, 94 ± 5% specificity, 93 ± 4% accuracy, and an AUC of 0.95 ± 0.03 
for early keratoconus detection, alongside an F1 score of 0.90 ± 0.04 for 
refractive surgery eligibility. The generated bilingual reports were rated 
≥4.8/5 for logical clarity, clinical usefulness, and comprehensibility, with 
representative cases fully concordant with expert judgment. Comparative 
benchmarking against baseline CNN and ViT models demonstrated superior 
diagnostic accuracy (AUC = 0.95 ± 0.03 vs. 0.88 and 0.90, p < 0.05), confirming 
the added value of the neuro-symbolic reasoning layer. All analyses were 
executed on a workstation equipped with an NVIDIA RTX 4090 GPU and implemented 
in Python 3.10/PyTorch 2.2.1 for full reproducibility. By explicitly coupling 
symbolic medical knowledge with advanced language models and embedding 
explainable artificial intelligence (XAI) principles throughout data processing, 
reasoning, and reporting, this framework provides a transparent, rapid, and 
clinically actionable AI solution. The approach holds significant promise for 
improving early ectatic disease detection and supporting individualized 
refractive surgery planning in routine ophthalmic practice.

DOI: 10.3390/bioengineering12111174
PMCID: PMC12649685
PMID: 41301129

Conflict of interest statement: The author declares no conflicts of interest.


1000. Dig Liver Dis. 2025 Dec 22:S1590-8658(25)01226-5. doi: 
10.1016/j.dld.2025.11.026. Online ahead of print.

Performance of large language models in addressing patient queries on colorectal 
cancer screening in different languages: An international study across 28 
countries.

M M(1), A P(2), S G(3), T V(4), Lhs L(5), S B(6), P P(7), M M(8), T Z(9), H 
U(10), Ejt A(11), D B(12), H D(13), T D(14), A G(15), T K(16), S L(17), B L(18), 
H M(19), R N(20), Y O(21), A R(22), A T(23), I M(24); World Endoscopy 
Organization (WEO) Emerging Stars Program.

Collaborators: G C(25), Qdh D(26), J G(27), Dv H(28), H H(29), A K(30), Lee 
SH(31), R L(32), Mhj M(33), Cd M(34), R M(35), Km P(36), T R(37), J SA(38), I 
S(38), A V(39), A V(4).

Author information:
(1)Department of Medicine and Surgery, University of Enna `Kore', Enna, Italy; 
Gastroenterology Unit, Umberto I Hospital, Enna, Italy. Electronic address: 
marcello.maida@unikore.it.
(2)Department of Gastroenterology, General University Hospital of Larissa, 
Larissa, Greece.
(3)Department of Gastroenterology and Hepatology, Westmead Hospital, Sydney, 
Australia; Clinical School of Medicine, University of Sydney, Sydney, Australia; 
The Center for Advanced Therapeutic Endoscopy and Endoscopic Oncology, 
University of Toronto, St. Michael's Hospital, Toronto, Canada.
(4)Gastroenterology Department, Colentina Clinical Hospital, Carol Davila 
Faculty of Medicine, Bucharest, Romania.
(5)Department of Medicine and Therapeutics, Faculty of Medicine, The Chinese 
University of Hong Kong, China; Li Ka Shing Institute of Health Sciences, The 
Chinese University of Hong Kong, China; Institute of Digestive Diseases, The 
Chinese University of Hong Kong, China.
(6)Department of Endoscopy, Barretos Cancer Hospital, Barretos, Brazil.
(7)Medical Gastroenterology, Asian Institute of Gastroenterology, Somajiguda, 
Hyderabad, India.
(8)Department of Surgery and Gastroenterology, Avenue HealthCare, Nairobi, 
Kenya; College of Surgeons of East, Central and Southern Africa, Arusha, 
Tanzania.
(9)Division of Internal Medicine, Henry Ford Hospital, Department of 
Gastroenterology and Hepatology, Detroit, USA.
(10)Hospital Universitario Germans Trias I Pujol and Centro de Investigación 
Biomédica en Red de Enfermedades Hepáticas y Digestivas (CIBEREHD), Badalona, 
Spain.
(11)Institute of Digestive and Liver Diseases, St. Luke's Medical Center Global 
City, Taguig, Philippines; School of Medicine and Public Health, Ateneode Manila 
University, Pasig, Philippines.
(12)Centre Anti Cancer de Batna, Algeria.
(13)Vrije Universiteit Brussel (VUB), Universitair Ziekenhuis Brussel, Belgium, 
Department of Gastroenterology and Hepatology, Brussels, Belgium.
(14)Department of Gastroenterology, Hitit University Faculty of Medicine, Çorum, 
Turkiye.
(15)Department of Internal Medicine, Hepatology and Gastroenterology unit, 
Mansoura University, Mansoura, Egypt.
(16)Endoscopy Center, Mongolia-Japan Hospital, Mongolian National University of 
Medical Sciences, Ulaanbaatar, Mongolia.
(17)Robert Wood Johnson Barnabas Health, NJ, USA.
(18)Gastroenterology and Endoscopy Department, Ulaanbaatar Songdo Hospital, 
Ulaanbaatar, Mongolia.
(19)Division of Gastroenterology, Pancreatobiliary and Digestive Endoscopy. 
Department of Internal Medicine, Universitas Indonesia, Cipto Mangunkusumo 
National Hospital, Indonesia.
(20)Department of Gastroenterology, Sao Paulo Cancer Institute, Sao Paulo, 
Brazil.
(21)Department of Gastrointestinal Oncology, Osaka International Cancer 
Institute, Osaka, Osaka, Japan.
(22)Interventional Digestive Endoscopy, ASST Spedali Civili di Brescia, Brescia, 
Italy.
(23)The Mohammed VI university of Health Sciences, Mohammed VI international 
University Hospital, Bouskoura, Casablanca, Morocco.
(24)Department of Gastroenterology and Hepatology, Theodor Bilharz Research 
Institute Cairo, Egypt.
(25)Department of Gastroenterology and Hepatology, Metz, France.
(26)Department of Endoscopy, Cho Ray Hospital, Ho Chi Minh, Vietnam.
(27)Endoscopy and Gastroenterology Unit, Edouard Herriot Hospital, Hospices 
Civils de Lyon, Lyon, France.
(28)Endoscopy Center, Hanoi Medical University hospital, Hanoi Medical 
University, Vietnam.
(29)Department of Gastroenterology and Hepatology, University Digestive Health 
Care Center Basel-Clarunis, Basel, Switzerland.
(30)Endoscopy Unit, The Second University Clinic, I.M. Sechenov First Moscow 
State Medical University (Sechenov University), Moscow, Russia.
(31)Department of Internal Medicine and Liver Research Institute, Seoul National 
University Hospital, Seoul National University College of Medicine, South Korea.
(32)Gastroenterology unit, Limoges university hospital, France.
(33)Department of Gastroenterology and Hepatology, Radboudumc, Nijmegen, the 
Netherlands.
(34)Internal Medicine Unit, S. Elia Hospital, Caltanissetta, Italy.
(35)Gastroenterology Department, Centro Hospitalar Universitário São João, 
Porto, Portugal.
(36)Endoscopy Unit, Hospital of The Ministry of Interior and Administration in 
Szczecin, Poland; Department of Physiology, Pomeranian Medical University in 
Szczecin, Poland.
(37)Department of Medicine I, Ludwig Demling Endoscopy Center, University 
Hospital Erlangen, Erlangen, Germany.
(38)Gastroenterology Department, Faculty of Medicine, Hospital S. João, Porto, 
Portugal.
(39)Department of Medicine and Surgery, University of Enna 'Kore', Enna, Italy; 
Gastroenterology Unit, Umberto I Hospital, Enna, Italy.

BACKGROUND: Colorectal cancer (CRC) screening reduces incidence and mortality, 
yet patient adherence remains suboptimal. Large language models may improve 
participation by addressing patient questions in native languages, but their 
multilingual performance has not been systematically assessed.
METHODS: From April to June 2025, we conducted a cross-continental study 
involving 28 countries and 23 languages. A standardized set of 15 CRC 
screening-related questions was translated into each language and submitted to 
ChatGPT (GPT-4o). Responses were independently evaluated by 140 
gastroenterologists (five per country) for accuracy, completeness, and 
comprehensibility on a 5-point Likert scale. Statistical analyses included 
t-test, Chi-square, and two-way ANOVA.
RESULTS: The study included experts and data from Europe, Asia, Africa, America, 
and Oceania. Mean scores (±SD) for accuracy, completeness, and comprehensibility 
were 4.1 ± 1.0, 4.1 ± 1.0, and 4.2 ± 0.9, respectively. Most languages achieved 
high ratings, with 73.9%, 86.9%, and 82.6% scoring ≥4 for accuracy, 
completeness, and comprehensibility. However, lower scores were observed in 
Chinese, Dutch, and Greek. Variability was also noted between countries sharing 
the same language, highlighting language- and context-dependent performance.
DISCUSSION: ChatGPT showed strong ability to answer CRC screening questions 
across multiple languages, supporting its promise as a multilingual patient 
education tool. Nonetheless, regional variability requires careful validation 
before clinical integration.

Copyright © 2025 The Author(s). Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.dld.2025.11.026
PMID: 41436291

Conflict of interest statement: Declaration of competing interest All the 
authors have no proprietary, financial, professional or other personal interest 
of any nature or kind in any product, service and/or company that could be 
construed as influencing the position presented in, or the review of this 
manuscript.


1001. Pharmacy (Basel). 2025 Mar 1;13(2):36. doi: 10.3390/pharmacy13020036.

Blueprint for Constructing an AI-Based Patient Simulation to Enhance the 
Integration of Foundational and Clinical Sciences in Didactic Immunology in a US 
Doctor of Pharmacy Program: A Step-by-Step Prompt Engineering and Coding 
Toolkit.

Malhotra A(1), Buller M(2), Modi K(3), Pajazetovic K(1), Wijesinghe DS(2)(4)(5).

Author information:
(1)Department of Pharmaceutical and Biomedical Sciences, California Northstate 
University College of Pharmacy, Elk Grove, CA 95758, USA.
(2)Department of Pharmacotherapy and Outcome Sciences, School of Pharmacy, 
Virginia Commonwealth University, Richmond, VA 23298, USA.
(3)Soap Note AI, East Brunswick, NJ 08816, USA.
(4)Da Vinci Center, Virginia Commonwealth University, Richmond, VA 23284, USA.
(5)Institute for Structural Biology Drug Discovery and Development (ISB3D), 
School of Pharmacy, Virginia Commonwealth University, Richmond, VA 23298, USA.

While pharmacy education successfully employs various methodologies including 
case-based learning and simulated patient interactions, providing consistent, 
individualized guidance at scale remains challenging in team-based learning 
environments. Artificial intelligence (AI) offers potential solutions through 
automated facilitation, but its possible utility in pharmacy education remains 
unexplored. We developed and evaluated an AI-guided patient case discussion 
simulation to enhance learners' ability to integrate foundational science 
knowledge with clinical decision-making in a didactic immunology course in a US 
PharmD program. We utilized a large language model programmed with specific 
educational protocols and rubrics. Here, we present the step-by-step prompt 
engineering protocol as a toolkit. The system was structured around three core 
components in an immunology team-based learning activity: (1) symptomatology 
analysis, (2) laboratory test interpretation, and (3) pharmacist role definition 
and PPCP. Performance evaluation was conducted using a comprehensive rubric 
assessing multiple clinical reasoning and pharmaceutical knowledge domains. The 
standardized evaluation rubric showed reliable assessment across key 
competencies including condition identification (30% weighting), laboratory test 
interpretation (40% weighting), and pharmacist role understanding (30% 
weighting). Our AI patient simulator offers a scalable solution for 
standardizing clinical case discussions while maintaining individualized 
learning experiences.

DOI: 10.3390/pharmacy13020036
PMCID: PMC11932309
PMID: 40126309

Conflict of interest statement: D.W. and K.M. are the founders of 
PharmTutorAI.com, a platform dedicated to the creation and deployment of 
educational content to supplement traditional pharmacy education and a platform 
for PharmD students and faculty members to deploy their digital health 
applications for use by others.


1002. Healthcare (Basel). 2025 Jul 29;13(15):1843. doi: 10.3390/healthcare13151843.

Evaluating a Nationally Localized AI Chatbot for Personalized Primary Care 
Guidance: Insights from the HomeDOCtor Deployment in Slovenia.

Gams M(1)(2), Horvat T(1), Kolar Ž(1), Kocuvan P(1), Mishev K(3), Misheva MS(3).

Author information:
(1)Department of Intelligent Systems, Jožef Stefan Institute, 1000 Ljubljana, 
Slovenia.
(2)Applied Artificial Intelligence, Alma Mater Europaea University, Slovenska 
Street 17, 2000 Maribor, Slovenia.
(3)Faculty of Computer Science and Engineering, Ss. Cyril and Methodius 
University in Skopje, 1000 Skopje, North Macedonia.

Background/Objectives: The demand for accessible and reliable digital health 
services has increased significantly in recent years, particularly in regions 
facing physician shortages. HomeDOCtor, a conversational AI platform developed 
in Slovenia, addresses this need with a nationally adapted architecture that 
combines retrieval-augmented generation (RAG) and a Redis-based vector database 
of curated medical guidelines. The objective of this study was to assess the 
performance and impact of HomeDOCtor in providing AI-powered healthcare 
assistance. Methods: HomeDOCtor is designed for human-centered communication and 
clinical relevance, supporting multilingual and multimedia citizen inputs while 
being available 24/7. It was tested using a set of 100 international clinical 
vignettes and 150 internal medicine exam questions from the University of 
Ljubljana to validate its clinical performance. Results: During its six-month 
nationwide deployment, HomeDOCtor received overwhelmingly positive user feedback 
with minimal criticism, and exceeded initial expectations, especially in light 
of widespread media narratives warning about the risks of AI. HomeDOCtor 
autonomously delivered localized, evidence-based guidance, including self-care 
instructions and referral suggestions, with average response times under three 
seconds. On international benchmarks, the system achieved ≥95% Top-1 diagnostic 
accuracy, comparable to leading medical AI platforms, and significantly 
outperformed stand-alone ChatGPT-4o in the national context (90.7% vs. 80.7%, p 
= 0.0135). Conclusions: Practically, HomeDOCtor eases the burden on healthcare 
professionals by providing citizens with 24/7 autonomous, personalized triage 
and self-care guidance for less complex medical issues, ensuring that these 
cases are self-managed efficiently. The system also identifies more serious 
cases that might otherwise be neglected, directing them to professionals for 
appropriate care. Theoretically, HomeDOCtor demonstrates that domain-specific, 
nationally adapted large language models can outperform general-purpose models. 
Methodologically, it offers a framework for integrating GDPR-compliant AI 
solutions in healthcare. These findings emphasize the value of localization in 
conversational AI and telemedicine solutions across diverse national contexts.

DOI: 10.3390/healthcare13151843
PMCID: PMC12346038
PMID: 40805876

Conflict of interest statement: The authors declare no conflicts of interest.


1003. Healthcare (Basel). 2025 Dec 8;13(24):3208. doi: 10.3390/healthcare13243208.

Unveiling the Algorithm: The Role of Explainable Artificial Intelligence in 
Modern Surgery.

Lopes S(1), Mascarenhas M(2)(3)(4), Fonseca J(2)(3)(4), Fernandes MGO(2)(5), 
Leite-Moreira AF(2)(6).

Author information:
(1)Portuguese Institute of Oncology of Porto, 4200-072 Porto, Portugal.
(2)Faculty of Medicine, University of Porto, 4200-437 Porto, Portugal.
(3)Precision Medicine Unit, Department of Gastroenterology, Hospital São João, 
4200-437 Porto, Portugal.
(4)WGO Training Center, 4200-437 Porto, Portugal.
(5)Institute for Research and Innovation in Health-Associate Laboratory (i3s-LA) 
(IPATIMUP/I3S), 4200-135 Porto, Portugal.
(6)Department of Cardiothoracic Surgery, Hospital São João, 4200-437 Porto, 
Portugal.

Artificial Intelligence (AI) is rapidly transforming surgical care by enabling 
more accurate diagnosis and risk prediction, personalized decision-making, 
real-time intraoperative support, and postoperative management. Ongoing trends 
such as multi-task learning, real-time integration, and clinician-centered 
design suggest AI is maturing into a safe, pragmatic asset in surgical care. 
Yet, significant challenges, such as the complexity and opacity of many AI 
models (particularly deep learning), transparency, bias, data sharing, and 
equitable deployment, must be surpassed to achieve clinical trust, ethical use, 
and regulatory approval of AI algorithms in healthcare. Explainable Artificial 
Intelligence (XAI) is an emerging field that plays an important role in bridging 
the gap between algorithmic power and clinical use as surgery becomes 
increasingly data-driven. The authors reviewed current applications of XAI in 
the context of surgery-preoperative risk assessment, surgical planning, 
intraoperative guidance, and postoperative monitoring-and highlighted the 
absence of these mechanisms in Generative AI (e.g., ChatGPT). XAI will allow 
surgeons to interpret, validate, and trust AI tools. XAI applied in surgery is 
not a luxury: it must be a prerequisite for responsible innovation. Model bias, 
overfitting, and user interface design are key challenges that need to be 
overcome and will be explored in this review to achieve the integration of XAI 
into the surgical field. Unveiling the algorithm is the first step toward a 
safe, accountable, transparent, and human-centered surgical AI.

DOI: 10.3390/healthcare13243208
PMCID: PMC12732924
PMID: 41464276

Conflict of interest statement: The authors declare no conflicts of interest.


1004. Sensors (Basel). 2025 Aug 18;25(16):5117. doi: 10.3390/s25165117.

Development of an Interactive Digital Human with Context-Sensitive Facial 
Expressions.

Yang F(1), Fang L(1), Suo R(2), Zhang J(3), Whang M(3)(4).

Author information:
(1)Department of Emotion Engineering, Sangmyung University, Seoul 03016, 
Republic of Korea.
(2)College of Physical Education and Health Engineering, Hebei University of 
Engineering, Handan 056038, China.
(3)Jingjinji Spatial Intelligent Perception Collaborative Innovation Center, 
Hebei University of Engineering, Handan 056009, China.
(4)Department of Human-Centered Artificial Intelligence, Sangmyung University, 
Seoul 03016, Republic of Korea.

With the increasing complexity of human-computer interaction scenarios, 
conventional digital human facial expression systems show notable limitations in 
handling multi-emotion co-occurrence, dynamic expression, and semantic 
responsiveness. This paper proposes a digital human system framework that 
integrates multimodal emotion recognition and compound facial expression 
generation. The system establishes a complete pipeline for real-time interaction 
and compound emotional expression, following a sequence of "speech semantic 
parsing-multimodal emotion recognition-Action Unit (AU)-level 3D facial 
expression control." First, a ResNet18-based model is employed for robust 
emotion classification using the AffectNet dataset. Then, an AU motion curve 
driving module is constructed on the Unreal Engine platform, where dynamic 
synthesis of basic emotions is achieved via a state-machine mechanism. Finally, 
Generative Pre-trained Transformer (GPT) is utilized for semantic analysis, 
generating structured emotional weight vectors that are mapped to the AU layer 
to enable language-driven facial responses. Experimental results demonstrate 
that the proposed system significantly improves facial animation quality, with 
naturalness increasing from 3.54 to 3.94 and semantic congruence from 3.44 to 
3.80. These results validate the system's capability to generate realistic and 
emotionally coherent expressions in real time. This research provides a complete 
technical framework and practical foundation for high-fidelity digital humans 
with affective interaction capabilities.

DOI: 10.3390/s25165117
PMCID: PMC12390055
PMID: 40871980 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


1005. Bioengineering (Basel). 2025 Nov 26;12(12):1303. doi: 
10.3390/bioengineering12121303.

Agentic AI and Large Language Models in Radiology: Opportunities and 
Hallucination Challenges.

Salehi S(1), Singh Y(2), Horst KK(1), Hathaway QA(3), Erickson BJ(1).

Author information:
(1)Radiology Informatics Lab, Department of Radiology, Mayo Clinic, Rochester, 
MN 55905, USA.
(2)Department of Radiology, Mayo Clinic, Rochester, MN 55905, USA.
(3)Department of Radiology, University of Pennsylvania, Philadelphia, PA 19104, 
USA.

The field of radiology is experiencing rapid adoption of large language models 
(LLMs), yet their tendency to generate hallucinations (plausible but incorrect 
information) remains a significant barrier to trust. This comprehensive review 
evaluates emerging agentic artificial intelligence (AI) approaches, including 
multi-agent role-based systems, retrieval-augmented generation (RAG), and 
uncertainty quantification, to assess their potential for reducing 
hallucinations in radiology workflows. Evidence from 2024 to 2025 demonstrates 
that agentic AI can improve diagnostic accuracy and reduce error rates, though 
these methods remain computationally demanding and lack comprehensive clinical 
validation. Multi-agent frameworks enable cross-validation through role-based 
specialization and systematic workflow orchestration, while RAG strategies 
enhance accuracy by grounding responses in verified medical literature. Within 
multi-agent systems, uncertainty quantification enables agents to communicate 
confidence levels to one another, allowing them to appropriately weigh each 
other's contributions during collaborative analysis. While multi-agent 
frameworks and RAG strategies show significant promise, practical deployment 
will require careful integration with human oversight, robust evaluation metrics 
tailored to medical imaging tasks, and regulatory adaptation to ensure safe 
clinical use in diverse patient populations and imaging modalities.

DOI: 10.3390/bioengineering12121303
PMCID: PMC12729288
PMID: 41463600

Conflict of interest statement: The authors declare no conflicts of interest.


1006. Arthritis Care Res (Hoboken). 2026 Jan;78(1):123-133. doi: 10.1002/acr.25687. 
Epub 2025 Nov 11.

Centering Patient Voices in Lupus Pain: A Biopsychosocial Analysis of Reddit 
Narratives Using Large Language Models.

Walker A(1), Leung J(2), Alagappan A(3), Rajwal S(1), Lakamana S(1), Park T(1), 
Le N(3), Irani A(4), Sarker A(1)(5), Falasinnu T(3), Bozkurt S(1).

Author information:
(1)Deparment of Biomedical Informatics, Emory University School of Medicine, 
Atlanta, Georgia.
(2)Rollins School of Public Health, Emory University, Atlanta, Georgia.
(3)Division of Immunology and Rheumatology, Stanford University School of 
Medicine, Stanford, California.
(4)Mayo Clinic, Jacksonville, Florida.
(5)Biomedical Engineering, Georgia Institute of Technology, Atlanta, Georgia.

OBJECTIVE: Patients with chronic illness share their experiences in online 
communities and generate rich data on pain management. This study applied 
natural language processing methods, including large language models (LLMs), to 
Reddit discussions from lupus communities to characterize multidimensional pain 
experiences framed in the biopsychosocial model.
METHODS: We extracted Reddit posts from the r/Lupus and r/LupusSupport 
subreddits posted from June 9, 2010, through December 31, 2023. Pain-related 
posts were identified using a clinically informed pain lexicon. Topic modeling 
was used to identify thematic patterns, which were then compared with structured 
summaries generated by LLM instructions that were fine-tuned using the 
biopsychosocial model of pain. Two reviewers conducted content analysis of the 
LLM-generated summaries, evaluating thematic accuracy and coverage.
RESULTS: Data from Reddit included 31,785 posts from 10,857 authors. We 
identified common pain complaints, management strategies, and sociocultural, 
affective, and nociplastic dimensions of pain. Instruction fine-tuned LLMs 
produced structured summaries with an average thematic accuracy score of 3.1 of 
4 (kappa = .09) and content coverage score of 2.9 of 4 (kappa = .38). 
Sociocultural features presented in 123 posts (33.8%), including peer support 
and validation (n = 106) and provider interactions or access issues (n = 35). 
Nociplastic pain presented in 205 posts (56.3%).
CONCLUSION: Natural language processing methods can be used to extract rich, 
multidimensional insights into pain experiences from online communities focused 
on lupus. These approaches highlight the psychological, social, and cultural 
facets of pain that may be underrepresented in clinical settings, supporting 
more patient-centered approaches to care in rheumatology.

© 2025 American College of Rheumatology.

DOI: 10.1002/acr.25687
PMID: 41147795 [Indexed for MEDLINE]


1007. JMIR Dermatol. 2025 Nov 13;8:e81830. doi: 10.2196/81830.

Teledermatology to Support Self-Care in Chronic Spontaneous Urticaria.

Schuehlein L(1), Peters M(1), Jones G(2).

Author information:
(1)Novartis AG, Basel, Switzerland.
(2)Tufts University Medical Center, 800 Washington St, Boston, MA, 02115, United 
States, 1 8572757045.

Chronic spontaneous urticaria (CSU) is an autoimmune prompted skin disorder, 
whose hallmarks include the unpredictable onset of hives and itch. Symptom 
duration typically exceed 6 weeks, and flares can occur for up to 5 years or 
longer if untreated, impacting potentially any area of the body. The absence of 
obvious triggers and the variation in onset frequency often delays formal 
diagnosis which on average is approximately 2 years from first presentation. 
Initial standard of care is the use of low through to higher strength 
antihistamines in the first instance, with eventual escalation to prescription 
anti-inflammatory agents and potentially biologics once patients are under 
managed care. The societal impacts of delays in diagnosis are marked, with data 
suggesting CSU impacts up to 1% of the population, primarily of working age and 
with twice the prevalence in women. Herein, we advocate for the deployment of 
smartphone imaging and generative artificial intelligence technology to improve 
detection and early management of CSU through integrated self-care approaches. 
Such approaches embodying the tenets of P4 personalized medicine could have 
sustained impact on the disease through awareness campaigns, reducing the burden 
on the dermatology community and facilitating earlier access to curative 
therapeutic interventions.

© Laura Schuehlein, Martin Peters, Graham Jones. Originally published in JMIR 
Dermatology (http://derma.jmir.org).

DOI: 10.2196/81830
PMCID: PMC12614395
PMID: 41231969 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


1008. Sensors (Basel). 2025 Apr 10;25(8):2409. doi: 10.3390/s25082409.

A Survey of the State of the Art in Monocular 3D Human Pose Estimation: Methods, 
Benchmarks, and Challenges.

Guo Y(1), Gao T(1), Dong A(1), Jiang X(1), Zhu Z(1), Wang F(1).

Author information:
(1)Software College, Northeastern University, Shenyang 110004, China.

Three-dimensional human pose estimation (3D HPE) from monocular RGB cameras is a 
fundamental yet challenging task in computer vision, forming the basis of a wide 
range of applications such as action recognition, metaverse, self-driving, and 
healthcare. Recent advances in deep learning have significantly propelled the 
field, particularly with the incorporation of state-space models (SSMs) and 
diffusion models. However, systematic reviews that comprehensively cover these 
emerging techniques remain limited. This survey contributes to the literature by 
providing the first comprehensive analysis of recent innovative approaches, 
featuring diffusion models and SSMs within 3D HPE. It categorizes and analyzes 
various techniques, highlighting their strengths, limitations, and notable 
innovations. Additionally, it provides a detailed overview of commonly employed 
datasets and evaluation metrics. Furthermore, this survey offers an in-depth 
discussion on key challenges, particularly depth ambiguity and occlusion issues 
arising from single-view setups, thoroughly reviewing effective solutions 
proposed in recent studies. Finally, current applications and promising avenues 
for future research are highlighted to guide and inspire ongoing innovation in 
the area, with emerging trends such as integrating large language models (LLMs) 
to provide semantic priors and prompt-based supervision for improved 3D pose 
estimation.

DOI: 10.3390/s25082409
PMCID: PMC12031093
PMID: 40285099 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


1009. Cancers (Basel). 2025 Jul 17;17(14):2376. doi: 10.3390/cancers17142376.

Decoding the JAK-STAT Axis in Colorectal Cancer with AI-HOPE-JAK-STAT: A 
Conversational Artificial Intelligence Approach to Clinical-Genomic Integration.

Yang EW(1), Waldrup B(2), Velazquez-Villarreal E(2)(3).

Author information:
(1)PolyAgent, San Francisco, CA 94102, USA.
(2)Department of Integrative Translational Sciences, Beckman Research Institute 
of City of Hope, Duarte, CA 91010, USA.
(3)City of Hope Comprehensive Cancer Center, Duarte, CA 91010, USA.

Background/Objectives: The Janus kinase-signal transducer and activator of 
transcription (JAK-STAT) signaling pathway is a critical mediator of immune 
regulation, inflammation, and cancer progression. Although implicated in 
colorectal cancer (CRC) pathogenesis, its molecular heterogeneity and clinical 
significance remain insufficiently characterized-particularly within early-onset 
CRC (EOCRC) and across diverse treatment and demographic contexts. We present 
AI-HOPE-JAK-STAT, a novel conversational artificial intelligence platform built 
to enable the real-time, natural language-driven exploration of JAK/STAT pathway 
alterations in CRC. The platform integrates clinical, genomic, and treatment 
data to support dynamic, hypothesis-generating analyses for precision oncology. 
Methods: AI-HOPE-JAK-STAT combines large language models (LLMs), a natural 
language-to-code engine, and harmonized public CRC datasets from cBioPortal. 
Users define analytical queries in plain English, which are translated into 
executable code for cohort selection, survival analysis, odds ratio testing, and 
mutation profiling. To validate the platform, we replicated known associations 
involving JAK1, JAK3, and STAT3 mutations. Additional exploratory analyses 
examined age, treatment exposure, tumor stage, and anatomical site. Results: The 
platform recapitulated established trends, including improved survival among 
EOCRC patients with JAK/STAT pathway alterations. In FOLFOX-treated CRC cohorts, 
JAK/STAT-altered tumors were associated with significantly enhanced overall 
survival (p < 0.0001). Stratification by age revealed survival advantages in 
younger (age < 50) patients with JAK/STAT mutations (p = 0.0379). STAT5B 
mutations were enriched in colon adenocarcinoma and correlated with 
significantly more favorable trends (p = 0.0000). Conversely, JAK1 mutations in 
microsatellite-stable tumors did not affect survival, emphasizing the value of 
molecular context. Finally, JAK3-mutated tumors diagnosed at Stage I-III showed 
superior survival compared to Stage IV cases (p = 0.00001), reinforcing stage as 
a dominant clinical determinant. Conclusions: AI-HOPE-JAK-STAT establishes a new 
standard for pathway-level interrogation in CRC by empowering users to generate 
and test clinically meaningful hypotheses without coding expertise. This system 
enhances access to precision oncology analyses and supports the scalable, 
real-time discovery of survival trends, mutational associations, and 
treatment-response patterns across stratified patient cohorts.

DOI: 10.3390/cancers17142376
PMCID: PMC12293912
PMID: 40723258

Conflict of interest statement: The authors declare no conflicts of interest.


1010. Diagn Interv Imaging. 2026 Jan;107(1):7-16. doi: 10.1016/j.diii.2025.10.002. 
Epub 2025 Oct 25.

Agentic systems in radiology: Principles, opportunities, privacy risks, 
regulation, and sustainability concerns.

Tzanis E(1), Adams LC(2), Akinci D'Antonoli T(3), Bressem KK(4), Cuocolo R(5), 
Kocak B(6), Malamateniou C(7), Klontzas ME(8).

Author information:
(1)Artificial Intelligence and Translational Imaging (ATI) Lab, Department of 
Radiology, School of Medicine, University of Crete, 70013 Heraklion, Greece.
(2)Department of Diagnostic and Interventional Radiology, Technical University 
of Munich, School of Medicine and Health, Klinikum Rechts der Isar, TUM 
University Hospital, 81675 Munich, Germany.
(3)Department of Diagnostic and Interventional Neuroradiology, University 
Hospital Basel, CH-4031 Basel, Switzerland; Department of Pediatric Radiology, 
University Children's Hospital Basel, CH-4056 Basel, Switzerland.
(4)Department of Diagnostic and Interventional Radiology, Technical University 
of Munich, School of Medicine and Health, Klinikum Rechts der Isar, TUM 
University Hospital, 81675 Munich, Germany; Department of Cardiovascular 
Radiology and Nuclear Medicine, Technical University of Munich, School of 
Medicine and Health, German Heart Center, TUM University Hospital, 80636 Munich, 
Germany.
(5)Department of Medicine, Surgery and Dentistry, University of Salerno, 84081 
Baronissi, Italy.
(6)Department of Radiology, Basaksehir Cam and Sakura City Hospital, 34480 
Istanbul, Turkey.
(7)CRRAG Research group, Division of Radiography, School of Health and Medical 
Sciences, City St George's University of London, SW17 0RE London, UK.
(8)Artificial Intelligence and Translational Imaging (ATI) Lab, Department of 
Radiology, School of Medicine, University of Crete, 70013 Heraklion, Greece; 
Department of Medical Imaging, University Hospital of Heraklion, 71003 
Heraklion, Crete, Greece; Division of Radiology, Department of Clinical Science 
Intervention and Technology (CLINTEC), Karolinska Institute, SE-14152 Huddinge, 
Sweden. Electronic address: miklontzas@uoc.gr.

The rapid rise of transformer-based large language models (LLMs) has introduced 
new opportunities for automation and decision support in radiology, particularly 
in applications such as report generation, protocol optimization, and structured 
interpretation. Despite their impressive performance in producing contextually 
coherent text, conventional LLMs remain limited by their inability to interact 
autonomously with external systems, retrieve data, or execute code, restricting 
their role in real-world clinical and research workflows. To address these 
limitations, agentic systems have emerged as a new paradigm. By embedding LLMs 
within frameworks that enable reasoning, planning, and action, agentic systems 
extend LLM capabilities to dynamic interaction with users, tools, and data 
sources. This review provides a comprehensive overview of the foundations, 
architectures, and operational mechanisms of agentic systems, focusing on their 
applications in medical imaging and radiology. It summarizes key developments in 
the literature, including recent multi-agent frameworks for automated radiomics 
pipelines, and discusses the potential benefits of these systems in enhancing 
the reproducibility, interpretability, and accessibility of AI-driven workflows. 
The review critically examines current regulatory considerations, ethical 
implications, and sustainability challenges to highlight essential gaps that 
must be addressed for the safe and responsible clinical integration of these 
systems.

Copyright © 2025 The Author(s). Published by Elsevier Masson SAS.. All rights 
reserved.

DOI: 10.1016/j.diii.2025.10.002
PMID: 41432042 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial or personal relationships 
that could be viewed as influencing the work reported in this paper.


1011. Diagnostics (Basel). 2025 Dec 29;16(1):107. doi: 10.3390/diagnostics16010107.

ZebraMap: A Multimodal Rare Disease Knowledge Map with Automated Data 
Aggregation & LLM-Enriched Information Extraction Pipeline.

Islam MS(1)(2), Jamal A(1)(3), Alkhathlan A(1).

Author information:
(1)Department of Computer Science, FCIT, King Abdulaziz University, Jeddah 
21589, Saudi Arabia.
(2)Department of Computer Science, FSIT, Daffodil International University, 
Birulia 1216, Bangladesh.
(3)Center of Research Excellence in Artificial Intelligence and Data Science, 
King Abdulaziz University, Jeddah 21589, Saudi Arabia.

Background: Rare diseases often lead to delayed diagnosis because clinical 
knowledge is fragmented across unstructured research, individual case reports, 
and heterogeneous data formats. This study presents ZebraMap, a multimodal 
knowledge map created to consolidate rare disease information and transform 
narrative case evidence into structured, machine-readable data. Methods: Using 
Orphanet as the disease registry, we identified 1727 rare diseases and linked 
them to PubMed case reports. We retrieved 36,131 full-text case report articles 
that met predefined inclusion criteria and extracted publication metadata, 
patient demographics, clinical narratives (cases), and associated images. A 
central methodological contribution is an automated large language model (LLM) 
structuring pipeline, in which free-text case reports are parsed into 
standardized fields, such as symptoms, diagnostic methods, differential 
diagnoses, treatments, and outcome that produce structured case representations 
and image metadata matching the schema demonstrated in our extended dataset. In 
parallel, a retrieval-augmented generation (RAG) component generates concise 
summaries of epidemiology, etiology, clinical symptoms, and diagnostic 
techniques by retrieving peer-reviewed research to enhance missing disease-level 
descriptions. Results: The final dataset contains 69,146 structured 
patient-level case texts and 98,038 clinical images, each linked to a particular 
patient ID, disease entry, and publication. Overall cosine similarity between 
curated and generated text is 94.5% and performance in information extraction 
and structured data generation is satisfactory. Conclusions: ZebraMap provides 
the largest openly accessible multimodal resource for rare diseases and enables 
data-driven research by converting narrative evidence into computable knowledge.

DOI: 10.3390/diagnostics16010107
PMCID: PMC12785374
PMID: 41515602

Conflict of interest statement: The authors declare no conflicts of interest. 
The funders had no role in the design of this study; in the collection, 
analyses, or interpretation of data; in the writing of the manuscript; or in the 
decision to publish the results.


1012. Cancers (Basel). 2025 Nov 29;17(23):3836. doi: 10.3390/cancers17233836.

Deriving Real-World Evidence from Non-English Electronic Medical Records in 
Hormone Receptor-Positive Breast Cancer Using Large Language Models.

Meretukov D(1), Grechukhina K(2), Evdokimov V(3), Didych D(4), Kondratieva S(4), 
Rakitina O(4), Gordeev A(5), Shilo P(6), Khatkov I(2), Zhukova L(2).

Author information:
(1)Department of Science, N.N. Blokhin Cancer Research Center, Moscow 115478, 
Russia.
(2)SBIH Moscow Clinical Scientific Center Named After A.S. Loginov of DHM, 
Moscow 111123, Russia.
(3)SBIH Moscow Multidisciplinary Clinical Center "Kommunarka" of DHM, Moscow 
117638, Russia.
(4)Group of Gene Immuno-Oncotherapy, Department of Genomics and Postgenomic 
Technologies, Shemyakin-Ovchinnikov Institute of Bioorganic Chemistry of the 
Russian Academy of Sciences, Moscow 117997, Russia.
(5)LLC "Technology of Trust", Moscow 125167, Russia.
(6)Lahta Clinic Medical Center, St. Petersburg 197183, Russia.

Background/Objectives: Large language models (LLMs) have been proposed as a 
means of converting unstructured electronic medical records (EMRs) into 
structured datasets. However, concerns regarding the reliability of these models 
in non-English clinical text and their capacity to generate novel insights 
remain unresolved. We aimed to utilize an LLM to identify a hypothetical 
"Luminal B poor-prognosis" breast cancer subgroup (LPP) based on progesterone 
receptor (PR), the Ki-67 proliferation index, and grade characteristics, while 
concurrently validating the LLM's accuracy. Methods: We retrospectively compiled 
the EMRs on 7756 female breast cancer patients from five Moscow oncology 
centers. An LLM with a domain-engineered prompt extracted eight 
clinicopathological variables (Ki-67, estrogen receptor (ER)/PR Allred status, 
HER2 status, grade, relapse dates, and multiple primaries). The accuracy of the 
model was validated in 366 randomly sampled cases against oncologist annotations 
using Intraclass Correlation Coefficient (ICC) and weighted κ. Following data 
post-processing, the complete-case cohort (n = 2347) and the HR+/HER2- stage 
I-III sub-cohort (n = 1419) were analyzed. Survival was estimated with 
Kaplan-Meier/log-rank and modeled with Cox regression (adjusted for age, stage, 
and treatment). Ki-67 was modeled continuously; prespecified LPP definitions 
were compared. Results: LLM-human agreement was high (Ki-67 ICC = 0.882; grade κ 
= 0.887; ER κ = 0.997; PR κ = 0.975; HER2 κ = 0.935). Date extraction was 
characterized by a high degree of missing data. In HR+/HER2- stage I-III 
disease, ER < 5 was non-prognostic; however, PR < 4 and Ki-67 ≥ 40% were 
indicative of inferior survival (HR 2.25 and 1.85). The most effective LPP 
definition (PR < 4 and Ki-67 ≥ 40%) identified a subgroup (~5.3%) of patients 
with markedly poorer outcomes (age, stage, and treatment adjusted HR 2.60, 95% 
CI 1.53-4.43) compared to the Luminal B (HER2-) subgroup. Conclusions: The 
developed LLM has demonstrated the ability to reliably structure non-English 
EMRs and enable discovery of clinically meaningful subgroups. The discovered LPP 
phenotype defines a small, high-risk subset warranting external validation. 
Given the retrospective, single-system design of the study, it is imperative to 
interpret the discovered phenotype features as hypothesis-generating, rather 
than as definitive evidence.

DOI: 10.3390/cancers17233836
PMCID: PMC12691197
PMID: 41375036

Conflict of interest statement: Alexander Gordeev is currently employed by LLC 
"Technology of Trust". However, his contribution to this manuscript was made in 
a purely personal and voluntary capacity, outside of his professional duties, 
and is unrelated to any work or projects conducted at LLC "Technology of Trust”. 
He received no financial or material support from LLC "Technology of Trust" or 
any other organization for this work. His participation was motivated by 
personal scientific interest in oncology and artificial intelligence. The other 
authors declare no conflicts of interest. The funders had no role in the design 
of the study; in the collection, analyses, or interpretation of the data; in the 
writing of the manuscript; or in the decision to publish the results.


1013. Sensors (Basel). 2025 May 7;25(9):2952. doi: 10.3390/s25092952.

Research Progress on Data-Driven Industrial Fault Diagnosis Methods.

Lei L(1), Li W(1), Zhang S(2), Wu C(1), Yu H(3).

Author information:
(1)School of Artificial Intelligence, Xidian University, Xi'an 710071, China.
(2)School of Information Science and Technology, Northwestern University, Xi'an 
710127, China.
(3)Hangzhou Institute of Technology, Xidian University, Hangzhou 311231, China.

With the advent of Industry 5.0, fault diagnosis is playing an increasingly 
important role in routine equipment maintenance and condition monitoring. From 
the perspective of industrial big data, this paper systematically reviews the 
current mainstream industrial fault diagnosis methods. The content covers the 
main sources of industrial big data, commonly used datasets, and the 
construction of related platforms. In conjunction with the development of 
multi-source heterogeneous data, the paper explores the evolutionary path of 
fault diagnosis methods. Subsequently, it provides an in-depth analysis of 
data-driven fault diagnosis techniques in industrial applications, with 
particular emphasis on the pivotal role of deep learning algorithms in fault 
diagnosis. Next, it discusses the applications and development of large models 
in the field of fault diagnosis, focusing on their potential to enhance 
diagnostic intelligence and generalization under big data environments. Finally, 
the paper looks ahead to the future development of data-driven fault diagnosis 
methods, pointing out that data quality, interpretability of deep learning, and 
edge-based large models are important research directions that urgently require 
breakthroughs.

DOI: 10.3390/s25092952
PMCID: PMC12074220
PMID: 40363389

Conflict of interest statement: The authors declare no conflicts of interest.


1014. Sensors (Basel). 2025 Jul 4;25(13):4164. doi: 10.3390/s25134164.

Fault Detection of Cyber-Physical Systems Using a Transfer Learning Method Based 
on Pre-Trained Transformers.

Sajjadi P(1), Dinmohammadi F(1), Shafiee M(2).

Author information:
(1)School of Computing and Engineering, University of West London, London W5 
5RF, UK.
(2)School of Engineering, University of Surrey, Guildford GU2 7XH, UK.

As industries become increasingly dependent on cyber-physical systems (CPSs), 
failures within these systems can cause significant operational disruptions, 
underscoring the critical need for effective Prognostics and Health Management 
(PHM). The large volume of data generated by CPSs has made deep learning (DL) 
methods an attractive solution; however, imbalanced datasets and the limited 
availability of fault-labeled data continue to hinder their effective deployment 
in real-world applications. To address these challenges, this paper proposes a 
transfer learning approach using a pre-trained transformer architecture to 
enhance fault detection performance in CPSs. A streamlined transformer model is 
first pre-trained on a large-scale source dataset and then fine-tuned end-to-end 
on a smaller dataset with a differing data distribution. This approach enables 
the transfer of diagnostic knowledge from controlled laboratory environments to 
real-world operational settings, effectively addressing the domain shift 
challenge commonly encountered in industrial CPSs. To evaluate the effectiveness 
of the proposed method, extensive experiments are conducted on publicly 
available datasets generated from a laboratory-scale replica of a modern 
industrial water purification facility. The results show that the model achieves 
an average F1-score of 93.38% under K-fold cross-validation, outperforming 
baseline models such as CNN and LSTM architectures, and demonstrating the 
practicality of applying transformer-based transfer learning in industrial 
settings with limited fault data. To enhance transparency and better understand 
the model's decision process, SHAP is applied for explainable AI (XAI).

DOI: 10.3390/s25134164
PMCID: PMC12252413
PMID: 40648419

Conflict of interest statement: The authors declare no conflicts of interest.


1015. Bioengineering (Basel). 2025 Dec 10;12(12):1345. doi: 
10.3390/bioengineering12121345.

An External Validation Study on Two Pre-Trained Large Language Models for 
Multimodal Prognostication in Laryngeal and Hypopharyngeal Cancer: Integrating 
Clinical, Treatment, and Radiomic Data to Predict Survival Outcomes with 
Interpretable Reasoning.

Yap WK(1), Cheng SC(2), Lin CH(1)(3), Hsiao IT(4), Tsai TY(5), Yap WL(6), Chen 
WP(1), Lin CY(1), Huang SM(7).

Author information:
(1)Department of Radiation Oncology, Proton and Radiation Therapy Center, Linkou 
Chang Gung Memorial Hospital, College of Medicine, Chang Gung University, 
Kwei-Shan, Taoyuan 333, Taiwan.
(2)Department of Medical Imaging and Radiological Sciences, College of Medicine, 
Chang Gung University, Taoyuan 333, Taiwan.
(3)UTHealth Graduate School of Biomedical Sciences, The University of Texas MD 
Anderson Cancer Center, Houston, TX 77030, USA.
(4)Department of Medical Imaging and Radiological Sciences, Healthy Aging 
Research Center, Chang Gung University, Taoyuan 333, Taiwan.
(5)Department of Otolaryngology-Head and Neck Surgery, Linkou Chang Gung 
Memorial Hospital, College of Medicine, Chang Gung University, Kwei-Shan, 
Taoyuan 333, Taiwan.
(6)Department of Post-Baccalaureate Medicine, Kaohsiung Medical University, 
Kaohsiung 807, Taiwan.
(7)Department of Radiation Oncology, Keelung Chang Gung Memorial Hospital, 
Keelung 204, Taiwan.

Background: Laryngeal and hypopharyngeal cancers (LHCs) exhibit heterogeneous 
outcomes after definitive radiotherapy (RT). Large language models (LLMs) may 
enhance prognostic stratification by integrating complex clinical and imaging 
data. This study validated two pre-trained LLMs-GPT-4o-2024-08-06 and 
Gemma-2-27b-it-for outcome prediction in LHC. Methods: Ninety-two patients with 
non-metastatic LHC treated with definitive (chemo)radiotherapy at Linkou Chang 
Gung Memorial Hospital (2006-2013) were retrospectively analyzed. First-order 
and 3D radiomic features were extracted from intra- and peritumoral regions on 
pre- and mid-RT CT scans. LLMs were prompted with clinical variables, 
radiotherapy notes, and radiomic features to classify patients as high- or 
low-risk for death, recurrence, and distant metastasis. Model performance was 
assessed using sensitivity, specificity, AUC, Kaplan-Meier survival analysis, 
and McNemar tests. Results: Integration of radiomic features significantly 
improved prognostic discrimination over clinical/RT plan data alone for both 
LLMs. For death prediction, pre-RT radiomics were the most predictive: GPT-4o 
achieved a peak AUC of 0.730 using intratumoral features, while Gemma-2-27b 
reached 0.736 using peritumoral features. For recurrence prediction, mid-RT 
peritumoral features yielded optimal performance (AUC = 0.703 for GPT-4o; AUC = 
0.709 for Gemma-2-27b). Kaplan-Meier analyses confirmed statistically 
significant separation of risk groups: pre-RT intra- and peritumoral features 
for overall survival (for both GPT-4o and Gemma-2-27b, p < 0.05), and mid-RT 
peritumoral features for recurrence-free survival (p = 0.028 for GPT-4o; p = 
0.017 for Gemma-2-27b). McNemar tests revealed no significant performance 
difference between the two LLMs when augmented with radiomics (all p > 0.05), 
indicating that the open-source model achieved comparable accuracy to its 
proprietary counterpart. Both models generated clinically coherent, 
patient-specific rationales explaining risk assignments, enhancing 
interpretability and clinical trust. Conclusions: This external validation 
demonstrates that pre-trained LLMs can serve as accurate, interpretable, and 
multimodal prognostic engines for LHC. Pre-RT radiomic features are critical for 
predicting mortality and metastasis, while mid-RT peritumoral features uniquely 
inform recurrence risk. The comparable performance of the open-source 
Gemma-2-27b-it model suggests a scalable, cost-effective, and privacy-preserving 
pathway for the integration of LLM-based tools into precision radiation oncology 
workflows to enhance risk stratification and therapeutic personalization.

DOI: 10.3390/bioengineering12121345
PMCID: PMC12729448
PMID: 41463641

Conflict of interest statement: The authors declare that they have no known 
competing financial interests or personal relationships that could have appeared 
to influence the work reported in this paper.


1016. Healthcare (Basel). 2025 Aug 28;13(17):2144. doi: 10.3390/healthcare13172144.

Assessing the Accuracy and Completeness of AI-Generated Dental Responses: An 
Evaluation of the Chat-GPT Model.

Othman AA(1), Sharqawi AJ(2), MohammedAziz AA(3), Ali WA(3), Alatiyyah AA(3), 
Mirah MA(4).

Author information:
(1)Department of Oral and Maxillofacial Diagnostic Sciences, College of 
Dentistry, Taibah University, Al-Madinah Al-Munawwarah 42353, Saudi Arabia.
(2)Department of Preventive Dental Sciences, College of Dentistry, Taibah 
University, Al-Madinah Al-Munawwarah 42353, Saudi Arabia.
(3)College of Dentistry, Taibah University, Al-Madinah Al-Munawwarah 42353, 
Saudi Arabia.
(4)Department of Restorative Dental Sciences, College of Dentistry, Taibah 
University, Al-Madinah Al-Munawwarah 42353, Saudi Arabia.

Background: The rapid advancement of artificial intelligence (AI) in healthcare 
has opened new opportunities, yet the clinical validation of AI tools in 
dentistry remains limited. Objectives: This study aimed to assess the 
performance of ChatGPT in generating accurate and complete responses to academic 
dental questions across multiple specialties, comparing the capabilities of 
GPT-4 and GPT-3.5 models. Methodology: A panel of academic specialists from 
eight dental specialties collaboratively developed 48 clinical questions, 
classified by consensus as easy, medium, or hard, and as requiring either binary 
(yes/no) or descriptive responses. Each question was sequentially entered into 
both GPT-4 and GPT-3.5 models, with instructions to provide guideline-based 
answers. The AI-generated responses were independently evaluated by the 
specialists for accuracy (6-point Likert scale) and completeness (3-point Likert 
scale). Descriptive and inferential statistics were applied, including 
Mann-Whitney U and Kruskal-Wallis tests, with significance set at p < 0.05. 
Results: GPT-4 consistently outperformed GPT-3.5 in both evaluation domains. The 
median accuracy score was 6.0 for GPT-4 and 5.0 for GPT-3.5 (p = 0.02), while 
the median completeness score was 3.0 for GPT-4 and 2.0 for GPT-3.5 (p < 0.001). 
GPT-4 demonstrated significantly higher overall accuracy (5.29 ± 1.1) and 
completeness (2.44 ± 0.71) compared to GPT-3.5 (4.5 ± 1.7 and 1.69 ± 0.62, 
respectively; p = 0.024 and <0.001). When stratified by specialty, notable 
improvements with GPT-4 were observed in Periodontology, Endodontics, 
Implantology, and Oral Surgery, particularly in completeness scores. 
Conclusions: In academic dental settings, GPT-4 provided more accurate and 
complete responses than GPT-3.5. Despite both models showing potential, their 
clinical application should remain supervised by human experts.

DOI: 10.3390/healthcare13172144
PMCID: PMC12428179
PMID: 40941502

Conflict of interest statement: The authors declare no conflicts of interest.


1017. Diagnostics (Basel). 2025 Sep 12;15(18):2315. doi: 10.3390/diagnostics15182315.

Performance of a Vision-Language Model in Detecting Common Dental Conditions on 
Panoramic Radiographs Using Different Tooth Numbering Systems.

Liu Z(1), Ai QYH(2), Yeung AWK(1), Tanaka R(1), Nalley A(1), Hung KF(1).

Author information:
(1)Oral and Maxillofacial Radiology, Applied Oral Sciences and Community Dental 
Care, Faculty of Dentistry, The University of Hong Kong, Hong Kong SAR 999077, 
China.
(2)Department of Diagnostic Radiology, Li Ka Shing Faculty of Medicine, The 
University of Hong Kong, Hong Kong SAR 999077, China.

Objectives: The aim of this study was to evaluate the performance of GPT-4o in 
identifying nine common dental conditions on panoramic radiographs, both overall 
and at specific tooth sites, and to assess whether the use of different tooth 
numbering systems (FDI and Universal) in prompts would affect its diagnostic 
accuracy. Methods: Fifty panoramic radiographs exhibiting various common dental 
conditions including missing teeth, impacted teeth, caries, endodontically 
treated teeth, teeth with restorations, periapical lesions, periodontal bone 
loss, tooth fractures, cracks, retained roots, dental implants, osteolytic 
lesions, and osteosclerosis were included. Each image was evaluated twice by 
GPT-4o in May 2025, using structured prompts based on either the FDI or 
Universal tooth numbering system, to identify the presence of these conditions 
at specific tooth sites or regions. GPT-4o responses were compared to a 
consensus reference standard established by an oral-maxillofacial radiology 
team. GPT-4o's performance was evaluated using balanced accuracy, sensitivity, 
specificity, and F1 score both at the patient and tooth levels. Results: A total 
of 100 GPT-4o responses were generated. At the patient level, balanced accuracy 
ranged from 46.25% to 98.83% (FDI) and 49.75% to 92.86% (Universal), with the 
highest accuracies for dental implants (92.86-98.83%). F1-scores and 
sensitivities were highest for implants, missing, and impacted teeth, but zero 
for caries, periapical lesions, and fractures. Specificity was generally high 
across conditions. Notable discrepancies were observed between patient- and 
tooth-level performance, especially for implants and restorations. GPT-4o's 
performance was similar between using the two numbering systems. Conclusions: 
GPT-4o demonstrated superior performance in detecting dental implants and 
treated or restored teeth but inferior performance for caries, periapical 
lesions, and fractures. Diagnostic accuracy was higher at the patient level than 
at the tooth level, with similar performances for both numbering systems. Future 
studies with larger, more diverse datasets and multiple models are needed.

DOI: 10.3390/diagnostics15182315
PMCID: PMC12468776
PMID: 41008687

Conflict of interest statement: The authors declare no conflicts of interest.


1018. Nature. 2025 Nov;647(8088):248-256. doi: 10.1038/s41586-025-09529-3. Epub 2025 
Sep 17.

Learning the natural history of human disease with generative transformers.

Shmatko A(#)(1)(2)(3), Jung AW(#)(2)(4)(5)(6), Gaurav K(#)(2), Brunak S(4)(7), 
Mortensen LH(5)(7)(8), Birney E(9), Fitzgerald T(10), Gerstung 
M(11)(12)(13)(14)(15)(16).

Author information:
(1)Division of AI in Oncology, German Cancer Research Centre DKFZ, Heidelberg, 
Germany.
(2)European Molecular Biology Laboratory, European Bioinformatics Institute 
EMBL-EBI, Hinxton, UK.
(3)Faculty of Biosciences, Heidelberg University, Heidelberg, Germany.
(4)Novo Nordisk Foundation Center for Protein Research, Faculty of Health and 
Medical Sciences, University of Copenhagen, Copenhagen, Denmark.
(5)Statistics Denmark, Copenhagen, Denmark.
(6)Department of Biosystems Science and Engineering, ETH Zurich, Basel, 
Switzerland.
(7)Department of Public Health, University of Copenhagen, Copenhagen, Denmark.
(8)ROCKWOOL Foundation, Copenhagen, Denmark.
(9)European Molecular Biology Laboratory, European Bioinformatics Institute 
EMBL-EBI, Hinxton, UK. birney@ebi.ac.uk.
(10)European Molecular Biology Laboratory, European Bioinformatics Institute 
EMBL-EBI, Hinxton, UK. tomas@ebi.ac.uk.
(11)Division of AI in Oncology, German Cancer Research Centre DKFZ, Heidelberg, 
Germany. moritz.gerstung@dkfz.de.
(12)European Molecular Biology Laboratory, European Bioinformatics Institute 
EMBL-EBI, Hinxton, UK. moritz.gerstung@dkfz.de.
(13)Faculty of Mathematics and Computer Science, Heidelberg University, 
Heidelberg, Germany. moritz.gerstung@dkfz.de.
(14)Robert Bosch Center for Tumor Diseases, Stuttgart, Germany. 
moritz.gerstung@dkfz.de.
(15)Medical Faculty, Eberhard-Karls-University, Tübingen, Germany. 
moritz.gerstung@dkfz.de.
(16)University Hospital Tübingen, Tübingen, Germany. moritz.gerstung@dkfz.de.
(#)Contributed equally

Erratum in
    Nature. 2025 Nov;647(8091):E8. doi: 10.1038/s41586-025-09879-y.

Decision-making in healthcare relies on understanding patients' past and current 
health states to predict and, ultimately, change their future course1-3. 
Artificial intelligence (AI) methods promise to aid this task by learning 
patterns of disease progression from large corpora of health records4,5. 
However, their potential has not been fully investigated at scale. Here we 
modify the GPT6 (generative pretrained transformer) architecture to model the 
progression and competing nature of human diseases. We train this model, 
Delphi-2M, on data from 0.4 million UK Biobank participants and validate it 
using external data from 1.9 million Danish individuals with no change in 
parameters. Delphi-2M predicts the rates of more than 1,000 diseases, 
conditional on each individual's past disease history, with accuracy comparable 
to that of existing single-disease models. Delphi-2M's generative nature also 
enables sampling of synthetic future health trajectories, providing meaningful 
estimates of potential disease burden for up to 20 years, and enabling the 
training of AI models that have never seen actual data. Explainable AI methods7 
provide insights into Delphi-2M's predictions, revealing clusters of 
co-morbidities within and across disease chapters and their time-dependent 
consequences on future health, but also highlight biases learnt from training 
data. In summary, transformer-based models appear to be well suited for 
predictive and generative health-related tasks, are applicable to 
population-scale datasets and provide insights into temporal dependencies 
between disease events, potentially improving the understanding of personalized 
health risks and informing precision medicine approaches.

© 2025. The Author(s).

DOI: 10.1038/s41586-025-09529-3
PMCID: PMC12589094
PMID: 40963019 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: A patent has been filed for 
the use of generative transformer architectures to model competing risk and 
timings of diseases (application number: PCT/EP2025/065771; applicants: DKFZ, 
EMBL), with M.G., A.S., T.F., E.B., K.G. and A.W.J. listed as inventors. S.B. 
has ownership interests in Hoba Therapeutics Aps, Novo Nordisk, Lundbeck and Eli 
Lilly. E.B. is a consultant and shareholder of Oxford Nanopore. The other 
authors declare no competing interests.


1019. Diagnostics (Basel). 2025 Apr 4;15(7):930. doi: 10.3390/diagnostics15070930.

Information Extraction from Lumbar Spine MRI Radiology Reports Using GPT4: 
Accuracy and Benchmarking Against Research-Grade Comprehensive Scoring.

Ziegeler K(1), Kreutzinger V(1), Tong MW(1)(2)(3), Chin CT(1), Bahroos E(1), Wu 
PH(1)(4)(5), Bonnheim N(4)(5), Fields AJ(4)(5), Lotz JC(5), Link TM(1), Majumdar 
S(1).

Author information:
(1)Department of Radiology and Biomedical Imaging, University of California San 
Francisco, San Francisco, CA 94143, USA.
(2)Department of Bioengineering, University of California Berkeley, Berkeley, CA 
94720, USA.
(3)Department of Bioengineering, University of California San Francisco, San 
Francisco, CA 94143, USA.
(4)The UCSF REACH Center, The Core Center for Patient-Centric Mechanistic 
Phenotyping in Chronic Low Back Pain, San Francisco, CA 94143, USA.
(5)Department of Orthopaedic Surgery, University of California San Francisco, 
San Francisco, CA 94143, USA.

Background/Objectives: This study aimed to create a pipeline for standardized 
data extraction from lumbar-spine MRI radiology reports using a large language 
model (LLM) and assess the agreement of the extracted data with research-grade 
semi-quantitative scoring. Methods: We included a subset of data from a 
multi-site NIH-funded cohort study of chronic low back pain (cLBP) participants. 
After initial prompt development, a secure application programming interface 
(API) deployment of OpenAIs GPT-4 was used to extract different classes of 
pathology from the clinical radiology report. Unsupervised UMAP and 
agglomerative clustering of the pathology terms' embeddings provided insight 
into model comprehension for optimized prompt design. Model extraction was 
benchmarked against human extraction (gold standard) with F1 scores and 
false-positive and false-negative rates (FPR/FNR). Then, an expert MSK 
radiologist provided comprehensive research-grade scores of the images, and 
agreement with report-extracted data was calculated using Cohen's kappa. 
Results: Data from 230 patients with cLBP were included (mean age 53.2 years, 
54% women). The overall model performance for extracting data from clinical 
reports was excellent, with a mean F1 score of 0.96 across pathologies. The mean 
FPR was marginally higher than the FNR (5.1% vs. 3.0%). Agreement with 
comprehensive scoring was moderate (kappa 0.424), and the underreporting of 
lateral recess stenosis (FNR 63.6%) and overreporting of disc pathology (FPR 
42.7%) were noted. Conclusions: LLMs can accurately extract highly detailed 
information on lumbar spine imaging pathologies from radiology reports. Moderate 
agreement between the LLM and comprehensive scores underscores the need for less 
subjective, machine-based data extraction from imaging.

DOI: 10.3390/diagnostics15070930
PMCID: PMC11989208
PMID: 40218280

Conflict of interest statement: The authors declare no conflicts of interest.


1020. Sensors (Basel). 2025 May 25;25(11):3324. doi: 10.3390/s25113324.

Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study 
on Fatigue Detection.

Ronando E(1)(2), Inoue S(1).

Author information:
(1)Graduate School of Life Science and Systems Engineering, Kyushu Institute of 
Technology, 2-4 Hibikino, Wakamatsu Ward, Kitakyushu 808-0135, Japan.
(2)Department of Informatics, Universitas 17 Agustus 1945 Surabaya, Semolowaru 
No. 45, Kota Surabaya 60118, Indonesia.

In this paper, we propose a novel few-shot optimization with Hybrid Euclidean 
Distance with Large Language Models (HED-LM) to improve example selection for 
sensor-based classification tasks. While few-shot prompting enables efficient 
inference with limited labeled data, its performance largely depends on the 
quality of selected examples. HED-LM addresses this challenge through a hybrid 
selection pipeline that filters candidate examples based on Euclidean distance 
and re-ranks them using contextual relevance scored by large language models 
(LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection 
task using accelerometer data characterized by overlapping patterns and high 
inter-subject variability. Unlike simpler tasks such as activity recognition, 
fatigue detection demands more nuanced example selection due to subtle 
differences in physiological signals. Our experiments show that HED-LM achieves 
a mean macro F1-score of 69.13 ± 10.71%, outperforming both random selection 
(59.30 ± 10.13%) and distance-only filtering (67.61 ± 11.39%). These represent 
relative improvements of 16.6% and 2.3%, respectively. The results confirm that 
combining numerical similarity with contextual relevance improves the robustness 
of few-shot prompting. Overall, HED-LM offers a practical solution to improve 
performance in real-world sensor-based learning tasks and shows potential for 
broader applications in healthcare monitoring, human activity recognition, and 
industrial safety scenarios.

DOI: 10.3390/s25113324
PMCID: PMC12157906
PMID: 40968822 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


1021. Genes (Basel). 2025 Mar 31;16(4):413. doi: 10.3390/genes16040413.

circ2LO: Identification of CircRNA Based on the LucaOne Large Model.

Yu H(1), Yu Y(2), Xia Y(3).

Author information:
(1)Computer Science and Technology College, Heilongjiang Institute of 
Technology, No. 999 Hongqi Street, Harbin 150009, China.
(2)College of Animal Science, Jilin University, No. 1977 Xinzhu Road, Changchun 
130012, China.
(3)College of Wildlife and Protected Area, Northeast Forestry University, No. 26 
Hexing Road, Harbin 150040, China.

Circular RNA is a type of noncoding RNA with a special covalent bond structure. 
As an endogenous RNA in animals and plants, it is formed through RNA splicing. 
The 5' and 3' ends of the exons form circular RNA at the back-splicing sites. 
Circular RNA plays an important regulatory role in diseases by interacting with 
the associated miRNAs. Accurate identification of circular RNA can enrich the 
data on circular RNA and provide new ideas for drug development. At present, 
mainstream circular RNA recognition algorithms are divided into two categories: 
those based on RNA sequence position information and those based on RNA sequence 
biometric information. Herein, we propose a method for the recognition of 
circular RNA, called circ2LO, which utilizes the LucaOne large model for feature 
embedding of the splicing sites of RNA sequences as well as their upstream and 
downstream sequences to prevent semantic information loss caused by the 
traditional one-hot encoding method. Subsequently, it employs a convolutional 
layer to extract features and a self-attention mechanism to extract interactive 
features to accurately capture the core features of the circular RNA at the 
splicing sites. Finally, it uses a fully connected layer to identify circular 
RNA. The accuracy of circ2LO on the human dataset reached 95.47%, which is 
higher than the values shown by existing methods. It also achieved accuracies of 
97.04% and 72.04% on the Arabidopsis and mouse datasets, respectively, 
demonstrating good robustness. Through rigorous validation, the circ2LO model 
has proven its high-precision identification capability for circular RNAs, 
marking it as a potentially transformative analytical platform in the circRNA 
research field.

DOI: 10.3390/genes16040413
PMCID: PMC12026638
PMID: 40282373 [Indexed for MEDLINE]

Conflict of interest statement: The author declares no conflicts of interest.