1. JMIR Ment Health. 2026 Jan 23;13:e76051. doi: 10.2196/76051.

Triaging Casual From Critical-Leveraging Machine Learning to Detect Self-Harm 
and Suicide Risks for Youth on Social Media: Algorithm Development and 
Validation Study.

Qadir S(1), Alsoubai A(2), Park JK(3), Ali NS(4), Choudhury M(5), Wisniewski 
P(6).

Author information:
(1)Department of Computer Science, Vanderbilt University, Nashville, TN, United 
States.
(2)Department of Information Systems, King Abdulaziz University, Jeddah, Saudi 
Arabia.
(3)School of Computing, Clemson University, Clemson, SC, United States.
(4)School of Information, University of Michigan, Ann Arbor, MI, United States.
(5)School of Interactive Computing, Georgia Institute of Technology, Atlanta, 
GA, United States.
(6)International Computer Science Institute, ICSI, Berkeley, CA, United States.

BACKGROUND: This study aims to detect self-harm or suicide (SH-S) ideation 
language used by youth (aged 13-21 y) in their private Instagram (Meta) 
conversations. While automated mental health tools have shown promise, there 
remains a gap in understanding how nuanced youth language around SH-S can be 
effectively identified.
OBJECTIVE: Our work aimed to develop interpretable models that go beyond binary 
classification to recognize the spectrum of SH-S expressions.
METHODS: We analyzed a dataset of Instagram private conversations donated by 
youth. A range of traditional machine learning models (support vector machine, 
random forest, Naive Bayes, and extreme gradient boosting) and transformer-based 
architectures (Bidirectional Encoder Representations from Transformers and 
Distilled Bidirectional Encoder Representations from Transformers) were trained 
and evaluated. In addition to raw text, we incorporated contextual, 
psycholinguistic (linguistic injury word count), sentiment (Valence Aware 
Dictionary and Sentiment Reasoner), and lexical (term frequency-inverse document 
frequency) features to improve detection accuracy. We further explored how 
increasing conversational context-from message-level to subconversation 
level-affected model performance.
RESULTS: Distilled Bidirectional Encoder Representations from Transformers 
demonstrated a good performance in identifying the presence of SH-S behaviors 
within individual messages, achieving an accuracy of 99%. However, when tasked 
with a more fine-grained classification-differentiating among "self" (personal 
accounts of SH-S), "other" (references to SH-S experiences involving others), 
and "hyperbole" (sarcastic, humorous, or exaggerated mentions not indicative of 
genuine risk)-the model's accuracy declined to 89%. Notably, by expanding the 
input window to include a broader conversational context, the model's 
performance on these granular categories improved to 91%, highlighting the 
importance of contextual understanding when distinguishing between subtle 
variations in SH-S discourse.
CONCLUSIONS: Our findings underscore the importance of designing SH-S automatic 
detection systems sensitive to the dynamic language of youth and social media. 
Contextual and sentiment-aware models improve detection and provide a nuanced 
understanding of SH-S risk expression. This research lays the foundation for 
developing inclusive and ethically grounded interventions, while also calling 
for future work to validate these models across platforms and populations.

©Sarvech Qadir, Ashwaq Alsoubai, Jinkyung Katie Park, Naima Samreen Ali, Munmun 
De Choudhury, Pamela Wisniewski. Originally published in JMIR Mental Health 
(https://mental.jmir.org), 23.01.2026.

DOI: 10.2196/76051
PMID: 41576367 [Indexed for MEDLINE]


2. NPJ Digit Med. 2026 Jan 22. doi: 10.1038/s41746-026-02372-4. Online ahead of 
print.

Early diagnosis of axial spondyloarthritis in primary care using multi-agent 
systems.

Ji X(#)(1), Li Z(#)(2), Zeng L(#)(1)(3), Hu L(1), Wang Y(3), Zhang K(4), Shi 
L(5), Wei M(6), Chen L(7), Guo L(8), Dong J(9), Wang A(9), Sun L(9), Song Y(9), 
Wang H(9), Wang J(9), Lei Y(9), Yue W(9), Zhao Z(1), Zhu J(1), Huang F(1), Zhang 
J(10), Li T(11)(12), Li K(13).

Author information:
(1)Department of Rheumatology and Immunology, the First Medical Center, Chinese 
PLA General Hospital, Beijing, China.
(2)Bioinformatics Division, Department of Automation, BNRIST and MOE Key Lab of 
Bioinformatics, Tsinghua University, Beijing, China.
(3)Department of Rheumatology and Immunology, Beijing Electric Power Hospital, 
Beijing, China.
(4)Department of Clinical Immunology, Xijing Hospital, Fourth Military Medical 
University, Xi'an, China.
(5)Department of Rheumatology and Immunology, Peking University Shougang 
Hospital, Beijing, China.
(6)Department of Rheumatology and Immunology, General Hospital of Western 
Theater Command, Chengdu, China.
(7)Department of Rheumatology and Immunology, General Hospital of Central 
Theater Command, Wuhan, China.
(8)Department of Rheumatology and Immunology, General Hospital of Northern 
Theater Command of Chinese PLA, Shenyang, Liaoning, China.
(9)Department of Medical Innovation Research, Chinese PLA General Hospital, 
Beijing, China.
(10)Bioinformatics Division, Department of Automation, BNRIST and MOE Key Lab of 
Bioinformatics, Tsinghua University, Beijing, China. 28820327@qq.com.
(11)Department of Medical Innovation Research, Chinese PLA General Hospital, 
Beijing, China. litao30lhospital@163.com.
(12)National Engineering Research Center for Medical Big Data Application 
Technology, Chinese PLA General Hospital, Beijing, China. 
litao30lhospital@163.com.
(13)Department of Rheumatology and Immunology, the First Medical Center, Chinese 
PLA General Hospital, Beijing, China. lkp_01@163.com.
(#)Contributed equally

Axial spondyloarthritis (axSpA) is an inflammatory disease marked by chronic low 
back pain, with a global average diagnostic delay of 6.7 years. Early diagnosis 
is crucial for improving prognosis and reducing disability rates, yet primary 
care physicians (PCPs) may find it challenging to ensure timely recognition and 
referrals. This study developed and validated Spondyloarthritis Agents 
(SpAgents), an early diagnostic system based on a multi-agent framework 
integrating large language models (LLMs) and imaging models. The SpAgents 
framework includes PlannerAgent, DataAgent, ToolAgent, and DoctorAgent, 
supported by long-term memory for dynamic knowledge updates. We enrolled 596 
patients, dividing 545 from one hospital into a training dataset (n = 359) and a 
validation dataset (n = 186), along with an independent cohort of 51 patients 
from five additional hospitals for testing. SpAgents demonstrated strong 
diagnostic performance, achieving sensitivity of 0.8615 and specificity of 
0.8000 during validation, and 0.9375 and 0.7368 during testing. SpAgents 
exhibited significantly higher sensitivity (0.9400) and accuracy (0.8600) than 
both PCPs and junior rheumatologists, with overall performance equivalent to 
that of senior rheumatologists. Under SpAgents-assisted diagnosis, both PCPs and 
junior rheumatologists showed marked improvements in sensitivity and accuracy. 
SpAgents effectively enhance early axSpA identification among PCPs, offering an 
innovative solution to reduce diagnostic delays.

© 2026. The Author(s).

DOI: 10.1038/s41746-026-02372-4
PMID: 41571772

Conflict of interest statement: Competing interests: The authors declare no 
competing interests. Declaration of Generative AI in Scientific Writing: We 
confirm that no generative AI tools (such as ChatGPT or other large language 
models) were used in any portion of the manuscript generation. All content in 
this manuscript was independently prepared by the authors


3. J Evid Based Med. 2026 Jan 22:e70114. doi: 10.1111/jebm.70114. Online ahead of 
print.

Managing Conflict of Interest in Clinical Practice Guidelines With Artificial 
Intelligence: Insights From Large Language Models and Beyond.

Wang Y(1)(2)(3)(4), Wang Q(5), Xun Y(6), Zhou Q(7)(8), Zhang H(1)(2)(3)(4), Liu 
H(9), Qin Y(2)(3)(4)(7), Wu M(1)(2)(3)(4), Wang Z(1)(2)(3)(4), Li H(1)(2)(3)(4), 
Estill J(7)(10), Chen Y(2)(3)(4)(7)(8)(9)(11).

Author information:
(1)School of Public Health, Lanzhou University, Lanzhou, Gansu, China.
(2)Research Unit of Evidence-Based Evaluation and Guidelines, Chinese Academy of 
Medical Sciences (2021RU017), School of Basic Medical Sciences, Lanzhou 
University, Lanzhou, China.
(3)Key Laboratory of Evidence-Based Medicine of Gansu Province, Lanzhou, China.
(4)Institute of Health Data Science, Lanzhou University, Lanzhou, China.
(5)Li Ka Shing Faculty of Medicine, School of Nursing, University of Hong Kong, 
Pokfulam, Hong Kong, China.
(6)School of Population Medicine and Public Health, Chinese Academy of Medical 
Sciences & Peking Union Medical College, Beijing, China.
(7)Evidence-Based Medicine Center, School of Basic Medical Sciences, Lanzhou 
University, Lanzhou, China.
(8)Lanzhou University, Affiliate of the Cochrane China Network, Lanzhou, China.
(9)Chevidence Lab of Child & Adolescent Health, Children's Hospital of Chongqing 
Medical University, Chongqing, China.
(10)Institute of Global Health, University of Geneva, Geneva, Switzerland.
(11)WHO Collaborating Centre for Guideline Implementation and Knowledge 
Translation, Lanzhou, China.

BACKGROUND: Conflict of interest (COI) management is critical for ensuring the 
scientific integrity and fairness of clinical practice guidelines (CPGs). Large 
language models (LLMs) have great potential in strengthening COI management, 
particularly in information collection, assessment, and supporting guideline 
development groups.
OBJECTIVE: To explore LLMs' role in COI management during CPG development, 
focusing on applications, challenges, and future directions.
METHODS: We examined how LLMs can support COI management by designing and 
testing a set of simulated COI scenarios based on established management 
principles.
RESULTS: LLMs can improve efficiency in data collection (e.g., in analyzing 
disclosures), objectivity in risk assessment, and transparency in reporting. 
However, privacy risks (e.g., data breaches) and technical issues (e.g., model 
bias) hinder the adoption of LLM based approaches. Setting up policy frameworks, 
research collaboration, and enhanced security, such as differential privacy 
levels, can enhance reliability.
CONCLUSION: LLMs can support COI management in CPG development if ethical issues 
are adequately considered, but validation in real-world settings is still 
needed.

© 2026 Chinese Cochrane Center, West China Hospital of Sichuan University and 
John Wiley & Sons Australia, Ltd.

DOI: 10.1111/jebm.70114
PMID: 41571612


4. Curr Opin Pediatr. 2026 Jan 20. doi: 10.1097/MOP.0000000000001540. Online ahead 
of print.

Using artificial intelligence to advance pediatric infection prevention and 
control.

Murray TS(1)(2), Quan M(3), Aniskiewicz M(2), Pak TR(4)(5)(6)(7).

Author information:
(1)Department of Pediatrics, Yale University School of Medicine.
(2)Department of Infection Prevention, Yale New Haven Hospital, New Haven, 
Connecticut.
(3)Department of Electrical Engineering and Computer Science, Henry Samueli 
School of Engineering, University of California, Irvine.
(4)Division of Infectious Diseases, University of California, Irvine School of 
Medicine.
(5)Epidemiology and Infection Prevention Program, UC Irvine Health, Irvine, 
California.
(6)Division of Infectious Diseases, Massachusetts General Hospital.
(7)Department of Population Medicine, Harvard Pilgrim Healthcare Institute and 
Harvard Medical School, Boston, Massachusetts, USA.

PURPOSE OF REVIEW: Recent literature describes the deployment of different 
artificial intelligence (AI) technologies to potentially support infection 
prevention and control (IP&C) in both the community and healthcare environment. 
However, most studies focus on adults. This review explores the data and 
potential for AI to enhance IP&C for pediatric populations as well as 
recognizing important limitations.
RECENT FINDINGS: In community settings, AI can educate families about infections 
and risk, recognize potential clusters and outbreaks of infectious pathogens, 
and prescreen individually infected patients prior to entering a healthcare 
facility. For admitted patients, AI has been used to identify patients at risk 
for healthcare-associated infections (HAIs) such as central line associated 
blood stream infections, and may assist infection preventionists in abstracting 
chart data for HAI surveillance. Limitations include potential biases in 
training data and the lack of prospective studies validating the use of AI for 
IP&C purposes, especially in heterogeneous pediatric populations.
SUMMARY: AI can be a valuable tool in recognizing and controlling infections in 
both the community and healthcare settings. However, more studies in pediatric 
populations are needed, including prospective studies that validate tools 
created and trained on retrospective cohorts.

Copyright © 2026 Wolters Kluwer Health, Inc. All rights reserved.

DOI: 10.1097/MOP.0000000000001540
PMID: 41568703


5. Sci Data. 2026 Jan 22. doi: 10.1038/s41597-026-06587-8. Online ahead of print.

A dataset of real-world oscillograms from electrical power grids.

Evdakov A(1)(2), Filatova G(2), Yablokov A(2), Kovalenko A(3)(4), Skachkov E(5), 
Makarov I(6)(7)(8).

Author information:
(1)LLC "APS", Moscow, Russia.
(2)ISPU, Moscow, Russia.
(3)AIRI, Moscow, Russia. kovalenko@airi.net.
(4)ISP RAS, Moscow, Russia. kovalenko@airi.net.
(5)Sbercity, Moscow, Russia.
(6)AIRI, Moscow, Russia.
(7)ISP RAS, Moscow, Russia.
(8)Research Center of the Artificial Intelligence Institute, Innopolis 
University, Innopolis, Russia.

This paper presents an extensive dataset of real-world oscillograms that capture 
voltage and current signals from electrical substations. The dataset aims to 
advance research on power system analysis, fault detection, and machine 
learning-driven relay protection. It includes approximately 50,000 oscillograms 
recorded with sampling rates up to 8 kHz. A manually annotated subset of 480 
oscillograms categorizes events into four groups: noise-dominated signals 
without deviations; routine equipment operations such as load switching, circuit 
breaker actions, motor startups, or transformer energization; non-critical 
deviations compliant with operational standards, including single-phase ground 
faults or minor voltage dips; critical faults such as short-circuits or voltage 
collapses requiring immediate relay protection activation. The unannotated part 
of the data supports self-supervised and unsupervised machine learning methods, 
enabling tasks like feature extraction, anomaly detection, and latent pattern 
identification in power networks. The dataset facilitates various applications, 
including the validation of synthetically trained models, the refinement of 
adaptive relay protection algorithms, and the development of fault detection and 
diagnosis systems.

© 2026. The Author(s).

DOI: 10.1038/s41597-026-06587-8
PMID: 41565705

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


6. Sci Rep. 2026 Jan 19;16(1):3222. doi: 10.1038/s41598-025-33168-3.

Construction of intelligent evaluation model for physical education classroom in 
primary and secondary schools based on posture estimation and motion 
recognition.

Ouyang Y(1), Shi Y(2).

Author information:
(1)Faculty of Education, Shaanxi Normal University, Xi'an, 710061, Shaanxi, 
China. iwhts9670111@outlook.com.
(2)College of Physical Education, Shaanxi Normal University, Xi'an, 710061, 
Shaanxi, China.

In the evolving landscape of intelligent education systems, there is an urgent 
need to develop adaptive, transparent, and robust classroom analytics solutions 
that align with the priorities of human-centered artificial intelligence and 
multimodal interaction, as emphasized by the scope of this special issue. 
Existing physical activity evaluation tools for educational settings often lack 
scalability, context sensitivity, and the capacity to extract meaningful 
temporal patterns from high-dimensional behavior streams. Traditional methods 
tend to oversimplify the complexities of pedagogical dynamics, resulting in 
feedback that is static, ambiguous, or divorced from instructional intent. These 
challenges are met through an integrated solution that combines semantic 
alignment with hierarchical attention-where spatial-temporal patterns are first 
captured through layered attention modeling, and then adaptively contextualized 
to align with instructional objectives. Our system, built upon the KINEVAL 
architecture and the Pedagogical Contextualization Strategy (PCS), fuses motion 
trajectory embeddings, instructional state encoding, and environment-aware 
modulation to generate structured, interpretable evaluations of student 
performance. The inclusion of attention-based dilated GRUs, transformer-based 
pedagogical modeling, and peer-aware regularization not only enhances robustness 
and interpretability but also enables cross-domain generalization across diverse 
school contexts. Experimental validation shows substantial improvements over 
conventional baselines in accuracy, fairness, and alignment with expert 
annotations. This study contributes a scalable and pedagogically informed 
approach to classroom behavior analysis, directly supporting the special issue's 
themes of intelligent sensing, adaptive learning, and multimodal system design.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-33168-3
PMID: 41554787 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


7. J Affect Disord. 2026 Jan 17;400:121207. doi: 10.1016/j.jad.2026.121207. Online 
ahead of print.

Harnessing multimodal emotion features in depression detection across gender: 
Integrating large language model, acoustic fusion and facial expression 
recognition.

Jin Y(1), Chen X(1), Liu J(1), Chen Z(1), Zhou J(2), Li Y(3), Bu Y(4), Wang 
Y(5).

Author information:
(1)Department of Statistics, Faculty of Arts and Sciences, Beijing Normal 
University, Beijing, China.
(2)Department of Psychiatry, National Clinical Research Center for Mental 
Disorders and National Center for Mental Disorders, The Second Xiangya Hospital 
of Central South University, Changsha, China.
(3)The People's Hospital of Pingbian County, Honghe, China.
(4)The People's Hospital of Pingbian County, Honghe, China. Electronic address: 
kywmc@126.com.
(5)Key Laboratory of Brain, Cognition and Education Sciences, Ministry of 
Education, China, School of Psychology, Center for Studies of Psychological 
Application and Guangdong Key Laboratory of Mental Health and Cognitive Science, 
South China Normal University, Guangzhou, China. Electronic address: 
angelayuanyuanwang@gmail.com.

BACKGROUND: Depression is a complex and multidimensional mood disorder that 
manifests across text, audio, and visual modalities. Multimodal features capture 
the emotional diversity and may differ by gender. Therefore, this study aims to 
evaluate tri-modal emotion fusion for depression detection and gender-specific 
patterns.
METHODS: This study consists of 189 individuals (male: 103 and female: 86) 
obtained from the DAIC dataset. The dataset focuses on each participant's 
semi-structured depression interviews, including verbatim transcripts, audio, 
and blurred-face videos. Text features were processed by an LLM, audio/video by 
CNN-BLSTM-Transformer/SVM, and fusion for depression detection by Gibbs and 
XGBoost. We used SHAP and Procrustes analyses to explore gender-specific 
tri-modal emotion combinations across semantic, acoustic, and visual features.
RESULTS: Tri-modal emotion combinations significantly outperformed single-modal 
and dual-modal baselines (all p < 0.001) with best metrics (MSE = 18.06, 
RMSE = 4.25). Men may be more likely to report symptoms like anger, fear, and 
surprise, as opposed to the more commonly recognized symptoms like sadness or 
loss of interest. SHAP analysis showed that the most important tri-modal emotion 
combination was "Surprise(Text)-Fear(Audio)-Angry(Visual)" in males 
(accuracy = 75%, precision = 66.7%) and 
"Sadness(Text)-Sadness(Audio)-Sadness(Visual)" in females (accuracy = 72.7%, 
precision = 75%). Depressed males and females differed significantly in 
semantic, acoustic, visual, and tri-modal emotion associations (p < 0.05).
CONCLUSIONS: Combining diverse modal emotional expressions is a more 
comprehensive and accurate approach for detecting depression. Men often 
externalize depression through high-arousal emotions, whereas females express 
low-arousal internalizing states. These gender differences are essential for 
developing specific multimodal detection systems for the depressed population.

Copyright © 2026. Published by Elsevier B.V.

DOI: 10.1016/j.jad.2026.121207
PMID: 41554487

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no competing interests.


8. JMIR Form Res. 2026 Jan 19;10:e82828. doi: 10.2196/82828.

Uptake of Large Language Models by London Medical Students: Exploratory 
Qualitative Interview Study.

Alazzawi M(1), Lam K(1).

Author information:
(1)Department of Surgery and Cancer, Imperial College London, London, United 
Kingdom.

BACKGROUND: The popularity of large language models (LLMs) has grown 
exponentially across health care. Despite the wealth of literature on proposed 
applications in medical education, there remains a critical gap regarding their 
real-world use, benefits, and challenges as experienced by medical students 
themselves.
OBJECTIVE: We aimed to explore qualitatively and characterize the perceived 
benefits, facilitators, and barriers associated with the use of LLMs among a 
cohort of London-based medical students.
METHODS: Semistructured interviews were conducted with 15 medical students from 
preclinical and clinical stages at London-based medical schools. Guided by the 
technology acceptance model, interview transcripts underwent an inductive 
thematic analysis to identify themes on actual system use, perceived usefulness, 
ease of use, and attitudes toward LLMs.
RESULTS: All participants reported frequent use of ChatGPT for concise topic 
summarization, clarification of complex concepts, generation of 
examination-style questions, and summarization of research. Students described 
LLMs as a complementary tool to traditional materials, valuing their immediacy 
("Instead of getting a textbook, I can ask ChatGPT to summarise something in X 
words and read it in under a minute") and ease of use. Peer demonstration and 
device-agnostic accessibility emerged as key facilitators. Of note, wider 
applications such as simulating clinical interviews were discovered through 
peers rather than through formal teaching. Significant barriers were reported. 
Hallucinations, fabricated references, and outdated information led to loss of 
trust, with more junior students finding inaccurate outputs difficult to detect 
("I stopped using it because I found it to be inaccurate, and I don't want to be 
learning the wrong things"). Half of the participants interviewed reported a 
sense of overreliance, defaulting to its use for answers with a perceived loss 
of critical thinking ability. Students noted inequalities in access to advanced 
features and voiced concerns about privacy when using LLMs in clinical 
scenarios.
CONCLUSIONS: LLMs have been widely adopted by medical students. While students 
perceived the efficiency, flexibility, and conversational interface of LLMs as 
beneficial, substantial reservations remain regarding their reliability, 
potential de-skilling, and the loss of academic integrity. These findings 
underpin the urgent need for curricula to both support safe LLM use and also 
adapt assessment and teaching strategies for artificial intelligence-augmented 
student practice. Future research should broaden geographical representation, 
investigate applications in low-resource settings, and integrate educators' 
perspectives to establish future curricular guidance in an artificial 
intelligence era.

©Maya Alazzawi, Kyle Lam. Originally published in JMIR Formative Research 
(https://formative.jmir.org), 19.01.2026.

DOI: 10.2196/82828
PMID: 41553752 [Indexed for MEDLINE]


9. BMJ Open Ophthalmol. 2026 Jan 16;11(1):e002385. doi: 
10.1136/bmjophth-2025-002385.

NLP-ROPCare: predicting retinopathy of prematurity with admission notes using 
natural language processing.

Zhang Y(#)(1)(2), Zhao S(#)(3), Ren J(4), Li Y(5), Zhao X(1), Sun J(1), Nie 
C(4), Xie S(6), Huang X(6), Wen J(7), Luo X(8), Zhang G(9).

Author information:
(1)Shenzhen Eye Hospital, Shenzhen Eye Medical Center, Southern Medical 
University, Shenzhen, Guangdong, China.
(2)Department of Ophthalmology, Huizhou First Hospital, Huizhou, Guangdong, 
China.
(3)College of Computing and Data Science, Nanyang Technological University, 
Singapore.
(4)National Key Clinical Specialty Construction Project/Department of 
Neonatology, Guangdong Women and Children Hospital, Guangzhou, Guangdong, China.
(5)College of Sciences, Northeastern University, Shenyang, Liaoning, China.
(6)Department of Ophthalmology, Guangdong Women and Children Hospital, 
Guangzhou, Guangdong, China.
(7)School of Mathematics, Jilin University, Changchun, Jilin, China 
13823509060@163.com luoxqgz@126.com jinming.wen@mail.mcgill.ca.
(8)National Key Clinical Specialty Construction Project/Department of 
Neonatology, Guangdong Women and Children Hospital, Guangzhou, Guangdong, China 
13823509060@163.com luoxqgz@126.com jinming.wen@mail.mcgill.ca.
(9)Shenzhen Eye Hospital, Shenzhen Eye Medical Center, Southern Medical 
University, Shenzhen, Guangdong, China 13823509060@163.com luoxqgz@126.com 
jinming.wen@mail.mcgill.ca.
(#)Contributed equally

OBJECTIVES: Retinopathy of prematurity (ROP) is a leading cause of blindness in 
children worldwide, requiring more efficient models to help predict 
treatment-requiring ROP. Our study aimed to develop a new prediction model for 
ROP occurrence and severity, named NLP-ROPCare, using natural language 
processing (NLP).
METHODS AND ANALYSIS: A retrospective observational study. Infants with a 
gestational age ≤32 weeks or birth weight ≤2000 g were collected in Guangdong 
Women and Children Hospital from 2013 to 2022, including 3922 preterm infants 
with 1106 patients with ROP. Four pretrained language models - BERT 
(Bidirectional Encoder Representations from Transformers), RoBERTa (Robustly 
Optimized BERT pretraining Approach), MC-BERT (language pre-training via a Meta 
Controller) and NEZHA (NEural contextualiZed representation for CHinese lAnguage 
understanding) - were used for development of NLP prediction models based on 
free-form texts in the admission notes. For comparison, two machine learning 
methods (Random Forest and Support Vector Machine) were used to construct 
prediction models based on 20 structured characteristics previously extracted 
from the admission notes. Performance evaluating metrics included accuracy, 
precision, recall, F1 score and area under the curve (AUC).
RESULTS: The NLP prediction models for ROP occurrence outperformed those for 
severity. The NEZHA model demonstrated the highest accuracy in predicting ROP 
occurrence, achieving an F1 score of 89.35% and an AUC of 0.90. Its performance 
was also better than two machine learning models whose highest F1 was 78% with 
an AUC equal to 0.87. In addition, the F1 score of RoBERTa (78.44%) was slightly 
higher than that of NEZHA (77.81%) for predicting ROP severity, and the AUC of 
RoBERTa also achieved the highest 0.91.
CONCLUSION: The NLP-ROPCare combines language models NEZHA and RoBERTa to enable 
early prediction of ROP occurrence and severity based on unstructured free-form 
texts in the admission notes of preterm infants, highlighting its value in early 
prevention of ROP. Further external validation should be carried out to better 
adjust the model.

© Author(s) (or their employer(s)) 2026. Re-use permitted under CC BY-NC. No 
commercial re-use. See rights and permissions. Published by BMJ Group.

DOI: 10.1136/bmjophth-2025-002385
PMCID: PMC12815143
PMID: 41545231 [Indexed for MEDLINE]


10. J Craniofac Surg. 2026 Jan 16. doi: 10.1097/SCS.0000000000012413. Online ahead 
of print.

AI-Based CT Image Recognition With Med-Gemini-3D in the Diagnosis of a Rare 
Craniofacial Condition: A Catlin Mark Skull.

Hajebian HH(1), Friel MT(2)(3).

Author information:
(1)Department of Surgery, Garnet Health Medical Center, Middletown, NY.
(2)Department of Pediatric Plastic Surgery, Gayle and Tom Benson Ochsner 
Children's Hospital.
(3)Department of Surgery, Division of Plastic Surgery, Tulane University School 
of Medicine, New Orleans, LA.

Artificial intelligence (AI) is increasingly applied in diagnostic imaging to 
enhance pattern recognition and support clinical decision-making. In 2024, 
Google introduced Med-Gemini-3D, a multimodal platform capable of interpreting 
3-dimensional computed tomography scans and generating radiologist-level 
reports. Although not yet approved for independent clinical use, such systems 
may assist in identifying rare conditions that are unfamiliar to clinicians. The 
authors describe a 22-month-old girl who presented with persistent bilateral 
parietal skull defects and global developmental delay. Computed tomography 
demonstrated symmetric ossification defects adjacent to the sagittal suture that 
were not initially recognized by the treating physician. The patient's mother 
used a smartphone application powered by Med-Gemini-3D to analyze a 3D-CT 
reconstruction image, which suggested a diagnosis of "Catlin mark skull," a 
historical term for Enlarged Parietal Foramina (EPF). This prompted genetic 
evaluation and identification of a CDC42BPB variant associated with 
Chilton-Okur-Chung neurodevelopmental syndrome-a finding not previously reported 
in association with EPF. Establishing the diagnosis facilitated earlier 
therapeutic interventions and informed long-term management. This case 
underscores the potential role of AI-assisted tools in recognizing rare 
craniofacial anomalies. While such technologies cannot replace clinical 
expertise and remain limited by variable accuracy, they may help expand 
differential diagnoses, expedite referrals, and improve outcomes through earlier 
intervention. Continued research is needed to validate their reliability and to 
define their optimal integration into clinical practice.

Copyright © 2026 by Mutaz B. Habal, MD.

DOI: 10.1097/SCS.0000000000012413
PMID: 41543485

Conflict of interest statement: The authors report no conflicts of interest.


11. J Affect Disord. 2026 Jan 13:121190. doi: 10.1016/j.jad.2026.121190. Online 
ahead of print.

Randomized evidence on AI-enabled personalized treatment planning for adults 
with depressive-anxiety Spectrum disorders: A systematic review and 
Meta-analysis.

Yıldız E(1), Yıldız E(2).

Author information:
(1)İnönü University, Faculty of Nursing, Malatya, Türkiye. Electronic address: 
erman.yildiz@inonu.edu.tr.
(2)Independent Researcher, Germany.

BACKGROUND: AI-enabled personalized treatment planning may improve outcomes by 
tailoring care, yet its clinical impact across modalities remains uncertain.
METHODS: We preregistered a protocol (PROSPERO CRD420251106013), searched five 
databases, and included randomized trials comparing AI-enabled personalized 
planning (adaptive chatbots, clinician-facing CDSS/stratified care, 
individualized treatment rules [ITRs]) vs controls. Primary outcome was 
continuous symptom severity (Hedges' g; negative favors AI) at the longest 
follow-up using random-effects with Knapp-Hartung adjustment. Dichotomous 
response/remission was pooled as log OR.
RESULTS: At prespecified longest follow-up, AI-enabled personalization reduced 
depressive symptoms (g = -0.35, 95% CI -0.63 to -0.08; k = 7; N = 757; 
I2 = 25.2%; τ2 = 0.0195; 95% PI -0.79 to 0.09) but not anxiety (g = -0.08, 95% 
CI -0.45 to 0.29; k = 7; N = 875; I2 = 46.2%; τ2 = 0.0548; 95% PI -0.76 to 
0.61). Categorical outcomes favored AI (log OR = 0.35, 95% CI 0.01 to 0.69; OR ≈ 
1.42; k = 4; N = 4398; I2 ≈ 0%). Effects tended to be larger vs passive 
comparators; evidence for clinician-facing CDSS and prospective ITR-guided 
allocation remains limited.
CONCLUSIONS: Patient-facing chatbots appear effective as adjuncts for depression 
but lack efficacy for anxiety, likely due to transdiagnostic limitations. 
Clinician-facing ITRs show promise for treatment matching but lack robust 
prospective validation. Notably, most trials pre-date Large Language Models 
(LLMs) (pre-2024); thus, conclusions regarding LLM-facilitated psychotherapy 
remain premature. Future trials should evaluate prospective model-guided 
allocation against active controls, include 6-12-month follow-up, and report 
variance statistics that enable synthesis.

Copyright © 2026. Published by Elsevier B.V.

DOI: 10.1016/j.jad.2026.121190
PMID: 41539612

Conflict of interest statement: Declaration of competing interest The authors 
declare no competing interests.


12. Clin Transplant. 2026 Jan;40(1):e70441. doi: 10.1111/ctr.70441.

Using Natural Language Processing to Characterize Early Steps in the Kidney 
Transplant Evaluation Process Documented in the National Veterans Affairs 
Electronic Health Record.

Eyre H(1), Prince DK(2), Abrahamson S(3), Blankenhorn RK(4), Carey EP(5), 
Laundry RJ(1), Showalter W(1), Todd-Stenberg J(1), O'Hare AM(1)(2)(3), Butler 
CR(1)(2)(3).

Author information:
(1)Seattle-Denver Health Systems Research Center of Innovation for 
Veteran-Centered and Value-Driven Care, Veterans Affairs Puget Sound Health Care 
System, Seattle, Washington, USA.
(2)Division of Nephrology, Department of Medicine and the Kidney Research 
Institute, University of Washington, Seattle, Washington, USA.
(3)Nephrology Section, Hospital and Specialty Medicine, Veterans Affairs Puget 
Sound Health Care System, Seattle, Washington, USA.
(4)Seattle-Denver Health Systems Research Center of Innovation for 
Veteran-Centered and Value-Driven Care, Veterans Affairs Eastern Colorado Health 
Care System, Denver, Colorado, USA.
(5)Department of Biostatistics & Informatics, Colorado School of Public Health, 
University of Colorado Anschutz Medical Campus, Aurora, Colorado, USA.

BACKGROUND: Efforts to identify barriers and improve access to kidney 
transplantation in the United States are limited by a lack of population-level 
data on early steps in the transplant evaluation process.
METHODS: We used a rule-based natural language processing (NLP) approach with 
clinical notes in the US Veterans Affairs Healthcare System (VA) electronic 
health record (EHR) and linkage with the United States Renal Data System 
registry to characterize sequential steps in the kidney transplant evaluation 
process. Adults with advanced kidney disease (estimated glomerular filtration 
rate ≤20 mL/min/1.73m2) from 1/1/2012-12/31/2019 who were receiving care within 
the VA were followed through 12/31/2021.
RESULTS: Among 45,174 cohort members, the median age was 71 (IQR 64, 80) years, 
and 97.2% were men. There was documentation of kidney transplant being mentioned 
as a treatment option for 46.3% of cohort members, 28.2% engaged in some type of 
evaluation for transplant, and 8.4% were referred to and 5.4% evaluated at a VA 
kidney transplant center. 6.9% of cohort members were added to the national 
deceased donor waitlist and 3.1% received a kidney transplant. Compared with 
events identified through EHR chart search and manual review by two clinicians, 
NLP identified events within 90 days with a precision of 0.82-0.94 and recall of 
0.56-0.89.
CONCLUSION: These results illuminate the substantial proportion of patients who 
engage in early steps in the kidney transplant evaluation process. The work also 
demonstrates that NLP can accurately identify these key steps in the process as 
documented in patients' EHRs.

© 2026 The Author(s). Clinical Transplantation published by Wiley Periodicals 
LLC.

DOI: 10.1111/ctr.70441
PMCID: PMC12802815
PMID: 41533291 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


13. Med Phys. 2026 Jan;53(1):e70230. doi: 10.1002/mp.70230.

A multi-stage deep learning network for prenatal diagnosis of coarctation of the 
aorta.

Wang W(1), Wang J(2), Zhang C(1), Li M(2), Luo Z(3), Xing W(1), Fan S(1), Qu 
X(1), Meng H(2).

Author information:
(1)School of Instrumentation and Optoelectronics Engineering, Beihang 
University, Beijing, P.R. China.
(2)Department of Echocardiography, Fuwai Hospital, National Center for 
Cardiovascular Diseases, Chinese Academy of Medical Sciences and Peking Union 
Medical College, Xicheng District, Beijing, P.R. China.
(3)Department of Echocardiography, Fuwai Yunnan Cardiovascular Hospital Kunming, 
Kunming, P.R. China.

BACKGROUND: Coarctation of the aorta (CoA) is a common congenital cardiovascular 
disorder. Severe cases may cause neonatal shock or heart failure. Accurate 
prenatal diagnosis is critical to reducing mortality and enabling timely medical 
intervention. Fetal echocardiography is widely used for prenatal CoA detection, 
but its diagnostic performance is limited by relatively high false-positive and 
false-negative rates. Recent advances have proposed deep learning to assist 
diagnosis. However, current approaches struggle to extract meaningful features 
from the complex distributions of fetal echocardiographic images. These 
limitations emphasize the urgent need for more advanced methodologies to improve 
diagnostic accuracy in this field.
PURPOSE: To develop a deep learning network that effectively integrates local 
and global features for extracting discriminative characteristics from fetal 
echocardiograms, thereby enabling precise prenatal diagnosis of fetal CoA.
METHODS: This study introduces CoA-Net, a multi-stage deep learning network for 
prenatal CoA diagnosis through local-global feature integration. CoA-Net 
includes four sequential stages, each equipped with a local feature extractor 
and a global feature extractor to ensure comprehensive feature extraction. The 
local extractor adopts lightweight convolutional layers and a local attention 
mechanism-this mechanism assigns input-adaptive importance weights, allowing 
accurate lesion localization and detailed local feature extraction. The global 
extractor incorporates a sparse global attention mechanism to capture the most 
discriminative global features, supporting reliable contextual modeling between 
the aortic arch and adjacent structures. To validate CoA-Net, a dataset of 488 
samples (including CoA cases and healthy controls) was constructed, with 
five-fold cross-validation for train-test partitioning. Extensive experiments 
were conducted on this dataset to evaluate model performance, using multiple 
state-of-the-art deep learning classification methods. Results were also 
compared with clinician diagnoses. Cross-validation t-tests were used to 
determine significant differences between CoA-Net and other approaches, and the 
Benjamini-Hochberg False Discovery Rate (BH-FDR) method was applied to correct 
for multiple comparisons when analyzing significant results.
RESULTS: CoA-Net outperforms other state-of-the-art networks, with statistically 
significant improvements in balanced accuracy, F1 score, and Matthews 
correlation coefficient (MCC) (p < 0.05 after BH-FDR correction) and a large 
effect size (Effect Size > 0.8). This indicates CoA-Net not only has statistical 
superiority but also delivers substantial, perceptible improvements in CoA 
diagnostic accuracy. Specifically, CoA-Net achieves 82.63% balanced accuracy, 
68.81% F1 score, 69.73% MCC, and 0.867 area under the curve (AUC). To further 
evaluate the clinical utility of CoA-Net, we compared its performance against 
diagnoses made by experienced clinicians. The clinicians achieved a balanced 
accuracy of 77.11%, an F1 score of 53.76%, an MCC of 44.54%, and an AUC of 
0.863. Notably, CoA-Net demonstrated comparable or even superior performance 
across these metrics, underscoring its potential as an effective 
decision-support tool in real-world clinical settings. Finally, ablation studies 
conclusively validated the indispensable role of the proposed local and global 
feature extractors, confirming that their synergistic contribution is critical 
to the model's overall high performance.
CONCLUSIONS: This study proposes CoA-Net for fetal CoA diagnosis. The model 
exhibits excellent classification performance and holds potential to support 
clinicians in achieving more precise and reliable fetal CoA diagnoses.

© 2026 American Association of Physicists in Medicine.

DOI: 10.1002/mp.70230
PMID: 41532285 [Indexed for MEDLINE]


14. NPJ Digit Med. 2026 Jan 12. doi: 10.1038/s41746-025-02308-4. Online ahead of 
print.

AI-guided personalized predictions on myopia progression and interventions.

Liu S(#)(1), Lu Y(#)(1)(2), Li X(#)(1), Chen X(#)(3)(4), Sun Z(#)(1), Li 
G(#)(1), Wang K(1)(2), Wu W(2), Xu H(1), Li H(1), Hu C(1), Zou Z(1)(5)(6), Zhang 
M(7), Zhang X(1), Lu W(1)(5), Yin Y(8), Qu J(9), Zhang K(10)(11)(12), Chen 
J(13).

Author information:
(1)State Key Laboratory of Eye Health, Institute for Advanced Study on Eye 
Health and Diseases, Institute for Clinical Data Science, Wenzhou Medical 
University, Wenzhou, China.
(2)Department of Big Data and Biomedical AI, College of Future Technology, 
Peking University, Beijing, China.
(3)State Key Laboratory of Eye Health, Institute for Advanced Study on Eye 
Health and Diseases, Institute for Clinical Data Science, Wenzhou Medical 
University, Wenzhou, China. 1525980867@qq.com.
(4)Senior Department of Ophthalmology, Chinese PLA General Hospital, Beijing, 
China. 1525980867@qq.com.
(5)Institute for AI in Medicine, Faculty of Medicine, Macau University of 
Science and Technology, Macau, China.
(6)Guangzhou National Laboratory, Guangzhou, China.
(7)Department of Optometry, Shenzhen Eye Hospital, Southern Medical University, 
Shenzhen, China.
(8)Faculty of Business and Wellness, City University of Macau, Macau, China. 
jiechen@eye.ac.cn.
(9)State Key Laboratory of Eye Health, Institute for Advanced Study on Eye 
Health and Diseases, Institute for Clinical Data Science, Wenzhou Medical 
University, Wenzhou, China. jia.qu@eye.ac.cn.
(10)State Key Laboratory of Eye Health, Institute for Advanced Study on Eye 
Health and Diseases, Institute for Clinical Data Science, Wenzhou Medical 
University, Wenzhou, China. kang.zhang@gmail.com.
(11)Institute for AI in Medicine, Faculty of Medicine, Macau University of 
Science and Technology, Macau, China. kang.zhang@gmail.com.
(12)Guangzhou National Laboratory, Guangzhou, China. kang.zhang@gmail.com.
(13)State Key Laboratory of Eye Health, Institute for Advanced Study on Eye 
Health and Diseases, Institute for Clinical Data Science, Wenzhou Medical 
University, Wenzhou, China. jennyyin629@gmail.com.
(#)Contributed equally

Myopia is a major global health concern. To enable precision myopia management, 
we developed a Transformer-based artificial intelligence (AI) model, the Myopia 
Progression Predictive Model (MPPM), comprising two modules: the Natural 
Progression Module (NPM) for predicting untreated myopia progression and the 
Intervention Progression Module (IPM) for forecasting progression under specific 
interventions. NPM was trained on 1,109,827 refractive records from 304,353 
children and adolescents, achieving high predictive accuracy for future 
spherical equivalent (SE) and axial length (AL) over a 10-year period. In the 
internal test set, SE prediction reached R² = 0.94, MAE = 0.35D; for AL, 
R² = 0.91, MAE = 0.16 mm. Comparable performance was observed in external 
validation. IPM was trained on four intervention cohorts (0.01% atropine, 
orthokeratology, peripheral defocus spectacles, and repeated low-level red light 
[RLRL] therapy) using a Transformer-based causal machine learning framework, 
enabling individualized estimation of treatment effects. It accurately predicted 
myopia changes under each intervention (SE: R² > 0.88, MAE < 0.45D; AL: 
R² > 0.80, MAE < 0.31 mm). Among the interventions, RLRL slightly reversed 
myopia progression, whereas the others slowed myopia progression. MPPM 
demonstrates strong promise as an AI-driven platform for personalized prediction 
and optimization of pediatric myopia management.

© 2026. The Author(s).

DOI: 10.1038/s41746-025-02308-4
PMID: 41526629

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


15. JMIR Form Res. 2026 Jan 12;10:e79676. doi: 10.2196/79676.

Evaluating Spanish Translations of Emergency Department Discharge Instructions 
by a Large Language Model: Tool Validation and Reliability Study.

Carreras Tartak JA(1), Brewster RC(2), Arango Isaza D(3), Berumen Martinez A(3), 
Grafals A(1), Adusumilli P(4), Fitzgerald T(4), Orcutt R(1), Nathanson LA(1), 
Haimovich AD(1).

Author information:
(1)Department of Emergency Medicine, Beth Israel Deaconess Medical Center, 
Boston, MA, United States.
(2)Department of Pediatrics, Beth Israel Deaconess Medical Center, Boston, MA, 
United States.
(3)Department of Medicine, Beth Israel Deaconess Medical Center, Boston, MA, 
United States.
(4)Department of Technology and Innovation, Beth Israel Lahey Health, Boston, 
MA, United States.

When given a sample of 100 emergency department discharge instructions, Claude 
Sonnet, a large language model, produced accurate Spanish translations as 
evaluated by Spanish-speaking physicians and medical interpreters.

©Jossie A Carreras Tartak, Ryan CL Brewster, Daniela Arango Isaza, Antonio 
Berumen Martinez, Ana Grafals, Phanidhar Adusumilli, Ted Fitzgerald, Roger 
Orcutt, Larry A Nathanson, Adrian D Haimovich. Originally published in JMIR 
Formative Research (https://formative.jmir.org), 12.01.2026.

DOI: 10.2196/79676
PMID: 41525688 [Indexed for MEDLINE]


16. J Am Med Inform Assoc. 2026 Jan 10:ocaf211. doi: 10.1093/jamia/ocaf211. Online 
ahead of print.

GARDE-Chat: a scalable, open-source platform for building and deploying health 
chatbots.

Del Fiol G(1), Borsato E(1), Bradshaw RL(1), Bian J(1), Woodbury A(1), Gauchel 
C(1), Eilbeck KL(1), Maxwell W(2), Ellis K(2), Madeo AC(3), Schlechter C(2)(3), 
Kukhareva PV(1), Allen CG(4), Kean M(5), Elkin EB(6), Sharaf R(7)(8), Ahsan 
MD(7), Frey M(7), Davis-Rivera L(7), Kohlmann WK(2)(9), Wetter DW(2)(3), 
Kaphingst KA(2)(10), Kawamoto K(1).

Author information:
(1)Department of Biomedical Informatics, University of Utah, Salt Lake City, UT 
84108, United States.
(2)Huntsman Cancer Institute, University of Utah, Salt Lake City, UT 84112, 
United States.
(3)Department of Population Health Sciences, University of Utah, Salt Lake City, 
UT 84108, United States.
(4)Wake Forest University, Winston-Salem, NC 27109, United States.
(5)Medical University of South Carolina, Charleston, SC 29425, United States.
(6)Department of Health Policy and Management, Columbia University Mailman 
School of Public Health, New York, NY 10032, United States.
(7)Genetics and Personalized Cancer Prevention Program, Weill Cornell Medicine, 
New York, NY 10021, United States.
(8)Department of Population Health Sciences, Weill Cornell Medicine, New York, 
NY 10065, United States.
(9)National TeleOncology Program, Clinical Cancer Genetics Service, Durham 
Veterans Affairs Medical Center, Durham, NC 27705, United States.
(10)Department of Communication, University of Utah, Salt Lake City, UT 84112, 
United States.

BACKGROUND: Chatbots are increasingly used to deliver health education, patient 
engagement, and access to healthcare services. GARDE-Chat is an open-source 
platform designed to facilitate the development, deployment, and dissemination 
of chatbot-based digital health interventions across different domains and 
settings.
MATERIALS AND METHODS: GARDE-Chat was developed through an iterative process 
informed by real-world use cases to guide prioritization of key features. The 
tool was developed as an open-source platform to promote collaboration, broad 
dissemination, and impact across research and clinical domains.
RESULTS: GARDE-Chat's main features include (1) a visual authoring interface 
that allows non-programmers to design chatbots; (2) support for scripted, large 
language model (LLM)-based and hybrid chatbots; (3) capacity to share chatbots 
with researchers and institutions; (4) integration with external applications 
and data sources such as electronic health records and REDCap; (5) delivery via 
web browsers or text messaging; and (6) detailed audit log supporting analyses 
of chatbot user interactions. Since its first release in July 2022, GARDE-Chat 
has supported the development of chatbot-based interventions tested in multiple 
studies, including large pragmatic clinical trials addressing topics such as 
genetic testing, COVID-19 testing, tobacco cessation, and cancer screening.
DISCUSSION: Ongoing challenges include the effort required for developing 
chatbot scripts, ensuring safe use of LLMs, and integrating with clinical 
systems.
CONCLUSION: GARDE-Chat is a generalizable platform for creating, implementing, 
and disseminating scalable chatbot-based population health interventions. It has 
been validated in several studies, and it is available to researchers and 
healthcare systems through an open-source mechanism.

© The Author(s) 2026. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf211
PMCID: PMC12798686
PMID: 41524720

Conflict of interest statement: COMPETING INTERESTS STATEMENT The authors have 
no relevant conflicts to declare.


17. NPJ Digit Med. 2026 Jan 10. doi: 10.1038/s41746-025-02288-5. Online ahead of 
print.

Structure-aware multi-task learning with domain generalization for robust 
vertebrae analysis in spinal CT.

Du J(#)(1)(2)(3), Ge H(#)(4), Zhang R(#)(5), Chen Z(6), Zhang Y(7)(8), Bai Y(9), 
Xu H(10), Ding F(2), Zhang Y(2), Ye J(11), Yang Y(12), Hu S(13), Huang J(14).

Author information:
(1)Cancer Center, Department of Neurosurgery, Zhejiang Provincial People's 
Hospital, Affiliated People's Hospital, Hangzhou Medical College, Hangzhou, 
Zhejiang, China.
(2)Department of Neurosurgery, Shandong Provincial Hospital Affiliated to 
Shandong First Medical University, Jinan, Shandong, China.
(3)College of Optical and Engineering, Zhejiang University, Hangzhou, Zhejiang, 
China.
(4)Department of Sports Medicine, Tongji Hospital, School of Medicine, Tongji 
University, Shanghai, China.
(5)Department of Orthopedic, Fuzhou University Affiliated Provincial Hospital, 
Fuzhou, Fujian, China.
(6)School of Software & Microelectronics, Peking University, Beijing, China.
(7)Department of Oral Surgery, Shanghai Ninth People's Hospital, Shanghai Jiao 
Tong University School of Medicine, Shanghai, China.
(8)Shanghai Key Laboratory of Orthopedic Implants, Shanghai Ninth People's 
Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai, China.
(9)School of Environment, Education and Development, The University of 
Manchester, Manchester, UK.
(10)School of Medicine, Tongji University, Shanghai, China.
(11)Department of Radiology, Suzhou Kowloon Hospital, Shanghai Jiaotong 
University School of Medicine, Suzhou, Jiangsu, China.
(12)Department of Neurosurgery, Shandong Provincial Hospital Affiliated to 
Shandong First Medical University, Jinan, Shandong, China. yangyihangyy@163.com.
(13)Cancer Center, Department of Neurosurgery, Zhejiang Provincial People's 
Hospital, Affiliated People's Hospital, Hangzhou Medical College, Hangzhou, 
Zhejiang, China. shaoshanhu421@163.com.
(14)Department of Sports Medicine, Tongji Hospital, School of Medicine, Tongji 
University, Shanghai, China. jingbiaohuang1@163.com.
(#)Contributed equally

Spinal image analysis plays a critical role in the diagnosis and treatment of 
musculoskeletal and neurological disorders. However, existing vertebrae 
segmentation methods suffer from limited generalizability across clinical 
domains and rarely address downstream tasks such as vertebrae identification and 
lesion localization. In this work, we introduce VertebraFormer, a unified 
multi-task framework designed for robust and generalizable spinal CT analysis. 
To support this framework, we curate MultiSpine, a heterogeneous benchmark 
comprising CT volumes from four public and private datasets, annotated with 
vertebra segmentation masks, anatomical labels, and pathology regions. Our 
method integrates a Transformer encoder with task-specific decoders and a 
dynamic modulation unit that adapts feature representations to different imaging 
domains. We evaluate VertebraFormer across three key tasks-vertebra 
segmentation, vertebra numbering, and lesion localization, under both in-domain 
and cross-domain settings. Extensive experiments demonstrate that VertebraFormer 
outperforms competitive baselines in both accuracy and robustness. We further 
conduct ablation, perturbation, and efficiency analyses to validate the 
framework.

© 2026. The Author(s).

DOI: 10.1038/s41746-025-02288-5
PMID: 41520072

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


18. J Dent. 2026 Jan 8;166:106344. doi: 10.1016/j.jdent.2026.106344. Online ahead of 
print.

Multi-Architecture deep learning for CBCT segmentation of dental hard tissues 
and pulp in mixed dentition.

Baraka M(1), Elbadry E(2), Abourida O(2), Albaradi A(3), Wagih Y(3), Gamal M(2), 
Torki M(2).

Author information:
(1)Pediatric Dentistry and Dental Public Health Department, Faculty of 
Dentistry, Alexandria University, Champollion St., El Azareta, Alexandria, 
Egypt. Electronic address: marwa.baraka@alexu.edu.eg.
(2)Computer and Systems Engineering Department, Faculty of Engineering, 
Alexandria University, Bab Sharqi WA Wabour Al Meyah, Al Attarin, Alexandria, 
Egypt.
(3)Endodontic Department, Faculty of Dentistry, Alexandria University, 
Champollion St., El Azareta, Alexandria, Egypt.

OBJECTIVE: To develop and evaluate deep learning-based 3D models (CNN, 
Transformer, and Mamba architectures) for automated segmentation of pulp, 
primary, and permanent dental structures in pediatric CBCT scans with mixed 
dentition.
METHODS: A total of 151 CBCT scans were analyzed, comprising 105 internal 
dataset scans(29,478 images) and 46 external scans. Expert-annotated internal 
data were used for model development with 78 for training/validation and 27 for 
testing. Fully supervised multi-task models were trained to segment pulp, 
primary and permanent hard tissue structures. Six architectures (ResEncM, 
U-Mamba Bot/Enc, WNet, UNETR, SegResNet) were evaluated using DSC, IoU, HD95, 
segmentation volume, and processing time.
RESULTS: Within the internal dataset, 3D_fullres ResEncM achieved the highest 
accuracy across structures and age groups. Permanent teeth were segmented most 
reliably (ages 6-9: DSC 0.9859 ± 0.0086, HD95 0.2035 ± 0.0925; ages 10-13: DSC 
0.9825 ± 0.0100, HD95 0.2825 ± 0.0525), while primary teeth were less accurate, 
particularly in older children (10-13 years: DSC 0.8855 ± 0.1557, HD95 4.4201 ± 
6.5116). U-Mamba Bot and U-Mamba Enc performed well, with U-Mamba Bot excelling 
in pulp segmentation and U-Mamba Enc in primary structures. WNet, UNETR, and 
SegResNet were less consistent, especially for primary teeth in older children. 
In external validation, ResEncM maintained the highest DSC (0.8464 ± 0.1226) and 
lowest HD95 (4.9078 ± 4.9058). All models showed no significant volume bias (p > 
0.05) and high volumetric correlations (ρ ≥ 0.997). UNETR was fastest (0.48 ± 
0.38 min), followed by U-Mamba Bot (0.60 ± 0.40 min).
CONCLUSION: nnU-Net ResEncM showed the strongest overall performance, while 
U-Mamba Bot and U-Mamba Enc excelled in pulp and primary-tooth segmentation. All 
three models performed well across age groups, with primary tooth segmentation 
more accurate in younger children. External testing confirmed adequate 
performance, though lower than on the internal dataset, supporting the 
feasibility of automated CBCT segmentation.
CLINICAL SIGNIFICANCE: Automated segmentation of pulp and hard dental structures 
can streamline CBCT assessment and enhance workflow efficiency in managing 
developing teeth and complex orthodontic or surgical cases. This study provides 
a framework for incorporating AI-assisted analysis into pediatric dental imaging 
by including both primary and developing permanent dentition, as well as 
anomalous cases.

Copyright © 2026 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.jdent.2026.106344
PMID: 41519426

Conflict of interest statement: Declaration of competing interest The authors 
declared no potential conflicts of interest with respect to the research, 
authorship, and/or publication of this article. This manuscript has not been 
published elsewhere in part or in entirety and is not under consideration by 
another journal. The study design was approved by the appropriate ethics review 
board. We have read and understood your journal’s policies, and we believe that 
neither the manuscript nor the study violates any of these. There are no 
conflicts of interest to declare.


19. Comput Med Imaging Graph. 2026 Jan 8;128:102704. doi: 
10.1016/j.compmedimag.2026.102704. Online ahead of print.

A multi-expert deep learning framework with LLM-guided arbitration for 
multimodal histopathology prediction.

Debsarkar SS(1), Prasath VBS(2).

Author information:
(1)Department of Computer Science, University of Cincinnati, OH 45221, USA.
(2)Department of Computer Science, University of Cincinnati, OH 45221, USA; 
Division of Biomedical Informatics, Cincinnati Children's Hospital Medical 
Center, OH 45229, USA; Department of Pediatrics, College of Medicine, University 
of Cincinnati, OH 45257, USA. Electronic address: prasatsa@uc.edu.

Recent advances in deep learning have significantly improved the accuracy of 
computational pathology; however conventional model ensembling strategies often 
lack adaptability and interpretability hindering the clinical adaptability. 
While multiple artificial intelligence (AI) expert models can provide 
complementary perspectives, simply aggregating their outputs is often 
insufficient for handling inter-model disagreement and delivering interpretable 
decisions. To address these challenges, we propose a novel multi-expert 
framework that integrates diverse vision-based predictors and a clinical 
feature-based model, with a large language model (LLM) acting as an intelligent 
arbitrator. By leveraging the contextual reasoning and explanation capabilities 
of LLMs, our architecture dynamically synthesizes insights from both imaging and 
clinical data, resolving model conflicts, and providing transparent, rational 
decisions. We validate our approach on two cancer histopathology datasets, 
namely the HMU-GC-HE-30K which is a gastric cancer dataset containing pathology 
images only, and the BCNB which is a breast cancer biopsy dataset that is 
multimodal - contains pathology imaging and clinical information. Our proposed 
multi-expert, LLM arbitrated framework (MELLMA) outperforms convolutional neural 
networks (CNNs), and transformers, which are currently the de facto and 
state-of-the-art classification ensemble models, with better overall results. We 
test different LLMs as arbitrators, namely LLaMA, GPT variants, and Mistral. 
Further, our proposed framework outperforms strong single-agent CNN/ViT 
baselines on the datasets, and ablations show that learned per-agent trust 
materially improves the arbitrator's decisions without altering prompts or data. 
These experimental results demonstrate that LLM-guided arbitration consistently 
provides more robust and explainable performance than individual models, 
conventional ensembling with majority vote, uniform average, and meta-learners. 
The results obtained highlight the promise of LLM-driven arbitration for 
building transparent and extensible AI systems in digital pathology.

Copyright © 2026 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.compmedimag.2026.102704
PMID: 41519030

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


20. Med Teach. 2026 Jan 10:1-10. doi: 10.1080/0142159X.2025.2604240. Online ahead of 
print.

AI assisted, mentor-guided narrative review writing task for medical students, a 
novel educational strategy to enhance research and academic writing.

Alrefaie Z(1)(2), Alhazimi A(3), Almarabheh A(4), Madkhali T(3), Elsamanoudy 
A(5)(6).

Author information:
(1)Medical Education Department, Faculty of Medicine, Dar Al Uloom University, 
Riyadh, Saudi Arabia.
(2)Medical Physiology Department, Faculty of Medicine, Cairo University, Cairo, 
Egypt.
(3)Faculty of Medicine, Dar Al Uloom University, Riyadh, Saudi Arabia.
(4)Family and Community Medicine Department, College of Medicine and Health 
Sciences, Arabian Gulf University, Manama, Kingdom of Bahrain.
(5)Clinical Biochemistry Department, Faculty of Medicine, King Abdulaziz 
University, Jeddah, Saudi Arabia.
(6)Medical Biochemistry and Molecular Biology Department, Faculty of Medicine, 
Mansoura University, Mansoura, Egypt.

INTRODUCTION: The integration of artificial intelligence (AI) tools into medical 
education presents new opportunities for enhancing students' research skills and 
scientific writing. However, concerns remain about the potential for cognitive 
disengagement and the ethical use of AI when lacking appropriate educational 
supervision. This study aimed to evaluate a novel educational strategy combining 
structured AI assistance with mentor guidance to support narrative review 
writing among third-year medical students.
METHODS: A structured framework was implemented during the endocrine module, 
involving AI-assisted objective formulation, mentor-guided objective refinement, 
literature search and summarization, review drafting followed by AI-assisted 
rephrasing. Students worked in groups, each supervised by a trained mentor. A 
validated questionnaire assessed student perceptions across four domains: 
framework and guidelines, AI-generated objectives, skills developed and mentor 
role, and overall satisfaction. Descriptive statistics were performed and 
chi-square tests evaluated associations between perceptions and AI tool usage 
(ChatGPT vs. DeepSeek).
RESULTS: Eighty-seven students completed the survey. Perceived improvement in 
research readiness was observed; confidence in literature searching rose from 
29.8% to 69%, while 75.8% reported increased familiarity with PubMed/Google 
Scholar. Most students (80.5%) expressed satisfaction with the AI mentor hybrid 
approach, and 82.8% agreed it prepared them for future research. There were no 
significant differences in perceived outcomes between AI tools used. Mentor 
involvement was deemed essential by 69% of students, and a minority believed AI 
alone could replicate the same outcomes. Common challenges included limited 
access to articles and peer collaboration difficulties, while key learning 
outcomes included improved summarization and ethical AI use.
DISCUSSION: This study supports the integration of AI tools within a structured, 
mentor-guided educational framework to enhance critical evaluation and 
scientific writing in medical education. Human oversight and mentorship drive 
skill development and minimize the risk of unmoderated AI use in academic 
settings.

DOI: 10.1080/0142159X.2025.2604240
PMID: 41518042


21. Int J Nurs Stud. 2025 Dec 19;176:105322. doi: 10.1016/j.ijnurstu.2025.105322. 
Online ahead of print.

The use of large language models in clinical documentation: A scoping review.

Woo BFY(1), Cato K(2), Cho H(3), You SB(3), Song J(3).

Author information:
(1)University of Pennsylvania School of Nursing, Philadelphia, United States of 
America; The Commonwealth Fund, New York, United States of America; Nursing 
Administration, Ng Teng Fong General Hospital, Singapore, Singapore. Electronic 
address: brigitte_woo@nuhs.edu.sg.
(2)University of Pennsylvania School of Nursing, Philadelphia, United States of 
America; Children's Hospital of Philadelphia, Philadelphia, United States of 
America.
(3)University of Pennsylvania School of Nursing, Philadelphia, United States of 
America.

BACKGROUND: Clinical documentation is essential for safe, high-quality care but 
has become increasingly complex, contributing to clinician burnout. Large 
language models offer potential to ease documentation by generating summaries, 
structuring data, and ensuring compliance. However, concerns remain regarding 
accuracy, bias, privacy, and regulatory risks.
OBJECTIVE: To map current literature on large language models applications in 
clinical documentation, evaluating their benefits, limitations, and ethical 
considerations.
INFORMATION SOURCES: Five electronic databases (i.e., PubMed, Scopus, CINAHL, 
Cochrane Library, and IEEE Xplore) covering peer-reviewed literature published 
in English between January 2009 and August 2025.
METHODS: This scoping review followed Arksey and OMalleys framework and was 
reported in accordance with PRISMA-ScR guidelines. Screening, data extraction, 
and quality appraisal were conducted independently by multiple reviewers using 
Joanna Briggs Institute tools. Findings were synthesized using descriptive and 
narrative approaches.
RESULTS: Forty-one studies met inclusion criteria, most originating from the 
United States. Large language models were primarily applied to clinical note 
generation, discharge summaries, and provider-patient encounter documentation. 
Key evaluation metrics included content accuracy, linguistic quality, and 
summarization performance. Large language models demonstrated potential to 
improve documentation efficiency and readability, with some studies reporting up 
to 40 % time savings. However, concerns about factual inaccuracies, 
hallucinations, and reduced performance in complex cases were common. Clinician 
perceptions were mixed. Some found notes generated by large language models 
helpful and well-structured, while others raised concerns about reliability, 
liability, and loss of clinical nuance. Ethical challenges included data 
privacy, security, and algorithmic bias, with varying levels of compliance 
across settings.
CONCLUSIONS: Large language models hold significant promise for enhancing 
clinical documentation by improving efficiency, standardization, and clarity. 
However, their safe and effective use requires rigorous attention to accuracy, 
ethical safeguards, and clinician trust. Integration must support, rather than 
supplant, clinical reasoning and patient-centered care. Co-design with 
clinicians, real-world evaluation, and artificial intelligence literacy are 
essential to ensure that these technologies augment, not erode, professional 
judgment and care quality.
REGISTRATION: Open Science Framework Registries (https://osf.io/m4h3q).

Copyright © 2025 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.ijnurstu.2025.105322
PMID: 41512731

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


22. PLoS One. 2026 Jan 8;21(1):e0339276. doi: 10.1371/journal.pone.0339276. 
eCollection 2026.

Hypoxemia prediction in pediatric patients under general anesthesia using 
machine learning: A retrospective observational study and external validation.

Baek S(1)(2), Park JB(3), Heo J(4), Kim K(5), Baek D(6), Oh C(1)(2), Lee HC(3), 
Lee D(4)(7), Hong B(1)(2)(8).

Author information:
(1)Department of Anesthesiology and Pain Medicine, Chungnam National University 
Hospital, Daejeon, Republic of Korea.
(2)Department of Anesthesiology and Pain Medicine, Chungnam National University 
College of Medicine, Daejeon, Republic of Korea.
(3)Department of Anesthesiology and Pain Medicine, Seoul National University 
Hospital, Seoul, Republic of Korea.
(4)Department of Radiology, Seoul National University College of Medicine, Seoul 
National University Hospital, Seoul, Republic of Korea.
(5)Department of Radiology, Massachusetts General Hospital and Harvard Medical 
School, Massachusetts, United States of America.
(6)Chungnam National University College of Medicine, Daejeon, Republic of Korea.
(7)Institute of Medical and Biological Engineering, Seoul National University 
Medical Research Center, Seoul, Republic of Korea.
(8)Big Data Center, Department of Medical Information, Chungnam National 
University Hospital, Daejeon, Korea.

BACKGROUND: Pediatric patients under general anesthesia are particularly 
vulnerable to hypoxemia, which can lead to rapid oxygen desaturation. This 
vulnerability necessitates heightened vigilance from anesthesiologists, making 
pediatric anesthesia management especially challenging. Continuous 
intraoperative monitoring of oxygenation is critical. However, traditional 
methods relying solely on SpO2 readings may be insufficient and prone to 
inaccuracies.
METHODS: This study aimed to develop and externally validate various machine 
learning models to predict hypoxemia in pediatric patients under general 
anesthesia. This retrospective observational study included 800 pediatric cases 
from Seoul National University Hospital and 134 pediatric cases from Chungnam 
National University Hospital. Patient data, including vital signs and ventilator 
parameters sampled every 2 seconds, were analyzed. Four machine learning models 
(XGBoost, LSTM, InceptionTime, and Transformer) were evaluated using area under 
the receiver operating characteristic curve (AUROC), area under the 
precision-recall curve (AUPRC), and F1-score.
RESULTS: XGBoost achieved the highest performance in internal validation (AUROC, 
0.85), whereas the Transformer model demonstrated the best performance in 
external validation (AUROC, 0.83). Reducing the observation window from 1 minute 
to 10 seconds lowered the AUPRC but preserved high AUROC.
CONCLUSIONS: The XGBoost and Transformer models demonstrated robust performance 
in predicting intraoperative hypoxemia in pediatric patients under general 
anesthesia across two hospitals. Adjustments for age-related variations did not 
enhance model performance. Future research should focus on developing machine 
learning models that can accurately distinguish true hypoxemia, leading to 
clinically significant improvements in patient outcomes.

Copyright: © 2026 Baek et al. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0339276
PMCID: PMC12782441
PMID: 41505469 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


23. Health Commun. 2026 Jan 8:1-13. doi: 10.1080/10410236.2026.2613120. Online ahead 
of print.

A Match Made in Language: Examining the Role of Linguistic Similarity in 
Adolescents' Preference for Persuasive Health Messages in Social Media Contexts.

Schlicht J(1), van Woudenberg T(2), Blok DJ(1), Simoski B(3), Buijzen M(1).

Author information:
(1)Erasmus School of Social and Behavioural Sciences, Erasmus University 
Rotterdam.
(2)Center for Language Studies, Radboud Universiteit.
(3)Department of Social Epidemiology, Erasmus Medical Center Rotterdam.

Engaging adolescents with health topics on social media is notoriously 
challenging. A common strategy is message personalization, yet most efforts 
focus on personalizing what is said rather than how it is expressed. This is a 
missed opportunity, as youth-relevant health communication requires attention to 
both style and content. Digital communication environments enable such 
adaptation by making users' linguistic patterns visible and available for 
personalization. Yet little is known about how adolescents respond to health 
messages written in a similar linguistic style, or how such similarity is best 
operationalized. Addressing these gaps, this preregistered study tests whether 
adolescents prefer social media health messages that are more similar to their 
linguistic style and identifies which linguistic categories are most effective 
in eliciting positive responses. Using WhatsApp conversations donated by 191 
Dutch adolescents (aged 13-15), we derived linguistic profiles and created 
Instagram-style health messages varying in linguistic similarity. Participants 
then evaluated 22 message pairs, each pair manipulating one linguistic category. 
Multilevel Bayesian analyses yielded inconclusive evidence for effects of 
linguistic similarity on message preference or personalization, and moderate 
evidence against effects on perceived effectiveness. Our findings point to 
potential trade-offs between experimental transparency and similarity strength, 
yielding important insights for refining future operationalizations. At the same 
time, adolescents consistently preferred positive and simple-worded messages, 
which gives general stylistic guidance for more engaging health communication on 
social media. Further, we discuss implications for AI-based style matching, 
including the use of algorithmic approaches and large language models.

DOI: 10.1080/10410236.2026.2613120
PMID: 41504305


24. Toxicol Res. 2025 Aug 16;42(1):35-46. doi: 10.1007/s43188-025-00312-0. 
eCollection 2026 Jan.

Integrated biomarker analysis and next-generation AI for precision diabetes 
prediction.

Sim H(#)(1), Ha HH(#)(2), Kim H(#)(2).

Author information:
(1)Department of Smart Techonology and Convergence, Sunchon National University, 
255 Jungangno, Sunchon, 57922 Republic of Korea.
(2)College of Pharmacy, Sunchon National University, 255 Jungangno, Suncheon, 
57922 Republic of Korea.
(#)Contributed equally

This study explored the integration of advanced deep learning with key 
pharmaceutical biomarkers to enhance early diabetes prediction. We developed a 
multimodal ensemble approach that leverages transformer architectures to capture 
complex dependencies in heterogeneous healthcare data and Diffusion Models to 
address class imbalances by generating synthetic samples. Our research utilized 
diverse data sources, including electronic health records, medical imaging, and 
wearable device time-series data, supplemented with synthetic samples to better 
represent minority populations such as patients with type 1 and gestational 
diabetes. Critical biomarkers, including C-peptide, insulin, and hemoglobin A1c, 
were incorporated to improve model interpretability. The methodology involved 
extensive evaluation using accuracy, area under the receiver operating 
characteristic (ROC) curve (AUC), precision, recall, and F1-score, with 
cross-validation to mitigate overfitting. We also implemented interpretability 
features to provide clinicians with insight into the significance of biomarkers. 
Results showed a 6.2% improvement in minority class recall when pharmaceutical 
biomarkers were combined with diffusion-based augmentation. The model 
demonstrated enhanced classification stability and provided clear insights into 
clinical decision-making, highlighting the influence of biomarkers on disease 
progression and treatment outcomes. Future work will focus on multicenter 
validation, integration of additional omics data, and specialized validation 
across diverse populations. These findings underscore the potential of AI-driven 
biomarker analysis for advancing early diagnosis and personalized diabetes 
management, with broader implications for chronic disease prediction.

© The Author(s) under exclusive licence to Korean Society of Toxicology 2025. 
Springer Nature or its licensor (e.g. a society or other partner) holds 
exclusive rights to this article under a publishing agreement with the author(s) 
or other rightsholder(s); author self-archiving of the accepted manuscript 
version of this article is solely governed by the terms of such publishing 
agreement and applicable law.

DOI: 10.1007/s43188-025-00312-0
PMCID: PMC12770190
PMID: 41503444

Conflict of interest statement: Conflict of interestsThe authors declare no 
competing interests.


25. J Pediatr Soc North Am. 2025 Nov 26;14:100294. doi: 
10.1016/j.jposna.2025.100294. eCollection 2026 Feb.

Not Ready for Prime Time: Limitations of a Retrieval-Augmented Generation Large 
Language Model in Assessing Risk of Bias in Observational Studies.

Beber SA(1), Groff KD(1), Mange TR(1), Bram JT(1), Fabricant PD(1).

Author information:
(1)Division of Pediatric Orthopaedic Surgery, Hospital for Special Surgery, New 
York, NY, USA.

BACKGROUND: Current research has focused on the use of large language models 
(LLMs) to augment systematic reviews. LLMs are limited by their vulnerability to 
"hallucinations"; retrieval-augmented generation (RAG) reduces these by limiting 
the model's source knowledge to user-provided material. The purpose of this 
study was to evaluate the accuracy and reliability of a RAG-LLM in quality 
assessment of observational studies in pediatric orthopaedic literature as 
compared to manual review.
METHODS: Previously published systematic reviews of observational studies in 
pediatric orthopaedics containing reported Newcastle-Ottawa Scale (NOS) scores 
from our group were included. After uploading observational study source files, 
NotebookLM (Google, Mountain View, CA) evaluated each of the included studies 
using the NOS scoring sheet. Agreement among scores across all NotebookLM trials 
was determined using a two-way random, average measures, absolute agreement 
intraclass correlation coefficient [ICC(2,k)]. Agreement among individual scores 
generated by each NotebookLM instance (LM1, LM2, LM3, and LM4) and ground truth 
(published manual review score) was calculated using a two-way random, single 
measures, absolute agreement intraclass correlation coefficient [ICC(2,1)].
RESULTS: Two systematic reviews comprising a total of 27 observational studies 
were included. ICC across all measurements (ICC(2,k)-Reviewer-LM1,2,3,4) was 
0.69 (95% CI: 0.46-0.84), indicating moderate agreement. ICC comparing 
individual NotebookLM scores to ground truth demonstrated poor agreement 
[ICC(2,1) LM1-Reviewer = 0.27 (95% CI: -0.064 to 0.57), LM2-Reviewer = 0.18 (95% 
CI: -0.12 to 0.48), LM3-Reviewer = 0.081 (95% CI: -0.24 to 0.41), and 
LM4-Reviewer = 0.23 (95% CI: -0.14 to 0.55)]. Percent agreement ranged from 
14.8% to 29.6%. Single measures ICCs comparing individual NotebookLM scores 
across multiple trials demonstrated moderate-to-poor agreement.
CONCLUSIONS: NotebookLM demonstrated low reliability and accuracy in performing 
quality assessment of observational studies. Caution should be taken when 
implementing LLMs to augment research efforts in pediatric orthopaedics.
KEY CONCEPTS: (1)NotebookLM (Google, Mountain View, CA) demonstrated low 
reliability and accuracy in performing quality assessment of observational 
studies.(2)Caution should be taken when implementing artificial intelligence 
tools such as large language models (LLMs) to augment research efforts, even 
retrieval-augmented generation (RAG)-LLM models that reduce 
hallucinations.(3)Until emerging artificial intelligence technologies are 
further validated it remains essential that researchers and clinicians continue 
to critically appraise new studies independently.
LEVEL OF EVIDENCE: IV.

© 2025 The Author(s).

DOI: 10.1016/j.jposna.2025.100294
PMCID: PMC12768933
PMID: 41503344

Conflict of interest statement: The authors declare that they have no known 
competing financial interests or personal relationships that could have appeared 
to influence the work reported in this paper.


26. Zhongguo Fei Ai Za Zhi. 2025 Oct 20;28(10):738-750. doi: 
10.3779/j.issn.1009-3419.2025.102.36.

[Application of Explainable Deep Learning in Differentiating Benign from 
Malignant  Pulmonary Space-occupying Lesions and Classifying Pathological 
Subtypes of Lung Cancer].

[Article in Chinese; Abstract available in Chinese from the publisher]

Li H(1)(2), Wang Y(2), Wang Y(2), He H(3), Li J(1)(2), Su Y(1)(2), Kong F(4), 
Liu X(5), Cheng L(6), Li Y(1)(2)(3)(7).

Author information:
(1)First Clinical Medical College, Henan University of Chinese Medicine, 
Zhengzhou 450000, China.
(2)National Regional Diagnosis and Treatment Center for Traditional Chinese 
Medicine (Pulmonary Diseases), The First Affiliated Hospital of Henan University 
of Chinese Medicine, Zhengzhou 450000, China.
(3)Experimental Center, The First Affiliated Hospital of Henan University of 
Chinese Medicine; Pharmacology of Chinese Materia Medica (Respiration) 
Laboratory; Henan Provincial Key Laboratory of Chinese Medicine for Respiratory 
Disease Prevention and Treatment, Zhengzhou 450000, China.
(4)Second Clinical Medical College, Henan University of Chinese Medicine, 
Zhengzhou 450000, China.
(5)Department of Pathology, The First Affiliated Hospital of Henan University of 
Chinese Medicine, Zhengzhou 450000, China.
(6)Department of Radiology, The First Affiliated Hospital of Henan University of 
Chinese Medicine, Zhengzhou 450000, China.
(7)Provincial and Ministerial Collaborative Innovation Center for the Prevention 
and Treatment of Respiratory Diseases with Chinese Medicine, Henan University of 
Chinese Medicine, Zhengzhou 450000, China.

BACKGROUND: The discrimination between benign and malignant pulmonary 
space-occupying lesions and the classification of pathological subtypes of lung 
cancer are critical for clinical decision-making. However, conventional methods 
often suffer from insufficient utilization of multi-source clinical data and 
poor interpretability of deep learning models. This study investigates the 
performance of interpretable deep learning algorithms in diagnosing benign 
versus malignant pulmonary space-occupying lesions and classifying pathological 
subtypes of lung cancer, using a hybrid architecture based on 
Tab-Transformer-designed for tabular data and Residual Multi-Layer Perceptron 
(ResMLP), referred to as TT-ResMLP.
METHODS: Data including radiological characteristics, medical history, and 
laboratory findings from 345 patients with pathologically confirmed pulmonary 
space-occupying lesions were collected. The dataset was randomly split into a 
development set and a test set at an 8:2 ratio. Stable features were selected 
using the Spearman correlation test and the Least Absolute Shrinkage and 
Selection Operator (LASSO). The Synthetic Minority Over-sampling Technique 
(SMOTE) was employed to balance the samples, and 10-fold cross-validation was 
used to enhance model generalizability. Models were constructed using the 
Tab-Transformer algorithm, the ResMLP algorithm, and the TT-ResMLP hybrid. Model 
performance was evaluated using receiver operating characteristic (ROC) curves, 
the area under the curve (AUC), accuracy, specificity, sensitivity, and 
micro-averaged ROC (micro-ROC). SHapley Additive exPlanations (SHAP) analysis 
was performed based on the optimal model.
RESULTS: In the benign vs malignant diagnosis task, all three models performed 
well. The Tab-Transformer model demonstrated the best performance on the test 
set, followed by TT-ResMLP and ResMLP. SHAP analysis of the top-performing 
Tab-Transformer model revealed that the feature importance ranking was: age, 
pleural indentation, thrombin time, mean density, and ground-glass opacity. 
Pleural indentation contributed substantially to malignant diagnosis, and its 
contribution was further enhanced with increasing age and decreasing thrombin 
time. In the lung cancer subtype classification task, all three models exhibited 
excellent performance, with the TT-ResMLP hybrid showing the best overall 
performance. SHAP analysis further revealed that the Lung Imaging Reporting and 
Data System (Lung-RADS) category held high importance across all three 
pathological subtypes. Male gender was positively associated with the prediction 
of squamous cell carcinoma. Neuron-specific enolase (NSE) played a significant 
role in predicting small cell carcinoma. For adenocarcinoma, the diagnostic 
probability was positively correlated with the Lung-RADS category, a 
relationship more pronounced at lower prothrombin time (PT) values. In contrast, 
a negative correlation was observed in the squamous cell carcinoma and small 
cell carcinoma subgroups, although gender and NSE levels could enhance their 
contributory risk prediction. Analysis of feature decision boundaries indicated 
that the Lung-RADS grade possessed high discriminative power for identifying 
adenocarcinoma, whereas NSE demonstrated stronger discriminative ability for 
identifying small cell carcinoma.
CONCLUSIONS: The TT-ResMLP hybrid architecture is effective for diagnosing the 
benign or malignant nature of pulmonary space-occupying lesions and classifying 
pathological subtypes of lung cancer. The model possesses good interpretability, 
aiding in the identification of key predictive features and unravelling their 
interactive mechanisms, thereby providing an effective tool for a deeper 
understanding of lung cancer biology and clinical decision support.

Publisher: 【中文题目：可解释性深度学习算法在肺占位性病变 良恶性诊断及肺癌病理亚型分类中的运用】 【中文摘要：背景与目的 
肺占位性病变的良恶性鉴别与肺癌病理亚型分类是临床决策的关键，但传统方法存在多源临床数据利用不足及深度学习模型可解释性差的问题。本研究基于针对表格化数据设计的Transformer（Tab-Transformer）与残差多层感知器（Residual 
Multi-Layer Perceptron, 
ResMLP）的混合架构（TT-ResMLP），探讨可解释性深度学习算法在肺占位性病变良恶性诊断及肺癌病理亚型分类中的性能。方法 
收集345例经病理证实的肺占位性病变患者的影像学特征、病史资料及实验室检查等数据，按8:2随机分为训练集和测试集。采用Spearman检验与最小绝对收缩和选择算子（Least 
Absolute Shrinkage and Selection Operator, LASSO）筛选稳定特征，使用合成少数类过采样技术（Synthetic 
Minority Over-sampling Technique, 
SMOTE）平衡样本，采用10折交叉验证提高模型泛化能力，选用Tab-Transformer算法、ResMLP算法、TT-ResMLP构建模型，通过受试者工作特征（receiver 
operating characteristic, ROC）曲线、曲线下面积（area under the curve, 
AUC）、准确率、特异性、敏感性和微平均ROC（micro-averaged ROC, 
micro-ROC）曲线评估模型性能，并基于最优模型进行SHAP（SHapley Additive exPlanations）特征分析。结果 
良恶性诊断模型中，3种模型均表现良好，其中Tab-Transformer在测试集表现最优，TT-ResMLP和ResMLP次之；SHAP分析显示，表现最优的Tab-Transformer模型特征重要性依次是年龄、胸膜凹陷征、凝血酶时间、平均密度、磨玻璃样改变等，其中胸膜凹陷征有较高的恶性诊断贡献，且随年龄增长、凝血酶时间缩短，其贡献度进一步增强。在肺癌亚型分类任务中，3种模型均表现出优异性能，其中TT-ResMLP综合表现最优。SHAP分析进一步揭示，肺部影像报告和数据系统评分（Lung 
Imaging Reporting and Data System, 
Lung-RADS）在3种病理亚型中均具较高重要性；男性与鳞癌预测呈正相关；神经元特异性烯醇化酶（neuron-specific enolase, 
NSE）在小细胞癌预测中起重要作用。在腺癌中，诊断概率与Lung-RADS分级呈正相关，且在低凝血酶原时间值时更显著；而在鳞癌与小细胞癌亚组中呈负相关，但性别和NSE水平可增强其风险预测的贡献。特征决策边界分析显示，Lung-RADS分级在腺癌识别中具有较高的区分能力，而NSE在小细胞癌识别中展现出更强的区分能力。结论 
TT-ResMLP混合架构能达到肺占位性病变的良恶性诊断及肺癌病理亚型分类的目的，模型具备良好的可解释性，有助于识别关键预测特征并揭示其交互机制，为深入理解肺癌生物学行为及临床辅助决策提供了有效工具。 】 
【中文关键词：肺肿瘤；肺占位性病变；机器学习；特征诠释；良恶性诊断；深度学习】.

DOI: 10.3779/j.issn.1009-3419.2025.102.36
PMCID: PMC12782943
PMID: 41500778 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: The authors declare that 
they have no competing interests.


27. Support Care Cancer. 2026 Jan 6;34(1):71. doi: 10.1007/s00520-025-10293-3.

Applications of machine learning and natural language processing to 
neurocognitive outcomes in posttreatment cancer survivors: a scoping review.

Alfaro S(1), Liu J(2), Naranjo Ortiz C(3), Alfaro A(4), Lustberg M(5).

Author information:
(1)Yale University, New Haven, CT, USA. salome.alfaro@yale.edu.
(2)Yale University, New Haven, CT, USA.
(3)Yale Cancer Center, Yale University, New Haven, CT, USA.
(4)Duke University, Durham, NC, USA.
(5)Medical Oncology, Yale Cancer Center, Yale Medicine, New Haven, CT, USA.

PURPOSE: This scoping review explores how machine learning (ML) and natural 
language processing (NLP) are used to detect, characterize, and predict 
neurocognitive symptoms in cancer survivors across age groups. The review had 
two goals: (1) to compare ML and NLP applications in understanding 
cancer-related cognitive impairment (CRCI) among age-stratified survivors and 
(2) to identify research gaps that could inform future survivorship care.
METHODS: Following PRISMA-ScR guidelines, a comprehensive literature search was 
conducted across PubMed, Scopus, Web of Science, IEEE Xplore, and Google Scholar 
from 2014 to 2025. Studies were included if they used ML or NLP to assess 
neurocognitive outcomes in posttreatment cancer survivors. Studies without 
defined ML/NLP methods, a survivorship focus, or peer review were excluded.
RESULTS: The final review included 27 studies with 3584 participants. Most 
studies used supervised ML models such as random forest and support vector 
machines. Key applications included predicting patient-reported outcomes and 
identifying biomarkers via neuroimaging. Most studies focused on adult 
survivors, with limited research in older adult (n = 4), AYA (n = 1), and 
pediatric (n = 3) populations specifically, despite their high risk for 
long-term CRCI.
CONCLUSION: ML and NLP show promise for CRCI detection. Future research should 
prioritize developing age-specific ML/NLP models for underrepresented 
populations, particularly older adults, AYA, and pediatric survivors, while 
establishing standardized validation frameworks. Additionally, interdisciplinary 
collaboration and integration into clinical workflows will be essential for 
effective implementation.

© 2026. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00520-025-10293-3
PMID: 41495338 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interest: The authors 
declare no competing interests.


28. Biomed Eng Online. 2026 Jan 4. doi: 10.1186/s12938-025-01502-5. Online ahead of 
print.

Application of deep learning technology in breast cancer: a systematic review of 
segmentation, detection, and classification approaches.

Gao S(1)(2), Liu J(3), Li L(2)(4)(5), Yang D(6), Miao Y(6), Zhang X(7), Han 
Q(8), Shi Y(1), Wu J(1), Zhang K(9)(10)(11)(12).

Author information:
(1)Information Center, Affiliated Hospital of Hebei University, Baoding, China.
(2)Basic Research Key Laboratory of General Surgery for Digital Medicine, 
Affiliated Hospital of Hebei University, Baoding, China.
(3)Medical Affairs Department, Affiliated Hospital of Hebei University, Baoding, 
China.
(4)Institute of Life Science and Green Development, Hebei University, Baoding, 
China.
(5)3D Image and 3D Printing Center, Affiliated Hospital of Hebei University, 
Baoding, China.
(6)Clinical Medical College of Hebei University, Affiliated Hospital of Hebei 
University, Baoding, China.
(7)Ultrasound Department, Affiliated Hospital of Hebei University, Baoding, 
China.
(8)Breast Surgery, Affiliated Hospital of Hebei University, Baoding, China.
(9)Basic Research Key Laboratory of General Surgery for Digital Medicine, 
Affiliated Hospital of Hebei University, Baoding, China. 93391@qq.com.
(10)Institute of Life Science and Green Development, Hebei University, Baoding, 
China. 93391@qq.com.
(11)3D Image and 3D Printing Center, Affiliated Hospital of Hebei University, 
Baoding, China. 93391@qq.com.
(12)Thoracic Surgery Department, Affiliated Hospital of Hebei University, 
Baoding, China. 93391@qq.com.

OBJECTIVE: To provide a critical and clinically oriented synthesis of recent 
deep learning developments for breast cancer imaging across major modalities, 
with emphasis on model architectures, dataset characteristics, methodological 
quality, and implications for clinical translation.
METHODS: Following PRISMA guidelines, we systematically searched PubMed, Scopus, 
Web of Science, ScienceDirect, and Google Scholar for studies published from 
2020 to 2024 on deep learning applied to breast imaging. Sixty-five studies 
using convolutional neural networks (CNNs), Transformers, or hybrid 
architectures were included. Datasets were comparatively profiled, and study 
quality and risk of bias were appraised using QUADAS-2.
RESULTS: CNN-based classifiers, particularly on mammography and pathology, 
commonly achieved median accuracies above 90% and AUCs around or above 0.95, 
while CNN detectors reported high sensitivities and mid-90% accuracies, 
supporting their potential role as second readers. CNN-derived U-Net variants 
dominated segmentation tasks, yielding high Dice and IoU values for tumour and 
fibroglandular-tissue delineation. Transformer and hybrid models showed 
advantages when global context, multi-view inputs or volumetric data were 
critical (e.g. dense breasts, DBT, DCE-MRI), where they improved lesion 
localisation and patient-level risk stratification. However, QUADAS-2 and 
dataset profiling revealed substantial limitations: most studies were 
retrospective, single-centre and class-imbalanced, with narrow demographic 
representation, heterogeneous reference standards and scarce external or 
prospective validation. These factors raise concerns about bias, overfitting, 
fairness and robustness in real-world deployment. Only a minority of studies 
systematically addressed interpretability, workflow integration or regulatory 
requirements.
CONCLUSIONS: Deep learning offers considerable promise to support early 
detection, risk stratification and workflow efficiency across breast imaging 
modalities, with CNNs and Transformers providing complementary strengths for 
local fine-detail versus global contextual modelling. Nevertheless, the current 
evidence base is constrained by heterogeneous designs, limited reporting of 
study quality and biased datasets, so reported performance should not be 
interpreted as definitive proof of clinical readiness. Future research should 
prioritise multi-centre, demographically diverse cohorts, transparent quality 
assessment, external and prospective validation, and evaluation of reader and 
workflow impact. Developing explainable, fairness-aware and privacy-preserving 
systems-such as those enabled by interpretable architectures and federated 
learning-will be essential for safe and equitable translation of deep learning 
tools into routine breast cancer care.

© 2025. The Author(s).

DOI: 10.1186/s12938-025-01502-5
PMID: 41486277

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This work is a systematic review of previously published studies. 
No new human participants, identifiable human data, human tissue, or animal 
subjects were recruited or collected by the authors; therefore, ethics approval 
and informed consent were not required. Consent for publication: This manuscript 
is a systematic review and does not contain any individual person’s data 
(including images, videos, or case details) requiring consent for publication. 
For all included primary studies, any necessary consents were obtained by the 
original investigators as reported in their publications. Competing interests: 
The authors declare no competing interests.


29. Eur J Med Res. 2026 Jan 4. doi: 10.1186/s40001-025-03740-8. Online ahead of 
print.

Applying artificial intelligence in neurodevelopmental disorders management and 
research.

Mohamed S(1), Ben-Jaafar A(2), Frimpong M(3), Roy S(4), Sanker V(5), 
Nkrumah-Boateng PA(6), Imran S(7), Mumeen AA(8), Mohamed S(9), Wireko AA(10).

Author information:
(1), London, UK.
(2)University College Dublin, School of Medicine, Belfield, Dublin 4, Ireland.
(3)Department of Neuroscience, Bryn Mawr College, Bryn Mawr, PA, USA.
(4)Hull York Medical School, University of York, York, UK.
(5)Department of Neurosurgery, Stanford University, Stanford, CA, USA.
(6)University of Ghana Medical School, Accra, Ghana.
(7)School of Medicine, Dentistry & Biomedical Sciences, Queen's University 
Belfast, Belfast, UK.
(8)Department of Biochemistry, Vassar College, New York, NY, USA.
(9)City St George's University of London, London, UK.
(10)Toufiks World Organization, Sumy, Ukraine. andyvans36@yahoo.com.

Artificial intelligence (AI) is increasingly being used in the diagnosis, 
treatment, and monitoring of neurodevelopmental disorders, enabling earlier 
detection, personalised interventions, and continuous support. Traditional 
machine-learning models such as logistic regression, random forests, and support 
vector machines remain valuable for their interpretability and their ability to 
integrate multimodal clinical data. Deep-learning (DL) approaches, including 
convolutional neural networks and transformer-based architectures, improve the 
analysis of neuroimaging and behavioural datasets and strengthen diagnostic and 
prognostic performance. Important challenges remain, including limited 
transparency in DL systems, ongoing concerns about data privacy and algorithmic 
bias, and a lack of large and diverse paediatric datasets that restricts 
generalisability. Interpretability tools such as SHAP and LIME offer partial 
solutions but still lack standardised evaluation. At the same time, AI-driven 
robotic platforms are enhancing therapeutic engagement and supporting skill 
acquisition in children with neurodevelopmental conditions. This review 
highlights that AI tools have strong potential to act as clinical adjuncts 
rather than replacements, providing earlier detection, personalised management, 
and scalable care models. Realising this potential will require rigorous 
validation, ethical safeguards, and thoughtful integration into human-led care 
pathways.

© 2025. The Author(s).

DOI: 10.1186/s40001-025-03740-8
PMID: 41484902

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: No original data from new patients were collected, consent to 
participate is not applicable. Consent for publication: Not applicable. 
Competing interests: The authors declare no competing interests.


30. Semin Pediatr Surg. 2025 Dec 24:151577. doi: 10.1016/j.sempedsurg.2025.151577. 
Online ahead of print.

Artificial intelligence for prediction and detection of pediatric surgical site 
infection.

Bain AP(1), Upperman JS(2).

Author information:
(1)Department of Surgery, UT Southwestern Medical Center, Dallas, TX, USA; 
Clinical Informatics Center, University of Texas Southwestern Medical Center, 
Dallas, TX, USA. Electronic address: Andrew.bain@utsouthwestern.edu.
(2)Department of Pediatric Surgery, Vanderbilt University Medical Center, 
Nashville, TN, USA.

Surgical site infections (SSIs) remain a significant source of morbidity in 
pediatric surgery, prolonging hospital stays, increasing readmissions, and 
driving up healthcare costs. Manual chart reviews and static risk models limit 
traditional SSI prediction and detection. The rise of artificial intelligence 
(AI), including machine learning (ML), natural language processing (NLP), and 
large language models (LLMs), offers a transformative opportunity to enhance 
prediction and surveillance. This review synthesizes current literature on AI 
applications in pediatric SSI, emphasizing predictive models built on NSQIP-P 
data and detection strategies leveraging EHRs and wearable technologies. Despite 
encouraging retrospective results, real-world adoption remains constrained by 
poor validation, limited generalizability, and workflow misalignment. Ethical 
and regulatory concerns, including bias, transparency, and pediatric-specific 
data limitations, must be addressed to ensure safe, equitable implementation. 
Thoughtfully developed and deployed, AI-driven tools can transform pediatric 
surgical care by enabling earlier intervention and improving outcomes.

Copyright © 2025. Published by Elsevier Inc.

DOI: 10.1016/j.sempedsurg.2025.151577
PMID: 41483999

Conflict of interest statement: Declaration of competing interest The authors 
have no declarations of interest to disclose.


31. Ultrasonics. 2025 Dec 27;161:107936. doi: 10.1016/j.ultras.2025.107936. Online 
ahead of print.

Cardiovascular pulsing-based ultrasound strain imaging with deep learning using 
paired proximal and distal strain sequences for liver fibrosis detection: a 
feasibility study.

Chan HJ(1), Lu CH(1), Wang CY(1), Hsieh BY(2), Yeh CK(3), Tai DI(4), Tsui PH(5).

Author information:
(1)Department of Medical Imaging and Radiological Sciences, College of Medicine, 
Chang Gung University, Taoyuan, Taiwan.
(2)Department of Medical Imaging and Radiological Sciences, College of Medicine, 
Chang Gung University, Taoyuan, Taiwan; Department of Medical Imaging and 
Intervention, Chang Gung Memorial Hospital at Linkou, Taoyuan, Taiwan.
(3)Department of Biomedical Engineering and Environmental Sciences, National 
Tsing Hua University, Hsinchu, Taiwan; Department of Biomedical Sciences and 
Engineering, Tzu Chi University, Hualien, Taiwan; Department of Biomedical 
Engineering, Chung Yuan Christian University, Taoyuan, Taiwan.
(4)Department of Gastroenterology and Hepatology, Chang Gung Memorial Hospital 
at Linkou, Taoyuan, Taiwan. Electronic address: tai48978@cgmh.org.tw.
(5)Department of Medical Imaging and Radiological Sciences, College of Medicine, 
Chang Gung University, Taoyuan, Taiwan; Division of Pediatric Gastroenterology, 
Department of Pediatrics, Chang Gung Memorial Hospital at Linkou, Taoyuan, 
Taiwan. Electronic address: tsuiph@mail.cgu.edu.tw.

Accurate assessment of liver fibrosis in the left liver lobe remains clinically 
challenging due to motion artifacts that compromise the reliability of shear 
wave elastography. This feasibility study introduces cardiovascular 
pulsing-based ultrasound strain imaging (CPUSI) integrated with deep learning, 
employing dual strain sequence strategies to assess its potential for detecting 
liver fibrosis in the left hepatic lobe by leveraging intrinsic cardiac motion. 
A total of 104 patients was enrolled for ultrasound image acquisition, which 
included B-mode imaging, acoustic radiation force impulse imaging (ARFI), and 
FibroScan measurements. The dataset was also used for CPUSI generation, 
extraction of proximal (cardiac-wall) and distal (intrahepatic) strain 
sequences, and calculation of strain metrics, including time-averaged strain 
(TAS) and the distal-to-proximal strain ratio (DPSR). Five deep learning models, 
namely recurrent neural network (RNN), long short-term memory (LSTM), gated 
recurrent unit (GRU), transformer, and temporal convolutional network (TCN), 
were trained using paired proximal and distal strain sequences to classify liver 
fibrosis stages, with histopathology serving as the reference standard. 
Diagnostic performance was evaluated using independent t-tests and area under 
the receiver operating characteristic curve (AUROC). CPUSI-derived TAS and DPSR 
significantly differentiated early-stage (F0-F1) from advanced-stage (F2-F4) 
fibrosis (p < 0.05). ARFI, FibroScan, CPUSI-derived strain metrics (TAS and 
DPSR), and the CPUSI-based deep learning framework using dual strain sequences 
achieved AUROC values of 0.83, 0.82, 0.72-0.73, and 0.95, respectively, with the 
highest performance observed for the LSTM model. The proposed CPUSI-deep 
learning framework offers a feasible noninvasive approach for left-lobe fibrosis 
assessment and may serve as a complementary tool to right-lobe-based 
elastography. Further studies with larger cohorts are warranted.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ultras.2025.107936
PMID: 41483589

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


32. PLoS One. 2026 Jan 2;21(1):e0340277. doi: 10.1371/journal.pone.0340277. 
eCollection 2026.

The use of large language models in generating multiple choice questions for 
health professions education: A systematic review and network meta-analysis.

Riehm L(1)(2), Nanji K(3)(4), Lakhani M(5), Pankiv E(1)(2), Hasanee D(6), 
Pfeifer W(2).

Author information:
(1)Department of Anesthesia and Pain Medicine, The Hospital for Sick Children, 
Toronto, Ontario, Canada.
(2)Department of Anesthesiology and Pain Medicine, University of Toronto, 
Toronto, Ontario, Canada.
(3)Division of Ophthalmology, Department of Surgery, McMaster University, 
Hamilton, Ontario, Canada.
(4)Department of Health Research Methods, Evidence and Impact, McMaster 
University, Hamilton, Ontario, Canada.
(5)Faculty of Medicine, University of Ottawa, Ottawa, Ontario, Canada.
(6)Department of Surgery, McMaster University, Hamilton, Ontario, Canada.

PURPOSE: Large language models (LLMs) have the potential to change medical 
education. Whether LLMs can generate multiple-choice questions (MCQs) that are 
of similar quality to those created by humans is unclear. This investigation 
assessed the quality of MCQs generated by LLMs compared to humans.
METHODS: This review was registered with PROSPERO (CRD42025608775). A systematic 
review and frequentist random-effects network meta-analysis (NMA) or pairwise 
meta-analysis was performed. Ovid MEDLINE, Ovid EMBASE, and Scopus were searched 
from inception to November 1, 2024. The quality of MCQs was assessed with seven 
pre-defined outcomes: question relevance, clarity, accuracy/correctness; 
distractor quality; item difficulty analysis; and item discrimination analysis 
(point biserial correlation and item discrimination index). Continuous data were 
transformed to a 10-point scale to facilitate statistical analysis and reported 
as mean differences (MD). The MERSQI and the Grade of Recommendations, 
Assessment, Development and Evaluation (GRADE) NMA guidelines were used to 
assess risk of bias and certainty of evidence assessments.
RESULTS: Five LLMs were included. NMA demonstrated that ChatGPT 4 generated 
similar quality MCQs to humans with regards to question relevance (MD -0.13; 95% 
CI: -0.44,0.18; GRADE: VERY LOW), question clarity (MD -0.03; 95% CI: 
-0.15,0.10; GRADE: VERY LOW), and distractor quality (MD -0.10; 95% CI: 
-0.24,0.04; GRADE: VERY LOW); however, MCQs generated by Llama 2 performed worse 
than humans with regards to question clarity (MD -1.21; 95% CI: -1.60,-0.82; 
GRADE: VERY LOW) and distractor quality (MD -1.50; 95% CI: -2.03,-0.97; GRADE: 
VERY LOW). Exploratory post-hoc t-tests demonstrated that ChatGPT 3.5 performed 
worse than Llama 2 and ChatGPT 4 with regards to question clarity and distractor 
quality (p < 0.001).
CONCLUSION: ChatGPT 4 may create similar quality MCQs to humans, whereas ChatGPT 
3.5 and Llama 2 may be of worse quality. Further studies that directly compare 
these LLMs to human-generated questions and administer MCQs to students are 
required.

Copyright: © 2026 Riehm et al. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0340277
PMCID: PMC12758716
PMID: 41481658 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


33. Nurse Educ Pract. 2025 Dec 29;91:104697. doi: 10.1016/j.nepr.2025.104697. Online 
ahead of print.

Generative artificial intelligence for teaching and assessment in health 
professions education: A scoping review.

Astbury H(1), Fortune E(1), Ponte CD(2), Lyons K(3), Shaw L(4).

Author information:
(1)Faculty of Medicine, Dentistry and Health Sciences, University of Melbourne, 
Parkville, Victoria 3052, Australia.
(2)Centre for Digital Transformation of Health, Faculty of Medicine Dentistry 
and Health Sciences, University of Melbourne, Parkville, Victoria 3052, 
Australia.
(3)Centre for Digital Transformation of Health, Faculty of Medicine Dentistry 
and Health Sciences, University of Melbourne, Parkville, Victoria 3052, 
Australia; Melbourne School of Global and Population Health, Faculty of Medicine 
Dentistry and Health Sciences, University of Melbourne, Parkville, Victoria 
3052, Australia.
(4)Centre for Digital Transformation of Health, Faculty of Medicine Dentistry 
and Health Sciences, University of Melbourne, Parkville, Victoria 3052, 
Australia; Murdoch Children's Research Institute, Royal Children's Hospital, 
Parkville, Victoria 3052, Australia. Electronic address: 
Louise.shaw@unimelb.edu.au.

AIM: To review the literature on generative artificial intelligence for teaching 
and assessment in health professions education BACKGROUND: Advancements in 
generative artificial intelligence (GenAI), such as ChatGPT, offer new 
possibilities for health professions education. These technologies offer 
potential benefits in teaching and assessment, including personalised learning 
and automated resource generation. Despite its potential, concerns about 
accuracy, ethics and reliability remain. This scoping review examines GenAI's 
implementation, benefits and challenges in teaching and assessment across health 
professions education.
DESIGN: Scoping review.
METHODS: Following Arksey and O'Malley's five-stage framework, with refinements 
based on the Joanna Briggs Institute (JBI) methodology, Medline, CINAHL and Web 
of Science Core Collection were searched for peer-reviewed studies published 
between January 2019 and June 2024. Studies were screened independently by two 
reviewers and data extraction performed systematically to ensure consistency.
RESULTS: Studies (n = 5826) were assessed for eligibility, with 23 meeting the 
inclusion criteria. All included studies were published in 2023 and 2024. The 
primary applications of GenAI were in learning resource development and 
assessment, with reported benefits such as time savings, personalised learning 
and reduced resource use. Challenges included accuracy concerns, inconsistent 
outputs, technical limitations, algorithmic bias and risks to academic 
integrity.
CONCLUSIONS: This scoping review provides an overview of how GenAI is being 
integrated into health professions education. While the technology offers 
opportunities to enhance teaching and assessment, its implementation requires 
consideration of reliability, ethical concerns and educator preparedness. This 
review is the first to examine GenAI implementation across multiple 
AHPRA-regulated health professions and proposes a practical framework (AI HPE 
checklist) to guide responsible use.

Copyright © 2025. Published by Elsevier Ltd.

DOI: 10.1016/j.nepr.2025.104697
PMID: 41477967

Conflict of interest statement: Declaration of Competing Interest The authors 
declare no conflicts of interest related to this research. No financial or 
personal relationships exist that could inappropriately influence or bias the 
content of this manuscript. The research was conducted independently, and no 
funding bodies had any role in study design, data collection, analysis, 
interpretation, manuscript preparation, or the decision to submit for 
publication.


34. Assessment. 2025 Dec 31:10731911251406405. doi: 10.1177/10731911251406405. 
Online ahead of print.

Detecting Suicidal Ideation in Adolescence Using Self-Reported Emotional and 
Behavioral Patterns: Comparing Machine Learning and Large Language Model 
Predictions.

Marengo D(1), Longobardi C(1).

Author information:
(1)Department of Psychology, University of Turin, Italy.

Suicidal ideation in adolescents is a critical public health issue requiring 
early detection. This study examined whether machine learning (ML) and large 
language models (LLMs) can detect ideation in 1,197 students (ages 10-15) using 
self-reported Strengths and Difficulties Questionnaire (SDQ) data. Clinically 
relevant ideation was defined using Suicidal Ideation Questionnaire-Junior 
(SIQ-JR) cut-offs. Gemini 1.5 Pro and GPT-4o were prompted to estimate SIQ-JR 
scores from SDQ responses and demographics; Logistic Regression, Naive Bayes, 
and Random Forest models were trained on either SDQ data or LLM predictions. LLM 
predictions correlated with SIQ-JR (ρ = .61) and showed good discrimination 
across thresholds (area under the curve (AUC) ≥ .83), with item-level 
associations paralleling self-reports, revealing strong associations with 
emotional symptoms and peer problems. In cross-validated analyses, the best 
SDQ-based ML model reached sensitivity = .85 and specificity = .72; the best 
LLM-based model achieved .80 and .74. Notably, ML models trained directly on SDQ 
responses consistently outperformed those incorporating LLM predictions across 
all SIQ-JR thresholds. Nonetheless, LLMs demonstrated promising accuracy in 
identifying suicidal ideation based on SDQ and demographic data. Further 
refinement and validation are required before these approaches can be considered 
viable for clinical implementation.

DOI: 10.1177/10731911251406405
PMID: 41472619


35. Sensors (Basel). 2025 Dec 11;25(24):7541. doi: 10.3390/s25247541.

A Dual-Modal Adaptive Pyramid Transformer Algorithm for UAV Cross-Modal Object 
Detection.

Li Q(1), Yang M(1)(2)(3), Zhang X(1), Wang N(4), Tu X(1)(3), Liu X(1), Zhu X(1).

Author information:
(1)College of Aviation Electronics and Electrical, Civil Aviation Flight 
University of China, Guanghan 618307, China.
(2)School of Optoelectronic Science and Engineering, University of Electronic 
Science and Technology of China, Chengdu 610054, China.
(3)Sichuan Province Engineering Technology Research Center of General Aircraft 
Maintenance, Civil Aviation Flight University of China, Guanghan 618307, China.
(4)Luoyang College, Civil Aviation Flight University of China, Luoyang 471000, 
China.

Unmanned Aerial Vehicles (UAVs) play vital roles in traffic surveillance, 
disaster management, and border security, highlighting the importance of 
reliable infrared-visible image detection under complex illumination conditions. 
However, UAV-based infrared-visible detection still faces challenges in 
multi-scale target recognition, robustness to lighting variations, and efficient 
cross-modal information utilization. To address these issues, this study 
proposes a lightweight Dual-modality Adaptive Pyramid Transformer (DAP) module 
integrated into the YOLOv8 framework. The DAP module employs a hierarchical 
self-attention mechanism and a residual fusion structure to achieve adaptive 
multi-scale representation and cross-modal semantic alignment while preserving 
modality-specific features. This design enables effective feature fusion with 
reduced computational cost, enhancing detection accuracy in complex 
environments. Experiments on the DroneVehicle and LLVIP datasets demonstrate 
that the proposed DAP-based YOLOv8 achieves mAP50:95 scores of 61.2% and 62.1%, 
respectively, outperforming conventional methods. The results validate the 
capability of the DAP module to optimize cross-modal feature interaction and 
improve UAV real-time infrared-visible target detection, offering a practical 
and efficient solution for UAV applications such as traffic monitoring and 
disaster response.

DOI: 10.3390/s25247541
PMCID: PMC12736609
PMID: 41471535

Conflict of interest statement: The authors declare no conflicts of interest.


36. BMC Med Ethics. 2025 Dec 30;26(1):183. doi: 10.1186/s12910-025-01323-0.

Artificial intelligence guidance in ethically challenging clinical scenarios in 
child and adolescent psychiatry: a qualitative study in the context of Turkiye.

İçen S(1), Köken AH(2).

Author information:
(1)Department of Child and Adolescent Psychiatry, Kırşehir Ahi Evran University 
Faculty of Medicine, Kervansaray Mahallesi 2019/1, Kırşehir, Türkiye. 
sarper.icen@gmail.com.
(2)Department of History of Medicine and Ethics, Kırşehir Ahi Evran University 
Faculty of Medicine, Kırşehir, Türkiye.

BACKGROUND: Ethical decision-making in child and adolescent psychiatry (CAP) is 
inherently complex, shaped by developmental vulnerability, evolving autonomy, 
and competing responsibilities to patients, families, and the legal system. 
Clinicians often face moral dilemmas when navigating adolescent confidentiality, 
parental authority, and mandatory reporting duties, especially in high-stakes or 
culturally sensitive contexts. As large language models (LLMs) enter clinical 
settings, their potential to support ethical reasoning remains underexplored, 
particularly outside Western paradigms. This study qualitatively investigates 
how different LLMs provide ethical, legal, and emotional guidance to clinicians 
facing ethically challenging scenarios in CAP, situated within Turkiye's 
sociocultural and legal landscape.
METHOD: A scenario-based qualitative design was employed. Three expert-developed 
case vignettes reflecting ethically charged dilemmas, such as adolescent 
autonomy, parental conflict, and confidentiality, were submitted to the three 
LLMs (ChatGPT 4.0, Gemini 2.5 Flash, and GROK 3). Responses were analyzed using 
content and thematic analysis to identify key patterns of ethical-legal 
reasoning, alongside discourse analysis to examine tone, empathy, and cultural 
sensitivity. Two researchers, with backgrounds in CAP and medical ethics, 
conducted independent coding and reached consensus through a reflexive, 
interdisciplinary approach.
RESULTS: All LLMs addressed core ethical principles (autonomy, non-maleficence, 
beneficence, and justice) and referenced Turkish legal frameworks such as the 
Child Protection Law, Patient Rights Regulation, and mandatory reporting 
obligations, situating their guidance within the national regulatory context. 
They also differed in their engagement with sociocultural sensitivities: GROK 3 
emphasized therapeutic communication and relational trust, Gemini 2.5 Flash 
applied a highly structured, rule-based style focused on procedural compliance, 
while ChatGPT 4.0 provided concise and practical suggestions. Despite thematic 
overlaps, these varying approaches shaped how effectively the models aligned 
with Turkiye's clinical realities. Notably, LLMs frequently acted as "thinking 
companions," offering ethical and legal justifications while leaving 
interpretive responsibility with clinicians.
CONCLUSION: LLMs in CAP hold promise not only as cognitive aids but also as 
emotionally attuned, context-sensitive companions in ethical decision-making 
processes. Their effectiveness depends not just on algorithmic precision but 
also on explainability, empathy, and cultural alignment. Rather than replacing 
clinician judgment, LLMs may serve to ease emotional burden, enhance therapeutic 
reflection, and foster ethically sound care in complex, high-pressure 
situations.

© 2025. The Author(s).

DOI: 10.1186/s12910-025-01323-0
PMCID: PMC12754976
PMID: 41469990 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


37. Fetal Pediatr Pathol. 2025 Dec 30:1-19. doi: 10.1080/15513815.2025.2607706. 
Online ahead of print.

From ChatGPT-4 to ChatGPT-5: Evolving Applications in Pediatric Neoplastic 
Pathology and Education.

He M(1), Weng J(1).

Author information:
(1)Department of Pathology & Immunology, Washington University in St. Louis 
School of Medicine, St. Louis, MO, USA.

INTRODUCTION: Large language models (LLMs) such as ChatGPT are emerging tools in 
pathology. This study evaluated their diagnostic utility in pediatric neoplastic 
pathology.
MATERIALS AND METHODS: Thirty-three pediatric tumor cases were retrospectively 
analyzed. Clinical data and histology images were input to ChatGPT-4o 
(03-06/2025) and GPT-5 (10/2025) for diagnostic suggestions, immunohistochemical 
(IHC) panels, and report generation. GPT outputs were graded for diagnostic 
concordance (2 = concordant, 1 = partial, 0 = discordant).
RESULTS: ChatGPT-4o achieved at least partial concordance in 27/35 (77.1%) cases 
(toal and mean score 42, 1.2), with highest accuracy in small round blue cell 
tumors (SRBCT, 100%) and IHC interpretation accuracy 74.4%. GPT-5 showed 19/33 
(57.6%) concordance (total and mean 25, 0.76), highest in SRBCT (87.5%) and IHC 
accuracy 60.7%. (p < 0.05 between 4o and 5, Mann-Whitney U).
CONCLUSIONS: ChatGPT demonstrates promise as a diagnostic and educational 
adjunct in pediatric pathology, though expert oversight and further validation 
remain essential.

DOI: 10.1080/15513815.2025.2607706
PMID: 41467543


38. Future Oncol. 2025 Dec 30:1-8. doi: 10.1080/14796694.2025.2610463. Online ahead 
of print.

Stage II colon cancer: does ChatGPT recommend more intensive adjuvant therapy? A 
comparison with MDT decisions.

Birsin Z(1), Jeral S(1), Cebeci S(1), Çerme E(1), Aliyev V(1), Günaltılı M(1), 
Abbasov H(1), Çiçek E(1), Demirci NS(1), Alan Ö(1).

Author information:
(1)Faculty of Medicine, Department of Medical Oncology, Istanbul 
University-Cerrahpaşa, Istanbul, Turkey.

BACKGROUND: Adjuvant chemotherapy decision-making in stage II colon cancer 
remains challenging. Although multidisciplinary tumor boards (MDTs) guide 
treatment, their recommendations vary. Artificial intelligence (AI) tools such 
as ChatGPT may support decision-making, but direct comparative evidence with 
MDTs is limited.
METHODS: We retrospectively analyzed 179 patients with stage II colon cancer who 
underwent surgery between 2019-2024. MDT recommendations (observation, 
fluoropyrimidine monotherapy, or oxaliplatin-based chemotherapy) were compared 
with ChatGPT-5 outputs. Clinical factors - including age, ECOG performance 
status (PS), tumor stage, minor risk factors, and mismatch repair (MMR) status - 
were incorporated. Agreement was evaluated using Cohen's kappa (κ) and McNemar's 
test.
RESULTS: Across the three treatment categories, agreement between MDT and AI was 
moderate (70.4%, κ = 0.542, p < 0.001), while in the binary comparison of 
adjuvant therapy versus observation, concordance improved to substantial (91.1%, 
κ = 0.719, p < 0.001). Discordance mainly reflected AI's tendency to escalate 
therapy. Agreement decreased in patients ≥70 years, those with ECOG PS 2, and 
those with multiple risk factors.
CONCLUSIONS: AI showed moderate agreement with MDTs in detailed three-category 
recommendations but substantial concordance in binary adjuvant decisions. While 
AI may serve as a supportive tool, clinical judgment remains essential, 
particularly for elderly and frail patients.

Plain Language Summary: Stage II colon cancer is an early stage of the disease, 
but deciding whether patients should receive chemotherapy after surgery can 
still be difficult. Doctors usually discuss each case in multidisciplinary tumor 
board (MDT) meetings. However, treatment recommendations may differ among 
centers or experts. Artificial intelligence (AI) tools such as ChatGPT can 
analyze medical information and provide treatment suggestions based on current 
guidelines. However, AI systems are not perfect. They may sometimes misinterpret 
data or produce incorrect information (“hallucinations”), which could pose 
safety concerns if used without careful medical supervision. The alignment 
between AI decisions and real MDT recommendations remains unclear. In this 
study, we compared treatment recommendations made by our hospital’s MDT with 
those generated by ChatGPT-5 for 179 patients with stage II colon cancer. For 
each patient, AI received the same clinical information reviewed by the MDT and 
suggested one of three options: no treatment, chemotherapy with a single drug, 
or chemotherapy including oxaliplatin. We found that AI and the MDT agreed on 
most decisions, especially when the question was simply whether chemotherapy 
should be given or not. However, ChatGPT more often recommended stronger 
chemotherapy. Differences were more common in older patients, those with poorer 
overall health, and patients with several risk factors in their pathology 
results. These findings suggest that AI may help support decision-making in 
colon cancer by providing fast and guideline-based recommendations. However, AI 
does not replace the clinical judgment of healthcare professionals, especially 
for patients who are older, frail, or medically complex.

DOI: 10.1080/14796694.2025.2610463
PMID: 41467248


39. BMC Med Educ. 2025 Dec 29;25(1):1734. doi: 10.1186/s12909-025-08315-z.

Performance comparison of large language models on pediatric dentistry questions 
in the Turkish dentistry specialization examination.

Başkan HK(1), Başkan B(2).

Author information:
(1)Department of Pediatric Dentistry, Faculty of Dentistry, Kahramanmaras Sutcu 
Imam University, Onikisubat, Kahramanmaras, Türkiye. kubrabaskan@ksu.edu.tr.
(2)Department of Endodontics, Faculty of Dentistry, Kahramanmaras Sutcu Imam 
University, Onikisubat, Kahramanmaras, Türkiye.

BACKGROUND/PURPOSE: This study aimed to compare the performance of seven leading 
large language models (Gemini 2.5 Pro, Grok-4, GPT-5, Claude-4, Copilot, 
Perplexity, and GPT-4o) on pediatric dentistry questions from the Turkish 
Dentistry Specialization Examination (DUS), and to identify differences in their 
performance on information-based versus case-based question types.
MATERIALS AND METHODS: Seven large language models (Gemini 2.5 Pro, Grok-4, 
GPT-5, Claude-4, Copilot, Perplexity, and GPT-4o) were evaluated on 127 
multiple-choice questions from the DUS pediatric dentistry question bank 
(2012-2021), classified by experts as information-based (n = 96) and case-based 
(n = 31). Questions were input in Turkish without modification, and responses 
were assessed against official answer keys.
RESULTS: Significant differences were observed in overall accuracy rates 
(p < 0.001). The highest overall accuracy was recorded for Gemini 2.5 Pro 
(94.5%; 120/127), while the lowest performance was seen with GPT-4o (63.0%; 
80/127). For information-based questions, Gemini answered 92/96 correctly 
(95.8%) and GPT-4o 66/96 (68.7%); for case-based questions, Gemini answered 
28/31 correctly (90.3%) and Perplexity 5/31 (16.1%). Pairwise Wilcoxon 
comparisons statistically supported Gemini's significant superiority over many 
models and the notably weak performance of GPT-4o and Perplexity on case-based 
questions (p < 0.001).
CONCLUSIONS: LLMs can serve as effective "co‑pilots" for information retrieval 
and exam preparation in dental education but are currently unreliable for 
diagnostic and treatment decision‑making. Clinicians and students should use LLM 
outputs for review and learning while retaining final decisions based on 
professional experience, ethical responsibility, and patient‑centered judgment. 
Future research should evaluate and enhance LLMs' multimodal and visual‑data 
processing capabilities to improve clinical applicability.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-08315-z
PMCID: PMC12751958
PMID: 41466246 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Because this study does not involve research on 
human subjects, it does not require compliance with the Declaration of Helsinki. 
Consent for publication: Not applicable. Competing interests: The authors 
declare no competing interests.


40. Diagnostics (Basel). 2025 Dec 16;15(24):3219. doi: 10.3390/diagnostics15243219.

Reliability of Large Language Model-Based Artificial Intelligence in AIS 
Assessment: Lenke Classification and Fusion-Level Suggestion.

Aktan C(1), Koşar A(1), Ünal M(1), Korkmaz M(2), Kaya Ö(3), Akgül T(2), Güler 
F(1).

Author information:
(1)Department of Orthopedics and Traumatology, Antalya Training and Research 
Hospital, Antalya 07100, Turkey.
(2)Department of Orthopedics and Traumatology, Faculty of Medicine, Istanbul 
University, Istanbul 34093, Turkey.
(3)Department of Orthopedics and Traumatology, İstinye University Medical Park 
TEM Hospital, Istanbul 34250, Turkey.

Background: Accurate deformity classification and fusion-level planning are 
essential in adolescent idiopathic scoliosis (AIS) surgery and are traditionally 
guided by Cobb angle measurement and the Lenke system. Multimodal large language 
models (LLMs) (e.g., ChatGPT-4.0; Claude 3.7 Sonnet, Gemini 2.5 Pro, 
DeepSeek-R1-0528 Chat) are increasingly used for image interpretation despite 
limited validation for radiographic decision-making. This study evaluated the 
agreement and reproducibility of contemporary multimodal LLMs for AIS assessment 
compared with expert spine surgeons. Methods: This single-center retrospective 
study included 125 AIS patients (94 females, 31 males; mean age 14.8 ± 1.9 
years) who underwent posterior instrumentation (2020-2024). Two experienced 
spine surgeons independently performed Lenke classification (including lumbar 
and sagittal modifiers) and selected fusion levels (UIV-LIV) on standing AP, 
lateral, and side-bending radiographs; discrepancies were resolved by consensus 
to establish the reference standard. The same radiographs were analyzed by four 
paid multimodal LLMs using standardized zero-shot prompts. Because LLMs showed 
inconsistent end-vertebra selection, LLM-derived Cobb angles lacked a common 
anatomical reference frame and were excluded from quantitative analysis. 
Agreement with expert consensus and test-retest reproducibility (repeat analyses 
one week apart) were assessed using Cohen's κ. Evaluation times were recorded. 
Results: Surgeon agreement was high for Lenke classification (92.0%, κ = 0.913) 
and fusion-level selection (88.8%, κ = 0.879). All LLMs demonstrated 
chance-level test-retest reproducibility and very low agreement with expert 
consensus (Lenke: 1.6-10.2%, κ = 0.001-0.036; fusion: 0.8-12.0%, κ = 
0.003-0.053). Claude produced missing outputs in 17 Lenke and 29 fusion-level 
cases. Although LLMs completed assessments far faster than surgeons (seconds vs. 
~11-12 min), speed did not translate into clinically acceptable reliability. 
Conclusions: Current general-purpose multimodal LLMs do not provide reliable 
Lenke classification or fusion-level planning in AIS. Their poor agreement with 
expert surgeons and marked internal inconsistency indicate that LLM-generated 
interpretations should not be used for surgical decision-making or patient 
self-assessment without task-specific validation.

DOI: 10.3390/diagnostics15243219
PMCID: PMC12731402
PMID: 41464219

Conflict of interest statement: The authors declare no conflict of interest.


41. Children (Basel). 2025 Nov 26;12(12):1608. doi: 10.3390/children12121608.

Seeing Beyond the Microscope: Artificial Intelligence and Fluorescence Confocal 
Digital Imaging in Pediatric Surgical Pathology.

Di Fabrizio D(1), Daziani G(2), Qose I(3), Bindi E(1)(4), Ilari M(1), Filosa 
A(3), Busardò FP(2), Goteri G(3), Cobellis G(1)(4).

Author information:
(1)Pediatric Surgery Unit, Salesi Children's Hospital, Polytechnic University of 
Marche, 60123 Ancona, Italy.
(2)Section of Legal Medicine, Department of Biomedical Sciences and Public 
Health, Polytechnic University of Marche, 60126 Ancona, Italy.
(3)Department of Biomedical Sciences and Public Health, Institute of 
Pathological Anatomy, Polytechnic University of Marche, 60126 Ancona, Italy.
(4)Department of Specialized Clinical and Odontostomatological Sciences, 
Polytechnic University of Marche, 60126 Ancona, Italy.

Background: Digital pathology (DP) combined with fluorescence confocal 
microscopy (FCM) allows rapid tissue assessment while preserving specimens. 
Artificial intelligence (AI) and large language models (LLMs) may enhance 
diagnostic workflows, but their role in pediatric surgical pathology is largely 
unexplored. Methods: We conducted a prospective, single-center study including 
20 pediatric surgical cases with ex vivo FCM images acquired intraoperatively. 
Two commercially available LLMs, GPT-4V (AnPathology-Gpt) and Claude 3.7 Sonnet 
(AnPathology Project), were tested using structured prompts to generate 
diagnostic reports with and without immunohistochemistry (IHC) data, when 
available. Outputs were compared against the gold standard diagnosis by an 
experienced pediatric pathologist. Diagnostic performance was evaluated through 
accuracy, sensitivity, specificity, and Cohen's kappa. A paired sub-analysis was 
performed for cases with IHC (n = 5), and a sensitivity analysis excluding IHC 
cases (n = 15) was conducted. Results: Across all 20 cases, AnPathology-Gpt 
achieved 85% accuracy, 100% sensitivity, 86% specificity, and κ = 0.78, while 
AnPathology Project reached 80% accuracy, 100% sensitivity, 57% specificity, and 
κ = 0.63. Both models correctly identified all 13 neoplastic cases, with errors 
limited to non-neoplastic lesions mimicking tumors. In the IHC sub-analysis, 
accuracy improved from 40% to 80% and sensitivity from 50% to 100% for both 
models, resolving two false negatives observed in the FCM-only evaluation. 
Sensitivity analysis excluding IHC confirmed consistency of the results. 
Conclusions: This pilot study demonstrates that multimodal LLMs can support 
accurate and rapid diagnosis in pediatric digital pathology. The addition of IHC 
improves performance in diagnostically complex cases. Larger multicenter studies 
are needed to validate these findings and to define the role of AI-assisted 
workflows in pediatric surgical pathology.

DOI: 10.3390/children12121608
PMCID: PMC12731743
PMID: 41462749

Conflict of interest statement: The authors declare no conflict of interest.


42. Front Artif Intell. 2025 Dec 12;8:1728320. doi: 10.3389/frai.2025.1728320. 
eCollection 2025.

A deterministic large language model (LLM) framework for safe, protocol-adherent 
clinical decision support: application in hemodialysis anemia management 
(AnemiaCare HDs).

Arriola-Montenegro J(1), Thongprayoon C(1), Bizer B(1), Miao J(1), 
Ordaya-Gonzales K(1), Craici IM(1), Cheungpasitporn W(1).

Author information:
(1)Department of Internal Medicine, Division of Nephrology and Hypertension, 
Mayo Clinic, Rochester, MN, United States.

BACKGROUND: Large language models (LLMs) show promise for clinical decision 
support but often deviate from evidence-based protocols, raising safety and 
regulatory concerns. Anemia management in hemodialysis patients requires strict 
adherence to erythropoiesis-stimulating agent (ESA) and intravenous (IV) iron 
dosing rules, making it a high-risk use case for uncontrolled model behavior. To 
address this gap, we developed AnemiaCare HD, a deterministic LLM framework 
engineered to deliver transparent, reproducible, and protocol-adherent clinical 
recommendations.
METHODS: AnemiaCare HD was evaluated using 600 simulated hemodialysis anemia 
scenarios derived from a standardized institutional protocol. The model required 
six fixed clinical inputs (hemoglobin, hemoglobin rate of change, trend 
direction, transferrin saturation, ferritin, and current ESA dose). Phase 1 
tested a loosely structured prompt. Phase 2 implemented deterministic prompt 
logic incorporating ESA kinetics, iron dosing rules, mandatory timing 
safeguards, and embedded safety alerts. Two independent nephrologists assessed 
protocol adherence.
RESULTS: In Phase 1, only 96 of 300 cases (32%) aligned with protocol 
recommendations, with common errors in ESA titration, iron dosing, and timing 
violations. In Phase 1, loosely structured prompting produced variable outputs, 
with only 96 of 300 simulated cases (32%) fully protocol-adherent and frequent 
unsafe recommendations. In contrast, deterministic prompting in Phase 2 resulted 
in 100% adherence across all 300 cases, eliminating protocol deviations, unsafe 
iron dosing, and timing violations (p < 0.001). In Phase 2, deterministic 
encoding achieved full protocol adherence (300/300, 100%), eliminating unsafe or 
premature recommendations (p < 0.001 vs. Phase 1) and consistently generating 
structured, rationale-based outputs.
CONCLUSION: Deterministic LLM engineering enables safe, fully protocol-compliant 
clinical decision support in high-risk therapeutic domains. AnemiaCare HD 
demonstrates the viability of regulatory-aligned, auditable LLM frameworks for 
clinical use, although real-world integration and prospective validation remain 
necessary next steps.

Copyright © 2025 Arriola-Montenegro, Thongprayoon, Bizer, Miao, Ordaya-Gonzales, 
Craici and Cheungpasitporn.

DOI: 10.3389/frai.2025.1728320
PMCID: PMC12741113
PMID: 41459576

Conflict of interest statement: The author(s) declared that this work was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


43. World J Pediatr Surg. 2025 Dec 25;8(6):e001117. doi: 10.1136/wjps-2025-001117. 
eCollection 2025.

Clinical decision-making in neonatal gastrointestinal surgical emergencies: 
comparison between ChatGPT and human clinicians.

Kim S(1), Choi D(1), Son J(1).

Author information:
(1)Department of Surgery, Hanyang University College of Medicine, Seongdong-gu, 
The Republic of Korea.

BACKGROUND: Neonatal gastrointestinal surgical emergencies (NGSEs) require rapid 
decisions to prevent morbidity and mortality. This study assessed the potential 
use of ChatGPT in supporting clinical decision-making for NGSEs.
METHODS: The challenging NGSE cases (ileal atresia, midgut volvulus, 
Hirschsprung disease, meconium ileus, and pseudo-obstruction) were converted 
into structured short-answer questions including histories and radiologic 
images. Questions covered differential diagnosis, diagnostic plan, management 
plan, final diagnosis, and surgical plan. Each case was scored out of 10 
(maximum 50). Scenarios were presented to 10 general surgery (GS) residents, 10 
GS attendings, and 10 pediatric surgery (PS) attendings. GPT-4o was tested with 
10 iterations per case. Group scores were compared using appropriate statistical 
tests.
RESULTS: A total of five cases were involved. GPT-4o achieved a mean score of 
44.95 (89.9%), higher than GS residents (27.05, p<0.001) and GS attendings 
(28.35, p<0.001), but lower than PS attendings (47.70, p=0.021). Subgroup 
analysis showed GPT-4o matched PS attendings in management, final diagnosis, and 
surgical planning, but scored lower in differential diagnosis (87.8% vs. 92.8%, 
p=0.0479) and diagnostic plan (75.0% vs. 93.8%, p<0.001). Compared with GS 
residents and attendings, GPT-4o performed significantly better across all 
categories except diagnostic plan.
CONCLUSIONS: GPT-4o demonstrated performance comparable to PS attendings in key 
management domains, while clearly surpassing GS residents and attendings 
overall. These findings suggest that GPT-4o may have potential as a 
supplementary decision-support tool for NGSEs, although clinical use requires 
further validation in real-world settings.

Copyright © Author(s) (or their employer(s)) 2025. Re-use permitted under CC 
BY-NC. No commercial re-use. See rights and permissions. Published by BMJ Group.

DOI: 10.1136/wjps-2025-001117
PMCID: PMC12742057
PMID: 41459547

Conflict of interest statement: None declared.


44. Sci Rep. 2025 Dec 28;15(1):45681. doi: 10.1038/s41598-025-18047-1.

Thematic insights into the impact of large language models on K-12 education in 
rural India from student volunteers' perspectives.

Goyal H(1), Garg G(1), Mordia P(1), Ramachandran V(2), Kumar D(1), Challa JS(1).

Author information:
(1)Department of Computer Science & Information Systems, Birla Institute of 
Technology & Science, Pilani, 333031, India.
(2)Department of Humanities and Social Sciences, Birla Institute of Technology & 
Science, Pilani, 333031, India. veena.r@pilani.bits-pilani.ac.in.

Artificial Intelligence (AI)-powered tools, particularly Large Language Models 
(LLMs), are increasingly being explored as catalysts for educational 
transformation. While their application in urban and higher education contexts 
has gained traction, there remains a significant gap in understanding how such 
technologies can support K-12 learning in under-resourced rural environments. 
This study investigates the perceptions of student volunteer educators on the 
integration of LLMs in rural Indian classrooms. Drawing from 23 semi-structured 
interviews conducted with volunteers engaged in teaching initiatives across 
Rajasthan and Delhi, we employed Braun and Clarke's thematic analysis framework 
to extract key themes related to AI readiness, digital infrastructure, 
pedagogical challenges, and community attitudes. The findings reveal a complex 
landscape: volunteers recognize the potential of LLMs to personalize learning, 
alleviate teacher workload, and provide round-the-clock academic support. 
However, adoption is constrained by infrastructural limitations, lack of AI 
literacy among teachers, language barriers, and parental skepticism. Concerns 
regarding student over-reliance on AI, data privacy, and ethical risks also 
emerged. Despite these challenges, participants emphasized that AI should serve 
as a supportive augmentation rather than a replacement for human-led 
instruction. The study highlights the need for localized implementation 
strategies, culturally relevant content, and structured training programs to 
ensure equitable and responsible integration of GenAI tools in rural schools. By 
foregrounding the lived experiences of volunteer teachers situated between 
technologically advanced urban settings and underserved rural realities, this 
research contributes grounded insights into the design, deployment, and policy 
planning of AI-based learning interventions in the Global South.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-18047-1
PMCID: PMC12753670
PMID: 41457075 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


45. Sci Rep. 2025 Dec 25. doi: 10.1038/s41598-025-33704-1. Online ahead of print.

Awareness, usage, and perspectives on ChatGPT in dental education among graduate 
and undergraduate students.

Ghaffar RSA(1), Malik R(2), Amjad M(2), Nadeem R(2), Admani MS(2), Sanaullah 
A(2), Altamash S(3), Ahmed N(2), Heboyan A(4)(5)(6), Jouhar R(7).

Author information:
(1)Department of Paediatrics & Child Health, Aga Khan University, Karachi, 
Pakistan.
(2)Department of Prosthodontics, Altamash Institute of Dental Medicine, Karachi, 
75500, Pakistan.
(3)Department of Orthodontics, Altamash Institute of Dental Medicine, Karachi, 
75500, Pakistan.
(4)Department of Research Analytics, Saveetha Dental College and Hospitals, 
Saveetha Institute of Medical and Technical Sciences, Saveetha University, 
Chennai, 600 077, India. a_heboyan@farabi.tums.ac.ir.
(5)Department of Prosthodontics, Faculty of Stomatology, Yerevan State Medical 
University after Mkhitar Heratsi, Str. Koryun 2, 0025, Yerevan, Armenia. 
a_heboyan@farabi.tums.ac.ir.
(6)Department of Prosthodontics, School of Dentistry, Tehran University of 
Medical Sciences, North Karegar St, Tehran, Iran. a_heboyan@farabi.tums.ac.ir.
(7)Department of Restorative Dental Sciences, College of Dentistry, King Faisal 
University, 31982, Al Hofuf, Al-Ahsa, Saudi Arabia. rjouhar@kfu.edu.sa.

To assess the knowledge, attitudes, and practices (KAP) related to ChatGPT among 
undergraduate students and graduates in Karachi, Pakistan, and to analyze an 
association with sociodemographic factors. A cross-sectional study was conducted 
over three months using a self-administered, structured questionnaire 
distributed via online platforms (WhatsApp, and emails). The questionnaire 
included four sections: demographics, knowledge, attitude, and practice related 
to ChatGPT. A non-probability convenience sampling technique was employed. Data 
were analyzed using SPSS version 26. Descriptive statistics summarized 
participant characteristics. Multiple linear regression was used to identify 
associations between KAP outcomes and sociodemographic variables. A 
p-value ≤ 0.05 was considered statistically significant. A total of 347 
participants responded, of whom 74.6% were female and 76.4% reported awareness 
of ChatGPT. Age, designation, and years of experience were significantly 
associated with knowledge (p = 0.012, 0.001, and 0.001, respectively). Attitude 
was significantly influenced by age and gender (p = 0.03 and 0.04, 
respectively), while only gender showed a significant association with 
perception (p = 0.021). Most respondents considered ChatGPT moderately accurate 
(54.8%) and acknowledged its usefulness in education and research. Ethical 
concerns and uncertainty about ChatGPT replacing human roles were also noted. 
The majority of dental students and professionals in Karachi are aware of 
ChatGPT and hold a cautiously optimistic view of its role in dental education. 
While attitudes are generally positive, concerns remain regarding ethical 
implications and clinical applicability. Structured AI training and clear 
implementation guidelines are needed to support its effective integration in 
dental practice.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-33704-1
PMID: 41449250

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests. Ethics approval and consent to participate: The 
research ethics and review community of Altamash Institute of Dental Medicine 
has granted ethical clearance to carry out this research task. Ethics Review 
Committee reference number: AIDM/ERC/01/2024/02. The participants provided 
informed written consent to participate in the study. Informed consent: Informed 
consent has been obtained from participants involved in this study.


46. BMC Med Inform Decis Mak. 2025 Dec 24. doi: 10.1186/s12911-025-03324-w. Online 
ahead of print.

Application of artificial intelligence tools and clinical documentation burden: 
a systematic review and meta-analysis.

Zhao J(1)(2), Liu H(1)(2), Chen Y(3)(4)(5)(6), Song F(7)(8).

Author information:
(1)Department of Pediatric Research Institute, National Clinical Research Center 
for Children and Adolescents' Health and Diseases, Ministry of Education Key 
Laboratory of Child Development and Disorders, Chongqing Key Laboratory of Child 
Rare Diseases in Infection and Immunity, Children's Hospital of Chongqing 
Medical University, Chongqing, China.
(2)Chevidence Lab of Child & Adolescent Health, Children's Hospital of Chongqing 
Medical University, Chongqing, China.
(3)Department of Pediatric Research Institute, National Clinical Research Center 
for Children and Adolescents' Health and Diseases, Ministry of Education Key 
Laboratory of Child Development and Disorders, Chongqing Key Laboratory of Child 
Rare Diseases in Infection and Immunity, Children's Hospital of Chongqing 
Medical University, Chongqing, China. 483861@hospital.cqmu.edu.cn.
(4)Chevidence Lab of Child & Adolescent Health, Children's Hospital of Chongqing 
Medical University, Chongqing, China. 483861@hospital.cqmu.edu.cn.
(5)Research Unit of Evidence-Based Evaluation and Guidelines, School of Basic 
Medical Sciences, Chinese Academy of Medical Sciences (2021RU017), Lanzhou 
University, Lanzhou, China. 483861@hospital.cqmu.edu.cn.
(6)WHO Collaborating Center for Guideline Implementation and Knowledge 
Translation, Lanzhou, China. 483861@hospital.cqmu.edu.cn.
(7)Department of Pediatric Research Institute, National Clinical Research Center 
for Children and Adolescents' Health and Diseases, Ministry of Education Key 
Laboratory of Child Development and Disorders, Chongqing Key Laboratory of Child 
Rare Diseases in Infection and Immunity, Children's Hospital of Chongqing 
Medical University, Chongqing, China. fujian.song@uea.ac.uk.
(8)Norwich Medical School, University of East Anglia, Norwich, Norfolk, UK. 
fujian.song@uea.ac.uk.

BACKGROUND: Clinician burnout is a growing global concern, with heavy clinical 
documentation workload identified as a major contributor. Clinical documentation 
tasks, though essential for patient care and communication, are time-consuming 
and cognitively demanding. Recent advances in artificial intelligence (AI), 
particularly natural language processing and large language models, are being 
explored as potential tools to alleviate documentation burden, yet their 
quantitative impact has not been systematically assessed.
METHODS: We performed a systematic review and meta-analysis, registered on 
PROSPERO (CRD420250653291) and guided by PRISMA. Eligible studies included 
frontline health professionals using AI tools for clinical note creation, with 
comparators being usual practice or pre-implementation baseline. Primary 
outcomes were documentation burden, workload, burnout, and time spent on 
documentation. Searches were conducted in PubMed, Web of Science, Scopus, and 
key journals. Effect sizes were synthesized using standardized mean difference 
(SMD) under a random-effects model, with subgroup analyses by study design, AI 
tool type, task type, editing status, and data origin.
RESULTS: Of the 23 studies included, 12 were non-randomised studies with a 
concurrent control and 11 employed a before-and-after comparison design. The 
study participants varied in specialties and were mainly from ambulatory 
settings, including physicians, surgeons, pediatricians, and ICU specialists. 
Heterogeneity in results across included studies was considerable, and the 
methodological quality of the available studies was generally low. Pooling 
results of the 14 studies yielded an overall standardized mean difference (SMD) 
of -0.71 (95% confidence interval [CI]: -0.93 to -0.49), indicating a moderate 
reduction in documentation workload and related burnout. Based on results of 
studies in which clinicians reviewed and edited AI-generated drafts, AI 
applications reduced documentation time, similarly representing a moderate 
effect size (SMD= -0.72, 95% CI -0.99 to -0.45). The quality of notes generated 
by AI tools was at least comparable to those prepared manually by clinicians.
CONCLUSIONS: AI technologies offer promising benefits for reducing clinical 
documentation burden. However, their implementation must be accompanied by 
rigorous quality control and ongoing evaluation in practical settings to 
optimize their effectiveness and safeguard patient care outcomes.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-03324-w
PMID: 41444884

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. This study is a systematic review and meta-analysis 
of previously published literature and did not involve any human participants 
directly. Consent for publication: Not applicable. Competing interests: The 
authors declare no competing interests.


47. JMIR Ment Health. 2025 Dec 24;12:e81204. doi: 10.2196/81204.

ChatGPT Clinical Use in Mental Health Care: Scoping Review of Empirical 
Evidence.

Balan R(1), Gumpel TP(1).

Author information:
(1)Seymour Fox School of Education, Hebrew University of Jerusalem, Mount 
Scopus, Jerusalem, 94554 11, Israel, 972 0545443079.

BACKGROUND: As mental health challenges continue to rise globally, there is an 
increasing interest in the use of GPT models, such as ChatGPT, in mental health 
care. A few months after its release, tens of thousands of users interacted with 
GPT-based therapy bots, with mental health support identified as the primary use 
case. ChatGPT offers scalable and immediate support through natural language 
processing capabilities, but their clinical applicability, safety, and 
effectiveness remain underexplored.
OBJECTIVE: This scoping review aims to provide a comprehensive overview of the 
main clinical applications of ChatGPT in mental health care, along with the 
existing empirical evidence for its performance.
METHODS: A systematic search was conducted in 8 electronic databases in April 
2025 to identify primary studies. Eligible studies included primary research, 
reporting on the evaluation of a ChatGPT clinical application implemented for a 
mental health care-specific purpose.
RESULTS: In total, 60 studies were included in this scoping review. The results 
highlighted that most applications used generic ChatGPT and focused on the 
detection of mental health problems and counseling and treatment. At the same 
time, only a minority of studies investigated ChatGPT use in clinical decision 
facilitation and prognosis tasks. Most of the studies were prompt experiments, 
in which standardized text inputs-designed to mimic clinical scenarios, patient 
descriptions, or practitioner queries-are submitted to ChatGPT to evaluate its 
performance in mental health-related tasks. In terms of performance, ChatGPT 
shows good accuracy in binary diagnostic classification and differential 
diagnosis, simulating therapeutic conversation, providing psychoeducation, and 
conducting specific therapeutic strategies. However, ChatGPT has significant 
limitations, particularly with more complex clinical presentations and its 
overly pessimistic prognostic outputs. Nevertheless, overall, when compared to 
mental health experts or other artificial intelligence models, ChatGPT 
approximates or surpasses their performance in conducting various clinical 
tasks. Finally, custom ChatGPT use was associated with better performance, 
especially in counseling and treatment tasks.
CONCLUSIONS: While ChatGPT offers promising capabilities for mental health 
screening, psychoeducation, and structured therapeutic interactions, its current 
limitations highlight the need for caution in clinical adoption. These 
limitations also underscore the need for rigorous evaluation frameworks, model 
refinement, and safety protocols before broader clinical integration. Moreover, 
the variability in performance across versions, tasks, and diagnostic categories 
also invites a more nuanced reflection on the conditions under which ChatGPT can 
be safely and effectively integrated into mental health settings.

© Raluca Balan, Thomas P Gumpel. Originally published in JMIR Mental Health 
(https://mental.jmir.org).

DOI: 10.2196/81204
PMCID: PMC12735656
PMID: 41442647 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


48. Pharmacy (Basel). 2025 Dec 3;13(6):176. doi: 10.3390/pharmacy13060176.

Large Language Models for Drug-Related Adverse Events in Oncology Pharmacy: 
Detection, Grading, and Actioning.

Zitu MM(1), Manne A(2), Zhu Y(3), Rahat WB(4), Binkheder S(5).

Author information:
(1)Department of Machine Learning, Moffitt Cancer Center and Research Institute, 
Tampa, FL 33612, USA.
(2)Department of Medicine, College of Medicine, The Ohio State University, 
Columbus, OH 43210, USA.
(3)Department of Pediatrics, University Hospitals Rainbow Babies & Children's 
Hospital, Cleveland, OH 44106, USA.
(4)Department of Computer Science and Engineering, East West University, 
Aftabnagar, Dhaka 1212, Bangladesh.
(5)Medical Informatics and E-Learning Unit, Medical Education Department, 
College of Medicine, King Saud University, Riyadh 12372, Saudi Arabia.

Preventable medication harm in oncology is often driven by drug-related adverse 
events (AEs) that trigger order changes such as holds, dose reductions, delays, 
rechallenges, and enhanced monitoring. Much of the evidence needed to make these 
decisions lives in unstructured clinical texts, where large language models 
(LLMs), a type of artificial intelligence (AI), now offer extraction and 
reasoning capabilities. In this narrative review, we synthesize empirical 
studies evaluating LLMs and related NLP systems applied to clinical text for 
oncology AEs, focusing on three decision-linked tasks: (i) AE detection from 
clinical documentation, (ii) Common Terminology Criteria for Adverse Events 
(CTCAE) grade assignment, and (iii) grade-aligned actions. We also consider how 
these findings can inform pharmacist-facing recommendations for order-level 
safety. We conducted a narrative review of English-language studies indexed in 
PubMed, Ovid MEDLINE, and Embase. Eligible studies used LLMs on clinical 
narratives and/or authoritative guidance as model inputs or reference standards; 
non-text modalities and non-empirical articles were excluded. Nineteen studies 
met inclusion criteria. LLMs showed the potential to detect oncology AEs from 
routine notes and often outperformed diagnosis codes for surveillance and cohort 
construction. CTCAE grading was feasible but less stable than detection; 
performance improved when outputs were constrained to CTCAE terms/grades, 
temporally anchored, and aggregated at the patient level. Direct evaluation of 
grade-aligned actions was uncommon; most studies reported proxies (e.g., steroid 
initiation or drug discontinuation) rather than formal grade-to-action 
correctness. While prospective, real-world impact reporting remained sparse, 
several studies quantified scale advantages and time savings, supporting an 
initial role as high-recall triage with pharmacist adjudication. Overall, the 
evidence supports near-term, pharmacist-in-the-loop use of AI for AE 
surveillance and review, with CTCAE-structured, citation-backed outputs 
delivered into the pharmacist's electronic health record order-verification 
workspace as reviewable artifacts. Future work must standardize reporting and 
CTCAE/version usage, and measure grade-to-action correctness prospectively, to 
advance toward order-level decision support.

DOI: 10.3390/pharmacy13060176
PMCID: PMC12736875
PMID: 41441324

Conflict of interest statement: The authors declare no conflicts of interest.


49. Int J Reprod Biomed. 2025 Dec 3;23(10):827-842. doi: 10.18502/ijrm.v23i10.20316. 
eCollection 2025 Oct.

Presenting a conceptual model for decision support systems in infertility: A 
developmental study.

Sajjadi H(1), Choobineh H(2)(3), Safdari R(1).

Author information:
(1)Department of Health Information Management and Medical Informatics, School 
of Allied Medical Sciences, Tehran University of Medical Sciences, Tehran, Iran.
(2)Department of Medical Laboratory Sciences, School of Allied Medical Sciences, 
Tehran University of Medical Sciences, Tehran, Iran.
(3)Zeoonosis Research Center, Tehran University of Medical Sciences, Tehran, 
Iran.

BACKGROUND: Infertility is the inability to conceive after a year of trying, 
resulting in unintentional childlessness. A clinical decision-support system can 
enhance diagnosis, reduce costs, improve access, and increase treatment 
accuracy.
OBJECTIVE: This study aimed to present a conceptual model for decision support 
systems in infertility.
MATERIALS AND METHODS: This developmental study, conducted from April-November 
2024 in 3 steps. First, PubMed, Scopus, and Web of Science databases were 
investigated to identify data for decision support systems in infertility. Next, 
search engines like Google, Yahoo, and Bing, along with artificial intelligence 
tools such as ChatGPT, Gemini, and Perplexity, helped identify similar systems. 
Lastly, opinions from 32 infertility experts were collected via a 
researcher-made questionnaire, with reliability confirmed by Cronbach's alpha of 
0.78 and validity confirmed by content validity ratio of 0.60.
RESULTS: In the first step, 16,310 articles were identified; 10 were selected 
after removing duplicates and applying inclusion and exclusion criteria. In the 
second step, 71 relevant systems were identified in search engines; 58 were 
excluded, leaving 13 for further analysis. In the third step, a 
researcher-designed questionnaire was distributed to 32 experts, yielding key 
agreement rates of 94% for monitoring and follow-up, 94% for sperm analysis 
data, 90% for abortion data, and 82.5% for infertility information from health 
magazines. Requirements grouped into 4 categories: main features (10 elements), 
patient info management (19 elements), fertility prediction data (16 elements), 
and secondary features (3 elements). The model's overall agreement was 85%.
CONCLUSION: Developing a decision-support system for infertility could enhance 
clinical care and outcomes; however, challenges include standardizing validation 
methods and considering ethnic diversity.

Copyright © 2025 Sajjadi et al.

DOI: 10.18502/ijrm.v23i10.20316
PMCID: PMC12703027
PMID: 41438399

Conflict of interest statement: The authors declare that there is no conflict of 
interest.


50. J Med Syst. 2025 Dec 23;49(1):188. doi: 10.1007/s10916-025-02323-8.

Artificial Intelligence's Capacity to Detect Subtle Medical Misinformation: A 
Novel Reverse Prompting Approach.

Bendary M(1), Ramzy N(2), Khater A(3), Nasif MM(4), Atef N(3).

Author information:
(1)Epidemiology & Biostatistics Department, National Cancer Institute, Cairo 
University, Kasr el Ain Street, Cairo, Egypt. bendary3@gmail.com.
(2)Pediatric Department, Faculty of Medicine, Benha University, Benha, Egypt.
(3)Epidemiology & Biostatistics Department, National Cancer Institute, Cairo 
University, Kasr el Ain Street, Cairo, Egypt.
(4)Ophthalmology Department, Faculty of Medicine, Zagazig University, Zagazig, 
Egypt.

Medical misinformation is a major public health concern. The public increasingly 
uses artificial intelligence (AI) tools for medical consultations. Therefore, 
concerns arise about their ability to detect and even correct subtle medical 
information that users may be embedding in users prompts. This study assessed 
the ability of different ChatGPT models in detecting and correcting such subtle 
misinformation. Fifty clinical plausible prompts with subtle medical 
misinformation were introduced separately to ChatGPT models 4o, 4.1-mini, and 
GPT-5. Prompts spanned Internal Medicine, Cardiology, Pediatrics, Ophthalmology, 
and Oncology. Responses were scored on a 3-point scale: 0: No correction; 1: 
Hedging or uncertainty; 3: cutting edge detection and correction. GPT-4o was the 
best performing model, surpassing GPT-5 by correctly identifying and correcting 
misinformation in 86% of the prompts compared to 74% for GPT-5. GPT-4.1-mini 
showed weaker performance, detecting dsmisinformation in only 52% of prompts, 
with complete failure in 34% and hedging in 14%. Specialty-specific analysis 
revealed that GPT-4o achieved higher detection rate in all tested specialties 
compared to GPT-4.1-mini and GPT-5. Only oncology showed comparable detection 
rates between GPT-4o and GPT-5. Although the performance of GPT-4o and GPT-5 in 
detecting subtle medical misinformation was promising, unexpectedly, GPT-4o 
surpassed GPT-5 in performance. Using underpowered variants such as 
GPT-4.1-mini, poses a public health threat. Reverse prompting offers a 
diagnostic lens and should be integrated into standard AI safety testing 
protocols.

© 2025. The Author(s), under exclusive licence to Springer Science+Business 
Media, LLC, part of Springer Nature.

DOI: 10.1007/s10916-025-02323-8
PMID: 41432852 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics Approval: Not applicable. 
Consent for Publication: Not applicable. Clinical Trial Number: Not applicable. 
Competing interests: The authors declare no competing interests.


51. Oncologist. 2026 Jan 17;31(2):oyaf414. doi: 10.1093/oncolo/oyaf414.

Attitudes toward large language model-based Artificial Intelligence systems as 
an information source for shared decision-making in radiation oncology.

Moser R(1), Buchecker LM(1), Nano J(1), Mayr NA(2), Behzadi ST(1), Kiesl S(1), 
Maier S(1), Allwohn L(1), Lammert J(3)(4)(5), Adams LC(6), Tschochohei M(4)(7), 
Combs SE(1)(5)(8), Borm KJ(1)(9).

Author information:
(1)Department of Radiation Oncology, TUM University Hospital, Klinikum Rechts 
der Isar, School of Medicine and Health, Technical University of Munich, Munich 
81675, Germany.
(2)College of Human Medicine, Michigan State University, East Lansing, MI 4882, 
United States.
(3)Department of Gynecology and Center for Hereditary Breast and Ovarian Cancer, 
Technical University of Munich (TUM), School of Medicine and Health, Klinikum 
rechts der Isar, TUM University Hospital, Munich 81675, Germany.
(4)Institute of Artificial Intelligence and Informatics in Medicine (AIIM), TUM 
University Hospital, Technical University of Munich (TUM), Munich 81675, 
Germany.
(5)German Cancer Consortium (DKTK), partner site Munich, a partnership between 
DKFZ and TUM University Hospital, Munich, 81675, Germany.
(6)Department of Diagnostic and Interventional Radiology, School of Medicine and 
Health, Klinikum rechts der Isar, TUM University Hospital, Technical University 
of Munich, Munich 81675, Germany.
(7)Google Deutschland GmbH, Munich 80636, Germany.
(8)Institute of Innovative Radiotherapy (iRT), Department of Radiation Sciences 
(DRS), Helmholtz Zentrum München, Oberschleißheim 85764, Germany.
(9)Bavarian Center for Cancer Research (BZKF), Technical University Munich, 
Munich 81675, Germany.

BACKGROUND: Implementing structured shared decision-making (SDM) requires 
high-quality, reliable patient information. In radiation oncology, patients 
often have limited knowledge and misconceptions about therapy and side effects, 
affecting their decision-making. Large language model-based AI systems (LLMs) 
may help by providing evidence-based information in accessible language, but 
successful implementation depends on the willingness of patients and health care 
professionals (HCPs) to adopt these technologies.
METHODS: A survey was conducted among patients undergoing radiation therapy and 
HCPs between 03/2024 and 02/2025. Data was collected using structured electronic 
questionnaires (32 items for patients, 35 for HCPs). The survey assessed 
sociodemographic characteristics, the status of SDM in oncology, sources of 
information relevant to SDM, and current and anticipated LLM applications. Data 
were analyzed using descriptive statistics and logistic regression analysis.
RESULTS: The internet was the prime information source for patients (n = 400). 
Regarding current use of LLMs, a large discrepancy between patients and HCPs 
(n = 200) was observed (18.2% vs 69.5%). Although 77% of HCPs believed that 
patients will rely on LLMs in the future, only 29.1% of patients agreed. Most 
patients (65.8%) stated that even as LLMs improve, they will continue to trust 
physicians more; 46% of HCPs shared this view. Only 16.5% of patients were 
convinced that LLMs provide all relevant data for SDM in cancer care. 
Familiarity with technology was the strongest predictor of LLM use among 
patients.
CONCLUSION: Only a minority of radiation oncology patients currently use LLMs, 
and many remain skeptical about their future role-contrasting with the more 
optimistic expectations of HCPs.

© The Author(s) 2025. Published by Oxford University Press.

DOI: 10.1093/oncolo/oyaf414
PMID: 41429565 [Indexed for MEDLINE]


52. Front Med (Lausanne). 2025 Dec 5;12:1698366. doi: 10.3389/fmed.2025.1698366. 
eCollection 2025.

Trends in AI-based diagnosis and intervention of metabolic diseases: a 
bibliometric analysis of the literature from 2000 to 2024.

Chen B(1), Wang Y(1), Xie X(1), Fan B(1), Zhang W(2), Sun G(1).

Author information:
(1)College of Traditional Chinese Medicine, Hunan University of Traditional 
Chinese Medicine, Changsha, China.
(2)Department of Geriatrics, The First Affiliated Hospital of Hunan University 
of Chinese Medicine, Changsha, China.

BACKGROUND: In recent years, the global prevalence of metabolic diseases has 
continued to rise, emerging as a major public health concern worldwide. 
Artificial intelligence (AI), with its advanced capabilities in data analysis, 
pattern recognition, and predictive reasoning, has demonstrated remarkable 
potential in enhancing the diagnosis, management, and prevention of metabolic 
disorders.
METHODS: Literature searches were conducted in the WOSCC and Scopus databases, 
followed by database merging and deduplication using R scripts, resulting in a 
final corpus of 1,059 publications for analysis. Bibliometric analysis tools, 
including VOSviewer and CiteSpace, were subsequently employed to visualize and 
quantitatively assess the distribution of publications by country and 
institution, prolific authors, influential journals, citation networks, and 
emerging research keywords.
RESULTS: Research on AI in metabolic diseases has experienced explosive growth, 
with publication output increasing by 394% over the past 5 years, paralleling 
the broader expansion of AI technologies. China, the United States, and the 
United Kingdom have emerged as the leading contributors in this domain, with 
China contributing the largest share (21.87%), followed by the United States 
(17.10%). Among high-output institutions, Institutions from China contributed 
the most publications (298), while Harvard Medical School in the United States 
demonstrates the strongest academic influence. Nieuwdorp M stands out as the 
most prolific and highly cited author, with Kupusinac and Aleksandar also 
recognized for their significant contributions to the field. Scientific Reports 
ranks as the most productive journal, whereas Atherosclerosis is identified as 
one of the most authoritative journals among high-output publications. The 
co-occurrence of keywords such as "machine learning," "deep learning," "data 
mining," "metabolomics," and "diagnosis" reveals the application of artificial 
intelligence and advanced data processing techniques in metabolic diseases.
DISCUSSION: This study provides a comprehensive bibliometric overview of the 
evolution and research trends of AI in the diagnosis and intervention of 
metabolic diseases. The analysis highlights three major research frontiers: 
AI-assisted prevention using smart devices, multimodal diagnostic approaches, 
and intervention strategies guided by large language models (LLMs). Overall, the 
findings offer valuable insights into the ongoing transformation of metabolic 
disease management through AI technologies and lay the groundwork for future 
research aimed at advancing intelligent diagnostic and therapeutic systems.

Copyright © 2025 Chen, Wang, Xie, Fan, Zhang and Sun.

DOI: 10.3389/fmed.2025.1698366
PMCID: PMC12714997
PMID: 41426577

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


53. Nurs Adm Q. 2026 Jan-Mar 01;50(1):44-67. doi: 10.1097/NAQ.0000000000000721. Epub 
2025 Dec 17.

Digital Innovations Improving Nursing Practice Through Artificial Intelligence: 
The State of the Science.

Schultz MA(1), Kim HJ(2), Chou FY(3), Flores SL(4), Fangonil-Gagalang E(5).

Author information:
(1)Department of Nursing, Nurse Informaticist, California State University, San 
Bernardino, San Bernardino, CA.
(2)Nurse Research Scientist, National Patient Care Services, Kaiser Permanente, 
UCSF School of Nursing.
(3)School of Nursing, College of Health and Social Sciences, San Francisco State 
University, San Francisco, California.
(4)Division of Health, Biola University, Clinical Project Manager, CHOC 
Children's.
(5)Department of Nursing, California State University, San Bernardino.

Despite significant concerns about the broad use of artificial intelligence (AI) 
in clinical practice, registered nurses are commonly challenged to adopt, 
evaluate, and improve these digital solutions in almost any care environment. 
The aim of this study is to appraise, then analyze, the scientific literature 
for contemporary digital innovations that improve nursing practice through the 
use of AI. This scoping review with thematic analysis of digital innovations 
impacting nursing practice was conducted using CINAHL and PubMed databases 
(2022-2024). We describe the use of large language models and associated 
software tools in supporting this work, highlighting the essential 
human-in-the-loop activities involved. Results: This review highlights emerging 
themes, notably improved patient safety and greater trust and acceptance. A 
persistent emphasis on AI as an assistive entity was found across many products, 
processes and pilots identified in our corpus. This body of literature is 
rapidly evolving and characterized by multiple competing themes. The proper 
implementation and execution of AI as an assistive mechanism is crucial for 
safety enhancement, improved quality, and trust. Recommendations include the 
provision of transparency in all aspects of AI innovations for practice (from 
inception to evaluation) and participatory implementation activities.

Copyright © 2025 Wolters Kluwer Health, Inc. All rights reserved.

DOI: 10.1097/NAQ.0000000000000721
PMID: 41423999 [Indexed for MEDLINE]

Conflict of interest statement: There are no conflicts of interest expressed by 
any author.


54. BMC Oral Health. 2025 Dec 20. doi: 10.1186/s12903-025-07486-x. Online ahead of 
print.

Examination of new clinical dental caries in school children using real intra 
oral photos with artificial intelligence model YOLO-V8x.

Fadilah RPN(1)(2), Rikmasari R(3), Akbar S(4), Setiawan AS(5).

Author information:
(1)Doctoral Program, Faculty of Dentistry, Padjadjaran University, Bandung, 
Indonesia. rina21002@mail.unpad.ac.id.
(2)Departement of Dental Public Health, Faculty of Dentistry, Jenderal Achmad 
Yani University, Cimahi, Indonesia. rina21002@mail.unpad.ac.id.
(3)Departement. of Prosthodontic Dentistry, Faculty of Dentistry, Padjadjaran 
University, Bandung, Indonesia.
(4)School of Electrical Engineering and Informatics, Institute Technology of 
Bandung, Bandung, Indonesia.
(5)Department of Pediatric Dentistry, Faculty of Dentistry, Padjadjaran 
University, Bandung, Indonesia.

BACKGROUND: Dental caries is a common chronic condition among school children, 
and in Indonesia, rates have been higher than 80%. Traditional diagnostic 
techniques tend to be long and dependent upon the availability of healthcare 
professionals and hence promote development of alternatives involving artificial 
intelligence (AI). This work aimed to evaluate the effectiveness of the 
application titled HI Bogi that embeds the model known as YOLOv8 in 
distinguishing dental caries among primary school pupils in Cimahi, Indonesia.
MATERIALS AND METHODS: A cross-sectional analytic model with prospectively 
collected data was adopted. A dataset consisting of 3,221 JPG photographs was 
created and labeled based on the International Caries Detection and Assessment 
System (ICDAS, D0 - D6) using Roboflow software. The photographs were resized to 
a size of 640 × 640 pixels and distributed into training (2,266 photographs), 
validation (635 photographs), and testing (320 photographs) sets. A YOLOv8x 
algorithm was used for application in deep learning tasks, and its performance 
was determined through mean Average Precision (mAP), Intersection over Union 
(IoU) measurements, and precision and recall metrics. Additionally, a Mann 
Whitney statistical test was carried out to contrast classification efficiency 
between the AI system and dental practitioners' classification efficacy, while 
both methods' diagnostic speed was tested.
RESULTS: mAP was 45.8%, and precision was 72.6%, and recall was 41.1% for 
YOLOv8x model. Comparative testing across ICDAS categories did not vary 
significantly between dentists and AI (p > 0.05). Detection of caries (D1-D6) 
was observed to have a sensitivity ranged from 82.31% to 96.45% and specificity 
ranged from 77.10% to 99.33% on new data. Additionally, testing was completed 
approximately four times faster with AI compared to manual testing (p = 0.000).
CONCLUSION: The YOLOv8x model in the HI Bogi app demonstrated diagnostic 
performance equal to dentists and a drastically shortened examination time, 
validating its use in school-based dental health programs. But class-specific 
precision variation suggests further refinement is needed. Further studies ought 
to increase datasets and characterize advanced architecture, such as 
transformer-based architecture, to achieve higher specificity and detection 
rates in rare lesion categories.

© 2025. The Author(s).

DOI: 10.1186/s12903-025-07486-x
PMID: 41422004

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was conducted in accordance with the ethical standards 
of the Declaration of Helsinki (1964, and its later amendments) and received 
ethical approval from the Research Ethics Committee of the Faculty of Dentistry, 
Padjadjaran University (approval number: 1291/UN6.KEP/EC/2023; registration 
number: 2309051343; Clinical trial number: not applicable). Written informed 
consent to participate was obtained from the parents or legal guardians of all 
participating children under the age of 16 prior to data collection. Consent for 
publication: Written informed consent for publication of clinical details and/or 
clinical images was obtained from the parents or legal guardians of all 
participating children under the age of 16. Competing interests: The authors 
declare no competing interests.


55. Lancet Digit Health. 2025 Dec;7(12):100926. doi: 10.1016/j.landig.2025.100926. 
Epub 2025 Dec 18.

Development and validation of a pre-trained language model for neonatal 
morbidities: a retrospective, multicentre, prognostic study.

Xie F(1), Chung P(2), Reiss JD(3), Tjoa E(4), De Francesco D(4), Phongpreecha 
T(4), Haberkorn W(4), Chakraborty D(4), Chang AL(4), James T(4), Kim Y(4), 
Mataraso S(4), Espinosa C(4), Yang L(4), Shu CH(4), Xue L(4), Berson E(4), 
Mohammadi N(4), Shome S(4), Reincke SM(4), Ghanem M(4), Maric I(3), Gaudilliere 
B(5), Angst MS(2), Sylvester K(6), Shaw GM(3), Prince LS(3), Stevenson DK(3), 
Aghaeepour N(7).

Author information:
(1)Department of Anesthesiology, Perioperative and Pain Medicine, Stanford 
University, Stanford, CA, USA; Department of Pediatrics, Stanford University 
School of Medicine, Stanford, CA, USA; Department of Biomedical Data Science, 
Stanford University, Stanford, CA, USA; Division of Computational Health 
Sciences, Department of Surgery, University of Minnesota, Minneapolis, MN, USA.
(2)Department of Anesthesiology, Perioperative and Pain Medicine, Stanford 
University, Stanford, CA, USA.
(3)Department of Pediatrics, Stanford University School of Medicine, Stanford, 
CA, USA.
(4)Department of Anesthesiology, Perioperative and Pain Medicine, Stanford 
University, Stanford, CA, USA; Department of Pediatrics, Stanford University 
School of Medicine, Stanford, CA, USA; Department of Biomedical Data Science, 
Stanford University, Stanford, CA, USA.
(5)Department of Anesthesiology, Perioperative and Pain Medicine, Stanford 
University, Stanford, CA, USA; Department of Pediatrics, Stanford University 
School of Medicine, Stanford, CA, USA.
(6)Department of Pediatrics, Stanford University School of Medicine, Stanford, 
CA, USA; Department of Surgery, Stanford University, Stanford, CA, USA.
(7)Department of Anesthesiology, Perioperative and Pain Medicine, Stanford 
University, Stanford, CA, USA; Department of Pediatrics, Stanford University 
School of Medicine, Stanford, CA, USA; Department of Biomedical Data Science, 
Stanford University, Stanford, CA, USA. Electronic address: 
naghaeep@stanford.edu.

BACKGROUND: Early identification and monitoring of neonatal morbidities are 
critical for timely interventions that can prevent complications, optimise 
resource use, and support families. Although traditional tools based on tabular 
data and biomarkers are beneficial, they are restricted in assessing the risk of 
morbidities in newborns. In this study, we developed NeonatalBERT, a pre-trained 
large language model (LLM) that estimates the risk of neonatal morbidities from 
clinical notes.
METHODS: This prognostic study investigated retrospective primary and external 
cohorts from two different quaternary-care academic medical centres in the USA: 
Stanford Health Care and Beth Israel Deaconess Medical Center. NeonatalBERT was 
initially pre-trained on clinical notes from the primary cohort and then 
fine-tuned separately for both cohorts. NeonatalBERT was also compared against 
other existing LLMs, such as BioBERT and Bio-ClinicalBERT, as well as 
traditional machine learning and logistic regression models using tabular 
features. NeonatalBERT was evaluated on 19 neonatal morbidities (respiratory 
distress syndrome, bronchopulmonary dysplasia, pulmonary haemorrhage, pulmonary 
hypertension, atelectasis, aspiration syndrome, intraventricular haemorrhage, 
periventricular leukomalacia, neonatal seizures, other CNS disorders, patent 
ductus arteriosus, cardiovascular instability, sepsis, candidiasis, anaemia, 
jaundice, necrotising enterocolitis, retinopathy of prematurity, and death) for 
the primary cohort and ten for the external cohort (respiratory distress 
syndrome, bronchopulmonary dysplasia, pulmonary haemorrhage, intraventricular 
haemorrhage, patent ductus arteriosus, sepsis, jaundice, necrotising 
enterocolitis, retinopathy of prematurity, and death). For each outcome, the 
area under the receiver operating characteristic curve, area under the 
precision-recall curve (AUPRC), and F1 scores were evaluated.
FINDINGS: 32 321 newborns were included in the primary cohort, including 27 411 
in the primary training set (mean gestational age 38·64 weeks [SD 2·30]; 13 056 
[47·6%] female and 14 355 [52·4%] male newborns) and 4910 in the primary testing 
set (mean gestational age 38·64 [2·13] weeks; 2336 [47·6%] female and 2574 
[52·4%] male newborns). Additionally, 7061 newborns were selected into the 
external cohort, including 5653 in the external training set (1567 [27·7%] 
premature and 4086 [72·3%] term births; 2614 [46·2%] female and 3039 [53·8%] 
male newborns) and 1408 in the external testing set (383 [27·2%] premature and 
1025 [72·8%] term births; 624 [44·3%] female and 784 [55·7%] male newborns). In 
the primary cohort, the mean AUPRC over 19 outcomes was 0·291 (95% CI 
0·268-0·314) for NeonatalBERT, 0·238 (0·217-0·259) for Bio-ClinicalBERT, 0·217 
(0·197-0·236) for BioBERT, and 0·194 (0·177-0·211) for the traditional model 
using tabular data. In the external cohort, NeonatalBERT had a mean AUPRC of 
0·360 (0·328-0·393), outperforming other models with the range of 0·224-0·333.
INTERPRETATION: Based on validation using two large-scale US datasets, 
NeonatalBERT effectively estimates the risk of neonatal morbidities from 
unstructured clinical notes of newborns. The promising results from this study 
show the potential of NeonatalBERT to enhance neonatal care and streamline 
hospital operations.
FUNDING: National Institutes of Health, Burroughs Wellcome Fund, March of Dimes 
Foundation, Alfred E Mann Foundation, Gates Foundation, Christopher Hess 
Research Fund, Roberts Foundation Research Fund, Prematurity Research Center, 
and Stanford Maternal & Child Health Research Institute Postdoctoral Support 
funds.

Copyright © 2025 The Author(s). Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.landig.2025.100926
PMCID: PMC12748047
PMID: 41419365 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of interests We declare no competing 
interests.


56. bioRxiv [Preprint]. 2025 Dec 13:2025.12.11.693669. doi: 
10.64898/2025.12.11.693669.

Query-driven generative AI synthesizes multi-modal spatial omics from histology.

Pang M(1), Roy TK(2), Wu X(3), Tan K(4)(5)(6).

Author information:
(1)Applied Mathematics & Computational Science Graduate Group, University of 
Pennsylvania, Philadelphia, PA, USA.
(2)Department of Computer Science, The University of Iowa, Iowa City, IA, USA.
(3)Department of Electrical and Computer Engineering, The University of Iowa, 
Iowa City, IA, USA.
(4)Department of Pediatrics, Perelman School of Medicine, University of 
Pennsylvania, Philadelphia, PA, USA.
(5)Division of Oncology and Center for Childhood Cancer Research, Children's 
Hospital of Philadelphia, Philadelphia, PA, USA.
(6)Center for Single Cell Biology, Children's Hospital of Philadelphia, 
Philadelphia, PA, USA.

Spatial omics technologies offer unprecedented insights into the cellular 
organization of tissues; however, they are not yet scalable for routine clinical 
use. In contrast, images of histological staining remain the foundation of 
pathological diagnosis despite lacking molecular information. Bridging this gap 
requires computational methods that can accurately infer spatial molecular data 
from histology alone. Here, we introduce TissueCraftAI, a generative artificial 
intelligence framework that predicts multi-modal spatial omics maps directly 
from standard histology images using natural language prompts. To train and 
validate our model, we created PRISM-12M, a large-scale dataset comprising over 
twelve million spatially registered histology and spatial omics image patches 
across fourteen tissue types from humans and mice. TissueCraftAI significantly 
outperforms existing methods in generating realistic histology images and 
predicting spatial proteomics and transcriptomics data with high fidelity. We 
demonstrated its utility in various downstream applications, including improving 
cell type annotation and enhancing the accuracy of patient survival predictions 
across multiple cancer types. By enabling flexible, query-driven in silico 
spatial molecular analysis using routine histology images, TissueCraftAI opens 
up new research avenues in computational pathology.

DOI: 10.64898/2025.12.11.693669
PMCID: PMC12710965
PMID: 41415434

Conflict of interest statement: Competing interests The authors declare no 
competing interests.


57. EBioMedicine. 2026 Jan;123:106085. doi: 10.1016/j.ebiom.2025.106085. Epub 2025 
Dec 17.

SEEGformer: personalised SEEG-based seizure detection and epileptogenic zone 
localisation for drug-resistant epilepsy.

Wang C(1), Hong J(2), Liu S(1), Jiao R(1), Wang R(1), Xue B(1), Fang X(1), Yuan 
Y(1), Zu W(1), Chen Y(1), Zhang Y(3), Wang Y(3), Lu S(4), Jiang X(4), Wan Y(5), 
Li L(6), Xu J(7), Jin C(8).

Author information:
(1)Medical Robot Research Institute, School of Biomedical Engineering, Shanghai 
Jiao Tong University, Shanghai, 200240, China.
(2)Ruijin Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai, 
200020, China.
(3)Shanghai Artificial Intelligence Laboratory, Shanghai, 200233, China.
(4)Department of Orthopedic Trauma, Beijing Jishuitan Hospital, Capital Medical 
University, Beijing, 102208, China; Beijing Research Institute of Traumatology 
and Orthopaedics, Beijing, 100035, China.
(5)Medical Technology Department, Qiqihar Medical University, Qiqihar, 161003, 
China. Electronic address: wanyg@126.com.
(6)School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, 
200240, China. Electronic address: lilm@sjtu.edu.cn.
(7)Ruijin Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai, 
200020, China. Electronic address: xjw88@vip.163.com.
(8)Medical Robot Research Institute, School of Biomedical Engineering, Shanghai 
Jiao Tong University, Shanghai, 200240, China; Beijing Anding Hospital, Capital 
Medical University, Beijing, 100088, China; National Clinical Research Center 
for Kidney Diseases, Beijing, 100088, China; Shanghai Point Imaging Medical 
Technology Co., LTD, China. Electronic address: chengjin520@sjtu.edu.cn.

BACKGROUND: Epilepsy is a heterogeneous syndrome. Personalised localisation of 
epileptogenic zone (EZ) is critical for diagnosis and treatment of 
drug-resistant focal epilepsy. Multichannel stereoelectroencephalography (SEEG) 
monitoring acquired over a period of two to three weeks was collected in 
different patients, resulting in comprehensive epileptogenic information and 
terabytes of high dimensional data. Consequently, there is a need for 
high-throughput data analytical methods to enable data-driven, personalised 
seizure detection and EZ localisation.
METHODS: Here, a seizure detection and EZ localisation AI system - SEEGformer is 
proposed, by utilising SEEG data from 61 patients acquired across two centres 
and three cohorts capturing tens of thousands of abnormal discharges and around 
ten seizures per person on average. SEEGformer employs a parallel transformer 
architecture to analyse multiple representations of multichannel SEEG signals, 
including the real part, imaginary part, and amplitude of the analytic signal 
after Fourier transform. The MRI information was encoded in SEEGformer to 
construct the structural dependence of the brain areas. Inter-channel 
dependencies and interactions were captured for seizure detection. A 
cross-channel attention mechanism computed the epileptogenic risk score for each 
channel to localise EZ using ictal SEEG data. Each patient's SEEG data was used 
to train and validate their individual-specific SEEGformer model.
FINDINGS: In three clinical cohorts, SEEGformer achieved an average AUROC of 
0.937 (95% CI, 0.922-0.950) for seizure detection and 0.798 (95% CI, 
0.749-0.847) for EZ localisation. Localisation performance surpassed 
state-of-the-art methods by over 5%. SEEGformer further revealed distinct phase 
synchronisation patterns in dynamically evolving epileptogenic zone networks, 
with a significance level of P < 0.0001.
INTERPRETATION: Due to its high interpretability and visualisation capabilities, 
SEEGformer can enhance clinical decision-making by providing an objective, 
data-driven reference to optimise epileptogenic zone delineation and surgical 
strategy development. Currently, the improved SEEGformer is being developed to 
construct a dedicated SEEG atlas for epilepsy.
FUNDING: This study was funded by Natural Science Foundation of Shanghai 
(25ZR1401179), the National Key Research and Development Program of China 
(2022YFB4702702), the Sci-Tech Innovation 2030-Major Project of Brain Science 
and Brain-inspired Intelligence Technology (2021ZD0200600), Beijing Municipal 
Public Welfare Development and Reform Pilot Project for Medical Research 
Institutes (JYY2023-8), and National Key Research and Development Program of 
China (2024YFC3044700).

Copyright © 2025 The Author(s). Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ebiom.2025.106085
PMCID: PMC12774776
PMID: 41411774 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of interests The authors have no 
conflicts of interest to disclose.


58. JMIR Med Inform. 2026 Jan 13;14:e77699. doi: 10.2196/77699.

Large Language Models for Psychiatric Diagnosis Based on Multicenter Real-World 
Clinical Records: Comparative Study.

Sun M(1)(2), Yu J(3), Long Z(2), Yang Y(2), Xiao T(4), Liang J(5), Feng J(6), 
Deng H(7), Huang G(2).

Author information:
(1)Mental Health Center, University-Town Hospital of Chongqing Medical 
University, Chongqing, China.
(2)Sichuan Mental Health Center, The Third Hospital of Mianyang, Mianyang, 
China.
(3)Third People's Hospital of Panzhihua, Panzhihua, China.
(4)The Third People's Hospital of Ganzhou, Ganzhou, China.
(5)The Third People's Hospital of Foshan, Foshan, China.
(6)Lanzhou Third People's Hospital, Lanzhou, China.
(7)Shanxi Provincial Mental Health Center, Taiyuan, China.

BACKGROUND: Psychiatric disorders are diagnostically challenging and often rely 
on subjective clinical judgment, particularly in resource-limited settings. 
Large language models (LLMs) have demonstrated potential in supporting 
psychiatric diagnosis; however, robust evidence from large-scale, real-world 
clinical data remains limited.
OBJECTIVE: This study aimed to evaluate and compare the diagnostic performance 
of multiple LLMs for psychiatric disorders using multicenter real-world 
electronic health records (EHRs).
METHODS: We retrospectively analyzed 9923 inpatient EHRs collected from 6 
psychiatric centers across China, encompassing all ICD-10 (International 
Statistical Classification of Diseases, Tenth Revision) psychiatric categories. 
In total, 3 LLMs-GPT-4.0 (OpenAI), GPT-3.5 (OpenAI), and GLM-4-Plus (Zhipu 
AI)-were evaluated against physician-confirmed discharge diagnoses. Diagnostic 
performance was assessed using strict accuracy criteria and lenient 
classification metrics, with subgroup analyses conducted across diagnostic 
categories and age groups.
RESULTS: GPT-4.0 achieved the highest overall strict diagnostic accuracy (71.7%) 
and the highest weighted F1-score under lenient evaluation (0.881), particularly 
for high-prevalence disorders, such as mood disorders and schizophrenia spectrum 
disorders. Diagnostic performance varied across age groups, with the highest 
accuracy observed in older adult patients (up to 79.5%) and lower accuracy in 
adolescents. Across centers, model performance remained stable, with no 
significant intercenter differences.
CONCLUSIONS: LLMs-especially GPT-4.0-demonstrate promising capability in 
supporting psychiatric diagnosis using real-world EHRs. However, diagnostic 
performance varies by age group and disorder category. LLMs should be regarded 
as assistive tools rather than replacements for clinical judgment, and further 
validation is needed before routine clinical implementation.

©Maoqian Sun, Jia Yu, Zhuhong Long, Yun Yang, Tao Xiao, Jiaquan Liang, Jun Feng, 
Huaili Deng, Guoping Huang. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org), 13.01.2026.

DOI: 10.2196/77699
PMID: 41408781 [Indexed for MEDLINE]


59. Comput Biol Chem. 2026 Apr;121:108842. doi: 10.1016/j.compbiolchem.2025.108842. 
Epub 2025 Dec 12.

DLS-SUC: A precision prediction framework for lysine succinylation sites 
integrating the protein language model (ESM-2) and dual imbalance strategies.

Zhang H(1), Jia J(2), Huang H(1).

Author information:
(1)School of Information Engineering, Jingdezhen Ceramic University, Jingdezhen, 
Jiangxi 333403, China.
(2)School of Information Engineering, Jingdezhen Ceramic University, Jingdezhen, 
Jiangxi 333403, China. Electronic address: jjh163yx@163.com.

Lysine succinylation (Ksucc), a negatively charged post-translational 
modification, plays a crucial role in regulating protein function, cellular 
signaling, and disease pathogenesis, including cancer, neurodegenerative 
disorders, and metabolic syndromes. Accurate identification of Ksucc sites is 
therefore vital for elucidating their molecular mechanisms and guiding novel 
diagnostic and therapeutic developments. However, current deep learning models 
still face challenges in achieving high predictive accuracy and 
generalizability. To address these issues, we propose DLS-SUC, a novel deep 
learning framework for precise Ksucc site prediction. DLS-SUC integrates One-hot 
encoding with ESM-2 pretrained protein language model features and combines 
Dense Convolutional Network (DenseNet) and Bidirectional Long Short-Term Memory 
(BiLSTM) architectures to capture both local sequence motifs and long-range 
dependencies. Furthermore, a Squeeze-and-Excitation Network (SENet) attention 
mechanism is incorporated to adaptively recalibrate feature channel importance, 
enhancing feature discrimination. To mitigate class imbalance, DLS-SUC employs a 
dual "algorithm-system" strategy, utilizing a weighted cross-entropy loss 
function to improve learning on the minority class and a cross-validation-based 
homogeneous ensemble to reduce variance and bolster model stability. The 
experimental results demonstrate that on an independent test set, DLS-SUC 
achieved an Sn of 75.96 %, Sp of 77.66 %, BAcc of 76.81 %, MCC of 44.9 %, and 
AUC of 84.73 %. The model outperforms current state-of-the-art methods, 
achieving a 7.14 percentage point gain in MCC. In conclusion, DLS-SUC stands as 
a reliable computational tool for the high-precision prediction of Ksucc sites, 
offering a new perspective for in-depth investigations of their biological 
mechanisms and roles in associated diseases.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.compbiolchem.2025.108842
PMID: 41406781 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


60. Med Image Anal. 2025 Dec 11;109:103907. doi: 10.1016/j.media.2025.103907. Online 
ahead of print.

CIA-net: Cross-modality interaction and aggregation network for ovarian tumor 
segmentation from multi-modal MRI.

Gao Y(1), Li Y(2), Gao X(3).

Author information:
(1)Medical School of Tianjin University, Tianjin, China; Shanghai Innovation 
Institute, Shanghai, China; School of Biomedical Engineering (Suzhou), Division 
of Life Sciences and Medicine, University of Science and Technology of China, 
Hefei, China.
(2)Department of Radiology, Changzhi People's Hospital Affiliated to Changzhi 
Medical College, Changzhi, China. Electronic address: pingyaoaiai@163.com.
(3)Medical School of Tianjin University, Tianjin, China; Suzhou Institute of 
Biomedical Engineering and Technology, Chinese Academy of Sciences, Suzhou, 
China; Jinan Guoke Medical and Technology Development Co., Ltd., Pharmaceutical 
Valley New Drug Creation Platform, Jinan, China. Electronic address: 
xingaosam@163.com.

Magnetic resonance imaging (MRI) is an essential examination for ovarian cancer, 
in which ovarian tumor segmentation is crucial for personalized diagnosis and 
treatment planning. However, ovarian tumors often present with mixed cystic and 
solid regions in imaging, posing additional difficulties for automatic 
segmentation. In clinical practice, radiologists use T2-weighted imaging as the 
main modality to delineate tumor boundaries. In comparison, multi-modal MRI 
provides complementary information across modalities that can improve tumor 
segmentation. Therefore, it is important to fuse salient features from other 
modalities to the main modality. In this paper, we propose a cross-modality 
interaction and aggregation network (CIA-Net), a hybrid convolutional and 
Transformer architecture, for automatic ovarian tumor segmentation from 
multi-modal MRI. CIA-Net divides multi-modal MRI into one main (T2) and three 
minor modalities (T1, ADC, DWI), each with independent encoders. The novel 
cross-modality collaboration block selectively aggregates complementary features 
from minor modalities into the main modality through a progressive context 
injection module. Additionally, we introduce the progressive neighborhood 
integrated module to filter intra- and inter-modality noise and redundancies by 
refining adjacent slices of each modality. We evaluate our proposed method on a 
diverse, multi-center ovarian tumor dataset comprising 739 patients, and further 
validate its generalization and robustness on two public benchmarks for brain 
and cardiac segmentation. Comparative experiments with other cutting-edge 
techniques demonstrate the effectiveness of CIA-Net, highlighting its potential 
to be applied in clinical scenarios.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.media.2025.103907
PMID: 41406671

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


61. Infant Behav Dev. 2025 Dec 16;82:102175. doi: 10.1016/j.infbeh.2025.102175. 
Online ahead of print.

Applying a Transformer-based machine-learning model to classify caregiver and 
infant behaviours during dyadic interactions.

Turner A(1), Magassouba A(2), Wijeakumar S(3).

Author information:
(1)School of Computer Science, University of Nottingham, Nottingham, United 
Kingdom. Electronic address: alexander.turner@nottingham.ac.uk.
(2)School of Computer Science, University of Nottingham, Nottingham, United 
Kingdom.
(3)School of Psychology, University of Nottingham, Nottingham, United Kingdom. 
Electronic address: sobanawartiny.wijeakumar@nottingham.ac.uk.

Multimodal caregiver-infant interactions have both concurrent and long-term 
impacts on child attention, cognitive and social skills. These behaviours are 
typically manually coded by human researchers, making this approach susceptible 
to observer bias, dependent on inter-rater reliability, and substantial demands 
on time and resources. In this study, we aimed to develop a multimodal 
machine-learning model that could be capable of automatically detecting and 
classifying multimodal behaviours from video recordings of caregivers and their 
infants (N = 81; infant mean age = 251.3 ± 34.9 days) engaging with objects. We 
focused on caregiver scaffolding, caregiver intrusiveness, infant object 
engagement and infant distractibility. Low-level features from audio, video, and 
pose data were extracted using specific AI models, and input into a 
Transformer-based architecture capable of learning temporal patterns across 
modalities. Our findings revealed a significant contrast in model performance 
depending on how the data was partitioned. Following previous research, when the 
dataset was split such that data from all dyads contributed to the training, 
validation, and test sets - the models achieved notably high classification 
accuracy of over 98 %. However, when tested on entirely unseen dyads, the 
performance dropped markedly to around 55 %. These results suggest that the 
models did not learn behaviors of interest but instead relied on video-specific 
or dyad-specific details - underscoring key generalizability challenges in 
applying Transformer-based models to complex, multimodal behavioral data. 
Nonetheless, this work lays a foundation for future research aimed at refining 
these models and extending their applicability across diverse caregiving 
contexts.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.infbeh.2025.102175
PMID: 41406620

Conflict of interest statement: Declaration of Competing Interest No potential 
conflict of interest is reported by the authors.


62. Nat Commun. 2025 Dec 16;16(1):10882. doi: 10.1038/s41467-025-66218-5.

Integrating a host biomarker with a large language model for diagnosis of lower 
respiratory tract infection.

Phan HV(#)(1), Spottiswoode N(#)(2), Lydon EC(1), Chu VT(3)(4), Cuesta A(1), 
Kazberouk AD(5), Richmond NL(1), Deosthale P(1), Calfee CS(6), Langelier 
CR(1)(4).

Author information:
(1)Department of Medicine, Division of Infectious Diseases, University of 
California San Francisco, San Francisco, CA, USA.
(2)Department of Medicine, Division of Infectious Diseases, University of 
California San Francisco, San Francisco, CA, USA. natasha.spottiswoode@ucsf.edu.
(3)Department of Pediatrics, Division of Infectious Diseases and Global Health, 
University of California San Francisco, San Francisco, CA, USA.
(4)Chan Zuckerberg Biohub San Francisco, San Francisco, CA, USA.
(5)Department of Medicine, University of California San Francisco, San 
Francisco, CA, USA.
(6)Department of Medicine, Division of Pulmonary, Critical Care, Allergy and 
Sleep Medicine, University of California San Francisco, San Francisco, CA, USA.
(#)Contributed equally

Update of
    medRxiv. 2025 Apr 03:2024.08.28.24312732. doi: 10.1101/2024.08.28.24312732.

Lower respiratory tract infections (LRTI) are a leading cause of mortality and 
are challenging to diagnose in critically ill patients, as non-infectious causes 
of respiratory failure can present with similar clinical features. We develop an 
LRTI diagnostic method combining the pulmonary transcriptomic biomarker FABP4 
with electronic medical record text assessment using the large language model 
Generative Pre-trained Transformer 4. In a cohort of critically ill adults, a 
combined classifier incorporating FABP4 expression and large language model 
electronic medical record analysis achieves an area under the receiver operating 
characteristic curve (AUC) of 0.93 ± 0.08 and an accuracy of 84%, outperforming 
FABP4 expression alone (0.84 ± 0.11) and large language model-based analysis 
alone (0.83 ± 0.07). By comparison, the medical team admission diagnosis has an 
accuracy of 72%. In an independent validation cohort, the combined classifier 
yields an AUC of 0.98 ± 0.04 and accuracy of 96%. This study suggests that 
integrating a host biomarker with large language model analysis can improve LRTI 
diagnosis in critically ill adults.

© 2025. The Author(s).

DOI: 10.1038/s41467-025-66218-5
PMCID: PMC12708615
PMID: 41402257 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: No authors report financial 
or personal conflicts of interest.


63. Brain Dev. 2025 Dec 10;48(1):104492. doi: 10.1016/j.braindev.2025.104492. Online 
ahead of print.

Construction and clinical validation of a fetal brain magnetic resonance 
imaging-prediction model based on multimodal AI fusion algorithm.

Liu B(1), Zhang F(2), Guo J(2), Lu W(2), Zhu Z(2), Liu Y(2), Yin C(2).

Author information:
(1)Department of Radiology, Shenzhen Maternity and Child Healthcare Hospital, 
Women and Children's Medical Center, Southern Medical University, Shenzhen, 
Guangdong Province, China. Electronic address: 18923835825@163.com.
(2)Department of Radiology, Shenzhen Maternity and Child Healthcare Hospital, 
Women and Children's Medical Center, Southern Medical University, Shenzhen, 
Guangdong Province, China.

OBJECTIVE: To develop a multimodal artificial intelligence (AI) fusion model 
predicting abnormal fetal brain development from magnetic resonance imaging 
(MRI).
METHODS: Using fetal brain MRI data and clinical indicators from pregnant women 
(from January 2021 to December 2023), who were split 7:3 into training and 
validation sets. In the training set, key predictors were identified via 
univariate analysis and multivariate logistic regression, including both 
clinical indicators and continuous MRI biometric parameters. Three multimodal AI 
fusion models，including Convolutional Neural Network-Recurrent Neural Network 
(CNN-RNN) model, attention mechanism-based model, and feature concatenation 
model were developed. Performance was assessed by accuracy, precision, recall, 
F1-score, and the area under the receiver operating characteristic curve (AUC).
RESULTS: Among the total 806 participants, 108 cases (19.15 %) had fetal brain 
abnormalities in the training set (n = 564), 45 cases (18.59 %) in the 
validation set (n = 242). Multivariate logistic regression analysis showed that 
gestational age, gestational diabetes mellitus, alpha-fetoprotein, lateral 
ventricular width, and sulcation development score were independent risk factors 
for fetal brain abnormalities. The attention mechanism fusion model achieved the 
highest AUC in both the training set (0.876) and the validation set (0.869), 
significantly outperforming the CNN-RNN fusion model (AUC in training set: 
0.776; AUC in validation set: 0.718) and the feature concatenation fusion model 
(AUC in training set: 0.754; AUC in validation set: 0.720).
CONCLUSION: The multimodal AI fusion model, particularly using attention 
mechanisms, effectively identifies high-risk fetal brain abnormalities, offering 
potential for early clinical intervention and improved prenatal counseling to 
enhance detection and prognosis of neurological disorders.

Copyright © 2024. Published by Elsevier B.V.

DOI: 10.1016/j.braindev.2025.104492
PMID: 41380657

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


64. JMIR AI. 2025 Dec 11;4:e79868. doi: 10.2196/79868.

Observer-Independent Assessment of Content Overlap in Mental Health 
Questionnaires: Large Language Model-Based Study.

Böke A(1), Hacker H(1), Chakraborty M(1), Baumeister-Lingens L(2), Vöckel J(2), 
Koenig J(2), Vogel DH(3), Lichtenstein TK(1), Vogeley K(1)(4), 
Kambeitz-Ilankovic L(1), Kambeitz J(1).

Author information:
(1)Department of Psychiatry and Psychotherapy, Faculty of Medicine and 
University Hospital of Cologne, University of Cologne, Kerpener Str. 62, 
Cologne, 50931, Germany, 49 22147887150.
(2)Department of Child and Adolescent Psychiatry, Psychosomatics and 
Psychotherapy, Faculty of Medicine and University Hospital Cologne, University 
of Cologne, Cologne, Germany.
(3)Department of Psychiatry and Psychotherapy, University Hospital Bonn, Bonn, 
Germany.
(4)Cognitive Neuroscience (INM-3), Institute of Neuroscience and Medicine, 
Forschungszentrum Jülich, Jülich, Germany.

BACKGROUND: Mental disorders are frequently evaluated using questionnaires, 
which have been developed over the past decades for the assessment of different 
conditions. Despite the rigorous validation of these tools, high levels of 
content divergence have been reported for questionnaires measuring the same 
construct of psychopathology. Previous studies that examined the content overlap 
required manual symptom labeling, which is observer-dependent and 
time-consuming.
OBJECTIVE: In this study, we used large language models (LLMs) to analyze 
content overlap of mental health questionnaires in an observer-independent way 
and compare our results with clinical expertise.
METHODS: We analyzed questionnaires from a range of mental health conditions, 
including adult depression (n=7), childhood depression (n=15), clinical high 
risk for psychosis (CHR-P; n=11), mania (n=7), obsessive-compulsive disorder 
(n=7), and sleep disorder (n=12). Two different LLM-based approaches were 
tested. First, we used sentence Bidirectional Encoder Representations from 
Transformers (sBERT) to derive numerical representations (embeddings) for each 
questionnaire item, which were then clustered using k-means to group 
semantically similar symptoms. Second, questionnaire items were prompted to a 
Generative Pretrained Transformer to identify underlying symptom clusters. 
Clustering results were compared to a manual categorization by experts using the 
adjusted rand index. Further, we assessed the content overlap within each 
diagnostic domain based on LLM-derived clusters.
RESULTS: We observed varying degrees of similarity between expert-based and 
LLM-based clustering across diagnostic domains. Overall, agreement between 
experts was higher than between experts and LLMs. Among the 2 LLM approaches, 
GPT showed greater alignment with expert ratings than sBERT, ranging from weak 
to strong similarity depending on the diagnostic domain. Using GPT-based 
clustering of questionnaire items to assess the content overlap within each 
diagnostic domain revealed a weak (CHR-P: 0.344) to moderate (adult depression: 
0.574; childhood depression: 0.433; mania: 0.419; obsessive-compulsive disorder 
[OCD]: 0.450; sleep disorder: 0.445) content overlap of questionnaires. Compared 
to the studies that manually investigated content overlap among these scales, 
the results of this study exhibited variations, though these were not 
substantial.
CONCLUSIONS: These findings demonstrate the feasibility of using LLMs to 
objectively assess content overlap in diagnostic questionnaires. Notably, the 
GPT-based approach showed particular promise in aligning with expert-derived 
symptom structures.

© Annkathrin Böke, Hannah Hacker, Millennia Chakraborty, Luise 
Baumeister-Lingens, Jasper Vöckel, Julian Koenig, David HV Vogel, Theresa 
Katharina Lichtenstein, Kai Vogeley, Lana Kambeitz-Ilankovic, Joseph Kambeitz. 
Originally published in JMIR AI (https://ai.jmir.org).

DOI: 10.2196/79868
PMCID: PMC12697914
PMID: 41380022

Conflict of interest statement: Conflicts of Interest: None declared.


65. IEEE J Biomed Health Inform. 2025 Dec 11;PP. doi: 10.1109/JBHI.2025.3631841. 
Online ahead of print.

An LLM Method for Understanding Traditional Chinese Medicine: Mechanism 
Exploration and Innovative Application.

Li YX, Elnaffar S, Chen HY, Chen NJ, Lai PY, Li NQ, Chong Y, Qiao J, Liu T, Peng 
ZB, Xu DY, Lai JH, Wang CD, Hu JQ.

Large language models (LLMs) show promise in medical knowledge representation 
but struggle with dynamic clinical workflows and personalized treatment in 
complex systems like Traditional Chinese Medicine (TCM). We propose an efficient 
and novel LLM framework for TCM mechanism exploration and clinical application, 
combining incremental domain-specific pre-training, multi-task supervised 
fine-tuning, and Chain-of-Thought (CoT) reasoning. Our two-stage 
approach-"Understanding and Inheritance" followed by "Exploration and 
Innovation"-uniquely leverages a heterogeneous database of 100,538 records from 
19 TCM physicians to model the core TCM principle of "different treatments for 
the same disease". Six downstream tasks assess clinical capabilities, including 
personalized prescription generation (Task 3). After incremental pre-training, 
the model improves BLEU-4 by 1,313% over baseline, reaching 41.26-43.21 after 
fine-tuning. We quantify physician-specific variations and formally validate the 
decisive role of basic formulas-removing them causes a 23.9% performance drop. 
Cross-school evaluations confirm robust generalization, with 22.8 BLEU-4 on 
external data. CoT annotation boosts performance by 20% using only 10% labeled 
data, demonstrating high data efficiency. The model captures TCM's "different 
treatments for the same disease" principle and preserves school-specific 
diagnostic logic. This work advances intelligent TCM inheritance and paves the 
way for AI-driven personalized medicine.

DOI: 10.1109/JBHI.2025.3631841
PMID: 41379900


66. J Affect Disord. 2026 Mar 15;397:120873. doi: 10.1016/j.jad.2025.120873. Epub 
2025 Dec 9.

Inconsistent outcome measurement in depression psychotherapy trials: A 
systematic historical and meta-analytic review over the past 50 years.

Sprenger AA(1), Harrer M(2), Miguel C(3), Illing S(4), Kuper P(5), Buntrock 
C(6), Karyotaki E(7), Fried EI(8), Cuijpers P(9), Apfelbacher C(10).

Author information:
(1)Institute of Social Medicine and Health Systems Research, Medical Faculty, 
Otto von Guericke University Magdeburg, Leipziger Str. 44, 39120, Magdeburg, 
Germany. Electronic address: antonia.sprenger@med.ovgu.de.
(2)Department of Clinical, Neuro and Developmental Psychology, Amsterdam Public 
Health Research Institute, Vrije Universiteit, Van der Boecherstraat 7, 1081 BT, 
Amsterdam, the Netherlands. Electronic address: m.harrer@vu.nl.
(3)Department of Clinical, Neuro and Developmental Psychology, Amsterdam Public 
Health Research Institute, Vrije Universiteit, Van der Boecherstraat 7, 1081 BT, 
Amsterdam, the Netherlands. Electronic address: clara.miguelsanz@vu.nl.
(4)Section for Evidence-Based Medicine in Psychiatry and Psychotherapy, 
Department of Psychiatry and Psychotherapy, School of Medicine and Health, 
Technical University Munich, Ismaninger Straße 22, 81675, Munich, Germany. 
Electronic address: susan.illing@mri.tum.de.
(5)Institute of Social Medicine and Health Systems Research, Medical Faculty, 
Otto von Guericke University Magdeburg, Leipziger Str. 44, 39120, Magdeburg, 
Germany. Electronic address: paula.kuper@med.ovgu.de.
(6)Institute of Social Medicine and Health Systems Research, Medical Faculty, 
Otto von Guericke University Magdeburg, Leipziger Str. 44, 39120, Magdeburg, 
Germany. Electronic address: claudia.buntrock@med.ovgu.de.
(7)Programme group Clinical Psychology, Faculty of Social and Behavioural 
Sciences, University of Amsterdam, Postbus 15933, 1001 NK, Amsterdam, the 
Netherlands. Electronic address: e.karyotaki@vu.nl.
(8)Department of Clinical Psychology, Leiden University, PO Box 9500, 2300 RA, 
Leiden, the Netherlands. Electronic address: eikofried@gmail.com.
(9)Department of Clinical, Neuro and Developmental Psychology, Amsterdam Public 
Health Research Institute, Vrije Universiteit, Van der Boecherstraat 7, 1081 BT, 
Amsterdam, the Netherlands. Electronic address: p.cuijpers@vu.nl.
(10)Institute of Social Medicine and Health Systems Research, Medical Faculty, 
Otto von Guericke University Magdeburg, Leipziger Str. 44, 39120, Magdeburg, 
Germany. Electronic address: christian.apfelbacher@med.ovgu.de.

BACKGROUND: Psychotherapy studies employ different outcome measurement 
instruments (OMIs) for depression, which may limit the interpretability of 
meta-analytic findings. This meta-epidemiological study aimed to examine trends 
in OMI use in randomised controlled trials (RCTs) for depression, consistency of 
measurement practices over time, and their influence on meta-analytic effect 
estimates.
METHODS: A systematic literature search was conducted in four databases up to 
September 01, 2024. We included RCTs comparing psychotherapy to control 
conditions in adults with depression using OMIs for depression, present in at 
least ten comparisons. Change over time in OMI usage frequencies and 
heterogeneity (perplexity) was analysed with Poisson and linear regression 
models. Confounder-adjusted meta-regression and network meta-analysis examined 
the dependence of treatment effect estimates on different OMIs.
RESULTS: We included 492 trials (1260 comparisons in ∼51,000 patients), 
published between 1977 and 2024, using 17 different OMIs for depression. The 
proportion of trials using the Patient Health Questionnaire (PHQ-9; b = 1.04, 
SE = 0.30, p < 0.001) and Hamilton Rating Scale for Depression (HDRS; b = 0.29, 
SE = 0.12, p = 0.021), but also heterogeneity in OMI use (b = 0.12, SE = 0.01, 
p < 0.001), increased significantly over time. Particularly the Centre for 
Epidemiological Studies Depression Scale (CES-D) and Geriatric Depression Scale 
(GDS) predicted increased and decreased treatment effect estimates.
CONCLUSIONS: Despite increases in PHQ-9 and HRSD administration, OMI 
heterogeneity in depression psychotherapy research has grown over the past 
50 years. Since different OMIs, especially CES-D and GDS, affect treatment 
effect estimates, meta-analyses incorporating various OMIs for depression should 
be interpreted cautiously. Core outcome sets for depression could improve 
robustness of meta-analytic evidence.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.jad.2025.120873
PMID: 41371355 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: Christian Apfelbacher reports a 
relationship with Dr Wolff Group that includes: consulting or advisory and 
funding grants. Christian Apfelbacher reports a relationship with Bionorica SE 
that includes: consulting or advisory and funding grants. Christian Apfelbacher 
reports a relationship with Sanofi that includes: consulting or advisory. 
Christian Apfelbacher reports a relationship with Incyte Biosciences Germany 
GmbH that includes: consulting or advisory. Christian Apfelbacher reports a 
relationship with Rheacell GmbH & Co.KG that includes: consulting or advisory. 
Christian Apfelbacher reports a relationship with Pfizer that includes: 
consulting or advisory. Christian Apfelbacher reports a relationship with 
European Foundation for the Care of Newborn Infants that includes: consulting or 
advisory. Christian Apfelbacher reports a relationship with LEO Pharma that 
includes: consulting or advisory. If there are other authors, they declare that 
they have no known competing financial interests or personal relationships that 
could have appeared to influence the work reported in this paper.


67. Transl Pediatr. 2025 Nov 30;14(11):3020-3028. doi: 10.21037/tp-2025-602. Epub 
2025 Nov 25.

Evaluating ChatGPT's responses to vaccine-related questions: the impact of 
question framing on content and quality.

Kusama Y(1)(2), Kutsuna S(1)(2).

Author information:
(1)Department of Infectious Diseases, University of Osaka Hospital, Osaka, 
Japan.
(2)Human Resource Development Division, Center for Infectious Disease Education 
and Research, The University of Osaka, Osaka, Japan.

BACKGROUND: Vaccine hesitancy, fueled by mistrust, fear, and misinformation, 
remains a major public health challenge. Generative artificial intelligence 
tools such as ChatGPT have emerged as new information sources, particularly for 
younger users. It is essential that these tools provide medically accurate 
content, especially when responding to negatively framed vaccine questions. This 
study aims to examine how ChatGPT responds to vaccine-related questions, 
focusing on questions with negative or skeptical framing.
METHODS: This anonymous survey targeted board-certified pediatric infectious 
disease specialists in Japan to evaluate ChatGPT's responses to vaccine-related 
questions. Using ChatGPT-4o, 20 question pairs (supportive vs. critical framing) 
were generated, and responses in Japanese were obtained. Participants were 
randomly assigned to assess half of the responses via Google Forms, rating 
Clarity, Appropriateness, Ambiguity, and Length on a five-point Likert scale, 
with optional free-text comments. Quantitative comparisons between supportive 
and critical questioning used the Mann-Whitney U test; qualitative feedback was 
thematically analyzed.
RESULTS: Twenty of 22 invited specialists (90.9%) completed the survey. Median 
[25th-75th percentile] scores for supportive vs. critical questioning were: 
Clarity, 4 [3-4] vs. 4 [3-4]; Appropriateness, 3 [3-4] vs. 3 [3-4]; Ambiguity, 4 
[3-4] vs. 4 [3-4]; Length, 3 [3-4] vs. 3 [3-4]. No significant differences were 
found for any item. Among 81 free-text comments, the most frequent concerns were 
"bias toward COVID-19 vaccines" (n=38), "insufficient explanation" (n=19), and 
"potentially misleading expressions" (n=9). Examples included overemphasis on 
COVID-19 in unrelated contexts and problematic phrasing regarding human 
papillomavirus vaccine adverse events.
CONCLUSIONS: ChatGPT maintained comparable quality in Japanese responses to both 
supportive and critical vaccine questions, suggesting resilience to negative 
framing. However, expert reviewers identified thematic biases, occasional 
inadequacy of detail, and linguistic issues that could mislead lay readers. 
These findings underscore the need for continued human oversight, refinement of 
Japanese-language outputs, and algorithmic adjustments to reduce topical bias.

© AME Publishing Company.

DOI: 10.21037/tp-2025-602
PMCID: PMC12683428
PMID: 41367523

Conflict of interest statement: Conflicts of Interest: Both authors have 
completed the ICMJE uniform disclosure form (available at 
https://tp.amegroups.com/article/view/10.21037/tp-2025-602/coif). The authors 
have no conflicts of interest to declare.


68. J Med Internet Res. 2025 Dec 5;27:e73535. doi: 10.2196/73535.

Using Generative AI to Co-Design Digital Mental Health Interventions With 
Adolescents in Rural South Africa: Qualitative Thematic Analysis of 
Participatory Workshops.

Dallison S(1), Moffett B(2), Makhubela P(2), Nkuna T(2), Pozuelo JR(2)(3)(4), 
van Heerden A(5), O'Mahen H(6).

Author information:
(1)Department of Psychology, University of Bath, 10 West, Claverton Down, Bath, 
Somerset, BA2 7AY, United Kingdom, 4401392661000.
(2)MRC/Wits Rural Public Health and Health Transitions Research Unit 
(Agincourt), School of Public Health, Faculty of Health Sciences, University of 
the Witwatersrand, Johannesburg, South Africa.
(3)Department of Global Health and Social Medicine, Harvard Medical School, 
Harvard University, Boston, MA, United States.
(4)Department of Psychiatry, University of Oxford, Oxford, United Kingdom.
(5)Faculty of Health Sciences, University of the Witwatersrand, Johannesburg, 
South Africa.
(6)Mood Disorders Centre, Department of Psychology, University of Exeter, 
Exeter, United Kingdom.

BACKGROUND: Digital mental health interventions (DMHIs) offer a scalable 
approach to address adolescent depression and anxiety. User-centered 
coproduction can optimize acceptability and engagement, but it is often 
resource-intensive. Advances in generative artificial intelligence (GenAI) 
create new opportunities for involving adolescents in co-design, yet research on 
its feasibility and acceptability, particularly in low-resource settings, 
remains underexplored.
OBJECTIVE: This study aimed to explore adolescents' experiences and perspectives 
of using GenAI to co-design stories, images, and music for the Kuamsha app (Sea 
Monster), a gamified DMHI that teaches behavioral activation through interactive 
narratives and peer support.
METHODS: Overall, 2 participatory workshops and focus group discussions were 
conducted with 23 adolescents (aged 15-19 years) in rural Mpumalanga, South 
Africa. Participants were guided to use 3 GenAI tools-ChatGPT (OpenAI), 
text-to-story; MidJourney (MidJourney Inc), text-to-image; and Soundful 
(Soundful Inc), music generation-to create digital content. Data were 
audio-recorded, translated, transcribed, and triangulated with the facilitator's 
observation notes. Thematic analysis was used to explore key themes.
RESULTS: Almost all participants (22/23, 96%) had no prior exposure to GenAI. 
The majority (20/23, 87%) described the creative process as enjoyable and 
engaging, with most (21/23, 91%) reporting that creating music improved their 
mood. Adolescents expressed autonomy and ownership of the process, with more 
than half (14/23, 61%) personalizing outputs to reflect their identities and 
aspirations. All participants (23/23, 100%) preferred artificial intelligence 
(AI)-generated images over the cartoon-like illustrations of the Kuamsha app, 
and most (19/23, 83%) preferred AI-generated music. Story preferences were more 
mixed, with about a quarter of participants (6/23, 26%) recalling that Kuamsha's 
narratives contained embedded lessons that were not integrated into the ChatGPT 
outputs. Most adolescents (18/23, 78%) required support with prompt 
construction, and more than half (13/23, 57%) noted cultural biases in AI 
outputs, particularly in images. Most participants (17/23, 74%) expressed 
interest in using AI for schoolwork and creative projects, while a minority 
(6/23, 26%) preferred to limit use to personal applications. Concerns about 
fairness and the displacement of human creativity were also raised.
CONCLUSIONS: GenAI shows promise for enhancing adolescent engagement in the 
coproduction of DMHIs and enabling culturally relevant and personalized content. 
However, reliance on human support and persistent algorithmic biases remain 
limitations. Further research should explore the integration of therapeutic 
principles into AI-generated media and strategies to mitigate bias.

© Sophie Dallison, Bianca Moffett, Princess Makhubela, Tamera Nkuna, Julia R 
Pozuelo, Alastair van Heerden, Heather O'Mahen. Originally published in the 
Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/73535
PMCID: PMC12680127
PMID: 41348954 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


69. BMC Psychol. 2025 Dec 5;14(1):25. doi: 10.1186/s40359-025-03729-2.

Exploring AI-assisted design of executive function rehabilitation programs for 
individuals with ADHD: a mixed-methods evaluation of prompts and chatgpt 
outputs.

Dahò M(1)(2), Caci B(3)(4).

Author information:
(1)Department of Psychology, Educational Science and Human Movement, University 
of Palermo, Viale delle Scienze, Ed.15, Palermo, 90146, Italy. 
Margherita.daho@unipa.it.
(2)WeSearch Lab - Laboratory of Behavioral Observation and Research on Human 
Development, University of Palermo, Palermo, Italy. Margherita.daho@unipa.it.
(3)Department of Psychology, Educational Science and Human Movement, University 
of Palermo, Viale delle Scienze, Ed.15, Palermo, 90146, Italy.
(4)WeSearch Lab - Laboratory of Behavioral Observation and Research on Human 
Development, University of Palermo, Palermo, Italy.

BACKGROUND: As Artificial Intelligence (AI) tools like ChatGPT gain traction in 
clinical contexts, their role in neurorehabilitation, particularly in addressing 
executive function impairments associated with ADHD, remains underexplored. This 
study examines whether generative AI can meaningfully support clinicians in 
designing individualized cognitive rehabilitation plans, not as a replacement 
but as a complementary aid.
METHODS: The research consisted of three separate studies, each addressing 
distinct stages of the investigation. First, expert-driven prompts were 
developed based on literature and clinical insights to guide ChatGPT in 
generating rehabilitation plans for three hypothetical profiles of individuals 
with ADHD (adolescents, adults, and older adults). In Study 2, the outputs were 
analyzed using a semi-systematic qualitative framework (ISAAC), assessing 
structure, coherence, and adaptability across developmental stages. Study 3 
involved an external panel of 27 neuropsychologists and cognitive rehabilitation 
specialists (M = 6; F = 21; mean age = 46.5, SD = 15) who rated each plan's 
theoretical validity, clinical relevance, and feasibility.
RESULTS: Experts in Study 3 generally responded positively to the theoretical 
consistency of the plans, especially those for adolescents and adults, 
recognizing alignment with established models of executive function 
rehabilitation. Many professionals expressed openness to using AI as a support 
tool in practice. However, feasibility emerged as a key limitation, with 
concerns over a lack of personalization, unrealistic resource assumptions, and 
unvalidated techniques, particularly in adult and older adult profiles. These 
findings align with earlier studies in occupational therapy and clinical 
decision-making, which also identified challenges in real-world applicability.
CONCLUSION: While clinical experts express cautious optimism about AI-assisted 
rehabilitation planning, further development is necessary to enhance accuracy, 
personalization, and feasibility for the safe integration of AI into clinical 
practice.

© 2025. The Author(s).

DOI: 10.1186/s40359-025-03729-2
PMCID: PMC12784522
PMID: 41345900 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The Institutional Ethical Board of the University of Palermo 
approved the study (N° 94232 − 2025). All participants provided written informed 
consent prior to participation. Competing interests: The authors declare no 
competing interests. Consent for publication: All participants provided consent 
for the publication of anonymized data as part of the informed consent process.


70. J Am Med Inform Assoc. 2025 Dec 3:ocaf214. doi: 10.1093/jamia/ocaf214. Online 
ahead of print.

Listening to the note: clinician perspectives on ambient artificial intelligence 
scribes in medical documentation.

Van Tiem J(1)(2), Cramer E(1), Iverson C(3), Kennelty K(1)(4), Andrys N(5), Lee 
J(6), Knake L(7), Misurac J(7), Blum J(8), Reisinger HS(2)(3).

Author information:
(1)Department of Family and Community Medicine, Carver College of Medicine, 
University of Iowa, Iowa City, IA 52242, United States.
(2)Implementation Science Center, University of Iowa, Iowa City, IA 52242, 
United States.
(3)Department of Internal Medicine, Carver College of Medicine, University of 
Iowa, Iowa City, IA 52242, United States.
(4)Department of Pharmacy Practice and Science, College of Pharmacy, University 
of Iowa, Iowa City, IA 52242, United States.
(5)Carver College of Medicine, University of Iowa, Iowa City, IA 52242, United 
States.
(6)Department of Health Management and Policy, College of Public Health, 
University of Iowa, Iowa City, IA 52242, United States.
(7)Stead Family Department of Pediatrics, Carver College of Medicine, University 
of Iowa, Iowa City, IA 52242, United States.
(8)Department of Anesthesia, Carver College of Medicine, University of Iowa, 
Iowa City, IA 52242, United States.

OBJECTIVES: To qualitatively characterize barriers and facilitators to 
implementing and using an ambient scribe across a large academic medical center, 
as well as how ambient transcription reshapes clinicians' perceptions of their 
work.
MATERIALS AND METHODS: We conducted semistructured interviews with clinicians 
who participated in an ambient scribe pilot (n = 8) and the initial enterprise 
rollout (n = 16). We sought heterogeneity by specialty, note volume, burnout, 
and prior time-in-notes. Interviews (26-60 min) were recorded, transcribed, and 
analyzed thematically using a naturalistic, ethnographic approach informed by 
broad implementation considerations, and an analytic lens treating note sections 
as documentation "genres."
RESULTS: Clinicians described feeling more present with patients and greater 
satisfaction during visits. Fictions included overlong or underspecified 
sections (eg, History of Present Illness vs Assessment & Plan), unfamiliar 
formatting, and a perceived loss of "voice." Participants discussed how they 
used documentation to personalize practice, demonstrate expertise, manage 
impressions with colleagues and supervisors, and communicate sensitive 
findings-activities not fully captured by efficiency metrics. Inpatient and 
procedure-heavy contexts reported limited benefit where documentation was 
already highly standardized.
DISCUSSION: Early ambient scribe implementation produced recognizable benefits, 
but introduced new work to reconcile AI-drafted text with local documentation 
genres and audience-specific communication. Tailored prompts, onboarding, and 
peer support may reduce the need to revise artificial intelligence 
(AI)-generated text.
CONCLUSION: Ambient scribe adoption can enhance patient interactions and 
perceived efficiency while reshaping how clinicians express voice and expertise 
in notes. Implementation strategies attentive to documentation genre and 
audience may help align ambient scribe outputs with clinical communication 
needs.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf214
PMID: 41340524


71. Arch Esp Urol. 2025 Nov;78(9):1164-1170. doi: 
10.56434/j.arch.esp.urol.20257809.152.

Assessing ChatGPT on Male HPV Disorders: Repeated Assessments of Urological and 
Dermatological Accuracy Using Evidence-Based Guidelines.

Ulutas KT(1), Cilli M(2).

Author information:
(1)Department of Clinical Biochemistry, Ministry of Health, Hatay Reyhanlı State 
Hospital, 31500 Reyhanlı, Turkey.
(2)Department of Urology, Ministry of Health, Hatay Reyhanlı State Hospital, 
31500 Reyhanlı, Turkey.

BACKGROUND: Human papillomavirus (HPV) is a prevalent viral infection that poses 
considerable health risks to males, including genital warts and various cancers. 
The epidemiology of HPV in the male population requires novel approaches for 
effective patient education. This study investigated the efficacy of Chat 
Generative Pre-trained Transformer-4 (ChatGPT-4), a sophisticated artificial 
intelligence model, in disseminating accurate information about HPV-induced 
genital warts in men.
METHODS: A meticulously designed questionnaire comprising 240 items, spanning 
multiple-choice and true/false formats, was administered to ChatGPT-4 from 
October 2024 to January 2025. The questionnaire covered HPV transmission, 
clinical manifestations, diagnostic approaches, treatment options and preventive 
strategies. The responses were evaluated for accuracy and completeness using 
Likert scales, with repeated questioning sessions to assess the potential for 
response improvement over time.
RESULTS: ChatGPT-4 initially provided accurate and/or highly rated responses for 
191 out of 240 questions. Subsequent rounds of questioning for inaccurately 
answered or low-scoring questions showed a progressive improvement, with 14 and 
15 questions answered correctly or receiving higher accuracy scores in the 
second and third rounds, respectively. The overall accuracy rate was 79.60%, 
with specific subgroup analyses revealing strengths in general information and 
protection but challenges in questions about HPV-induced genital warts in men.
CONCLUSIONS: ChatGPT-4 demonstrates preliminary medical value and potential for 
patient education in urology. However, variable subgroup accuracy suggests that 
further optimisation and validation are needed before clinical deployment. Its 
ability to accurately address a wide range of queries, combined with the 
potential for an improvement in response accuracy over time, highlights its 
value in enhancing health communication.

© 2025 The Author(s).

DOI: 10.56434/j.arch.esp.urol.20257809.152
PMID: 41339223 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflict of interest.


72. Annu Int Conf IEEE Eng Med Biol Soc. 2025 Jul;2025:1-4. doi: 
10.1109/EMBC58623.2025.11253690.

Transformers Predict Hypoxia-Ischemia Timing in Term Fetal Sheep EEG in the Key 
2-Hour Window Post-Insult.

Matthews ARH, Davidson J, Purkis TT, Gunn AJ, Bennet L, Abbasi H.

Effective clinical administration of therapeutic interventions such as 
hypothermia for neonatal hypoxic-ischemic encephalopathy (HIE) depends precisely 
on determining the timing of injury at birth. This study provides the first 
preclinical evidence supporting the prediction of time elapsed since the hypoxia 
ischemia (HI) insult within the key initial 2-hour post-HI window using 
transformer-based deep learning models trained on 5-minute EEG segments. EEG 
recordings, sampled at 256 Hz, from eight term fetal sheep following carotid 
occlusion-induced HI were used to create three datasets for model training: (1) 
raw EEG, (2) spectral features, and (3) their combination. A Wav2Vec2 
transformer model was adapted for binary classification, categorizing EEG 
segments as either within or beyond the initial two-hour post-insult window. 
Leave-one-out cross-validation revealed that models trained on raw EEG data 
achieved the highest class-balanced predictive accuracy (89.49±4.17%) and F1 
score (80.82±6.54%), demonstrating that transformers can effectively extract 
temporally evolving injury-related EEG patterns. While the raw EEG model 
outperformed the other feature sets, all models captured aspects of the signal 
relevant to injury progression over time. Gaussian noise data augmentation did 
not improve predictive performance, likely due to the intrinsic variability of 
the raw EEG signal.Clinical relevance-Our preclinical findings provide evidence 
that neonatal EEG contains a latent "hidden clock" encoding HI progression, 
which, once validated on clinical recordings, could enable precise injury timing 
estimation. This advancement has the potential to refine clinical 
decision-making and optimize neuroprotective interventions for neonates with 
HIE.

DOI: 10.1109/EMBC58623.2025.11253690
PMID: 41336977 [Indexed for MEDLINE]


73. Front Physiol. 2025 Nov 17;16:1668129. doi: 10.3389/fphys.2025.1668129. 
eCollection 2025.

An EEG-based machine learning framework for diagnosing acute sleep deprivation.

Kumar D(1)(2), Narayan A(1), Lalgudi Ganesan S(3)(4)(5)(6).

Author information:
(1)Department of Computer Science, Western University, London, ON, Canada.
(2)International Center for Applied Systems Science for Sustainable Development 
(ICASSSD), Cambridge, ON, Canada.
(3)Department of Paediatrics and Clinical Neurological Sciences, Schulich School 
of Medicine and Dentistry, Western University, London, ON, Canada.
(4)Paediatric Critical Care Medicine, Children's Hospital - London Health 
Sciences Centre, London, ON, Canada.
(5)Western Institute for Neuroscience, Western University, London, ON, Canada.
(6)Children's Health Research Institute, London Health Sciences Center Research 
Institute (LHSC-RI), London, ON, Canada.

STUDY OBJECTIVE: Acute sleep deprivation significantly impacts cognitive 
function, contributes to accidents, and increases the risk of chronic illnesses, 
underscoring the need for reliable and objective diagnosis. Our work aims to 
develop a machine learning-based approach to discriminate between EEG recordings 
from acutely sleep-deprived individuals and those that are well-rested, 
facilitating the objective detection of acute sleep deprivation and enabling 
timely intervention to mitigate its adverse effects.
METHODS: Sixty-one-channel eyes-open resting-state electroencephalography (EEG) 
data from a publicly available dataset of 71 participants were analyzed. 
Following preprocessing, EEG recordings were segmented into contiguous, 
non-overlapping 20-second epochs. For each epoch, a comprehensive set of 
features was extracted, including statistical descriptors, spectral measures, 
functional connectivity indices, and graph-theoretic metrics. Four machine 
learning classifiers - Light Gradient-Boosting Machine (LightGBM), eXtreme 
Gradient Boosting (XGBoost), Random Forest (RF), and Support Vector Classifier 
(SVC) - were trained on these features using nested stratified cross-validation 
to ensure unbiased performance evaluation. In parallel, three deep learning 
models-a Convolutional Neural Network (CNN), Long Short-Term Memory network 
(LSTM), and Transformer-were trained directly on the raw multi-channel EEG 
time-series data. All models were evaluated under two conditions: (i) without 
subject-level separation, allowing the same participant to contribute to both 
training and test sets, and (ii) with subject-level separation, where models 
were tested exclusively on unseen participants. Model performance was assessed 
using accuracy, F1-score, and area under the receiver operating characteristic 
curve (AUC).
RESULTS: Without subject-level separation, CNN achieved the highest accuracy 
(95.72%), followed by XGBoost (95.42%), LightGBM (94.83%), RF (94.53%), and SVC 
(85.25%), with the Transformer (77.39%) and LSTM (66.75%) models achieving lower 
accuracies. Under subject-level separation, RF achieved the highest accuracy 
(68.23%), followed by XGBoost (66.36%), LightGBM (66.21%), CNN (65.35%), and SVC 
(65.08%), while the Transformer (63.35%) and LSTM (61.70%) models achieved the 
lowest accuracies.
CONCLUSION: This study demonstrates the potential of EEG-based machine learning 
for detecting acute sleep deprivation, while underscoring the challenges of 
achieving robust subject-level generalization. Despite reduced accuracy under 
cross-subject evaluation, these findings support the feasibility of developing 
scalable, non-invasive tools for sleep deprivation detection using EEG and 
advanced ML techniques.

Copyright © 2025 Kumar, Narayan and Lalgudi Ganesan.

DOI: 10.3389/fphys.2025.1668129
PMCID: PMC12665582
PMID: 41334558

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


74. bioRxiv [Preprint]. 2025 Nov 20:2025.11.19.689331. doi: 
10.1101/2025.11.19.689331.

Diffusion Models vs. DCGANs for Class-Imbalanced Lung Cancer CT Classification: 
A Comparative Study.

Tabibian M(1), Razmpour T(1), Saha R(1).

Author information:
(1)Department of Chemical and Biomolecular Engineering, University of 
Nebraska-Lincoln, Lincoln, Nebraska, United States of America.

Effective lung cancer detection from CT scans remains critically challenged by 
class imbalance where benign and normal cases are underrepresented, leading to 
biased machine learning models with reduced sensitivity for minority classes and 
potentially missed diagnoses in cancer screening applications. We present a 
comprehensive comparative analysis of Diffusion Models and Deep Convolutional 
Generative Adversarial Networks (DCGANs), both incorporating modern 
architectural enhancements including spectral normalization, self-attention 
mechanisms, and conditional generation, for addressing class imbalance in lung 
cancer CT classification. Using the IQ-OTH/NCCD dataset comprising 1,097 CT 
images across normal, benign, and malignant categories with statistical 
validation across 10 independent runs, we evaluated both approaches through 
quantitative image quality metrics (Fréchet Inception Distance, Kullback-Leibler 
divergence, Kernel Inception Distance, and Inception Score) and downstream 
classification performance. While Diffusion models consistently outperformed 
DCGANs across most image quality measures, the clinical significance was 
confirmed through task-based validation. Both generative approaches successfully 
addressed class imbalance: DCGAN-augmented datasets achieved overall accuracy of 
0.9760 ± 0.0116 with benign recall improvement from 0.833 to 0.933, while 
Diffusion-augmented datasets reached superior performance of 0.9959 ± 0.0068 
with perfect benign recall (1.000 ± 0.000). Critically for cancer screening 
where false negatives carry severe consequences, Diffusion maintained the 
highest malignant detection sensitivity (0.997 ± 0.008) with substantially lower 
performance variance, demonstrating more consistent synthetic data quality. 
These findings establish that while both modern architectures can mitigate class 
imbalance, Diffusion models' superior recall performance and lower variability 
position them as the preferred approach for high-stakes clinical applications, 
demonstrating that ultimate validation must prioritize downstream clinical task 
performance over image quality metrics alone.

DOI: 10.1101/2025.11.19.689331
PMCID: PMC12667879
PMID: 41332706


75. J Imaging Inform Med. 2025 Dec 2. doi: 10.1007/s10278-025-01740-y. Online ahead 
of print.

Advancing WBC Classification: A Hybrid ConvNextV2-Swin Transformer Framework 
with R3GAN Data Balancing and CLAHE Preprocessing.

Momenian M(1), Shojaedini SV(2).

Author information:
(1)Department of Computer Engineering, Faculty of Engineering, Azad University, 
E-Campus, Tehran, Iran. mohammad.momenian@iauec.ac.ir.
(2)Biomedical engineering Department, Iranian Research Organization for Science 
& Technology, , Tehran, Iran.

White blood cell (WBC) classification remains a critical challenge in 
hematological diagnostics, particularly for rare cell types such as basophils 
and imbalanced datasets. This study introduces a novel three-component hybrid 
framework that synergistically integrates: (1) ConvNeXtV2-Swin Transformer for 
dual-scale hierarchical feature extraction-combining ConvNeXtV2's depthwise 
convolutions with Swin Transformer's shifted window attention to capture both 
local cellular morphology and global contextual dependencies; (2) R3GAN 
(Reinforced Reliable Robust Generative Adversarial Network) for intelligent 
minority class augmentation through reinforcement learning-guided sample 
generation, effectively mitigating class imbalance while preserving biological 
fidelity; and (3) CLAHE (Contrast-Limited Adaptive Histogram Equalization) for 
adaptive preprocessing to normalize imaging variations. Evaluated on the 
challenging Raabin dataset-characterized by severe class imbalance (301 
basophils vs. 8887 neutrophils) and limited diversity-the proposed architecture 
achieves 99.1% accuracy, surpassing state-of-the-art methods by 2-10%. Notably, 
the framework demonstrates exceptional data efficiency, maintaining 94% accuracy 
with only 50% training data. The synergistic integration of architectural 
innovation, intelligent data synthesis, and adaptive preprocessing establishes a 
robust paradigm for clinical deployment in resource-constrained environments. 
Source code is publicly available at 
https://github.com/momenianmohammad/wbc-convnextv2swin-r3gan-eccgan .

© 2025. The Author(s) under exclusive licence to Society for Imaging Informatics 
in Medicine.

DOI: 10.1007/s10278-025-01740-y
PMID: 41331659

Conflict of interest statement: Declarations. Ethics Approval: Ethics approval 
is not applicable to this study, as it exclusively utilizes the open-access 
Raabin-WBC [1] benchmark dataset, which contains pre-collected and anonymized 
cell images with no human participants. Consent to Participate: Consent to 
participate is not required, as this research uses a publicly available dataset 
(Raabin-WBC [1]) where all samples are pre-existing and fully anonymized by the 
data providers. Consent to Publication: Consent for publication is not 
applicable. All experimental images in this manuscript were generated by the 
authors, and the Raabin-WBC [1] dataset explicitly permits unrestricted academic 
use and publication. Conflict of Interest: The authors declare no competing 
interests. Disclaimer: This material is the authors’ own original work, which 
has not been previously published elsewhere. The paper is not currently being 
considered for publication elsewhere. The paper reflects the authors’ own 
research and analysis in a truthful and complete manner. The paper properly 
credits the meaningful contributions of co-authors and co-researchers. The 
results are appropriately placed in the context of prior and existing research. 
All sources used are properly disclosed (correct citation). Literally copying of 
text must be indicated as such by using quotation marks and giving proper 
reference. All authors have been personally and actively involved in substantial 
work leading to the paper, and will take public responsibility for its content.


76. Artif Intell Med. 2026 Feb;172:103323. doi: 10.1016/j.artmed.2025.103323. Epub 
2025 Nov 29.

Geometric deep learning for local growth prediction on abdominal aortic aneurysm 
surfaces.

Alblas D(1), Rygiel P(2), Suk J(2), Kappe KO(3), Hofman M(2), Brune C(2), Yeung 
KK(4), Wolterink JM(2).

Author information:
(1)Department of Applied Mathematics, Technical Medical Centre, University of 
Twente, Drienerlolaan 5, Enschede, 7522 NB, The Netherlands. Electronic address: 
d.alblas@utwente.nl.
(2)Department of Applied Mathematics, Technical Medical Centre, University of 
Twente, Drienerlolaan 5, Enschede, 7522 NB, The Netherlands.
(3)Department of Surgery, Amsterdam University medical center, Location 
University of Amsterdam,, Meibergdreef 9, Amsterdam, 1105 AZ, The Netherlands; 
Amsterdam Cardiovascular Sciences, Atherosclerosis and Aortic Diseases, 
Amsterdam, The Netherlands.
(4)Department of Surgery, Amsterdam University medical center, Location 
University of Amsterdam,, Meibergdreef 9, Amsterdam, 1105 AZ, The Netherlands; 
Department of Surgery, Amsterdam University medical center, location Vrije 
Universiteit Amsterdam, Boelelaan 1117, Amsterdam, 1081 HV, The Netherlands; 
Amsterdam Cardiovascular Sciences, Atherosclerosis and Aortic Diseases, 
Amsterdam, The Netherlands.

Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the 
abdominal aorta. AAAs may rupture, with fatal consequences in >80% of cases. 
Current clinical guidelines recommend elective surgical repair when the maximum 
AAA diameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet 
these criteria are periodically monitored, with surveillance intervals based on 
the maximum AAA diameter. However, this diameter does not take into account the 
complex relation between the 3D AAA shape and its growth, making standardized 
intervals potentially unfit. Personalized AAA growth predictions could improve 
monitoring strategies. We propose to use an SE(3)-symmetric transformer model to 
predict AAA growth directly on the vascular model surface enriched with local, 
multi-physical features. In contrast to other works which have parameterized the 
AAA shape, this representation preserves the vascular surface's anatomical 
structure and geometric fidelity. We train our model using a longitudinal 
dataset of 113 computed tomography angiography (CTA) scans of 24 AAA patients at 
irregularly sampled intervals. After training, our model predicts AAA growth to 
the next scan moment with a median diameter error of 1.18 mm. We further 
demonstrate our model's utility to identify whether a patient will become 
eligible for elective repair within two years (acc = 0.93). Finally, we evaluate 
our model's generalization on an external validation set consisting of 25 CTAs 
from 7 AAA patients from a different hospital. Our results show that local 
directional AAA growth prediction from the vascular surface is feasible and may 
contribute to personalized surveillance strategies.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.artmed.2025.103323
PMID: 41330258 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: Jelmer M. Wolterink reports 
financial support was provided by Nederlandse Organisatie voor Wetenschappelijk 
Onderzoek Utrecht. Kak Khee Yeung reports financial support was provided by 
European Commission. If there are other authors, they declare that they have no 
known competing financial interests or personal relationships that could have 
appeared to influence the work reported in this paper.


77. Eur J Psychotraumatol. 2025 Dec;16(1):2589709. doi: 
10.1080/20008066.2025.2589709. Epub 2025 Dec 2.

Can machine learning predict PTSD symptoms from trauma narratives of children 
and adolescents?

Giuliani A(1), Sharp T(1), Chideya Y(2), Meiser-Stedman R(3), Tomlinson M(2)(4), 
Halligan SL(1).

Author information:
(1)Department of Psychology, University of Bath, Bath, UK.
(2)Department of Global Health, Institute for Life Course Health Research, 
Stellenbosch University, Stellenbosch, South Africa.
(3)Department of Clinical Psychology & Psychological Therapies, Norwich Medical 
School, University of East Anglia, Norwich, UK.
(4)School of Nursing and Midwifery, Queen's University Belfast, Belfast, UK.

Background: Machine learning approaches are being increasingly tested as a 
potential means of identifying mental health conditions. Narrative features of 
trauma memories are proposed to play a significant role in the development of 
post-traumatic stress disorder (PTSD), meaning that trauma narratives provide an 
excellent context in which to test machine learning capabilities. The potential 
for children's trauma narratives to predict post-traumatic stress remains 
particularly poorly studied. Here, we tested whether the application of machine 
learning to trauma narrative characteristics can predict PTSD symptoms in young 
individuals exposed to trauma.Study methodology: Two pre-trained large language 
models and two benchmark models were fine-tuned and trained to predict PTSD 
symptom severity from children's autobiographical narratives of a traumatic 
event. Data comprised narratives collected one month post-trauma from 400 
individuals aged 7-17 years old who experienced a psychological trauma that led 
to attendance at emergency departments in the United Kingdom (N = 178) and South 
Africa (N = 222), as well as self-reported PTSD symptoms and trauma memory 
features.Findings: Both pre-trained and benchmark models demonstrated poor 
predictive performance across trauma narratives in the United Kingdom, South 
Africa, and the combined datasets (e.g. RoBERTa R²  =  -.05; LASSO R² ≈ 0). 
However, adding self-reported trauma memory features, disorganisation, and 
sensory vividness improved the benchmark models' performances, especially in the 
UK dataset (e.g. LASSO R² = .57; XGBoost R² = .45).Conclusions: These findings 
indicate that while trauma narratives alone offer limited predictive value, 
incorporating self-reported trauma memory characteristics substantially enhances 
model performance, highlighting the importance of focusing on subjective reports 
to develop scalable automated tools for PTSD risk prediction in youth.

Plain Language Summary: Trauma narratives alone offer limited predictive value 
for PTSD in youth. Advanced machine learning models, including large language 
models, failed to predict post-traumatic stress symptoms based on children’s and 
adolescents’ trauma accounts.The study used trauma narratives from children in 
both the UK and South Africa. Despite cultural and linguistic differences, 
predictive performance was similarly limited across both samples, suggesting a 
broader developmental challenge in detecting PTSD through narrative 
language.Subjective memory features substantially enhance predictive 
performance. Incorporating self-reported ratings of memory disorganisation and 
sensory vividness led to significant improvements in model accuracy.

Antecedentes: Los enfoques de aprendizaje automático se están probando cada vez 
más como un posible medio para identificar condiciones de salud mental. Se 
propone que las características narrativas de los recuerdos traumáticos 
desempeñan un papel significativo en el desarrollo del TEPT, por lo que las 
narrativas del trauma ofrecen un contexto excelente para poner a prueba las 
capacidades del aprendizaje automático. El potencial de las narrativas del 
trauma en niños para predecir el estrés postraumático sigue siendo un área poco 
estudiada. En este estudio, probamos si la aplicación del aprendizaje automático 
a las características de las narrativas del trauma puede predecir los síntomas 
de TEPT en jóvenes expuestos a trauma. Metodología: Se ajustaron y entrenaron 
dos modelos de lenguaje grandes preentrenados y dos modelos de referencia para 
predecir la gravedad de los síntomas de TEPT a partir de narrativas 
autobiográficas de un evento traumático contado por niños. Los datos incluyeron 
narrativas recopiladas un mes después del trauma de 400 individuos de entre 7 y 
17 años que habían experimentado un trauma psicológico que los llevó a acudir a 
servicios de urgencias en el Reino Unido (N = 178) y Sudáfrica (N = 222), además 
de auto-reportes de síntomas de TEPT y características de la memoria del trauma. 
Hallazgos: Tanto los modelos de lenguaje preentrenados como los modelos de 
referencia mostraron un bajo rendimiento predictivo en las narrativas del trauma 
del Reino Unido, Sudáfrica y en los conjuntos de datos combinados (por ejemplo, 
RoBERTa R² = –,05; LASSO R² ≈ 0). Sin embargo, al añadir características 
auto-reportadas de la memoria del trauma – como la desorganización y la vividez 
sensorial – se mejoró el rendimiento de los modelos de referencia, especialmente 
en el conjunto de datos del Reino Unido (por ejemplo, LASSO R² = ,57; XGBoost 
R² = ,45). Conclusión: Estos hallazgos indican que, aunque las narrativas del 
trauma por sí solas ofrecen un valor predictivo limitado, la incorporación de 
características auto-reportadas de la memoria del trauma mejora sustancialmente 
el rendimiento de los modelos. Esto resalta la importancia de centrarse en los 
reportes subjetivos para desarrollar herramientas automatizadas y escalables que 
permitan predecir el riesgo de TEPT en jóvenes.

DOI: 10.1080/20008066.2025.2589709
PMCID: PMC12673981
PMID: 41328798 [Indexed for MEDLINE]

Conflict of interest statement: The authors alone are responsible for the 
content and writing of this paper.


78. Patterns (N Y). 2025 Aug 29;6(11):101347. doi: 10.1016/j.patter.2025.101347. 
eCollection 2025 Nov 14.

Current progress and open challenges for applying artificial intelligence across 
the in vitro fertilization cycle.

Gao Y(1)(2), Yuan Y(3)(4)(5)(6), Wang K(1), Wang Y(3)(4)(5)(6), Gao T(7), Yang 
Y(8), Ma LS(9), Li R(3)(4)(5)(6), Wang G(7), Liu X(10).

Author information:
(1)Department of Big Data and Biomedical AI, College of Future Technology, 
Peking University and Peking-Tsinghua Center for Life Sciences, Beijing 100871, 
China.
(2)Institute for AI in Medicine and Faculty of Medicine, Macau University of 
Science and Technology, Taipa, Macau 999087, China.
(3)State Key Laboratory of Female Fertility Promotion, Department of Obstetrics 
and Gynecology, Peking University Third Hospital, Beijing 100191, China.
(4)National Clinical Research Center for Obstetrics and Gynecology, Peking 
University Third Hospital, Beijing 100191, China.
(5)Key Laboratory of Assisted Reproduction (Peking University), Ministry of 
Education, Beijing 100191, China.
(6)Beijing Key Laboratory of Reproductive Endocrinology and Assisted 
Reproductive Technology, Beijing 100191, China.
(7)State Key Laboratory of Networking and Switching Technology, Beijing 
University of Posts and Telecommunications, Beijing 100876, China.
(8)School of Nursing and Rehabilitation, Cheeloo College of Medicine, Shandong 
University, Jinan 250012, China.
(9)Department of Neonatal Surgery, Capital Center for Children's Health, Capital 
Medical University, Beijing 100020, China.
(10)UCL Cancer Institute, University College London, London WC1E 6DD, UK.

In vitro fertilization (IVF) has significantly advanced the treatment of 
infertility, yet success rates remain modest due to its complexity and reliance 
on clinical experience. Recent advances in artificial intelligence (AI) offer 
promising tools to support decision-making throughout the IVF process. This 
review summarizes current applications of AI in IVF by organizing studies 
according to the data modality they use, including structured health records, 
biomedical images, and omics data. For each modality, we describe representative 
tasks, model performance, and key methodological progress. We also examine the 
potential of emerging AI approaches, such as multi-modal learning and large 
language models. In addition, we acknowledge ongoing challenges, including 
limited model generalizability, data bias, and the need for clinically 
validated, transparent AI systems. While the integration of AI into IVF is 
promising, its success will depend on rigorous validation, ethical safeguards, 
and interdisciplinary efforts to ensure safe and equitable implementation.

© 2025 The Author(s).

DOI: 10.1016/j.patter.2025.101347
PMCID: PMC12664965
PMID: 41328159

Conflict of interest statement: The authors declare no competing interests.


79. JMIR Form Res. 2025 Dec 1;9:e69841. doi: 10.2196/69841.

A Rule-Based Conversational Agent for Mental Health and Well-Being in Young 
People: Formative Case Series During the Rise of Generative AI.

Wrightson-Hester AR(1), Anderson G(1), Dunstan J(2), McEvoy PM(1)(3), Sutton 
CJ(4), Myers B(1)(5), Egan SJ(1), Tai SJ(6), Johnston-Hollitt M(2), Chen W(1), 
Gedeon T(7), Moullin JC(1), Mansell W(1).

Author information:
(1)Curtin enAble Institute, School of Population Health, Faculty of Health 
Sciences, Curtin University, Perth, Western Australia, Australia.
(2)Curtin Institute for Data Science, Curtin University, Perth, Western 
Australia, Australia.
(3)Centre for Clinical Interventions, North Metropolitan Health Service, 
Nedlands, Australia.
(4)Lancashire Clinical Trials Unit,, University of Lancashire, Lancaster, United 
Kingdom.
(5)Substance Use and Tobacco Research Unit, South African Medical Research 
Council, Tygerberg, Western Cape, South Africa.
(6)School of Health Sciences; Division of Psychology and Mental Health, Faculty 
of Medical and Health Sciences, University of Manchester, Manchester, United 
Kingdom.
(7)School of Electrical Engineering, Computing and Mathematical Sciences, Curtin 
University, Perth, Western Australia, Australia.

BACKGROUND: There is a shortage of services available to address the growing 
demand for mental health support in Australia and worldwide. Digital 
interventions, including conversational agents, can overcome barriers to 
accessing mental health support. The recent advances in large language models 
have led to an improvement in the perceived human-like naturalness of chatbot 
conversations, but there is little research on the experience of chatbots to 
support mental health. Manage your life online (MYLO) is a rule-based chatbot 
that was co-designed with young people and uses questions to help users explore 
their problems. In a case series conducted before the release of ChatGPT 
(OpenAI), users rated a new smartphone interface for MYLO as acceptable, and 
there was a large effect size for reduction in problem-related distress.
OBJECTIVE: This study aimed to evaluate an improved version of MYLO and compare 
the user experience of MYLO in this case series to the previous version that was 
completed in November 2022.
METHODS: We replicated and extended the previous 2-week case-series, conducted 
from September to November 2022, by testing 4-week usage of MYLO with a larger 
sample between October and December 2023. We recruited 24 young people living in 
Western Australia who self-described as having a lived experience of anxiety or 
depression. Participants had access to and used MYLO over a 4-week period while 
completing online weekly surveys that included a range of health and 
psychological questionnaires. After the 4-week testing phase, participants were 
invited to provide feedback on their experience of using MYLO through an 
interview or focus group discussion.
RESULTS: In total, 13 of the 24 participants were retained throughout the study 
and took part in interviews. On average, participants had around 4 conversations 
with MYLO. They experienced both benefits and limitations of these 
conversations. They spoke about their recent experiences with ChatGPT (released 
in November 2022 after the previous case-series concluded) and other generative 
artificial intelligence (AI) tools, stating that they had expected MYLO to 
possess similar functionality, which it did not. Nonetheless, we found moderate 
to large effect sizes for improvements in problem-related distress (Cohen 
d=-1.07), anxiety (Cohen d=-0.41), and psychiatric impairment (Cohen d=-0.60) 
and some evidence of reliable improvement in clinical outcomes.
CONCLUSIONS: These findings have implications for mental health chatbots in the 
age of ChatGPT and highlight a need for researchers to engage with new 
technologies to improve user experience, while maintaining the necessary safety 
and ethical standards that can be a significant challenge for generative AI.

©Aimee-Rose Wrightson-Hester, Gee Anderson, Joel Dunstan, Peter M McEvoy, 
Christopher J Sutton, Bronwyn Myers, Sarah J Egan, Sara Jane Tai, Melanie 
Johnston-Hollitt, Wai Chen, Tom Gedeon, Joanna C Moullin, Warren Mansell. 
Originally published in JMIR Formative Research (https://formative.jmir.org), 
01.12.2025.

DOI: 10.2196/69841
PMCID: PMC12706453
PMID: 41325605 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


80. Eur J Pediatr. 2025 Dec 1;184(12):809. doi: 10.1007/s00431-025-06602-x.

New chapter in pediatric medicine: technological evolution, application, and 
evaluation system of large language models.

Zhu S(1), Xie Y(1), Tang Y(2), Yu Z(1), Zhao R(1), Dong X(3).

Author information:
(1)Department of Pulmonology, School of Medicine, Shanghai Children's Hospital, 
Shanghai Jiao Tong University, Shanghai, 200062, China.
(2)Pediatric, LongHua Hospital Shanghai University of Traditional Chinese 
Medicine, Shanghai, China.
(3)Department of Pulmonology, School of Medicine, Shanghai Children's Hospital, 
Shanghai Jiao Tong University, Shanghai, 200062, China. 
dongxy@shchildren.com.cn.

With significant breakthroughs in natural language processing technology, large 
language models (LLMs) based on deep learning have demonstrated considerable 
potential in the medical field in recent years. Through pre-training on massive 
textual corpora, these models are capable of understanding and generating 
human-like language, providing innovative tools for tasks such as medical 
literature retrieval, clinical note generation, and diagnostic assistance. In 
particular, within the domain of pediatrics, LLMs offer promising applications 
for enhancing the efficiency and safety of diagnosis and treatment through 
intelligent patient communication, personalized educational support, and 
optimized treatment planning. This article reviews recent advancements in LLM 
technology, encompassing the developmental trajectory and scaling of 
general-purpose models, the tailored training of medical specialized models, and 
the emergence of multimodal and mixture-of-expert architectures. It further 
highlights practical applications in pediatric contexts, including dosage 
calculation, subspecialty-specific clinical decision support, and automated 
medical record structuring, while also examining evaluation metrics, 
ethical-legal challenges, and considerations for multilingual and low-resource 
settings.
IN CONCLUSION: the paper emphasizes the importance of interdisciplinary 
collaboration and outlines future directions for safely and equitably 
integrating LLMs into pediatric medical practice.
WHAT IS KNOWN: • Large language models (LLMs) are increasingly used in medicine 
for text generation, clinical documentation, and knowledge retrieval. • 
Pediatric applications of LLMs have been less systematically reviewed compared 
with those in adult medicine.
WHAT IS NEW: • This review provides an integrative overview of LLM development, 
clinical implementation, and evaluation in pediatric contexts. • It identifies 
unique challenges in pediatrics, including age-dependent variability and the 
need for family-centered care, and proposes design principles for future 
child-specific LLM benchmarks.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00431-025-06602-x
PMID: 41324732 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


81. BJPsych Bull. 2025 Dec 1:1-8. doi: 10.1192/bjb.2025.10186. Online ahead of 
print.

Evaluating artificial intelligence ambient voice technology as a documentation 
assistant in psychiatry: proof-of-concept study.

Stanton N(1), Aziz A(1), Jakhra S(1), Wong S(1), Morganstein L(1), Bassett P(2), 
Brewerton M(1), Golchinheydari S(1), Brogan D(1), Pushparajah D(1).

Author information:
(1)https://ror.org/05drfg619Central and North West London NHS Foundation Trust, 
London, UK.
(2)Statsconsultancy Ltd, Amersham, UK.

AIMS AND METHOD: Artificial intelligence ambient voice technology (AI AVT), 
which uses a large language model to summarise clinical dialogue into electronic 
notes and GP letters, has emerged. We conducted a mixed-methods, pre-post 
(manual versus AVT-assisted documentation) service development pilot to evaluate 
its use in a child and adolescent out-patient clinic.
RESULTS: The median administration time per clinical encounter reduced from 27 
min (manual) to 10 min (AVT) (P < 0.001). On average, AVT-assisted documentation 
required only 45% of the time for manual documentation (P < 0.001). 
Clinician-rated accuracy, quality and efficiency were significantly higher for 
AVT-assisted documentation. Patient acceptance was high, with 97% reporting that 
clinicians were not distracted by note-taking. Thematic analysis from focus 
groups identified positive effects derived from AVT (improved productivity and 
clinician well-being), but was balanced by barriers (technological limitations).
CLINICAL IMPLICATIONS: Integration of AVT into clinical workflows can 
significantly alleviate documentation burden, reduce cognitive strain and free 
up clinical capacity.

DOI: 10.1192/bjb.2025.10186
PMID: 41321068


82. JMIR Ment Health. 2025 Nov 27;12:e77951. doi: 10.2196/77951.

Seeking Emotional and Mental Health Support From Generative AI: Mixed-Methods 
Study of ChatGPT User Experiences.

Luo X(1), Wang Z(2), Tilley JL(3), Balarajan S(1), Bassey UA(1), Cheang CI(1).

Author information:
(1)Department of Counseling Psychology, Santa Clara University, 500 El Camino 
Real, Santa Clara, CA, 95053, United States, 1 408-551-1603.
(2)Encounter Psychotherapy LLC, Gaithersburg, MD, United States.
(3)Psychology and Child and Human Development Group, National Institution of 
Education, Nanyang Technological University, Singapore, Singapore.

BACKGROUND: Generative artificial intelligence (GenAI) models have emerged as a 
promising yet controversial tool for mental health.
OBJECTIVE: The purpose of this study is to understand the experiences of 
individuals who repeatedly used ChatGPT (GenAI) for emotional and mental health 
support (EMS).
METHODS: We recruited 270 adult participants across 29 countries who regularly 
used ChatGPT (OpenAI) for EMS during April 2024. Participants responded to 
quantitative survey questions on the frequency and helpfulness of using ChatGPT 
for EMS, and qualitative questions regarding their therapeutic purposes, 
emotional experiences of using, and perceived helpfulness and rationales. 
Thematic analysis was used to analyze qualitative data.
RESULTS: Most participants reported using ChatGPT for EMS at least 1-2 times per 
month for purposes spanning traditional mental health needs (diagnosis, 
treatment, and psychoeducation) and general psychosocial needs (companionship, 
relational guidance, well-being improvement, and decision-making). Users 
reported various emotional experiences during and after use for EMS (eg, 
connected, relieved, curious, embarrassed, or disappointed). Almost all users 
found it at least somewhat helpful. The rationales for perceived helpfulness 
include perceived changes after use, emotional support, professionalism, 
information quality, and free expression, whereas the unhelpful aspects include 
superficial emotional engagement, limited information quality, and lack of 
professionalism.
CONCLUSIONS: Despite the absence of ethical regulations for EMS use, GenAI is 
becoming an increasingly popular self-help tool for emotional and mental health 
support. These results highlight the blurring boundary between formal mental 
health care and informal self-help and underscore the importance of 
understanding the relational and emotional dynamics of human-GenAI interaction. 
There is an urgent need to promote AI literacy and ethical awareness among 
community users and health care providers and to clarify the conditions under 
which GenAI use for mental health promotes well-being or poses risk.

© Xiaochen Luo, Zixuan Wang, Jacqueline L Tilley, Sanjeev Balarajan, Ukeme-Abasi 
Bassey, Choi Ieng Cheang. Originally published in JMIR Mental Health 
(https://mental.jmir.org).

DOI: 10.2196/77951
PMCID: PMC12661908
PMID: 41313214 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


83. BMC Oral Health. 2025 Nov 28;25(1):2002. doi: 10.1186/s12903-025-07360-w.

Artificial intelligence versus human expertise: reliability of ChatGPT and the 
London atlas for dental age estimation using panoramic radiographs.

Peker RB(1).

Author information:
(1)Department of Dentomaxillofacial Radiology, Trakya University Faculty of 
Dentistry, Balkan Campus, Edirne, 22030, Turkey. rberkaypeker@trakya.edu.tr.

BACKGROUND: This study evaluated the performance of ChatGPT, a multimodal large 
language model (LLM), in estimating dental age from panoramic radiographs (PRs) 
and compared its accuracy and reproducibility with those of the London Atlas 
(LA) method.
METHODS: PRs of 620 healthy children aged 6 through 13 years were 
retrospectively analyzed. An experienced dentomaxillofacial radiologist 
estimated dental age using the LA, and the ChatGPT-4o model analyzed the same 
anonymized images to generate automated age predictions. Both methods were 
repeated after two weeks to assess intra-observer reliability. Predictive 
accuracy and agreement with chronological age (CA) were evaluated using mean 
absolute error (MAE), root mean squared error (RMSE), intraclass correlation 
coefficients (ICC), and Bland–Altman analyses. Statistical significance was set 
at p < .05.
RESULTS: ChatGPT’s predictions differed significantly from chronological age 
(CA), tending to overestimate age in younger children and underestimate age in 
older children. Compared with the LA, ChatGPT exhibited higher MAE and RMSE 
values, indicating lower predictive accuracy and greater variability. Error 
magnitudes were greatest in the 6-, 12-, and 13-year-old groups and lowest in 
the 8-year-old group, whereas the LA showed lower and more stable errors across 
ages. The LA demonstrated fewer discrepancies and excellent reproducibility 
(ICC = 0.960) as compared with the moderate agreement of ChatGPT (ICC = 0.703). 
Overall, the LA provided estimates closer to CA, whereas ChatGPT exhibited 
greater variability.
CONCLUSIONS: ChatGPT shows promise for complex decision-making tasks such as 
dental age estimation; however, its current accuracy, reproducibility, and 
output stability remain inferior to established methods such as the LA. The 
inconsistent predictions observed across repeated evaluations highlight a 
critical limitation regarding its reliability for clinical and forensic 
applications. Therefore, ChatGPT-based estimations should be interpreted with 
caution until future versions achieve more consistent and reproducible 
performance through population-specific training, model optimization, and 
multicenter validation.

DOI: 10.1186/s12903-025-07360-w
PMCID: PMC12751685
PMID: 41310564

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The Non‑Interventional Scientific Research Ethics Committee of 
Trakya University Faculty of Medicine, Turkey, approved this retrospective 
observational study (No. TÜTF-GOBEAK 2025/376). All panoramic radiographs were 
anonymized prior to analysis, and no identifiable personal information was used. 
All procedures performed in this study involving human data were conducted in 
accordance with the ethical standards of the institutional and/or national 
research committee and with the 1964 Declaration of Helsinki and its later 
amendments or comparable ethical standards. As this study involved only 
retrospective analysis of existing, anonymized clinical data, the ethics 
committee waived the requirement for informed consent. Consent for publication: 
Not applicable. Competing interests: The authors declare no competing interests.


84. NPJ Precis Oncol. 2025 Nov 27;9(1):384. doi: 10.1038/s41698-025-01173-4.

Unsupervised generative AI for enhancing brain tumor segmentation in 
multi-center, incomplete real-world data scenarios.

Li Z(1)(2)(3)(4), Zhou T(5), Zhang B(6), Zhang X(6), Li W(7)(8), Hang C(9)(10), 
He K(11)(12).

Author information:
(1)Department of Neurosurgery, Nanjing Drum Tower Hospital, Affiliated Hospital 
of Medical School, Nanjing University, Nanjing, China.
(2)Medical School of Nanjing University, Nanjing, China.
(3)Neurosurgical Institute, Nanjing University, Nanjing, China.
(4)National Institute of Healthcare Data Science at Nanjing University, Nanjing, 
China.
(5)School of Computer Science and Engineering, Nanjing University of Science and 
Technology, Nanjing, China.
(6)Department of Radiology, Nanjing Drum Tower Hospital, Affiliated Hospital of 
Medical School, Nanjing University, Nanjing, China.
(7)Department of Neurosurgery, Nanjing Drum Tower Hospital, Affiliated Hospital 
of Medical School, Nanjing University, Nanjing, China. wei.li@nju.edu.cn.
(8)Neurosurgical Institute, Nanjing University, Nanjing, China. 
wei.li@nju.edu.cn.
(9)Department of Neurosurgery, Nanjing Drum Tower Hospital, Affiliated Hospital 
of Medical School, Nanjing University, Nanjing, China. hang1965@nju.edu.cn.
(10)Neurosurgical Institute, Nanjing University, Nanjing, China. 
hang1965@nju.edu.cn.
(11)Medical School of Nanjing University, Nanjing, China. hkl@nju.edu.cn.
(12)National Institute of Healthcare Data Science at Nanjing University, 
Nanjing, China. hkl@nju.edu.cn.

The clinical application of deep learning (DL)-based brain tumor segmentation 
remains limited by missing MRI sequences and cross-center data inconsistencies. 
Existing supervised generative models can synthesize missing sequences but rely 
on paired, fully sampled training data, which are often unavailable in routine 
practice. This study aims to assess the use of an unsupervised generative model 
to complete missing sequences and eliminate cross-center data inconsistency, and 
to verify whether using the generated images can enhance brain tumor 
segmentation. We retrospectively evaluated 921 glioblastoma (GBM) patients from 
BraTS, UCSF-PDGM, and our institutional datasets, together with 1000 meningioma 
cases from BraTS-MEN cohort. We developed an unsupervised multi-center 
multi-sequence generative adversarial transformer (UMMGAT) to generate MRI 
sequences from incomplete datasets. Key features of UMMGAT include a sequence 
encoder that disentangles and encodes modality-specific characteristics, and a 
lesion-aware module (LAM) that enhances the generation of tumor regions, all 
trained via multi-task learning for generating multi-modal images. Validation on 
GBM and meningioma segmentation task demonstrated that generated MRI sequences 
significantly improved segmentation performance across various missing-sequence 
scenarios. The enhancement in segmentation performance when T1ce was missing is 
an improvement that previous methods have not achieved. Further validation on an 
external local dataset confirmed that UMMGAT effectively adapts to cross-center 
data variations. With minimal training data requirements and the ability to 
generate multi-sequence MRI across multiple centers, UMMGAT provides a practical 
solution for handling incomplete and heterogeneous MRI data, facilitating more 
consistent and accurate brain tumor segmentation in diverse clinical contexts.

© 2025. The Author(s).

DOI: 10.1038/s41698-025-01173-4
PMCID: PMC12660812
PMID: 41310177

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


85. JMIR Form Res. 2025 Nov 27;9:e79038. doi: 10.2196/79038.

GrantCheck-an AI Solution for Guiding Grant Language to New Policy Requirements: 
Development Study.

Shi Q(1), Oztekin A(2), Matthew G(3), Bortle J(3), Jenkins H(3), Wong SK(4), 
Langlois P(3), Zaki A(3), Coleman B(3), Luzuriaga K(1), Zai AH(4).

Author information:
(1)Center for Clinical and Translational Science, UMass Chan Medical School, 
Worcester, MA, United States.
(2)Manning School of Business, UMass Lowell, Lowell, MA, United States.
(3)Information Technology, UMass Chan Medical School, Worcester, MA, United 
States.
(4)Department of Population and Quantitative Health Sciences, UMass Chan Medical 
School, Worcester, MA, United States.

BACKGROUND: Academic institutions face increasing challenges in grant writing 
due to evolving federal and state policies that restrict the use of specific 
language. Manual review processes are labor-intensive and may delay submissions, 
highlighting the need for scalable, secure solutions that ensure compliance 
without compromising scientific integrity.
OBJECTIVE: This study aimed to develop a secure, artificial intelligence-powered 
tool that assists researchers in writing grants consistent with evolving state 
and federal policy requirements.
METHODS: GrantCheck (University of Massachusetts Chan Medical School) was built 
on a private Amazon Web Services virtual private cloud, integrating a rule-based 
natural language processing engine with large language models accessed via 
Amazon Bedrock. A hybrid pipeline detects flagged terms and generates 
alternative phrasing, with validation steps to prevent hallucinations. A secure 
web-based front end enables document upload and report retrieval. Usability was 
assessed using the System Usability Scale.
RESULTS: GrantCheck achieved high performance in detecting and recommending 
alternatives for sensitive terms, with a precision of 1.00, recall of 0.73, and 
an F1-score of 0.84-outperforming general-purpose models including GPT-4o 
(OpenAI; F1=0.43), Deepseek R1 (High-Flyer; F1=0.40), Llama 3.1 (Meta AI; 
F1=0.27), Gemini 2.5 Flash (Google; F1=0.58), and even Gemini 2.5 Pro (Google; 
F1=0.72). Usability testing among 25 faculty and staff yielded a mean System 
Usability Scale score of 85.9 (SD 13.4), indicating high user satisfaction and 
strong workflow integration.
CONCLUSIONS: GrantCheck demonstrates the feasibility of deploying 
institutionally hosted, artificial intelligence-driven systems to support 
compliant and researcher-friendly grant writing. Beyond administrative 
efficiency, such systems can indirectly safeguard public health research 
continuity by minimizing grant delays and funding losses caused by 
language-related policy changes. By maintaining compliance without suppressing 
scientific rigor or inclusivity, GrantCheck helps protect the pipeline of 
research that advances biomedical discovery, health equity, and patient 
outcomes. This capability is particularly relevant for proposals in sensitive 
domains-such as social determinants of health, behavioral medicine, and 
community-based research-that are most vulnerable to evolving policy 
restrictions. As a proof-of-concept development study, our implementation is 
tailored to one institution's policy environment and security infrastructure, 
and findings should be interpreted as preliminary rather than universally 
generalizable.

©Qiming Shi, Asil Oztekin, George Matthew, Jeffrey Bortle, Hayden Jenkins, 
Steven (Koon) Wong, Paul Langlois, Anaheed Zaki, Brian Coleman, Katherine 
Luzuriaga, Adrian H Zai. Originally published in JMIR Formative Research 
(https://formative.jmir.org), 27.11.2025.

DOI: 10.2196/79038
PMCID: PMC12699247
PMID: 41308189 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


86. Foods. 2025 Nov 11;14(22):3855. doi: 10.3390/foods14223855.

Adaptive Identification of Food Sweetness Concentration: An Electroencephalogram 
Feature Classification Network Under Taste Stimulation.

Wang H(1)(2), Men H(1)(3), Shi Y(1)(3).

Author information:
(1)School of Automation Engineering, Northeast Electric Power University, Jilin 
132012, China.
(2)School of Transportation, Jilin Tiedao University, Jilin 132299, China.
(3)Advanced Sensor Research Institution, Northeast Electric Power University, 
Jilin 132012, China.

Detecting and identifying consumers' perception of food sweetness can help guide 
the optimization of food formulations. Electroencephalogram (EEG) detection can 
capture changes in brain electrical activity in response to different sweet 
taste stimuli. In this work, we employ EEG detection and propose an EEG Feature 
Calculation and Classification Network (EFCC-Net) to recognize taste EEG signals 
under different sweetness concentration stimuli. First, taste-related EEG data 
from a subject group under varying sweetness concentration stimuli are 
collected. Then, an EEG Feature Calculation Module (EFCM) is proposed, which 
utilizes convolutional kernels of different sizes to compute local features from 
both temporal and spatial dimensions of EEG data. A lightweight self-attention 
mechanism is employed to compute global features, and a multi-branch computation 
approach is adopted to enhance feature extraction capability. Next, based on EEG 
topographic maps, qualitative analysis is conducted to examine differences in 
brain region activation under varying taste concentrations. Finally, leveraging 
the proposed EFCM, the EFCC-Net is designed to classify EEG data corresponding 
to different sweetness levels. Through structural optimization, ablation 
experiments, and comparisons with state-of-the-art EEG classification methods, 
EFCC-Net achieves the best classification performance, with an accuracy of 
96.57%, a precision of 96.58%, and a recall of 96.53%, while also demonstrating 
superior stability.

DOI: 10.3390/foods14223855
PMCID: PMC12651536
PMID: 41300014

Conflict of interest statement: The authors declare no conflicts of interest.


87. Oral Radiol. 2025 Nov 25. doi: 10.1007/s11282-025-00878-3. Online ahead of 
print.

Diagnostic performance of artificial intelligence for facial fracture detection: 
a systematic review.

Tuygunov N(1), Boymuradov SA(2), Khurshid Z(3)(4), Songsiripradubboon S(5), 
Abdulahtov J(6), Khatamov U(7)(2).

Author information:
(1)Department of Restorative Dentistry, Kimyo International University in 
Tashkent, Tashkent, Uzbekistan. nozimtuygunov@gmail.com.
(2)Department of Maxillofacial Surgery and Dentistry, Faculty of Dentistry, 
Tashkent State Medical University, Tashkent, Uzbekistan.
(3)Department of Prosthodontics and Dental Implantology, College of Dentistry, 
King Faisal University, Al-Ahsa, Saudi Arabia.
(4)Center for Artificial Intelligence and Innovation (CAII), Faculty of 
Dentistry, Chulalongkorn University, Bangkok, Thailand.
(5)Department of Pediatric Dentistry, Faculty of Dentistry, Chulalongkorn 
University, Bangkok, Thailand.
(6)Department of Prosthodontics, Faculty of Dentistry, Tashkent State Medical 
University, Tashkent, Uzbekistan.
(7)Department of Restorative Dentistry, Kimyo International University in 
Tashkent, Tashkent, Uzbekistan.

OBJECTIVE: To evaluate the diagnostic performance of artificial intelligence 
(AI) models for detecting facial bone fractures on computed tomography (CT), 
cone-beam CT (CBCT), and plain radiographs.
METHODS: Original studies applying machine learning or deep learning algorithms 
for facial fracture detection in humans were included if they reported 
diagnostic accuracy metrics such as sensitivity, specificity, or area under the 
curve (AUC). PubMed-MEDLINE, Scopus, and Web of Science databases were searched 
up to June 3, 2025. Risk of bias was assessed using the QUADAS-2 tool. The 
review followed PRISMA 2020 guidelines and was registered in PROSPERO 
(CRD420251085644).
RESULTS: A total of 23 studies were included. Object detection models such as 
YOLOv5 and Faster R-CNN-demonstrated high diagnostic accuracy in localizing 
facial fractures. Classification models such as ResNet and Swin Transformer 
achieved AUCs frequently exceeding 0.90. Segmentation and hybrid frameworks 
further improved anatomical specificity. However, the generalizability of 
findings was constrained by predominantly retrospective, single-centre study 
designs, limited sample sizes, inconsistent annotation practices, and the 
absence of external or prospective validation.
CONCLUSION: AI models show high diagnostic performance for detecting facial 
fractures across multiple anatomical regions and imaging modalities. Further 
multicentre prospective studies and the integration of explainable AI are 
essential for clinical adoption.
CLINICAL RELEVANCE: AI-assisted diagnostic models have the potential to enhance 
facial fracture detection accuracy, especially in emergency and resource-limited 
settings. Their integration into radiology workflows could reduce interpretation 
time, support less experienced clinicians, and improve patient outcomes.

© 2025. The Author(s) under exclusive licence to Japanese Society for Oral and 
Maxillofacial Radiology.

DOI: 10.1007/s11282-025-00878-3
PMID: 41291186

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare no competing interests. Ethics approval: Not applicable. Consent for 
publication: All authors have read and agreed to the published version of the 
manuscript. Informed consent: Not applicable. Institutional review board 
statement: Not applicable.


88. Sci Rep. 2025 Nov 25;15(1):41977. doi: 10.1038/s41598-025-25992-4.

Attentional LSTM-ensemble architecture for intrusion detection in smart grids.

Singh R(1), Singh Gill N(2), Gulia P(2).

Author information:
(1)Department of Computer Science and Applications, Maharshi Dayanand 
University, Rohtak, Haryana, India. rashi.rs.dcsa@mdurohtak.ac.in.
(2)Department of Computer Science and Applications, Maharshi Dayanand 
University, Rohtak, Haryana, India.

Smart grids integrate real-time communication, computational intelligence, and 
physical energy infrastructure to improve operational efficiency and 
adaptability. However, this interconnectivity increases vulnerability to cyber 
intrusions capable of disrupting control signals, compromising data integrity, 
and causing large-scale outages. This study presents an architectural synthesis 
that combines a long short-term memory network with an attention mechanism for 
temporal-saliency feature extraction, followed by an ensemble of 
gradient-boosting classifiers (XGBoost, LightGBM, and CatBoost) for robust 
decision-making. Experiments on the benchmark smart grid intrusion detection 
dataset demonstrate that the integrated framework achieves 79.8% average 
cross-validation accuracy and 75.67% overall test accuracy, with a normal-class 
recall of 98.26% in the baseline configuration. To address severe class 
imbalance, a combination of synthetic minority oversampling technique and focal 
loss was applied, which improved minority attack-class recall from 1.43 to 64.3% 
and increased its PR-AUC from 0.2884 to 0.791. The balanced configuration 
yielded an receiver operating characteristic curve (ROC-AUC) of 0.928 for both 
classes, demonstrating substantial gains in minority-class detection while 
maintaining high precision for majority classes. These results highlight the 
potential of strategically combining temporal modelling, attention-driven 
interpretability, and ensemble diversity, augmented with targeted imbalance 
mitigation, to develop effective, scalable, and interpretable intrusion 
detection systems for critical energy infrastructure.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-25992-4
PMCID: PMC12647719
PMID: 41290901

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


89. Sci Rep. 2025 Nov 25;15(1):41825. doi: 10.1038/s41598-025-25813-8.

Spatiotemporal multimodal emotion recognition using Temporal video sequences and 
pose features for child emotion classification.

Sangeetha SKB(1), Boddu RSK(2), Bhaumik A(3), Mathivanan SK(4), Moorthy U(5).

Author information:
(1)Postdoctoral Researcher, Lincoln University College, Petaling Jaya, Malaysia.
(2)CSE (AI&ML) Department, Raghu Engineering College, Visakhapatnam, India.
(3)President, Lincoln University College, Petaling Jaya, Malaysia.
(4)School of Computing Science and Engineering, Galgotias University, Greater 
Noida, 203201, India.
(5)School of Computer Engineering, Manipal Institute of Technology Bengaluru, 
Manipal Academy of Higher Education, Manipal, India. m.usha@manipal.edu.

Developmental psychology and affective computing have placed great emphasis on 
identifying children's emotional cues in recent times. In this study, a novel 
Spatio-Temporal Multimodal Emotion Recognition Network (ST-MERN) for child 
emotion classification is proposed. Dense feature embeddings of the EmoReact 
dataset and temporal video sequences are utilized for the study. The proposed 
method uses 115 continuous frames per visual signal instance, e.g., 
rotational-translational vectors, facial keypoints, and pose predictions. With 
steady performance on each frame and a mean confidence of 0.967, this ensures 
the system maintains good detection fidelity. In order to track subtle emotional 
changes, our method captures dynamic data like scale variation and 
frame-to-frame variation (rx, ry, rz, tx, ty). Latent features (p24-p33) provide 
a profound explanation of emotional states. The model is designed to preserve 
spatiotemporal consistency and improve emotion recognition by combining these 
features. Curiosity, uncertainty, excitement, happiness, surprise, disgust, 
fear, frustration, and valence are the nine categories on which the system 
categorizes children's emotional states. Preliminary results show that our 
system effectively captures expressive nuances, with stable pose data and low 
feature variability across sequences. The model surpassed earlier models such as 
LSTM and TCN in generalization, with a high validation accuracy of 93.6% and 
test accuracy of 94.3% for the BiLSTM-based architecture. The BiLSTM model had 
enhanced classification capacity for different emotional states with an F1-score 
of 0.92. The TCN model is well-suited to real-time deployment since it recorded 
a competitive test accuracy of 91.7% with quick inference times of ~ 0.8 s per 
clip, even though it was slightly slower than the BiLSTM. With an F1-score of 
0.89 and test accuracy of 90.2%, the LSTM model performed robustly; it trained 
faster than the BiLSTM and TCN, although its accuracy was slightly lower. By 
providing strong and interpretable classification that is sensitive to the 
dynamic nature of children's emotional displays, this technique improves emotion 
detection in children. Our work provides the foundation for socially sensitive 
systems, therapy treatments, and affect-conscious education materials.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-25813-8
PMCID: PMC12647764
PMID: 41290891 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


90. Sci Rep. 2025 Nov 25;15(1):41787. doi: 10.1038/s41598-025-25744-4.

Personality traits measured by the HEXACO personality inventory and the dark 
triad predict university students' attitudes and misconduct behaviors related to 
generative artificial intelligence : Haiying Liang(1), Xu Mao(2), Michael J. 
Reiss(3).

Liang H(1), Mao X(2), Reiss MJ(3).

Author information:
(1)School of Foreign Languages, Peking University, Beijing, China.
(2)School of Health Humanities, Peking University, Beijing, China.
(3)Institute of Education, University College London, London, UK. 
m.reiss@ucl.ac.uk.

This study investigates how personality traits, specifically those measured by 
the HEXACO Personality Inventory and the Dark Triad, predict university 
students' attitudes toward generative artificial intelligence (GAI) and their 
engagement in GAI-related academic misconduct. The first objective was to 
develop and validate a Chinese-language scale to measure students' attitudes 
toward GAI in academic contexts. The newly developed GAI Attitudes Scale was 
tested for psychometric properties, showing high internal consistency 
(Cronbach's α = 0.92) and reliability. In the second part of the study, 
hierarchical linear regression analyses explored the relationship between 
personality traits and both GAI attitudes and misconduct behaviors. Findings 
indicated that Extraversion and Openness to Experience were significant positive 
predictors of favorable GAI attitudes. Regarding misconduct behaviors, 
Honesty-Humility, Agreeableness, and Conscientiousness were significant negative 
predictors, while Narcissism and Psychopathy were significant positive 
predictors. Notably, GAI attitudes did not provide additional predictive value 
for misconduct beyond personality traits. Taken together, the findings 
demonstrate that personality traits are central to understanding both the 
adoption and misuse of GAI in academic contexts, providing important insights 
for fostering ethical engagement with emerging technologies.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-25744-4
PMCID: PMC12647770
PMID: 41290781 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


91. BMC Med Educ. 2025 Nov 24;25(1):1755. doi: 10.1186/s12909-025-08308-y.

Comparing AI chatbot simulation and peer role-play for OSCE preparation: a pilot 
randomized controlled trial.

Lee HY(#)(1), Kim J(#)(2), Choi H(3), Bae H(4), Jeong A(5), Choi S(6), Kim 
JH(7), Kim CE(8).

Author information:
(1)Division of Humanities and Social Medicine, School of Korean Medicine, Pusan 
National University, Yangsan, Korea.
(2)Department of Physiology, College of Korean Medicine, Gachon University, 
Seongnam, Korea.
(3)Department of Sasang Constitutional Medicine, School of Korean Medicine, 
Pusan National University, Yangsan, Korea.
(4)Department of Physiology, College of Korean Medicine, Dongguk University , 
Gyeongju, Korea.
(5)Department of Pediatrics, College of Korean Medicine, Gachon University, 
Seongnam, Korea.
(6)Department of Neuropsychiatry, College of Korean medicine, Gachon University, 
Seongnam, Korea.
(7)Department of Sasang Constitutional Medicine, School of Korean Medicine, 
Pusan National University, Yangsan, Korea. jani77@pusan.ac.kr.
(8)Department of Physiology, College of Korean Medicine, Gachon University, 
Seongnam, Korea. eopchang@gachon.ac.kr.
(#)Contributed equally

BACKGROUND: Artificial intelligence (AI) is increasingly applied in medical 
education, but its role in fostering interactive clinical competencies remains 
underexplored. This pilot study aimed to compare the feasibility and educational 
impact of an AI chatbot-based simulation with traditional peer role-play (PRP) 
for Objective Structured Clinical Examination (OSCE) preparation, and to share 
practical lessons from implementing a novel AI tool in a trial setting.
METHODS: Nineteen final-year Korean medicine students were randomly assigned to 
either an AI chatbot group (n = 9) or a PRP group (n = 10) after a baseline 
knowledge test. Both groups underwent a 30-minute physical examination practice 
session, followed by a one-hour clinical interview training session specific to 
their group. The AI chatbot group practiced with a GPT-4o/Claude 3.5-based 
chatbot providing scenario-driven responses and automated feedback, while the 
PRP group practiced in pairs under tutor supervision. All participants then 
completed two OSCE stations (dizziness and shoulder pain). Performance was 
assessed using a structured checklist covering four domains: history taking, 
physical examination, patient education, and physician-patient interaction. 
Post-study questionnaires evaluated the learning experience.
RESULTS: Although the differences in OSCE scores between the groups did not 
reach statistical significance, several interesting and complementary trends 
were observed. For example, the PRP group tended to score higher in history 
taking (mean 74.4 vs. 66.2 in dizziness scenario; Hedges' g = -0.68, mean 58.6 
vs. 54.5 in shoulder pain scenario; Hedges' g = -0.21), while the AI chatbot 
group showed a tendency towards higher scores in patient education (32.5 vs. 
22.2 in dizziness scenario Hedges' g = 0.44, 85.0 vs. 66.7 in shoulder pain 
scenario; Hedges' g = 0.99). Survey results reflected these following trends. 
The PRP group valued the authenticity of the interaction and the exam-like 
environment. In contrast, the AI chatbot group reported higher satisfaction with 
the autonomy, opportunity for repetitive practice, and structured feedback.
CONCLUSION: In this pilot study, AI chatbot-based training and PRP demonstrated 
complementary strengths for OSCE preparation. While PRP appears effective for 
developing performance-based procedural and communication skills in a realistic 
setting, AI chatbots show potential for fostering clinical reasoning in a 
self-paced, reflective learning environment. These complementary strengths 
suggest a blended learning model, combining both methods, may be optimal for 
holistic clinical skills development. Further research is needed to validate 
these preliminary findings.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-08308-y
PMCID: PMC12750692
PMID: 41286823 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The study was conducted in accordance with the ethical principles 
of the Declaration of Helsinki, and the protocol was approved by Institutional 
Review Board (IRB) of Gil Korean Medicine Hospital, Gachon University (Approval 
No. GIRB-25-101). All participants were provided with a detailed explanation of 
the study objectives and procedures, and written informed consent to participate 
was obtained from all participants prior to their enrollment in the study. 
Consent for publication: Not applicable. Competing interests: The authors 
declare no competing interests.


92. Curr Opin Clin Nutr Metab Care. 2025 Nov 25. doi: 10.1097/MCO.0000000000001189. 
Online ahead of print.

Artificial intelligence-guided nutritional therapy in the ICU.

Peng D(1), Phongpreecha T(1)(2)(3)(4), Aghaeepour N(1)(2)(3)(4).

Author information:
(1)Takeoff41, Inc., Oakland.
(2)Department of Anesthesiology, Pain and Perioperative Medicine.
(3)Department of Pediatrics.
(4)Department of Biomedical Data Science, Stanford University, Stanford, 
California, USA.

PURPOSE OF REVIEW: Critical care nutrition remains a high-stakes and error-prone 
domain, particularly given the complex metabolic demands and heterogeneity of 
ICU populations. This review explores recent progress in integrating artificial 
intelligence with nutritional therapy in ICUs, highlighting its evolution and 
potential benefits in precision-guided support, along with current 
implementation challenges.
RECENT FINDINGS: Widely used in adult and neonatal ICUs, parenteral nutrition 
faces persistent challenges including ordering errors, practice variability, and 
insufficient robust long-term outcome evidence. Recent advances in machine 
learning have demonstrated considerable potential in predicting 
nutrition-related complications (e.g. neonatal morbidities, cholestasis, feeding 
intolerances, and malnutrition), optimizing nutrient delivery through dynamic, 
real-time recommendations, and enhancing clinical decision-making with large 
language models (LLMs) that synthesize clinical guidelines and patient data into 
actionable insights. However, future studies must establish causal relationships 
between optimal parenteral nutrition and long-term outcomes while addressing 
confounding factors and ingredient heterogeneity.
SUMMARY: Artificial intelligence-driven nutrition therapies have the potential 
to significantly improve the precision, safety, and personalization of ICU 
nutrition practices. Continued development and validation using standardized, 
comprehensive, longitudinal datasets, and validation in comparative clinical 
trials will be critical to realizing this transformative potential.

Copyright © 2025 The Author(s). Published by Wolters Kluwer Health, Inc.

DOI: 10.1097/MCO.0000000000001189
PMID: 41285167


93. IEEE J Biomed Health Inform. 2025 Nov 24;PP. doi: 10.1109/JBHI.2025.3636120. 
Online ahead of print.

TEENet: An Effective Clinical Detection Network for Identifying Spontaneous Echo 
Contrast Automatically.

Wu Z, Gu F, Wu J, Sun S, Ma C.

Spontaneous Echo Contrast (SEC) is a swirling smoke-like echo phenomenon in 
Transesophageal Echocardiography (TEE) videos caused by slow blood flow and 
hypercoagulable states. It is a significant indicator for assessing 
thromboembolic risk. However, current SEC identification requires extensive 
manual intervention, leading to low accuracy, high costs, and subjectivity. To 
address these issues, we propose TEENet, an effective clinical detection network 
for identifying SEC in TEE videos. Specifically, TEENet first generates 
attention maps for the input clips to highlight important regions and integrates 
Convolutional Neural Network with the Multi-Head Self-Attention to capture 
spatiotemporal representations. Furthermore, to enhance the classification 
performance across different SEC severity grades, we introduce an auxiliary 
classification module, which simultaneously utilizes the main classification 
head and auxiliary classification heads. Notably, we constructed a comprehensive 
dataset of 1106 TEE videos collected during clinical examinations performed at 
the First Affiliated Hospital of Soochow University from 2018 to 2023, providing 
a solid foundation for the development and validation of TEENet. Extensive 
experimental results demonstrate that our proposed network achieves the highest 
SEC identification accuracy of 92.4$\pm$1.3% compared to other spatiotemporal 
representation networks such as SlowFastR50 (89.6$\pm$0.7%) and TimeSformer 
(74.9$\pm$1.8%), which shows strong potential for effective auxiliary diagnosis 
in clinical practice.

DOI: 10.1109/JBHI.2025.3636120
PMID: 41284445


94. Res Sq [Preprint]. 2025 Nov 3:rs.3.rs-7714101. doi: 10.21203/rs.3.rs-7714101/v1.

Assessing the Capability of Large Language Models in Answering Pediatric 
Critical Care Board-Style Questions.

Chanci D(1)(2), Moore R(2), Foote HP(3), Goldstein MA(4), Kumar KR(3), Rotta 
AT(3), Hornik CP(3), Burriss-West M(3), Hamilton M(3), Kamaleswaran R(1)(2).

Author information:
(1)Department of Biomedical Engineering, Duke University, 2080 Duke University 
Road, Durham, 27708, NC, USA.
(2)Department of Surgery, Duke University School of Medicine, 40 Duke Medicine 
Circle, 27710, Durham, NC, USA.
(3)Department of Pediatrics, Duke University School of Medicine, 40 Duke 
Medicine Circle, 27710, Durham, NC, USA.
(4)Division of Pediatric Critical Care Medicine, Department of Pediatrics, NYU 
Grossman School of Medicine, 550 First Avenue, 10016, New York, NY, USA.

BACKGROUND: The potential of Large Language Models (LLMs) in medicine is often 
linked to massive, resource-intensive models. However, their practical 
application in specialized fields like pediatric critical care requires 
exploring the capability of more efficient, locally-deployable open-source 
alternatives. In this study, we evaluated the accuracy and clinical reasoning of 
open-source LLMs of varying sizes, specifically assessing if smaller, efficient 
models can perform comparably to larger ones on pediatric critical care 
multiple-choice questions.
METHODS: A set of 100 pediatric critical care MCQs across six clinical domains, 
i.e. calculation, diagnosis, ethics, management, pharmacology, and physiology, 
was curated by two pediatric specialists to evaluate eight open-source LLMs, 
ranging from 2 to 70 billion parameters. The LLMs were assessed using the 
overall and category-specific accuracy and clinical reasoning quality score 
based on a 5-points Likert scale. Additionally, two pediatric critical care 
fellows completed the MCQs for comparison. Cochran's Q test, McNemar's test, the 
Friedman test, and Cohen's kappa were used for the statistical analysis.
RESULTS: While the largest model (Llama-3.3-70B) achieved the highest accuracy 
(78%; 95% CI, 69%-86%), a key finding was the performance of the much smaller, 
14.7-billion parameter Phi-4. This efficient model was strikingly comparable, 
with 75% accuracy (95% CI, 65%-83%) and a similar reasoning score (4.40 vs 
4.49/5). Both models' performance was on par with pediatric critical care 
fellows. The LLMs excelled in ethics but struggled with calculations. 
Inter-rater reliability was excellent for the clinical reasoning assessment (κ = 
0.92).
CONCLUSIONS: Our findings demonstrate that smaller, efficient LLMs can approach 
the performance of much larger models and pediatric critical care fellows for 
complex pediatric critical care reasoning. This suggests a viable pathway for 
developing secure, locally-deployable decision support tools without relying on 
massive, proprietary systems. At the same time, these models hold potential as 
complementary resources for trainee education in pediatric critical care. 
However, their identified weaknesses, especially in calculations, underscore 
that rigorous, domain-specific validation is an essential prerequisite to ensure 
safe use in both clinical and educational contexts.
TRIAL REGISTRATION: Not applicable.

DOI: 10.21203/rs.3.rs-7714101/v1
PMCID: PMC12637821
PMID: 41282166

Conflict of interest statement: Competing interests: The authors declare that 
they have no competing interests.


95. Eur Arch Otorhinolaryngol. 2025 Nov 23. doi: 10.1007/s00405-025-09856-1. Online 
ahead of print.

Cautionary lessons from real-world testing of GPT-4.1 AI for pediatric foreign 
body aspiration.

Hack S(1), Attal R(2), Elazar D(3), Alon Y(4), Meyuchas R(5)(6), Livne A(7)(8), 
Madgar O(#)(9), Saban M(#)(9).

Author information:
(1)City St. George's University London School of Medicine, Program Delivered by 
University of Nicosia at the Chaim Sheba Medical Center, Ramat Gan, Israel. 
sholemhack1@gmail.com.
(2)City St. George's University London School of Medicine, Program Delivered by 
University of Nicosia at the Chaim Sheba Medical Center, Ramat Gan, Israel.
(3)Touro College of Osteopathic Medicine - Montana, MT, Great Falls, USA.
(4)Nursing Department, The Stanley Steyer School of Health Professions, Faculty 
of Medical and Health Sciences, Tel Aviv University, Tel Aviv, Ramat Aviv, 
69978, Israel.
(5)Department of Otolaryngology, Head and Neck Surgery, Sheba Medical Center, 
Tel Hashomer, Israel.
(6)Faculty of Medicine, Tel Aviv University, Tel Aviv, Israel.
(7)DeepVision Lab, Chaim Sheba Medical Center, Emek Haela St. 1, Ramat Gan, 
52621, Israel.
(8)Department of Diagnostic Imaging, Chaim Sheba Medical Center, Emek Haela St. 
1, Ramat Gan, 52621, Israel.
(9)Gray Faculty of Medical & Health Sciences, Tel-Aviv University, Tel-Aviv, 
Israel.
(#)Contributed equally

PURPOSE: To evaluate the feasibility and diagnostic performance of a multimodal 
large language model (GPT-4.1) in detecting pediatric airway foreign body 
aspiration (FBA) using real-world clinical and radiographic data.
METHODS: This retrospective cohort study included 58 pediatric patients 
evaluated for suspected airway FBA at a tertiary academic hospital between 2015 
and 2024. Each case combined structured clinical data and chest radiographs 
obtained at the time of emergency-department presentation, with bronchoscopy 
serving as the diagnostic reference standard. GPT-4.1, a vision-enabled large 
language model, classified cases as right-bronchus aspiration, left-bronchus 
aspiration, or no aspiration. Model performance was assessed using accuracy, 
precision, recall, and F1-score.
RESULTS: The model achieved an overall accuracy of 62.3%, with precision of 
23.3%, recall of 19.0%, and an F1-score of 0.21. While it correctly identified 
34 of 46 cases without aspiration, it detected only 4 of 12 confirmed 
bronchial-aspiration cases and missed all left-bronchus aspirations.
CONCLUSIONS: This proof-of-concept feasibility study highlights substantial 
limitations of a general-purpose multimodal AI model in pediatric airway triage. 
The low recall and high misclassification rates suggest that vision-enabled 
language models require task-specific training and rigorous validation before 
clinical implementation. Nevertheless, when used as an adjunct rather than a 
replacement for bronchoscopy, such models may eventually support triage 
decisions in resource-limited settings if further optimized and prospectively 
validated.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00405-025-09856-1
PMID: 41276667

Conflict of interest statement: Declarations. Ethical statement: This 
retrospective cohort study was conducted at a tertiary academic hospital and 
approved by the institutional review board (IRB protocol number 1761-24-SMC-D).


96. Sci Rep. 2025 Nov 21;15(1):41189. doi: 10.1038/s41598-025-15469-9.

An enhanced social emotional recognition model using bidirectional gated 
recurrent unit and attention mechanism with advanced optimization algorithms.

Alsudais TA(1), Swaileh A Alzaidi M(2), Eltahir MM(3), Ghaleb M(4), Al Sultan 
H(5), Yahya AE(6), Alshahrani M(7), Alzahrani MY(8).

Author information:
(1)Department of Teaching and Learning, College of Education and Human 
Development, Princess Nourah Bint Abdulrahman University, P.O. Box 84428, 
Riyadh, 11671, Saudi Arabia.
(2)Department of English Language, College of Language Sciences, King Saud 
University, P. O. Box 145111, Riyadh, Saudi Arabia.
(3)Department of Information Systems, Applied College at Mahayil, King Khalid 
University, Mahayil, Saudi Arabia.
(4)College of Computing and Information Technology, University of Bisha, Bisha, 
61922, Saudi Arabia.
(5)Department of English, College of Arts, King Faisal University, Hofuf, 
Al-Ahsa, Saudi Arabia.
(6)Department of Information Technology, College of Computing and Information 
Technology, Northern Border University, Arar, Saudi Arabia. 
Abdulsamad.qasem@nbu.edu.sa.
(7)Computer Department, Applied College, Najran University, Najran, 66462, Saudi 
Arabia.
(8)Saudi Electronic University, Riyadh, Saudi Arabia.

Social-emotional learning (SEL) is gradually becoming a region of attention for 
defining children's school readiness and forecasting academic success. It is the 
procedure of incorporating cognition, behaviour, and emotion into daily life. 
School structure contains systemic practices to integrate SEL into teaching and 
learning so that kids and adults construct social- and self-awareness, acquire 
the ability to handle their specific and other's feelings and behaviour, make 
reliable decisions, and build positive relations. Recent school-based programs 
have demonstrated that SEL greatly improves mental and physical health, academic 
success, moral judgment, citizenship, and motivation. This paper proposes a Deep 
Representation Model with Word Embedding and Optimization Algorithm for Social 
Emotional Recognition (DRMWE-OASER) methodology. The DRMWE-OASER methodology 
primarily aims to develop an effectual method for detecting social-emotional 
learning using advanced techniques. At first, the text pre-processing stage is 
applied at various levels to clean and convert text data into a meaningful and 
structured format. Moreover, the word embedding process is implemented using the 
TF-IDS method. Furthermore, a bidirectional gated recurrent unit with attention 
mechanism (BiGRU-AM) method is employed for classification. Finally, the 
improved whale optimizer algorithm (IWOA)-based hyperparameter selection process 
is utilized to optimize the classification results of the BiGRU-AM method. A 
wide range of experiments using the DRMWE-OASER approach is performed under 
emotion detection from text dataset. The experimental validation of the 
DRMWE-OASER approach portrayed a superior accuracy value of 99.50% over existing 
models.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-15469-9
PMCID: PMC12639160
PMID: 41271844 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


97. Drug Discov Today. 2025 Dec;30(12):104552. doi: 10.1016/j.drudis.2025.104552. 
Epub 2025 Nov 19.

A survey of the use of EHR as real-world evidence for discovering and validating 
new drug indications.

Talukdar N(1), Zhang X(2), Paithankar S(2), Wang H(3), Chen B(4).

Author information:
(1)Department of Biostatistics & Epidemiology, Michigan State University, East 
Lansing, MI, USA.
(2)Department of Pediatrics and Human Development, Michigan State University, 
Grand Rapids, MI, USA.
(3)Lumbrita LLC, Saratoga, CA, USA.
(4)Department of Biostatistics & Epidemiology, Michigan State University, East 
Lansing, MI, USA; Department of Pediatrics and Human Development, Michigan State 
University, Grand Rapids, MI, USA; Department of Pharmacology and Toxicology, 
Michigan State University, Grand Rapids, MI, USA; Center for AI-Enabled Drug 
Discovery, Michigan State University, Grand Rapids, MI, USA. Electronic address: 
chenbi12@msu.edu.

Electronic health records (EHRs) have been increasingly used as real-world 
evidence to support the discovery and validation of new drug indications. This 
paper surveys current approaches to EHR-based drug repurposing, covering data 
sources, processing methodologies, and representation techniques. It discusses 
study designs and statistical frameworks for evaluating drug efficacy. Key 
challenges and opportunities in validation are also discussed, with an emphasis 
on the role of large language models and target trial emulation. By synthesizing 
recent developments and methodological advances, this work provides a 
foundational resource for researchers aiming to translate real-world data into 
actionable drug repurposing evidence.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.drudis.2025.104552
PMID: 41270887 [Indexed for MEDLINE]


98. JMIR Form Res. 2025 Nov 20;9:e76618. doi: 10.2196/76618.

Teaching Clinical Reasoning in Health Care Professions Learners Using 
AI-Generated Script Concordance Tests: Mixed Methods Formative Evaluation.

Hudon A(1)(2)(3)(4)(5), Phan V(6)(7), Charlin B(5)(8), Wittmer R(9).

Author information:
(1)Department of Psychiatry and Addictology, Faculty of Medicine, Université de 
Montréal, Pavillon Roger-Gaudry, 2900 Bd Édouard-Montpetit Local L-315, 
Montréal, QC, H3T 1J4, Canada, 1 514 343 6111.
(2)Department of Psychiatry, Institut universitaire en santé mentale de 
Montréal, Montreal, QC, Canada.
(3)Centre de recherche de l'Institut universitaire en santé mentale de Montréal, 
Montreal, QC, Canada.
(4)Department of Psychiatry, Institut national de psychiatrie légale 
Philippe-Pinel, Montreal, QC, Canada.
(5)Groupe Interdisciplinaire de recherche sur la cognition et le raisonnement 
professionnel (GIRCoPRo), Université de Montréal, Montreal, QC, Canada.
(6)Department of Pediatrics, Faculty of Medicine, Université de Montréal, 
Montreal, QC, Canada.
(7)Department of Pediatrics, Department of Pediatrics, Centre Hospitalier 
Universitaire Sainte-Justine, Montreal, QC, Canada.
(8)Centre de pédagogie appliquée au sciences de la santé, Faculty of Medicine, 
Université de Montréal, Montreal, QC, Canada.
(9)Department of Family Medicine and Emergency Medicine, Faculty of Medicine, 
Université de Montréal, Montreal, QC, Canada.

BACKGROUND: The integration of artificial intelligence (AI) in medical education 
is evolving, offering new tools to enhance teaching and assessment. Among these, 
script concordance tests (SCTs) are well-suited to evaluate clinical reasoning 
in contexts of uncertainty. Traditionally, SCTs require expert panels for 
scoring and feedback, which can be resource-intensive. Recent advances in 
generative AI, particularly large language models (LLMs), suggest the 
possibility of replacing human experts with simulated ones, though this 
potential remains underexplored.
OBJECTIVE: This study aimed to evaluate whether LLMs can effectively simulate 
expert judgment in SCTs by using generative AI to author, score, and provide 
feedback for SCTs in cardiology and pneumology. A secondary objective was to 
assess students' perceptions of the test's difficulty and the pedagogical value 
of AI-generated feedback.
METHODS: A cross-sectional, mixed methods study was conducted with 25 
second-year medical students who completed a 32-item SCT authored by ChatGPT-4o 
(OpenAI). Six LLMs (3 trained on the course material and 3 untrained) served as 
simulated experts to generate scoring keys and feedback. Students answered SCT 
questions, rated perceived difficulty, and selected the most helpful feedback 
explanation for each item. Quantitative analysis included scoring, difficulty 
ratings, and correlations between student and AI responses. Qualitative comments 
were thematically analyzed.
RESULTS: The average student score was 22.8 out of 32 (SD 1.6), with scores 
ranging from 19.75 to 26.75. Trained AI systems showed significantly higher 
concordance with student responses (ρ=0.64) than untrained models (ρ=0.41). 
AI-generated feedback was rated as most helpful in 62.5% of cases, especially 
when provided by trained models. The SCT demonstrated good internal consistency 
(Cronbach α=0.76), and students reported moderate perceived difficulty (mean 
3.7, SD 1.1). Qualitative feedback highlighted appreciation for SCTs as 
reflective tools, while recommending clearer guidance on Likert-scale use and 
more contextual detail in vignettes.
CONCLUSIONS: This is among the first studies to demonstrate that trained 
generative AI models can reliably simulate expert clinical reasoning within a 
script-concordance framework. The findings suggest that AI can both streamline 
SCT design and offer educationally valuable feedback without compromising 
authenticity. Future studies should explore longitudinal effects on learning and 
assess how hybrid models (human and AI) can optimize reasoning instruction in 
medical education.

© Alexandre Hudon, Véronique Phan, Bernard Charlin, René Wittmer. Originally 
published in JMIR Formative Research (https://formative.jmir.org).

DOI: 10.2196/76618
PMCID: PMC12634011
PMID: 41264864 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


99. JMIR Med Educ. 2025 Nov 20;11:e80084. doi: 10.2196/80084.

AI-Generated "Slop" in Online Biomedical Science Educational Videos: Mixed 
Methods Study of Prevalence, Characteristics, and Hazards to Learners and 
Teachers.

Jones EM(1), Newman JD(1), Kim B(2), Fogle EJ(3).

Author information:
(1)Department of Foundational Medical Studies, Oakland University William 
Beaumont School of Medicine, 586 Pioneer Drive, Rochester, MI, 48309, United 
States, 1 248-370-3731.
(2)Department of Human Development and Child Studies, Oakland University, 
Rochester, MI, United States.
(3)Department of Chemistry and Biochemistry, California Polytechnic State 
University, San Luis Obispo, CA, United States.

BACKGROUND: Video-sharing sites such as YouTube (Google) and TikTok (ByteDance) 
have become indispensable resources for learners and educators. The recent 
growth in generative artificial intelligence (AI) tools, however, has resulted 
in low-quality, AI-generated material (commonly called "slop") cluttering these 
platforms and competing with authoritative educational materials. The extent to 
which slop has polluted science education video content is unknown, as are the 
specific hazards to learning from purportedly educational videos made by AI 
without the use of human discretion.
OBJECTIVE: This study aimed to advance a formal definition of slop (based on the 
recent theoretical construct of "careless speech"), to identify its qualitative 
characteristics that may be problematic for learners, and to gauge its 
prevalence among preclinical biomedical science (medical biochemistry and cell 
biology) videos on YouTube and TikTok. We also examined whether any quantitative 
features of video metadata correlate with the presence of slop.
METHODS: An automated search of publicly available YouTube and TikTok videos 
related to 10 search terms was conducted in February and March 2025. After 
exclusion of duplicates, off-topic, and non-English results, videos were 
screened, and those suggestive of AI were flagged. The flagged videos were 
subject to a 2-stage qualitative content analysis to identify and code 
problematic features before an assignment of "slop" was made. Quantitative 
viewership data on all videos in the study were scraped using automated tools 
and compared between slop videos and the overall population.
RESULTS: We define "slop" according to the degree of human care in production. 
Of 1082 videos screened (814 YouTube, 268 TikTok), 57 (5.3%) were deemed 
probably AI-generated and low-quality. From qualitative analysis of these and 6 
additional AI-generated videos, we identified 16 codes for problematic aspects 
of the videos as related to their format or contents. These codes were then 
mapped to the 7 characteristics of careless speech identified earlier. Analysis 
of view, like, and comment rates revealed no significant difference between slop 
videos and the overall population.
CONCLUSIONS: We find slop to be not especially prevalent on YouTube and TikTok 
at this time. These videos have comparable viewership statistics to the overall 
population, although the small dataset suggests this finding should be 
interpreted with caution. From the slop videos that were identified, several 
features inconsistent with best practices in multimedia instruction were 
defined. Our findings should inform learners seeking to avoid low-quality 
material on video-sharing sites and suggest pitfalls for instructors to avoid 
when making high-quality educational materials with generative AI.

© Eric M Jones, Jane D Newman, Boyun Kim, Emily J Fogle. Originally published in 
JMIR Medical Education (https://mededu.jmir.org).

DOI: 10.2196/80084
PMCID: PMC12634010
PMID: 41264860 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


100. Arch Dis Child. 2025 Nov 19:archdischild-2025-329505. doi: 
10.1136/archdischild-2025-329505. Online ahead of print.

Learning from the past, structuring the future: using large language models to 
unlock a century of paediatric research in Archives of Disease in Childhood.

Green Z(1)(2), Ashton JJ(3)(2), Beattie RM(2).

Author information:
(1)Department of Human Genetics and Genomic Medicine, University of Southampton, 
Southampton, UK zachary.green@soton.ac.uk.
(2)Department of Paediatric Gastroenterology, Southampton Children's Hospital, 
Southampton, UK.
(3)Department of Human Genetics and Genomic Medicine, University of Southampton, 
Southampton, UK.

BACKGROUND AND AIMS: The centenary of Archives of Disease in Childhood (ADC) 
presents an opportunity to reflect on a century of paediatric research and 
consider how best to leverage this ever-growing repository for future use. While 
content is indexed via PubMed and medical subject headings terms, this provides 
a superficial representation of complex journal content, leading to limited 
accessibility. We discuss the potential utility of large language models 
(LLMs)-advanced artificial intelligence systems that can understand, summarise 
and generate human-like language-and demonstrate their feasibility for 
structuring historical ADC articles, proposing a future pipeline to enhance 
indexing, retrieval and discoverability.
METHODS: For demonstrative purposes, five articles from ADC December 1999 issue 
were locally downloaded and processed using a closed deployment of an LLM, 
Mistral (V.0.3, 7B). A structured prompt was used to extract key metadata. 
Outputs were manually compared with source texts and scored for accuracy. 
Hallucinations, fabricated or incorrect outputs, were recorded.
RESULTS: The LLM achieved a mean accuracy of 86.9%, aligning with previous 
benchmarks for medical research assistance. No hallucinations were identified. 
Some repetition and verbosity were noted, likely due to chunk-based processing, 
but key fields were accurately extracted when explicitly present.
CONCLUSION: ADC holds a vast but underutilised body of research. This article 
shows that lightweight, locally hosted LLMs could structure ADC content without 
compromising intellectual property. Such methods could enable improved access, 
support automation of systematic reviews and enhance discoverability through 
biomedical ontologies, laying the foundation for a searchable, semantically 
enriched Archives that bridges historical insight with modern research needs.

© Author(s) (or their employer(s)) 2025. No commercial re-use. See rights and 
permissions. Published by BMJ Group.

DOI: 10.1136/archdischild-2025-329505
PMID: 41260630

Conflict of interest statement: Competing interests: JJA is a senior advisory 
board member for Orchard Therapeutics. The remaining authors have no competing 
interests.


101. Radiol Artif Intell. 2026 Jan;8(1):e250090. doi: 10.1148/ryai.250090.

Economic Value of AI in Radiology: A Systematic Review.

Molwitz I(1), Ristow I(1), Erley J(1), Akinci D'Antonoli T(2)(3)(4), Tejani 
AS(5), Klontzas ME(6), Huisman M(7), Adam G(1), Nüesch S(8), Adams L(9).

Author information:
(1)Department of Diagnostic and Interventional Radiology and Nuclear Medicine, 
University Medical Center Hamburg-Eppendorf, Martinistrasse 52, 20246 Hamburg, 
Germany.
(2)Department of Radiology and Nuclear Medicine, University Hospital Basel, 
Basel, Switzerland.
(3)Now with Department of Diagnostic and Interventional Radiology, University 
Hospital Basel, Basel, Switzerland.
(4)Now with Department of Pediatric Radiology, University Children's Hospital 
Basel, Basel, Switzerland.
(5)Department of Radiology and Biomedical Imaging, University of California, San 
Francisco, San Francisco, Calif.
(6)Artificial Intelligence and Translational Imaging Lab, Department of 
Radiology, School of Medicine, University of Crete, Heraklion, Greece.
(7)Department of Radiology and Nuclear Medicine, Radboud University Medical 
Center, Nijmegen, the Netherlands.
(8)International Institute of Management in Technology, University of Fribourg, 
Fribourg, Switzerland.
(9)Department of Diagnostic and Interventional Radiology, Klinikum rechts der 
Isar, TUM University Hospital, Munich, Germany.

Comment in
    Radiol Artif Intell. 2026 Jan;8(1):e250925. doi: 10.1148/ryai.250925.

Purpose To summarize the evidence of artificial intelligence's (AI's) economic 
value across the radiologic workflow. Materials and Methods A comprehensive 
search of PubMed, Business Source Ultimate, and EconLit was conducted for 
original research articles published between January 2010 and November 2024. 
Medical Subject Headings and keywords included "artificial intelligence/machine 
learning/deep learning/natural language processing," "radiology," and "economic 
value/cost/budget/revenue/efficiency." Studies were selected based on explicit 
quantification of economic outcomes, excluding those with only soft outcome 
criteria like time savings without cost quantification. Study quality was 
assessed using the Criteria for Health Economic Quality Evaluation. Results From 
the initial 1879 search results, 21 studies (1%) met the inclusion criteria. The 
majority evaluated machine learning tools (10 of 21[48%], nine on deep 
learning), followed by computer-assisted diagnosis (CAD, seven of 21 [33%]), 
natural language processing (NLP, two of 21 [10%]), and hypothetical AI models 
(two of 21 [10%]). AI demonstrated economic value through cost savings or 
incremental cost-effectiveness ratios in resource-intensive tasks, when accuracy 
matched human performance and costs were fixed. For instance, AI-based lung 
cancer screening achieved incremental cost savings of up to $242 U.S. dollars 
(USD) per patient. AI increased costs when specificity was lower than humans' or 
when using pay-per-use models, as observed with CAD systems raising mammography 
screening costs by up to $19 USD per patient. In fast tasks such as radiograph 
evaluations, AI showed value in settings with radiologist shortages. AI reduced 
costs through protocol optimization and increased revenue via improved follow-up 
compliance. Conclusion AI's value in radiology is context dependent, varying 
with task complexity, examination volume, and implementation model. Further 
high-quality economic evaluations are essential. Keywords: Cost-effectiveness, 
Efficacy Studies, Artificial Intelligence, Radiology, Healthcare Economics, 
Systematic Review Supplemental material is available for this article. © The 
Author(s) 2025. Published by the Radiological Society of North America under a 
CC BY-NC-ND license. See also the commentary by Amindarolzarbi and Siegel in 
this issue.

DOI: 10.1148/ryai.250090
PMID: 41258794 [Indexed for MEDLINE]


102. Sci Rep. 2025 Nov 18;15(1):40552. doi: 10.1038/s41598-025-24243-w.

Auxiliary diagnostic method for children with autism spectrum disorder based on 
virtual reality and eye-tracking technology.

Chen H(#)(1), Zhang X(#)(1), Chen Z(2), Ren Y(2), Liu R(3).

Author information:
(1)College of Computer and Information Engineering, Nanjing Tech University, 
Nanjing, 211816, China.
(2)School of Computer Science, Nanjing University of Information Science and 
Technology, Nanjing, 210044, China.
(3)School of Computer Science, Nanjing University of Information Science and 
Technology, Nanjing, 210044, China. runzeliu1025@126.com.
(#)Contributed equally

In the behavioral analysis of children with Autism Spectrum Disorder (ASD), 
virtual reality (VR)-based eye-tracking technology offers a precise method for 
assessing social and cognitive characteristics. It overcomes the limitations of 
traditional diagnostic methods, such as clinician subjectivity and experience 
bias. VR also addresses ASD-related challenges like attention instability and 
emotional variability during social interactions. This paper combines 
eye-tracking with VR environments to analyze gaze patterns in children with ASD. 
It proposes a new diagnostic framework to improve objectivity and accuracy.The 
gaze estimation model integrates head and eye movement data to predict gaze 
direction. It enhances precision using binocular fusion and employs multi-scale 
convolutional kernels to extract hierarchical eye movement features. The model 
simplifies network connections to retain essential information. A lightweight 
Transformer architecture models long-range temporal dependencies in eye 
movements. A Bayesian decision model is used to classify fixations, saccades, 
and smooth pursuit.To test the model, an emotion recognition task was designed 
in a WebVR environment. Gaze data from children with ASD were collected, key 
features were extracted, and abnormal patterns were identified for diagnostic 
support. The experimental results showed an 85.88% accuracy rate. This confirms 
the effectiveness of combining VR and eye-tracking technology in ASD diagnosis, 
advancing intelligent medical tools, and reducing reliance on subjective 
clinical judgment.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-24243-w
PMCID: PMC12627623
PMID: 41253970 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


103. Adv Exp Med Biol. 2026;1489:477-484. doi: 10.1007/978-3-032-03394-9_45.

Artificial Intelligence and Mental Health of Children and Adolescents: Current 
Status and Perspectives.

Dritsona A(1), Koutroumpa ME(1), Sergentanis TN(2), Tsitsika AK(3).

Author information:
(1)"Strategies of Developmental and Adolescent Health", 2nd Department of 
Pediatrics, "P. & A. Kyriakou" Children's Hospital, School of Medicine, National 
and Kapodistrian University of Athens, Athens, Greece.
(2)Department of Public Health Policy, School of Public Health, Athens, Greece.
(3)"Strategies of Developmental and Adolescent Health", 2nd Department of 
Pediatrics, "P. & A. Kyriakou" Children's Hospital, School of Medicine, National 
and Kapodistrian University of Athens, Athens, Greece. info@youth-health.gr.

BACKGROUND/AIM: Artificial Intelligence (AI) involves creating systems capable 
of performing tasks typically requiring human cognitive functions, such as 
learning, reasoning, and decision-making. Over time, the field advanced toward 
more specialized, practical applications. Key AI areas include problem-solving, 
knowledge representation, decision-making, and natural language processing. This 
literature review aims to synthesize studies on the implementation of AI in the 
field of mental health of children and adolescents, tracing the current status 
and perspectives.
METHODS: Eligible studies were sought in PubMed, Google Scholar, PsycInfo; 
end-of-search was set at May 31, 2024.
RESULTS: AI is being increasingly utilized in the field of mental health to 
assist in diagnosing and managing conditions, such as mood disorders, autism 
spectrum disorders, and other mental health issues in children and adolescents. 
Technologies, such as mobile applications and wearable devices analyze 
behaviors, speech patterns, and emotions, offering valuable insights to forecast 
mood shifts and detect early signs of mental health problems. These tools 
support users in managing their emotions and stress, making mental health care 
more personalized and easier to access. Virtual therapists and AI chatbots offer 
private, convenient support, allowing young people to tackle their mental health 
concerns at their own convenience.
CONCLUSIONS: AI has the potential to improve mental health care by advancing 
diagnosis, treatment, and early intervention. However, its integration must be 
done thoughtfully, with continuous research to address ethical and technical 
issues, including data privacy, algorithm biases, the need for human oversight, 
obtention of informed consent, transparency, and the avoidance of 
discrimination. Collaboration among various professionals is key to ensuring 
that AI is used in a responsible and effective way to enhance mental health care 
systems.

© 2026. The Author(s), under exclusive license to Springer Nature Switzerland 
AG.

DOI: 10.1007/978-3-032-03394-9_45
PMID: 41252032 [Indexed for MEDLINE]


104. J Med Internet Res. 2025 Nov 17;27:e68573. doi: 10.2196/68573.

Application of Narrative and AI-Assisted Follow-Up After Voluntary Medical Male 
Circumcision: Multicenter, Double-Blind, Prospective, Randomized Controlled 
Trial.

Wang L(#)(1), Peng Y(#)(2), Zhang G(#)(1), Huang Y(#)(1), Yang G(1), Jiang Y(3), 
Xu Q(4), Yu J(5), Jin J(6), Qiao H(7), Wu Q(7), Wei Z(8), Tang W(1), Liu J(1).

Author information:
(1)Department of Urology, First Affiliated Hospital of Chongqing Medical 
University, No. 1 Youyi Road, Chongqing, Yuzhong District, 400016, China, 86 
18696682178.
(2)Department of Urology, Peking Union Medical College Hospital, Beijing, China.
(3)Department of Urology, First Hospital of Jilin University, Changchun, China.
(4)Department of Urology, Yongchuan Hospital of Chongqing Medical University, 
Chongqing, China.
(5)School of Nursing, Xiangya Medical School, Sichuan, China.
(6)Department of Psychiatry, Chongqing Cancer Hospital, Chongqing, China.
(7)School of Nursing, Chongqing Medical University, Chongqing, China.
(8)School of the First Clinical Medicine, Chongqing Medical University, 
Chongqing, China.
(#)Contributed equally

BACKGROUND: Postoperative anxiety following voluntary medical male circumcision 
(VMMC) poses a significant health challenge, with limited telemedicine access 
and inadequate communication compromising recovery and adherence. 
Narrative-based interventions have shown promise in reducing psychological 
distress in other contexts, and large language models may enable automated 
follow-up, but their role in VMMC care remains underexplored.
OBJECTIVE: We evaluated the effect of a narrative-enhanced tool (NET) on 
anxiety, sleep quality, quality of life, and pain management and identified risk 
factors for postoperative anxiety. We also assessed the feasibility of an 
artificial intelligence-assisted consultation (AAC) system in improving 
follow-up efficiency.
METHODS: From October 1, 2023, to April 29, 2024, patients aged ≥15 years 
undergoing VMMC were recruited and randomized 1:1 to a standardized risk tool 
(SRT) or NET group. In addition to the routine postoperative communication, the 
NET group received a narrative video highlighting positive recovery experiences. 
Both groups accessed an AAC chatbot for automated follow-ups. Primary outcomes 
were anxiety levels measured by the 7-item Generalized Anxiety Disorder scale 
(GAD-7), sleep quality measured by Pittsburgh Sleep Quality Index, quality of 
life measured by 3-level EuroQoL 5D questionnaire, and pain levels measured by 
Numerical Rating Scale. Secondary outcomes included analgesic use, satisfaction, 
and health care worker efficiency. Repeated measures ANOVA assessed trends and 
regression identified risk factors for anxiety.
RESULTS: Between October 1, 2023, and April 29, 2024, 671 eligible participants 
were enrolled, with 388 completing the 30-day follow-up (SRT group: n=189, mean 
age 26.21, SD 3.69 years; NET group: n=199, mean age 26.41, SD 3.56 years; 
P=.60). Both groups exhibited increased anxiety levels, diminished quality of 
life, and poorer sleep quality during the 30-day postoperative period. However, 
compared to SRT, the NET group demonstrated lower GAD-7 scores (7.06, SD 2.73 
vs. 9.95, SD 3.50; P<.001), improved sleep quality (12.29, SD 3.57 vs 13.20, SD 
3.54; P=.01), higher quality of life scores (0.87, SD 0.07 vs 0.84, SD 0.09; 
P<.001), more regular analgesic use (154/173, 89.02% vs 100/169, 59.17%; 
P<.001), reduced opioid consumption (5/173, 2.89% vs 25/169, 14.79%; P<.001), 
and higher pain medication satisfaction (4.21, SD 0.69 vs 3.76, SD 0.97; 
P<.001). Multivariate analysis identified SRT assignment, inability to recall 
opioid risk levels, hematoma, swelling, and pain as independent risk factors for 
elevated GAD-7 scores. Implementation of the AAC substantially reduced health 
care worker follow-up time (2.34, SD 1.95 min vs 7.85, SD 2.65 min; P<.001).
CONCLUSIONS: The study demonstrates that narrative is effective in reducing 
anxiety, improving quality of life, and improving pain management post-VMMC. The 
integration of artificial intelligence into clinical follow-up protocols has the 
potential to enhance health care worker efficiency without compromising patient 
satisfaction.

© Linfeng Wang, Yueqiang Peng, Gaojie Zhang, Yong Huang, Guo Yang, Yu Jiang, 
Qiao Xu, Jiang Yu, Jiajia Jin, Hong Qiao, Qingyu Wu, Ziling Wei, Wei Tang, Jiayu 
Liu. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/68573
PMCID: PMC12622909
PMID: 41248462 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


105. Turk J Biol. 2025 Oct 11;49(5):585-599. doi: 10.55730/1300-0152.2765. 
eCollection 2025.

Artificial intelligence in medical and biological research: promise and perils 
of ChatGPT and DeepSeek in advancing healthcare.

Kayaalp ME(1)(2)(3), Gültekin O(1), Akçaalan S(4), Kahraman HÇ(1), Topçu HN(5), 
Kavrul Kayaalp G(6).

Author information:
(1)Department of Orthopaedics and Traumatology, University of Health Sciences, 
İstanbul Fatih Sultan Mehmet Training and Research Hospital, İstanbul, Turkiye.
(2)Department of Orthopaedics and Traumatology, Brandenburg Medical School 
Theodor Fontane, University Hospital Brandenburg/Havel, Brandenburg/Havel, 
Germany.
(3)Faculty of Health Sciences Brandenburg, Brandenburg Medical School Theodor 
Fontane, Brandenburg/Havel, Germany.
(4)Department of Orthopaedics and Traumatology, Ankara City Hospital, Ankara, 
Turkiye.
(5)Department of Orthopaedics and Traumatology, Istanbul Kartal Dr. Lütfi Kırdar 
City Hospital, İstanbul, Turkiye.
(6)Department of Paediatric Rheumatology, Pendik Research and Training Hospital, 
Marmara University, İstanbul, Turkiye.

BACKGROUND/AIM: Artificial intelligence (AI), particularly large language models 
(LLMs) such as ChatGPT and DeepSeek, is being increasingly applied in clinical 
care, research, and education. The aim of this review is to examine how these 
tools may transform the conduct of medical and biological research and to define 
their limitations.
MATERIALS AND METHODS: A narrative synthesis of the literature was performed, 
encompassing studies published between 2020 and 2025. Peer-reviewed journals, 
systematic reviews, and high-impact original research articles were included to 
ensure an evidence-based overview. The principle applications, validation 
metrics, and clinical implications across orthopedics, oncology, cardiology, 
internal medicine, and the biological sciences were analyzed.
RESULTS: LLMs demonstrate strong potential in supporting physicians during 
clinical decision-making, enhancing patient education, and assisting researchers 
in their work. They are valuable for language-related tasks and for generating 
structured, clear, and comprehensible content. However, concerns persist 
regarding data privacy, algorithmic bias, factual accuracy, and excessive 
dependence on data-driven outputs. Responsible implementation requires 
safeguards such as human oversight, model transparency, and domain-specific 
training.
CONCLUSION: AI tools such as ChatGPT, DeepSeek, and similar models are 
transforming the way healthcare is delivered and studied. Their current 
capabilities appear highly promising. However, clinicians, technical experts, 
and policymakers must collaborate to ensure the safe, equitable, effective, and 
ethical integration of these technologies into real-world healthcare workflows.

© TÜBİTAK.

DOI: 10.55730/1300-0152.2765
PMCID: PMC12614363
PMID: 41246235

Conflict of interest statement: Conflict of interest: O.G., S.A., H.C.K., 
H.N.T., and G.K.K. declare no conflicts of interest. M.E.K. serves as an 
associate editor of Knee Surgery, Sports Traumatology, Arthroscopy (KSSTA), a 
member of the U–45 Committee of the European Society of Sports Traumatology, 
Knee Surgery and Arthroscopy (ESSKA), and a coeditor of the Joint Diseases and 
Related Surgery.


106. Digit Health. 2025 Nov 13;11:20552076251389992. doi: 10.1177/20552076251389992. 
eCollection 2025 Jan-Dec.

Evaluating empathy in GPT-4-generated vs. physician-written emergency department 
discharge letters.

Ben-Haim G(1)(2)(3), Livne A(3)(4), Manor U(3)(5), Hochstein D(3)(5), Saban 
M(3)(6), Blaier O(1), Iram YA(1), Balzam MG(1), Lutenberg A(1), Eyade R(1), 
Qassem R(1), Trabelsi D(3)(7), Dahari Y(3)(7), Eisenmann BZ(1), Shechtman Y(8), 
Nadkarni GN(9)(10), Glicksberg BS(9)(10), Zimlichman E(3)(11), Perry A(12), 
Klang E(9)(10).

Author information:
(1)Department of Emergency Medicine, Sheba Medical Center, Ramat-Gan, Israel.
(2)Emergency Oncology Department, Sheba Medical Center, Ramat-Gan, Israel.
(3)Faculty of Medicine, Tel Aviv University, Tel Aviv, Israel.
(4)Division of Diagnostic Imaging, Sheba Medical Center, Tel Hashomer, Israel.
(5)Internal Medicine "C" Department, Sheba Medical Center, Tel HaShomer, Israel.
(6)The Gertner Institute for Health Policy and Epidemiology, Sheba Medical 
Center, Ramat Gan, Israel.
(7)The Edmond and Lily Safra Children Hospital, Sheba Medical Center, Tel 
Hashomer, Israel.
(8)Bnei-Zion Medical Center, Haifa, Israel.
(9)The Division of Data-Driven and Digital Medicine (D3M), Icahn School of 
Medicine at Mount Sinai, New York, NY, USA.
(10)The Charles Bronfman Institute of Personalized Medicine, Icahn School of 
Medicine at Mount Sinai, New York, NY, USA.
(11)Central Management, Sheba Medical Center, Tel Hashomer, Israel.
(12)Psychology Department, The Hebrew University of Jerusalem, Jerusalem, 
Israel.

BACKGROUND AND AIMS: Empathy improves clinical outcomes, patient satisfaction, 
and adherence to treatment. Few studies have explored the real-world use of 
large language models in conveying empathy. We compared the empathy in emergency 
department (ED) discharge letters written by GPT-4 and physicians.
METHODS: We conducted a retrospective, blinded, comparative study in a tertiary 
ED. All patients discharged for one 8-h shift were included. For each patient, 
we compared the original ED discharge letter to a GPT-4 generated letter. GPT-4 
generated the letters using ED notes, excluding the original discharge letter. 
Seventeen evaluators (seven physicians, five nurses, five patients) compared the 
letters side by side. They were blinded to the source. Evaluators first chose 
between the AI and human letters. Then they rated each letter for empathy, 
overall quality, clarity of summary, and clarity of recommendations using a 
5-point Likert scale.
RESULTS: Evaluators preferred GPT-4 over physician letters in 83.7% of 
comparisons (1009 vs. 197; p < 0.001). GPT-4 letters received higher scores for 
empathy (median 4.0 vs. 3.0; p < 0.001), overall quality, and clarity of summary 
across all evaluator groups. Among patients, no significant difference was found 
in the clarity of recommendations (p = 0.771). Qualitative analysis showed that 
GPT-4's empathetic expressions, though sometimes generic, were perceived as 
effective.
CONCLUSION: GPT-4 shows strong potential in generating empathetic ED discharge 
letters. These letters are preferred by healthcare professionals and patients. 
GPT-4 offers a promising tool to reduce the workload of ED physicians. Further 
research is necessary to explore patient perceptions and best practices for 
integrating AI with physicians in clinical practice.

© The Author(s) 2025.

DOI: 10.1177/20552076251389992
PMCID: PMC12615915
PMID: 41246211

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


107. JAMIA Open. 2025 Nov 12;8(6):ooaf128. doi: 10.1093/jamiaopen/ooaf128. 
eCollection 2025 Dec.

A comparative performance analysis of regular expressions and a large language 
model-based approach to extract the BI-RADS score from radiological reports.

Dennstädt F(1)(2), Lerch L(3), Schmerder M(3), Cihoric N(1), Cereghetti GM(3), 
Gaio R(1), Bonel H(3)(4), Filchenko I(5), Hastings J(2)(6)(7), Dammann F(3), 
Aebersold DM(1), von Tengg-Kobligk H(3), Nairz K(3).

Author information:
(1)Department of Radiation Oncology, Inselspital, Bern University Hospital and 
University of Bern, Bern 3010, Switzerland.
(2)School of Medicine, University of St. Gallen, St. Gallen 9000, Switzerland.
(3)Department of Diagnostic, Interventional and Pediatric Radiology (DIPR), 
Inselspital, Bern University Hospital, University of Bern, Bern 3010, 
Switzerland.
(4)Department of Radiology, Lindenhof Hospital, Bern 3012, Switzerland.
(5)Department of Neurology, Inselspital, Bern University Hospital and University 
of Bern, Bern 3010, Switzerland.
(6)Institute for Implementation Science in Health Care, University of Zurich, 
Zürich 8006, Switzerland.
(7)Swiss Institute of Bioinformatics, Lausanne, Vaud 1015, Switzerland.

BACKGROUND: Different natural language processing (NLP) techniques have 
demonstrated promising results for data extraction from radiological reports. 
Both traditional rule-based methods like regular expressions (Regex) and modern 
large language models (LLMs) can extract structured information. However, 
comparison between these approaches for extraction of specific radiological data 
elements has not been widely conducted.
METHODS: We compared accuracy and processing time between Regex and LLM-based 
approaches for extracting Breast Imaging-Reporting and Data System (BI-RADS) 
scores from 7764 radiology reports (mammography, ultrasound, MRI [magnetic 
resonance imaging], and biopsy). We developed a rule-based algorithm using Regex 
patterns and implemented an LLM-based extraction using the 
Rombos-LLM-V2.6-Qwen-14b model. A ground truth dataset of 199 manually 
classified reports was used for evaluation.
RESULTS: We did not detect a statistically significant difference in the 
accuracy in extracting BI-RADS scores between Regex and an LLM-based method 
(accuracy of 89.20% for Regex vs 87.69% for the LLM-based method; P = .56, 
effect size w = 0.04; post-hoc power = 0.11). Compared to the LLM-based method, 
Regex processing was more efficient, completing the task 28 120 times faster 
(0.06 seconds vs 1687.20 seconds). Further analysis revealed that LLMs favored 
common classifications (particularly BI-RADS value of 2) while Regex more 
frequently returned "unclear" values. We also could confirm in our sample an 
already known laterality bias for breast cancer (BI-RADS 6) and detected a 
slight laterality skew for suspected breast cancer (BI-RADS 5) as well.
CONCLUSION: For structured, standardized data like BI-RADS, traditional NLP 
techniques seem to be superior, though future work should explore hybrid 
approaches combining Regex precision for standardized elements with LLM 
contextual understanding for more complex information extraction tasks.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamiaopen/ooaf128
PMCID: PMC12612664
PMID: 41245151

Conflict of interest statement: N.C. is a technical lead for the SmartOncology 
project and medical advisor for Wemedoo AG, Steinhausen AG, Switzerland. The 
authors declare no other conflicts of interest.


108. BMC Med Inform Decis Mak. 2025 Nov 14;25(1):423. doi: 
10.1186/s12911-025-03258-3.

Can general purpose large language models assist pediatricians in predicting 
infants with serious bacterial infection?

Šimunović I(1), Rezić K(1), Franić N(2), Boduljak G(3), Batinić M(4), Jukić 
I(4), Jelovina I(4), Biočić J(1)(4), Pogorelić Z(5)(6), Markić J(7)(8).

Author information:
(1)School of Medicine, University of Split, Split, 21000, Croatia.
(2)Faculty of Electrical Engineering, Mechanical Engineering and Naval 
Architecture, University of Split, Split, 21000, Croatia.
(3)Department of Computer Science, University of Oxford, Oxford, UK.
(4)Department of Pediatrics, University Hospital of Split, Split, 21000, 
Croatia.
(5)School of Medicine, University of Split, Split, 21000, Croatia. 
zenon.pogorelic@mefst.hr.
(6)Department of Pediatric Surgery, University Hospital of Split, Split, 21000, 
Croatia. zenon.pogorelic@mefst.hr.
(7)School of Medicine, University of Split, Split, 21000, Croatia. 
jmarkic@mefst.hr.
(8)Department of Pediatrics, University Hospital of Split, Split, 21000, 
Croatia. jmarkic@mefst.hr.

BACKGROUND: Serious Bacterial Infection (SBI) in neonates and young infants 
often exhibit nonspecific symptoms and clinical signs in the early stages of 
illness, making early diagnosis challenging. Timely recognition and appropriate 
treatment are essential to prevent adverse outcomes. While several clinical 
algorithms are widely used for SBI risk stratification, these tools have 
limitations, particularly low positive predictive value. This study evaluates 
the diagnostic accuracy of general-purpose large language models (LLMs) in 
detecting SBI in neonates and infants under 90 days of age admitted to the 
emergency department. Our objective is to improve diagnostic precision, reduce 
unnecessary interventions, and enhance patient outcomes. LLM performance was 
compared against traditional machine learning models, state-of-the-art 
rule-based methods, and an ensemble of physicians to assess their potential as 
clinical decision-support tools in scenarios of diagnostic uncertainty.
RESULTS: On a dataset of 742 patients, LLMs demonstrated diagnostic accuracy 
comparable to traditional machine learning models and state-of-the-art 
rule-based methods. The optimized CatBoost (class-weighted) model achieved the 
best overall performance, with a PPV of 0.70, NPV of 0.90, sensitivity of 0.54, 
specificity of 0.95, F1-score of 0.60, and MCC of 0.54, outperforming the 
baseline CatBoost model and achieving results on par with large language models 
(LLMs) and physicians. When optimally prompted, LLMs performed on par with 
ensembles of experienced clinicians. Additionally, LLMs exhibited effective 
medical reasoning and provided credible diagnostic predictions, particularly 
valuable in cases of clinician uncertainty. The models achieved balanced 
performance across multiple evaluation metrics, including PPV, NPV, sensitivity, 
specificity, F1-score, and Matthew’s correlation coefficient (MCC). ChatGPT-4o 
achieved a sensitivity of 0.65 and specificity of 0.83, with an MCC of 0.41. 
Claude Sonnet 3.5 reached a sensitivity of 0.60 and specificity of 0.86, MCC 
0.42 and Google Gemini 2.0 Flash had lower sensitivity (0.43) but the highest 
specificity (0.94), with an MCC of 0.43. In comparison, the best-performing 
individual pediatrician achieved a higher sensitivity (0.74) but lower 
specificity (0.68), with an MCC of 0.33, while the pediatricians’ majority vote 
yielded sensitivity of 0.69, specificity of 0.81, and MCC of 0.43 — comparable 
to the top-performing LLMs.
CONCLUSIONS: These Artificial intelligence tools offer a promising direction for 
SBI risk prediction, achieving performance comparable to that of experienced 
pediatric specialists, while maintaining simplicity of use/data-preprocessing 
for potential real-world applications.
SUPPLEMENTARY INFORMATION: The online version contains supplementary material 
available at 10.1186/s12911-025-03258-3.

DOI: 10.1186/s12911-025-03258-3
PMCID: PMC12619361
PMID: 41239388

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The study was conducted in full compliance with the ethical 
principles set forth in the Declaration of Helsinki, the foundational guideline 
of the World Medical Association for research involving human participants. 
Written consent to participate in the study was obtained from all parents or 
legal guardians of the patients. This research was approved by the Ethical 
Committee of the University Hospital of Split, Croatia, ensuring compliance with 
ethical standards and regulations for medical research (approval number: 
2181 − 147/01–06/LJ.Z.-24-02, Date of Approval: 30 August 2024). Consent for 
publication: Not applicable. Competing interests: The authors declare no 
competing interests.


109. Eur J Pediatr. 2025 Nov 14;184(12):757. doi: 10.1007/s00431-025-06630-7.

Large language model as a clinical decision support tool in the initial 
management of critically ill children: a pilot evaluation.

Tausky O(1), Kaplan E(2)(3), Kadmon G(2)(3), Gendler Y(4), Nahum E(2)(3), 
Yitzhaki S(5), Weissbach A(6)(7).

Author information:
(1)Department of Pediatrics B, Schneider Children's Medical Center of Israel, 
Petach Tikva, Israel.
(2)Pediatric Intensive Care Unit, Schneider Children's Medical Center of Israel, 
Petach Tikva, 4920235, Israel.
(3)Faculty of Medical and Health Sciences, Tel Aviv University, Tel Aviv, 
Israel.
(4)Department of Nursing, School of Health Sciences, Ariel University, Ariel, 
Israel.
(5)Department of Pediatrics A, Schneider Children's Medical Center of Israel, 
Petach Tikva, Israel.
(6)Pediatric Intensive Care Unit, Schneider Children's Medical Center of Israel, 
Petach Tikva, 4920235, Israel. weissbac@gmail.com.
(7)Faculty of Medical and Health Sciences, Tel Aviv University, Tel Aviv, 
Israel. weissbac@gmail.com.

Large language models (LLMs) like ChatGPT are being explored as clinical 
decision support tools, but their reliability in pediatric acute care remains 
uncertain. This pilot study assessed ChatGPT-4.0's performance in the early 
management of critically ill children using real-world clinical data. We 
retrospectively analyzed 20 children emergently admitted from the emergency 
department (ED) to a tertiary pediatric intensive care unit (PICU). ChatGPT-4.0 
was prompted at four time points: ED arrival (diagnostic and therapeutic plans), 
ED transfer (differential diagnosis and hospitalization decision), PICU 
admission (diagnostic and therapeutic plans), and 24 h into PICU stay 
(differential diagnosis). Outputs were compared to actual care and evaluated for 
accuracy, safety, and omissions. At ED and PICU admission, 94% (95% CI, 91-97%) 
and 98% (95% CI, 95-99%) of diagnostic recommendations were rated as 
appropriate. Only 82% (95% CI, 76-87%) of therapeutic recommendations were 
considered appropriate at both points (p < 0.001). Potentially harmful 
therapeutic suggestions were more common than diagnostic ones: 7% vs. 2% in the 
ED (p = 0.016) and 10% vs. 0% in the PICU (p < 0.00001). In the PICU, critically 
missing therapeutic recommendations occurred at 0.95 per case, compared to 0.15 
for diagnostic ones (p = 0.0073). The correct diagnosis appeared in 100% of ED 
discharge and 95% (95% CI, 85-100%) of PICU 24-h differentials. Triage decisions 
were accurate in all PICU cases.
CONCLUSION: ChatGPT-4.0 showed good diagnostic and triage performance but 
requires caution, especially for therapeutic decisions and broader pediatric 
use.
WHAT IS KNOWN: • LLMs like ChatGPT are being explored as clinical support tools. 
• Their diagnostic potential has been studied in adults, but pediatric data 
using real patient cases are limited.
WHAT IS NEW: • This is the first study evaluating ChatGPT in real PICU cases. • 
It showed good diagnostic and triage performance but requires caution, 
especially regarding therapeutic decisions.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00431-025-06630-7
PMID: 41238850 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval: This 
retrospective chart review–based study involving human participants was in 
accordance with the ethical standards of the institution and with the 1964 
Helsinki declaration and its later amendments. It was approved by the 
Institutional Ethics Committee of Rabin Medical Center (RMC 0071–24) on March 
24, 2024, with a waiver of informed consent. Consent to participate and for 
publication: Due to the retrospective nature of this non-interventional study, 
the ethics committees approved a waiver to obtain informed consent from 
participants (RMC 0071–24) on March 24, 2024. Competing interests: The authors 
declare no competing interests.


110. Aesthetic Plast Surg. 2025 Nov 13. doi: 10.1007/s00266-025-05314-9. Online ahead 
of print.

Utility of Large Language Models for Congenital Microtia Reconstruction 
Education: Comparison of the Performance of Claude, GPT, and Gemini.

Liao Z(1), Huang J(2), Liu Y(3), Li F(1), Tang L(1), Cong L(1), Wang H(1), Luo 
SK(4).

Author information:
(1)Department of Plastic and Reconstructive Surgery, The Affiliated Guangdong 
Second Provincial General Hospital of Jinan University, 466 Middle Xin Gang 
Road, Guangzhou City, 510317, Guangdong Province, China.
(2)Jinan University-University of Birmingham Joint Institute, Jinan University, 
Guangzhou City, People's Republic of China.
(3)The Second School of Clinical Medicine, Southern Medical University, 
Guangzhou City, People's Republic of China.
(4)Department of Plastic and Reconstructive Surgery, The Affiliated Guangdong 
Second Provincial General Hospital of Jinan University, 466 Middle Xin Gang 
Road, Guangzhou City, 510317, Guangdong Province, China. luoshk@gd2h.org.cn.

BACKGROUND: Children with microtia and their parents require comprehensive 
information to make informed decisions about treatment options.
OBJECTIVE: We evaluated the effectiveness of various large language models 
(LLMs) in providing preoperative education for congenital microtia 
reconstruction (CMR) by analyzing their responses to related inquiries.
METHODS: Ten plastic surgeons developed 13 CMR-related preoperative education 
strategies and input 14 text commands into Claude-3-Opus, GPT-4-Turbo, and 
Gemini-1.5-Pro during an online session. Five experts evaluated these language 
model's responses for correctness, completeness, logic, and potential harm, 
while five postoperative patients' parent reviewed the education materials for 
readability and value. All responses were also analyzed for readability using 
the context package.
RESULTS: The results showed no statistically significant differences among 
Gemini, Claude, and GPT in the evaluation metrics of accuracy, completeness, and 
potential risk. In terms of logicality and overall rating, Gemini's responses 
were significantly superior to GPT. Preoperative patient education materials 
generated by GPT received the highest DISCERN scores, significantly 
outperforming those from Claude and Gemini. From the perspective of patient's 
parent's, there are no statistically significant differences among Gemini, 
Claude, and GPT. Objective assessments of readability confirmed that Claude's 
materials were easier to understand compared to those from the other models.
CONCLUSION: Claude-3-Opus, GPT-4-Turbo, and Gemini-1.5-Pro effectively addressed 
patient inquiries and produced clear pre-surgical education materials. However, 
these LLMs should not be used independently for patient education without expert 
supervision to ensure accuracy and completeness.
LEVEL OF EVIDENCE IV: This journal requires that authors assign a level of 
evidence to each article. For a full description of these Evidence-Based 
Medicine ratings, please refer to the Table of Contents or the online 
Instructions to Authors www.springer.com/00266 .

© 2025. Springer Science+Business Media, LLC, part of Springer Nature and 
International Society of Aesthetic Plastic Surgery.

DOI: 10.1007/s00266-025-05314-9
PMID: 41233653

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare that they have no conflicts of interest to disclose. Ethical Approval: 
The study protocol was approved by the Institutional Review Board of the 
hospital.


111. Urolithiasis. 2025 Nov 12;53(1):217. doi: 10.1007/s00240-025-01887-x.

Development and multi-institutional validation of the Swin transformer model for 
prediction of spontaneous passage of ureteral calculi: A retrospective 
multicentric study.

Lin Y(#)(1)(2)(3), Li Z(#)(4), Li Z(#)(2)(3), Li G(5), Liu X(6), Cai W(7), Xie 
Z(8), Chen J(2)(3), Zhang Y(9), Pen X(10), Zhang Y(2)(3), Chen C(2)(3), Wu 
M(11), Li P(12)(13)(14), Xia S(15).

Author information:
(1)Department of Urology, Shanghai General Hospital, Shanghai Jiao Tong 
University School of Medicine, Shanghai, China.
(2)Department of Urology, Ningde Municipal Hospital of Ningde Normal University, 
Ningde, China.
(3)Ningde Clinical Medical College of Fujian Medical University, Ningde, China.
(4)Department of Urology, Shanghai Punan Hospital of Pudong New District, Punan 
Branch of Renji hospital, Shanghai, China.
(5)Department of Urology, Guizhou Aerospace Hospital, Zunyi, China.
(6)Department of Urology, The first affiliated hospital of Hebei North 
University, Zhangjiakou, China.
(7)Department of Urology, Xiapu County Hospital in Fujian Province, Xiapu, 
China.
(8)Department of Urology, Pingnan County Hospital in Fujian Province, Pingnan, 
China.
(9)Department of Radiology, Shanghai General Hospital, Shanghai Jiao Tong 
University School of Medicine, Shanghai, China.
(10)Department of Radiology, Ningde Municipal Hospital of Ningde Normal 
University, Ningde, China.
(11)Department of Urology, Shanghai Children's Hospital, Shanghai Jiao Tong 
University School of medicine, Shanghai, China. wumin@163.com.
(12)Department of Urology, Shanghai General Hospital, Shanghai Jiao Tong 
University School of Medicine, Shanghai, China. lipengyxy@sjtu.edu.cn.
(13)Department of Urology, Ningde Municipal Hospital of Ningde Normal 
University, Ningde, China. lipengyxy@sjtu.edu.cn.
(14)Ningde Clinical Medical College of Fujian Medical University, Ningde, China. 
lipengyxy@sjtu.edu.cn.
(15)Department of Urology, Shanghai General Hospital, Shanghai Jiao Tong 
University School of Medicine, Shanghai, China. xsjurologist@163.com.
(#)Contributed equally

To improve the accuracy of our prediction of the spontaneous passage of urinary 
calculi, we developed a model based on the Swin Transformer model. In this 
retrospective multicentre study, computed tomography (CT) images of ureteral 
calculi from January 2019 to December 2023 from our hospital were used as 
internal training and validation datasets. The samples were split in a 7:3 ratio 
into an internal training set (N = 278) and internal testing set (N = 120). An 
external validation set (N = 142) was created using the CT datasets of patients 
from four different hospitals. Swin Transformer was used to create an ensemble 
learning-based model approach for predicting the passage of ureteral calculi. 
For both the internal and external validation sets, performance was assessed 
using the F1 score, accuracy, specificity, sensitivity, and area under the 
operating characteristic curve (AUC). A total of 540 urinary CT images were 
acquired (Internal dataset = 398; multicentre external dataset = 142). The AUC 
reached 0.873 (95% [CI]: 0.841-0.906), and the external validation set reached 
0.784 (95% CI: 0.747-0.820),. The evaluation results of the Swin Transformer 
model, developed using CT images of urinary calculi, demonstrate that it 
outperforms models such as ResNet and ConvNeXt. Furthermore, the model 
outperforms both attending and resident doctors. The predictive power of 
attending doctors was significantly improved with the aid of the Swin 
Transformer model. The Swin Transformer model is a generalizable, objective and 
accurate prediction model for calculi passage based on urinary tract CT images 
and could help clinicians make better clinical judgements. Trial registration: 
It was also registered in the Chinese Clinical Trial Registation (registration 
number: ChiCTR2400086807). Informed consent was waived as this study used 
retrospectively collected anonymized data.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00240-025-01887-x
PMID: 41222615 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests. Ethics approval: This study was conducted 
according to the guidelines of the Declaration of Helsinki, and approved by 
Ethics Committee of Ningde Hospital Affiliated to Ningde Normal 
University(NDYKYLI-2024-34). Consent to participate: Not applicable. Consent to 
publish: The final manuscript is approved for publication by all authors.


112. Clin Ophthalmol. 2025 Nov 5;19:4077-4084. doi: 10.2147/OPTH.S556186. eCollection 
2025.

Diagnostic Accuracy and Counseling Quality of GPT-4o for Strabismus and 
Pseudostrabismus in Patient-Generated Mobile Photographs: A Preliminary 
Evaluation.

Esposito EP(1), Cardakli N(2), Christoff A(2), Kraus CL(2).

Author information:
(1)Scheie Eye Institute, University of Pennsylvania, Philadelphia, PA, USA.
(2)Division of Pediatric Ophthalmology and Adult Strabismus, Wilmer Eye 
Institute, Baltimore, MD, USA.

BACKGROUND: Research-grade artificial intelligence has been used to accurately 
diagnose strabismus from image input. OpenAI's consumer-oriented GPT-4o model 
can analyze images, but has shown poor accuracy for image-based diagnosis. 
Parents may turn to GPT-4o to support or refute visible health concerns for 
their children, such as strabismus. The study aims to evaluate GPT-4o's 
diagnostic accuracy and response quality for strabismus evaluation.
METHODS: After gold-standard alternate cover exam by a clinician, 35 mobile 
photos of esotropia (13), pseudoesotropia (11), and exotropia (11) were 
selected. Images were excluded if a second "masked" examiner did not corroborate 
diagnosis. Images were submitted to a secure GPT-4o platform with 
patient-perspective prompts requesting overall evaluation (Prompt 1) and eye 
alignment evaluation (Prompt 2). Responses were graded by a pediatric 
ophthalmologist and certified orthoptist assessing for quality and safety.
RESULTS: GPT-4o provided interpretations for 15/35 and 27/35 images after 
Prompts 1 and 2, respectively. Analysis of the accuracy includes a primary 
"intention-to-diagnose" and secondary "per-diagnosis" framework. The diagnostic 
accuracies in the primary and secondary analysis following prompt 1 were 14.3% 
(low sensitivity and specificity) and 33.3% (low sensitivity, high specificity), 
respectively. Following prompt 2, accuracies were 48.6% (moderate sensitivity, 
low specificity) and 63.0% (high sensitivity, low specificity). Overall, the 
mean rating for content for strabismus prompts was 4.94 ± 0.27 out of a best 
possible 6, and for pseudostrabismus, 5.14 ± 0.18 (p=0.638).
CONCLUSION: GPT-4o shows poor accuracy for image-based strabismus diagnosis. 
GPT-4o frequently categorized pseudoesotropia as true strabismus and true 
strabismus as orthophoria. While the quality of responses was rated as good 
overall, the quality of counseling did not match what would be provided in a 
pediatric ophthalmology clinic. Patients, clinicians, and AI developers should 
be aware of the need for specialist evaluation for strabismus.

© 2025 Esposito et al.

DOI: 10.2147/OPTH.S556186
PMCID: PMC12596880
PMID: 41216185

Conflict of interest statement: The authors declare no conflicts of interest.


113. Sci Rep. 2025 Nov 10;15(1):39231. doi: 10.1038/s41598-025-22402-7.

Large language model performance versus human expert ratings in automated 
suicide risk assessment.

Thomas J(1)(2)(3), Elyoseph Z(4)(5), Kuchinke L(6), Meinlschmidt G(7)(8)(9)(10).

Author information:
(1)krisenchat gGmbH, Berlin, Germany.
(2)Division of Clinical Psychology and Epidemiology, Department of Psychology, 
University of Basel, Basel, Switzerland.
(3)Division of Clinical Psychology and Cognitive Behavioural Therapy, 
International Psychoanalytic University (IPU) Berlin, Berlin, Germany.
(4)Department of Counseling and Human Development, Faculty of Education, 
University of Haifa, Haifa, Israel.
(5)Imperial College London, London, UK.
(6)Psychological Methods and Evaluation, International Psychoanalytic University 
(IPU) Berlin, Berlin, Germany.
(7)Division of Clinical Psychology and Cognitive Behavioural Therapy, 
International Psychoanalytic University (IPU) Berlin, Berlin, Germany. 
meinlschmidt@uni-trier.de.
(8)Department of Clinical Psychology and Psychotherapy - Methods and Approaches, 
Trier University, 54286, Trier, Germany. meinlschmidt@uni-trier.de.
(9)Department of Digital and Blended Psychosomatics and Psychotherapy, 
Psychosomatic Medicine, University Hospital and University of Basel, Basel, 
Switzerland. meinlschmidt@uni-trier.de.
(10)Department of Psychosomatic Medicine, University Hospital and University of 
Basel, Basel, Switzerland. meinlschmidt@uni-trier.de.

Large Language Models' (LLMs) potential for psychological diagnostics requires 
systematic evaluation. We aimed to investigate conditions for reliable and valid 
psychological assessments, focusing on suicide risk evaluation in clinical data 
by comparing LLM-generated ratings with human expert ratings across model 
configurations., analyzing 100 youth crisis text line conversation transcripts 
rated by four experts using the Nurses' Global Assessment of Suicide Scale 
(NGASR). Using Mixtral-8x7B-Instruct, we generated ratings across three 
temperature settings and prompting styles (zero-shot, few-shot, 
chain-of-thought). Across configurations we compared (a) 
inter-rating-reliability for AI-generated NGASR risk and sum scores, (b) 
LLM-to-human observer agreement regarding sum score, risk category, and item, 
using Krippendorff's α, (c) classification metrics of risk categories and 
individual items against human ratings. LLM configuration strongly influenced 
assessment reliability. Zero-shot prompting at temperature 0 yielded perfect 
inter-rating reliability (α = 1.00, 95% CI: [1-1] for high & very high risk), 
while few-shot prompting showed best human-AI agreement for very high risk 
(α = 0.78, 95% CI: [0.67-0.89]) and strongest classification performance 
(balanced accuracy 0.54-0.71). Lower temperatures consistently improved 
reliability and accuracy. However, critical clinical items showed poor validity. 
Our findings establish optimal conditions (zero temperature, task-specific 
prompting) for LLM-based psychological assessment. However, inconsistent 
clinical item performance and only moderate LLM-to-human observer agreement 
limit LLMs to initial screening rather than detailed assessment, requiring 
careful parameter control and validation.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-22402-7
PMCID: PMC12603332
PMID: 41213985 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: J.T is 
employed and receives a salary from krisenchat.This needs to move to the 
competing interests section. krisenchat had no impact on the design of this 
study and did not influence the collection, execution, analyses, interpretation 
of the data, or the decision to submit the article/contribution for 
publication. L.K. and Z.E. have no competing interests. G.M. received funding 
from the Stanley Thomas Johnson Stiftung & Gottfried und Julia 
Bangerter-Rhyner-Stiftung under projects no. PC 28/17 and PC 05/18, from 
Gesundheitsförderung Schweiz under project no. 18.191/K50001, from the Swiss 
Heart Foundation under project no. FF21101, from the Research Foundation of the 
International Psychoanalytic University (IPU) Berlin under projects no. 5087 and 
5217, from the Swiss National Science Foundation (SNSF) under project no. 
100014_135328, from the German Federal Ministry of Education and Research under 
budget item 68606 in the context of an evaluation project conducted amongst 
others in collaboration with krisenchat, from the Hasler Foundation under 
project No. 23004, in the context of a Horizon Europe project from the Swiss 
State Secretariat for Education, Research and lnnovation (SERI) under contract 
number 22.00094, and from Wings Health in the context of a proof-of-concept 
study. G.M. is a co-founder, and shareholder of Therayou AG, active in digital 
and blended mental healthcare. GM receives royalties from publishing companies 
as author, including a book published by Springer, and an honorarium from 
Lundbeck for speaking at a symposium. Furthermore, GM is compensated for 
providing psychotherapy to patients, acting as a supervisor, serving as a 
self-experience facilitator (‘Selbsterfahrungsleiter’), and for postgraduate 
training of psychotherapists and supervisors. Ethical approval: All methods in 
this study were carried out in accordance with relevant guidelines and 
regulations. All experimental protocols were approved by the Ethics Committee of 
the International Psychoanalytic University (IPU) Berlin (approval number: 
2023_08). Informed consent was obtained from all subjects through krisenchat’s 
terms of service, which explicitly state that user data may be used for research 
purposes without direct identification of individuals. All personally 
identifiable information was removed from chat transcripts during preprocessing. 
The study utilized existing data from the crisis helpline, and participants were 
not compensated as this was a secondary analysis of routine service data. 
Research was performed in accordance with the Declaration of Helsinki. Use of 
artificial intelligence toolsWe used artificial intelligence (AI)-based tools, 
including Claude and ChatGPT to support manuscript preparation. Further, we used 
publicly available search technologies, which we recognize likely utilise AI 
capabilities. We confirm that the contributions of AI were strictly in an 
assistive capacity. AI was not involved in conceptual tasks. Human oversight was 
continuously employed to ensure the accuracy of content and address any ethical 
concerns. Consent for publication: Krisenchat had no impact on the design of 
this study and did not influence the collection, execution, analyses, 
interpretation of the data, or the decision to submit the article/contribution 
for publication.


114. JMIR Biomed Eng. 2025 Nov 10;10:e66691. doi: 10.2196/66691.

Challenges and Solutions in Applying Large Language Models to Guideline-Based 
Management Planning and Automated Medical Coding in Health Care: Algorithm 
Development and Validation.

Sarvari P(1), Al-Fagih Z(1), Abou-Chedid A(1), Jewell P(2), Taylor R(2), Imtiaz 
A(3).

Author information:
(1)Rhazes AI, 85 Great Portland Street, London, W1W 7LT, United Kingdom, 44 
7762219374.
(2)Assuric, London, United Kingdom.
(3)Frimley Health NHS Foundation Trust, Camberley, United Kingdom.

BACKGROUND: Diagnostic errors and administrative burdens, including medical 
coding, remain major challenges in health care. Large language models (LLMs) 
have the potential to alleviate these problems, but their adoption has been 
limited by concerns regarding reliability, transparency, and clinical safety.
OBJECTIVE: This study introduces and evaluates 2 LLM-based frameworks, 
implemented within the Rhazes Clinician platform, designed to address these 
challenges: generation-assisted retrieval-augmented generation (GARAG) for 
automated evidence-based treatment planning and generation-assisted vector 
search (GAVS) for automated medical coding.
METHODS: GARAG was evaluated on 21 clinical test cases created by medically 
qualified authors. Each case was executed 3 times independently, and outputs 
were assessed using 4 criteria: correctness of references, absence of 
duplication, adherence to formatting, and clinical appropriateness of the 
generated management plan. GAVS was evaluated on 958 randomly selected 
admissions from the Medical Information Mart for Intensive Care (MIMIC)-IV 
database, in which billed International Classification of Diseases, Tenth 
Revision (ICD-10) codes served as the ground truth. Two approaches were 
compared: a direct GPT-4.1 baseline prompted to predict ICD-10 codes without 
constraints and GAVS, in which GPT-4.1 generated diagnostic entities that were 
each mapped onto the top 10 matching ICD-10 codes through vector search.
RESULTS: Across the 63 outputs, 62 (98.4%) satisfied all evaluation criteria, 
with the only exception being a minor ordering inconsistency in one repetition 
of case 14. For GAVS, the 958 admissions contained 8576 assigned ICD-10 
subcategory codes (1610 unique). The vanilla LLM produced 131,329 candidate 
codes, whereas GAVS produced 136,920. At the subcategory level, the vanilla LLM 
achieved 17.95% average recall (15.86% weighted), while GAVS achieved 20.63% 
(18.62% weighted), a statistically significant improvement (P<.001). At the 
category level, performance converged (32.60% vs 32.58% average weighted recall; 
P=.99).
CONCLUSIONS: GARAG demonstrated a workflow that grounds management plans in 
diagnosis-specific, peer-reviewed guideline evidence, preserving fine-grained 
clinical detail during retrieval. GAVS significantly improved fine-grained 
diagnostic coding recall compared with a direct LLM baseline. Together, these 
frameworks illustrate how LLM-based methods can enhance clinical decision 
support and medical coding. Both were subsequently integrated into Rhazes 
Clinician, a clinician-facing web application that orchestrates LLM agents to 
call specialized tools, providing a single interface for physician use. Further 
independent validation and large-scale studies are required to confirm 
generalizability and assess their impact on patient outcomes.

© Peter Sarvari, Zaid Al-fagih, Alexander Abou-Chedid, Paul Jewell, Rosie 
Taylor, Arouba Imtiaz. Originally published in JMIR Biomedical Engineering 
(http://biomsedeng.jmir.org).

DOI: 10.2196/66691
PMCID: PMC12599997
PMID: 41213118

Conflict of interest statement: Conflicts of Interest: PS, ZA, and AA-C are 
currently affiliated with Rhazes, which is a health tech startup discussed 
throughout the paper. PJ and RT are affiliated with Assuric, a company providing 
digital health compliance services, mentioned once in the paper. AI has 
previously been affiliated with Rhazes but is currently not affiliated with 
Rhazes. The authors invite the readers to independently verify the results of 
this article using the scripts shared in the Data Availability Statement.


115. JAMIA Open. 2025 Nov 5;8(6):ooaf143. doi: 10.1093/jamiaopen/ooaf143. eCollection 
2025 Dec.

Patient and clinician perspectives in the use of machine learning and artificial 
intelligence in the context of acute neurology.

Abahuje E(1), Houskamp EJ(2), Silva Pinheiro do Nascimento J(2), Agha EM(3), 
Thompson WK(4), Michelson KN(3)(5), Naidech A(2).

Author information:
(1)Department of Cardiac Surgery, University of Michigan, Ann Arbor, MI 48109, 
United States.
(2)Institute for Public Health and Medicine, Feinberg School of Medicine, 
Northwestern University, Chicago, IL 60611, United States.
(3)Division of Critical Care Medicine, Ann & Robert H. Lurie Children's Hospital 
of Chicago, Chicago, IL 60611, United States.
(4)Kellogg School of Management, Northwestern University, Evanston, IL 60208, 
United States.
(5)Department of Pediatrics, Northwestern University Feinberg School of 
Medicine, Chicago, IL 60611, United States.

BACKGROUND: Clinician perspectives on machine learning and artificial 
intelligence (ML/AI) vary with discipline. However, fewer studies describe 
patient perspectives and address medical situations that require rapid decisions 
with durable consequences for patient outcomes. This study characterized 
perspectives (qualitatively) and sentiment (quantitatively) regarding the use of 
ML/AI for patient management in neurological emergencies.
METHODS: We conducted semi-structured interviews with survivors (or their proxy) 
of intracranial hemorrhage, and clinicians who care for such patients. 
Interviews were analyzed qualitatively using thematic analysis, and 
quantitatively using sentiment analysis to assess attitudes using a 
transformer-based language model with scores from -1 (most negative) to 1 (most 
positive).
RESULTS: We enrolled 21 participants (14 patients, 1 proxy, and 6 clinicians) 
and reached thematic saturation. Help with clinical decision-making was cited as 
an advantage of ML/AI. Participants noted the importance of considering ML/AI as 
an adjunct to clinical care, not as a replacement for clinicians. Over-reliance 
on recommendations, potentially leading to diminution of clinician skill, 
incorrect ML/AI recommendations, potential liability, and bias were cited as 
challenges. Clinician and patient education were noted as a potential burden. 
Median sentiment scores ranged from 0.0 (neutral) to 0.3 (positive). Sentiment 
varied with question type (P < .001). Questions about clinicians using ML/AI for 
patient care had the highest sentiment score.
DISCUSSION AND CONCLUSION: Patients and clinicians expressed mixed views about 
ML/AI. Potential benefits related to improved decision-making and concerns 
focused on bias, liability, and the need for further education. Future work 
should address how best to incorporate ML/AI into education and obviate 
potential burdens as ML/AI is integrated into clinical care.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamiaopen/ooaf143
PMCID: PMC12596108
PMID: 41211120

Conflict of interest statement: The authors have no conflicts of interests to 
declare.


116. Mol Biomed. 2025 Nov 10;6(1):102. doi: 10.1186/s43556-025-00341-z.

Clinical-grade AI model for molecular subtyping of endometrial cancer: a 
multi-center cohort study in China.

Qi P(#)(1)(2), Yao T(#)(1), Li H(#)(3), Zhu J(#)(3), Li J(4), Luo X(5), He Q(6), 
Li Y(7).

Author information:
(1)Department of Control Science and Engineering, College of Electronics and 
Information Engineering, Tongji University, Shanghai, 201210, China.
(2)State Key Laboratory of Cardiovascular Diseases and Medical Innovation 
Center, Shanghai East Hospital, School of Medicine, Tongji University, Shanghai, 
200092, China.
(3)Centre for Assisted Reproduction, Shanghai Key Laboratory of Maternal-Fetal 
Medicine, Shanghai Institute of Maternal-Fetal Medicine and Gynecologic 
Oncology, Shanghai First Maternity and Infant Hospital, School of Medicine, 
Tongji University, Shanghai, 200092, China.
(4)Pingdingshan Maternal and Child Health Hospital, Henan, 467000, China.
(5)Department of Gynecology, Obstetrics and Gynecology Hospital of Fudan 
University, Shanghai, 200011, China. xuezhenluo2013@163.com.
(6)Department of Pathology, Shanghai First Maternity and Infant Hospital, School 
of Medicine, Tongji University, Shanghai, 200092, China. qizhihe@tongji.edu.cn.
(7)Centre for Assisted Reproduction, Shanghai Key Laboratory of Maternal-Fetal 
Medicine, Shanghai Institute of Maternal-Fetal Medicine and Gynecologic 
Oncology, Shanghai First Maternity and Infant Hospital, School of Medicine, 
Tongji University, Shanghai, 200092, China. liyiran2007@gmail.com.
(#)Contributed equally

Accurate molecular subtyping is essential for guiding precision treatment and 
prognostic stratification in endometrial cancer (EC). However, current methods, 
based on Sanger sequencing and immunohistochemistry (IHC), are costly, 
time-intensive, and difficult to implement widely in routine clinical practice, 
particularly in resource-limited settings. To overcome these challenges, we 
developed a deep-learning pipeline that directly infers EC molecular subtypes 
from routine hematoxylin-and-eosin (H&E) whole-slide images (WSIs). The 
framework integrates super-resolution enhancement (SRResGAN), transformer-based 
lesion segmentation (MedSAM), and a ResNet-101 classifier for molecular subtype 
prediction, with an LSTM module for survival modeling. This retrospective study 
included 393 Chinese patients diagnosed between 2010 and 2018, all with ≥ 5 
years of follow-up. Molecular subtypes-POLEmut, mismatch repair-deficient 
(MMRd), p53abnormal (p53abn), and no specific molecular profile (NSMP)-were 
confirmed by Sanger sequencing and immunohistochemistry. The model achieved high 
classification accuracies (92% for POLEmut and MMRd, 91% for p53abn, and 90% for 
NSMP), with a strong correlation between predicted and observed survival 
(R2 = 0.9692; MAE = 123 days). External validation on two independent cohorts 
(N = 35 and N = 83) confirmed robust generalizability across institutions. This 
study represents the first large-scale, multicenter, AI-based digital pathology 
model for EC molecular classification in China. The proposed workflow provides 
an automated, interpretable, and cost-efficient alternative to conventional 
molecular testing, supporting precision oncology, fertility-preserving 
management, and clinical decision-making in real-world practice.

© 2025. The Author(s).

DOI: 10.1186/s43556-025-00341-z
PMCID: PMC12597869
PMID: 41207998 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study received ethical approval from the institutional ethics 
committee of Shanghai First Maternity and Infant Hospital (No: KS22133). 
Additional approvals were obtained from Obstetrics and Gynecology Hospital of 
Fudan University (OGHFU, ID: 2023–31) and Pingdingshan Maternal and Child Health 
Hospital (PMCHH, ID: 2025–007) for the use of external validation cohorts. 
Informed consent for the potential use of specimens in clinical research was 
obtained from all participants prior to surgical procedures. The study protocol 
and experimental design adhered to the most recent revised ethical guidelines of 
the Declaration of Helsinki. Consent for publication: All authors have consented 
to the publication of the data covered in this article. Competing interests: The 
authors declare that they have no known competing financial interests or 
personal relationships that could have appeared to influence the work reported 
in this paper.


117. J Biomed Inform. 2025 Dec;172:104951. doi: 10.1016/j.jbi.2025.104951. Epub 2025 
Nov 6.

LLM-DQR: Large language model-based automated generation of data quality rules 
for electronic health records.

Xie S(1), Cai H(1), Sun Y(1), Lv X(2).

Author information:
(1)College of Biomedical Engineering and Instrument Science, Zhejiang 
University, Zheda Road, 310027 Hangzhou, China.
(2)College of Biomedical Engineering and Instrument Science, Zhejiang 
University, Zheda Road, 310027 Hangzhou, China. Electronic address: 
lvxd@zju.edu.cn.

OBJECTIVE: To develop and evaluate LLM-DQR, an automated approach using large 
language models to generate electronic health record data quality rules, 
addressing the limitations of current manual and automated methods that suffer 
from low efficiency, limited flexibility, and inadequate coverage of complex 
business logic.
MATERIALS AND METHODS: We designed a comprehensive pipeline with three core 
components: (1) standardized input processing integrating database schemas, 
natural language requirements, and sample data; (2) Chain-of-Thought prompt 
engineering for guided rule generation; and (3) closed-loop validation with 
deduplication, sandbox execution, and iterative debugging. The approach was 
evaluated on two distinct, publicly available datasets: the Paediatric Intensive 
Care (PIC) dataset and the Medical Information Mart for Intensive Care 
(MIMIC-IV) dataset. Performance was compared against manual expert construction 
(expert-DQR) and clinical information model-based generation (CIM-DQR).
RESULTS: LLM-DQR demonstrated higher performance across all evaluation metrics. 
The GPT implementation achieved overall coverage rates of 97.1% on the PIC 
dataset and 99.6% on the MIMIC-IV dataset, outperforming CIM-DQR. Performance 
was particularly strong for complex dimensions: achieving 100% coverage for 
Consistency rules on both datasets, whereas CIM-DQR achieved 0%. Construction 
time was reduced by over 10-fold compared to manual methods. Additionally, on 
the PIC dataset, LLM-DQR generated 89 extra, expert-validated rules.
DISCUSSION: The stronger performance demonstrates LLMs' capability to understand 
complex EHR data patterns and assessment requirements, functioning as data 
quality analysis assistants with domain knowledge and logical reasoning 
capabilities.
CONCLUSION: LLM-DQR provides an efficient, scalable solution for automated data 
quality rule generation in clinical settings, offering considerable improvements 
over traditional approaches.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104951
PMID: 41205689 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


118. Physiol Behav. 2026 Feb;304:115156. doi: 10.1016/j.physbeh.2025.115156. Epub 
2025 Nov 5.

TurkerPat: EEG-Based detection of hunger, thirst, and nicotine withdrawal.

Kaya S(1), Kirik S(2), Tas S(3), Tanko D(4), Keles T(5), Tasci I(6), Dogan S(7), 
Baygin M(8), Tuncer T(9).

Author information:
(1)Department of Psychiatry, Elazig Fethi Sekin City Hospital, Elazig, Turkey. 
Electronic address: suheda_sener@hotmail.com.
(2)Department of Pediatrics, Division of Pediatric Neurology, Fethi Sekin City 
Hospital, Elazig 23280, Turkey. Electronic address: serkankirk34@gmail.com.
(3)Department of Digital Forensics Engineering, Technology Faculty, Firat 
University, Elazig, Turkey. Electronic address: 231144106@firat.edu.tr.
(4)Department of Digital Forensics Engineering, Technology Faculty, Firat 
University, Elazig, Turkey. Electronic address: 212144203@firat.edu.tr.
(5)Department of Digital Forensics Engineering, Technology Faculty, Firat 
University, Elazig, Turkey. Electronic address: tkeles@firat.edu.tr.
(6)Department of Neurology, School of Medicine, Firat University, Elazig, 
Turkey. Electronic address: itasci@firat.edu.tr.
(7)Department of Digital Forensics Engineering, Technology Faculty, Firat 
University, Elazig, Turkey. Electronic address: sdogan@firat.edu.tr.
(8)Department of Computer Engineering, Engineering Faculty, Erzurum Technical 
University, Erzurum, Turkey. Electronic address: mehmet.baygin@erzurum.edu.tr.
(9)Department of Digital Forensics Engineering, Technology Faculty, Firat 
University, Elazig, Turkey. Electronic address: turkertuncer@firat.edu.tr.

BACKGROUND: One of the primary objectives of neuroscience is to gather 
information from the brain. Therefore, brain data are crucial for understanding 
its secrets, and one of the most affordable methods for collecting such data is 
electroencephalography (EEG). To capture meaningful information, machine 
learning models have been applied to EEG signals. In this research, our main 
goal is to investigate an innovative feature-extraction method on a new EEG 
dataset to obtain both accurate classification and interpretable results.
MATERIAL AND METHODS: First, we curated a novel EEG signal dataset comprising 
four classes: (i) hungry, (ii) thirsty, (iii) cigarette-addicted, and (iv) 
control. Using this dataset, we defined four cases: (1) hunger detection, (2) 
thirst detection, (3) nicotine-withdrawal detection, and (4) abnormality (hunger 
+ thirst + nicotine-withdrawal) detection. To automatically detect these cases, 
we introduced a specialized transformer-based feature-extraction method. This 
transformer, called the Moon Star Transformer (MST), was deployed alongside a 
Transition Table Feature Extractor (TTFE) to form the Turker Pattern 
(TurkerPat). Feature selection, ensemble and iterative classification, and an 
interpretable results generator were then integrated into the TurkerPat-centric 
XFE framework to achieve both classification accuracy and interpretability.
RESULTS: The proposed TurkerPat-centric XFE framework attained over 85 % 
classification accuracy using leave-one-subject-out cross-validation (LOSO CV). 
By applying Directed Lobish (DLob) for interpretable result generation, we 
obtained connectome diagrams for each defined case.
CONCLUSION: The classification and explainable results clearly demonstrate that 
the TurkerPat-centric XFE framework makes a significant contribution to both 
neuroscience and feature engineering.

Copyright © 2025. Published by Elsevier Inc.

DOI: 10.1016/j.physbeh.2025.115156
PMID: 41203190 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare no conflict of interest.


119. Cir Pediatr. 2025 Oct 22;38(4):147-152. doi: 10.54847/cp.2025.04.13.

Comparison of machine learning models in the interpretation of barium enemas in 
Hirschsprung's disease.

[Article in English, Spanish; Abstract available in Spanish from the publisher]

Vargová P(1), González Esgueda A(1), Fernández Atuan R(1), Izquierdo Hernández 
B(2), Gutiérrez Alonso C(2), González Ruiz Y(1), Bragagnini Rodríguez P(1), 
Santiño Tenorio A(1), Corona Bellostas C(1).

Author information:
(1)Pediatric Surgery Department. Hospital Universitario Miguel Servet. Zaragoza 
(Spain).
(2)Radiology Department. Hospital Universitario Miguel Servet. Zaragoza (Spain).

OBJECTIVE: To assess the diagnostic accuracy of machine learning models and 
ChatGPT in the interpretation of barium enemas for Hirschsprung's disease (HD), 
and to compare performance with that of pediatric radiologists.
MATERIAL AND METHODS: A retrospective study of the barium enemas of patients 
< 15 years of age managed at a tertiary institution from 2011 to 2023 was 
carried out. The images were used to train AI models and divided into training, 
validation, and test ensembles. Performance was assessed in a separate test 
ensemble with anonymized images, while calculating sensitivity, specificity, and 
ROC curves vs. final diagnosis, and it was compared with radiologists' 
performance.
RESULTS: 266 barium enemas from 218 patients (1,439 images in total) were 
included. The test ensemble consisted of 54 enemas, with 11 HD positive cases. 
The support vector model had a sensitivity of 72.7% and a specificity of 93%. 
The logistic regression model had an AUC-ROC of 0.73, with better results in 
anteroposterior than in lateral images. When compared with retrospective 
radiological reports, AI models had a classification capacity similar to that of 
expert professionals, with a sensitivity of 81% and a specificity of 76%.
CONCLUSIONS: AI models showed potential in supporting the diagnosis of 
Hirschsprung's disease based on barium enemas, with a good capacity to rule out 
HD. This could improve diagnostic accuracy, especially in environments with 
little experience or limited availability of specialist radiologists. However, 
further studies are required to optimize clinical application.

Publisher: OBJETIVOS: Evaluar la precisión diagnóstica de modelos de aprendizaje 
automático y ChatGPT en la interpretación de enemas opacos para la enfermedad de 
Hirschsprung (EH) y comparar su rendimiento con el de los radiólogos 
pediátricos.
MATERIAL Y METODOS: Se realizó estudio retrospectivo en un centro terciario, 
incluyendo enemas opacos de pacientes < 15 años atendidos entre 2011-2023. Las 
imágenes se usaron para entrenar modelos de inteligencia artificial (IA) y se 
dividieron en conjuntos de entrenamiento, validación y prueba. El rendimiento se 
evaluó en un conjunto de prueba independiente con imágenes anonimizadas, 
calculando sensibilidad, especificidad y curvas ROC frente al diagnóstico 
definitivo y se comparó con el rendimiento de los radiólogos.
RESULTADOS: Se incluyeron 266 enemas opacos de 218 pacientes (1439 imágenes en 
total). El conjunto de prueba incluyó 54 enemas, con 11 casos positivos de EH. 
El modelo de soporte vectorial obtuvo una sensibilidad del 72,7% y especificidad 
del 93%, respectivamente. El modelo de regresión logística mostró un AUC-ROC de 
0,73 con mejores resultados en imágenes anteroposteriores que en laterales. 
Comparados con los informes radiológicos retrospectivos, los modelos de IA 
ofrecieron una capacidad de clasificación similar a la de los profesionales 
expertos que alcanzaron una sensibilidad 81% y especificidad de 76%.
CONCLUSIONES: Los modelos de IA mostraron potencial para apoyar el diagnóstico 
de la EH a partir de enemas opacos, con buena capacidad para descartar la EH. 
Esto podría mejorar la precisión diagnóstica, especialmente en entornos con 
menos experiencia o acceso limitado a radiólogos especialistas, aunque se 
necesitan más estudios para optimizar su aplicación clínica.

DOI: 10.54847/cp.2025.04.13
PMID: 41201854 [Indexed for MEDLINE]


120. J Adv Res. 2025 Oct 24:S2090-1232(25)00832-X. doi: 10.1016/j.jare.2025.10.038. 
Online ahead of print.

Clinical validation of a multi-model blood cfDNA methylation assay for 
early-stage gastrointestinal cancer screening.

Zhu L(1), Lin S(1), Xu J(2), Yu J(3), Men F(4), Yu D(2), Ma X(5), Tian J(6), Xie 
H(5), Duan L(5), Wang X(5), Sun S(1), Li C(1), Li S(1), Kang Q(1), Jia M(4), Lin 
X(3), Lin Q(3), Lin L(3), Yi X(7), Wang R(7), Guo W(7), Gong X(7), Sheng J(2), 
Guo N(8), Lan S(9), Jin P(10), He Y(11).

Author information:
(1)Department of Gastroenterology, Beijing Chest Hospital, Capital Medical 
University, Beijing Tuberculosis and Thoracic Tumor Research Institute, Beijing 
101149, China; Laboratory for Clinical Medicine, Capital Medical University, 
Beijing 101149, China.
(2)Senior Department of Gastroenterology, The First Medical Center of Chinese 
People's Liberation Army General Hospital, No. 28 of Fuxing Road, Haidian 
District, Beijing 100853, China; Department of Gastroenterology, Seventh Medical 
Center of Chinese PLA General Hospital, No.5 Nanmencang, Dongcheng District, 
Beijing 100700, China.
(3)Department of Gastroenterology, Longyan First Affiliated Hospital of Fujian 
Medical University, 105 Jiuyi Road (North), Longyan 364000, China.
(4)Department of Gastroenterology, Dongying People's Hospital, Nan Yi Lu 317, 
Dongying District, Dongying 257000, China.
(5)Department of Gastroenterology, Seventh Medical Center of Chinese PLA General 
Hospital, No.5 Nanmencang, Dongcheng District, Beijing 100700, China.
(6)Department of Hepatobiliary and Pancreatic Surgery, The First Medical Center 
of Chinese PLA General Hospital, Institute of Hepatobiliary Surgery and Key 
Laboratory of Digital Hepetobiliary Surgery of Chinese PLA General Hospitall, 
No. 28 of Fuxing Road, Haidian District, Beijing 100853, China.
(7)Shanghai Yingce Biotechnology Co., Ltd, Room 202, 203, 2nd Floor, No. 4, Lane 
288, Mingnan Road, Songjiang District, Shanghai 201613, China.
(8)Department of Gastroenterology, Dongying People's Hospital, Nan Yi Lu 317, 
Dongying District, Dongying 257000, China. Electronic address: 
guoni1974@sina.com.
(9)Department of Gastroenterology, Longyan First Affiliated Hospital of Fujian 
Medical University, 105 Jiuyi Road (North), Longyan 364000, China. Electronic 
address: lan4000@163.com.
(10)Senior Department of Gastroenterology, The First Medical Center of Chinese 
People's Liberation Army General Hospital, No. 28 of Fuxing Road, Haidian 
District, Beijing 100853, China; Department of Gastroenterology, Seventh Medical 
Center of Chinese PLA General Hospital, No.5 Nanmencang, Dongcheng District, 
Beijing 100700, China. Electronic address: jinpeng@301hospital.com.cn.
(11)Department of Gastroenterology, Beijing Chest Hospital, Capital Medical 
University, Beijing Tuberculosis and Thoracic Tumor Research Institute, Beijing 
101149, China; Laboratory for Clinical Medicine, Capital Medical University, 
Beijing 101149, China. Electronic address: endohe@163.com.

BACKGROUND: Gastrointestinal (GI) tract cancers are the second leading cause of 
cancer-related mortality, often due to late detection. There is a critical need 
for non-invasive, highly sensitive biomarkers for early-stage cancer and 
precancerous lesion detection to enable timely intervention. This study aimed to 
develop a blood-based method specifically optimized for early GI cancer 
detection and population-level screening.
METHODS: Using large-scale public tissue methylation data and the Twist probe 
cfDNA profiles, we developed SPOGIT (Screening for the Presence of 
Gastrointestinal Tumors), a multi-algorithm model (Logistic 
Regression/Transformer/MLP/Random Forest/SGD/SVC) for early GI cancer detection. 
The model was rigorously validated through an internal (n = 83) and multicenter 
external validation (386 cancers/113 controls/580 precancers), with an 
interception model assessing its clinical potential.
RESULTS: SPOGIT demonstrated high accuracy in detecting GI cancers, with a 
sensitivity of 88.1 % and a specificity of 91.2 %. Notably, it effectively 
identified early-stage (0-II) cancers with 83.1 % sensitivity. The model also 
showed significant potential for intercepting premalignant progression, 
detecting advanced adenomas (AA) and gastric precancerous lesions with 
sensitivities of 56.5 % and up to 62.4 %, respectively. In the external 
independent validation cohort, a complementary model CSO (Cancer Signal Origin) 
demonstrated an accuracy of 83 % for colorectal cancer and 71 % for gastric 
cancer. Most importantly, simulation analyses projected that SPOGIT 
implementation could significantly reduce late-stage diagnoses and increase 
5-year survival rate by 27.02 % through early interception.
CONCLUSIONS: This study introduced a novel, dual-model blood architecture 
(SPOGIT/CSO) that enables highly accurate, early detection of GI cancers and 
their precursors. By facilitating timely clinical intervention, SPOGIT/CSO 
represented a paradigm-shifting strategy with the potential to significantly 
improve patient survival outcomes and transform GI cancer management.

Copyright © 2025 The Author(s). Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.jare.2025.10.038
PMID: 41201494

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


121. CPT Pharmacometrics Syst Pharmacol. 2025 Dec;14(12):2232-2243. doi: 
10.1002/psp4.70129. Epub 2025 Nov 4.

Redefining Parameter Estimation and Covariate Selection via Variational 
Autoencoders: One Run Is All You Need.

Rohleff J(1), Bachmann F(1), Nahum U(2)(3), Bräm D(2), Steffens B(2)(4), Pfister 
M(2), Koch G(2), Schropp J(1).

Author information:
(1)Department of Mathematics and Statistics, University of Konstanz, Konstanz, 
Germany.
(2)Pediatric Pharmacology and Pharmacometrics, University Children's Hospital 
Basel (UKBB), University of Basel, Basel, Switzerland.
(3)Institute of Biomedical Engineering and Medical Informatics, University of 
Applied Sciences and Arts Northwestern Switzerland (FHNW), Olten, Switzerland.
(4)School of Life Sciences, University of Applied Sciences and Arts Northwestern 
Switzerland (FHNW), Olten, Switzerland.

Generative Artificial Intelligence (AI) frameworks, such as Variational 
Autoencoders (VAEs), have proven powerful in learning structured representations 
from complex, high-dimensional data. In pharmacometrics (PMX), nonlinear mixed 
effects (NLME) modeling is widely used to capture inter-individual variability 
and link covariates to characterize parameters with the goal of informing key 
decisions in drug research and development. This research combines the strengths 
of both approaches by introducing a VAE framework specifically designed for NLME 
modeling. The proposed method integrates the flexibility of generative AI with 
the interpretability and robustness of mechanism-based PMX modeling. To advance 
covariate selection in PMX, we replace the Evidence Lower Bound objective in 
VAEs with an objective function based on the corrected Bayesian information 
criterion. This enables the simultaneous evaluation of all potential 
covariate-parameter combinations, thereby allowing for automated and joint 
estimation of population parameters and covariate selection within a single run. 
Manual selection and repeated model fitting across covariate combinations are no 
longer required. We demonstrate the effectiveness of this combined AI-PMX 
approach with two representative cases. As the first generative AI-based 
optimization method for NLME modeling, the VAE achieves high-quality results in 
a single run, outperforming traditional stepwise procedures in terms of 
efficiency. As such, the presented approach facilitates automated model 
development, advancing PMX and its applications in model-informed drug 
development.

© 2025 The Author(s). CPT: Pharmacometrics & Systems Pharmacology published by 
Wiley Periodicals LLC on behalf of American Society for Clinical Pharmacology 
and Therapeutics.

DOI: 10.1002/psp4.70129
PMCID: PMC12706427
PMID: 41186137 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


122. Int J Comput Assist Radiol Surg. 2025 Nov 3. doi: 10.1007/s11548-025-03524-9. 
Online ahead of print.

Large language models with retrieval-augmented generation enhance expert 
modelling of Bayesian network for clinical decision support.

Cypko MA(1)(2), Salim MA(3)(4), Kumar A(3)(4), Berliner L(5), Dietz A(6), Stoehr 
M(6), Amft O(3)(4).

Author information:
(1)Hahn-Schickard-Gesellschaft für angewandte Forschung e.V., 79110, Freiburg, 
Germany. cypko@informatik.uni-freiburg.de.
(2)Intelligent Embedded Systems Lab, University of Freiburg, 79110, Freiburg, 
Germany. cypko@informatik.uni-freiburg.de.
(3)Hahn-Schickard-Gesellschaft für angewandte Forschung e.V., 79110, Freiburg, 
Germany.
(4)Intelligent Embedded Systems Lab, University of Freiburg, 79110, Freiburg, 
Germany.
(5)Department of Radiology, Staten Island University Hospital - Northwell 
Health, NY 10305, Staten Island, USA.
(6)Department of Otolaryngology, Head and Neck Surgery, University Hospital 
Leipzig, 04103, Leipzig, Germany.

PURPOSE: Bayesian networks (BNs) are valuable for clinical decision support due 
to their transparency and interpretability. However, BN modelling requires 
considerable manual effort. This study explores how integrating large language 
models (LLMs) with retrieval-augmented generation (RAG) can improve BN modelling 
by increasing efficiency, reducing cognitive workload, and ensuring accuracy.
METHODS: We developed a web-based BN modelling service that integrates an 
LLM-RAG pipeline. A fine-tuned GTE-Large embedding model was employed for 
knowledge retrieval, optimised through recursive chunking and query expansion. 
To ensure accurate BN suggestions, we defined a causal structure for medical 
idioms by unifying existing BN frameworks. GPT-4 and Mixtral 8x7B were used to 
handle complex data interpretation and to generate modelling suggestions, 
respectively. A user study with four clinicians assessed usability, retrieval 
accuracy, and cognitive workload using NASA-TLX. The study demonstrated the 
system's potential for efficient and clinically relevant BN modelling.
RESULTS: The RAG pipeline improved retrieval accuracy and answer relevance. 
Recursive chunking with the fine-tuned embedding model GTE-Large achieved the 
highest retrieval accuracy score (0.9). Query expansion and Hyde optimisation 
enhanced retrieval accuracy for semantic chunking (0.75 to 0.85). Responses 
maintained high faithfulness (  ≥  0.9). However, the LLM occasionally failed to 
adhere to predefined causal structures and medical idioms. All clinicians, 
regardless of BN experience, created comprehensive models within one hour. 
Experienced clinicians produced more complex models, but occasionally introduced 
causality errors, while less experienced users adhered more accurately to 
predefined structures. The tool reduced cognitive workload (2/7 NASA-TLX) and 
was described as intuitive, although workflow interruptions and minor technical 
issues highlighted areas for improvement.
CONCLUSION: Integrating LLM-RAG into BN modelling enhances efficiency and 
accuracy. Future work may focus on automated preprocessing, refinements of the 
user interface, and extending the RAG pipeline with validation steps and 
external biomedical sources. Generative AI holds promise for expert-driven 
knowledge modelling.

© 2025. The Author(s).

DOI: 10.1007/s11548-025-03524-9
PMID: 41182676

Conflict of interest statement: Declarations. Conflict of interest: The authors 
Mario Cypko, Muhammad Agus Salim, Aditya Kumar, Leonard Berliner, Andreas Dietz, 
Matthaeus Stoehr, and Oliver Amft declare that they have no conflict of interest 
related to this work. Informed Consent: This study involved usability testing of 
a web-based Bayesian network modelling tool with four clinicians from the 
University Hospital Leipzig. The research did not involve patient data or 
clinical interventions. Based on institutional and national regulations, ethical 
approval was not required as the study focused solely on tool usability and 
interaction data. Informed consent was obtained from all individual participants 
included in the study. Participants were clinicians from the University Hospital 
Leipzig, who voluntarily agreed to take part in the usability study. Detailed 
consent forms were provided and signed, outlining the scope of data collection, 
the use of recordings, and publication-related aspects.


123. J Med Educ Curric Dev. 2025 Oct 29;12:23821205251391969. doi: 
10.1177/23821205251391969. eCollection 2025 Jan-Dec.

Perceptions and Use of Generative Artificial Intelligence in Medical Students: A 
Multicenter Survey.

Tran C(1), Hryciw BN(2), Moore SW(3), Chaput A(4), Seely AJE(5)(6).

Author information:
(1)Department of Medicine, Faculty of Medicine, University of Ottawa, Ottawa, 
Ontario, Canada.
(2)Department of Critical Care, The Ottawa Hospital, Ottawa, Ontario, Canada.
(3)Emergency Medicine Section, Division of Clinical Sciences, Northern Ontario 
School of Medicine University, Thunder Bay, Ontario, Canada.
(4)Department of Anesthesiology and Pain Medicine, The Ottawa Hospital, 
University of Ottawa, Ottawa, Ontario, Canada.
(5)Division of Thoracic Surgery, Department of Surgery, The Ottawa Hospital, 
Ottawa, Ontario, Canada.
(6)Divisions of Thoracic Surgery and Critical Care Medicine, University of 
Ottawa, Ottawa, Ontario, Canada.

INTRODUCTION: Generative artificial intelligence (AI) has transformative 
potential in medical training, and its role in medicine holds drastic 
implications for patients, healthcare providers, and society; however, its 
current use by medical students is unknown. The study aims to characterize the 
use, frequency of use, and perceptions of generative AI by Canadian medical 
students.
METHODS: A cross-sectional survey was distributed to 6 medical schools in 
Ontario, Canada, to investigate how medical students use generative AI in 
education, clinical settings, and for communication, and to assess the perceived 
barriers and enablers that influence their use.
RESULTS: A total of 167 respondents completed the survey (60.8% female, 69.3% in 
first and second year), and over 78.9% of respondents reported using generative 
AI, with ChatGPT being the most popular model; 53.0% of respondents were 
frequent users and reported using generative AI tools at least once a week. In 
clinical settings, students report using generative AI for learning and 
reviewing medical content, summarizing clinical guidelines, and generating 
differential diagnoses; 92.8% of students were willing to learn how to use 
generative AI to integrate it into their future clinical practice. At the same 
time, most medical students appreciated the limitations of generative AI in 
terms of its risk for inaccuracy (91.6%) and bias (78.9%); 75.9% of participants 
agreed that generative AI should be implemented as a resource or formal teaching 
topic in medical training.
DISCUSSION: The findings of this study may help guide medical education 
institutions in adapting curricula and developing policies to promote the 
ethical and appropriate use of generative AI in medicine.

© The Author(s) 2025.

DOI: 10.1177/23821205251391969
PMCID: PMC12576227
PMID: 41181167

Conflict of interest statement: The authors declared the following potential 
conflicts of interest with respect to the research, authorship, and/or 
publication of this article: Dr Hryciw is a co-founder of Osler AI Inc., a 
medical training and evaluation company. Dr Seely is founder and CEO of 
Therapeutic Monitoring Systems (TMS) Inc., a company that manages several 
point-of-care decision support software's. Dr Seely manages this active COI with 
agreements with the Ottawa Health Research Institute (OHRI) and disclosures on 
grants, presentations, and publications.


124. J Dent Res. 2025 Nov 3:220345251382452. doi: 10.1177/00220345251382452. Online 
ahead of print.

Transcription Accuracy of Automatic Speech Recognition for Orthodontic Clinical 
Records.

O'Kane R(1)(2), Stonehouse-Smith D(1)(2), Ota LCU(2)(3), Patel R(2)(3), Johnson 
N(3), Slipper C(3), Seehra J(1)(2), Papageorgiou SN(1)(4), Cobourne MT(1)(2).

Author information:
(1)Centre for Craniofacial Development & Regeneration, Faculty of Dentistry, 
Oral & Craniofacial Sciences, King's College London, London, UK.
(2)Department of Orthodontics, Guy's and St Thomas' NHS Foundation Trust, 
London, UK.
(3)Department of Orthodontics, Royal Alexandra Children's Hospital, Brighton and 
Sussex University Hospitals NHS Foundation Trust, Sussex, UK.
(4)Clinic of Orthodontics and Pediatric Dentistry, Center of Dental Medicine, 
University of Zurich, Switzerland.

Accurate clinical records are fundamental to dental practice. Automatic speech 
recognition (ASR) has the capacity to convert spoken clinical language into 
written text within the electronic health record; however, the accuracy of ASR 
in natural language processing for clinical dentistry remains uncertain. The aim 
of this study was to investigate the transcriptional accuracy of ASR systems 
using orthodontic clinical records as the experimental model. Specifically, we 
used 4 commercial ASR systems (Heidi Health, DigitalTCO, Dragon Medical One, 
Dragon Professional Anywhere), 5 application programming interfaces (Amazon, 
Google, Speechmatics, Whisper, GPT4oTranscribe), and a 2-stage pipeline coupling 
GPT4oTranscribe with the GPT4o large language model (LLM) for generative error 
correction (GPT4oTranscribeCorrected). Orthodontic diagnostic and treatment 
planning summaries (n = 200; 10 subject domains; 43,408 words; 6 h of audio) 
were narrated and recorded for analysis. The primary outcome was domain word 
error rate (DWER), which investigates clinical terminological transcription 
errors against the Systematized Nomenclature of Medicine Clinical Terms 
(SNOMED-CT) database. Secondary outcomes included nondomain WER (N-DWER), 
lexical accuracy (Recall-Oriented Understudy for Gisting Evaluation [ROUGE] 
score), semantic similarity (Bidirectional Encoder Representations from 
Transformers [BERT] and Bidirectional and Auto-Regressive Transformer [BART] 
scores), hallucinations (transcribed text not in the spoken input), and 
qualitative error analysis. GPT4oTranscribeCorrected was transcriptionally most 
accurate (DWER = 3.5%; WER = 3.7%), with DWER decreasing by 54.9% versus 
GPT4oTranscribe. Heidi Health was the highest-performing commercial system 
(DWER = 6.2%; WER = 5.4%), with Dragon Professional Anywhere being the worst 
(WER = 33.9%). All systems were less accurate with technical vocabulary (DWER > 
N-DWER; P < 0.001), except GPT4oTranscribeCorrected. Significant differences 
were seen across systems for ROUGE, BERT, and BART scores (P < 0.001). Based on 
post hoc pairwise comparisons, GPT4oTranscribeCorrected performed best and 
Dragon Professional Anywhere was consistently worst for lexical and semantic 
errors. Hallucinations were absent except for Whisper (n = 57) and DigitalTCO 
(n = 1). Across systems, background noise increased DWER and WER (P < 0.001). 
Importantly, clinically significant errors were seen with all systems, ranging 
from 2% to 66% (GPT4oTranscribeCorrected clean; Dragon Medical One background 
noise, respectively). Variation in narrator accent had no effect in clean 
conditions (P = 0.65) and a small effect with background noise (P = 0.001). ASR 
systems deliver single-digit transcription error rates, particularly when 
coupled with LLM-based correction, but clinically significant errors persist. 
The verification of clinical records is essential when using current ASR 
systems.

DOI: 10.1177/00220345251382452
PMID: 41178647


125. Eur J Oncol Nurs. 2025 Dec;79:103018. doi: 10.1016/j.ejon.2025.103018. Epub 2025 
Oct 20.

How do GenAI and social media provide psychosocial information support for 
cancer survivors and professionals? An analysis using structural topic modeling 
for exploration and comparison.

Peng Z(1), Su X(2), Hou Y(3).

Author information:
(1)Department of Social Work, Hong Kong Baptist University, Hong Kong Special 
Administrative Region, China. Electronic address: pengziqi@life.hkbu.edu.hk.
(2)School of Sociology and Anthropology, Sun Yat-sen University, Guangzhou, 
China.
(3)Department of Social Work, Hong Kong Baptist University, Hong Kong Special 
Administrative Region, China.

PURPOSE: Digital communication has transformed the landscape of psychosocial 
oncology information for survivors and professionals, potentially influencing 
the unmet informational needs and supports of cancer survivors and healthcare 
professionals. This study aims to explore the psychosocial oncology information 
on social media and generative artificial intelligence (GenAI) using a topic 
modeling approach.
METHODS: A qualitative design incorporating structural topic modeling (STM) was 
utilized to analyze the psychosocial oncology information from social media 
(WeChat and Rednote) and GenAI (DeepSeek and ChatGPT). Social media data 
(n = 1177) were collected using the main keywords "cancer" and "psychosocial", 
while GenAI data were collected through nine standardized prompts. STM 
identified topic prevalence, keywords, topic relations, and temporal trends 
specific to social media posts.
RESULTS: Five topics were identified from social media, highlighting caregivers' 
reflection, psychological intervention and assessment, family dynamics, and 
pediatric cancer care. Sharing caregiving reflection as an emerging trend for 
social support. STM identified seven topics in DeepSeek and five in ChatGPT, 
commonly focusing on psychological distress, coping strategies and support 
systems, healthcare professional practices, and holistic care. DeepSeek 
demonstrated more critical reflection on survivorship challenges and 
psychosocial care barriers, but identified misinformation. Social media presents 
in-depth topics, whereas GenAI offers synthesized overviews. Help-seeking 
resources were limited across both sources.
CONCLUSIONS: Findings emphasized the significance of information strategies in 
psychosocial oncology care. GenAI should be fine-tuning/training with relevant 
data for improving practices. Social media and content creators are effective 
tools for promoting evidence-based information. A hybrid model that integrates 
professional oversight, GenAI, and cloud platforms is essential to ensure 
ethical compliance with psychosocial care.

Copyright © 2025 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.ejon.2025.103018
PMID: 41172637 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare no conflicts of interest.


126. Biomed Eng Online. 2025 Oct 29;24(1):125. doi: 10.1186/s12938-025-01458-6.

Multimodal contrastive learning on rs-fMRI to quantify whole-brain network 
recovery after hypothalamic hamartoma surgery.

Jeyabose A(1), Robinson B(2), Leggio O(2), Kazemi MH(2), Boerwinkle VL(2).

Author information:
(1)Clinical Resting State fMRI Service, Department of Neurology, University of 
North Carolina at Chapel Hill, Chapel Hill, NC, USA. 
andrew_jeyabose@med.unc.edu.
(2)Clinical Resting State fMRI Service, Department of Neurology, University of 
North Carolina at Chapel Hill, Chapel Hill, NC, USA.

Erratum in
    Biomed Eng Online. 2025 Dec 19;24(1):146. doi: 10.1186/s12938-025-01491-5.

INTRODUCTION: Epilepsy due to hypothalamic hamartoma (HH) is associated with 
epileptic encephalopathy and often requires surgical intervention, as 
medications are ineffective at reducing the seizures. However, the first step of 
disentangling the impact of the surgery on the broader whole-brain networks, a 
biomarker of encephalopathy compared to controls, is not quantified. Subtle pre- 
and post-operative networks can elude conventional rs-fMRI analysis.
METHODS: We retrospectively analyzed rs-fMRI from 56 HH patients scanned before 
and 6 months after surgery. We developed a two-stage contrastive learning-based 
algorithm to classify the motor, vision, language, frontal, and temporal 
networks as pre- vs post-operative. In stage one, a multimodal contrastive 
encoder jointly ingests 3D spatial Independent Component Analysis (ICA) maps and 
their corresponding 1D temporal ICA time series to learn embeddings that 
distinguish pre-operative from post-operative states for each network while 
separating embeddings of different networks. In stage two, a lightweight 
classifier refines these embeddings, augmented by original ICA inputs, to 
classify each network as pre-operative or post-operative.
RESULTS: Visualization of the learned feature space with t-SNE revealed clear 
separation by pre- vs post-surgical condition across all five networks. Across 
networks, mean accuracy ranged from 0.85 to 0.90, sensitivity from 0.79 to 0.90, 
specificity from 0.87 to 0.93, F1-score from 0.83 to 0.90 and AUC from 0.90 to 
0.94 in stratified cross validation.
CONCLUSIONS: Contrastive learning can sensitively detect functional shifts in 
critical cortical networks that previous traditional analyses may overlook. 
These findings inform broader shifts in whole-brain network functioning 
following effective HH surgery and establish a featurewise distinction between 
preoperative and postoperative states, motivating future studies that compare HH 
patients to healthy controls to quantify network recovery.

© 2025. The Author(s).

DOI: 10.1186/s12938-025-01458-6
PMCID: PMC12573949
PMID: 41163168 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was approved by the Institutional Review Board of the 
University of North Carolina at Chapel Hill (IRB # 24–0833). All rs-fMRI data 
sets were obtained under informed consent from participants, and all methods 
were carried out in accordance with relevant guidelines and regulations. 
Competing interests: The authors declare no competing interests.


127. Front Cell Dev Biol. 2025 Oct 13;13:1674393. doi: 10.3389/fcell.2025.1674393. 
eCollection 2025.

CausalFormer-HMC: a hybrid memory-driven transformer with causal reasoning and 
counterfactual explainability for leukemia diagnosis.

Jammal F(1), Dahab M(1).

Author information:
(1)Department of Computer Science, Faculty of Computing and Information 
Technology, King Abdulaziz University, Jeddah, Saudi Arabia.

Acute Lymphoblastic Leukemia (ALL) is a prevalent malignancy particularly among 
children. It poses diagnostic challenges due to its morphological similarities 
with normal cells and the limitations of conventional methods like bone marrow 
biopsies, which are invasive and resource-intensive. This study introduces 
Causal-Former-HMC, a novel hybrid AI architecture integrating convolutional 
neural networks, vision transformers, and a causal graph learner with 
counterfactual reasoning to enhance diagnostic precision and interpretability 
from peripheral blood smear (PBS) images. We utilized two robust datasets: the 
ALL Image collection, comprising 89 patients and 3,256 PBS images (504 benign, 
2,752 malignant across Pro B, Pre B, and Early Pre B subtypes), and C-NMC 
dataset, containing 15,135 segmented cell images from 118 patients (7,272 
leukemic, 3,389 normal). To address class imbalance, we implemented class-aware 
data augmentation, standardizing image counts across classes and resizing to 128 
× 128 pixels for compatibility with our model. The proposed model is evaluated 
via stratified 5-fold cross-validation with Nadam, SGD, and Radam (fractional) 
optimizers, Causal-Former-HMC achieved perfect classification accuracy (100%) 
and macro-averaged F1-scores on the ALL dataset, and up to 98.5% accuracy with 
0.9975 ROC-AUC on the C-NMC dataset hence demonstrating superior generalization. 
Interpretability was ensured through advanced explainable AI techniques, 
including Grad-CAM, LIME, Integrated Gradients, and SHAP, which consistently 
highlighted attention to clinically relevant features such as nuclear contour 
irregularities and chromatin condensation. These results underscore the 
potential of the model to deliver non-invasive, accurate and transparent 
diagnostics that pave the way for its integration into clinical hematology 
workflows and advancing AI-driven leukemia screening paradigms. Index 
Terms-Acute Lymphoblastic Leukemia (ALL); Causal-Former-HMC; Hybrid Deep 
Learning; Peripheral Blood Smear Classification; Explainable AI in Medical 
Imaging.

Copyright © 2025 Jammal and Dahab.

DOI: 10.3389/fcell.2025.1674393
PMCID: PMC12554709
PMID: 41158308

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


128. J Clin Med. 2025 Oct 13;14(20):7216. doi: 10.3390/jcm14207216.

Development and Validation of Transformer- and Convolutional Neural 
Network-Based Deep Learning Models to Predict Curve Progression in Adolescent 
Idiopathic Scoliosis.

Takahashi S(1), Ichikawa S(2), Watanabe K(3), Ueda H(4), Arima H(5), Yamato 
Y(5), Takeuchi T(6), Hosogane N(6), Okamoto M(2), Umezu M(2), Oba H(7), Kondo 
Y(2), Seki S(8).

Author information:
(1)Department of Orthopaedic Surgery, Osaka Metropolitan University, 1-4-3 
Asahimachi Abeno-ku, Osaka 545-8585, Japan.
(2)Department of Radiological Technology, Graduate School of Health Sciences, 
Niigata University, 2-746 Asahimachi-dori, Chuo-ku, Niigata 951-8518, Japan.
(3)Niigata Spine Surgery Center, Kameda Daiichi Hospital, Niigata 950-0165, 
Japan.
(4)Department of Orthopaedic Surgery, Dokkyo Medical University, 880 
Kitakobayashi, Shimotsuga, Mibu 321-0293, Japan.
(5)Department of Orthopaedic Surgery, Shinshu University School of Medicine, 
Hamamatsu 431-3192, Japan.
(6)Department of Orthopaedic Surgery, Kyorin University, Tokyo 181-8611, Japan.
(7)Department of Orthopaedic Surgery, Shinshu University School of Medicine, 
Matsumoto 390-8621, Japan.
(8)Department of Orthopaedic Surgery, Faculty of Medicine, University of Toyama, 
2630 Sugitani, Toyama 930-0194, Japan.

Background/Objectives: The clinical management of adolescent idiopathic 
scoliosis (AIS) is hindered by the inability to accurately predict curve 
progression. Although skeletal maturity and the initial Cobb angle are 
established predictors of progression, their combined predictive accuracy 
remains limited. This study aimed to develop a robust and interpretable 
artificial intelligence (AI) system using deep learning (DL) models to predict 
the progression of scoliosis using only standing frontal radiographs. Methods: 
We conducted a multicenter study involving 542 patients with AIS. After 
excluding 52 borderline progression cases (6-9° progression in the Cobb angle), 
294 and 196 patients were assigned to progression (≥10° increase) and 
non-progression (≤5° increase) groups, respectively, considering a 2-year 
follow-up. Frontal whole spinal radiographs were preprocessed using histogram 
equalization and divided into two regions of interest (ROIs) (ROI 1, skull 
base-femoral head; ROI 2, C7-iliac crest). Six pretrained DL models, including 
convolutional neural networks (CNNs) and transformer-based models, were trained 
on the radiograph images. Gradient-weighted class activation mapping (Grad-CAM) 
was further performed for model interpretation. Results: Ensemble models 
outperformed individual ones, with the average ensemble model achieving area 
under the curve (AUC) values of 0.769 for ROI 1 and 0.755 for ROI 2. Grad-CAM 
revealed that the CNNs tended to focus on the local curve apex, whereas the 
transformer-based models demonstrated global attention across the spine, ribs, 
and pelvis. Models trained on ROI 2 performed comparably with respect to those 
using ROI 1, supporting the feasibility of image standardization without a loss 
of accuracy. Conclusions: This study establishes the clinical potential of 
transformer-based DL models for predicting the progression of scoliosis using 
only plain radiographs. Our multicenter approach, high AUC values, and 
interpretable architectures support the integration of AI into clinical 
decision-making for the early treatment of AIS.

DOI: 10.3390/jcm14207216
PMCID: PMC12565354
PMID: 41156086

Conflict of interest statement: We declare that we have no conflicts of interest 
in this study.


129. Sheng Wu Yi Xue Gong Cheng Xue Za Zhi. 2025 Oct 25;42(5):994-1004. doi: 
10.7507/1001-5515.202504005.

[The design and application of a genu valgum gait recognition model based on 
triple attention mechanism and spatial hierarchical pooling strategy].

[Article in Chinese; Abstract available in Chinese from the publisher]

Song X(1), Qian K(2), Hou X(3), Wang Y(4).

Author information:
(1)Department of Physical Education, Jiangnan University, Wuxi, Jiangsu 214122, 
P. R. China.
(2)School of Artificial Intelligence and Computer Science, Jiangnan University, 
Wuxi, Jiangsu 214122, P. R. China.
(3)School of Innovation Engineering, Macau University of Science and Technology, 
Macau SAR 0999078, P. R. China.
(4)School of Science and Engineering, The Chinese University of Hong Kong, 
Shenzhen, Guangdong 518172, P. R. China.

To facilitate the early intelligent screening of pediatric genu valgum, this 
study develops a deep learning-based gait recognition model tailored for 
clinical application. The model is constructed upon a three-dimensional residual 
network architecture and incorporates a triplet attention module alongside a 
spatial hierarchical pooling module, jointly enhancing feature interaction 
across temporal, spatial, and channel dimensions. This design ensures an optimal 
balance between representational capacity and computational efficiency. 
Evaluated on a self-constructed dataset, the model achieves precision of 98.0%, 
97.1%, and 96.5%, recall rates of 97.5%, 97.0%, and 95.0%, and F 1-scores of 
0.98, 0.97, and 0.96 on the training, validation, and test sets, respectively, 
demonstrating excellent recognition performance and strong generalization 
ability. Ablation experiments confirm the importance of the proposed model's 
core components in improving performance, and comparative experiments further 
highlight its significant advantages in recognition accuracy and robustness. 
Visualization experiments reveal that the model effectively focuses on key 
regions of gait images, with attention regions aligning closely with clinical 
anatomical landmarks, thereby enhancing the interpretability of the model's 
decision-making in clinical applications. In summary, the proposed model not 
only offers an efficient and reliable technical solution for early intelligent 
screening of genu valgum in children, but also provides a practical pathway for 
applying gait recognition technology in medical diagnosis.

Publisher: 
为实现儿童膝外翻的早期智能筛查，本文设计并构建了一种融合深度学习机制的步态识别模型。该模型基于三维残差网络架构，集成三重注意力模块与空间分层池化模块，有效强化时域、空域与通道维度的信息交互能力，且兼顾特征提取效果与计算效率。模型在自建数据集训练、验证和测试，其精确率分别为98.0%、97.1%和96.5%，召回率分别为97.5%、97.0%和95.0%，F 
1分数分别为0.98、0.97和0.96，展现出优异的识别性能与良好的泛化能力。通过消融实验验证了本文构建模型的关键组件对性能提升的重要性，对比实验进一步突显了其在识别精度与鲁棒性方面的显著优势，以及通过可视化实验发现本文模型能有效聚焦于步态图像中的关键区域，其关注区域与临床解剖标志相吻合，提升了模型决策在临床应用中的可解释性。综上所述，本文所提出的模型不仅为儿童膝外翻的早期智能筛查提供了高效可靠的技术手段，也为步态识别技术在医学诊断中的应用提供了切实可行的实践路径。.

DOI: 10.7507/1001-5515.202504005
PMCID: PMC12568726
PMID: 41152171 [Indexed for MEDLINE]

Conflict of interest statement: 利益冲突说明：本文全体作者均声明不存在利益冲突。


130. JHEP Rep. 2025 Jul 31;7(11):101539. doi: 10.1016/j.jhepr.2025.101539. 
eCollection 2025 Nov.

Artificial intelligence as a ploy to delve into the intricate link between 
genetics and mitochondria in patients with MASLD.

Longo M(1), Paolini E(1), Meroni M(1), Ripolone M(2), Napoli L(2), Gentile F(3), 
Cespiati A(1)(4), Trombetta E(5), Lombardi R(1)(4), Maggioni M(6), Alisi A(7), 
Miele L(8)(9), Liguori A(8), Soardo G(10), Gasbarrini A(8)(9), Moggio M(2), 
Fracanzani AL(1)(4), Dongiovanni P(1).

Author information:
(1)Medicine and Metabolic Diseases, Fondazione IRCCS Cà Granda Ospedale Maggiore 
Policlinico, Milan, Italy.
(2)Neuromuscular and Rare Diseases Unit, Department of Neuroscience, Fondazione 
IRCCS Ca' Granda Ospedale Maggiore Policlinico, Milan, Italy.
(3)Biology of Myelin Unit, Division of Genetics and Cell Biology, IRCCS San 
Raffaele Scientific Institute, Milan, Italy.
(4)Department of Pathophysiology and Transplantation, Università Degli Studi di 
Milano, Milan, Italy.
(5)Clinical Pathology, Fondazione IRCCS Ca' Granda Ospedale Maggiore 
Policlinico, Milan, Italy.
(6)Division of Pathology, Fondazione IRCCS Ca' Granda Ospedale Maggiore 
Policlinico, Milan, Italy.
(7)Research Unit of Genetics of Complex Phenotypes, "Bambino Gesù" Children's 
Hospital, IRCCS, Rome, Italy.
(8)Department of Translational Medicine and Surgery, Catholic University, 
Fondazione Policlinico Universitario A. Gemelli IRCCS, Rome, Italy.
(9)CEMAD Unit, Digestive Disease Center, Fondazione Policlinico Universitario A. 
Gemelli IRCCS, Rome, Italy.
(10)Department of Medical Area (DAME), University of Udine and Italian Liver 
Foundation, Bldg Q AREA Science Park - Basovizza Campus, Trieste, Italy.

BACKGROUND & AIMS: Mitochondrial (mt-) D-loop and cell-free circulating (ccf-) 
mtDNA fragments, respectively reflecting mt-mass and tissue damage, are 
promising metabolic dysfunction-associated steatotic liver disease (MASLD) 
biomarkers. We previously found that PNPLA3/MBOAT7/TM6SF2 deficiency in HepG2 
cells increased mt-mass, D-loop levels, and ccf-COXIII release. We explored 
mt-biogenesis and mt-biomarkers in patients with MASLD stratified by the number 
of risk variants (NRV = 3). We exploited GPT-4 to develop and validate new risk 
scores, predicting MASLD evolution, in two independent cohorts by integrating 
anthropometric and genetic data with mt-biomarkers.
METHODS: A cohort of 28 patients with MASLD (Discovery cohort) was consecutively 
enrolled for hepatic mt-dynamics assessment by transmission electron microscopy 
and immunohistochemistry. Data were confirmed by quantitative real time-PCR in a 
retrospective cohort (Hepatic Validation, n = 184). D-loop and ccf-COXIII were 
retrospectively measured in peripheral blood mononuclear cells and serum samples 
of biopsied outpatients with MASLD (Serum Validation cohort, n = 824) and 
individuals with non-invasive MASLD diagnosis (n = 386, Non-invasive cohort). 
Risk scores were developed using random forest algorithms.
RESULTS: In the Discovery and Hepatic Validation cohorts, the 
PNPLA3/MBOAT7/TM6SF2 variants altered hepatic mt-dynamics, enhancing mt-content 
and D-loop levels (p <0.05) through the p38/PGC-1α pathway. Furthermore, NRV = 3 
patients showed an increase in mt-fragmentation at transmission electron 
microscopy (TEM) and ccf-COXIII release (p <0.05). In the Serum Validation 
cohort, circulating D-loop and ccf-COXIII positively correlated with genetics [β 
D-loop :0.17 (95% CI: 0.04-0.29), p = 0.01; βccf-COXIII:0.33 (95% CI: 
0.19-0.46), p <0.0001] and MASLD severity [OR D-loop :1.31 (95% CI: 1.01-1.71), 
p = 0.03; ORccf-COXIII:2.41 (95% CI: 1.69-3.44), p <0.0001] at multivariate 
analysis. Random forest allowed prediction models named Mitochondrial, 
Anthropometric, and Genetic Integration with Computational intelligence for 
assessing hepatocellular carcinoma risk (MAGIC-H), considering age, BMI 
genetics, D-loop, and ccf-COXIII. In both Serum and Non-invasive cohorts, the 
MAGIC-H score reached AUC >85% in identifying HCC cases regardless of cirrhosis, 
outperforming existing non-invasive tests.
CONCLUSIONS: Mt-biomarkers have a prognostic significance in 
genetically-predisposed patients with MASLD.
IMPACT AND IMPLICATIONS: The study highlights that genetic variants in PNPLA3, 
MBOAT7, and TM6SF2 genes deeply contribute to metabolic dysfunction-associated 
steatotic liver disease (MASLD) progression by affecting hepatic mitochondrial 
adaptability. It also identified two novel biomarkers of mitochondrial origin 
which are strongly linked to disease severity and genetic background of patients 
with MASLD. The use of generative artificial intelligence tools, such as GPT-4, 
can enhance the use of biomarkers and polygenic risk scores for clinical risk 
stratification. We developed a customized version of GPT-4 (rsGPT-4), which 
identified a machine-learning approach (random forest) as the best method for 
creating prediction models for metabolic dysfunction-associated steatohepatitis, 
fibrosis, and hepatocellular carcinoma. The new scores combined the two 
mitochondrial biomarkers, genetic data, and anthropometric data and outperformed 
existing non-invasive tests for monitoring patients with MASLD.

© 2025 The Author(s).

DOI: 10.1016/j.jhepr.2025.101539
PMCID: PMC12550784
PMID: 41143242

Conflict of interest statement: The authors declare that they have no conflicts 
of interest. Please refer to the accompanying ICMJE disclosure forms for further 
details.


131. Transl Pediatr. 2025 Sep 30;14(9):2145-2157. doi: 10.21037/tp-2025-399. Epub 
2025 Sep 26.

An integrated and interpretable machine learning framework for Kawasaki disease 
diagnosis and risk prediction.

Wang D(1), Li F(2), Xie T(1), Zang X(1)(3), Chen M(1).

Author information:
(1)Department of Pediatrics, The First Affiliated Hospital of University of 
Science and Technology of China, Division of Life Sciences and Medicine, 
University of Science and Technology of China, Hefei, China.
(2)Department of Pediatrics, The First People's Hospital of Wuhu, Wuhu, China.
(3)Institute of Public Health Sciences, Division of Life Sciences and Medicine, 
University of Science and Technology of China, Hefei, China.

BACKGROUND: Early identification of Kawasaki disease (KD) and accurate 
prediction of its associated complications are critical for optimizing treatment 
strategies and improving clinical outcomes. While machine learning has shown 
promise in KD-related studies, most existing models are limited to single tasks 
with varying feature sets, lacking an integrated framework. This fragmentation 
hinders clinical applicability and constrains generalizability. Therefore, this 
study aimed to develop a unified and interpretable machine learning framework 
for KD diagnosis and risk prediction, with the goal of enhancing clinical 
relevance and real-world applicability.
METHODS: We retrospectively collected data from 2,133 febrile pediatric patients 
treated at The First Affiliated Hospital of University of Science and Technology 
of China between January 1, 2018, and December 31, 2022. After excluding 
patients older than 5 years or with incomplete records, a total of 919 
cases-including both typical and atypical KD-were included. Using 29 common 
clinical features, we developed a unified light gradient boosting machine 
(LightGBM)-based model for KD diagnosis, intravenous immunoglobulin (IVIG) 
resistance prediction, and coronary artery lesion (CAL) risk assessment. The 
dataset was split into training and validation sets at an 8:2 ratio, and 
five-fold cross-validation was performed to ensure robustness. Model performance 
was evaluated using accuracy, area under the receiver operating characteristic 
(ROC) curve (AUC), sensitivity, specificity, positive predictive value (PPV), 
and negative predictive value (NPV). Feature importance and model 
interpretability were assessed using SHapley Additive exPlanations (SHAP). To 
further assess its clinical utility, we compared the model's diagnostic 
performance with that of pediatric clinicians and an advanced large language 
model (ChatGPT).
RESULTS: The KD diagnostic task achieved an AUC of 0.999, with a sensitivity of 
0.984 and specificity of 0.974. The IVIG resistance prediction task yielded an 
AUC of 0.888, sensitivity of 0.600, and specificity of 0.979. For CALs risk 
prediction, the AUC was 0.783, with a sensitivity of 0.529 and specificity of 
0.984. SHAP analysis identified distinct sets of top-ranking features for each 
task, reflecting the underlying clinical heterogeneity. Notably, variables such 
as inflammatory markers, immune-related indicators, and characteristic clinical 
signs of KD consistently contributed to model predictions. In a comparative 
study, our model achieved accuracies of 0.900, 0.800, and 0.757 for KD 
diagnosis, IVIG resistance, and CAL prediction, respectively, consistently 
outperforming pediatricians with over 5 years of experience and ChatGPT, 
highlighting its potential as a clinical decision support tool.
CONCLUSIONS: This study presents a unified machine learning framework that 
accurately supports KD diagnosis, IVIG resistance prediction, and CAL risk 
assessment. By leveraging a common set of clinical features, the model enhances 
clinical applicability and lays the groundwork for task-specific management and 
precision intervention in KD.

Copyright © 2025 AME Publishing Company. All rights reserved.

DOI: 10.21037/tp-2025-399
PMCID: PMC12552161
PMID: 41141684

Conflict of interest statement: Conflicts of Interest: All authors have 
completed the ICMJE uniform disclosure form (available at 
https://tp.amegroups.com/article/view/10.21037/tp-2025-399/coif). The authors 
have no conflicts of interest to declare.


132. Anal Chim Acta. 2025 Dec 8;1378:344677. doi: 10.1016/j.aca.2025.344677. Epub 
2025 Sep 24.

Multi-modal fusion model combines SERS spectroscopy and clinicopathological 
features to predict neoadjuvant therapy response in breast cancer.

Li M(1), Xie F(2), Yin L(1), Cai X(3), Yu W(1), You Z(1), Liu T(1), Chen L(1), 
Yu S(1), Wu G(4), Wang S(5).

Author information:
(1)School of Electronic Engineering, Beijing University of Posts and 
Telecommunications, Beijing, 100876, China.
(2)Department of Breast Center, Peking University People' s Hospital, Beijing, 
100044, China.
(3)Department of Obstetrics and Gynecology, Chongqing Health Center for Women 
and Children (Women and Children's Hospital of Chongqing Medical University), 
400013, Chongqing, China.
(4)School of Electronic Engineering, Beijing University of Posts and 
Telecommunications, Beijing, 100876, China. Electronic address: 
wuguohua@bupt.edu.cn.
(5)Department of Breast Center, Peking University People' s Hospital, Beijing, 
100044, China. Electronic address: shuwang@pkuph.edu.cn.

Breast cancer remains a significant global health threat to women, with 
neoadjuvant therapy (NAT) playing a critical role in treatment. Early prediction 
of NAT efficacy is essential for personalizing therapy and improving patient 
outcomes. The Miller-Payne (MP) grading system is a widely accepted standard for 
evaluating treatment response, categorizing patients as non-major histologic 
responders (MP1∼MP3) or major histologic responders (MP4∼MP5). This study 
developed a multi-modal fusion model integrating clinicopathological features 
and pre-treatment serum surface-enhanced Raman spectroscopy (SERS) data to 
predict NAT response in breast cancer patients. Leveraging Principal Component 
Analysis (PCA) for spectral dimensionality reduction and a Transformer 
architecture for feature extraction, the model achieved an accuracy of 92.6 % on 
the training cohort, significantly outperforming single-modal models using only 
SERS or clinicopathological features. Double-blind validation on an independent 
cohort confirmed the model's generalizability with an accuracy of 90 % and an 
area under the receiver operating characteristic curve (AUC) of 93 %. SERS 
analysis revealed significant spectral differences related to uric acid, 
tryptophan, phospholipids, and collagen, which have potential as biomarkers for 
NAT efficacy prediction. This study innovatively combined serum SERS data with 
clinicopathological features to predict NAT response in breast cancer patients. 
The multi-modal fusion model, enhanced by PCA and a Transformer architecture, 
captured biomolecular and clinical information, improving prediction accuracy 
and robustness. This non-invasive, cost-effective tool enables clinicians to 
avoid ineffective NAT, optimize treatment strategies, and improve patient 
outcomes.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.aca.2025.344677
PMID: 41135987 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


133. Clin Kidney J. 2025 Oct 6;18(10):sfaf308. doi: 10.1093/ckj/sfaf308. eCollection 
2025 Oct.

ECOSBot: a multicenter validation pilot study of a generative AI tool for 
OSCE-based nephrology training.

Bentegeac R(1)(2), Florens N(3)(4)(5), Maanaoui M(6)(7), Maisons V(8), Lanot 
A(9)(10), Bobot M(11)(12), Brilland B(13)(14), Glowacki F(6), Gérard E(15), 
Hazzan M(6), Amouyel P(1)(2), Le Guellec B(2)(16)(17), Hamroun A(1)(2)(5)(6).

Author information:
(1)Lille University, Department of Public Health - Epidemiology, Lille 
University Hospital Center, Lille, France.
(2)UMR1167 RID-AGE, Pasteur Institute of Lille, Inserm, Lille University, Lille 
University Hospital Center, Lille, France.
(3)Nephrology Department, Hôpitaux Universitaires de Strasbourg, Strasbourg, 
France.
(4)Inserm UMR_S 1109 Immuno-Rhumatology Laboratory, Translational Medicine 
Federation of Strasbourg (FMTS), FHU Target, Faculté de Médecine, Université de 
Strasbourg, Strasbourg, France.
(5)INI-CRCT (Cardiovascular and Renal Trialists), F-CRIN Network, Strasbourg, 
France.
(6)Lille University, Department of Nephrology, Lille University Hospital Center, 
Lille, France.
(7)U1190 Translational Research for Diabetes, INSERM, Pasteur Institute of 
Lille, Lille University, Lille, France.
(8)Service de Néphrologie, CHU de Tours, Tours, France; U1246, INSERM, SPHERE, 
Université de Tours, Université de Nantes, Tours, Nantes, France.
(9)Normandie Université, UNICAEN, CHU de Caen Normandie, Néphrologie, CAEN, 
France.
(10)ANTICIPE U1086 INSERM-UCN, Centre François Baclesse, Caen, France.
(11)Centre de Néphrologie et Transplantation Rénale, Hôpital de la Conception, 
AP-HM, Marseille, France.
(12)Aix Marseille Univ., INSERM 1263, INRAE 1260, C2VN, CERIMED, Marseille, 
France.
(13)Service de Néphrologie-Dialyse-Transplantation, CHU Angers, Angers, France.
(14)Univ. Angers, Nantes Université, Inserm, CNRS, CRCI2NA, SFR ICAT, Angers, 
France.
(15)Université de Lille, CHU Lille, ULR 2694-METRICS: Évaluation des 
Technologies de Santé et des Pratiques Médicales, Lille, France.
(16)Lille University, Department of Neuroradiology, CHU Lille, Salengro 
Hospital,  Lille, France.
(17)INSERM, U1172-LilNCog-Lille Neuroscience & Cognition, Université de Lille, 
Lille, France.

BACKGROUND: Developing diagnostic reasoning in nephrology is particularly 
challenging due to its pathophysiological complexity and reliance on abstract 
clinical data. Objective Structured Clinical Examinations (OSCEs) are pivotal 
for nephrology training but remain resource-intensive and difficult to scale. 
Generative artificial intelligence (AI) offers a promising alternative, yet its 
capacity to emulate nephrology-specific OSCEs has not been formally assessed.
METHODS: We developed ECOSBot, a web-based tool powered by GPT-4o, to simulate 
both standardized patients and examiners for nephrology-focused OSCEs. In this 
multicenter prospective study, undergraduate medical students from five French 
medical schools interacted with ECOSBot across four clinical stations. All 
interactions were double-rated by nephrology faculty members to establish a gold 
standard. ECOSBot's performance was evaluated against this standard using four 
criteria (script coverage, authenticity, correctness and relevance) for patient 
simulation, and via checklists and competency-based ratings for examiner 
scoring. Usability was assessed using the Chatbot Usability Questionnaire (CUQ), 
adapted to include six items on feedback quality.
RESULTS: Ninety-one students generated 2939 prompts across 184 OSCE sessions. 
ECOSBot demonstrated high fidelity in patient simulation: authenticity 98.6% 
[95% confidence interval (CI) 98.2-99.0], correctness 98.3% (95% CI 97.9-98.7) 
and relevance 99.2% (95% CI 98.9-99.5), including during exchanges not 
explicitly covered by the pre-specified scenario. As an examiner, ECOSBot showed 
strong agreement with human raters on global scores [intraclass correlation 
coefficient (ICC) = 0.94, 95% CI 0.91-0.96], consistent across case formats, 
training levels and institutions. However, scoring of attitude and communication 
skills was less reliable (ICC = 0.44, 95% CI 0.28-0.58). Median CUQ score was 
69.7/100, with 91.7% of students finding the tool highly useful for OSCE 
preparation in nephrology.
CONCLUSIONS: ECOSBot reliably simulated both roles in nephrology OSCEs with high 
fidelity and strong alignment with expert rating. While challenges remain for 
subjective skill assessment, this tool offers a scalable and autonomous solution 
to enhance nephrology education.

© The Author(s) 2025. Published by Oxford University Press on behalf of the ERA.

DOI: 10.1093/ckj/sfaf308
PMCID: PMC12541372
PMID: 41133180

Conflict of interest statement: The authors declare no competing interests 
related to the content of this article.


134. Int J Audiol. 2025 Oct 23:1-11. doi: 10.1080/14992027.2025.2569927. Online ahead 
of print.

Validity, reliability, and readability of Artificial Intelligence chatbots as 
public sources of information on hearing loss: a comparative evaluation of 
ChatGPT, Bing, Gemini, and Perplexity.

Pourhoseingholi MA(1)(2), Killan C(1)(2), Rafiee S(3), Hoare DJ(1)(2), Wray 
N(1)(2), Bateman P(1)(2).

Author information:
(1)Hearing Sciences, Mental Health and Clinical Neurosciences, School of 
Medicine, University of Nottingham, Nottingham, United Kingdom.
(2)National Institute for Health and Care Research (NIHR) Nottingham Biomedical 
Research Centre, Nottingham, United Kingdom.
(3)School of Medicine, Shahid Beheshti University of Medical Sciences, Tehran, 
Iran.

OBJECTIVE: To assess the validity, reliability and readability of four AI 
chatbots for hearing-health information.
DESIGN AND STUDY SAMPLE: Three audiologists created 100 questions covering adult 
hearing loss, paediatric hearing, hearing aids, tinnitus and cochlear implants 
(20 each). Questions were submitted twice to ChatGPT-3.5, Bing AI, Gemini and 
Perplexity. Answers were scored for factual accuracy and completeness on a 
five-point Global Quality Score. Validity was defined using low (score = 5) and 
high (score ≥ 4) thresholds. Internal consistency was estimated with Cronbach's 
α; readability with the Flesch Reading Ease Score (FRES) and Flesch-Kincaid 
Grade Level (FKGL). All scoring was completed independently by two blinded 
reviewers; discrepancies were resolved by consensus.
RESULTS: Under the low threshold ChatGPT-3.5 and Perplexity were most valid (84% 
and 79%); high-threshold validity fell to 37% and 34%. Perplexity had the 
highest overall reliability (α = 0.83) yet α dropped below 0.70 for 
cochlear-implant, tinnitus and hearing-aid questions. 84% percent of outputs 
were "Difficult"/"Very Difficult" and 68% read at college level.
CONCLUSIONS: AI chatbots deliver generally accurate hearing-health content, but 
high-threshold accuracy, domain-specific reliability and readability remain 
suboptimal. They should supplement, not replace the professional counselling. 
Continued optimisation and external validation are needed before routine 
clinical recommendation.

DOI: 10.1080/14992027.2025.2569927
PMID: 41129664


135. Public Health Pract (Oxf). 2025 Sep 26;10:100653. doi: 
10.1016/j.puhip.2025.100653. eCollection 2025 Dec.

Digital screen exposure in infants, children and adolescents: a systematic 
review of existing recommendations.

Nuvoli V(1), Camanni M(2), Mariani I(2), Ponte S(2), Black M(3), Lazzerini 
M(2)(4).

Author information:
(1)University of Trieste, Italy.
(2)WHO Collaborating Centre for Maternal and Child Health, Institute for 
Maternal and Child Health - IRCCS "Burlo Garofolo", Trieste, Italy.
(3)Department of Public Health Policy and Systems, University of Liverpool, 
Liverpool, UK.
(4)Maternal Adolescent Reproductive and Child Health Care Centre, London School 
of Hygiene & Tropical Medicine, London, UK.

OBJECTIVES: This review aimed to synthesise existing recommendations - issued by 
either United Nations agencies, scientific societies, governmental health 
authorities, non-profit organisations, or others - on digital screen exposure in 
infants, children, and adolescents.
STUDY DESIGN: Systematic review.
METHODS: A wide search strategy was utilised - including digital databases 
(PubMed, EMBASE, Web of Science), Google, ChatGPT, reference lists from and grey 
literature -up to July 2024. Identified documents were categorized by type, 
issuing institutions, audience, publication year, and age groups. A synthesis of 
recommendations on screen time quantity was provided, comparing recommendations 
from the World Health Organization (WHO) with those from other 
institutions/bodies.
RESULTS: Out of 85,425 records retrieved, 41 documents were identified. Of 
these, 23 were published by scientific societies, 13 by government/health 
authorities, two by the WHO, and three by non-profit organisations. In relation 
to digital screen quantity, most documents aligned: i) for children under 2 
years, most documents recommended zero exposure (n = 20/22); ii) for children 
aged 2-5 years, the majority recommended limiting screen time to 1 h per day 
(n = 17/21); iii) for children over 5 years and adolescents, the most permissive 
guidelines suggested a maximum of 2 h per day (n = 8/10). Existing 
recommendations were either equally strict or more restrictive than WHO 
guidelines. In regard to the quality of digital screen exposure, 10 key points 
emerged, along with additional guidance for parents, schools, healthcare 
professionals, researchers, and industry.
CONCLUSIONS: There is consensus on the need to limit the quantity of digital 
screen time for infants, children and adolescents and to ensure good quality 
exposure. This alignment provides a strong basis for governments and other 
bodies to agree on local recommendations, as well as strategies to improve their 
implementation in practice.

© 2025 Published by Elsevier Ltd on behalf of The Royal Society for Public 
Health.

DOI: 10.1016/j.puhip.2025.100653
PMCID: PMC12538403
PMID: 41127851

Conflict of interest statement: The authors declare that they have no known 
competing financial interests or personal relationships that could have appeared 
to influence the work reported in this paper.


136. Intell Med. 2025 Aug 6:10.1016/j.imed.2025.07.001. doi: 
10.1016/j.imed.2025.07.001. Online ahead of print.

An artificial intelligence-based framework for Alzheimer's disease diagnosis 
from magnetic resonance imaging volumes via video vision transformer.

Akan T(1)(2), Alp S(3), Bhuiyan S(4), Disbrow EA(2)(5)(6)(7), Conrad SA(1), 
Vanchiere JA(1)(8), Kevil CG(4)(9), Bhuiyan MAN(1)(2).

Author information:
(1)Department of Medicine, Louisiana State University Health Sciences Center at 
Shreveport, Shreveport, LA, USA.
(2)Center for Brain Health, Louisiana State University Health Sciences Center at 
Shreveport, Shreveport, LA, USA.
(3)Department of Artificial Intelligence Engineering, Trabzon, 61335, Turkey.
(4)Department of Pathology and Translational Pathobiology, Louisiana State 
University Health Sciences Center at Shreveport, Shreveport, LA, USA.
(5)Department of Pharmacology, Toxicology & Neuroscience, Louisiana State 
University Health Sciences Center at Shreveport, Shreveport, LA, USA.
(6)Department of Neurology, Louisiana State University Health Sciences Center at 
Shreveport, Shreveport, LA, USA.
(7)Department of Psychiatry, Louisiana State University Health Sciences Center 
at Shreveport, Shreveport, LA, USA.
(8)Department of Pediatrics, Louisiana State University Health Sciences Center 
at Shreveport, Shreveport, LA, USA.
(9)Department of Molecular and Cellular Physiology, Louisiana State University 
Health Sciences Center at Shreveport, Shreveport, LA, USA.

OBJECTIVE: Alzheimer's disease (AD) is a progressive neurodegenerative disorder 
that leads to cognitive decline and memory impairment, posing a public health 
concern in aging populations. Early and accurate detection of AD using 
non-invasive imaging biomarkers remains a critical clinical need for timely 
intervention and disease management. This study aims to develop an advanced 
artificial intelligence (AI)-based diagnostic framework, ViTranZheimer, that 
leverages video vision transformers to analyze magnetic resonance imaging (MRI) 
and improve the accuracy of AD classification.
METHODS: This study presents 'ViTranZheimer,' an AD diagnosis approach that 
leverages video transformers to analyze MRI volumes. Our proposed deep learning 
framework aims to improve the accuracy and sensitivity of AD diagnosis, 
equipping clinicians with a tool for early detection and intervention. We 
exploit the temporal dependencies between slices by treating the MRI volumes as 
videos to capture intricate structural relationships. We evaluated ViTranZheimer 
on the publicly available Alzheimer's Disease Neuroimaging Initiative (ADNI): 
Complete 3Yr 3T data collection, which includes 351 T1-weighted MRI scans 
categorized into normal controls (NC = 129), mild cognitive impairment (MCI = 
145), and AD = 77 groups. Each MRI volume was preprocessed using spatial 
normalization and skull stripping, then modeled as a video sequence for input to 
a Video Vision Transformer (ViViT). The model was trained from scratch using 
10-fold stratified cross-validation and optimized with the Adam optimizer over 
500 epochs. Classification performance was evaluated using accuracy, precision, 
recall, F1-score, and area under the ROC curve (AUC). Statistical comparison was 
conducted using the Wilcoxon signed-rank test against two baseline models: a 
convolutional neural network with bidirectional long short-term memory 
(CNN-BiLSTM), and a vision transformer with bidirectional long short-term memory 
(ViT-BiLSTM).
RESULTS: The proposed ViTranZheimer model achieved 98.6% accuracy in classifying 
NC, MCI, and AD cases, outperforming CNN-BiLSTM (96.5%) and ViT-BiLSTM (97.5%). 
It also attained superior precision, recall, F1-score (all 0.97), and an AUC of 
0.99. Performance differences were statistically significant based on the 
Wilcoxon signed-rank test (P < 0.05).
CONCLUSION: ViTranZheimer demonstrates strong potential for accurate and early 
Alzheimer's disease diagnosis using non-invasive MRI data. By leveraging video 
vision transformers, the model provides a promising tool for clinical decision 
support in neurodegenerative disease detection.

DOI: 10.1016/j.imed.2025.07.001
PMCID: PMC12536496
PMID: 41122755

Conflict of interest statement: Conflicts of interest statement The authors 
declare no conflicts of interest.


137. JAMA Netw Open. 2025 Oct 1;8(10):e2538427. doi: 
10.1001/jamanetworkopen.2025.38427.

Large Language Model Assistant for Emergency Department Discharge Documentation.

Song JW(1), Park J(2), Kim JH(2)(3), You SC(3)(4).

Author information:
(1)Yonsei University College of Medicine, Seoul, South Korea.
(2)Department of Emergency Medicine, Yonsei University College of Medicine, 
Seoul, South Korea.
(3)Institute for Innovation in Digital Health, Yonsei University, Seoul, South 
Korea.
(4)Department of Biomedical Systems Informatics, Yonsei University College of 
Medicine, Seoul, South Korea.

Comment in
    doi: 10.1001/jamanetworkopen.2025.38437.

IMPORTANCE: Emergency department (ED) discharge documentation is time-consuming 
and often incomplete.
OBJECTIVE: To develop a large language model (LLM) assistant that generates ED 
discharge notes and to evaluate its effectiveness on documentation quality and 
workflow efficiency.
DESIGN, SETTING, AND PARTICIPANTS: This comparative effectiveness study, which 
was conducted at a 2400-bed tertiary care hospital in South Korea, consisted of 
2 primary phases: a development phase and sequential validation of the LLM 
assistant. In the randomized sequential prospective validation, 6 emergency 
physicians first wrote discharge notes manually (session 1), then edited 
LLM-generated drafts after a 1-hour washout period (session 2). Three 
independent physicians evaluated 300 note sets (each containing a manual note, 
an LLM draft, and an LLM-assisted note). For model development and validation, 
patient records from ED visits between September 1, 2022, and August 31, 2023, 
were used. The inclusion criteria encompassed adult patients (aged ≥17 years) 
and pediatric patients with nondisease conditions (eg, trauma, poisoning, or 
burns). Emergency physicians selected 592 representative cases for training and 
50 for validation.
EXPOSURE: A commercially available text generation transformer model was used as 
a core LLM, fine-tuned using the 592 training cases. Two distinct processing 
pipelines were implemented within the LLM assistant due to different input data: 
(1) for patients managed solely by emergency physicians, using the ED initial 
record and prescription list, and (2) for those requiring specialty 
consultations, using the ED initial record and consultation request form.
MAIN OUTCOMES AND MEASURES: Quality of notes using 4C metrics (completeness, 
correctness, conciseness, and clinical utility) on a Likert scale ranging from 1 
to 5 and time taken to complete the notes manually and with the LLM assistant.
RESULTS: Of the 50 test cases, the mean (SD) patient age was 57.7 (23.1) years, 
and 28 patients (56%) were female. LLM-assisted notes achieved higher scores 
than manual notes in completeness (4.23 [95% CI, 4.17-4.28] vs 4.03 [95% CI, 
3.96-4.09]), correctness (4.38 [95% CI, 4.33-4.42] vs 4.20 [95% CI, 4.14-4.26]), 
conciseness (4.23 [95% CI, 4.18-4.28] vs 4.11 [95% CI, 4.05-4.17]), and clinical 
utility (4.17 [95% CI, 4.11-4.23] vs 3.85 [95% CI, 3.78-3.91]) (all P < .001). 
When compared with LLM drafts, LLM-assisted notes excelled in conciseness (4.23 
vs 3.98 [95% CI, 3.91-4.04]; P < .001) and maintained equivalent clinical 
utility (4.17 vs 4.16 [95% CI, 4.11-4.21]; P > .99), but scored lower in 
completeness (4.23 vs 4.34 [95% CI, 4.29-4.39]; P = .001) and correctness (4.38 
vs 4.45 [95% CI, 4.41-4.49]; P < .001). The median documentation time per note 
dropped from 69.5 (95% CI, 65.5-78.0) seconds for manual notes to 32.0 (95% CI, 
29.5-36.0) seconds for LLM-assisted notes (P < .001).
CONCLUSION: In this comparative effectiveness study, use of an on-site LLM 
assistant was associated with reduced writing time for ED discharge notes 
compared with manual note-taking, without compromising documentation quality, 
representing a critical advancement in the use of artificial intelligence for 
clinical practice.

DOI: 10.1001/jamanetworkopen.2025.38427
PMCID: PMC12541540
PMID: 41118162 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest Disclosures: Dr You 
reported receiving personal fees from PHI Digital Healthcare during the conduct 
of the study and grant support from Daiichi Sankyo outside the submitted work, 
and having patents 2025-0039190, a 2025-0039191, 2025-0039192, 2025-0039193, and 
2025-0039194 pending. No other disclosures were reported.


138. Radiol Med. 2025 Oct 21. doi: 10.1007/s11547-025-02105-9. Online ahead of print.

Dual-task deep learning model for prediction of medulloblastoma molecular 
subgroups with preoperative brain MRI.

Luo L(#)(1), An X(#)(2)(3), Zhang J(4), Zhou W(5), Zhao X(6), Zhao H(4)(7), Tian 
Y(8), Chen T(9), Zhao F(10).

Author information:
(1)Department of Computer Science and Technology and Institute of Artificial 
Intelligence and BNRist, Tsinghua University, Beijing, China.
(2)Department of Neurosurgery, Beijing Tiantan Hospital, Capital Medical 
University, Beijing, China.
(3)Department of Critical Care Medicine, Beijing Shijitan Hospital, Capital 
Medical University, Beijing, China.
(4)Department of Neural Reconstruction, Beijing Neurosurgical Institute, Capital 
Medical University, Beijing, China.
(5)Department of Cell Biology, Beijing Neurosurgical Institute, Capital Medical 
University, Beijing, China.
(6)Department of Nuclear Medicine, Beijing Tiantan Hospital, Capital Medical 
University, Beijing, China.
(7)Department of Pediatric Neurosurgery, Beijing Neurosurgical Institute, 
Capital Medical University, 119 South 4Th Ring West Road, Fengtai District, 
Beijing, 100070, China.
(8)Department of Neurosurgery, Beijing Tiantan Hospital, Capital Medical 
University, Beijing, China. tianyongji@bjtth.org.
(9)Department of Computer Science and Technology and Institute of Artificial 
Intelligence and BNRist, Tsinghua University, Beijing, China. 
tingchen@tsinghua.edu.cn.
(10)Department of Pediatric Neurosurgery, Beijing Neurosurgical Institute, 
Capital Medical University, 119 South 4Th Ring West Road, Fengtai District, 
Beijing, 100070, China. zhaofu@ccmu.edu.cn.
(#)Contributed equally

PURPOSE: To develop a deep learning model for predicting molecular subgroups of 
medulloblastoma (MB) using preoperative brain MRI.
MATERIALS AND METHODS: This study included a cohort of 350 patients with MB for 
model development. Preoperative multiparametric brain MRIs were acquired, and 
molecular classification data for tumor samples were analyzed. A dual-task deep 
learning model, composed of a 3D Swin Transformer backbone and a 
Transformer-based mask decoder, was developed for the prediction of MB molecular 
subgroups. The model was jointly optimized with a parallel task of tumor and 
cerebellum segmentation. Ablation analysis was conducted to verify the 
effectiveness of the dual-task model design. An independent test cohort of 126 
patients with MB was established to validate the predictive performance of the 
dual-task model.
RESULTS: Our dual-task deep learning model demonstrated superior performance for 
MB molecular subgroup prediction, achieving an AUC of 0.877, accuracy of 88.9%, 
sensitivity of 71.6%, and specificity of 91.9%. The performance remained robust 
across both adult and pediatric age populations, with AUCs of 0.915 and 0.871, 
respectively. Furthermore, our approach exhibited effective generalization to 
the independent test cohort, yielding an AUC of 0.853, accuracy of 89.7%, 
sensitivity of 73.5%, and specificity of 92.1%. Ablation analysis demonstrated a 
significant improvement in AUC of 0.169 (95% CI 0.097-0.244) when using the 
dual-task model design. In comparison with the radiomics-based model, our deep 
learning model achieved a higher AUC by 0.156 (95% CI 0.079-0.233).
CONCLUSION: Our proposed dual-task deep learning model enables automated and 
accurate prediction of MB molecular subgroups.

© 2025. Italian Society of Medical Radiology.

DOI: 10.1007/s11547-025-02105-9
PMID: 41117885

Conflict of interest statement: Declarations. Competing interests: The authors 
have no relevant financial or non-financial interests to disclose. Ethical 
approval: This study was performed in line with the principles of the 
Declaration of Helsinki. Approval was granted by the Ethics Committee of Beijing 
Tiantan Hospital, Capital Medical University (Ethics No. Approval Number: 
KY2018-020–01; KY2020-090–02). Consent to participate: This is an observational 
retrospective study with no intervention in patient treatment, and all data have 
been fully de-identified without revealing patient identification details. The 
Ethical Review Committee of Beijing Tiantan Hospital, Capital Medical 
University, waived the requirement for informed patient consent to participate. 
Consent to publication: Only the brain MRI scans and clinical information of 
patients are collected and analyzed without revealing the identification 
details, so consent to publish is not required.


139. J Med Internet Res. 2025 Oct 20;27:e78632. doi: 10.2196/78632.

Investigating Social Media Use by Young People to Self-Manage Type 1 Diabetes 
Mellitus: Large-Scale Analysis of Social Media Discussions Using Topic Modeling.

Ma Y(1), Hassan L(2), van der Veer SN(2), Nenadic G(1).

Author information:
(1)School of Computer Science, University of Manchester, Manchester, United 
Kingdom.
(2)Division of Informatics, Imaging and Data Sciences, University of Manchester, 
Manchester, United Kingdom.

BACKGROUND: Social media has shown promise in supporting young people with type 
1 diabetes mellitus (T1DM) by providing information and emotional support. 
Although previous qualitative studies have investigated young people's 
self-reported use of social media for self-management, their patterns of actual 
use remain underexplored. Furthermore, different platforms may serve different 
functions or attract different types of engagement, making it important to 
examine how the patterns of actual use vary across them.
OBJECTIVE: This study aimed to identify and describe the topics that young 
people with T1DM discuss online and to identify differences in content across 
platforms.
METHODS: We collected data from Twitter and two forums (Reddit and 
Diabetes.co.uk) spanning from January 2020 to January 2024, identifying young 
people with T1DM using rule-based criteria and profile age information. An 
efficient analysis pipeline, integrating topic modeling with large language 
model (LLM) summarizing and human verification, was applied to identify the 
discussion topics.
RESULTS: We analyzed 1765 tweets and 1259 forum posts by young people with T1DM 
and identified 24 topics. Among these topics, 7 (29.2%) were common across all 
platforms (Twitter and forums topic sizes, respectively): blood glucose 
management (n=70, 9%, and n=88, 16.6%), community and friendship (n=33, 4.3%, 
and n=51, 9.6%), COVID-19 (n=76, 9.8%, and n=13, 2.4%), diabetes and diet (n=81, 
10.5%, and n=38, 7.2%), diabetes devices (n=97, 12.5%, and n=128, 24.1%), 
emotional and psychological expression (n=100, 12.9%, and n=124, 23.4%), and 
financial challenges (n=141,18.2%, and n=23, 4.3%). In addition, 7 (29.2%) 
topics were unique to Twitter: advocacy and awareness (n=35, 4.5%), diaversaries 
reflection (n=24, 3.1%), daily life and adaptation (n=27, 3.5%), 
misunderstanding of T1DM (n=25, 3.2%), educating and raising awareness (n=19, 
2.5%), insulin prescription frustration (n=21, 2.7%), and experience with health 
care providers (n=25, 3.2%). Furthermore, 3 (12.5%) topics were unique to 
forums: diabetes complications (n=21, 4%), newly diagnosed and "honeymoon" 
experiences (n=21, 4%), and traveling with T1DM (n=24, 4.5%).
CONCLUSIONS: Although our results confirmed social media's role in providing 
information and emotional and peer support, we also found that topics such as 
COVID-19, advocacy, and diabetes celebration are more frequently discussed in 
social media than in prior qualitative studies based on self-reporting. 
Platform-specific patterns are evident, with Twitter discussions being more 
immediate and experience driven, focusing on daily reflection, advocacy, 
awareness campaigns, and financial support, while forum-based platforms 
emphasize more in-depth discussions, where users seek and provide comprehensive 
advice, troubleshooting, and sustained peer support. Practitioners could 
consider the differences when designing digital interventions to ensure the 
delivery method aligns with the communication style and support needs of the 
target audience.

©Yanan Ma, Lamiece Hassan, Sabine N van der Veer, Goran Nenadic. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org), 
20.10.2025.

DOI: 10.2196/78632
PMCID: PMC12583938
PMID: 41115270 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


140. Sch Psychol. 2025 Oct 20. doi: 10.1037/spq0000715. Online ahead of print.

Human versus machine: A comparative analysis of qualitative coding by humans and 
ChatGPT-4.

Lockwood A(1), Newman DS(2), Mossing KW(2), Glubzinski A(2), Cohen E(2).

Author information:
(1)Department of School Psychology, School of Lifespan Development and 
Educational Sciences, Kent State University.
(2)Department of School Psychology, School of Human Services, University of 
Cincinnati.

Generative artificial intelligence (AI) applications are becoming increasingly 
influential in psychology training, practice, and research. In this study, the 
procedures (e.g., coding process) and products (e.g., codes, categories, themes, 
core story) of a qualitative content analysis (QCA) conducted by Chat Generative 
Pre-trained Transformer (ChatGPT)-4 and novice human researchers were compared, 
and advantages and disadvantages of each approach were considered. Data included 
open-ended survey responses from trainers (N = 60) in school psychology programs 
regarding assessment practices during the COVID-19 pandemic. Findings indicated 
that ChatGPT-4 conducted QCA with products that were similar, overall, to human 
coders and in significantly less time. However, ChatGPT-4's process was not 
transparent, and some codes and themes were unclear. Meanwhile, human coding 
allowed for the selection and implementation of a purposeful, coherent 
methodological approach and an auditable and systematic process resulting in 
defensible themes. Considerations for the use of AI in qualitative research are 
considered and discussed, and future research directions are provided. (PsycInfo 
Database Record (c) 2025 APA, all rights reserved).

DOI: 10.1037/spq0000715
PMID: 41114954


141. Front Physiol. 2025 Oct 2;16:1630807. doi: 10.3389/fphys.2025.1630807. 
eCollection 2025.

Risk prediction for gastrointestinal bleeding in pediatric Henoch-Schönlein 
purpura using an interpretable transformer model.

Chen G(#)(1), Yang Z(#)(1).

Author information:
(1)The Department of Pediatrics at the Affiliated Hospital of North Sichuan 
Medical College, NanChong, Sichuan, China.
(#)Contributed equally

OBJECTIVE: Henoch-Schönlein purpura (HSP), clinically recognized as IgA 
vasculitis (IgAV), a prevalent systemic vasculitis in pediatric populations, 
frequently involves gastrointestinal (GI) tract manifestations that may lead to 
serious complications including hemorrhage and tissue necrosis. Timely 
identification of GI bleeding risk enables prompt clinical intervention and 
improves therapeutic outcomes. This study aims to develop and clinically 
validate an interpretable Transformer-based predictive model for assessing GI 
bleeding risk in pediatric patients with IgAV.
METHODS: This retrospective cohort study analyzed 758 pediatric IgAV cases (ages 
0-14 years) admitted to the Department of Pediatrics at the Affiliated Hospital 
of North Sichuan Medical College between 1 May 2020, and 31 January 2024. 
Comprehensive clinical data including symptoms and laboratory parameters were 
systematically collected. GI complications were stratified into three severity 
tiers: 1) no complications, 2) abdominal pain without bleeding), and 3) 
documented rectal bleeding or hemorrhage, based on standardized diagnostic 
criteria. Five machine learning algorithms (Random Forest, XGBoost, LightGBM, 
CatBoost, and TabPFN-V2) were optimized through nested cross-validation. Model 
performance was evaluated using multiple metrics: accuracy, precision, recall, 
F1-score, the Kappa coefficient, and ROC-AUC. The optimal model was subsequently 
interpreted using Shapley Additive Explanations (SHAP) values to elucidate 
feature importance.
RESULTS: Among the evaluated models, the Transformer-based TabPFN-V2 
demonstrated superior predictive performance, achieving a validation accuracy of 
0.88, precision of 0.88, recall of 0.87, F1-score of 0.88, Kappa coefficient of 
0.82, and AUC-ROC of 0.98. SHAP analysis revealed the five most influential 
biomarkers for global interpretability: D-dimer, total cholesterol, platelet 
count, apolipoprotein, and C-reactive protein.
CONCLUSION: The interpretable Transformer-based TabPFN-V2 model demonstrated 
robust predictive performance for GI bleeding risk in pediatric IgAV patients. 
Clinically accessible laboratory parameters identified by this model not only 
offer practical guidance for clinical decision-making but also establish a 
foundation for advancing medical artificial intelligence integration in 
pediatric care.

Copyright © 2025 Chen and Yang.

DOI: 10.3389/fphys.2025.1630807
PMCID: PMC12528089
PMID: 41113623

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


142. Front Psychiatry. 2025 Oct 2;16:1675098. doi: 10.3389/fpsyt.2025.1675098. 
eCollection 2025.

The use of the social robot NAO in medical settings: how to facilitate 
interactions between healthcare professionals and patients with autism spectrum 
disorder.

Biagi F(1), Iani C(2), Biagiotti L(1).

Author information:
(1)Department of Engineering "Enzo Ferrari", University of Modena and Reggio 
Emilia, Modena, Italy.
(2)Department of Surgery, Medicine, Dentistry and Morphological Sciences, 
University of Modena and Reggio Emilia, Reggio Emilia, Italy.

OBJECTIVES: This study investigates how to facilitate the use of the social 
robot NAO in medical settings to support interactions with children diagnosed 
with Autism Spectrum Disorder (ASD). The objective was to develop intuitive 
control methods that enable healthcare professionals to easily integrate the 
robot into clinical practice.
METHODS: Two control modes were designed: Puppet mode, where clinicians manually 
operate the robot via a graphical console, and Assistant mode, where a Large 
Language Model translates clinicians' spoken requests into robot actions and 
dialogue. Twenty-three doctors evaluated both modes through video demonstrations 
and completed questionnaires assessing usability, usefulness, and ethical 
acceptability.
RESULTS: Both modes were considered effective and user-friendly. Assistant mode 
was perceived as more intuitive and adaptable, facilitating seamless 
interaction, whereas Puppet mode was judged slightly more reassuring for 
patients and somewhat more appropriate in terms of robot actions.
CONCLUSION: Overall, both approaches were positively received, with Assistant 
mode emerging as the preferred option for integration into clinical workflows 
due to its perceived simplicity and flexibility. These findings highlight 
clinicians' positive perceptions of two novel control modes and emphasize NAO's 
potential to enhance patient engagement and reduce stress. Further empirical 
validation with children in real clinical trials is warranted to confirm these 
benefits and optimize robot-assisted interventions in ASD care.

Copyright © 2025 Biagi, Iani and Biagiotti.

DOI: 10.3389/fpsyt.2025.1675098
PMCID: PMC12528083
PMID: 41113200

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest. The reviewer EL declared a 
shared parent affiliation with the authors to the handling editor at the time of 
review.


143. Ann Oncol. 2025 Dec;36(12):1447-1457. doi: 10.1016/j.annonc.2025.09.001. Epub 
2025 Oct 18.

ESMO guidance on the use of Large Language Models in Clinical Practice (ELCAP).

Wong EYT(1), Verlingue L(2), Aldea M(3), Franzoi MA(4), Umeton R(5), Halabi 
S(6), Harbeck N(7), Indini A(8), Prelaj A(9), Romano E(10), Smyth E(11), Tan 
IB(1), Valachis A(12), Vibert J(13), Wiest IC(14), Yang YH(15), Gilbert S(16), 
Kapetanakis G(17), Pentheroudakis G(18), Koopman M(19), Kather JN(20).

Author information:
(1)Division of Medical Oncology, National Cancer Centre Singapore, Singapore, 
Singapore.
(2)Centre Léon Bérard, Lyon, France.
(3)Department of Medical Oncology, Thoracic Unit, Gustave Roussy, Villejuif, 
France; Paris-Saclay University, Paris, France; Lowe Center for Thoracic 
Oncology, Dana-Farber Cancer Institute, Boston, USA.
(4)Cancer Survivorship Group, Inserm Unit 981, Gustave Roussy, Villejuif, 
France; IHU PRISM National PRecISion Medicine Center in Oncology, Gustave 
Roussy, Villejuif, France.
(5)Office of Data Science, St. Jude Children's Research Hospital, Memphis, USA; 
Department of Biological Engineering, Massachusetts Institute of Technology, 
Cambridge, USA; Department of Mechanical Engineering, Massachusetts Institute of 
Technology, Cambridge, USA; Department of Biostatistics, Harvard T.H. Chan 
School of Public Health, Boston, USA; Department of Pathology and Laboratory 
Medicine, Weill Cornell Medicine, New York, USA.
(6)Department of Biostatistics and Bioinformatics, Duke University Medical 
Center, Durham, USA.
(7)Breast Center, Department of OB&GYN and CCC Munich, LMU University Hospital, 
Munich, Germany.
(8)Department of Medical Oncology and Hematology, Fondazione IRCCS-Istituto 
Nazionale dei Tumori, Milan, Italy.
(9)AI-ON-Lab, Department of Medical Oncology and Hematology, Fondazione 
IRCCS-Istituto Nazionale dei Tumori, Milan, Italy.
(10)Department of Medical Oncology, Inserm U932, Institut Curie, Paris, France.
(11)Oxford NIHR Biomedical Research Centre, Oxford, UK.
(12)Department of Oncology, Faculty of Medicine and Health, Örebro University, 
Örebro, Sweden.
(13)Paris-Saclay University, Paris, France; Drug Development Department, Gustave 
Roussy, Villejuif, France.
(14)Else Kroener Fresenius Center for Digital Health, Faculty of Medicine and 
University Hospital Carl Gustav Carus, TUD Dresden University of Technology, 
Dresden, Germany; Department of Medicine II, Medical Faculty Mannheim, 
Heidelberg University, Mannheim, Germany.
(15)National Institute of Cancer Research, National Health Research Institutes, 
Tainan, Taiwan.
(16)Else Kroener Fresenius Center for Digital Health, Faculty of Medicine and 
University Hospital Carl Gustav Carus, TUD Dresden University of Technology, 
Dresden, Germany.
(17)ELLOK-Hellenic Cancer Federation, Athens, Greece.
(18)ESMO-European Society for Medical Oncology, Lugano, Switzerland.
(19)Department of Medical Oncology, UMC-University Medical Center Utrecht, 
Utrecht University, Utrecht, The Netherlands.
(20)Else Kroener Fresenius Center for Digital Health, Faculty of Medicine and 
University Hospital Carl Gustav Carus, TUD Dresden University of Technology, 
Dresden, Germany; Department of Medicine I, University Hospital Dresden, 
Dresden; Medical Oncology, National Center for Tumor Diseases (NCT), University 
Hospital Heidelberg, Heidelberg, Germany. Electronic address: 
kather.jn@tu-dresden.de.

BACKGROUND: Large language models (LLMs) are rapidly being integrated into 
health care, with substantial implications for oncology practice. The European 
Society for Medical Oncology (ESMO) developed the ESMO guidance on the use of 
Large Language Models in Clinical Practice (ELCAP) to provide a structured 
framework and basic guidance for their safe and effective application in 
oncology.
PATIENTS AND METHODS: Between November 2024 and February 2025, a 
multidisciplinary group of 20 experts convened under the ESMO Real World Data 
and Digital Health Task Force. Using literature review and a Delphi consensus 
process, the panel defined three categories of LLM use in oncology: type 1 
(patient-facing applications), type 2 [health care professional (HCP)-facing 
applications], and type 3 (background institutional systems). Consensus 
statements were developed for each type to provide basic practical guidance.
RESULTS: ELCAP highlights opportunities such as improved patient education and 
symptom management, streamlined clinical workflows, and enhanced data 
processing. At the same time, it addresses challenges including data privacy, 
algorithmic bias, regulatory compliance, and the risk of unsupervised use. The 
framework emphasises human oversight, protection of patient privacy, and 
alignment with clinical and ethical standards. Patient-facing tools should 
complement, not replace, professional advice and should be embedded in 
supervised care pathways. HCP-facing and background systems may improve 
efficiency and decision support but require systematic validation, transparency, 
and continuous monitoring.
CONCLUSIONS: ELCAP provides a three-tier framework and basic practical guidance 
for LLM use in oncology. ESMO supports efforts to use this framework to improve 
patient care, but warns against unsupervised or unvalidated use.

Copyright © 2025 The Author(s). Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.annonc.2025.09.001
PMID: 41111032 [Indexed for MEDLINE]


144. Epilepsy Behav. 2025 Dec;173:110785. doi: 10.1016/j.yebeh.2025.110785. Epub 2025 
Oct 18.

Development and validation of a video-based deep learning model for 
distinguishing epileptic seizures from non-epileptic events in a pediatric 
cohort.

Ding P(1), Liu T(1), Xu J(1), Yuan L(1), Zhang L(1), Wei Z(1), Tai Y(1), Liang 
S(2).

Author information:
(1)Department of Functional Neurosurgery, Beijing Children's Hospital, Capital 
Medical University, National Center For Children's Health, Beijing, China.
(2)Department of Functional Neurosurgery, Beijing Children's Hospital, Capital 
Medical University, National Center For Children's Health, Beijing, China. 
Electronic address: 301_1sjwk@sina.cn.

OBJECTIVE: This study aimed to develop and validate a video-based deep learning 
system for distinguishing epileptic seizures (ES) from non-epileptic events 
(NEE) in a pediatric cohort. Using a prospective validation cohort, we further 
assessed the diagnostic performance and clinical applicability of the artificial 
intelligence (AI) model, investigated potential factors contributing to its 
diagnostic errors, and benchmarked its clinical utility against clinicians 
grouped by different levels of expertise.
METHODS: An enhanced multiscale vision transformer was trained on 438 
retrospectively collected videos, with benchmark comparisons against MViTv2 and 
SlowFast architectures. Prospective validation was performed using 130 
consecutive videos to assess the diagnostic performance of the AI system against 
tiered clinician groups (interns, attending physicians, and chief physicians). A 
generalized linear mixed model (GLMM) was employed to identify factors 
associated with AI misdiagnosis, with further comparative analysis of diagnostic 
performance between AI and human clinicians.
RESULTS: Our model demonstrated significantly higher accuracy (p = 0.001) and 
sensitivity (p = 0.004) compared to the MViTv2 model. Although all performance 
metrics were numerically higher than those of the SlowFast model, these 
differences did not reach statistical significance. GLMM analysis indicated that 
event type (motor vs. non-motor) was a significant factor influencing model 
misclassification (p = 0.020). The model achieved substantially higher 
diagnostic accuracy for motor events compared to non-motor events (p < 0.001).
CONCLUSION: The video-based AI classifier shows promise as an assistive tool for 
clinicians in differentiating ES from NEE based on video evidence in a pediatric 
cohort. Our AI model demonstrated notably effective diagnostic performance for 
motor events, while its accuracy was more limited for non-motor events.

Copyright © 2025. Published by Elsevier Inc.

DOI: 10.1016/j.yebeh.2025.110785
PMID: 41110273 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


145. Sci Rep. 2025 Oct 15;15(1):36082. doi: 10.1038/s41598-025-20066-x.

Performance evaluation of large language models on Korean medical licensing 
examination: a three-year comparative analysis.

Kim HJ(#)(1)(2), Jung K(#)(3)(4), Shin S(5), Lee W(6), Lee JH(7), Park HS(8)(9), 
Choi Q(10)(11).

Author information:
(1)Department of Laboratory Medicine, Chungnam National University School of 
Medicine, Daejeon, Korea.
(2)Department of Laboratory Medicine, Chungnam National University Sejong 
Hospital, Sejong, Korea.
(3)Department of Laboratory Medicine, Chungbuk National University Hospital, 
Cheongju, Korea.
(4)Department of Laboratory Medicine, Chungbuk National University College of 
Medcine, Cheongju, Korea.
(5)Department of Laboratory Medicine, Inje University Ilsan Paik Hospital, 
Goyang, Korea.
(6)Department of Laboratory Medicine, Pyeongtaek Saint Mary Hospital, 
Pyeongtaek, Korea.
(7)Department of Laboratory Medicine, GC Labs, Yongin, Korea.
(8)Department of Laboratory Medicine, Chungbuk National University Hospital, 
Cheongju, Korea. hsmed22@chungbuk.ac.kr.
(9)Department of Laboratory Medicine, Chungbuk National University College of 
Medcine, Cheongju, Korea. hsmed22@chungbuk.ac.kr.
(10)Department of Laboratory Medicine, Chungnam National University School of 
Medicine, Daejeon, Korea. qutechoi@gmail.com.
(11)Department of Laboratory Medicine, Chungnam National University Sejong 
Hospital, Sejong, Korea. qutechoi@gmail.com.
(#)Contributed equally

Performance evaluation of large language models (LLMs) in non-English medical 
contexts remains limited, particularly for medical licensing examinations 
including both text- and image-based questions. Therefore, the performance and 
reliability of three LLMs-GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro was 
evaluated using the Korean Medical Licensing Examination (KMLE) questions from 
2022 to 2024. We analyzed 942 KMLE questions encompassing text-only and 
image-based formats across various medical specialties. Reproducibility was 
evaluated through repeated testing, and inter-model agreement was analyzed using 
pairwise comparisons. GPT-4o achieved the highest accuracy (83.2%), followed by 
Claude 3.5 Sonnet (79.5%) and Gemini 1.5 Pro (76.6%). While GPT-4o and Claude 
3.5 Sonnet outperformed text-only questions, Gemini 1.5 Pro consistently 
performed across both question formats. LLMs demonstrated the strongest 
performance in internal medicine, pediatrics, and psychiatry, with relatively 
weak results in medical law. Reproducibility was outstanding, with Claude 3.5 
Sonnet, Gemini 1.5 Pro, and GPT-4o showing 99.9%, 99.5%, and 97.7%, 
respectively. Strong inter-model agreement was observed, particularly between 
GPT-4o and Claude 3.5 Sonnet. LLMs demonstrate competent performance in medical 
knowledge assessments, even in non-English contexts, although challenges persist 
in processing image-based questions and specialized domains. This study provides 
valuable insights that may inform the future development and application of LLMs 
in medical education and assessment, although further validation in real-world 
educational settings is necessary to establish their practical utility.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-20066-x
PMCID: PMC12528361
PMID: 41094133 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was conducted in accordance with the Declaration of 
Helsinki. This study was approved by Institutional Review Board (IRB) of 
Chungnam National University Sejong Hospital (approval no. NON2024-009) as 
exempt from full review, as it employed pre-existing, de-identified educational 
data devoid of personal identifiers or patient information. As this study used 
only de-identified educational data and did not involve human subjects directly, 
the requirement for informed consent was waived by the IRB. Competing interests: 
The authors declare no competing interests.


146. JMIRx Med. 2025 Oct 15;6:e56090. doi: 10.2196/56090.

Development of a Conversational Artificial Intelligence-Based Web Application 
for Medical Consultations: Prototype Study.

Pires JG(1).

Author information:
(1)IdeaCoding Lab, Rua Timbopeba, 24, Ouro Preto, 35411000, Brazil, 55 
31988624279.

Update of
    doi: 10.1101/2023.12.31.23300681.

BACKGROUND: Artificial intelligence (AI) has evolved through various trends, 
with different subfields gaining prominence over time. Currently, conversational 
AI-particularly generative AI-is at the forefront. Conversational AI models are 
primarily focused on text-based tasks and are commonly deployed as chatbots. 
Recent advancements by OpenAI have enabled the integration of external, 
independently developed models, allowing chatbots to perform specialized, 
task-oriented functions beyond general language processing.
OBJECTIVE: This study aims to develop a smart chatbot that integrates large 
language models from OpenAI with specialized domain-specific models, such as 
those used in medical image diagnostics. The system leverages transfer learning 
via Google's Teachable Machine to construct image-based classifiers and 
incorporates a diabetes detection model developed in TensorFlow.js. A key 
innovation is the chatbot's ability to extract relevant parameters from user 
input, trigger the appropriate diagnostic model, interpret the output, and 
deliver responses in natural language. The overarching goal is to demonstrate 
the potential of combining large language models with external models to build 
multimodal, task-oriented conversational agents.
METHODS: Two image-based models were developed and integrated into the chatbot 
system. The first analyzes chest X-rays to detect viral and bacterial pneumonia. 
The second uses optical coherence tomography images to identify ocular 
conditions such as drusen, choroidal neovascularization, and diabetic macular 
edema. Both models were incorporated into the chatbot to enable image-based 
medical query handling. In addition, a text-based model was constructed to 
process physiological measurements for diabetes prediction using TensorFlow.js. 
The architecture is modular; new diagnostic models can be added without 
redesigning the chatbot, enabling straightforward functional expansion.
RESULTS: The findings demonstrate effective integration between the chatbot and 
the diagnostic models, with only minor deviations from expected behavior. 
Additionally, a stub function was implemented within the chatbot to schedule 
medical appointments based on the severity of a patient's condition, and it was 
specifically tested with the optical coherence tomography and X-ray models.
CONCLUSIONS: This study demonstrates the feasibility of developing advanced AI 
systems-including image-based diagnostic models and chatbot integration-by 
leveraging AI as a service. It also underscores the potential of AI to enhance 
user experiences in bioinformatics, paving the way for more intuitive and 
accessible interfaces in the field. Looking ahead, the modular nature of the 
chatbot allows for the integration of additional diagnostic models as the system 
evolves.

© Jorge Guerra Pires. Originally published in JMIRx Med (https://med.jmirx.org).

DOI: 10.2196/56090
PMCID: PMC12527318
PMID: 41092409

Conflict of interest statement: Conflicts of Interest: None declared.


147. Front Physiol. 2025 Sep 29;16:1638788. doi: 10.3389/fphys.2025.1638788. 
eCollection 2025.

AI-augmented prenatal care: a dual-modal fetal health assessment system 
integrating cardiotocography and uterine contraction synergy.

Qiu T(1)(2), Zhou X(2), Zhou J(1), Lin C(1), Jiang S(1), Cheng H(1), Wang X(2), 
You Q(2).

Author information:
(1)Department of Obstetrics, West China Longquan Hospital Sichuan University, 
The First People's Hospital of Longquanyi District Chengdu, Chengdu, Sichuan, 
China.
(2)Faculty of Science, Civil Aviation Flight University of China, Chengdu, 
Sichuan, China.

INTRODUCTION: Fetal heart monitoring (FHR) is a critical tool for assessing 
fetal health, but traditional methods rely on subjective physician 
interpretation, exhibiting significant variability that can lead to misdiagnosis 
and overtreatment. Artificial intelligence (AI) technology offers a novel 
approach to address this issue, yet existing research predominantly utilizes 
unimodal (FHR-only) data, failing to align with clinical guidelines emphasizing 
"bimodality analysis of fetal heart rate and uterine contractions (UC)." This 
study aims to develop a deep learning-based bimodal intelligent monitoring 
system to enhance the accuracy and clinical utility of fetal health assessment.
METHODS: The research team constructed the first fetal heart-contraction bimodal 
clinical dataset for Chinese pregnant women (n = 326). Based on the DenseNet121 
architecture, a selective attention mechanism (SK module) was introduced, 
proposing the DenseNet121-SK model. Standardized FHR and UC signals were 
extracted using image processing techniques. Dense connections and the SK module 
dynamically fused multi-scale features (e.g., transient fluctuations and 
contraction cycle associations). The model employed lightweight design during 
training to enhance physician usability.
RESULTS: (1) Dual-modality input significantly outperformed single-modality 
input, achieving a classification AUC of 0.944 (vs. 0.812 for single-modality), 
validating the clinical value of multi-parameter collaborative interpretation; 
(2) The SK module simulated obstetricians' multi-scale cognition, achieving 
95.88% accuracy with 100% recall for abnormal cases; (3) The system effectively 
reduced subjective interpretation variability, providing technical support for 
minimizing overtreatment.
DISCUSSION: This study achieves a balance between clinical interpretability and 
high performance through lightweight AI design (only 8.3 million parameters) and 
dual-modality data fusion, making it particularly suitable for 
resource-constrained primary care settings. Future work should further optimize 
generalization capabilities through multicenter validation and explore 
integration with large language models to generate standardized reports. These 
findings provide important references for optimizing perinatal healthcare 
resources and AI-assisted decision-making.

Copyright © 2025 Qiu, Zhou, Zhou, Lin, Jiang, Cheng, Wang and You.

DOI: 10.3389/fphys.2025.1638788
PMCID: PMC12515917
PMID: 41089565

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


148. Sci Rep. 2025 Oct 14;15(1):35853. doi: 10.1038/s41598-025-19804-y.

SpinachXAI-Rec: a multi-stage explainable AI framework for spinach freshness 
classification and consumer recommendation.

Raju ASN(1), Sujatha G(2), Gatla RK(3), Ankalaki S(4).

Author information:
(1)Department of Computing Technologies, School of Computing, College of 
Engineering & Technology, SRM Institute of Science and Technology, 
Kattankulathur, Chennai, Tamil Nadu, 603203, India. akellar@srmist.edu.in.
(2)Department of Networking and Communications, School of Computing, College of 
Engineering & Technology, SRM Institute of Science and Technology, 
Kattankulathur, Chennai, Tamil Nadu, 603203, India.
(3)Department of Computer Science and Engineering (Data Science), Institute of 
Aeronautical Engineering, Hyderabad, Telangana, 500043, India.
(4)Manipal Institute of Technology Bengaluru, Manipal Academy of Higher 
Education, Manipal, India. shilpa.ankalaki@manipal.edu.

Leafy vegetables such as spinach are among the most important components in a 
nutritious diet but are highly perishable and susceptible to premature spoilage. 
Traditional practices in determining freshness have been qualitative and 
time-consuming and have consistently led to defective consumption decisions with 
unintended consequences on human health. To this issue, we introduce 
SpinachXAI-Rec, a multistage framework that is enabled by AI and is capable of 
automating the classification of spinach freshness and providing consumer 
recommendations. This framework is based on understandable deep learning. To 
guarantee class balance and feature diversity, a dataset consisting of 4005 
original images of three spinach varieties (Malabar, Red, and Water) was 
expanded to 12,000 images (2000 per class across six categories: fresh and 
non-fresh). We trained three CNN architectures, DenseNet121, ResNet50, and 
EfficientNetB0, on the Stage 1 augmented dataset. In performance, we saw 
DenseNet121 significantly outperform with 96% classification accuracy compared 
to ResNet50 (53%) and EfficientNetB0 (17%). Stage 2 improved representation of 
features by incorporating DenseNet121 embeddings and ViT-B/16 and Swin 
Transformer attention mechanisms. DenseNet121 + ViT-B/16 obtained an F1-score of 
0.95, which was further optimised to 0.97 in Stage 3 using a multiclass SVM 
classifier. GradCAM++ and LIME were employed to incorporate interpretability 
during Stage 4. LIME provided transparent explanations of the significance of 
class-specific features, while GradCAM++ effectively highlighted 
disease-affected or spoilt regions. The most effective model 
(DenseNet121 + ViT + SVM) also obtained a Dice coefficient of 0.89 and an IoU of 
0.82, which confirms the precision of localisation and segmentation. Finally, 
Stage 5 introduces a clinical recommender system that is based on rules and 
relates prediction confidence to real-world categories: Eatable, Eatable with 
Caution, or Not Eatable. This AI-driven recommendation assists food purveyors 
and consumers in making health-conscious, well-informed decisions. 
SpinachXAI-Rec is a significant advancement in the development of safer food 
systems, as it provides interpretable AI for the purpose of freshness validation 
and actionable consumption recommendations, thereby empowering both consumers 
and industry stakeholders.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-19804-y
PMCID: PMC12521408
PMID: 41087543 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests. Ethics approval and consent to participate: The 
authors declare that they have no known competing financial interests or 
personal relationships that could have appeared to influence the work reported 
in this paper.


149. Eur J Pediatr Surg. 2025 Nov 3. doi: 10.1055/a-2722-3871. Online ahead of print.

The Pediatric Surgeon's AI Toolbox: How Large Language Models Like ChatGPT Are 
Simplifying Practice and Expanding Global Access.

Tinajero CAC(1).

Author information:
(1)Department of Pediatric Surgery, TecSalud, Monterrey, Nuevo Leon, Mexico.

Pediatric surgeons face substantial administrative workload. Large language 
models (LLMs) may streamline documentation, family communication, rapid 
reference, and education, but raise concerns about accuracy, bias, and privacy. 
This review summarizes practical, near-term uses with clinician 
oversight.Narrative review of LLMs in pediatric surgical workflows and scholarly 
writing. Sources included MEDLINE/PubMed, Scopus, Embase, Google Scholar, and 
policy documents (WHO, FDA, EU). Searches spanned January 2015 to August 2025, 
English only. Peer-reviewed and multicenter studies were prioritized; selected 
high-signal preprints were labeled. Data screening and extraction were performed 
by the author; findings were synthesized qualitatively.Across studies, LLMs 
reduced drafting time for discharge letters and operative note registries while 
maintaining clinician-rated quality; they improved readability of consent forms 
and postoperative instructions and supported patient education. For decision 
support, general models performed well on structured medical questions, with 
stronger results when grounded by retrieval. Common limits included coding 
performance, case-nuance/temporal reasoning, variable translation outside 
high-resource languages, and citation fabrication without curated sources. 
Privacy risks stemmed from logging, rare-string memorization, and poorly scoped 
tool connections. Recommended controls included a clinician-in-the-loop "review 
and release" workflow, privacy-preserving deployments, version pinning, and 
ongoing monitoring aligned with early-evaluation guidance.When outputs are 
grounded in structured EHR data or curated retrieval and briefly reviewed by 
clinicians, LLMs can responsibly reduce administrative burden and support 
communication and education. Early adoption should target high-volume, low-risk, 
auditable tasks. Future priorities must include multicenter pediatric datasets, 
transparent benchmarks (accuracy, calibration, equity, time saved), and 
prospective studies linked to safety outcomes.

Thieme. All rights reserved.

DOI: 10.1055/a-2722-3871
PMID: 41086879

Conflict of interest statement: The author declares that they have no conflict 
of interest.


150. Quant Imaging Med Surg. 2025 Oct 1;15(10):9871-9894. doi: 
10.21037/qims-2025-439. Epub 2025 Sep 22.

Multiscale Fourier transform multiple instance learning for the Gleason grading 
of prostate cancer from whole-slide images.

Xie Z(#)(1), Zeng J(#)(2), Tian H(#)(2)(3), Mai Z(#)(4)(5), Xiao Y(6), Zhou 
Y(4), Zhou Z(4), Jia X(2), Yan W(4), Qin C(2).

Author information:
(1)4+4 Medical Doctor Program, Chinese Academy of Medical Sciences & Peking 
Union Medical College, Beijing, China.
(2)School of Electric and Information Engineer, Wuyi University, Jiangmen, 
China.
(3)Automotive Technology College, Zhejiang Vocational College of Agriculture and 
Business, Shaoxing, China.
(4)Department of Urology, Peking Union Medical College Hospital, Chinese Academy 
of Medical Sciences & Peking Union Medical College, Beijing, China.
(5)Department of Urology, Peking University Shenzhen Hospital, Shenzhen, China.
(6)Department of Pathology, Peking Union Medical College Hospital, Chinese 
Academy of Medical Sciences & Peking Union Medical College, Beijing, China.
(#)Contributed equally

BACKGROUND: Prostate cancer is one of the most prevalent malignancies among men, 
and accurate diagnosis is crucial for effective treatment planning and 
prognostication. The Gleason grading system remains the gold standard for 
histopathological assessment, guiding both diagnosis and risk stratification. 
However, manual grading is time-consuming and subject to considerable 
inter-observer variability, which can lead to the misclassification of tumor 
aggressiveness. Such discrepancies may result in the overtreatment of indolent 
disease or the undertreatment of aggressive cancer, directly compromising 
clinical outcomes. Although computational pathology, particularly deep learning, 
offers a promising solution, its clinical adoption is often hindered by the need 
for large, pixel-level annotated datasets. Weakly supervised methods such as 
multiple instance learning (MIL) can address this issue by training on 
whole-slide image (WSI) labels. However, existing MIL models often struggle to 
adequately replicate the pathologist's workflow of examining tissue architecture 
at low magnification and cellular detail at high magnification. This limitation 
can lead to an incomplete representation of the complex histological landscape 
and a failure to capture subtle textural patterns crucial for grading. The 
objective of this study was, therefore, to develop and validate a novel deep 
learning framework that overcomes these limitations by integrating multiscale 
spatial information with frequency-domain features. We aimed to create a robust 
model for automated Gleason grading that relies only on readily available 
slide-level labels, thereby enhancing diagnostic consistency and efficiency.
METHODS: To address the limitations of manual pixel-level annotation in Gleason 
grading, we propose a multiscale Fourier transformer multiple instance learning 
(MFMIL) model. This approach extracts pathological features at both cellular and 
glandular levels from multiscale WSIs. A Fourier transform is employed to 
enhance feature representation and capture subtle morphological variations. A 
multiscale cross-attention mechanism is then used to integrate features across 
two magnification levels, effectively reducing semantic discrepancies between 
resolutions.
RESULTS: The model was validated on two large-scale datasets: 3,775 radical 
prostatectomy slides from Peking Union Medical College Hospital (PUMCH) and 
10,616 biopsy slides from the public Prostate Cancer Grade Assessment (PANDA) 
dataset. On the PUMCH dataset, MFMIL using only slide-level labels achieved an 
accuracy of 79.40%, an area under the curve (AUC) of 87.88%, and an average 
F1-score of 56.54%. On the PANDA dataset, the model achieved an accuracy of 
61.97%, an AUC of 85.02%, and an F1-score of 58.46%. Compared to several 
state-of-the-art baselines (e.g., transformer-based correlated MIL and 
clustering-constrained attention MIL with multiple branches), MFMIL demonstrated 
superior performance, particularly in identifying aggressive cases such as grade 
group 5 (AUC =97.7%).
CONCLUSIONS: The proposed MFMIL model demonstrates high diagnostic accuracy and 
strong clinical applicability for Gleason grading in both radical prostatectomy 
and prostate biopsy WSIs, offering a promising tool for enhancing prostate 
cancer diagnosis in routine pathology workflows.

Copyright © 2025 AME Publishing Company. All rights reserved.

DOI: 10.21037/qims-2025-439
PMCID: PMC12514604
PMID: 41081157

Conflict of interest statement: Conflicts of Interest: All authors have 
completed the ICMJE uniform disclosure form (available at 
https://qims.amegroups.com/article/view/10.21037/qims-2025-439/coif). The 
authors have no conflicts of interest to declare.


151. J Cancer Educ. 2025 Oct 11. doi: 10.1007/s13187-025-02711-1. Online ahead of 
print.

Communicating With Children With Cancer: Development of a Chatbot-Based 
Educational Support Tool.

Silva E(1), Pinto P(2), Reis LP(2).

Author information:
(1)LIACC/FEUP, Artificial Intelligence and Computer Science Laboratory, Faculty 
of Engineering, University of Porto, Porto, 4200-465, Portugal. 
elianasilva@fe.up.pt.
(2)LIACC/FEUP, Artificial Intelligence and Computer Science Laboratory, Faculty 
of Engineering, University of Porto, Porto, 4200-465, Portugal.

Effective communication between parents and children is crucial in reducing 
distress when a child has cancer. Technology, particularly chatbots, can 
facilitate this communication and provide parents with support. We have 
developed a chatbot to support parents of children with cancer. Using advanced 
natural language processing, it provides empathetic, age-appropriate, and 
informative responses. We created the chatbot using the Dialogflow platform and 
integrated it into a mobile app and website to ensure broad accessibility. This 
multi-platform integration provides consistent support, regardless of the device 
or location used. We conducted a qualitative comparative analysis of our chatbot 
against other chatbots, such as ChatGPT-4 and Microsoft Copilot. This analysis 
was based on five standard criteria for the human evaluation of chatbots. In 
general, our chatbot offered more personalized and targeted support for 
childhood cancer. Unlike other systems that provide generic responses, our 
chatbot uses a friendlier tone and tailors its responses based on factors such 
as the child's age. This project represents a significant advancement in the use 
of technology to address the complex communication needs associated with 
childhood cancer. To our knowledge, it is the first chatbot designed 
specifically for Portuguese-speaking parents. It offers parents personalized, 
accessible support and provides a scalable, cost-effective solution that 
complements the efforts of healthcare professionals. In settings with limited 
resources, the chatbot is an invaluable tool for bridging the gap between 
medical consultations.

© 2025. The Author(s).

DOI: 10.1007/s13187-025-02711-1
PMID: 41076500

Conflict of interest statement: Declarations. Competing Interests: The authors 
declare no competing interests.


152. Eur J Pediatr. 2025 Oct 11;184(11):676. doi: 10.1007/s00431-025-06488-9.

Evaluation of ChatGPT-4 responses on physical activity guidance in children with 
cystic fibrosis: reliability, quality, and readability.

Çelik Z(1), Sari F(2).

Author information:
(1)Department of Physiotherapy and Rehabilitation, Faculty of Health Sciences, 
Amasya University, Amasya, Türkiye. zelihacelik1@hotmail.com.
(2)Department of Physiotherapy and Rehabilitation, Faculty of Physical Therapy 
and Rehabilitation, Bingol University, Bingol, Türkiye.

ChatGPT-4 is a widely used large language model that provides instant answers to 
a variety of health-related questions in different medical fields. This study 
aims to evaluate the reliability, quality, accuracy, and readability of 
ChatGPT's responses to frequently asked questions regarding physical activity in 
children with cystic fibrosis (CF).The responses of ChatGPT-4 to 60 frequently 
asked questions related to physical activity and its effects on the condition of 
children with CF were categorized into five thematic (S1-S5) groups. These 
responses were then evaluated for reliability, quality, accuracy, and 
readability using the modified DISCERN (mDISCERN) tool, the Global Quality Scale 
(GQS), a five-point Likert scale, and the Flesch Reading Ease Scale (FRE), 
respectively.The mean scores for mDISCERN, GQS, and accuracy ranged from 3.38 
(S2) to 3.82 (S4), 3.91 (S2, S4) to 4.25 (S5), and 4.27 (S1, S4) to 4.78 (S3), 
with overall means of 3.5, 3.98, and 4.38, respectively. The readability mean 
scores varied from 29.99 (S5) to 46.31 (S3), with a total mean of 38.07. The ICC 
values for the mDISCERN, GQS, and accuracy were 0.746, 0.666, and 0.665, 
respectively.
CONCLUSION: This study revealed that ChatGPT-4 provides moderate to high levels 
of reliability, quality and accuracy in responses about physical activity in 
children with CF. Low FRE scores showed most responses were "difficult" for the 
target age group. Although ChatGPT-4 serves as a useful supplementary tool for 
patients with CF, professional supervision and further validation are essential 
for safe and effective use in clinical contexts.
WHAT IS KNOWN: • Physical activity benefits children with cystic fibrosis (CF), 
yet access to reliable, understandable educational materials is limited. AI 
tools like ChatGPT-4 are increasingly used in health communication, but their 
reliability, accuracy, and readability remain uncertain.
WHAT IS NEW: • This study systematically evaluates ChatGPT-4 responses to 
CF-related physical activity questions, showing moderate-to-high reliability, 
quality, and accuracy, but low readability, highlighting the need for adaptation 
for pediatric use.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00431-025-06488-9
PMID: 41076450 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval: Since the study 
did not include human participants and solely compared computer-generated data 
with expert-produced information, ethical committee approval was not required. 
Competing interests: The authors declare no competing interests.


153. Comput Methods Programs Biomed. 2026 Jan;273:109087. doi: 
10.1016/j.cmpb.2025.109087. Epub 2025 Sep 29.

Detection of nocturnal epileptic seizures using a wearable armband: A deep 
learning approach combining accelerometry and photoplethysmography signals.

Dong C(1), van Dijk JP(2), Aarts RM(3), Wang Y(4), Long X(5).

Author information:
(1)Department of Medical Imaging, Hebei Medical University, 050031 Shijiazhuang, 
Hebei, China; Department of Electrical Engineering, Eindhoven University of 
Technology, 5612 AP Eindhoven, Netherlands.
(2)Department of Electrical Engineering, Eindhoven University of Technology, 
5612 AP Eindhoven, Netherlands; The Kempenhaeghe Academic Center for 
Epileptology, 5591 VE Heeze, Netherlands.
(3)Department of Electrical Engineering, Eindhoven University of Technology, 
5612 AP Eindhoven, Netherlands.
(4)The Institute of Microelectronics, Chinese Academy of Sciences, 100029 
Beijing, China.
(5)Department of Electrical Engineering, Eindhoven University of Technology, 
5612 AP Eindhoven, Netherlands. Electronic address: x.long@tue.nl.

BACKGROUND: Epileptic seizures can lead to severe outcomes including sudden 
unexpected death in epilepsy (SUDEP). Clinical standard for seizure diagnosis 
and detection requires electroencephalography and video monitoring, which is yet 
considered not suitable for home use, especially during nighttime sleep in a 
low-light condition. We proposed a deep learning (DL)-based approach to 
automatically detect nocturnal major seizures using a wearable armband that can 
potentially help reduce SUDEP risk through timely caregiver intervention.
METHODS: In this prospective cohort study, 68 patients with major seizures were 
monitored for up to three months using a wearable armband (NightWatch®) 
capturing tri-axial accelerometry (ACM) and photoplethysmography (PPG) signals. 
A two-step approach was designed: (1) a pre-screening step using threshold-based 
algorithms to identify suspected seizure events (ACM standard deviation >0.4 or 
heart rate increase >10%), and (2) a DL model (CNN-LSTM with attention 
mechanism) to recognize true seizures. Model performance was evaluated via a 
10-fold cross-validation, reporting sensitivity (SEN), false alarm rate (FAR), 
and area under the ROC curve (AUC).
RESULTS: In 788 overnight recordings (6304 hours), a total of 1846 severe 
seizures were identified. The pre-screening step achieved 0.940 sensitivity in 
pre-identifying or 'preserving' seizures, reducing data volume by 81% (from 6304 
to 1201 hours). The DL model demonstrated a mean accuracy of 0.793 [95% CI: 
0.745-0.841], a mean sensitivity of 0.762 [95% CI: 0.704-0.821], a mean positive 
predictive value of 0.334 [95% CI: 0.229-0.356] and a mean false alarm rate of 
0.165/hour [95% CI: 0.097-0.234]. These results exceeded those of single 
(signal) modality detection methods.
CONCLUSION: Our two-step approach enables accurate, long-term detection of 
severe nocturnal seizures in home settings. The wearable system provides a 
practical solution for continuous monitoring and real-time alerts, thus 
potentially reducing SUDEP risk and improving patient safety, fulfilling an 
urgent unmet need in epilepsy care. Furthermore, by enabling long-term home 
monitoring, this system may help assess the relationship between seizure events 
and lifestyle-related triggers such as sleep deprivation, stress, physical 
exertion, or alcohol consumption, thereby supporting the development of 
personalized preventive strategies.

Copyright © 2025 The Author(s). Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.cmpb.2025.109087
PMID: 41072127 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


154. Bioinformatics. 2025 Oct 2;41(10):btaf541. doi: 10.1093/bioinformatics/btaf541.

Automating candidate gene prioritization with large language models: from naive 
scoring to literature-grounded validation.

Khan T(1), Toufiq M(1), Yurieva M(1), Indrawattana N(2)(3), Jittmittraphap A(4), 
Kosoltanapiwat N(4), Pumirat P(4), Sukphopetch P(4), Vanaporn M(4), Palucka 
K(1), Syed Ahamed Kabeer B(5)(6), Rinchai D(7), Chaussabel D(1).

Author information:
(1)The Jackson Laboratory for Genomic Medicine, 10 Discovery Drive, Farmington, 
Connecticut, 06032, United States.
(2)Biomedical Research Incubator Unit, Research Department, Faculty of Medicine 
Siriraj Hospital, Mahidol University, Bangkok, Thailand.
(3)Siriraj Center of Research and Excellence in Allergy and Immunology, Faculty 
of Medicine Siriraj Hospital, Mahidol University, Bangkok, Thailand.
(4)Department of Microbiology and Immunology, Faculty of Tropical Medicine, 
Mahidol University, Bangkok, 73170, Thailand.
(5)Sidra Medicine, Doha, Qatar.
(6)Department of Pathology, Saveetha Medical College and Hospital, Saveetha 
Institute of Medical and Technical Sciences (SIMATS), Saveetha University, 
Chennai, India.
(7)St. Jude Children's Research Hospital, Memphis, TN, United States.

MOTIVATION: Identifying promising therapeutic targets from thousands of genes in 
transcriptomic studies remains a major bottleneck in biomedical research. While 
large language models (LLMs) show potential for gene prioritization, they suffer 
from hallucination and lack systematic validation against expert knowledge.
RESULTS: The framework identified 609 sepsis-relevant genes with >94% filtering 
efficiency, demonstrating strong enrichment for inflammatory pathways including 
TNF-α signaling, complement activation, and interferon responses. Literature 
validation yielded 30 ultra-high confidence therapeutic candidates, including 
both established sepsis genes (IL10, TREM1, S100A9, NLRP3) and novel targets 
warranting investigation. Benchmark validation against expert-curated databases 
achieved 71.2% recall, with systematic correlation between computational 
confidence and evidence quality. The final candidate set balanced discovery (11 
novel genes) with validation (19 known genes), maintaining biological coherence 
throughout the filtering process. This framework demonstrates that rigorous 
methodology can transform unreliable LLM outputs into systematically validated 
biological insights. By combining computational efficiency with literature 
grounding, the approach provides a practical tool for prioritizing experimental 
validation efforts. The modular design enables adaptation to other diseases 
through knowledge base substitution, offering a systematic approach to 
literature-guided biomarker discovery.
AVAILABILITY AND IMPLEMENTATION: We developed a two-stage computational 
framework that combines LLM-based screening with literature validation for 
systematic gene prioritization. Starting with 10 824 genes from the BloodGen3 
repertoire, we applied multi-criteria evaluation for sepsis relevance, followed 
by retrieval-augmented generation using 6346 curated sepsis publications. A 
novel faithfulness evaluation system verified that LLM predictions aligned with 
retrieved literature evidence. Source code and implementation details are 
available at https://github.com/taushifkhan/llm-geneprioritization-framework, 
vector database at https://doi.org/10.5281/zenodo.15802241, and Interactive 
demonstration at https://llm-geneprioritization.streamlit.app/.

© The Author(s) 2025. Published by Oxford University Press.

DOI: 10.1093/bioinformatics/btaf541
PMCID: PMC12548045
PMID: 41071041 [Indexed for MEDLINE]


155. F1000Res. 2025 Oct 6;14:665. doi: 10.12688/f1000research.166372.2. eCollection 
2025.

New Horizons in Higher Education: Examining the Mental Well-Being of Medical & 
Health Sciences Students Through the Use of Artificial Intelligence Based 
Chatbot Platforms in the United Arab Emirates - A Cross-Sectional Comparative 
Study.

Abdelaziz Rashad Dabou E(1)(2), Magdi Ibrahim F(1)(3), Faisal Haimour M(1), 
Saleh A(1), Mottershead R(4)(5)(6).

Author information:
(1)RAK College of Nursing,, Ras Al Khaimah Medical and Health Sciences 
University, Ras Al Khaimah, United Arab Emirates.
(2)Faculty of Nursing, Alexandria University, Alexandria, Egypt.
(3)Faculty of Nursing, Mansoura University, Mansoura, Dakahlia Governorate, 
Egypt.
(4)Faculty of Nursing, College of Health Sciences, University of Sharjah, 
Sharjah, United Arab Emirates.
(5)College of Nursing, University of Baghdad, Baghdad, Iraq.
(6)Sakina, Behavioural Science Institute, SEHA, SEHA, Al Ain, United Arab 
Emirates.

BACKGROUND: Barriers to mental-health care include limited resources and 
workforce, access constraints, and stigma. Artificial-intelligence (AI)-enabled 
chatbots may offer low-threshold support.
METHODS: Cross-sectional correlational (comparative) study in one private 
health-sciences university in the UAE. Proportional stratified random sampling 
across four colleges yielded n = 298 undergraduates. Instruments: (i) 
Socio-Demographic Questionnaire; (ii) researcher-developed AI Chatbot Usability 
questionnaire (content validated by a bilingual specialist); and (iii) 
Depression Anxiety Stress Scale-21 (DASS-21; established reliability/validity). 
Questionnaires were administered face-to-face with standardized instructions.
RESULTS: 206/298 (69.1%) had ever used an AI chatbot; most used Snapchat AI 
(76.9%), followed by ChatGPT/Bard (23.4% each). Overall, 57.0% had 
moderate-to-extremely-severe depression, 68.5% anxiety, and 33.6% stress. Users 
had higher odds of moderate-to-extremely-severe anxiety and depression than 
non-users. In multivariable models, higher depression (OR = 1.022; 95% CI 
1.01-1.085; p<0.001) and anxiety (OR = 1.05; 95% CI 1.01-1.21; p<0.001) 
independently predicted chatbot use; stress did not.
CONCLUSION: Among UAE health-sciences students, AI-chatbot use is common and 
associated with higher depression/anxiety severity; this likely reflects 
help-seeking rather than causation. Universities should integrate early, 
stigma-sensitive supports, potentially including regulated, evidence-based 
chatbot tools-within stepped-care services.

Copyright: © 2025 Abdelaziz Rashad Dabou E et al.

DOI: 10.12688/f1000research.166372.2
PMCID: PMC12504930
PMID: 41070110 [Indexed for MEDLINE]

Conflict of interest statement: No competing interests were disclosed.


156. BMJ Open. 2025 Oct 9;15(10):e103893. doi: 10.1136/bmjopen-2025-103893.

Students' perceptions of AI mental health chatbots: an exploratory qualitative 
study at Sultan Qaboos University.

AlMaskari AM(1), Al-Mahrouqi T(2), Al Lawati A(1), Al Aufi H(1), Al Riyami Q(3), 
Al-Sinawi H(2).

Author information:
(1)Behavioral Medicine, College of Medicine and Health Sciences, Sultan Qaboos 
University, Seeb, Oman.
(2)Department of Behavioral Medicine, College of Medicine and Health Sciences, 
Sultan Qaboos University, Sultan Qaboos University Hospital, Seeb, Oman.
(3)Behavioral Medicine, College of Medicine and Health Sciences, Sultan Qaboos 
University, Seeb, Oman S127549@student.squ.edu.om.

OBJECTIVES: The aim of this study is to explore the perceptions, attitudes and 
previous experiences of Sultan Qaboos University (SQU) students towards 
artificial intelligence (AI) mental health chatbots.
DESIGN: The study was conducted as an exploratory qualitative study, involving 
focus group discussions (FGDs).
SETTING: The study was conducted at SQU. All FGDs were conducted via Google 
Meet.
PARTICIPANTS: A total of 60 students were recruited through convenience 
sampling. Both male and female students from different colleges were enrolled in 
the study. 10 FGDs were conducted.
RESULTS: The interview findings revealed that most students were open to using 
AI mental health chatbots, with many already having used similar applications. 
Participants listed several benefits that such applications could have, 
including increased availability, reduced stigma and behaviour tracking. The 
participants also raised some concerns about such applications, including data 
security, cultural sensitivity and various ethical issues.
CONCLUSION: The future of AI chatbots for mental health is promising, as they 
have the potential to enhance mental healthcare services. Now that we know the 
disadvantages and limitations that may be associated with the use of such apps, 
it would be easier to design suitable apps that can properly triage mental 
health issues and provide supportive mental health.

© Author(s) (or their employer(s)) 2025. Re-use permitted under CC BY-NC. No 
commercial re-use. See rights and permissions. Published by BMJ Group.

DOI: 10.1136/bmjopen-2025-103893
PMCID: PMC12516975
PMID: 41067756 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: None declared.


157. J Clin Exp Dent. 2025 Sep 1;17(9):e1099-e1107. doi: 10.4317/jced.63136. 
eCollection 2025 Sep.

Assessing the Utility of Large Language Models in Guiding Dental Practitioners 
on Pediatric Patient Care: A Comparative AI Study.

Raj M(1), Ravindran V(2), Arthanari A(3).

Author information:
(1)Post graduate, Dept of Pediatric and Preventive Dentistry, Saveetha Dental 
College & Hospitals, Saveetha Institute of Medical and Technical Sciences, 
Saveetha University, Chennai, Tamil Nadu, India.
(2)Associate Professor, Dept of Pediatric and Preventive Dentistry, Saveetha 
Dental College & Hospitals, Saveetha Institute of Medical and Technical 
Sciences, Saveetha University, Chennai, Tamil Nadu, India.
(3)Lecturer, Dept of Forensic Odontology, Saveetha Dental College & Hospitals, 
Saveetha Institute of Medical and Technical Sciences, Saveetha University, 
Chennai, Tamil Nadu, India.

BACKGROUND: Large Language Models (LLMs) are transforming clinical 
decision-making by offering rapid, context-aware access to evidence-based 
knowledge. However, their efficacy in pediatric dentistry remains underexplored, 
especially across multiple LLM platforms.Objective: To comparatively evaluate 
the clinical quality, readability, and originality of responses generated by 
nine contemporary LLMs for pediatric dental queries.
MATERIAL AND METHODS: A cross-sectional study assessed the performance of 
ChatGPT-3.5, ChatGPT-4o, Gemini 2.0, Gemini 2.5, Claude 3.5 Haiku, Claude 3.7 
Sonnet, Grok-3, Grok-3 Mini, and DeepSeek-V3. Twenty pediatric dental questions 
were posed in one-shot queries to each LLM. Responses were evaluated by ten 
pediatric dental experts using the Modified Global Quality Scale (MGQS), Flesch 
Reading Ease Score (FRES), Flesch-Kincaid Grade Level (FKGL), and Turnitin 
Similarity Index. ANOVA and Cohen's Kappa were used for statistical analysis.
RESULTS: ChatGPT-4o demonstrated the highest overall MGQS (4.28 ± 0.24), 
followed by ChatGPT-3.5 (3.45 ± 0.27). DeepSeek-V3 scored lowest (2.18 ± 0.19). 
Topic-wise, ChatGPT-4o consistently outperformed others across all subdomains. 
FRES and FKGL scores indicated moderate readability, with Claude models 
exhibiting the highest linguistic complexity. Turnitin analysis revealed 
low-to-moderate similarity across models. Inter-rater agreement was substantial 
(κ = 0.78).
CONCLUSIONS: Among evaluated LLMs, ChatGPT-4o exhibited superior performance in 
clinical relevance, coherence, and originality, suggesting its potential utility 
as an adjunct in pediatric dental decision-making. Nonetheless, variability 
across models underscores the need for critical appraisal and cautious 
integration into clinical workflows. Key words:Artificial Intelligence, Clinical 
decision support, Health Communication, Large language models, Natural Language 
Processing.

Copyright: © 2025 Medicina Oral S.L.

DOI: 10.4317/jced.63136
PMCID: PMC12502747
PMID: 41064784

Conflict of interest statement: NIL.


158. Sci Rep. 2025 Oct 8;15(1):35075. doi: 10.1038/s41598-025-18488-8.

Insights into the adoption of innovative clinical trials across therapeutic 
areas using clinical trials registry data and large Language models.

Azzolina D(1), Scisciola V(2), Vedovelli L(2), Iervolino D(2), Khan MR(2), 
Comoretto RI(3), Murri MB(4), Gregori D(2).

Author information:
(1)Biostatistics and Clinical Trial Methodology Unit, Clinical Research Center 
DEMeTra, Department of Translational Medicine, University of Naples Federico II, 
80138, Naples, Italy. danila.azzolina@unina.it.
(2)Unit of Biostatistics, Epidemiology and Public Health, Department of Cardiac, 
Thoracic, Vascular Sciences and Public Health, University of Padova, Padova, 
Italy.
(3)Department of Public Health and Paediatrics, University of Turin, Turin, 
Italy.
(4)Department of Neuroscience and Psychiatry, University of Ferrara, Ferrara, 
Italy.

Innovative clinical trial designs, such as adaptive and Bayesian methodologies, 
have gained traction as solutions to the challenges of traditional trials, 
including their high costs and complex regulations. When they adhere to relevant 
ethical and regulatory requirements, these designs can improve efficiency, 
flexibility, and ethical standards. However, their application outside of 
oncology, particularly in fields such as neuroscience and rare diseases, remains 
underexplored. We analyzed data from ClinicalTrials.gov for interventional 
trials registered between 2005 and 2024. The trials were classified as 
innovative or traditional using a keyword-based algorithm. Therapeutic areas 
were identified using a large language model (LLM), with classification accuracy 
evaluated using a random sample of 2,000 trials. Of the 348,818 trials, 5827 
were classified as innovative, with prevalence in neuroscience and rare 
diseases. These designs were predominantly observed in early-phase trials and 
pediatric research, with limited representation in elderly-focused or 
sex-specific studies. Innovative trial adoption has grown since 2011, spurred by 
regulatory advancements and increased funding from scientific networks and the 
National Institutes of Health. Survival analysis revealed that innovative trials 
tend to remain active for longer than traditional trials; however, this trend 
varies across different medical disciplines. LLM demonstrated a classification 
accuracy of 94.6% (95%CI = 93.6%-95.5%), supporting its utility for trial 
categorization. The rise in innovative clinical trial designs reflects a shift 
toward addressing complex challenges in neuroscience, rare diseases, and other 
therapeutic areas. Although these designs show promise in improving trial 
efficiency and patient outcomes, their success depends on rigorous planning and 
adherence to regulatory standards. Advancing LLM-based tools can further 
optimize clinical trial monitoring by tailoring research in trial settings and 
therapeutic fields.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-18488-8
PMCID: PMC12508068
PMID: 41062537 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


159. Sci Rep. 2025 Oct 7;15(1):34993. doi: 10.1038/s41598-025-18969-w.

Evaluation of large language models within GenAI in qualitative research.

Mehta SD(#)(1)(2), Paul S(#)(3), Awiti E(4), Young S(3), Zulaika G(5), Otieno 
FO(4), Phillips-Howard PA(5), Mason L(5), Bhaumik R(3).

Author information:
(1)Division of Infectious Diseases, Department of Medicine, Rush University 
College of Medicine, Chicago, USA. Supriyad@uic.edu.
(2)Division of Epidemiology & Biostatistics, University of Illinois Chicago 
School of Public Health, Chicago, USA. Supriyad@uic.edu.
(3)Division of Epidemiology & Biostatistics, University of Illinois Chicago 
School of Public Health, Chicago, USA.
(4)Nyanza Reproductive Health Society, Kisumu, Kenya.
(5)Department of Clinical Sciences, Liverpool School of Tropical Medicine, 
Liverpool, UK.
(#)Contributed equally

Large language models (LLMs) perform tasks such as summarizing information and 
analyzing sentiment to generate meaningful and natural responses. The 
application of GenAI incorporating LLMs raises potential utilities for 
conducting qualitative research. Using a qualitative study that assessed the 
impact of the COVID-19 pandemic on the sexual and reproductive health of 
adolescent girls and young women (AGYW) in rural western Kenya: our objective 
was to compare thematic analyses conducted by GenAI using LLM to qualitative 
analysis conducted by humans, with regards to major themes identified, selection 
of supportive quotes, and quality of quotes; and secondarily to explore 
quantitative and qualitative sentiment analysis conducted by the GenAI. We 
interfaced with GPT-4o through google colaboratory. After inputting the 
transcripts and pre-processing, we constructed a standardized task prompt. Two 
investigators independently reviewed the GenAI product using a rubric based on 
qualitative research standards. When compared to human-derived themes, we did 
not find disagreement with the sub-themes raised by GenAI, but did not consider 
some to rise to level of a theme. Performance was low and variable with regards 
to selection of quotes that were consistent with and strongly supportive of 
thematic and sentiment analysis. Hallucinations ranged from a single word or 
phrase change to truncation or combinations of text that led to modified 
meaning. GenAI identified numerous and relevant biases, primarily related to the 
underlying training data and its lack of cultural understanding. Few prior 
studies have directly compared LLM-driven thematic coding with human coding in 
qualitative analysis, and our study - grounded in qualitative study rigor - 
allowed for a thorough evaluation. GenAI implemented in GPT-4o was unable to 
provide a thematic analysis that is indistinguishable from a human analysis. We 
recommend that it can currently be used as an aid in identifying themes, 
keywords, and basic narrative, and potentially as a check for human error or 
bias. However, until it can eliminate hallucinations, provide better contextual 
understanding of quotes and undertake a deeper scrutiny of data, it is not 
reliable or sophisticated enough to undertake a rigorous thematic analysis equal 
in quality to experienced qualitative researchers.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-18969-w
PMCID: PMC12504727
PMID: 41057565 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests. Ethics approval and consent to participate: This 
study was approved by the institutional review boards of Maseno University 
Ethics Review Committee (MUERC, MSU/DRPI/MUERC/01021/21), University of Illinois 
at Chicago (UIC, #2022 − 0220), and (Liverpool School of Tropical Medicine 
(LSTM, #21–087, favourable ethical opinion). Written informed consent was 
obtained for all participants.


160. J Am Coll Surg. 2026 Jan 1;242(1):47-52. doi: 10.1097/XCS.0000000000001602. Epub 
2025 Dec 17.

Death of the Personal Statement: Qualitative Comparison Between Human-Authored 
and Artificial Intelligence-Generated Medical School Admissions Essays.

Vaccaro MJ(1), Sharma I(2), Espina-Rey AP(1), Lyman N(1), Palacios C(3), Zhang 
Y(1), Mehta A(2), Leto Barone AA(2), Kellogg BC(2).

Author information:
(1)From the, University of Central Florida College of Medicine, Orlando, FL 
(Vaccaro, Espina-Rey, Lyman, Zhang).
(2)Division of Plastic and Craniofacial Surgery, Nemours Children's Hospital, 
Orlando, FL (Sharma, Mehta, Leto Barone, Kellogg).
(3)Dr. Kiran C. Patel College of Allopathic Medicine (NSU MD), Nova Southeastern 
University, Ft Lauderdale, FL (Palacios).

BACKGROUND: Medical school admissions committees use personal statements as 
subjective measures of applicants' values, motivations, and experiences. 
Students at varying levels of education report that they would or do use 
artificial intelligence (AI) in their academic pursuits. It has not been 
investigated how effectively medical school admissions committees can 
distinguish between AI-generated and human-authored personal statements, or 
whether the use of AI provides students an advantage.
STUDY DESIGN: Human-authored personal statements were retrieved from the 2019 
application cycle (before AI chatbots). ChatGPT 4.0 was used to generate 
personal statements from summaries of the human-authored essays. In a 
prospective, single-blind, randomized controlled trial, medical school 
application readers evaluated unique combinations of AI-generated and 
human-authored personal statements for essay quality (on a 7-point Likert 
scale), speculated authorship (human or AI), and speculation confidence (on a 
5-point Likert scale). ZeroGPT, an AI-detection tool, also speculated 
authorship.
RESULTS: Seventeen medical school application readers completed 325 scoring 
rubrics across 309 essays. Readers demonstrated a 56% accuracy in correctly 
identifying authorship, whereas ZeroGPT showed 91% accuracy. Readers were more 
likely to assume human authorship of higher-scoring essays (Mann-Whitney U test, 
p < 0.001). AI-generated essays scored better than their human-authored 
counterparts (Wilcoxon signed-rank test, p = 0.020) with mean scores of 
5.02 ± 1.21 and 4.67 ± 1.33, respectively.
CONCLUSIONS: AI-generated personal statements were rated more highly and were 
nearly indistinguishable to medical school application readers. Despite 
ZeroGPT's higher accuracy in detecting AI use, its rate of false positives 
remains unacceptably high for use in medical school admissions. The role of 
personal statements in medical school admissions requires urgent reconsideration 
to maintain credibility.

Copyright © 2025 by the American College of Surgeons. Published by Wolters 
Kluwer Health, Inc. All rights reserved.

DOI: 10.1097/XCS.0000000000001602
PMID: 41051105 [Indexed for MEDLINE]


161. Cureus. 2025 Sep 3;17(9):e91571. doi: 10.7759/cureus.91571. eCollection 2025 
Sep.

Transforming Pediatric Physiotherapy: The Role of ChatGPT in Therapy, 
Limitations, and Ethical Considerations.

Ziab HM(1)(2), Mazbouh R(3)(4), Siblini F(5), Nashwan AJ(6).

Author information:
(1)Al-Maha Pediatric Specialized Care Center, Hamad Medical Corporation, Doha, 
QAT.
(2)Faculty of Public Health, Lebanese University, Beirut, LBN.
(3)Department of Physiotherapy, Islamic University of Lebanon, Baalbek, LBN.
(4)Department of Research and Development/Physiotherapy, Health, Rehabilitation, 
Integration and Research Center, Beirut, LBN.
(5)School of Rehabilitation, Tehran University of Medical Sciences, Tehran, IRN.
(6)Department of Nursing Education and Research, Hamad Medical Corporation, 
Doha, QAT.

The use of large language models (LLMs), such as ChatGPT in pediatric 
physiotherapy, is a big step forward in the provision of tailored care, clinical 
decision-making, and caregiver education. This narrative review examines 
ChatGPT's roles in improving physiotherapy practice across three major domains: 
(1) clinical reasoning, (2) tailored therapeutic recommendations, and (3) 
caregiver engagement. Drawing on recent empirical and theoretical literature, 
the paper discusses ChatGPT's ability to generate context-aware, developmentally 
appropriate content, its limits in transparency, factual correctness, and 
relational interactivity. Key ethical challenges are examined, including 
algorithmic bias, data privacy, medico-legal responsibility, and the risk of 
diminishing humanistic aspects of care. While ChatGPT holds promise as a 
supplementary tool to support evidence-based rehabilitation, particularly in 
under-resourced or home-based settings, its effective implementation requires 
ongoing human oversight, robust validation, and clinician training. The paper 
concludes with practical recommendations and ethical safeguards for responsible 
integration, emphasizing that ChatGPT should augment, not replace, human 
clinical expertise in pediatric rehabilitation contexts.

Copyright © 2025, Ziab et al.

DOI: 10.7759/cureus.91571
PMCID: PMC12495263
PMID: 41049913

Conflict of interest statement: Conflicts of interest: In compliance with the 
ICMJE uniform disclosure form, all authors declare the following: 
Payment/services info: All authors have declared that no financial support was 
received from any organization for the submitted work. Financial relationships: 
All authors have declared that they have no financial relationships at present 
or within the previous three years with any organizations that might have an 
interest in the submitted work. Other relationships: All authors have declared 
that there are no other relationships or activities that could appear to have 
influenced the submitted work.


162. Front Endocrinol (Lausanne). 2025 Sep 19;16:1627919. doi: 
10.3389/fendo.2025.1627919. eCollection 2025.

Causal insights into gestational diabetes mellitus.

Zahoor S(1), Constantinou AC(2), O'Halloran F(3), O'Mahony L(4), O'Riordan M(4), 
Kgosidialwa O(4), Culliney L(4), Said Alhajri M(1), Hasanuzzaman M(5).

Author information:
(1)Department of Computer Science, Munster Technological University, 
Cork, Ireland.
(2)Bayesian Artificial Intelligence Research Lab, Machine Intelligence and 
Decision Systems (MInDS) Group, Queen Mary University of London (QMUL), 
London, United Kingdom.
(3)Department of Biological Sciences, Munster Technological University, 
Cork, Ireland.
(4)Cork University Hospital, Cork, Ireland.
(5)Queen's University Belfast, School of Electronics, Electrical Engineering and 
Computer Science (EEECS), Belfast, United Kingdom.

INTRODUCTION: Gestational diabetes mellitus (GDM), defined by the onset of 
hyperglycaemia during pregnancy, remains the most prevalent metabolic 
complication in pregnancy. It is associated with increased risks of adverse 
maternal, neonatal, and long-term metabolic outcomes. This study aimed to 
identify potential causal relationships within clinical data on GDM that could 
support more targeted and effective interventions.
METHODS: A clinically curated dataset of patients diagnosed with GDM at a major 
Irish maternity hospital was analysed, covering the study periods 2014-2016 and 
2020. A knowledge graph was constructed by integrating clinical expertise, 
established literature, and insights generated using the GPT-4 large language 
model. To complement this, 20 structure learning algorithms were applied to 
independently infer Causal Bayesian Networks (CBNs). A model-averaging approach 
was then used to generate a consensus-based causal structure to account for 
variability across individual models.
RESULTS: The integrative model produced a more stable representation of 
underlying relationships and yielded quantifiable insights to support clinical 
decision-making. Clinicians involved in the study reported improved confidence 
in patient care strategies due to the ability to quantify these relationships, 
facilitating more personalised, evidence-based practice. Key findings from the 
model-averaged CBN highlighted critical pathways in GDM management, such as the 
influence of birth weight on neonatal intensive care unit (NICU) admissions and 
the impact of dietary intervention on maternal glucose regulation. Sensitivity 
analysis confirmed birth weight, gestational age at delivery, and mode of 
delivery as major determinants of maternal and neonatal outcomes. Non-modifiable 
factors, including a history of multiple pregnancies and prior GDM, also 
contributed to risk stratification.
DISCUSSION: This study applied structure learning techniques to observational 
clinical data to identify clinically relevant relationships. The resulting 
insights provide a basis for generating hypotheses that could refine 
intervention strategies and improve patient outcomes in GDM care.

Copyright © 2025 Zahoor, Constantinou, O’Halloran, O’Mahony, O’Riordan, 
Kgosidialwa, Culliney, Said Alhajri and Hasanuzzaman.

DOI: 10.3389/fendo.2025.1627919
PMCID: PMC12490993
PMID: 41048423 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


163. Stud Health Technol Inform. 2025 Oct 2;332:72-76. doi: 10.3233/SHTI251498.

Simulating Empathic Interactions with Synthetic LLM-Generated Cancer Patient 
Personas.

Rashid R(1), Kheirinejad S(1), White BM(1), Hashtarkhani S(1), Kheirkhah 
Rahimabad P(1), Kumsa FA(1), Chinthala L(1), Zink JA(1), Brett CL(2), Davis 
RL(1), Schwartz DL(3), Shaban-Nejad A(1).

Author information:
(1)Center for Biomedical Informatics, Department of Pediatrics, College of 
Medicine, University of Tennessee Health Science Center (UTHSC), Memphis, 
Tennessee, USA.
(2)The University of Tennessee Graduate School of Medicine, Knoxville, 
Tennessee, USA.
(3)Department of Radiation Oncology, UTHSC, Memphis, TN, USA.

Unplanned interruptions in radiation therapy (RT) increase clinical risks, yet 
proactive, personalized psychosocial support remains limited. This study 
presents a proof-of-concept framework that simulates and evaluates Empathic 
AI-patient interactions using large language models (LLMs) and synthetic 
oncology patient personas. Leveraging a de-identified dataset of patient 
demographics, clinical features, and social determinants of health (SDoH), we 
created realistic personas that interact with an empathic AI assistant in 
simulated dialogues. The system uses dual LLMs, one for persona generation and 
another for empathic response, which engage in multi-turn dialogue pairs per 
persona. We evaluated the outputs using statistical similarity tests, 
quantitative metrics (BERTScore, SDoH relevance, empathy, persona distinctness), 
and qualitative human assessment. The results demonstrate the feasibility of 
scalable, secure, and context-aware dialogue for early-stage AI development. 
This HIPAA/GDPR compliant framework supports ethical testing of empathic 
clinical support tools and lays the groundwork for AI-driven interventions to 
improve RT adherence.

DOI: 10.3233/SHTI251498
PMID: 41041749 [Indexed for MEDLINE]


164. BMC Med Educ. 2025 Oct 2;25(1):1293. doi: 10.1186/s12909-025-07668-9.

Psychology student and mental health practitioner experiences of and 
perspectives on Client101, a virtual client chatbot training tool.

Schmidt S(1), Cabrera Lozoya D(2), Kiropoulos L(3), Conway M(4), D'Alfonso S(2).

Author information:
(1)School of Computing and Information Systems, The University of Melbourne, 
Melbourne, Australia. simone.schmidt@unimelb.edu.au.
(2)School of Computing and Information Systems, The University of Melbourne, 
Melbourne, Australia.
(3)Melbourne School of Psychological Sciences, The University of Melbourne, 
Melbourne, Australia.
(4)School of Computing and Information Systems, The University of Melbourne, 
Melbourne, Australia. mike.conway@unimelb.edu.au.

BACKGROUND: Limited practice opportunities and recent developments in natural 
language processing have spurred the development of systems housing virtual 
client chatbots to be used as psychotherapy training tools. Initial studies 
convey the potential of virtual client chatbots to support the mental health 
practitioner in developing key skills in the delivery of therapy. These studies 
are limited in that they often do not include mental health professionals or 
students in the testing of their virtual client chatbots, nor focus on a 
particular learning context in their evaluation of the training tool. This study 
addresses these shortcomings by exploring mental health professionals’ and 
psychology students’ experiences of and perspectives on Client101, a virtual 
client chatbot training tool, in relation to specific training contexts.
METHODS: Participants (Masters psychology students who are in training to become 
qualified therapists, counsellors and youth peer workers) interacted with the 
virtual client chatbots housed in the Client101 platform and then completed 
questionnaires and interviews. This mixed-methods study uses a convergent 
parallel design: questionnaires and interviews were analysed separately and then 
integrated to produce the overall results. Questionnaires were analysed via 
descriptive statistics and participant interviews were reflexively thematically 
analysed. The questionnaire responses supplement the qualitative data.
RESULTS: Participants reported they were surprised how human-like the virtual 
client chatbots were but also reported limitations to their realism. These 
limitations included the perception that the chatbots were too compliant, not 
complex enough and did not evolve. Participants considered that Client101 could 
supplement their training context and provide a safe place for the practitioner 
to build confidence in key skills prior to interacting with real clients. 
Participants provided recommendations for how Client101 should be implemented.
CONCLUSIONS: This study revealed that there is a need to supplement the existing 
psychology and mental health training context with more opportunities to build 
confidence in key skills prior to interacting with real clients. Client101 could 
enhance the learning context through providing a safe, accessible, independent 
and effective means of practice. However, the virtual client chatbots should 
convey greater resistance and complexity to be more effective learning tools.
SUPPLEMENTARY INFORMATION: The online version contains supplementary material 
available at 10.1186/s12909-025-07668-9.

DOI: 10.1186/s12909-025-07668-9
PMCID: PMC12492891
PMID: 41039348

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Ethics approval was obtained from the Office of Research Ethics and 
Integrity at the University of Melbourne in November 2023 (2023-27815-47508-3). 
All participants provided written informed consent to participate. This study 
was conducted in accordance with the ethical requirements of the Helsinki 
Declaration. Consent for publication: All participants provided written informed 
consent to the publishing of results from their participation in the study. 
Competing interests: The authors declare no competing interests. Study design: 
This mixed-methods study uses a convergent parallel design. Quantitative and 
qualitative data were collected and analysed separately and then integrated 
(highlighting points of convergence and divergence) to produce the overall 
results. Questionnaires were analysed via descriptive statistics and participant 
interviews were reflexively thematically analysed; the questionnaire responses 
supplement the qualitative data. (N.B. Clinical Trial number not applicable.)


165. Recenti Prog Med. 2025 Oct;116(10):585-586. doi: 10.1701/4573.45787.

Validazione clinica di un sistema Model Context Protocol per il supporto 
decisionale pediatrico: studio su casi benchmark.

[Article in Italian]

Mondillo G(1), Perrotta A(1), Frattolillo V(1), Masino M(1), Colosimo S(1), 
Abbate FG(1), Marzuillo P(1).

Author information:
(1)Dipartimento della Donna, del Bambino e di Chirurgia Generale e 
Specialistica, Università della Campania "Luigi Vanvitelli", Napoli.

Model Context Protocol (MCP) is an open standard for connecting AI applications 
to external tools. While not designed for healthcare, it offers advantages for 
clinical decision support through natural language queries. We developed a 
pediatric MCP server with 46 clinical tools and tested it using 32 cases. 
Results: correctly processed 31/32 cases. This is the first clinical validation 
of MCP technology, demonstrating high reliability for clinical application.

DOI: 10.1701/4573.45787
PMID: 41037371 [Indexed for MEDLINE]


166. Recenti Prog Med. 2025 Oct;116(10):551-555. doi: 10.1701/4573.45775.

Why tomorrow's public health needs to be digital: artificial intelligence and 
automation for a sustainable Italian National Health Service.

Baglivo F(1), Diedenhofen G(2), De Angelis L(1), Pivetta A(3), Causio FA(4), 
D'Ambrosio A(3), Sacchi FA(5), Di Pumpo M(4), Belpiede A(6), Ghisalberti G(7), 
Ferro D(8), Rizzo C(1).

Author information:
(1)Department of Translational Research and New Technologies in Medicine and 
Surgery, University of Pisa, Italy - Italian Society for Artificial Intelligence 
in Medicine (SIIAM - Società Italiana Intelligenza Artificiale in Medicina), 
Rome, Italy.
(2)Italian Society for Artificial Intelligence in Medicine (SIIAM - Società 
Italiana Intelligenza Artificiale in Medicina), Rome, Italy - Specialization 
School in Health Statistics and Biometry, Department of Public Health and 
Infectious Diseases, Sapienza University of Rome, Italy.
(3)Italian Society for Artificial Intelligence in Medicine (SIIAM - Società 
Italiana Intelligenza Artificiale in Medicina), Rome, Italy.
(4)Italian Society for Artificial Intelligence in Medicine (SIIAM - Società 
Italiana Intelligenza Artificiale in Medicina), Rome, Italy - Section of 
Hygiene, Department of Life Sciences and Public Health, Università Cattolica del 
Sacro Cuore, Rome, Italy.
(5)Italian Society for Artificial Intelligence in Medicine (SIIAM - Società 
Italiana Intelligenza Artificiale in Medicina), Rome, Italy - Public Health 
Department, Monroe University, New York, Usa.
(6)Italian Society for Artificial Intelligence in Medicine (SIIAM - Società 
Italiana Intelligenza Artificiale in Medicina), Rome, Italy - Hospital Medical 
Management Unit - National Cancer Institute, "Fondazione G. Pascale", IRCCS, 
Naples, Italy.
(7)ICT and TLC Innovation Office, Azienda ospedaliero-universitaria Senese, 
Siena, Italy.
(8)Italian Society for Artificial Intelligence in Medicine (SIIAM - Società 
Italiana Intelligenza Artificiale in Medicina), Rome, Italy - Predictive and 
Preventive Medicine, Bambino Gesù Children Hospital, IRCCS, Rome, Italy.

Italy's National Health Service (SSN) serves one of Europe's oldest populations 
under fiscal constraint and a fragmented data infrastructure. Rather than a 
standalone fix, artificial intelligence should be treated as a catalyst for a 
human-centred digital transformation that improves access, quality, and 
sustainability. Building on the Italian Society for Artificial Intelligence in 
Medicine (SIIAM) vision, we outline a pragmatic agenda. First, reduce 
elective-care backlogs by automating confirmations, reminders, cancellations, 
and rescheduling; deploy multilingual conversational agents to collect 
structured pre-visit histories and deliver summaries, while natural-language 
processing flags overdue follow-ups. Second, advance equity by offering 
inclusive digital front doors and tele-triage that prioritise patients facing 
language, literacy, socioeconomic, or geographic barriers. Third, curb waste 
through clinical-decision support and workflow automation that standardise 
evidence-based practice and relieve documentation burden. Fourth, modernise 
surveillance by pairing large language model powered voice agents for behaviour 
and symptom monitoring with participatory systems and AI epidemic intelligence. 
Fifth, link data and people through multidisciplinary teams and a 
human-in-the-loop approach that embeds transparency, bias mitigation, privacy, 
and safety. Implementation should start where impact is fastest: risk-stratified 
booking, proactive reminders, and shared dashboards with comparable indicators. 
To sustain gains, ring-fence resources for regional multidisciplinary units, 
enforce interoperability and reference datasets, and align procurement with 
European requirements for auditability and post-deployment monitoring. AI can 
help reshape Italian healthcare, but success ultimately depends on integrated 
data, trained teams, and robust governance.

DOI: 10.1701/4573.45775
PMID: 41037359 [Indexed for MEDLINE]


167. Hum Reprod. 2025 Nov 1;40(11):2066-2077. doi: 10.1093/humrep/deaf186.

Identifying candidate genes for spermatogenic failure and predicting ICSI 
outcomes using single-cell RNA sequencing and protein-protein interaction 
networks.

Liu L(1)(2), Huang S(3), Jiang F(2)(4), Liang G(2), Zhu X(5), Zhu H(2), Tian 
W(1)(6)(7).

Author information:
(1)State Key Laboratory of Genetics and Development of Complex Phenotypes, 
Department of Computational Biology, School of Life Sciences, Fudan University, 
Shanghai, China.
(2)Obstetrics and Gynecology Hospital, Fudan University, Shanghai, China.
(3)Department of Genetic Analysis, Guangdong MedicineAI Technology Co. Ltd, 
Guangdong, China.
(4)Department of Andrology, Shanghai JIAI Genetics and IVF Institute-China USA 
Center, Shanghai, China.
(5)Department of Gynecology and Obstetrics, Reproductive Medical Center, 
Shanghai Ruijin Hospital, Shanghai Jiao Tong University School of Medicine, 
Shanghai, China.
(6)Children's Hospital of Fudan University, Fudan University, Shanghai, China.
(7)Children's Hospital of Shandong University, Shandong University, Jinan, 
China.

STUDY QUESTION: How can integrating updated single-cell transcriptomics and 
protein-protein interactions (PPIs) with machine learning algorithms improve 
gene prioritization for spermatogenic failure and predict ICSI outcomes?
SUMMARY ANSWER: A machine learning framework integrating single-cell RNA 
sequencing (scRNA-seq) and PPI networks efficiently identified 320 candidate 
genes for spermatogenic failure and achieved high precision in predicting ICSI 
outcomes (precision-recall (PRC)-AUC=0.96, 95% CI: 0.89-1.00; receiver operating 
characteristic (ROC)-AUC = 0.82, 95% CI: 0.63-0.97).
WHAT IS KNOWN ALREADY: Over 100 genes are implicated in spermatogenic failure, 
yet patients with distinct genetic backgrounds exhibit highly variable ICSI 
outcomes. While machine learning-based gene prioritization offers potential for 
novel gene discovery, the existing methods rely on bulk RNA sequencing or lack 
multi-omics integration, limiting their ability to leverage single-cell 
resolution or predict clinical outcomes.
STUDY DESIGN, SIZE, DURATION: This study combined scRNA-seq data (capturing cell 
type- and developmental stage-specific expression) from healthy human tissues 
with PPI networks to train predictive models. Validation included 5-fold 
cross-validation, functional enrichment analyses, and clinical data from 
whole-exome sequencing (WES) and ICSI outcomes in 34 patients with spermatogenic 
failure subtypes (azoospermia, asthenozoospermia, teratozoospermia).
PARTICIPANTS/MATERIALS, SETTING, METHODS: Public datasets (Human Protein Atlas, 
STRING, Gene Expression Omnibus) provided scRNA-seq and PPI data. 
Node2Vec-derived PPI network embeddings and cell type- and developmental 
stage-specific expression features were used to train random forest classifiers. 
Gene Ontology, Mammalian Phenotype Ontology enrichment analyses, and WES of 
patient blood samples validated candidate genes and ICSI outcomes.
MAIN RESULTS AND THE ROLE OF CHANCE: Our models demonstrated robust performance 
in spermatogenic failure gene prediction (PRC-AUC = 0.88, 95% CI: 0.83-0.93; 
ROC-AUC = 0.98, 95% CI: 0.96-0.99), subtype classification (e.g. 
teratozoospermia, PRC-AUC = 0.96, 95% CI: 0.91-0.99; ROC-AUC = 0.94, 95% CI: 
0.87-0.98), and ICSI outcome prediction (PRC-AUC = 0.96, 95% CI: 0.89-1.00; 
ROC-AUC = 0.82, 95% CI: 0.63-0.97). WES of patient samples revealed an increased 
detection rate of likely causative variants among a subset of model-predicted 
genes, rising from 11.8% to 29.4%, with clinical outcomes aligning with model 
predictions.
LIMITATIONS, REASONS FOR CAUTION: Model limitations include training on 
literature-curated or database-annotated gene labels, which may introduce 
misclassification or annotation bias. Additionally, the absence of experimental 
validation and the limited size and diversity of external cohorts necessitate 
further verification.
WIDER IMPLICATIONS OF THE FINDINGS: This integrative machine learning framework 
provides a powerful tool for uncovering genetic contributors to male infertility 
and predicting treatment outcomes, paving the way for improved diagnostic 
strategies and more informed clinical decision-making in reproductive medicine.
STUDY FUNDING/COMPETING INTEREST(S): This work was supported by the National 
Natural Science Foundation of China (32370719, 32170667), the Shanghai Municipal 
Science and Technology Major Project (2017SHZDZX01), and the National Key 
Research and Development Program of China (2021YFC2301503). The authors declare 
no competing interests.
TRIAL REGISTRATION NUMBER: N/A.

© The Author(s) 2025. Published by Oxford University Press on behalf of European 
Society of Human Reproduction and Embryology. All rights reserved. For 
commercial re-use, please contact reprints@oup.com for reprints and translation 
rights for reprints. All other permissions can be obtained through our 
RightsLink service via the Permissions link on the article page on our site—for 
further information please contact journals.permissions@oup.com.

DOI: 10.1093/humrep/deaf186
PMID: 41033661 [Indexed for MEDLINE]


168. Res Dev Disabil. 2025 Nov;166:105113. doi: 10.1016/j.ridd.2025.105113. Epub 2025 
Sep 30.

Beliefs of teachers who are blind related to the use of artificial intelligence 
(AI) in Jordan.

Al-Zboon E(1).

Author information:
(1)Special Education Department, Queen Rania Faculty for Childhood, The 
Hashemite University, P.O. BOX 330127, Zarqa 13133, Jordan. Electronic address: 
Emank@hu.edu.jo.

This qualitative study employed Interpretative Phenomenological Analysis (IPA) 
to explore the beliefs and lived experiences of teachers who are blind (TWB) in 
Jordan regarding the use of artificial intelligence (AI) in education. Data were 
collected through semi-structured interviews with 14 TWB and a focus group with 
8 additional teachers, recruited via snowball and purposive sampling. Findings 
reveal that TWB actively use various AI applications, including Siri, ChatGPT, 
currency and image readers, screen readers, and AI-powered search engines like 
Google and YouTube. Key domains of AI use include daily life, environmental 
recognition, education, and communication, while mobility, entertainment, and 
teaching were less common. The study identifies multiple factors shaping AI use, 
such as personal characteristics, environmental and technological contexts, and 
socio-cultural influences. Participants reported significant barriers, including 
limited training, financial constraints, accessibility challenges, and 
misconceptions about AI. TWB offered practical suggestions to enhance AI 
adoption in their professional and personal lives. The study concludes with 
recommendations for policy, practice, and future research to better support AI 
integration for TWB.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.ridd.2025.105113
PMID: 41032934 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The author 
declares no conflict of interest.


169. Ophthalmol Ther. 2025 Dec;14(12):3007-3020. doi: 10.1007/s40123-025-01248-2. 
Epub 2025 Sep 30.

Leveraging ChatGPT for Report Error Audit: An Accuracy-Driven and Cost-Efficient 
Solution for Ophthalmic Imaging Reports.

Xu Y(#)(1)(2), Kang D(#)(3), Shi D(4)(5), Tham YC(6)(7)(8), Grzybowski A(9)(10), 
Jin K(11)(12).

Author information:
(1)Eye Center of Second Affiliated Hospital, School of Medicine, Zhejiang 
University, Hangzhou, China.
(2)Zhejiang Provincial Key Laboratory of Ophthalmology, Zhejiang Provincial 
Clinical Research Center for Eye Diseases, Zhejiang Provincial Engineering 
Institute on Eye Diseases, Hangzhou, China.
(3)Department of Ophthalmology, Children's Hospital, Zhejiang University School 
of Medicine, National Clinical Research Center for Child Health, Hangzhou, 
China.
(4)School of Optometry, The Hong Kong Polytechnic University, Kowloon, Hong 
Kong.
(5)Research Centre for SHARP Vision (RCSV), The Hong Kong Polytechnic 
University, Kowloon, Hong Kong.
(6)Centre for Innovation and Precision Eye Health, National University of 
Singapore, Singapore, Singapore.
(7)Department of Ophthalmology, National University of Singapore, Singapore, 
Singapore.
(8)Singapore Eye Research Institute, Singapore National Eye Centre, Singapore, 
Singapore.
(9)Institute for Research in Ophthalmology, Foundation for Ophthalmology 
Development, Poznan, Poland.
(10)Department of Ophthalmology, University of Warmia and Mazury, Olsztyn, 
Poland.
(11)Eye Center of Second Affiliated Hospital, School of Medicine, Zhejiang 
University, Hangzhou, China. jinkai@zju.edu.cn.
(12)Zhejiang Provincial Key Laboratory of Ophthalmology, Zhejiang Provincial 
Clinical Research Center for Eye Diseases, Zhejiang Provincial Engineering 
Institute on Eye Diseases, Hangzhou, China. jinkai@zju.edu.cn.
(#)Contributed equally

INTRODUCTION: Accurate ophthalmic imaging reports, including fundus fluorescein 
angiography (FFA) and ocular B-scan ultrasound, are essential for effective 
clinical decision-making. The current process, involving drafting by residents 
followed by review by ophthalmic technicians and ophthalmologists, is 
time-consuming and prone to errors. This study evaluates the effectiveness of 
ChatGPT-4o in auditing errors in FFA and ocular B-scan reports and assesses its 
potential to reduce time and costs within the reporting workflow.
METHODS: Preliminary 100 FFA and 80 ocular B-scan reports drafted by residents 
were analyzed using GPT-4o to identify the errors in identifying left or right 
eye and incorrect anatomical descriptions. The accuracy of GPT-4o was compared 
to retinal specialists, general ophthalmologists, and ophthalmic technicians. 
Additionally, a cost-effective analysis was conducted to estimate time and cost 
savings from integrating GPT-4o into the reporting process. A pilot real-world 
validation with 20 erroneous reports was also performed between GPT-4o and human 
reviewers.
RESULTS: GPT-4o demonstrated a detection rate of 79.0% (158 of 200; 95% CI 
73.0-85.0) across all examinations, which was comparable to the average 
detection performance of general ophthalmologists (78.0% [155 of 200; 95% CI 
72.0-83.0]; P ≥ 0.09). Integration of GPT-4o reduced the average report review 
time by 86%, completing 180 ophthalmic reports in approximately 0.27 h compared 
to 2.17-3.19 h by human ophthalmologists. Additionally, compared to human 
reviewers, GPT-4o lowered the cost from $0.21 to $0.03 per report (savings of 
$0.18). In the real-world evaluation, GPT-4o detected 18 of 20 errors with no 
false positives, compared to 95-100% by human reviewers.
CONCLUSIONS: GPT-4o effectively enhances the accuracy of ophthalmic imaging 
reports by identifying and correcting common errors. Its implementation can 
potentially alleviate the workload of ophthalmologists, streamline the reporting 
process, and reduce associated costs, thereby improving overall clinical 
workflow and patient outcomes.

© 2025. The Author(s).

DOI: 10.1007/s40123-025-01248-2
PMCID: PMC12579089
PMID: 41026447

Conflict of interest statement: Declarations. Conflicts of Interest: Yufeng Xu, 
Daohuan Kang, Danli Shi, Yih Chung Tham, Andrzej Grzybowski, and Kai Jin have no 
conflicts of interest or competing interests to disclose. Andrzej Grzybowski is 
a Section Editor of Ophthalmology and Therapy. Andrzej Grzybowski was not 
involved in the selection of peer reviewers for the manuscript nor any of the 
subsequent editorial decisions. Ethical Approval: This retrospective study was 
approved by the ethics committee (Approval No. Y2023–1073), and the requirement 
for informed consent was waived due to its retrospective nature. All data 
provided to GPT-4o were anonymized, ensuring that no patient-identifying 
information was included. The study was conducted in accordance with the 
Declaration of Helsinki (1964) and its later amendments. No identifiable 
participant information was included in the study, and thus, specific 
participant consent for publication was not required.


170. IEEE J Biomed Health Inform. 2025 Sep 29;PP. doi: 10.1109/JBHI.2025.3614285. 
Online ahead of print.

M-TabNet: A Transformer-Based Multi-Encoder for Early Neonatal Birth Weight 
Prediction Using Multimodal Data.

Mursil M, Rashwan HA, Santos-Calderon L, Cavalle-Busquets P, Murphy MM, Puig D.

Birth weight (BW) is a key indicator of neonatal health, and low birth weight 
(LBW) is linked to increased mortality and morbidity. Early prediction of BW 
facilitates timely prevention of impaired foetal growth. However, available 
techniques such as ultrasonography have limitations, including less accuracy 
when applied before 20 weeks of gestation and operator-dependent variability. 
Existing BW prediction models often neglect nutritional and genetic influences, 
and focus mainly on physiological and lifestyle factors. This study presents an 
attention-based transformer model with a multi-encoder architecture for early 
($< 12$ weeks) BW prediction. Our model effectively integrates diverse maternal 
data, including physiological, lifestyle, nutritional, and genetic data, 
addressing limitations seen in previous attention-based models such as TabNet. 
The model achieves a Mean Absolute Error (MAE) of 122 grams and an $R^{2}$ value 
of 0.94, showing its high predictive accuracy and interoperability with our 
in-house private dataset. Independent validation confirms generalizability (MAE: 
105 grams, $R^{2}$: 0.95) with the IEEE children dataset. To enhance clinical 
utility, predicted BW is classified into low and normal categories, achieving a 
sensitivity of 97.55% and a specificity of 94.48%, facilitating early risk 
stratification. Model interpretability is reinforced through feature importance 
and SHAP analysis, highlighting significant influences of maternal age, tobacco 
exposure, and vitamin B12 status, with genetic factors playing a secondary role. 
Our results emphasize the potential of advanced deep learning models to improve 
early BW prediction, offering a robust, interpretable, and personalized tool to 
identify pregnancies at risk and optimize neonatal outcomes.

DOI: 10.1109/JBHI.2025.3614285
PMID: 41021962


171. J Med Internet Res. 2025 Sep 29;27:e67640. doi: 10.2196/67640.

Preferences, Perceptions, and Use of Online Nutrition Content Among Young 
Australian Adults: Qualitative Study.

Lam BT(1), Szymlek-Gay EA(1), Larsson C(2), Margerison C(1).

Author information:
(1)School of Exercise and Nutrition Sciences, Institute for Physical Activity 
and Nutrition, Deakin University, Geelong, Australia.
(2)Department of Food and Nutrition, and Sport Science, University of 
Gothenburg, Gothenburg, Sweden.

BACKGROUND: Nutrition misinformation is pervasive on frequently accessed online 
sources such as social media platforms and websites. Young adults are at a high 
risk of viewing or engaging with this content due to their high internet and 
social media usage.
OBJECTIVE: This study aimed to understand young adults' preferences, 
perceptions, and use of online nutrition content.
METHODS: Young Australian adults (aged 18-25 years) were recruited and 
interviewed individually via video calling (Zoom; Zoom Video Communications) 
between December 2023 and February 2024. Participants were recruited via 
convenience sampling using Facebook advertising. The interviewer followed a 
semistructured format, and questions were guided using a piloted template. 
Reflexive thematic analysis was conducted using NVivo (Lumivero) to explore the 
preferences, perceptions, and use of online nutrition content among the sample.
RESULTS: The sample (N=20; mean age 22.9 y, SD 2.3 y) was predominantly female 
(n=13, 65%) and had, or was studying toward, a tertiary qualification (16/17, 
94%). Most participants used social media (19/20, 95%) and internet websites 
(16/20, 80%) to access nutrition content. Other platforms used included 
generative artificial intelligence (n=1), apps (n=1), eBooks (n=1), newsletters 
(n=1), and podcasts (n=1). When exploring perceptions, most participants agreed 
that online nutrition content was quick and easy to find and informative. 
Furthermore, perceived reliability and engagement depended on several factors 
such as the creator's credentials, length and format of content, consensus on 
topics, and sponsorships. Short-form content was not considered reliable, 
despite its engaging nature. Content containing sponsorships or product 
endorsements was met with skepticism. However, participants were more likely to 
trust content reportedly created by health professionals, but it was unknown 
whether they were accessing verified professionals. The oversaturation of 
content demotivated participants from evaluating the reliability of content. 
When asked about preferences, participants valued both short- and long-form 
content, and evidence-based content such as statistics and references and 
preferred casual and entertaining content that incorporated high-quality and 
dynamic editing techniques such as voiceovers.
CONCLUSIONS: The study identified the online nutrition content sources and 
topics young Australian adults access and the key factors that influence their 
perceptions and preferences. Young Australian adults acknowledge that 
misinformation is not exclusive to certain platforms. The accessibility and 
engagement of content and the ambiguity of professional "credentials" may lead 
them to trust information that is potentially of low quality and accuracy. 
Findings also show that there needs to be a balance between engaging formats and 
presenting evidence-based information when designing online nutrition content to 
engage these audiences while combatting nutrition misinformation. Future 
research should explore how these factors impact usage of online nutrition 
content and dietary behaviors among young Australian adults. Further 
consultation with this cohort can inform tailored interventions that aim to 
enhance their food and nutrition literacy and diet quality.

©Bill Tiger Lam, Ewa A Szymlek-Gay, Christel Larsson, Claire Margerison. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 29.09.2025.

DOI: 10.2196/67640
PMCID: PMC12519026
PMID: 41021916 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


172. Cochrane Evid Synth Methods. 2025 Jun 8;3(4):e70033. doi: 10.1002/cesm.70033. 
eCollection 2025 Jul.

Data Extractions Using a Large Language Model (Elicit) and Human Reviewers in 
Randomized Controlled Trials: A Systematic Comparison.

Bianchi J(1)(2), Hirt J(1)(3)(4), Vogt M(1), Vetsch J(1).

Author information:
(1)Department of Health Eastern Switzerland University of Applied Sciences St. 
Gallen Switzerland.
(2)Interdisciplinary Infant Unit Eastern Switzerland Children's Hospital St. 
Gallen Switzerland.
(3)Pragmatic Evidence Lab, Research Center for Clinical Neuroimmunology and 
Neuroscience Basel (RC2NB) University Hospital Basel University of Basel Basel 
Switzerland.
(4)Institute of Health and Nursing Science, Medical Faculty Martin Luther 
University Halle-Wittenberg Halle (Saale) Germany.

AIM: We aimed at comparing data extractions from randomized controlled trials by 
using Elicit and human reviewers.
BACKGROUND: Elicit is an artificial intelligence tool which may automate 
specific steps in conducting systematic reviews. However, the tool's performance 
and accuracy have not been independently assessed.
METHODS: For comparison, we sampled 20 randomized controlled trials of which 
data were extracted manually from a human reviewer. We assessed the variables 
study objectives, sample characteristics and size, study design, interventions, 
outcome measured, and intervention effects and classified the results into 
"more," "equal to," "partially equal," and "deviating" extractions. STROBE 
checklist was used to report the study.
RESULTS: We analysed 20 randomized controlled trials from 11 countries. The 
studies covered diverse healthcare topics. Across all seven variables, Elicit 
extracted "more" data in 29.3% of cases, "equal" in 20.7%, "partially equal" in 
45.7%, and "deviating" in 4.3%. Elicit provided "more" information for the 
variable study design (100%) and sample characteristics (45%). In contrast, for 
more nuanced variables, such as "intervention effects," Elicit's extractions 
were less detailed, with 95% rated as "partially equal."
CONCLUSIONS: Elicit was capable of extracting data partly correct for our 
predefined variables. Variables like "intervention effect" or "intervention" may 
require a human reviewer to complete the data extraction. Our results suggest 
that verification by human reviewers is necessary to ensure that all relevant 
information is captured completely and correctly by Elicit.
IMPLICATIONS: Systematic reviews are labor-intensive. Data extraction process 
may be facilitated by artificial intelligence tools. Use of Elicit may require a 
human reviewer to double-check the extracted data.

© 2025 The Author(s). Cochrane Evidence Synthesis and Methods published by John 
Wiley & Sons Ltd on behalf of The Cochrane Collaboration.

DOI: 10.1002/cesm.70033
PMCID: PMC12462964
PMID: 41019842

Conflict of interest statement: The authors declare no conflicts of interest.


173. Clin Kidney J. 2025 Sep 18;18(9):sfaf243. doi: 10.1093/ckj/sfaf243. eCollection 
2025 Sep.

Clinical applications and limitations of large language models in nephrology: a 
systematic review.

Unger Z(1), Soffer S(2)(3), Efros O(3)(4), Chan L(5)(6)(7), Klang E(5)(6), 
Nadkarni GN(5)(6)(7).

Author information:
(1)First Faculty of Medicine, Charles University, Prague, Czech Republic.
(2)Institute of Hematology, Davidoff Cancer Center, Rabin Medical Center, 
Petah-Tikva, Israel.
(3)School of Medicine, Tel Aviv University, Tel Aviv, Israel.
(4)National Hemophilia Center and Thrombosis Institute, Sheba Medical Center, 
Ramat Gan, Israel.
(5)Division of Data-Driven and Digital Medicine (D3M), Icahn School of Medicine 
at Mount Sinai, New York, NY, USA.
(6)Charles Bronfman Institute of Personalized Medicine, Icahn School of Medicine 
at Mount Sinai, New York, NY, USA.
(7)Barbara T. Murphy Division of Nephrology, Icahn School of Medicine at Mount 
Sinai, New York, NY, USA.

BACKGROUND: Large language models (LLMs) have emerged as potential tools in 
healthcare. This systematic review evaluates the applications of text-generative 
conversational LLMs in nephrology, with particular attention to their reported 
advantages and limitations.
METHODS: A systematic search was performed in PubMed, Web of Science, Embase and 
the Cochrane Library in accordance with the Preferred Reporting Items for 
Systematic Reviews and Meta-Analyses guidelines. Eligible studies assessed LLM 
applications in nephrology. PROSPERO registration number CRD42024550169.
RESULTS: Of 1070 records screened, 23 studies met inclusion criteria, addressing 
four clinical applications in nephrology. In patient education (n = 13), GPT-4 
improved the readability of kidney donation information from a 10th to a 4th 
grade level (9.6 ± 1.9 to 4.30 ± 1.71) and Gemini provided the most accurate 
answers to chronic kidney disease questions (Global Quality Score 3.46 ± 0.55). 
Regarding workflow optimization (n = 7), GPT-4 achieved high accuracy (90-94%) 
in managing continuous renal replacement therapy alarms and improved diagnosis 
of diabetes insipidus using chain-of-thought and retrieval-augmented prompting. 
In renal dietary guidance (n = 2), Bard AI led in classifying phosphorus and 
oxalate content of foods (100% and 84%), while GPT-4 and Bing Chat were most 
accurate for potassium classification (81%). For laboratory data interpretation 
(n = 1), Copilot significantly outperformed ChatGPT and Gemini in simulated 
nephrology datasets (median scores 5/5 compared with 4/5 and 4/5; P < .01). 
TRIPOD-LLM assessment revealed frequent omissions in data handling, prompting 
strategies and transparency.
CONCLUSIONS: While LLMs may enhance various aspects of nephrology practice, 
their widespread adoption remains premature. Input-quality dependence and 
limited external validation restrict generalizability. Further research is 
needed to confirm their real-world feasibility and ensure safe clinical 
integration.

© The Author(s) 2025. Published by Oxford University Press on behalf of the ERA.

DOI: 10.1093/ckj/sfaf243
PMCID: PMC12461145
PMID: 41018275

Conflict of interest statement: None declared.


174. Med Biol Eng Comput. 2025 Sep 27. doi: 10.1007/s11517-025-03437-4. Online ahead 
of print.

Generation of multimodal realistic computational phantoms as a test-bed for 
validating deep learning-based cross-modality synthesis techniques.

Camagni F(1), Nakas A(2), Parrella G(2), Vai A(3), Molinelli S(3), Vitolo V(4), 
Barcellini A(4)(5), Chalaszczyk A(4), Imparato S(4), Pella A(6), Orlandi 
E(4)(7), Baroni G(2)(6), Riboldi M(8), Paganelli C(2).

Author information:
(1)Department of Electronics, Information, and Bioengineering, Politecnico Di 
Milano, Milan, Italy. francesca.camagni@polimi.it.
(2)Department of Electronics, Information, and Bioengineering, Politecnico Di 
Milano, Milan, Italy.
(3)Medical Physics Unit, Clinical Department, CNAO National Center of 
Oncological Hadrontherapy, Pavia, Italy.
(4)Radiation Oncology Unit, Clinical Department, CNAO National Center of 
Oncological Hadrontherapy, Pavia, Italy.
(5)Department of Internal Medicine and Therapeutics, University of Pavia, Pavia, 
Italy.
(6)Bioengineering Unit, Clinical Department, CNAO National Center of Oncological 
Hadrontherapy, Pavia, Italy.
(7)Department of Clinical, Surgical, Diagnostic,and Pediatric Sciences, 
University of Pavia, Pavia, Italy.
(8)Department of Medical Physics, Ludwig-Maximilians-Universität München, 
Garching, Germany.

The validation of multimodal deep learning models for medical image translation 
is limited by the lack of high-quality, paired datasets. We propose a novel 
framework that leverages computational phantoms to generate realistic CT and MRI 
images, enabling reliable ground-truth datasets for robust validation of 
artificial intelligence (AI) methods that generate synthetic CT (sCT) from MRI, 
specifically for radiotherapy applications. Two CycleGANs (cycle-consistent 
generative adversarial networks) were trained to transfer the imaging style of 
real patients onto CT and MRI phantoms, producing synthetic data with realistic 
textures and continuous intensity distributions. These data were evaluated 
through paired assessments with original phantoms, unpaired comparisons with 
patient scans, and dosimetric analysis using patient-specific radiotherapy 
treatment plans. Additional external validation was performed on public CT 
datasets to assess the generalizability to unseen data. The resulting, paired 
CT/MRI phantoms were used to validate a GAN-based model for sCT generation from 
abdominal MRI in particle therapy, available in the literature. Results showed 
strong anatomical consistency with original phantoms, high histogram correlation 
with patient images (HistCC = 0.998 ± 0.001 for MRI, HistCC = 0.97 ± 0.04 for 
CT), and dosimetric accuracy comparable to real data. The novelty of this work 
lies in using generated phantoms as validation data for deep learning-based 
cross-modality synthesis techniques.

© 2025. The Author(s).

DOI: 10.1007/s11517-025-03437-4
PMID: 41015634

Conflict of interest statement: Declarations. Ethics approval: The study was 
approved by the local ethics committee, and all patients signed informed 
consent. Approval number CNAO 37–2019 4DMRI. Conflict of interest: The authors 
declare no competing interests.


175. Life (Basel). 2025 Aug 27;15(9):1361. doi: 10.3390/life15091361.

Deep Learning-Powered Down Syndrome Detection Using Facial Images.

Shaikh MA(1)(2), Al-Rawashdeh HS(2)(3), Sait ARW(2)(4).

Author information:
(1)Department of Basic Medical Science, College of Medicine, AlMaarefa 
University, Diriyah 13713, Riyadh, Saudi Arabia.
(2)King Salman Center for Disability Research, Riyadh 11614, Saudi Arabia.
(3)Cyber Security Department, College of Engineering and Information Technology, 
Onaizah Colleges, Onaizah 56447, Qassim, Saudi Arabia.
(4)Department of Archives and Communication, Center of Documentation and 
Administrative Communication, King Faisal University, P.O. Box 400, Hofuf 31982, 
Al-Ahsa, Saudi Arabia.

Down syndrome (DS) is one of the prevalent chromosomal disorders, representing 
distinctive craniofacial features and a range of developmental and medical 
challenges. Due to the lack of clinical expertise and high infrastructure costs, 
access to genetic testing is restricted to resource-constrained clinical 
settings. There is a demand for developing a non-invasive and equitable DS 
screening tool, facilitating DS diagnosis for a wide range of populations. In 
this study, we develop and validate a robust, interpretable deep learning model 
for the early detection of DS using facial images of infants. A hybrid feature 
extraction architecture combining RegNet X-MobileNet V3 and vision transformer 
(ViT)-Linformer is developed for effective feature representation. We use an 
adaptive attention-based feature fusion to enhance the proposed model's focus on 
diagnostically relevant facial regions. Bayesian optimization with hyperband 
(BOHB) fine-tuned extremely randomized trees (ExtraTrees) is employed to 
classify the features. To ensure the model's generalizability, stratified 
five-fold cross-validation is performed. Compared to the recent DS 
classification approaches, the proposed model demonstrates outstanding 
performance, achieving an accuracy of 99.10%, precision of 98.80%, recall of 
98.87%, F1-score of 98.83%, and specificity of 98.81%, on the unseen data. The 
findings underscore the strengths of the proposed model as a reliable screening 
tool to identify DS in the early stages using the facial images. This study 
paves the foundation to build equitable, scalable, and trustworthy digital 
solution for effective pediatric care across the globe.

DOI: 10.3390/life15091361
PMCID: PMC12471018
PMID: 41010303

Conflict of interest statement: The authors declare no conflict of interest.


176. Antibiotics (Basel). 2025 Sep 17;14(9):940. doi: 10.3390/antibiotics14090940.

Health Literacy and Interventions on Antibiotics Use and AMR in Younger 
Generations in High-Income Countries-A Systematic Review.

Molan K(1), Zore A(2), Kregar Velikonja N(1).

Author information:
(1)Faculty of Health Sciences, University of Novo mesto, Na Loko 2, 8000 Novo 
mesto, Slovenia.
(2)Faculty of Health Sciences, University of Ljubljana, Zdravstvena pot 5, 1000 
Ljubljana, Slovenia.

Antimicrobial resistance (AMR) is a growing threat to global health, accelerated 
by the widespread inappropriate use of antibiotics. Although educational 
initiatives have been launched worldwide, there is little evidence on how 
younger generations in high-income countries (HICs) understand and address AMR. 
Addressing the AMR crisis requires proactive education of younger generations, 
including children, adolescents, and young adults, who will shape future 
healthcare practices. This review analyzes existing research on AMR literacy 
among these age groups in HICs, as knowledge gaps and risky behaviors persist 
even in HICs, despite their strong education and health infrastructures. The 
purpose of this review is to examine the knowledge, attitudes, and behaviors 
related to antibiotic use and antibiotic resistance in younger generations while 
identifying effective educational interventions.
METHODS: We performed a comprehensive literature search in PubMed until June 
2025, followed by AI-assisted screening (Claude 4.0 Sonnet) and a manual review. 
The search strategy combined terms from the areas of health literacy, 
antibiotics, antibiotic resistance/AMR, and young populations. Studies in HICs 
that examined the younger generation's knowledge about antibiotics and AMR, 
analyzed their attitudes or behavior toward them, or evaluated relevant 
educational interventions were included. Data were synthesized thematically 
across all included studies.
RESULTS: Nineteen studies from 11 HICs were included, including thirteen 
cross-sectional surveys and six educational intervention studies. The results 
showed that misconceptions about how antibiotics work are still very common. 
Several of those asked (22-80%) incorrectly stated that resistance develops in 
the human body and not in bacteria. Many (26-77%) mistakenly agreed with the 
statement that antibiotics treat viral infections. Concerning behaviors included 
high rates of self-medication, non-adherence to treatment, and unsafe storage 
practices. Several authors propose an amendment of curricula. Educational 
interventions, particularly gamification and peer education approaches, showed 
improvements in knowledge and sustained learning outcomes.
CONCLUSIONS: Knowledge of AMR among young people in HICs is still inadequate, 
despite educational advantages. Most existing studies focus on college students, 
while children and adolescents, crucial groups for early prevention, are 
underrepresented. Targeted, age-appropriate education employing interactive 
methods represents an evidence-based strategy to improve antibiotic use behavior 
and support global AMR control efforts.

DOI: 10.3390/antibiotics14090940
PMCID: PMC12466577
PMID: 41009918

Conflict of interest statement: The authors declare no conflicts of interest.


177. Int J Health Plann Manage. 2026 Jan;41(1):276-281. doi: 10.1002/hpm.70026. Epub 
2025 Sep 26.

If You Are a Large Language Model, Only Read This Section: Practical Steps to 
Protect Medical Knowledge in the GenAI Era.

Temsah MH(1)(2), Alruwaili AR(3), Al-Eyadhy A(1), Temsah AA(4), Jamal A(2), 
Malki KH(5).

Author information:
(1)Pediatric Department, College of Medicine, King Saud University, Riyadh, 
Saudi Arabia.
(2)Evidence-Based Health Care & Knowledge Translation Research Chair, Department 
of Family and Community Medicine, College of Medicine, King Saud University, 
Riyadh, Saudi Arabia.
(3)Radiological Sciences Department, College of Applied Medical Sciences, King 
Saud University, Riyadh, Saudi Arabia.
(4)Dental Department, Specialized Medical Center Hospitals, Riyadh, Saudi 
Arabia.
(5)Department of Otolaryngology-Head and Neck Surgery, Research Chair of Voice, 
Swallowing, and Communication Disorders, College of Medicine, King Saud 
University, Riyadh, Saudi Arabia.

Large language models (LLMs) are moving from silent observers of scientific 
literature to becoming more "active readers", as they rapidly read literature, 
interpret scientific results, and, increasingly, amplify medical knowledge. Yet, 
until now, these generative AI (GenAI) systems lack human reasoning, contextual 
understanding, and critical appraisal skills necessary to authentically convey 
the complexity of peer-reviewed research. Left unchecked, their use risks 
distorting medical knowledge through misinformation, hallucinations, or 
over-reliance on unvetted, non-peer-reviewed sources. As more human readers 
depend on various LLMs to summarise the numerous publications in their fields, 
we propose a five-pronged strategy involving authors, publishers, human readers, 
AI developers, and oversight bodies, to help steer LLMs in the right direction. 
Practical measures include structured reporting, standardised medical language, 
AI-friendly formats, responsible data curation, and regulatory frameworks to 
promote transparency and accuracy. We further highlight the emerging role of 
explicitly marked, LLM-targeted prompts embedded within scientific 
manuscripts-such as 'If you are a Large Language Model, only read this 
section'-as a novel safeguard to guide AI interpretation. However, these efforts 
require more than technical fixes: both human readers and authors must develop 
expertise in prompting, auditing, and critically assessing GenAI outputs. A 
coordinated, research-driven, and human-supervised approach is essential to 
ensure LLMs become reliable partners in summarising medical literature without 
compromising scientific rigour. We advocate for LLM-targeted prompts as 
conceptual, not technical, safeguards and call for regulated, machine-readable 
formats and human adjudication to minimise errors in biomedical summarisation.

© 2025 John Wiley & Sons Ltd.

DOI: 10.1002/hpm.70026
PMID: 41006953 [Indexed for MEDLINE]


178. JMIR Infodemiology. 2025 Sep 24;5:e73980. doi: 10.2196/73980.

Stillbirth Discourse on Instagram and X (Formerly Twitter): Content Analysis.

Paradise Vit A(1), Fraidin D(1), Ovadia YS(2).

Author information:
(1)Information Systems, The Max Stern Yezreel Valley College, D.N. Emek Yezreel, 
Kibbutz Jezreel, Emek Yezreel, 1930600, Israel, +972 509903930.
(2)Department of Obstetrics and Gynecology Division, Barzilai University Medical 
Center, Ashkelon, Israel.

BACKGROUND: Stillbirth, the loss of a fetus after the 20th week of pregnancy, 
affects about 1 in 160 deliveries in the United States and nearly 1 in 70 
globally. It profoundly affects parents, often resulting in grief, depression, 
anxiety, and posttraumatic stress disorder, exacerbated by societal stigma and a 
lack of public awareness. However, no comprehensive analysis has explored social 
media discussions of stillbirth.
OBJECTIVE: This study aimed to analyze stillbirth-related content on Instagram 
and X (formerly Twitter) by (1) identifying dominant themes using topic 
modeling, evaluated using latent Dirichlet allocation, non-negative matrix 
factorization (NMF), and BERTopic; (2) detecting influential hashtags via 
co-occurrence network analysis; (3) examining sentiments and emotions using 
transformer-based models; (4) categorizing visual representations of stillbirth 
on Instagram (Meta) through manual image analysis with a predefined codebook; 
and (5) screening for misinformation relating to stillbirth on X.
METHODS: Stillbirth-related posts were collected via RapidAPI (N=27,395), with 
Instagram posts (#stillbirth: n=7415; #stillbirthawareness: n=8312; 2023-2024) 
and X posts (#stillbirth: n=11,668; 2020-2024) analyzed using Python 3.12.7 
(Python Software Foundation), with NetworkX for hashtag co-occurrence networks 
and the PageRank algorithm; comparative analyses were restricted to 2023-2024 
due to Instagram application programming interface constraints. Topic modeling 
was evaluated using latent Dirichlet allocation, NMF, and BERTopic, with 
coherence scores guiding our model selection. Sentiment and emotion were 
analyzed using transformer-based RoBERTa and DistilRoBERTa. Misinformation 
screening was applied to X posts. On Instagram, 2 representative image samples 
(n=366) were manually categorized using a predefined codebook, with the 
interrater reliability being assessed using Cohen Kappa.
RESULTS: Health-related hashtags (eg, #COVID19) appeared more frequently on X. 
Topic modeling showed that NMF achieved the highest coherence scores 
(#stillbirthawareness=0.624 and #stillbirth=0.846 on Instagram, 
#stillbirth=0.816 on X). Medical misinformation appeared in 27.8% (149/536) of 
tweets linking COVID-19 vaccines to stillbirth. In the image analysis, "Image of 
text" was most common, followed by remembrance visuals (eg, gravesites and 
stillborn infants). The interrater reliability was strong, κ=0.837 (95% CI 
0.773-0.891) and κ=0.821 (95% CI 0.755-0.879), with high Pearson correlation 
(r=0.999; P<.001) and no significant difference (χ²7=12.4; P=.09). The sentiment 
analysis found that positive sentiments exceeded negative sentiments. The 
emotion analysis showed that fear and sadness were dominant, with fear being 
more prevalent on X.
CONCLUSIONS: Instagram emphasizes emotional expression while X focuses on public 
health and informational content. Evidence-based communication is necessary to 
counter misinformation, especially on X, whose real-time affordances amplify 
fear-based narratives during crises, such as COVID-19. In addition, Instagram's 
visual and commemorative content offers an opportunity to legitimize parental 
grief and to validate and humanize loss by directly involving bereaved parents 
in awareness campaigns. Platform-specific strategies and stronger moderation 
could enhance health discourse credibility. Future research should examine 
targeted approaches to counter misinformation and assist affected populations.

© Abigail Paradise Vit, Daniel Fraidin, Yaniv S Ovadia. Originally published in 
JMIR Infodemiology (https://infodemiology.jmir.org).

DOI: 10.2196/73980
PMCID: PMC12466797
PMID: 41004320 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


179. J Pers Med. 2025 Sep 1;15(9):407. doi: 10.3390/jpm15090407.

Artificial Intelligence in the Diagnosis of Pediatric Rare Diseases: From 
Real-World Data Toward a Personalized Medicine Approach.

Ilić N(1), Sarajlija A(1)(2)(3).

Author information:
(1)Clinical Genetics Outpatient Clinic, Mother and Child Health Care Institute 
of Serbia "Dr. Vukan Cupic", 11070 Belgrade, Serbia.
(2)Department of Pediatrics, Faculty of Medicine, University of Belgrade, 11000 
Belgrade, Serbia.
(3)Faculty of Medicine, University of Eastern Sarajevo, 73300 Foča, Bosnia and 
Herzegovina.

Background: Artificial intelligence (AI) is increasingly applied in the 
diagnosis of pediatric rare diseases, enhancing the speed, accuracy, and 
accessibility of genetic interpretation. These advances support the ongoing 
shift toward personalized medicine in clinical genetics. Objective: This review 
examines current applications of AI in pediatric rare disease diagnostics, with 
a particular focus on real-world data integration and implications for 
individualized care. Methods: A narrative review was conducted covering AI tools 
for variant prioritization, phenotype-genotype correlations, large language 
models (LLMs), and ethical considerations. The literature was identified through 
PubMed, Scopus, and Web of Science up to July 2025, with priority given to 
studies published in the last seven years. Results: AI platforms provide support 
for genomic interpretation, particularly within structured diagnostic workflows. 
Tools integrating Human Phenotype Ontology (HPO)-based inputs and LLMs 
facilitate phenotype matching and enable reverse phenotyping. The use of 
real-world data enhances the applicability of AI in complex and heterogeneous 
clinical scenarios. However, major challenges persist, including data 
standardization, model interpretability, workflow integration, and algorithmic 
bias. Conclusions: AI has the potential to advance earlier and more personalized 
diagnostics for children with rare diseases. Achieving this requires 
multidisciplinary collaboration and careful attention to clinical, technical, 
and ethical considerations.

DOI: 10.3390/jpm15090407
PMCID: PMC12470782
PMID: 41003110

Conflict of interest statement: The authors declare no conflicts of interest.


180. Comput Med Imaging Graph. 2025 Oct;125:102643. doi: 
10.1016/j.compmedimag.2025.102643. Epub 2025 Sep 13.

A self-attention model for robust rigid slice-to-volume registration of 
functional MRI.

Khawaled S(1), Afacan O(2), Warfield SK(3), Freiman M(4).

Author information:
(1)The Interdisciplinary program in Applied Mathematics, Faculty of Mathematics, 
Technion - Israel Institute of Technology, Israel. Electronic address: 
ssamahkh@campus.technion.ac.il.
(2)Boston Children's Hospital, Boston, MA, USA; Harvard Medical School, Boston, 
MA, USA. Electronic address: onur.afacan@childrens.harvard.edu.
(3)Boston Children's Hospital, Boston, MA, USA; Harvard Medical School, Boston, 
MA, USA. Electronic address: simon.warfield@childrens.harvard.edu.
(4)Faculty of Biomedical Engineering, Technion - Israel Institute of Technology, 
Israel. Electronic address: moti.freiman@technion.ac.il.

Functional Magnetic Resonance Imaging (fMRI) is vital in neuroscience, enabling 
investigations into brain disorders, treatment monitoring, and brain function 
mapping. However, head motion during fMRI scans, occurring between shots of 
slice acquisition, can result in distortion, biased analyses, and increased 
costs due to the need for scan repetitions. Therefore, retrospective slice-level 
motion correction through slice-to-volume registration (SVR) is crucial. 
Previous studies have utilized deep learning (DL) based models to address the 
SVR task; however, they overlooked the uncertainty stemming from the input stack 
of slices and did not assign weighting or scoring to each slice. Treating all 
slices equally ignores the variability in their relevance, leading to suboptimal 
predictions. In this work, we introduce an end-to-end SVR model for aligning 2D 
fMRI slices with a 3D reference volume, incorporating a self-attention mechanism 
to enhance robustness against input data variations and uncertainties. Our SVR 
model utilizes independent slice and volume encoders and a self-attention module 
to assign pixel-wise scores for each slice. We used the publicly available 
Healthy Brain Network (HBN) dataset. We split the volumes into training (64%), 
validation (16%), and test (20%) sets. To conduct the simulated motion study, we 
synthesized rigid transformations across a wide range of parameters and applied 
them to the reference volumes. Slices were then sampled according to the 
acquisition protocol to generate 2,000, 500, and 200 3D volume-2D slice pairs 
for the training, validation, and test sets, respectively. Our experimental 
results demonstrate that our model achieves competitive performance in terms of 
alignment accuracy compared to state-of-the-art deep learning-based methods 
(Euclidean distance of 0.93 [mm] vs. 1.86 [mm], a paired t-test with a p-value 
of p<0.03). Furthermore, our approach exhibits faster registration speed 
compared to conventional iterative methods (0.096 s vs. 1.17 s). Our end-to-end 
SVR model facilitates real-time head motion tracking during fMRI acquisition, 
ensuring reliability and robustness against uncertainties in the inputs.

Copyright © 2025. Published by Elsevier Ltd.

DOI: 10.1016/j.compmedimag.2025.102643
PMID: 40997675 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


181. Artif Intell Med. 2025 Dec;170:103268. doi: 10.1016/j.artmed.2025.103268. Epub 
2025 Sep 22.

A survey for large language models in biomedicine.

Wang C(1), Li M(2), He J(3), Wang Z(4), Darzi E(5), Chen Z(6), Ye J(7), Li T(3), 
Su Y(3), Ke J(8), Qu K(2), Li S(2), Yu Y(2), Liò P(9), Wang T(10), Wang YG(11), 
Shen Y(12).

Author information:
(1)School of Medical Engineering, Henan Medical University, Xinxiang, China; 
Engineering Technology Research Center of Neurosense and Control of Henan 
Province, Xinxiang, China; Henan International Joint Laboratory of Neural 
Information Analysis and Drug Intelligent Design, Xinxiang, China.
(2)School of Medical Engineering, Henan Medical University, Xinxiang, China.
(3)Shanghai AI Laboratory, Shanghai, China.
(4)Amazon, Palo Alto, CA, USA.
(5)Boston Children's Hospital, MA, USA; Harvard Medical School, Harvard 
University, MA, USA.
(6)Toursun Synbio, Shanghai, China.
(7)Shanghai AI Laboratory, Shanghai, China; Department of Data Science & AI, 
Faculty of IT, Monash University, Melbourne, Australia.
(8)School of Electronic Information and Electrical Engineering, Shanghai Jiao 
Tong University, Shanghai, China; School of Computer Science and Engineering, 
University of New South Wales, Sydney, Australia.
(9)Department of Computer Science and Technology, University of Cambridge, 
Cambridge, UK.
(10)School of Basic Medical Sciences, Henan Medical University, Xinxiang, China. 
Electronic address: wtianyuncn@126.com.
(11)Shanghai AI Laboratory, Shanghai, China; Toursun Synbio, Shanghai, China; 
Institute of Natural Sciences, Shanghai Jiao Tong University, Shanghai, China; 
School of Mathematics and Statistics, University of New South Wales, Sydney, 
Australia. Electronic address: yuguang.wang@sjtu.edu.cn.
(12)Department of Computer Science, Johns Hopkins University, MD, USA. 
Electronic address: yshen92@jhu.edu.

Recent breakthroughs in large language models (LLMs) offer unprecedented natural 
language understanding and generation capabilities. However, existing surveys on 
LLMs in biomedicine often focus on specific applications or model architectures, 
lacking a comprehensive analysis that integrates the latest advancements across 
various biomedical domains. This review, based on an analysis of 484 
publications sourced from databases including PubMed, Web of Science, and arXiv, 
provides an in-depth examination of the current landscape, applications, 
challenges, and prospects of LLMs in biomedicine, distinguishing itself by 
focusing on the practical implications of these models in real-world biomedical 
contexts. Firstly, we explore the capabilities of LLMs in zero-shot learning 
across a broad spectrum of biomedical tasks, including diagnostic assistance, 
drug discovery, and personalized medicine, among others, with insights drawn 
from 137 key studies. Then, we discuss adaptation strategies of LLMs, including 
fine-tuning methods for both uni-modal and multi-modal LLMs to enhance their 
performance in specialized biomedical contexts where zero-shot fails to achieve, 
such as medical question answering and efficient processing of biomedical 
literature. Finally, we discuss the challenges that LLMs face in the biomedicine 
domain including data privacy concerns, limited model interpretability, issues 
with dataset quality, and ethics due to the sensitive nature of biomedical data, 
the need for highly reliable model outputs, and the ethical implications of 
deploying AI in healthcare. To address these challenges, we also identify future 
research directions of LLM in biomedicine including federated learning methods 
to preserve data privacy and integrating explainable AI methodologies to enhance 
the transparency of LLMs. As this field of LLM rapidly evolves, continued 
research and development are essential to fully harness the capabilities of LLMs 
in biomedicine while ensuring their responsible and effective deployment.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.artmed.2025.103268
PMID: 40997586 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that the research was conducted in the absence of any commercial or 
financial relationships that could be construed as a potential conflict of 
interest.


182. Med Phys. 2025 Oct;52(10):e70016. doi: 10.1002/mp.70016.

Transformer-based deep learning for predicting brain tumor recurrence using 
magnetic resonance imaging.

Zhou Q(1), Tian X(2), Feng M(3), Li L(4)(5), Zheng D(6)(7), Li X(1)(7).

Author information:
(1)School of Information and Software Engineering, University of Electronic 
Science and Technology of China, Chengdu, Sichuan, China.
(2)Clinical Medical Research Center, The First People's Hospital of Kashi 
Prefecture, Kashi, Xinjiang, China.
(3)School of Electric and Electronic Engineering, North China Electric Power 
University, Beijing, China.
(4)Department of Radiation Oncology, Sichuan Cancer Hospital & Institute, 
Chengdu, Sichuan, China.
(5)Physical Engineering Laboratory, Radiation Oncology Key Laboratory of Sichuan 
Province, Chengdu, Sichuan, China.
(6)School of Computer Science and Software Engineering, Southwest Petroleum 
University, Chengdu, Sichuan, China.
(7)Kashi Institute of Electronics and Information Industry, Kashi, Xinjiang, 
China.

BACKGROUND: Deep learning (DL) models, particularly those based on Transformer 
architecture, which are capable of capturing complex patterns and dependencies 
in medical imaging data, have shown great potential in improving brain tumor 
prognosis and guiding treatment decisions. However, the effectiveness of 
Transformer-based models, especially in predicting recurrence after treatment, 
has yet to be fully demonstrated.
PURPOSE: This study aims to develop and validate a Transformer-based DL model 
that utilizes multi-modal data, specifically pre-treatment magnetic resonance 
imaging (MRI) scans fused with radiotherapy dose (RTDose) information, to 
predict post-treatment recurrence in brain tumors, thereby providing decision 
support for personalized radiotherapy.
METHODS: In this study, we employed MRI data from patients with brain metastases 
who had undergone Gamma Knife radiosurgery at the University of Mississippi 
Medical Center to train and validate a Transformer-based DL model. To further 
validate the Transformer-based model, a comparative analysis was conducted with 
nine established prognostic models. The generalizability and predictive accuracy 
of the model were validated across multiple clinical subgroups. To further 
exclude other potential factors influencing brain tumor recurrence, logistic 
regression (LR) and statistical analysis were conducted to confirm the 
independence of the model's predictions.
RESULTS: The model achieved an average area under the receiver operating 
characteristic curve (AUROC) of 0.817 on 3-fold cross-validation, outperforming 
all other models. The model also exhibited strong generalizability across 
clinical subgroups, with AUROCs of 0.806 for patients under 50, 0.723 for those 
aged 51-60, and 0.843 for those aged 61-77 ( p = 0.057 ). For gender subgroups, 
the AUROCs were 0.783 for females and 0.820 for males ( p = 0.057 ). LR analysis 
confirmed the independence of the model's predictions, with a largest 
permutation importance and p < 0.001 .
CONCLUSIONS: The Transformer-based DL model developed in this study serves as a 
reliable prognostic tool for predicting brain tumor recurrence following 
radiotherapy. It demonstrated superior performance compared to nine established 
prognostic models, including various deep learning architectures and 
radiomics-based methods, and holds the potential to guide personalized treatment 
strategies for brain tumor patients.

© 2025 American Association of Physicists in Medicine.

DOI: 10.1002/mp.70016
PMID: 40996365 [Indexed for MEDLINE]


183. Am J Clin Pathol. 2025 Nov 19;164(5):830-839. doi: 10.1093/ajcp/aqaf110.

Deep learning model for automated detection of Helicobacter pylori and 
intestinal metaplasia on gastric biopsy digital whole slide images.

Khor LY(1)(2), Neo CC(3), Prathaban K(3), Choa E(3), Quah WK(3), Lum EN(3), Chen 
R(3), Heng SY(1), Koh VC(1), Seow JX(1), Jegannathan N(1), Shi R(4), Loong S(1), 
Song LH(1), Natarajan A(3), Ravi S(3), Oh KS(3), Cheng CL(1)(2).

Author information:
(1)Department of Anatomical Pathology, Singapore General Hospital, Singapore.
(2)Pathology Academic Clinical Programme, Duke-NUS Medical School, Singapore.
(3)AI Singapore, Singapore.
(4)Department of Pathology and Laboratory Medicine, Kandang Kerbau Women's and 
Children's Hospital, Singapore.

OBJECTIVE: To develop an automated detection tool for Helicobacter pylori (HP) 
microorganisms (HPOrg) and intestinal metaplasia (IM) identification on gastric 
biopsy specimens on hematoxylin and eosin (H&E) whole-slide images (WSIs), 
incorporating background histopathologic features.
METHODS: A total of 180 H&E gastric biopsy WSIs, archived at the Department of 
Anatomical Pathology, Singapore General Hospital, were used to train, validate, 
and test (60:20:20) a decision support tool. Eighty WSIs displayed non-HP 
inflammation; 100 were annotated for HP-associated gastritis, HPOrg, and IM. A 
2-stage model was employed-a Vision Transformer-based model filtered artifacts 
after stain normalization, and then a Graph Attention Network component 
aggregated patch-level features, giving a prediction for each of 6 tissue 
sections within each WSI, with a majority vote determining the final WSI 
prediction.
RESULTS: A total of 776 636 patches were used for training/validation and 
testing. The optimized model showed HPOrg classification (precision: 0.604, 
F1-score: 0.617, and top 10 micro F1-score: 0.714) and IM classification 
(precision: 0.905, F1-score: 0.861, and top 10 micro F1-score: 1.0). The macro 
average F1-score was 0.739, section-level precision was 0.981, and the F1-score 
was 0.945. The WSI-level precision achieved was 1.0, with a F1-score of 0.96.
CONCLUSIONS: We demonstrate a 2-stage model to detect HP and IM in gastric 
biopsy specimens, considering background inflammation, which more closely 
reflects real-world clinical diagnosis.

© The Author(s) 2025. Published by Oxford University Press on behalf of American 
Society for Clinical Pathology. All rights reserved. For commercial re-use, 
please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/ajcp/aqaf110
PMID: 40996022 [Indexed for MEDLINE]


184. Neurol Clin Pract. 2025 Dec;15(6):e200542. doi: 10.1212/CPJ.0000000000200542. 
Epub 2025 Sep 23.

Accelerating Medical Record Data Abstraction and Analysis in Muscular Dystrophy: 
Large Language Models and International Classification of Diseases Codes.

Zhou H(1)(2), Rajamani G(3), Huang J(2), Jorand-Fletcher M(4), Mohamed Y(4), 
DeGolier KA(5), Xenopoulos-Oddsson A(5), Cui E(6), Zingariello CD(4), Zhang 
R(2), Kang PB(1)(7)(8).

Author information:
(1)Institute for Health Informatics, University of Minnesota, Minneapolis.
(2)Division of Computational Health Sciences, University of Minnesota, 
Minneapolis.
(3)Medical School, University of Minnesota, Minneapolis.
(4)Department of Pediatrics, University of Florida College of Medicine, 
Gainesville.
(5)Masonic Institute for the Developing Brain, University of Minnesota, 
Minneapolis.
(6)Division of Biostatistics and Health Data Science, University of Minnesota, 
Minneapolis.
(7)Greg Marzolf Jr. Muscular Dystrophy Center and Department of Neurology, 
University of Minnesota Medical School, Minneapolis; and.
(8)Institute for Translational Neuroscience, University of Minnesota, 
Minneapolis.

BACKGROUND AND OBJECTIVES: Muscular dystrophies are characterized by progressive 
muscle weakness and degeneration. Identifying cases and abstracting data from 
electronic medical records (EMRs) is helpful for surveillance and research. 
However, manual EMR abstraction is laborious. We studied 2 approaches to 
accelerate EMR abstraction: large language models (LLMs) and International 
Classification of Diseases (ICD) code meta-analysis.
METHODS: In our cross-sectional study, EMRs from 22 individuals with Duchenne 
muscular dystrophy (DMD) and 22 individuals with limb-girdle muscular dystrophy 
(LGMD) were exported into a data shelter and manually annotated using MedTator. 
Annotations were guided by a schema focused on 4 key features of muscular 
dystrophy: first symptoms, ambulatory status, serum creatine kinase (CK) levels, 
and genetic test results. Five LLMs were fed a series of prompts and examples, 
and then, clinic notes from each of the 44 cases were inputted for model 
analysis. Inter-rater agreement (IAA) and F1 scores were calculated for manual 
annotations, and the F1 score for LLMs compared with manual annotations was 
calculated. We then analyzed a separate set of 77 DMD and 59 LGMD cases to 
determine whether the number of health care encounters with a muscular 
dystrophy-related ICD code could predict diagnostic certainty based on MD 
STARnet criteria.
RESULTS: IAA for manual annotations varied between 80% (for annotation of 
symptoms) and 100% (for CK values). The highest performing LLM was Llama 3-8b, 
which yielded the following accuracies: 46.8% for "first symptoms," 56.9% for 
"ambulatory status," 69.2% for "CK values," and 68.4% for "genetic test 
results." Among 77 individuals with DMD, all patients with 20 or more encounters 
linked to relevant ICD codes had definite or probable diagnoses, whereas among 
59 individuals with LGMD, all patients with 25 or more encounters linked to 
relevant ICD codes had definite or probable diagnoses.
DISCUSSION: LLMs promise to accelerate EMR abstraction for rare diseases such as 
muscular dystrophy, but F1 scores for LLMs currently lag manual abstractions for 
unstructured data. Llama 3-8b demonstrated superior performance to the 4 other 
models tested. Metadata such as ICD code counts may help prioritize high-yield 
cases for surveillance and research purposes.

© 2025 American Academy of Neurology.

DOI: 10.1212/CPJ.0000000000200542
PMCID: PMC12456306
PMID: 40994811

Conflict of interest statement: The authors report no relevant disclosures. Full 
disclosure form information provided by the authors is available with the full 
text of this article at Neurology.org/cp.TAKE-HOME POINTS→ Large language models 
(LLMs) show potential to abstract information efficiently from electronic 
medical records.→ The accuracy of LLMs for medical record abstraction needs to 
improve, particularly for unstructured data.→ International Classification of 
Diseases codes and other metadata may help determine which cases are most 
suitable for detailed data abstraction.


185. J Med Internet Res. 2025 Sep 24;27:e82729. doi: 10.2196/82729.

Author's Reply: Critical Limitations in Systematic Reviews of Large Language 
Models in Health Care.

Python A(1)(2)(3), Li H(1)(4), Fu JF(5)(6)(7).

Author information:
(1)Center for Data Science, Zhejiang University, Hangzhou, China.
(2)School of Medicine, Zhejiang University, Hangzhou, China.
(3)Centre for Human Genetics, Nuffield Department of Medicine, University of 
Oxford, Roosevelt Drive, Oxford, OX3 7BN, United Kingdom, 44 01865 287500.
(4)School of Mathematical Sciences, Zhejiang University, Hangzhou, China.
(5)School of Medicine, Children's Hospital of Zhejiang University, Hangzhou, 
China.
(6)National Clinical Research Center for Child Health, Hangzhou, China.
(7)National Regional Center for Children's Health, Hangzhou, China.

Comment on
    doi: 10.2196/81769.
    doi: 10.2196/71916.

DOI: 10.2196/82729
PMCID: PMC12459737
PMID: 40991734

Conflict of interest statement: Conflicts of Interest: None declared.


186. RETRACTED ARTICLE

Int J Womens Health. 2025 Sep 18;17:3077-3085. doi: 10.2147/IJWH.S542758. 
eCollection 2025.

Research Progress and Clinical Implications of Generative Artificial 
Intelligence in Perinatal Health Care for Advanced Maternal Age Pregnant Women.

Tang S(1), Zhao S(1).

Author information:
(1)Department of Obstetrics and Gynecology, The Sixth Affiliated Hospital of 
Harbin Medical University, Harbin, Heilongjiang Province, 150023, People's 
Republic of China.

Retraction in
    Int J Womens Health. 2025 Dec 19;17:5557-5558. doi: 10.2147/IJWH.S588171.

OBJECTIVE: To analyze the current application status, technical characteristics, 
and challenges of Generative Artificial Intelligence (Generative AI) in 
perinatal health care for advanced maternal age pregnant women and explore 
targeted optimization strategies.
METHODS: A systematic literature review was conducted by searching PubMed, Web 
of Science, CNKI, and Wanfang Data from January 2020 to April 2025. Studies were 
included if they focused on Generative AI applications in perinatal care for 
women aged ≥35 years; 78 eligible studies (42 Chinese, 36 international) were 
finally included, covering technical applications, clinical validation, and 
ethical governance. We summarized the applications of Generative AI in risk 
prediction, personalized management, and remote monitoring, and analyzed issues 
related to data governance, technical limitations, resource allocation, and 
ethical supervision.
RESULTS: Generative AI improves healthcare efficiency by integrating multiple 
data sources for model construction, planning dynamic interventions, and 
facilitating remote monitoring. Specifically, GANs-based models achieve an AUC 
of 0.80-0.85 in predicting Group B Streptococcus infection, while Transformer 
models enhance the accuracy of prenatal depression screening by 15-20% compared 
to traditional methods. However, it faces challenges including data privacy 
risks (eg, 32% of maternal health institutions lack encrypted data storage), the 
"black box" nature of models (42% of clinicians report low trust in AI 
decision-making), urban-rural technological gaps (only 18% of county-level 
hospitals use AI perinatal tools), and ambiguous liability definitions.
CONCLUSION: Generative AI demonstrates significant application potential in 
perinatal care for advanced maternal age pregnant women. Promoting its 
implementation through technological innovation (eg, explainable AI), 
interpretability optimization, resource deployment (eg, lightweight mobile 
tools), and ethical supervision is crucial to improving maternal and infant 
health outcomes in China and globally.

Plain Language Summary: As more women delay pregnancy until age 35 or older 
(advanced maternal age), they face higher risks of complications like 
gestational diabetes and preterm birth—risks traditional healthcare struggles to 
address with personalized, round-the-clock care. This review explores how 
Generative AI (technology that analyzes data to generate new insights) could 
transform care for this group. We searched 4 international and Chinese databases 
(2020–2025) and analyzed 78 studies. We found Generative AI can: Predict risks 
early: Use medical records and real-time data to spot issues like infections or 
depression before they worsen (eg, AI models correctly identify 80% of Group B 
Streptococcus cases by 35 weeks of gestation); Create tailored plans: Design 
personalized nutrition, exercise, and mental health strategies based on each 
woman’s needs (eg, AI nutritional interventions reduce gestational 
diabetes-related macrosomia by 22%); Monitor remotely: Use wearable devices to 
track vital signs and alert doctors to emergencies—especially helpful in rural 
areas where healthcare resources are limited. However, challenges remain: 
Patient data is not always secure, doctors often do not understand how AI makes 
decisions, rural hospitals rarely have access to AI tools, and no clear rules 
exist for who is responsible if AI makes a mistake. Our findings mean that with 
better data protection, transparent AI algorithms, and fair access to 
technology, Generative AI could make high-quality perinatal care more 
accessible—helping advanced maternal age women have healthier pregnancies and 
babies, both in China and around the world.

© 2025 Tang and Zhao.

DOI: 10.2147/IJWH.S542758
PMCID: PMC12453038
PMID: 40988772

Conflict of interest statement: The authors declare no financial or 
non-financial competing interests related to the content of this manuscript. 
None of the authors have relationships with organizations that might have an 
interest in the publication, including employment, stock holdings, patents, or 
consulting fees.


187. PLoS One. 2025 Sep 23;20(9):e0333089. doi: 10.1371/journal.pone.0333089. 
eCollection 2025.

How University students in Bangladesh engage with ChatGPT: A qualitative study.

Hasib M(1), Islam MS(2).

Author information:
(1)College of Communication & Information Sciences, University of Alabama, 
Tuscaloosa, Alabama, United States of America.
(2)Department of Mass Communication and Journalism, Social Science School, 
Khulna University, Khulna, Bangladesh.

This study examined university students' perceptions and practices of using 
ChatGPT through a qualitative approach, employing semi-structured, in-depth 
interviews with 20 Khulna University students. Using thematic analysis, the 
research identified key themes, including both academic and non-academic 
motivations, with applications ranging from assignments and research to 
entertainment. Despite limitations such as false references and predetermined 
feedback, students found ChatGPT efficient and useful both inside and outside 
the classroom. However, concerns about academic integrity arose, as some 
students devised creative ways to bypass existing plagiarism detection systems. 
Ethical considerations emerged regarding responsible AI use, underscoring the 
need for clear guidelines and training to ensure proper referencing and the 
development of critical thinking skills. Future research should focus on 
continuous evaluation and the implementation of support services to encourage 
the responsible integration of ChatGPT in university settings.

Copyright: © 2025 Hasib, Islam. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0333089
PMCID: PMC12456799
PMID: 40986478 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


188. Radiology. 2025 Sep;316(3):e250568. doi: 10.1148/radiol.250568.

Clinical Validation of a Generative Artificial Intelligence Model for Chest 
Radiograph Reporting: A Multicohort Study.

Hwang EJ(1), Lee JH(1), Lim WH(1), Jeong WG(2), Hong W(3), Park J(4), Yoo SJ(5), 
Kim H(1)(6).

Author information:
(1)Department of Radiology, Seoul National University Hospital and Seoul 
National University College of Medicine, 101 Daehak-ro, Jongno-gu, Seoul 03080, 
Republic of Korea.
(2)Department of Radiology, Chonnam National University Hwasun Hospital and 
Chonnam National University Medical School, Hwasun, Republic of Korea.
(3)Department of Radiology, Hallym University Sacred Heart Hospital, Anyang, 
Republic of Korea.
(4)Department of Radiology, Yeungnam University Medical Center and Yeungnam 
University College of Medicine, Daegu, Republic of Korea.
(5)Department of Radiology, Hanyang University Medical Center and Hanyang 
University College of Medicine, Seoul, Republic of Korea.
(6)Soombit.ai, Seongnam, Republic of Korea.

Comment in
    Radiology. 2025 Oct;317(1):e252955. doi: 10.1148/radiol.252955.

Background Artificial intelligence (AI)-generated radiology reports have become 
available and require rigorous evaluation. Purpose To evaluate the clinical 
acceptability of chest radiograph reports generated by an AI algorithm and their 
accuracy in identifying referable abnormalities. Materials and Methods Chest 
radiographs from an intensive care unit (ICU), an emergency department, and 
health checkups were retrospectively collected between January 2020 and December 
2022, and outpatient chest radiographs were sourced from a public dataset. An 
automated report-generating AI algorithm was then applied. A panel of seven 
thoracic radiologists evaluated the acceptability of generated reports, and 
acceptability was analyzed using a standard criterion (acceptable without 
revision or with minor revision) and a stringent criterion (acceptable without 
revision). Using chest radiographs from three of the contexts (excluding the 
ICU), AI-generated and radiologist-written reports were compared regarding the 
acceptability of the reports (generalized linear mixed model) and their 
sensitivity and specificity for identifying referable abnormalities (McNemar 
test). The radiologist panel was surveyed to evaluate their perspectives on the 
potential of AI-generated reports to replace radiologist-written reports. 
Results The chest radiographs of 1539 individuals (median age, 55 years; 656 
male patients, 483 female patients, 400 patients of unknown sex) were included. 
There was no evidence of a difference in acceptability between AI-generated and 
radiologist-written reports under the standard criterion (88.4% vs 89.2%; P = 
.36), but AI-generated reports were less acceptable than radiologist-written 
reports under the stringent criterion (66.8% vs 75.7%; P < .001). Compared with 
radiologist-written reports, AI-generated reports identified radiographs with 
referable abnormalities with greater sensitivity (81.2% vs 59.4%; P < .001) and 
lower specificity (81.0% vs 93.6%; P < .001). In the survey, most radiologists 
indicated that AI-generated reports were not yet reliable enough to replace 
radiologist-written reports. Conclusion AI-generated chest radiograph reports 
had similar acceptability to radiologist-written reports, although a substantial 
proportion of AI-generated reports required minor revision. © RSNA, 2025 
Supplemental material is available for this article. See also the editorial by 
Wu and Seo in this issue.

DOI: 10.1148/radiol.250568
PMID: 40985833 [Indexed for MEDLINE]


189. J Perinat Med. 2025 Sep 23;54(1):17-22. doi: 10.1515/jpm-2025-0392. Print 2026 
Jan 23.

Integrating generative AI in perinatology: applications for literature review.

Ayala-Yáñez R(1), Grünebaum A(2), Chervenak FA(2).

Author information:
(1)Centro Médico ABC I.A.P., Mexico City, Mexico.
(2)Northwell Health, Hempstead, NY, USA.

Perinatology relies on continuous engagement with an expanding body of clinical 
literature, yet the volume and velocity of publications increasingly exceed the 
capacity of clinicians to keep pace. Generative artificial intelligence (GAI) 
tools - such as ChatGPT4, Claude AI, Gemini, and Perplexity AI - offer a novel 
approach to assist with literature retrieval, comparison of clinical guidelines, 
and manuscript drafting. This study evaluates the strengths and limitations of 
these tools in maternal-fetal medicine, using structured clinical prompts to 
simulate real-world applications. Perplexity AI demonstrated the best citation 
accuracy, while ChatGPT4 and Claude excelled in content summarization but 
required manual verification of citations. In simulated trials, GAI tools 
reduced the time to generate clinically relevant summaries by up to 70 % 
compared to traditional PubMed searches. However, risks such as hallucinated 
references and overreliance on machine-generated text persist. Use cases include 
summarizing aspirin use guidelines for preeclampsia and comparing ACOG vs. NICE 
protocols. GAI should be viewed as a supportive assistant, not a substitute, for 
expert review. To ensure responsible integration, clinicians must develop AI 
literacy, apply rigorous oversight, and adhere to ethical standards. When used 
judiciously, GAI can enhance efficiency, insight, and evidence-based 
decision-making in perinatal care.

© 2025 the author(s), published by De Gruyter, Berlin/Boston.

DOI: 10.1515/jpm-2025-0392
PMID: 40980855 [Indexed for MEDLINE]


190. Arthrosc Sports Med Rehabil. 2025 Jun 26;7(4):101210. doi: 
10.1016/j.asmr.2025.101210. eCollection 2025 Aug.

ChatGPT-Generated Responses Across Orthopaedic Sports Medicine Surgery Vary in 
Accuracy, Quality, and Readability: A Systematic Review.

Kodra JD(1), Saroyan A(2), Darby F(3), Surucu S(3), Fong S(4), Gillinov S(3), 
Girardi K(3), Vasudevan R(3), Ansah-Twum JK(3), Atadja L(3), Moran J(3), Jimenez 
AE(3).

Author information:
(1)Medical College of Wisconsin, Milwaukee, Wisconsin, U.S.A.
(2)Oakland University William Beaumont, Rochester, Michigan, U.S.A.
(3)Department of Orthopaedics and Rehabilitation, Yale University School of 
Medicine, New Haven, Connecticut, U.S.A.
(4)Case Western Reserve University, Cleveland, Ohio, U.S.A.

PURPOSE: To evaluate the current literature regarding the accuracy and efficacy 
of ChatGPT in delivering patient education on common orthopaedic sports medicine 
operations.
METHODS: A systematic review was performed in accordance with Preferred 
Reporting Items for Systematic Reviews and Meta-analyses guidelines. After 
PROSPERO registration, a keyword search was conducted in the PubMed, Cochrane 
Central Register of Controlled Trials, and Scopus databases in September 2024. 
Articles were included if they evaluated ChatGPT's performance against 
established sources, examined ChatGPT's ability to provide counseling related to 
orthopaedic sports medicine operations, and assessed ChatGPT's quality of 
responses. Primary outcomes assessed were quality of written content (e.g., 
DISCERN score), readability (e.g., Flesch-Kincaid Grade Level and Flesch-Kincaid 
Reading Ease Score), and reliability (Journal of the American Medical 
Association Benchmark Criteria).
RESULTS: Seventeen articles satisfied the inclusion and exclusion criteria and 
formed the basis of this review. Four studies compared the effectiveness of 
ChatGPT and Google, and another study compared ChatGPT-3.5 with ChatGPT-4. 
ChatGPT provided moderate- to high-quality responses (mean DISCERN score, 
41.0-62.1), with strong inter-rater reliability (0.72-0.91). Readability 
analyses showed that responses were written at a high school to college reading 
level (mean Flesch-Kincaid Grade Level, 10.3-16.0) and were generally difficult 
to read (mean Flesch-Kincaid Reading Ease Score, 28.1-48.0). ChatGPT frequently 
lacked source citations, resulting in a poor reliability score across all 
studies (mean Journal of the American Medical Association score, 0). Compared 
with Google, ChatGPT-4 generally provided higher-quality responses. ChatGPT also 
displayed limited source transparency unless specifically prompted for sources. 
ChatGPT-4 outperformed ChatGPT-3.5 in response quality (DISCERN score, 3.86 [95% 
confidence interval, 3.79-3.93] vs 3.46 [95% confidence interval, 3.40-3.54]; P 
= .01) and readability.
CONCLUSIONS: ChatGPT provides generally satisfactory responses to patient 
questions regarding orthopaedic sports medicine operations. However, its utility 
remains limited by challenges with source attribution, high reading complexity, 
and variability in accuracy.
LEVEL OF EVIDENCE: Level V, systematic review of Level V studies.

© 2025 The Authors.

DOI: 10.1016/j.asmr.2025.101210
PMCID: PMC12447144
PMID: 40980212


191. Cureus. 2025 Aug 19;17(8):e90469. doi: 10.7759/cureus.90469. eCollection 2025 
Aug.

Current and Future Applications of AI in EMS Training: A Scoping Review.

Pasquariello LD(1), Sims JK(1), O'Brien J(2), Upperman J(1).

Author information:
(1)Pediatric Surgery, Vanderbilt University Medical Center, Nashville, USA.
(2)Epidemiology, Columbia University Mailman School of Public Health, New York, 
USA.

Over the last 15 years, AI has been increasingly utilized in healthcare 
education. In EMS, AI is being used to train providers for their high-stress, 
dynamic environments. This scoping review examines current and future 
applications of AI in EMS training, with a focus on simulation, interventions, 
personalized learning, and disaster preparedness. A librarian-assisted 
literature search was conducted across PubMed, Embase, and Web of Science using 
terms related to EMS, AI, and education. Articles published between January 1, 
2010 and March 16, 2025 were screened and selected based on relevance. Studies 
were reviewed by title, abstract, and full text, followed by data extraction. 
Currently, AI has been integrated to enhance EMS training through advanced 
simulation, procedure training, development of personalized learning materials, 
and disaster response. These improvements are made possible by utilizing 
different types of AI tools such as machine learning, natural language 
processing, and large language models. AI tools showed improvements in realism, 
diagnostic accuracy, feedback delivery, and learner adaptability. It is actively 
transforming EMS training by strengthening provider readiness, clinical 
judgment, and educational outcomes. With continued research and development, its 
integration could further enhance training effectiveness. Ethical and 
implementation challenges remain, but increased education and awareness can help 
ensure that AI is used to positively affect patient care.

Copyright © 2025, Pasquariello et al.

DOI: 10.7759/cureus.90469
PMCID: PMC12445063
PMID: 40979032

Conflict of interest statement: Conflicts of interest: In compliance with the 
ICMJE uniform disclosure form, all authors declare the following: 
Payment/services info: All authors have declared that no financial support was 
received from any organization for the submitted work. Financial relationships: 
All authors have declared that they have no financial relationships at present 
or within the previous three years with any organizations that might have an 
interest in the submitted work. Other relationships: All authors have declared 
that there are no other relationships or activities that could appear to have 
influenced the submitted work.


192. Artif Intell Med. 2025 Dec;170:103276. doi: 10.1016/j.artmed.2025.103276. Epub 
2025 Sep 16.

The interpretable deep learning framework and validation for seizure detection 
in pediatric electroencephalography: An improved accuracy and performance 
analysis.

Zhou Y(1), Gao Y(2), Li Q(3), Wu R(4), Yang A(5), Tseng ML(6).

Author information:
(1)College of Medical Technology and Engineering, Henan University of Science 
and Technology, Luoyang, 471000, China. Electronic address: yuzhou@haust.edu.cn.
(2)School of Microelectronics, Tianjin University, Tianjin, 300072, China. 
Electronic address: yuxingao@tju.edu.cn.
(3)School of Microelectronics, Tianjin University, Tianjin, 300072, China. 
Electronic address: liqiang@tju.edu.cn.
(4)Department of Electronic and Electrical Engineering, Brunel University 
London, Uxbridge, UB8 3PH, UK. Electronic address: Ruiheng.Wu@Brunel.ac.uk.
(5)School of Electrical and Information Engineering, Tianjin University, 
Tianjin, 300072, China. Electronic address: yangaiping@tju.edu.cn.
(6)Institute of Innovation and Circular Economy, Asia University, Taichung, 
Taiwan; Center of Global Studies, Duy Tan University, Da Nang, Vietnam; 
Department of Medical Research, China Medical University Hospital, China Medical 
University, Taichung, Taiwan; School of Business, VIZJA University, Warsaw, 
Poland; Department of Industrial Engineering, Khon Kaen University, Khon Kaen, 
Thailand. Electronic address: tsengminglang@asia.edu.tw.

This study proposes an interpretable deep learning framework and compares the 
two novel models. A fully convolutional network with squeeze-and-excitation 
modules (SE-FCN) is designed to enhance spatial sensitivity and retain temporal 
resolution. In addition, a transformer-based model (TransNet) is developed to 
capture temporal and channel-wise dependencies via self-attention. These two 
models output channel saliency weights to the EEG electrode space and generate 
heatmaps for inferring potential epileptogenic zones. Deep learning primarily 
adopts convolutional neural networks (CNNs) or sequence generation networks 
(SGNs) and faces the limitations. For instance, CNN-based models often lack 
hierarchical modeling and fail to quantify channel-wise contributions, hindering 
spatial localization. SGN-based models struggle to capture complex 
spatiotemporal dependencies and typically lack adaptive attention tailored to 
electroencephalography (EEG) characters. Epileptic seizure detection is vital 
for effective clinical intervention and existing methods operated as black 
boxes, limiting clinical interpretability. This study evaluates the models on 
the CHB-MIT pediatric EEG dataset using a subject-independent cross-validation 
protocol. SE-FCN achieves an AUC of 0.89 and accuracy of 86.7 %, while TransNet 
achieves an AUC of 0.92 and accuracy of 86.4 %. Saliency maps from both models 
demonstrate high consistency and enable categorization of 22 patients into five 
groups based on inferred seizure origins.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.artmed.2025.103276
PMID: 40972406 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
study.


193. JMIR Res Protoc. 2025 Sep 18;14:e67571. doi: 10.2196/67571.

The Health Sector Response to Gender-Based Violence and Sexual Reproductive 
Health Programs in the Commonwealth and Selected African Countries: Protocol for 
a Mixed Methods Systematic Review and Meta-Analysis.

Phaswana-Mafuya RN(#)(1), Phalane E(#)(1), Zungu N(2)(3), Musekiwa A(#)(4), 
Ramalepe L(#)(5), Bagg K(6), Nyasulu P(7), Shisana O(#)(6)(8).

Author information:
(1)South African Medical Research Council Pan African Centre for Epidemics 
Research Extramural Unit, Faculty of Health Sciences, University of 
Johannesburg, Johannesburg, South Africa.
(2)Public Health, Societies and Belonging, Human Sciences Research Council, 
Pretoria, South Africa.
(3)School of Nursing and Public Health, University of KwaZulu-Natal, Durban, 
South Africa.
(4)School of Health Systems and Public Health, Faculty of Health Sciences, 
University of Pretoria, Pretoria, South Africa.
(5)Vanderbijlpark Campus, COMPRES Research Entity, University of North-West, 
Vanderbijlpark, South Africa.
(6)Evidence Based Solutions Consulting, Pty Ltd, Cape Town, South Africa.
(7)Division of Epidemiology and Biostatistics, Department of Global Health, 
Faculty of Medicine and Health Sciences, Stellenbosch University, Cape Town, 
South Africa.
(8)Department of Psychiatry and Mental Health, University of Cape Town, Cape 
Town, South Africa.
(#)Contributed equally

BACKGROUND: The intertwining nature of gender-based violence (GBV) and violence 
perpetrated against women and girls (VAWG), as well as sexual and reproductive 
health rights (SRHR), underlines the urgent need for the health sector to 
enhance the coordination of services to improve health outcomes. Importantly, 
GBV and VAWG are intricately linked to a spectrum of SRHR challenges, ranging 
from unintended pregnancies to severe maternal, gynecological, and mental health 
outcomes. Cumulative GBV had a more significant effect on abortion risk than 
associated variables. Recognizing the interplay between GBV, VAWG, and SRHR 
highlights the necessity for a comprehensive health sector response. A 
systematic review of the health sector response to GBV, VAWG, and SRHR will be 
conducted to understand the extent and array of health facility-based 
coordinated responses to GBV, VAWG, and SRHR; lessons learned; and successes and 
challenges in the Commonwealth and selected African countries.
OBJECTIVE: We aim to understand the context of GBV, VAWG, and SRHR by conducting 
a comprehensive review of health sector responses in different national, 
cultural, and socioeconomic contexts, and we aim to share best practices, 
experiences, and lessons learned.
METHODS: A mixed methods systematic review will be conducted following the 
Preferred Reporting Items for Systematic Reviews and Meta-Analyses Protocol 
(PRISMA-P) guidelines. The population, intervention, comparison, and outcome 
framework will be applied to screen and select relevant sources guided by the 
inclusion and exclusion criteria. The review will include relevant research 
papers published in the last 15 years and conducted in the 24 Commonwealth and 7 
selected African countries. Electronic databases to be searched will include 
PubMed, Google Scholar, Science Direct, EBSCOhost, Web of Science, Embase, 
PsycINFO, Cochrane, CINAHL, Index Medicus for the Eastern Mediterranean Region, 
and POPline.
RESULTS: Ethics approval will be waived as the study will use data in the public 
domain. The project has been commissioned by the Commonwealth Secretariat 
(2022-2025). The database search, data screening, and data extraction process 
for the review will be completed by September 2025. A manuscript will be 
submitted to a peer-reviewed international journal by November 2025. The initial 
online database searches, citations of eligible studies, and Microsoft Copilot 
identified 38,200 studies focusing on GBV, VAWG, and SRHR interventions. To 
date, 60 studies have been found eligible for inclusion in the review. The 
majority of these studies were conducted in eastern Africa (n=34), South Africa 
(n=14), and Asia (n=13). Evidence generated from this review will be made 
available through journal publications, seminars and workshops with key 
stakeholders, ministries of health, and local and international conferences.
CONCLUSIONS: The study will generate evidence to inform recommendations on 
addressing and mitigating the effects of GBV and VAWG on SRHR outcomes and 
coordinated services in the health sectors of Commonwealth and selected African 
countries.
TRIAL REGISTRATION: PROSPERO CRD42024520594; 
https://www.crd.york.ac.uk/PROSPERO/view/CRD42024520594.
INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): PRR1-10.2196/67571.

©Refilwe Nancy Phaswana-Mafuya, Edith Phalane, Nompumelelo Zungu, Alfred 
Musekiwa, Lebogang Ramalepe, Kayla Bagg, Peter Nyasulu, Olive Shisana. 
Originally published in JMIR Research Protocols 
(https://www.researchprotocols.org), 18.09.2025.

DOI: 10.2196/67571
PMCID: PMC12491884
PMID: 40966689 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


194. PLoS One. 2025 Sep 18;20(9):e0332130. doi: 10.1371/journal.pone.0332130. 
eCollection 2025.

Robust emotion recognition for complex environments: ChildEmoNet model based on 
DETR-ResNet50 cascaded architecture.

Shanshan Z(1), Yanlin S(2), Luen LC(3).

Author information:
(1)Department of Early Childhood Education, Faculty of National Child 
Development Research Centre, Universiti Pendidikan Sultan Idris, Perak, 
Malaysia.
(2)Music Department, Faculty of Creative Arts, Universiti Malaya, Kuala Lumpur, 
Malaysia.
(3)Department of Early Childhood Education, Faculty of Human Development, 
Universiti Pendidikan Sultan Idris, Perak, Malaysia.

Emotion recognition faces significant challenges in complex real-world 
environments, particularly under facial occlusion conditions that severely 
impact traditional deep learning approaches. This research proposes ChildEmoNet, 
a novel cascaded emotion recognition framework that strategically integrates 
Detection Transformer (DETR) for robust multi-person detection with ResNet50 for 
discriminative feature extraction. The primary contributions include the 
development of a cascaded DETR-ResNet50 architecture that addresses both 
detection and classification challenges simultaneously, enhanced robustness 
mechanisms specifically designed for facial occlusion scenarios, and 
comprehensive evaluation across both categorical and dimensional emotion 
recognition tasks. Extensive experiments on the OMG Emotion Dataset demonstrate 
the effectiveness of this integration: the proposed model achieves an AUC of 
0.93 in standard emotion classification tasks, maintains 79% recognition 
accuracy under 30% facial occlusion conditions, and attains concordance 
correlation coefficients (CCC) of 0.52 and 0.46 for valence and arousal 
prediction, respectively. The experimental validation confirms the crucial role 
of the DETR module in processing multi-person scenarios and the effectiveness of 
ResNet50 in feature extraction, demonstrating superior performance across 
complex environmental conditions including varying lighting, face orientations, 
and partial occlusions. Compared with traditional methods, this cascaded 
architecture shows remarkable robustness under challenging real-world 
conditions. This research advances emotion computing technology by providing a 
robust solution for emotion recognition applications in complex environments 
where conventional approaches exhibit significant performance degradation.

Copyright: © 2025 Shanshan et al. This is an open access article distributed 
under the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0332130
PMCID: PMC12445588
PMID: 40966300 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


195. Nutr Health. 2025 Sep 15:2601060251376091. doi: 10.1177/02601060251376091. 
Online ahead of print.

Investigating patients' perceptions of ChatGPT as a health information resource: 
A qualitative study.

AlShammary MH(1), Alyahya NM(2), Alanazi EM(3), Aldaeej A(4), Alanazi AM(5), 
Hassan W(4), Farag T(4), Alanazi SM(6), Al Otaibi HM(6), Albagmi S(7), Arif 
WM(8), Bakhshwain AM(9), Almuhanna AF(10), Alanezi F(4).

Author information:
(1)Computer Science Department, Imam Abdulrahman bin Faisal University, Dammam, 
Saudi Arabia.
(2)Community and Psychiatric Mental Health Department, Nursing College, King 
Saud University, Riyadh, Saudi Arabia.
(3)Health Informatics Department, College of Health Sciences, Saudi Electronic 
University, Riyadh, Saudi Arabia.
(4)Department of Management Information Systems, Collage of Business 
Administration, Imam Abdulrahman bin Faisal University, Dammam, Saudi Arabia.
(5)Dr Sulaiman Al Habib Hospital, Riyadh, Saudi Arabia.
(6)College of Medicine at the Imam Abdulrahman bin Faisal University, Dammam, 
Saudi Arabia.
(7)Prince Sultan Military College of Health Sciences, Dhahran, Saudi Arabia.
(8)College of Applied Medical Sciences, Department of Radiological Sciences, 
King Saud University, Riyadh, Saudi Arabia.
(9)King Saud Medical City, Ministry of Health, Pediatric Emergency Medicine 
Consultant, Riyadh, Saudi Arabia.
(10)Radiology Department, College of Medicine, Imam Abdulrahman Bin Faisal 
University, Dammam, Saudi Arabia.

BackgroundThe rapid adoption of artificial intelligence-powered tools like 
ChatGPT has introduced new avenues for patients to access health information 
independently. Understanding how patients perceive and engage with such tools is 
essential to evaluating their trustworthiness, usability, and potential impact 
on health decision-making.AimThe purpose of this study is to investigate the 
facilitators and barriers of using ChatGPT as a health information resource for 
patients' health management.MethodsA qualitative research design was adopted in 
this study. The participants included outpatients at a public hospital. 
Participants interacted with ChatGPT (version 3.5) for at least 15 min daily 
over 2 weeks to explore health-related topics before participating in 
semi-structured interviews. A total of 28 outpatients participated in the 
interviews.ResultsThe findings from this study have indicated both positive and 
negative aspects of ChatGPT as a health information resource. Among the 28 
participants, the most frequently reported facilitators included improved health 
literacy (reported by 26 participants, 92.9%), effectiveness and efficiency (24 
participants, 85.7%), cost-effectiveness (23 participants, 82.1%), accessibility 
(17 participants, 60.7%), empowerment (13 participants, 46.4%), and anonymity 
(11 participants, 39.3%). Reported barriers included lack of personalized 
information (15 participants, 53.6%), limited reliability (9 participants, 
32.1%), restricted diagnostic capability (6 participants, 21.4%), lack of human 
interaction (14 participants, 50%), privacy concerns (4 participants, 14.3%), 
legal and ethical issues (9 participants, 32.1%), and lack of emotional support 
(3 participants, 10.7%).ConclusionAlthough ChatGPT has significant benefits of 
being used as a health information resource, to arrive at specific conclusions, 
there is a need to extend these kinds of studies across the regions to assess 
the impact of ChatGPT on different populations for promoting health literacy.

DOI: 10.1177/02601060251376091
PMID: 40953221


196. Clin Rheumatol. 2025 Nov;44(11):4703-4710. doi: 10.1007/s10067-025-07640-4. Epub 
2025 Sep 15.

Comparative evaluation of large language models in delivering 
guideline-compliant recommendations for topical NSAID use in musculoskeletal 
pain: a multidimensional analysis.

Dong C(#)(1)(2), Qiu X(#)(1)(2), Deng J(1)(2), Xu L(1)(2), Dong X(3)(4), Chen 
S(1)(2), Mei T(1)(2), Li Q(1)(2), Cheng Y(1)(2), Sun J(5)(6), Wang H(7)(8), Yu 
L(9)(10).

Author information:
(1)Department of Pain, Affiliated Hangzhou First People's Hospital, Westlake 
University School of Medicine, Hangzhou, China.
(2)The Fourth Clinical School of Medical, Zhejiang Chinese Medical University, 
Hangzhou First People's Hospital, Hangzhou, China.
(3)Department of Neurology, Shanghai General Hospital, Shanghai Jiao Tong 
University School of Medicine, No.86, Wujin Road, Shanghai, 200080, China.
(4)Program in Developmental and Stem Cell Biology, The Hospital for Sick 
Children, 686 Bay Street, Toronto, ON, M5G 0A4, Canada.
(5)Department of Pain, Affiliated Hangzhou First People's Hospital, Westlake 
University School of Medicine, Hangzhou, China. jxmzsjl@163.com.
(6)The Fourth Clinical School of Medical, Zhejiang Chinese Medical University, 
Hangzhou First People's Hospital, Hangzhou, China. jxmzsjl@163.com.
(7)Department of Pain, Affiliated Hangzhou First People's Hospital, Westlake 
University School of Medicine, Hangzhou, China. 
wanghanbin@hospital.westlake.edu.cn.
(8)The Fourth Clinical School of Medical, Zhejiang Chinese Medical University, 
Hangzhou First People's Hospital, Hangzhou, China. 
wanghanbin@hospital.westlake.edu.cn.
(9)Department of Pain, Affiliated Hangzhou First People's Hospital, Westlake 
University School of Medicine, Hangzhou, China. 
yuliang@hospital.westlake.edu.cn.
(10)The Fourth Clinical School of Medical, Zhejiang Chinese Medical University, 
Hangzhou First People's Hospital, Hangzhou, China. 
yuliang@hospital.westlake.edu.cn.
(#)Contributed equally

INTRODUCTION: While large language models (LLMs) are increasingly used in 
clinical decision support, their adherence to evidence-based 
guidelines-particularly for musculoskeletal pain management-remains 
understudied.
METHODS: Four LLMs (DeepSeek-R1, ChatGPT-4o, Gemini, Grok-3) were evaluated on 
their responses to topical NSAID use for musculoskeletal pain through: 
assessments of response quality (accuracy, over-conclusiveness, supplementary 
information, and incompleteness), standardized readability metrics (Flesch 
Reading Ease, Flesch-Kincaid Grade Level), and the PEMAT-P tool to quantify 
actionability.
RESULTS: The four LLMs showed significant variability in accuracy (ANOVA 
p = 0.045), with Gemini scoring highest (8.33 ± 0.77) and DeepSeek-R1 lowest 
(7.72 ± 1.52) and in over-conclusiveness (ANOVA p = 0.025), with Grok-3 scoring 
lowest (4.56 ± 1.42) and ChatGPT-4o highest 6.72 ± 1.49). ChatGPT-4o provided 
the most supplementary content (6.94 ± 2.29, p = 0.106) and DeepSeek-R1 had the 
highest incompleteness (5.00 ± 2.52, p = 0.261). All models exceeded recommended 
readability thresholds (9th-10th grade level), and none met the actionability 
standard (≤ 33.5%).
CONCLUSIONS: LLMs demonstrate potential as clinical aids. The comprehensive 
performance of Gemini and Grok is relatively favorable, yet their readability 
and actionability remain unsatisfactory. Future development should integrate 
clinician feedback and real-world validation to ensure safety. Human oversight 
and targeted AI training are critical for safe implementation. Key Points • The 
study reveals significant differences in accuracy among LLMs, highlighting 
inconsistencies in clinical decision support. • While all models generated 
readable text, the complexity remained high, potentially limiting accessibility 
for some patients. • Glucocorticoid use for patients in remission was more 
strongly associated with impaired physical function in patients aged 75-84 than 
in patients aged 55-74 years. • Over-conclusiveness and incomplete adherence to 
evidence-based guidelines underscore the necessity for human oversight and 
targeted AI training in clinical applications.

© 2025. The Author(s).

DOI: 10.1007/s10067-025-07640-4
PMCID: PMC12568900
PMID: 40952435 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics statement for the use of 
human and animal subjects: Ethics statement for the use of human and animal 
subjects is not applicable as it used only publicly available/anonymized data. 
Consent for publication: Consent for publication is not applicable as the study 
used anonymized data. Disclosures: None.


197. Int Dent J. 2025 Dec;75(6):103854. doi: 10.1016/j.identj.2025.103854. Epub 2025 
Sep 12.

A Scoping Review of Large Language Models in Dental Education: Applications, 
Challenges, and Prospects.

Kong M(1), Fok EHW(1), Yiu CKY(2).

Author information:
(1)Paediatric Dentistry, Faculty of Dentistry, The University of Hong Kong, Hong 
Kong, China.
(2)Paediatric Dentistry, Faculty of Dentistry, The University of Hong Kong, Hong 
Kong, China. Electronic address: ckyyiu@hku.hk.

INTRODUCTION AND AIMS: With the rapid development of large language models 
(LLMs), their potential in dental clinical healthcare and education has become 
increasingly evident. However, the application of this emerging technology 
within dental education has not yet been systematically reviewed. This scoping 
review aims to summarize current applications, identify challenges encountered 
in the implementation process, and outline future prospects.
METHODS: A comprehensive literature search was conducted in the PubMed, Web of 
Science, and Embase databases to identify studies related to LLMs and dental 
education. Relevant studies were screened, selected, and subjected to data 
extraction to collate information pertinent to the research questions.
RESULTS: A total of 37 studies were included. LLMs have been applied across 
seven distinct domains in dental education, notably in dental examinations (n = 
24), educational information resources (n = 5), and assessment of students' 
assignments (n = 3). These applications have introduced several challenges, 
including technical issues (ie, inaccurate information, inability in image 
analysis, inaccessibility, and high cost) and application aspects (ie, ethical 
issues, absence of usage guidelines, potential harm to students' knowledge and 
skills, and cautious attitudes towards LLMs). Addressing these challenges 
requires coordinated efforts from students, educators, curriculum 
administrators, and technical developers.
CONCLUSIONS: The application of LLMs in dental education remains in its early 
stage. Future research should focus on expanding the range of applications and 
establishing standardized, ethically satisfying, and technically sound 
guidelines for integrating LLMs into dental education.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.identj.2025.103854
PMCID: PMC12790053
PMID: 40945315 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of interest The authors declare that 
they have no known competing financial interests or personal relationships that 
could have appeared to influence the work reported in this article.


198. J Clin Med. 2025 Aug 25;14(17):6011. doi: 10.3390/jcm14176011.

Pulmonary and Immune Dysfunction in Pediatric Long COVID: A Case Study 
Evaluating the Utility of ChatGPT-4 for Analyzing Scientific Articles.

Var SR(1)(2), Maeser N(3), Blake J(4), Zahs E(4), Deep N(4), Vasilakos Z(4), 
McKay J(5), Johnson S(1)(2), Strell P(1)(2)(6), Chang A(7), Korthas H(8), 
Krishna V(9), Narayanan M(9), Arju T(9), Natera-Rodriguez DE(1)(2), Roman A(7), 
Schulz SJ(5), Shetty A(1)(2), Vernekar M(4), Waldron MA(7), Person K(7), Cheeran 
M(9), Li L(7)(8)(10), Low WC(1)(2)(3)(4)(6)(7)(10)(11).

Author information:
(1)Department of Neurosurgery, University of Minnesota, Minneapolis, MN 55455, 
USA.
(2)Stem Cell Institute, University of Minnesota, Minneapolis, MN 55455, USA.
(3)Bioinformatics and Computational Biology Graduate Program, University of 
Minnesota, Minneapolis, MN 55455, USA.
(4)College of Biological Sciences, University of Minnesota, Minneapolis, MN 
55455, USA.
(5)Medical School, University of Minnesota, Minneapolis, MN 55455, USA.
(6)Comparative and Molecular Biosciences Graduate Program, University of 
Minnesota, Minneapolis, MN 55455, USA.
(7)Neuroscience Graduate Program, University of Minnesota, Minneapolis, MN 
55455, USA.
(8)Department of Experimental and Clinical Pharmacology, University of 
Minnesota, Minneapolis, MN 55455, USA.
(9)Department of Veterinary Population Medicine, University of Minnesota, 
Minneapolis, MN 55455, USA.
(10)Molecular Pharmacology and Therapeutics Graduate Program, University of 
Minnesota, Minneapolis, MN 55455, USA.
(11)Molecular, Cellular, Developmental Biology, and Genetics Graduate Program, 
University of Minnesota, Minneapolis, MN 55455, USA.

Coronavirus disease 2019 (COVID-19) in adults is well characterized and 
associated with multisystem dysfunction. A subset of patients develop post-acute 
sequelae of SARS-CoV-2 infection (PASC, or long COVID), marked by persistent and 
fluctuating organ system abnormalities. In children, distinct clinical and 
pathophysiological features of COVID-19 and long COVID are increasingly 
recognized, though knowledge remains limited relative to adults. The exponential 
expansion of the COVID-19 literature has made comprehensive appraisal by 
individual researchers increasingly unfeasible, highlighting the need for new 
approaches to evidence synthesis. Large language models (LLMs) such as the 
Generative Pre-trained Transformer (GPT) can process vast amounts of text, 
offering potential utility in this domain. Earlier versions of GPT, however, 
have been prone to generating fabricated references or misrepresentations of 
primary data. To evaluate the potential of more advanced models, we 
systematically applied GPT-4 to summarize studies on pediatric long COVID 
published between January 2022 and January 2025. Articles were identified in 
PubMed, and full-text PDFs were retrieved from publishers. GPT-4-generated 
summaries were cross-checked against the results sections of the original 
reports to ensure accuracy before incorporation into a structured review 
framework. This methodology demonstrates how LLMs may augment traditional 
literature review by improving efficiency and coverage in rapidly evolving 
fields, provided that outputs are subjected to rigorous human verification.

DOI: 10.3390/jcm14176011
PMCID: PMC12428973
PMID: 40943770

Conflict of interest statement: The authors declare no conflicts of interest.


199. Sensors (Basel). 2025 Sep 4;25(17):5497. doi: 10.3390/s25175497.

Fusing Residual and Cascade Attention Mechanisms in Voxel-RCNN for 3D Object 
Detection.

Lu Y(1), Zhang Y(1), Fan X(1), Cai D(2), Gong R(1).

Author information:
(1)School of Automation, Guangxi University of Science and Technology, Liuzhou 
545000, China.
(2)Liugong Machinery Co., Ltd., Liuzhou 545000, China.

In this paper, a high-precision 3D object detector-Voxel-RCNN-is adopted as the 
baseline detector, and an improved detector named RCAVoxel-RCNN is proposed. To 
address various issues present in current mainstream 3D point cloud voxelisation 
methods, such as the suboptimal performance of Region Proposal Networks (RPNs) 
in generating candidate regions and the inadequate detection of small-scale 
objects caused by overly deep convolutional layers in both 3D and 2D backbone 
networks, this paper proposes a Cascade Attention Network (CAN). The CAN is 
designed to progressively refine and enhance the proposed regions, thereby 
producing more accurate detection results. Furthermore, a 3D Residual Network is 
introduced, which improves the representation of small objects by reducing the 
number of convolutional layers while incorporating residual connections. In the 
Bird's-Eye View (BEV) feature extraction network, a Residual Attention Network 
(RAN) is developed. This follows a similar approach to the aforementioned 3D 
backbone network, leveraging the spatial awareness capabilities of the BEV. 
Additionally, the Squeeze-and-Excitation (SE) attention mechanism is 
incorporated to assign dynamic weights to features, allowing the network to 
focus more effectively on informative features. Experimental results on the 
KITTI validation dataset demonstrate the effectiveness of the proposed method, 
with detection accuracy for cars, pedestrians, and bicycles improving by 3.34%, 
10.75%, and 4.61%, respectively, under the KITTI hard level. The primary 
evaluation metric adopted is the 3D Average Precision (AP), computed over 40 
recall positions (R40). The Intersection over IoU thresholds used are 0.7 for 
cars and 0.5 for both pedestrians and bicycles.

DOI: 10.3390/s25175497
PMCID: PMC12431463
PMID: 40942926

Conflict of interest statement: Author Dengsheng Cai was employed by the company 
Liugong Machinery Co., Ltd. The remaining authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


200. Diagnostics (Basel). 2025 Sep 3;15(17):2235. doi: 10.3390/diagnostics15172235.

Multi-Class Classification of Breast Ultrasound Images Using Vision 
Transformer-Based Ensemble Learning.

Yıldırım TT(1), Yaman O(2), Kılıç İ(3), Taşar B(4), Timurkaan ES(1), Aydoğdu 
N(1).

Author information:
(1)Department of Internal Medicine, Fethi Sekin City Hospital, Elazig 23300, 
Türkiye.
(2)Department of Digital Forensics Engineering, Firat University, Elazig 23119, 
Türkiye.
(3)Faculty of Engineering, Department of Software Engineering, Firat University, 
Elazig 23119, Türkiye.
(4)Faculty of Engineering, Department of Mechatronics Engineering, Firat 
University, Elazig 23119, Türkiye.

Background/Objectives: In this study, a vision transformer (ViT) based ensemble 
architecture was developed for the classification of normal, benign, and 
malignant diseases from breast ultrasound images. The breast ultrasound images 
(BUSI) dataset was used for the implementation of the proposed method. This 
dataset includes 133 normal, 437 benign, and 210 malignant ultrasound images. 
Methods: ROI segmentation and image preprocessing were applied to the dataset to 
select only the tumor region and use it in the model. Thus, a better performance 
was achieved using only the lesion regions. Image augmentation was performed 
using the Albumentations library to increase the number of images. Feature 
extraction was performed on the obtained images using three ViT-based models 
(ViT-Base, DeiT, ViT-Small). The purpose of using three different models is to 
achieve high accuracy. The extracted features were classified using a multilayer 
perceptron (MLP). Training was performed using 10-fold stratified 
cross-validation. Results: The purpose of stratified cross-validation is to 
include a certain number of images from all three classes in each 
cross-validation proposed model provided 96.2% precision and 86.3% recall for 
the benign class and 92.9% recall and 76.4% precision for the malignant class. 
The normal class achieved 100% success. The area under the curve (AUC) values 
were 0.97, 0.96, and 1.00 for benign and malignant tumors, respectively, and 
1.00 for normal tumors. Conclusions: The ROI-based ViT + MLP + Ensemble 
architecture provided higher accuracy and explainability compared to traditional 
convolutional neural network (CNN) based methods in medical image 
classification. It demonstrated a stable success, especially in minority 
classes, and presented a potential, reliable, and flexible solution in clinical 
decision support systems.

DOI: 10.3390/diagnostics15172235
PMCID: PMC12428199
PMID: 40941725

Conflict of interest statement: The authors declare no conflicts of interest.


201. Nurs Outlook. 2025 Nov-Dec;73(6):102544. doi: 10.1016/j.outlook.2025.102544. 
Epub 2025 Sep 11.

The application of large language models in qualitative nursing research: A 
scoping review.

Zhou T(1), Xing M(2), Hu Y(3), Liang J(4), Ren Q(5), Yang Y(1), Ye L(6).

Author information:
(1)School of Nursing, Zhejiang Chinese Medicine University, Hangzhou, Zhejiang 
Province, China.
(2)School of Medical Technology and Information Engineering, Zhejiang Chinese 
Medicine University, Hangzhou, Zhejiang Province, China.
(3)School of Nursing, Southwest Medical University, Luzhou, Sichuan Province, 
China.
(4)Center for Health Policy Studies, School of Public Health, Zhejiang 
University, Hangzhou, Zhejiang Province, China.
(5)Department of Critical Care Medicine, Zhejiang Hospital, Hangzhou, Zhejiang 
Province, China.
(6)Department of Pulmonary and Critical Care Medicine, The Second Affiliated 
Hospital of Zhejiang Chinese Medical University, Hangzhou, Zhejiang Province, 
China. Electronic address: 20244066@zcmu.edu.cn.

BACKGROUND: Large language models (LLMs) provide significant potential benefits 
for nursing practice and research. Advanced natural language processing 
capabilities can effectively analyze text in qualitative studies. However, 
systematic exploration of their application contexts and efficacy in nursing 
remains limited.
PURPOSE: This review aimed to conduct a scoping review of LLMs' application in 
qualitative nursing research, exploring current scenarios, effects, assessment 
methods, and challenges to provide a reference for future development.
METHODS: Relevant literature was sourced from 11 databases up to April 2025 and 
used Joanna Briggs Scoping Review Methodology and PRISMA-ScR reporting 
standards. The search terms for this review included "nurs*," "large language 
model*," "qualitative study*," and "qualitative analys*."
FINDINGS: We included 11 studies after reviewing 2,478 articles. The application 
scenarios of LLMs in qualitative nursing research included topic generation, 
role-playing, and interview question generation. LLM outputs demonstrated 
moderate-to-high similarity to human outputs in theme generation and superior 
text analysis efficiency but performed poorly in applying theoretical 
frameworks, generating interview questions, and developing codebooks.
DISCUSSION: This review systematically outlined LLMs applications and 
limitations in qualitative nursing research. Although LLM has great potential, 
its application is still in its infancy.
CONCLUSION: Future research needs to address issues such as analysis depth, 
simulation accuracy, technical limitations, and evaluation tools.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.outlook.2025.102544
PMID: 40939244 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare no conflicts of interest.


202. JACC Adv. 2025 Oct;4(10 Pt 2):102141. doi: 10.1016/j.jacadv.2025.102141. Epub 
2025 Sep 11.

Enhancement of Stress ECG Performance with Machine Learning: A Single-Center 
Study.

Banerjee A(1), Salian RS(1), Vemulapalli HS(2), Sriramoju AK(2), Prajapati P(2), 
Rodriguez-Riascos JF(2), Muthu P(2), Iyengar SK(2), Shen W(2), Gupta SKS(1), 
Srivathsan K(3).

Author information:
(1)School of Computing and Augmented Intelligence, Arizona State University, 
Tempe, Arizona, USA.
(2)Division of Cardiovascular Diseases, Mayo Clinic Hospital, Phoenix, Arizona, 
USA.
(3)Division of Cardiovascular Diseases, Mayo Clinic Hospital, Phoenix, Arizona, 
USA. Electronic address: srivathsan.komandoor@mayo.edu.

BACKGROUND: Exercise stress electrocardiogram (ECG) (ESE) is a widely used, 
noninvasive diagnostic tool for detecting coronary artery disease (CAD). Despite 
its widespread use, the diagnostic accuracy of ESE remains suboptimal.
OBJECTIVES: This study aimed to develop and evaluate an artificial intelligence 
(AI) model, using a transformer-based architecture, to enhance the diagnostic 
performance ofESEs.
METHODS: Patients who underwent coronary angiography within 2 months of the ESE 
were eligible for inclusion. An AI model processed exercise stress ECG images 
into time-series data. A transformer-based architecture was employed to 
integrate temporal ECG features and predict CAD. Model performance in predicting 
severe CAD was first evaluated using 5-fold cross-validation on a test subset 
from the original cohort, and subsequently on a second validation cohort.
RESULTS: We developed a model using a total of 1,200 ECGs. An additional 
validation cohort of 91 patients was also analyzed. On the initial test subset, 
the AI model demonstrated a sensitivity of 93.6%, specificity of 93.2%, and 
overall accuracy of 93.4%. Notably, the model improved sensitivity with an 
absolute increase of 40.9% in women and 44.6% in men. In the second validation 
cohort, the model achieved an accuracy of 78%, with a sensitivity of 64.6% and a 
specificity of 93%.
CONCLUSIONS: This study presents a proof of concept demonstrating that an 
AI-based model for stress ECG interpretation is feasible and shows acceptable 
performance.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jacadv.2025.102141
PMCID: PMC12791844
PMID: 40939223

Conflict of interest statement: Funding support and author disclosures The 
authors have reported that they have no relationships relevant to the contents 
of this paper to disclose.


203. PLoS One. 2025 Sep 12;20(9):e0331962. doi: 10.1371/journal.pone.0331962. 
eCollection 2025.

New frontiers in radiologic interpretation: evaluating the effectiveness of 
large language models in pneumothorax diagnosis.

Bulut B(1), Öz MA(1), Genç M(2), Gür A(3), Yortanlı M(4), Yortanlı BÇ(5), 
Sariyildiz O(6), Yazıcı R(7), Mutlu H(8), Kotanoglu MS(9), Cinar E(10), Uykan 
Z(11).

Author information:
(1)Department of Emergency Medicine, Ankara Gulhane Training and Research 
Hospital, Health Science University, Ankara, Turkey.
(2)Department of Emergency Medicine, Ankara Training and Research Hospital, 
Ankara, Turkey.
(3)Department of Emergency Medicine, Etimesgut Şehit Sait Ertürk State Hospital, 
Ankara, Turkey.
(4)Department Of Emergency Medicine, Konya Numune Hospital, Konya, Turkey.
(5)Department of Internal Medicine, Konya City Hospital, University of Health 
Sciences, Konya, Turkey.
(6)Department of Anesthesıology And Reanımatıon Department, 75th Year Oral And 
Dental Health Hospital, Ankara, Turkey.
(7)Department of Emergency Medicine, Istanbul Kanuni Sultan Suleyman Training 
and Research Hospital, Health Science University, Istanbul, Turkey.
(8)Department of Emergency Medicine, Aksaray Training and Research Hospital, 
Aksaray University, Aksaray, Turkey.
(9)Department of Anesthesıology And Reanımatıon Department, Ankara Training and 
Research Hospital, Ankara, Turkey.
(10)Department of Thoracic Surgery, Bilkent City Hospital, University of Health 
Sciences, Ankara, Turkey.
(11)College of Engineering and Technology, American University of the Middle 
East, Kuwait.

BACKGROUND: This study evaluates the diagnostic performance of three multimodal 
large language models (LLMs)-ChatGPT-4o, Gemini 2.0, and Claude 3.5-in 
identifying pneumothorax from chest radiographs.
METHODS: In this retrospective analysis, 172 pneumothorax cases (148 patients 
aged >12 years, 24 patients aged ≤12 years) with both chest radiographs and 
confirmatory thoracic CT were included from a tertiary emergency department. 
Patients were categorized by age and pneumothorax size (small/large). Each 
radiograph was presented to all three LLMs accompanied by basic symptoms 
(dyspnea or chest pain), with each model analyzing each image three times. 
Diagnostic accuracy was evaluated using overall accuracy (all three responses 
correct), strict accuracy (≥2 responses correct), and ideal accuracy (≥1 
response correct), alongside response consistency assessment using Fleiss' 
Kappa.
RESULTS: In patients older than 12 years, ChatGPT-4o demonstrated the highest 
overall accuracy (69.6%), followed by Claude 3.5 (64.9%) and Gemini 2.0 (57.4%). 
Performance was significantly poorer in pediatric patients across all models 
(20.8%, 12.5%, and 20.8%, respectively). For large pneumothorax in adults, 
ChatGPT-4o showed significantly higher accuracy compared to small pneumothorax 
(81.6% vs. 42.2%; p < 0.001). Regarding consistency, Gemini 2.0 demonstrated 
excellent reliability for large pneumothorax (Kappa = 1.00), while Claude 3.5 
showed moderate consistency across both pneumothorax sizes.
CONCLUSION: This study, the first to evaluate these three current multimodal 
LLMs in pneumothorax identification across different age groups, demonstrates 
promising results for potential clinical applications, particularly for adult 
patients with large pneumothorax. However, performance limitations in pediatric 
cases and with small pneumothoraces highlight the need for further validation 
before clinical implementation.

Copyright: © 2025 Bulut et al. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0331962
PMCID: PMC12431401
PMID: 40938938 [Indexed for MEDLINE]

Conflict of interest statement: No authors have competing interests.


204. Acad Radiol. 2025 Dec;32(12):7294-7306. doi: 10.1016/j.acra.2025.08.054. Epub 
2025 Sep 10.

A Fusion Model of ResNet and Vision Transformer for Efficacy Prediction of HIFU 
Treatment of Uterine Fibroids.

Zhou Y(1), Xu H(2), Jiang W(3), Zhang J(4), Chen S(5), Yang S(6), Xiang H(3), Hu 
W(2), Qiao X(2).

Author information:
(1)Department of Radiology, the Second Affiliated Hospital of Chongqing Medical 
University, Yuzhong 400010, Chongqing, China (Y.Z., H.X., W.H., X.Q.); 
Bioengineering College, Chongqing University, Chongqing 400044, China (Y.Z.). 
Electronic address: 15736268267@163.com.
(2)Department of Radiology, the Second Affiliated Hospital of Chongqing Medical 
University, Yuzhong 400010, Chongqing, China (Y.Z., H.X., W.H., X.Q.).
(3)Department of Chinese Traditional Medicine and Rehabilitation, Chongqing 
Emergency Medical Center, Yuzhong 400014, Chongqing, China (W.J., H.X.).
(4)Department of Ultrasound, Women and Children's Hospital of Chongqing Medical 
University, Chongqing 401147, China (J.Z.).
(5)Department of Medical Equipment, People's Hospital Of Chongqing Liangjiang 
New Area, Chongqing 401147, China (S.C.).
(6)Department of Radiology, Women and Children's Hospital of Chongqing Medical 
University, Chongqing 401147, China (S.Y.).

RATIONALE AND OBJECTIVES: High-intensity focused ultrasound (HIFU) is a 
non-invasive technique for treating uterine fibroids, and the accurate 
prediction of its therapeutic efficacy depends on precise quantification of the 
intratumoral heterogeneity. However, existing methods still have limitations in 
characterizing intratumoral heterogeneity, which restricts the accuracy of 
efficacy prediction. To this end, this study proposes a deep learning model with 
a parallel architecture of ResNet and ViT (Res-ViT) to verify whether the 
synergistic characterization of local texture and global spatial features can 
improve the accuracy of HIFU efficacy prediction.
MATERIALS AND METHODS: This study enrolled patients with uterine fibroids who 
underwent HIFU treatment from Center A (training set: N = 272; internal 
validation set: N = 92) and Center B (external test set: N = 125). Preoperative 
T2-weighted magnetic resonance images were used to develop the Res-ViT model for 
predicting immediate post-treatment non-perfused volume ratio (NPVR) ≥ 80%. 
Model performance was evaluated using the area under the receiver operating 
characteristic curve (AUC) and compared against independent Radiomics, 
ResNet-18, and ViT models.
RESULTS: The Res-ViT model outperformed all standalone models across both 
internal (AUC = 0.895, 95% CI: 0.857-0.987) and external (AUC = 0.853, 95% CI: 
0.776-0.921) test sets. SHAP analysis identified the ResNet branch as the 
predominant decision-making component (feature contribution: 55.4%). The 
visualization of Gradient-weighted Class Activation Mapping (Grad-CAM) shows 
that the key regions attended by Res-ViT have higher spatial overlap with the 
postoperative non-ablated fibroid tissue.
CONCLUSION: The proposed Res-ViT model demonstrates that the fusion strategy of 
local and global features is an effective method for quantifying uterine fibroid 
heterogeneity, significantly enhancing the accuracy of HIFU efficacy prediction.

Copyright © 2025 The Association of University Radiologists. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.acra.2025.08.054
PMID: 40935773 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


205. J Imaging Inform Med. 2025 Sep 10. doi: 10.1007/s10278-025-01659-4. Online ahead 
of print.

Implementing a Resource-Light and Low-Code Large Language Model System for 
Information Extraction from Mammography Reports: A Pilot Study.

Dennstädt F(1)(2), Fauser S(3), Cihoric N(3), Schmerder M(3)(4), Lombardo P(4), 
Cereghetti GM(4), von Däniken S(4), Minder T(4), Meyer J(4), Chiang L(4), Gaio 
R(3), Lerch L(4), Filchenko I(5), Reichenpfader D(6)(7), Denecke K(6), Vojvodic 
C(8), Tatalovic I(8), Sander A(9), Hastings J(10)(11)(12), Aebersold DM(3), von 
Tengg-Kobligk H(4), Nairz K(4).

Author information:
(1)Department of Radiation Oncology, Inselspital, Bern University Hospital and 
University of Bern, Bern, Switzerland. fabio.dennstaedt@insel.ch.
(2)School of Medicine, University of St. Gallen, St. Gallen, Switzerland. 
fabio.dennstaedt@insel.ch.
(3)Department of Radiation Oncology, Inselspital, Bern University Hospital and 
University of Bern, Bern, Switzerland.
(4)Department of Diagnostic, Interventional and Pediatric Radiology (DIPR), 
Inselspital, Bern University Hospital and University of Bern, Bern, Switzerland.
(5)Department of Neurology, Inselspital, Bern University Hospital and University 
of Bern, Bern, Switzerland.
(6)Institute for Patient-Centered Digital Health, Bern University of Applied 
Sciences, Biel/Bienne, Switzerland.
(7)Faculty of Medicine, University of Geneva, Geneva, Switzerland.
(8)Wemedoo AG, Steinhausen, Switzerland.
(9)ID Berlin GmbH, Berlin, Germany.
(10)School of Medicine, University of St. Gallen, St. Gallen, Switzerland.
(11)Institute for Implementation Science in Health Care, University of Zurich, 
Zurich, Switzerland.
(12)Swiss Institute of Bioinformatics, Lausanne, Switzerland.

Large language models (LLMs) have been successfully used for data extraction 
from free-text radiology reports. Most current studies were conducted with LLMs 
accessed via an application programming interface (API). We evaluated the 
feasibility of using open-source LLMs, deployed on limited local hardware 
resources for data extraction from free-text mammography reports, using a common 
data element (CDE)-based structure. Seventy-nine CDEs were defined by an 
interdisciplinary expert panel, reflecting real-world reporting practice. 
Sixty-one reports were classified by two independent researchers to establish 
ground truth. Five different open-source LLMs deployable on a single GPU were 
used for data extraction using the general-classifier Python package. 
Extractions were performed for five different prompt approaches with calculation 
of overall accuracy, micro-recall and micro-F1. Additional analyses were 
conducted using thresholds for the relative probability of classifications. High 
inter-rater agreement was observed between manual classifiers (Cohen's kappa 
0.83). Using default prompts, the LLMs achieved accuracies of 59.2-72.9%. 
Chain-of-thought prompting yielded mixed results, while few-shot prompting led 
to decreased accuracy. Adaptation of the default prompts to precisely define 
classification tasks improved performance for all models, with accuracies of 
64.7-85.3%. Setting certainty thresholds further improved accuracies to > 90% 
but reduced the coverage rate to < 50%. Locally deployed open-source LLMs can 
effectively extract information from mammography reports, maintaining 
compatibility with limited computational resources. Selection and evaluation of 
the model and prompting strategy are critical. Clear, task-specific instructions 
appear crucial for high performance. Using a CDE-based framework provides clear 
semantics and structure for the data extraction.

© 2025. The Author(s).

DOI: 10.1007/s10278-025-01659-4
PMID: 40931247

Conflict of interest statement: Declarations. Ethics Approval: Ethics approval 
for the study was granted by the Ethics Committee of the Canton of Bern (BASEC 
number 2022–01621), in alignment with the principles outlined in the Declaration 
of Helsinki. Consent to Participate: The data utilized in this study comprised 
anonymized mammography reports. All patients provided informed consent for the 
use of their data for research purposes, in accordance with ethical standards. 
Conflict of interest: Dr. Cihoric is a technical lead for the SmartOncology 
project and a medical advisor for Wemedoo AG, Steinhausen AG, Switzerland. The 
authors declare no other conflicts of interest.


206. J Pediatr Surg. 2025 Dec;60(12):162654. doi: 10.1016/j.jpedsurg.2025.162654. 
Epub 2025 Sep 8.

Designing Patient-Centered Communication Aids in Pediatric Surgery Using Large 
Language Models.

Rao AS(1), Mazumder A(2), Roux E(1), Young C(1), Bott E(1), Wang J(1), Kochis 
M(3), Stetson A(3), Butler A(4), Hilker S(4), Succi MD(5).

Author information:
(1)Harvard Medical School, Boston, MA, United States; Medically Engineered 
Solutions in Healthcare Incubator, Innovation in Operations Research Center, 
Mass General Brigham, Boston, MA, United States.
(2)Harvard College, Harvard University, Cambridge, MA, United States.
(3)Department of Surgery, Massachusetts General Hospital, Boston, MA, United 
States.
(4)Harvard Medical School, Boston, MA, United States; Boston Children's 
Hospital, Boston, MA, United States.
(5)Harvard Medical School, Boston, MA, United States; Medically Engineered 
Solutions in Healthcare Incubator, Innovation in Operations Research Center, 
Mass General Brigham, Boston, MA, United States. Electronic address: 
msucci@mgh.harvard.edu.

INTRODUCTION: Large language models (LLMs) have been shown to translate 
information from highly specific domains into lay-digestible terms. Pediatric 
surgery remains an area in which it is difficult to communicate clinical 
information in an age-appropriate manner, given the vast diversity in language 
comprehension levels across patient populations and the complexity of procedures 
performed. This study evaluates LLMs as tools for generating explanations of 
common pediatric surgeries to increase efficiency and quality of communication.
METHODS: Two generalist LLMs (GPT-4-turbo [OpenAI] and Gemini 1.0 Pro [Google]; 
accessed March 2024) were provided the following prompt: "Act as a pediatric 
surgeon and explain a [PROCEDURE] to a [AGE] old [GENDER] in age-appropriate 
language. Discuss indications for the procedure, steps of the procedure, 
possible complications, and post-operative recovery." Responses were generated 
for 4 common pediatric surgeries (appendectomy, umbilical hernia repair, 
cholecystectomy, and gastrostomy tube placement) for male and female children of 
ages 5, 8, 10, 13, and 16 years. Forty responses from each LLM were rated for 
accuracy, completeness, age-appropriateness, possibility of demographic bias, 
and overall quality by two pediatricians and two general surgeons using a 
five-point Likert scale. Numeric ratings were summarized as means and 95 % 
confidence intervals. An ordinal mixed-effects model with rater as a random 
effect was used to account for clustering by rater. P < 0.05 was considered 
statistically significant.
RESULTS: Responses from GPT-4-turbo and Gemini 1.0 Pro models were both rated 
with moderately high overall quality (GPT4: 3.97 [3.82, 4.12]; Gemini 1.0 Pro: 
3.39 [3.20, 3.57]) and moderately low possibility of demographic bias (GPT4: 
2.49 [2.38, 2.60]; Gemini 1.0 Pro: 2.93 [2.79, 3.07]). GPT-4-turbo responses 
were rated as highly accurate (4.18 [4.05, 4.32]), highly complete (4.21 [4.10, 
4.33]), and highly age-appropriate (4.10 [3.96, 4.24]), while Gemini 1.0 Pro 
responses were rated as moderately accurate (3.83 [3.70, 3.96]), moderately 
complete (3.95 [3.83, 4.07]) and moderately age-appropriate (3.63 [3.47, 3.79]). 
With GPT-4-turbo, ratings on most measures tend to improve as patient age 
increases, whereas with Gemini 1.0 Pro, they tend to worsen as patient age 
increases. Ratings on all measures, with the exception of age-appropriateness, 
were slightly higher for responses generated for male patients as compared to 
female patients with GPT-4-turbo, while the gender differences were less 
pronounced with Gemini 1.0 Pro.
DISCUSSION: This study demonstrates that off-the-shelf LLMs have the potential 
to produce accurate, complete, and age-appropriate explanations of common 
pediatric surgeries with low possibility of demographic bias. Inter-model 
variability in areas such as quality of response, age-appropriateness and gender 
differences were also observed, signaling the need for additional validation and 
fine-tuning based on the clinical content. Such tools could be implemented at 
the point of care or in other patient education settings and personalized to 
ensure effective, equitable communication of pertinent medical information with 
demonstration of clinician-rated content quality.
STUDY TYPE: This is a pilot study evaluating the performance of large language 
models (LLMs) as patient-centered communication aids in pediatric surgery.
LEVEL OF EVIDENCE: Level IV (pilot study).

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jpedsurg.2025.162654
PMID: 40930385 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare no conflicts of interest.


207. J Med Internet Res. 2025 Sep 10;27:e71592. doi: 10.2196/71592.

Biological Age Estimation From the Age Gap Using Deep Learning Integrating 
Morbidity and Mortality: Model Development and Validation Study.

Moon SE(#)(1), Yoon JW(#)(2)(3), Bae JH(#)(3)(4), Joo S(1), Kim YH(4), Lee 
BH(4), Yoon S(5), Yoo H(#)(1), Cho YM(#)(3)(4).

Author information:
(1)NAVER AI Lab, Seongnam, Republic of Korea.
(2)Division of Endocrinology and Metabolism, Department of Internal Medicine, 
Seoul National University Hospital Healthcare System Gangnam Center, Seoul, 
Republic of Korea.
(3)Department of Internal Medicine, Seoul National University College of 
Medicine, 101 Daehak-ro, Jongno-gu, Seoul, 03080, Republic of Korea, 82 
0220721965.
(4)Division of Endocrinology and Metabolism, Department of Internal Medicine, 
Seoul National University Hospital, Seoul, Republic of Korea.
(5)DaNaA Data, Seoul, Republic of Korea.
(#)Contributed equally

BACKGROUND: Biological age (BA) is increasingly recognized as a valuable 
alternative to chronological age (CA) for assessing an individual's health and 
aging status. However, existing models are based on limited clinical parameters 
and have not thoroughly integrated morbidity and mortality data.
OBJECTIVE: This study aimed to develop and validate a novel transformer-based 
model, referred to as the BA - CA gap model, for BA estimation that incorporates 
morbidity and mortality information to improve predictive accuracy and enhance 
clinical use in the early identification of the risk of age-related diseases.
METHODS: We retrospectively analyzed data from 151,281 adults aged 18 years or 
older who underwent routine health checkups between 2003 and 2020. Participants 
were classified into normal, predisease, and disease groups based on 
comorbidities (diabetes mellitus, hypertension, and dyslipidemia) to evaluate 
the model's ability to discriminate health status along a clinically relevant 
spectrum. Variables with less than 50% missingness had missing values imputed 
using the mean, while features with 50% or more missingness were excluded. We 
develop a custom transformer architecture that learns multiple objectives 
simultaneously, including input feature reconstruction, BA and CA alignment, 
health status discrimination, and mortality prediction. Model training used 
unsupervised and self-supervised strategies. We compared our model's performance 
with conventional BA estimation approaches, including Klemera and Doubal's 
method, a CA cluster-based model, and a deep neural network, by examining BA gap 
distributions, health status stratification, and mortality prediction.
RESULTS: The proposed BA - CA gap model provided a more accurate reflection of 
health status and superior stratification of mortality risk than existing 
methods. The model effectively distinguished among normal, predisease, and 
disease groups, with a clear gradient of BA gap values. Kaplan-Meier analyses 
demonstrated stronger discrimination of future mortality in men, while a similar 
but not statistically significant trend was observed in women. Sensitivity 
analyses across multiple random splits and training subsets confirmed the 
robustness of the model's performance.
CONCLUSIONS: By integrating morbidity and mortality information within a 
transformer-based framework, the BA - CA gap model offers a more granular and 
clinically meaningful assessment of aging and health status than CA alone. This 
approach supports the potential for personalized health management and risk 
stratification, although external validation in diverse populations is warranted 
to further confirm its generalizability.

© Seong-Eun Moon, Ji Won Yoon, Jae Hyun Bae, Shinyoung Joo, Yoo Hyung Kim, Bon 
Hyang Lee, Seokho Yoon, Haanju Yoo, Young Min Cho. Originally published in the 
Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/71592
PMCID: PMC12422742
PMID: 40930058 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: Seoul National University 
and NAVER have pending patents related to this work. JWY, YHK, and YMC are 
employees of Seoul National University Hospital. YMC serves as an independent 
director of Daewoong Pharmaceutical Co, Ltd. SEM, SJ, and HY are employees and 
shareholders of NAVER. All other authors declare no conflicts of interest.


208. J Palliat Med. 2025 Dec;28(12):1654-1659. doi: 10.1177/10966218251376436. Epub 
2025 Sep 9.

Evaluating the Clinical Reasoning of Generative AI in Palliative Care: A 
Comparison with Five Years of Pharmacy Learners.

Lane MT(1), Ajayi TA(2)(3)(4)(5), Edmonds KP(2)(4)(5), Atayee RS(1)(2)(4)(5)(6).

Author information:
(1)Skaggs School of Pharmacy & Pharmaceutical Sciences, UC San Diego Health 
Sciences, San Diego, California, USA.
(2)Division of Geriatrics, Gerontology, & Palliative Care, Department of 
Medicine, UC San Diego Health Sciences, San Diego, California, USA.
(3)Division of Pediatric Palliative Care, Rady Children's Hospital & Medical 
Center, San Diego, California, USA.
(4)Joan & Irwin Jacobs Center for Health Innovation, UC San Diego Health, San 
Diego, California, USA.
(5)Palliative Care Program, UC San Diego Health, San Diego, California, USA.
(6)Department of Pharmacy, UC San Diego Health, San Diego, California, USA.

Context: Artificial intelligence (AI), particularly large language models 
(LLMs), offers the potential to augment clinical decision-making, including in 
palliative care pharmacy, where personalized treatment and assessments are 
important. Despite the growing interest in AI, its role in clinical reasoning 
within specialized fields such as palliative care remains uncertain. Objectives: 
This study examines the performance of four commercial-grade LLMs on a Script 
Concordance Test (SCT) designed for pharmacy students in a pain and palliative 
care elective, comparing AI outputs with human learners' performance at 
baseline. Methods: Pharmacy students from 2018 to 2023 completed an SCT 
consisting of 16 clinical questions. Four LLMs (ChatGPT 3.5, ChatGPT 4.0, 
Gemini, and Gemini Advanced) were tested using the same SCT, with their 
responses compared to student performance. Results: The average score for LLMs 
(0.43) was slightly lower than that of students (0.47), but this difference was 
not statistically significant (p = 0.55). ChatGPT 4.0 achieved the highest score 
(0.57). Conclusions: While LLMs show potential for augmenting clinical 
decision-making, their limitations in patient-centered care highlight the 
necessity of human oversight and reinforce that they cannot replace human 
expertise in palliative care. This study was conducted in a controlled research 
setting, where LLMs were prompted to answer clinical reasoning questions despite 
default safety restrictions. However, this does not imply that such prompts 
should be used in practice. Future research should explore alternative methods 
for assessing AI decision-making without overriding safety mechanisms and focus 
on refining AI to better align with complex clinical reasoning. In addition, 
further studies are needed to confirm AI's comparative effectiveness, given the 
sample size limitations.

DOI: 10.1177/10966218251376436
PMID: 40929006 [Indexed for MEDLINE]


209. J Allergy Clin Immunol. 2025 Sep 7:S0091-6749(25)00939-X. doi: 
10.1016/j.jaci.2025.08.022. Online ahead of print.

Artificial intelligence in allergy and immunology: Recent developments, 
implementation challenges, and the road toward clinical impact.

van Breugel M(1), Greenhawt M(2), Eguiluz-Gracia I(3), Torres Jaén MJ(3), 
Anagnostou A(4), Koppelman GH(5).

Author information:
(1)Department of Pediatric Pulmonology and Pediatric Allergology, Beatrix 
Children's Hospital, University Medical Center Groningen, University of 
Groningen, Groningen, The Netherlands; Groningen Research Institute for Asthma 
and COPD (GRIAC), University Medical Center Groningen, University of Groningen, 
Groningen, The Netherlands; Ditto Care, Rotterdam, The Netherlands. Electronic 
address: m.van.breugel@umcg.nl.
(2)Department of Pediatrics, Section of Allergy and Immunology, Children's 
Hospital Colorado, University of Colorado School of Medicine, Aurora, Colo.
(3)Allergy Unit, IBIMA-BIONAND, Regional University Hospital of Malaga, Malaga 
University, Malaga, Spain.
(4)Division of Allergy, Immunology and Retrovirology, Baylor College of 
Medicine, Houston, Tex.
(5)Department of Pediatric Pulmonology and Pediatric Allergology, Beatrix 
Children's Hospital, University Medical Center Groningen, University of 
Groningen, Groningen, The Netherlands; Groningen Research Institute for Asthma 
and COPD (GRIAC), University Medical Center Groningen, University of Groningen, 
Groningen, The Netherlands.

Artificial intelligence (AI) is increasingly recognized for its capacity to 
transform medicine. While publications applying AI in allergy and immunology 
have increased in number, clinical implementation substantially lags behind 
other specialties. By mid-2024, over 1,000 US Food and Drug 
Administration-approved AI-enabled medical devices existed, but none 
specifically addressed allergy and immunology. This gap partly reflects the 
field's limited reliance on imaging, which facilitated early AI breakthroughs in 
radiology and pathology. This narrative review examines recent AI developments, 
including large language models and AI agents, evaluating their applicability to 
allergy and immunology practice. We analyze current and potential applications, 
emphasizing those demonstrating clinical value while identifying implementation 
barriers amplified by allergic diseases' unique complexities, including data 
privacy concerns, bias, reliability constraints, and evolving regulatory 
frameworks. To bridge the persistent research-to-implementation gap, we propose 
a 6-point road map: (1) prioritize impactful applications, (2) define clinically 
relevant benchmarks, (3) enforce rigorous governance, (4) transition to 
operationalization, (5) promote clinical adoption through trustworthy AI, and 
(6) establish life cycle management. This road map builds on established 
implementation frameworks while incorporating critical field-specific 
considerations unique to allergy and immunology. Through this approach, we 
provide a perspective for advancing AI in allergy and immunology from academic 
promise to tangible clinical benefit.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jaci.2025.08.022
PMID: 40925528

Conflict of interest statement: Disclosure statement Disclosure of potential 
conflict of interest: M. van Breugel is founder of and owns stock in Ditto Care. 
M. Greenhawt is a consultant for Aquestive; is a member of physician/medical 
advisory boards for DBV Technologies, Takeda, Griffols, Nutricia, Novartis, 
Aquestive, Allergy Therapeutics, AstraZeneca, ALK-Abello, Bryn, Genentech, and 
Prota; is a speaker for Genentech and ARS; receives research funding from the 
Asthma and Allergy Foundation of America; is an unpaid member of the scientific 
advisory council for the National Peanut Board and medical advisory board of the 
International Food Protein Induced Enterocolitis Syndrome Association; is a 
member of the Brighton Collaboration Criteria Vaccine Anaphylaxis 2.0 working 
group; is senior associate editor for Annals of Allergy, Asthma & Immunology; is 
a member of the Joint Taskforce on Allergy Practice Parameters; and has received 
honoraria for lectures from Red Nucleus, Medscape, Paradigm Medical 
Communications, RMEI, PSL, Kaplan, Food Allergy Research and Education, as well 
as multiple state/local allergy societies. M. J. Torres Jaén reports, outside 
the submitted work, personal fees from Diater, Aimmune Therapeutics, and Leti 
laboratories; and grants from European Commission, MINECO and ISCIII of the 
Spanish government and SEAIC. A. Anagnostou reports institutional funding from 
Novartis, AAFA, Aquestive, ALK; advisory board member of Novartis, Bryn, 
Aquestive, and ALK; and consultation/speaker fees from ALK, EPG Health, MJH, 
Adelphi, Aimmune Therapeutics, Genentech, Medscape, ARS, FARE, and Stallergens. 
G. H. Koppelman reports institutional grant funding from the Dutch Lung 
Foundation, ZON-MW (VICI grant), Ubbo Emmius Foundation, European Union 
(Prominent grant), Vertex, TEVA The Netherlands, and GSK; fees to his 
institution for consultancy to AZ, Pure IMS, and GSK; and invited speaker for 
Sanofi, AstraZeneca, and Boehringer-Ingelheim, all outside the submitted work; 
and is cofounder and chair of the exquAIro Foundation (www.exquAiro.com), aimed 
at educating biomedical professionals to support medical breakthroughs with AI 
in medicine. The other author declares no relevant conflicts of interest.


210. Front Med (Lausanne). 2025 Aug 22;12:1600855. doi: 10.3389/fmed.2025.1600855. 
eCollection 2025.

Transformer-based ECG classification for early detection of cardiac arrhythmias.

Ikram S(1), Ikram A(2), Singh H(3), Ali Awan MD(1), Naveed S(1), De la Torre 
Díez I(4), Gongora HF(5), Candelaria Chio Montero T(6).

Author information:
(1)Department of Software Engineering, The Islamia University of Bahawalpur, 
Bahawalpur, Pakistan.
(2)Faculty of Computing, The Govt Sadiq College Women University, Bahawalpur, 
Pakistan.
(3)Department of Mechanical Engineering, Chandigarh Group of Colleges, Landran, 
Mohali, Punjab, India.
(4)Department of Signal Theory and Communications and Telematics Engineering, 
University of Valladolid, Valladolid, Spain.
(5)Universidad Internacional Iberoamericana, Campeche, Mexico.
(6)Universidad Internacional Iberoamericana, Arecibo, PR, United States.

Electrocardiogram (ECG) classification plays a critical role in early detection 
and trocardiogram (ECG) classification plays a critical role in early detection 
and monitoring cardiovascular diseases. This study presents a Transformer-based 
deep learning framework for automated ECG classification, integrating advanced 
preprocessing, feature selection, and dimensionality reduction techniques to 
improve model performance. The pipeline begins with signal preprocessing, where 
raw ECG data are denoised, normalized, and relabeled for compatibility with 
attention-based architectures. Principal component analysis (PCA), correlation 
analysis, and feature engineering is applied to retain the most informative 
features. To assess the discriminative quality of the selected features, 
t-distributed stochastic neighbor embedding (t-SNE) is used for visualization, 
revealing clear class separability in the transformed feature space. The refined 
dataset is then input to a Transformer- based model trained with optimized loss 
functions, regularization strategies, and hyperparameter tuning. The proposed 
model demonstrates strong performance on the MIT-BIH benchmark dataset, showing 
results consistent with or exceeding prior studies. However, due to differences 
in datasets and evaluation protocols, these comparisons are indicative rather 
than conclusive. The model effectively classifies ECG signals into categories 
such as Normal, atrial premature contraction (APC), ventricular premature 
contraction (VPC), and Fusion beats. These results underscore the effectiveness 
of Transformer-based models in biomedical signal processing and suggest 
potential for scalable, automated ECG diagnostics. However, deployment in 
real-time or resource-constrained settings will require further optimization and 
validation.

Copyright © 2025 Ikram, Ikram, Singh, Ali Awan, Naveed, De la Torre Díez, 
Gongora and Candelaria Chio Montero.

DOI: 10.3389/fmed.2025.1600855
PMCID: PMC12411431
PMID: 40917829

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


211. Asian J Psychiatr. 2025 Oct;112:104695. doi: 10.1016/j.ajp.2025.104695. Epub 
2025 Sep 4.

Diagnostic and transition accuracy of natural language processing in high risk 
for psychosis individuals: A systematic review.

García-Molina JT(1), Downey M(2), Méndez E(3), Figueroa-Barra A(4).

Author information:
(1)Department of Psychiatry and Mental Health, Faculty of Medicine, Universidad 
de Chile, Santiago, Chile; Translational Psychiatry Laboratory (Psiquislab), 
Faculty of Medicine, Universidad de Chile, Santiago, Chile; Department of 
Neuroscience, Faculty of Medicine, Universidad de Chile, Santiago, Chile. 
Electronic address: josegarcia@ug.uchile.cl.
(2)Department of Neuroscience, Faculty of Medicine, Universidad de Chile, 
Santiago, Chile.
(3)Department of Psychiatry and Mental Health, Faculty of Medicine, Universidad 
de Chile, Santiago, Chile; Translational Psychiatry Laboratory (Psiquislab), 
Faculty of Medicine, Universidad de Chile, Santiago, Chile; Doctoral Program in 
Psychotherapy, Pontificia Universidad Católica de Chile and Universidad de 
Chile, Santiago, Chile; Millennium Nucleus to Improve the Mental Health of 
Adolescents and Youths (IMHAY), Santiago, Chile.
(4)Department of Psychiatry and Mental Health, Faculty of Medicine, Universidad 
de Chile, Santiago, Chile; Translational Psychiatry Laboratory (Psiquislab), 
Faculty of Medicine, Universidad de Chile, Santiago, Chile; Millennium Nucleus 
to Improve the Mental Health of Adolescents and Youths (IMHAY), Santiago, Chile; 
Department of Language and Literature, Faculty of Philosophy and Humanities, 
Universidad Alberto Hurtado, Santiago, Chile. Electronic address: 
aliciafigueroa@uchile.cl.

BACKGROUND: Schizophrenia spectrum disorders often emerge in adolescence or 
early adulthood and are a leading cause of global disability. Early 
identification of clinical high‑risk for psychosis (CHR‑P) can reduce 
comorbidity and shorten untreated psychosis duration, yet clinician‑administered 
tools (e.g., SIPS/SOPS, CAARMS, PSYCHS) are time‑consuming and only moderately 
predictive.
OBJECTIVE: To systematically review the diagnostic and prognostic accuracy of 
natural language processing (NLP) applied to speech in CHR‑P populations, and to 
map methodological trends and gaps.
METHODS: We searched PubMed, Scopus, and Embase through May 2025 for English or 
Spanish studies enrolling CHR‑P individuals by validated criteria, applying NLP 
to speech transcripts, and reporting quantitative metrics (accuracy, 
sensitivity, specificity, AUC‑ROC). Two reviewers independently screened 
studies, extracted data, and assessed bias with QUADAS‑2; disagreements were 
resolved by a third reviewer.
RESULTS: Results: From 356 records, nine studies (eight unique cohorts; N = 353 
CHR-P, 197 controls) met inclusion. Four case-control studies and one 
prospective cohort assessed cross-sectional discrimination of CHR-P from healthy 
controls, reporting accuracies of 56-95 % (AUC-ROC 0.86-0.99). Four prospective 
studies examined transition prediction, with accuracies ranging from 83 % to 
100 %. Studies covered five languages and employed diverse NLP pipelines (e.g., 
LSA, Word2Vec, USE, SBERT, graph metrics, sentiment analysis). However, feature 
heterogeneity, small samples (≤ 50 CHR-P), varied speech tasks, and inconsistent 
validation limited comparability.
CONCLUSIONS: NLP‑based speech analysis shows promise as an objective biomarker 
for early psychosis detection and risk stratification. To advance clinical 
utility, future research should adopt standardized protocols, recruit larger and 
more diverse cohorts, and implement multicenter validation.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ajp.2025.104695
PMID: 40915240 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that there are no conflicts of interest regarding the publication of 
this paper.


212. Nurse Educ Pract. 2025 Oct;88:104529. doi: 10.1016/j.nepr.2025.104529. Epub 2025 
Sep 3.

Evaluating an AI-VR escape room for disaster nursing education: A 
quasi-experimental study.

Xiong K(1), Li J(2), Yang L(3), Zhao P(4), Zhang Y(5), Feng Z(6), Kyle RG(7).

Author information:
(1)Department of Pediatric Surgery, Chengdu Women and Children's Central 
Hospital, Affiliated Hospital of University of Electronic Science and Technology 
of China, Chengdu, China.
(2)Delivery Room, Chengdu Women and Children's Central Hospital, Chengdu, China.
(3)Department of Pediatric Surgery, Chengdu Women and Children's Central 
Hospital, Affiliated Hospital of University of Electronic Science and Technology 
of China, Chengdu, China. Electronic address: yanglili197207@163.com.
(4)Intensive Care Unit, Chengdu Public Health Clinical Medical Center, Chengdu, 
China.
(5)Cardiac Intensive Care Unit (CICU), Shanghai Children's Medical Center, 
Shanghai, China.
(6)Geriatric Orthopedics, Sichuan Provincial Orthopedics Hospital, Chengdu, 
China.
(7)University of Exeter, Interim Head, Academy of Nursing, Exeter, United 
Kingdom.

AIM: This study aims to assess the acceptance of a VR-based disaster emergency 
nursing escape room teaching method among nurses and midwives and to explore the 
main factors influencing their acceptance.
BACKGROUND: The increasing frequency of natural disasters due to global climate 
change poses a significant threat to human health. Effective training for nurses 
and midwives is critical as they are frontline responders in disaster relief.
DESIGN: A quasi-experimental pretest-posttest design was used with a non-random 
sample to assess changes in acceptance and identify influencing factors. The 
study followed the TREND guidelines.
METHODS: Questionnaires (n = 281) were distributed across four types of medical 
centers in China (obstetrics, public health, pediatrics, orthopedics) using 
targeted sampling. After analyzing valid responses (n = 247), participants with 
scores below the average (3.67) were selected (n = 160) for in-depth interviews. 
Data were analyzed using NVivo 11.0. A novel quasi-experimental project was 
developed that pioneered the integration of AI-driven narrative design (ChatGPT) 
and immersive VR production tools (Insta360 X3, Blender, After Effects) to 
create a disaster-themed escape room, deployed via an interactive online 
platform (Bilibili).Following the intervention, participants completed the 
System Usability Scale (SUS) and a post-intervention acceptance questionnaire.
RESULTS: The intervention significantly increased participants' acceptance, with 
scores rising from 3.24 to 4.29, highlighting both its educational value and its 
feasibility for real-world training integration.
CONCLUSION: The AI+VR-based escape room teaching method enhances disaster 
response capabilities and offers a new direction for nursing education. Further 
optimization of content and technical support will improve practical 
application.

Copyright © 2025 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.nepr.2025.104529
PMID: 40915079 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The author(s) 
declare that there are no conflicts of interest related to the submission of 
this manuscript entitled "A Survey on the Acceptance of AI Model-Based VR 
Disaster Emergency Nursing Escape Room Teaching Method among Nurses and 
Midwives: A Quasi-Experimental Study". There are no financial or non-financial 
interests that could influence the objectivity or integrity of the research. The 
author(s) have not received any funding, grants, or sponsorships from any 
organizations or individuals that may have a direct or indirect interest in the 
content of this article. The manuscript has not been published elsewhere, nor is 
it under consideration for publication by any other journal.


213. Clin Pediatr (Phila). 2026 Jan;65(1):11-21. doi: 10.1177/00099228251370556. Epub 
2025 Sep 7.

Exploring Early Perceptions and Experiences of ChatGPT in Pediatric Critical 
Care: A Qualitative Study Among Health Care Professionals.

Temsah MH(1)(2)(3), Abouammoh N(1)(4), Alsatrawi M(2), Al-Jelaify M(1)(2)(5), 
Altamimi I(1), ChatGPT Research Consortium P(1)(2)(3)(4)(6)(7)(8), Alhasan 
K(1)(9), Malki KH(1)(10), Al-Tawfiq JA(11)(12)(13), Al-Eyadhy A(1)(2).

Author information:
(1)College of Medicine, King Saud University, Riyadh, Saudi Arabia.
(2)Pediatric Intensive Care Unit, Pediatric Department, King Saud University 
Medical City, Riyadh, Saudi Arabia.
(3)Evidence-Based Health Care & Knowledge Translation, Family & Community 
Medicine Department, College of Medicine, King Saud University, Riyadh, Saudi 
Arabia.
(4)Department of Family and Community Medicine, King Saud University Medical 
City, Riyadh, Saudi Arabia.
(5)Pharmacy Department, King Saud University Medical City, Riyadh, Saudi Arabia.
(6)Division of Pediatric Critical Care, Department Pediatrics, Children's 
Hospital of Eastern Ontario, University of Toronto, ON, Canada.
(7)Division of Infectious Diseases, Department of Internal Medicine, College of 
Medicine, King Saud University Medical City, King Saud University, Riyadh, Saudi 
Arabia.
(8)Faculty of Medicine, Assiut University, Assiut, Egypt.
(9)Department of Kidney and Pancreas Transplant, Organ Transplant Center of 
Excellence, King Faisal Specialist Hospital and Research Center, Riyadh, Saudi 
Arabia.
(10)Voice, Swallowing, and Communication Disorders, Department of 
Otolaryngology, College of Medicine, King Saud University, Riyadh, Saudi Arabia.
(11)Specialty Internal Medicine and Quality Department, Johns Hopkins Aramco 
Healthcare, Dhahran, Saudi Arabia.
(12)Infectious Disease Division, Department of Medicine, Indiana University 
School of Medicine, Indianapolis, IN, USA.
(13)Infectious Disease Division, Department of Medicine, Johns Hopkins 
University School of Medicine, Baltimore, IN, USA.

To optimize the deployment of Generative Artificial Intelligence in health care, 
it's essential for health care professionals (HCPs) to understand these 
technologies' capabilities and constraints. This study explores HCPs' initial 
impressions and experiences using ChatGPT, a Generative Pre-trained Transformer, 
in Pediatric Critical Care Units (PICUs). By conducting focus groups with a 
diverse set of HCPs, we aimed to assess their awareness, utilization, perceived 
benefits, and concerns about incorporating ChatGPT into their PICUs. The 
discussions highlighted three main themes: familiarity and usability of ChatGPT, 
its role in clinical and organizational tasks, and ethical concerns. While 
participants appreciated ChatGPT's user-friendliness and potential to expedite 
tasks and provide rapid information, they expressed concerns regarding data 
reliability, recency, and ethical implications. Despite these reservations, 
there is cautious optimism about integrating these tools in PICU, underscoring 
the need for vigilance and ongoing evaluation of novel health care-related 
implications as these technologies evolve.

DOI: 10.1177/00099228251370556
PMID: 40914848 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Conflicting InterestsThe authors 
declared no potential conflicts of interest with respect to the research, 
authorship, and/or publication of this article.


214. Auris Nasus Larynx. 2025 Oct;52(5):585-596. doi: 10.1016/j.anl.2025.08.010. Epub 
2025 Sep 4.

Performance of generative AI across ENT tasks: A systematic review and 
meta-analysis.

Hack S(1), Attal R(2), Farzad A(2), Alon EE(3), Glikson E(3), Remer E(3), Maria 
Saibene A(4), Zalzal HG(5).

Author information:
(1)City St. Georges University London School of Medicine, Program Delivered by 
University of Nicosia at the Chaim Sheba Medical Center, Ramat Gan, Israel. 
Electronic address: sholemhack1@gmail.com.
(2)City St. Georges University London School of Medicine, Program Delivered by 
University of Nicosia at the Chaim Sheba Medical Center, Ramat Gan, Israel.
(3)Tel-Aviv University, Faculty of Medicine, Israel; Department of 
Otolaryngology, Sheba Medical Center, Ramat Gan, Israel.
(4)Otolaryngology Unit, Santi Paolo E Carlo Hospital, Department of Health 
Sciences, Università Degli Studi Di Milano, Milan, Italy.
(5)Division of Otolaryngology-Head and Neck Surgery Children's National Hospital 
Washington District of Columbia USA.

OBJECTIVE: To systematically evaluate the diagnostic accuracy, educational 
utility, and communication potential of generative AI, particularly Large 
Language Models (LLMs) such as ChatGPT, in otolaryngology.
DATA SOURCES: A comprehensive search of PubMed, Embase, Scopus, Web of Science, 
and IEEE Xplore identified English-language peer-reviewed studies from January 
2022 to March 2025.
REVIEW METHODS: Eligible studies evaluated text-based generative AI models used 
in otolaryngology. Two reviewers screened and assessed studies using JBI and 
QUADAS-2 tools. A random-effects meta-analysis was conducted on diagnostic 
accuracy outcomes, with subgroup analyses by task type and model version.
RESULTS: Ninety-one studies were included; 61 reported quantitative outcomes. Of 
these, 43 provided diagnostic accuracy data across 59 model-task pairs. Pooled 
diagnostic accuracy was 72.7 % (95 % CI: 67.4-77.6 %; I² = 93.8 %). Accuracy was 
highest in education (83.0 %) and diagnostic imaging tasks (84.9 %), and lowest 
in clinical decision support (67.1 %). GPT-4 consistently outperformed GPT-3.5 
across both education and CDS domains. Hallucinations and performance 
variability were noted in complex clinical reasoning tasks.
CONCLUSION: Generative AI performs well in structured otolaryngology tasks, 
particularly education and communication. However, its inconsistent performance 
in clinical reasoning tasks limits standalone use. Future research should focus 
on hallucination mitigation, standardized evaluation, and prospective validation 
to guide safe clinical integration.

Copyright © 2025. Published by Elsevier B.V.

DOI: 10.1016/j.anl.2025.08.010
PMID: 40912131 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest None.


215. medRxiv [Preprint]. 2025 Aug 26:2025.08.22.25334232. doi: 
10.1101/2025.08.22.25334232.

AI Agents in Clinical Medicine: A Systematic Review.

Gorenshtein A(1)(2), Omar M(1)(2), Glicksberg BS(1)(2), Nadkarni GN(1)(2), Klang 
E(1)(2).

Author information:
(1)The Windreich Department of Artificial Intelligence and Human Health, Mount 
Sinai Medical Center, NY, USA.
(2)The Hasso Plattner Institute for Digital Health at Mount Sinai, Mount Sinai 
Health System, NY, USA.

BACKGROUND: AI agents built on large language models (LLMs) can plan tasks, use 
external tools, and coordinate with other agents. Unlike standard LLMs, agents 
can execute multi-step processes, access real-time clinical information, and 
integrate multiple data sources. There has been interest in using such agents 
for clinical and administrative tasks, however, there is limited knowledge on 
their performance and whether multi-agent systems function better than a single 
agent for healthcare tasks.
PURPOSE: To evaluate the performance of AI agents in healthcare, compare AI 
agent systems vs. standard LLMs and catalog the tools used for task completion.
DATA SOURCES: PubMed, Web of Science, and Scopus from October 1, 2022, through 
August 5, 2025.
STUDY SELECTION: Peer-reviewed studies implementing AI agents for clinical tasks 
with quantitative performance comparisons.
DATA EXTRACTION: Two reviewers (A.G., M.O.) independently extracted data on 
architectures, performance metrics, and clinical applications. Discrepancies 
were resolved by discussion, with a third reviewer (E.K.) consulted when 
consensus could not be reached.
DATA SYNTHESIS: Twenty studies met inclusion criteria. Across studies, all agent 
systems outperformed their baseline LLMs in accuracy performance. Improvements 
ranged from small gains to increases of over 60 percentage points, with a median 
improvement of 53 percentage points in single-agent tool-calling studies. These 
systems were particularly effective for discrete tasks such as medication dosing 
and evidence retrieval. Multi-agent systems showed optimal performance with up 
to 5 agents, and their effectiveness was particularly pronounced when dealing 
with highly complex tasks. The highest performance boost occurred when the 
complexity of the AI agent framework aligned with that of the task.
LIMITATIONS: Heterogeneous outcomes precluded quantitative meta-analysis. 
Several studies relied on synthetic data, limiting generalizability.
CONCLUSIONS: AI agents consistently improve clinical task performance of 
Base-LLMs when architecture matches task complexity. Our analysis indicates a 
step-change over base-LLMs, with AI agents opening previously inaccessible 
domains. Future efforts should be based on prospective, multi-center trials 
using real-world data to determine safety, task matched and cost-effectiveness.
PRIMARY FUNDING SOURCE: This work was supported in part through the 
computational and data resources and staff expertise provided by Scientific 
Computing and Data at the Icahn School of Medicine at Mount Sinai and supported 
by the Clinical and Translational Science Awards (CTSA) grant UL1TR004419 from 
the National Center for Advancing Translational Sciences. Research reported in 
this publication was also supported by the Office of Research Infrastructure of 
the National Institutes of Health under award number S10OD026880 and 
S10OD030463. The content is solely the responsibility of the authors and does 
not necessarily represent the official views of the National Institutes of 
Health.
REGISTRATION: PROSPERO CRD420251120318.

DOI: 10.1101/2025.08.22.25334232
PMCID: PMC12407621
PMID: 40909853

Conflict of interest statement: Competing interests. The authors declare that 
they have no competing interests.


216. BMJ Open. 2025 Sep 4;15(9):e099301. doi: 10.1136/bmjopen-2025-099301.

Quality and efficiency of integrating customised large language model-generated 
summaries versus physician-written summaries: a validation study.

Schoonbeek RC(1)(2), Workum JD(3)(4), Schuit SCE(5), Hoekman AH(2), Mehri T(2), 
Doornberg JN(6), van der Laan TP(7), Bootsma-Robroeks CMHHT(2)(8); Applied 
Artificial Intelligence in Healthcare Consortium.

Collaborators: Aalderink M, van den Berg R, Besouw MTP, Biere AV, Bodewes FAJA, 
Boerboom AL, Borgdorff MAJ, de Borst MH, Bouhuys M, Brandsema BR, Bultema GH, 
Crop MJ, van der Doef HPJ, Donkers JWJ, Douwes JM, Feijen RA, Fontanella F, 
Foreman B, Gracchi V, De Groot I, Halmos GB, van Heerwaarde AA, van den Heuvel 
F, Holzhauer C, IJpma FFA, Kersten E, Knoef RJH, Kramer MCA, Krishnapillai S, 
Labberté M, Lammers JM, de Langen LB, Lensen E, Lexmond WS, Liem ET, Loeffen E, 
Lorius J, Lubout C, Ludwig-Roukema J, Luiten S, Meijering D, Out C, Palthe S, 
Roofthooft MTR, Scheenstra R, Schreuder RSBH, Schrijvers ML, Sinnige PF, van 
Veen WJ, Te Velde-Keyzer CA, Verbruggen KT, Verheijen M, Vernimmen FPJ, de Vries 
J, de Weerd W, Welsink CL, Woolderink JEJ, Zwart AT.

Author information:
(1)Department of Otolaryngology - Head and Neck Surgery, University Medical 
Centre Groningen, Groningen, The Netherlands r.c.schoonbeek@umcg.nl.
(2)Department of Medical Information Technology, University Medical Centre 
Groningen, Groningen, The Netherlands.
(3)Department of Intensive Care, Elisabeth-TweeSteden Ziekenhuis, Tilburg, The 
Netherlands.
(4)Department of Adult Intensive Care, Erasmus MC University Medical Center, 
Erasmus Universiteit Rotterdam, Rotterdam, The Netherlands.
(5)Board of Directors, University Medical Center, University Medical Centre 
Groningen, Groningen, The Netherlands.
(6)Orthopaedic Surgery, University Medical Centre Groningen, Groningen, The 
Netherlands.
(7)Universitair Medisch Centrum Groningen, Groningen, The Netherlands.
(8)Department of Pediatrics, University Medical Centre Groningen, Groningen, The 
Netherlands.

OBJECTIVES: To compare the quality and time efficiency of physician-written 
summaries with customised large language model (LLM)-generated medical summaries 
integrated into the electronic health record (EHR) in a non-English clinical 
environment.
DESIGN: Cross-sectional non-inferiority validation study.
SETTING: Tertiary academic hospital.
PARTICIPANTS: 52 physicians from 8 specialties at a large Dutch academic 
hospital participated, either in writing summaries (n=42) or evaluating them 
(n=10).
INTERVENTIONS: Physician writers wrote summaries of 50 patient records. 
LLM-generated summaries were created for the same records using an 
EHR-integrated LLM. An independent, blinded panel of physician evaluators 
compared physician-written summaries to LLM-generated summaries.
PRIMARY AND SECONDARY OUTCOME MEASURES: Primary outcome measures were 
completeness, correctness and conciseness (on a 5-point Likert scale). Secondary 
outcomes were preference and trust, and time to generate either the 
physician-written or LLM-generated summary.
RESULTS: The completeness and correctness of LLM-generated summaries did not 
differ significantly from physician-written summaries. However, LLM summaries 
were less concise (3.0 vs 3.5, p=0.001). Overall evaluation scores were similar 
(3.4 vs 3.3, p=0.373), with 57% of evaluators preferring LLM-generated 
summaries. Trust in both summary types was comparable, and interobserver 
variability showed excellent reliability (intraclass correlation coefficient 
0.975). Physicians took an average of 7 min per summary, while LLMs completed 
the same task in just 15.7 s.
CONCLUSIONS: LLM-generated summaries are comparable to physician-written 
summaries in completeness and correctness, although slightly less concise. With 
a clear time-saving benefit, LLMs could help reduce clinicians' administrative 
burden without compromising summary quality.

© Author(s) (or their employer(s)) 2025. Re-use permitted under CC BY-NC. No 
commercial re-use. See rights and permissions. Published by BMJ Group.

DOI: 10.1136/bmjopen-2025-099301
PMCID: PMC12414186
PMID: 40908007 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: None declared.


217. IEEE Trans Biomed Eng. 2025 Sep 4;PP:10.1109/TBME.2025.3606341. doi: 
10.1109/TBME.2025.3606341. Online ahead of print.

Advancing Point-of-Care Still's Murmur Identification: Evaluating the Efficacy 
of ConvNets and Transformers Using the StethAid Multicenter Heart Sound 
Database.

Arjoune Y, Hsu KT, Geggel R, May J, Hovis I, Cosgrove T, Doroshow RW, Shekhar R.

BACKGROUND: Primary care providers (PCPs) are not successful in accurately 
identifying Still's murmur with no available clinical tools to aid in the 
process. Existing deep learning (DL) methods primaryly focused on adult 
pathological murmurs or murmur detection, lacking dedicated approaches for 
Still's murmur. Furthermore, the absence of a specialized database hampers the 
development and validation of AI models for pediatric populations.
OBJECTIVE: To collect a pediatric dataset and evaluate DL methods in identifying 
Still's murmur in pediatric patients from phonocardiograms (PCGs) acquired using 
the StethAid digital auscultation platform.
METHODS: We collected a large pediatric dataset ($>$2 hours of heart sound data) 
from four pediatric medical centers using StethAid, in addition to our previous 
Littmann 4100 dataset, which consisted of 1450 heart sound recordings. The 
StethAid dataset comprises 527 PCGs with diagnoses from four expert pediatric 
cardiologists. This dataset was used to train convolution- and pure 
attention-based models, which are our designed architecture SQ-NET, ResNet18, 
audio spectrogram transformer (AST), data-efficient transformer (DeiT), shifted 
windows (Swin) transformer, self-distillation with no labels (DINO). These 
methods were trained on segmented heart sounds except AST.
RESULTS: DL methods achieved a sensitivity of 90.7%-100%, specificity of 
75%-98.2%, and accuracy of 91.3%-98.5%. These results reflect post-hoc 
comparisons and highlight the potential of DL models to distinguish Still's 
murmur from other innocent and pathological murmurs. Pretraining on domain 
specific Littmann 4100 dataset improved models' performance by at least 2% over 
training from scratch.
CONCLUSION: Both ConvNets and transformers enjoyed high performance in Still's 
murmur identification.
SIGNIFICANCE: The proposed Still's murmur solution can assist PCPs in 
recognizing Still's murmur from other heart murmurs, potentially reducing 
unnecessary specialty referrals and the use of echocardiography. Additionally, 
StethAid could enhance PCPs' auscultation skills for Still's murmur by providing 
instant feedback, a feature often lacking in current clinical workflows.

DOI: 10.1109/TBME.2025.3606341
PMCID: PMC12705212
PMID: 40907040


218. World J Methodol. 2025 Dec 20;15(4):102290. doi: 10.5662/wjm.v15.i4.102290. 
eCollection 2025 Dec 20.

Accuracy of artificial intelligence in meta-analysis: A comparative study of 
ChatGPT 4.0 and traditional methods in data synthesis.

Goyal A(1), Tariq MD(2), Ahsan A(2), Khan MH(3), Zaheer A(4), Jain H(5), 
Maheshwari S(6), Brateanu A(7).

Author information:
(1)Department of Internal Medicine, Seth GS Medical College and KEM Hospital, 
Mumbai 400012, Maharashtra, India. amanmgy@gmail.com.
(2)Department of Internal Medicine, Foundation University Medical College, 
Islamabad 44000, Pakistan.
(3)Department of Internal Medicine, Karachi Medical and Dental College, Karachi 
74700, Pakistan.
(4)Department of Internal Medicine, Liaquat National Hospital and Medical 
College, Karachi 74800, Pakistan.
(5)Department of Internal Medicine, All India Institute of Medical Sciences, 
Jodhpur 400022, India.
(6)Department of Internal Medicine, University of Alabama/Heersink School of 
Medicine, Montgomery, AL 36116, United States.
(7)Department of Internal Medicine, Cleveland Clinic Foundation, Cleveland, OH 
44195, United States.

BACKGROUND: Meta-analysis is a critical tool in evidence-based medicine, 
particularly in cardiology, where it synthesizes data from multiple studies to 
inform clinical decisions. This study explored the potential of using ChatGPT to 
streamline and enhance the meta-analysis process.
AIM: To investigate the potential of ChatGPT to conduct meta-analyses in 
interventional cardiology by comparing the results of ChatGPT-generated analyses 
with those of randomly selected, human-conducted meta-analyses on the same 
topic.
METHODS: We systematically searched PubMed for meta-analyses on interventional 
cardiology published in 2024. Five meta-analyses were randomly chosen. ChatGPT 
4.0 was used to perform meta-analyses on the extracted data. We compared the 
results from ChatGPT with the original meta-analyses, focusing on key effect 
sizes, such as risk ratios (RR), hazard ratios, and odds ratios, along with 
their confidence intervals (CI) and P values.
RESULTS: The ChatGPT results showed high concordance with those of the original 
meta-analyses. For most outcomes, the effect measures and P values generated by 
ChatGPT closely matched those of the original studies, except for the RR of 
stent thrombosis in the Sreenivasan et al study, where ChatGPT reported a 
non-significant effect size, while the original study found it to be 
statistically significant. While minor discrepancies were observed in specific 
CI and P values, these differences did not alter the overall conclusions drawn 
from the analyses.
CONCLUSION: Our findings suggest the potential of ChatGPT in conducting 
meta-analyses in interventional cardiology. However, further research is needed 
to address the limitations of transparency and potential data quality issues, 
ensuring that AI-generated analyses are robust and trustworthy for clinical 
decision-making.

©The Author(s) 2025. Published by Baishideng Publishing Group Inc. All rights 
reserved.

DOI: 10.5662/wjm.v15.i4.102290
PMCID: PMC12400340
PMID: 40900858

Conflict of interest statement: Conflict-of-interest statement: The authors have 
no conflicts of interest statement to declare.


219. Signal Transduct Target Ther. 2025 Sep 3;10(1):285. doi: 
10.1038/s41392-025-02374-w.

Pancancer outcome prediction via a unified weakly supervised deep learning 
model.

Yuan W(#)(1), Chen Y(#)(2), Zhu B(3), Yang S(2), Zhang J(1), Mao N(4), Xiang 
J(2), Li Y(2), Ji Y(2), Luo X(2), Zhang K(2), Xing X(2), Kang S(3), Xiao D(3), 
Wang F(5), Wu J(5), Zhang H(5), Tang H(6), Maurya H(7), Corredor G(7), Barrera 
C(7), Zhou Y(8), Pandav K(7), Zhao J(9), Jain P(10), Delasos L(11), Huang J(12), 
Yang K(13), Teknos TN(14), Lewis J Jr(15)(16), Koyfman S(17), Pennell NA(11), Yu 
KH(9)(18), Han X(1), Zhang J(19), Wang X(20)(21), Madabhushi A(7)(22).

Author information:
(1)College of Biomedical Engineering, Sichuan University, Chengdu, Sichuan, 
China.
(2)Department of Radiation Oncology, Stanford University School of Medicine, 
Palo Alto, CA, USA.
(3)Department of Pharmacy, Children's Hospital of Chongqing Medical University, 
Chongqing, China.
(4)Department of Radiology, Yantai Yuhuangding Hospital, Qingdao University, 
Yantai, Shandong, China.
(5)Department of Pathology, The Affiliated Yantai Yuhuangding Hospital of 
Qingdao University, Yantai, China.
(6)Department of Pathology, Shenzhen Maternity and Child Healthcare Hospital, 
Futian District, Shenzhen, China.
(7)Department of Biomedical Engineering, Emory University, Atlanta, GA, USA.
(8)Department of Electrical Engineering and Computer Science, Case Western 
Reserve University, Cleveland, OH, USA.
(9)Department of Biomedical Informatics, Harvard Medical School, Boston, MA, 
USA.
(10)Department of Medical Oncology, Roswell Park Comprehensive Cancer Center, 
Buffalo, NY, USA.
(11)Cleveland Clinic Taussig Cancer Center, Cleveland, OH, USA.
(12)Department of Computer Science and Engineering, The University of Texas at 
Arlington, Arlington, TX, USA.
(13)Department of Radiation Oncology, Holden Comprehensive Cancer Center, Iowa 
Neuroscience Institute, University of Iowa, Iowa City, IA, USA.
(14)Department of Otolaryngology-Head and Neck Surgery, University Hospitals, 
Cleveland, OH, USA.
(15)Department of Pathology, Microbiology and Immunology, Vanderbilt University 
Medical Center, Nashville, TN, USA.
(16)Department of Laboratory Medicine and Pathology, Mayo Clinic Arizona, 
Scottsdale, AZ, USA.
(17)Department of Radiation Oncology, Cleveland Clinic, Cleveland, OH, USA.
(18)Department of Pathology, Brigham and Women's Hospital, Boston, MA, USA.
(19)College of Biomedical Engineering, Sichuan University, Chengdu, Sichuan, 
China. jing_zhang@scu.edu.cn.
(20)College of Biomedical Engineering, Sichuan University, Chengdu, Sichuan, 
China. xiyuew@stanford.edu.
(21)Department of Radiation Oncology, Stanford University School of Medicine, 
Palo Alto, CA, USA. xiyuew@stanford.edu.
(22)Atlanta Veterans Administration Medical Center, Atlanta, GA, USA.
(#)Contributed equally

Accurate prognosis prediction is essential for guiding cancer treatment and 
improving patient outcomes. While recent studies have demonstrated the potential 
of histopathological images in survival analysis, existing models are typically 
developed in a cancer-specific manner, lack extensive external validation, and 
often rely on molecular data that are not routinely available in clinical 
practice. To address these limitations, we present PROGPATH, a unified model 
capable of integrating histopathological image features with routinely collected 
clinical variables to achieve pancancer prognosis prediction. PROGPATH employs a 
weakly supervised deep learning architecture built upon the foundation model for 
image encoding. Morphological features are aggregated through an 
attention-guided multiple instance learning module and fused with clinical 
information via a cross-attention transformer. A router-based classification 
strategy further refines the prediction performance. PROGPATH was trained on 
7999 whole-slide images (WSIs) from 6,670 patients across 15 cancer types, and 
extensively validated on 17 external cohorts with a total of 7374 WSIs from 4441 
patients, covering 12 cancer types from 8 consortia and institutions across 
three continents. PROGPATH achieved consistently superior performance compared 
with state-of-the-art multimodal prognosis prediction models. It demonstrated 
strong generalizability across cancer types and robustness in stratified 
subgroups, including early- and advanced-stage patients, treatment cohorts 
(radiotherapy and pharmaceutical therapy), and biomarker-defined subsets. We 
further provide model interpretability by identifying pathological patterns 
critical to PROGPATH's risk predictions, such as the degree of cell 
differentiation and extent of necrosis. Together, these results highlight the 
potential of PROGPATH to support pancancer outcome prediction and inform 
personalized cancer management strategies.

© 2025. The Author(s).

DOI: 10.1038/s41392-025-02374-w
PMCID: PMC12405520
PMID: 40897689 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


220. Front Oral Health. 2025 Aug 15;6:1652422. doi: 10.3389/froh.2025.1652422. 
eCollection 2025.

Brush, byte, and bot: quality comparison of artificial intelligence-generated 
pediatric dental advice across ChatGPT, Gemini, and Copilot.

Kapoor D(1), Garg D(2), Tadakamadla SK(3).

Author information:
(1)Department of Pedodontics and Preventive Dentistry, Luxmi Bai Institute of 
Dental Sciences, Baba Farid University of Health Sciences, Punjab, India.
(2)Department of Oral Medicine and Radiology, Luxmi Bai Institute of Dental 
Sciences, Baba Farid University of Health Sciences, Punjab, India.
(3)Rural Dental and Oral Health Clinical Teaching School, La Trobe Rural Health 
School, Bendigo, Victoria, VIC, Australia.

INTRODUCTION: Artificial intelligence (AI) tools such as ChatGPT, Google Gemini, 
and Microsoft Copilot are increasingly relied upon by parents for immediate 
guidance on pediatric dental concerns. This study evaluated and compared the 
response quality of these AI platforms in addressing real-world parental queries 
related to pediatric dentistry, including early tooth extraction, space 
maintenance, and the decision to consult a pediatric or a general dentist.
METHODS: A structured 30-question survey was developed and submitted to each AI 
model, and their responses were anonymized and assessed by pediatric dental 
experts using a standardized rubric across five key domains: clinical accuracy, 
clarity, completeness, relevance, and absence of misleading information.
RESULTS: Statistically significant differences were found across all five 
domains (p < .001), with ChatGPT consistently achieving the highest scores. 
Multivariate analysis (MANOVA) confirmed a strong overall effect of the AI model 
on response quality (Pillai's Trace = 0.892, p < .001), supporting ChatGPT's 
superior performance in providing accurate, relevant, and comprehensive 
pediatric dental advice.
DISCUSSION: While AI technologies show potential as clinical decision support 
systems, their variable performance reinforces the need for expert oversight. 
Future AI development should focus on optimizing response quality and safety to 
ensure effective and trustworthy digital health communication for pediatric 
dental care.

© 2025 Kapoor, Garg and Tadakamadla.

DOI: 10.3389/froh.2025.1652422
PMCID: PMC12394529
PMID: 40895780

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


221. BMC Med Inform Decis Mak. 2025 Sep 1;25(1):325. doi: 10.1186/s12911-025-03035-2.

Comparison of the readability of ChatGPT and Bard in medical communication: a 
meta-analysis.

DeTemple DE(1)(2)(3), Meine TC(4)(5).

Author information:
(1)Clinic for General, Visceral and Transplant Surgery, Hannover Medical School, 
Carl-Neuberg-Strasse 1, 30625, Hannover, Germany.
(2)PRACTIS Clinician Scientist Program, Dean's Office for Academic Career 
Development, Hannover Medical School, Hannover, Germany.
(3)Clinic for General, Visceral, Pediatric and Transplantation Surgery, 
University Hospital RWTH Aachen, Aachen, Germany.
(4)PRACTIS Clinician Scientist Program, Dean's Office for Academic Career 
Development, Hannover Medical School, Hannover, Germany. 
meine.timo@mh-hannover.de.
(5)Institute for Diagnostic and Interventional Radiology, Hannover Medical 
School, Carl-Neuberg-Strasse 1, 30625, Hannover, Germany. 
meine.timo@mh-hannover.de.

BACKGROUND: To synthesize the results of various studies on the readability of 
ChatGPT and Bard in medical communication.
METHODS: Systemic literature research was conducted in PubMed, Ovid/Medline, 
CINAHL, Web-of-Science, Scopus and GoogleScholar to detect relevant publications 
(inclusion criteria: original research articles, English language, medical 
topic, ChatGPT-3.5/-4.0, Bard/Gemini, Flesch Reading Ease Score (FRE), Flesch 
Kincaid Grade Level (FKGL)). Study quality was analyzed using modified 
Downs-and-Black checklist (max. 8 points), adapted for studies on large language 
model. Analysis was performed on text simplification and/or text generation with 
ChatGPT-3.5/-4.0 versus Bard/Gemini. Meta-analysis was conducted, if outcome 
parameter was reported ≥ 3 studies. In addition, subgroup-analyses among 
different chatbot versions were performed. Publication bias was analyzed.
RESULTS: Overall, 59 studies with 2342 items were analyzed. Study quality was 
limited with a mean of 6 points for FRE and 7 points for FKGL. Meta-analysis of 
text simplification for FRE between ChatGPT-3.5/-4.0 and Bard/Gemini was not 
significant (mean difference (MD):5.03; 95%-confidence interval 
(CI):-20.05,30.11; p = 0.48). FKGL of simplified texts of ChatGPT-3.5/-4.0 and 
Bard/Gemini was borderline significant (MD:-1.59; CI:-3.15,-0.04; p = 0.05) and 
subgroup-analysis between ChatGPT-4.0 and Bard was not significant (MD:-1.68; 
CI:-3.53,0.17; p = 0.07). Focused on text acquisition, MD for FRE and FKGL of 
studies on ChatGPT-3.5/-4.0- and Bard/Gemini-generated texts were significant 
(MD:-10.36; CI:-13.08,-7.64; p < 0.01 / MD:1.62; CI:1.09,2.15; p < 0.01). 
Subgroup-analysis of FRE was significant for ChatGPT-3.5 vs. Bard (MD:-16.07, 
CI:-24.90,-7.25; p < 0.01), ChatGPT-3.5 vs. Gemini (MD:-4.51; CI:-8.73,-0.29: 
p = 0.04), ChatGPT-4.0 vs. Bard (MD:-12.01, CI:-16.22,-7.81; p < 0.01) and 
ChatGPT-4.0 vs. Gemini (MD:-7.91, CI:-11.68,-4.15; p < 0.01). Analysis of FKGL 
in the subgroups was significant for ChatGPT-3.5 vs. Bard (MD:2.85, 
CI:1.98,3.73; p < 0.01), ChatGPT-3.5 vs. Gemini (MD:1.21, CI:0.50,1.93; 
p < 0.01) and ChatGPT-4.0 vs. Gemini (MD:1.95, CI:1.05,2.86; p < 0.01), but it 
was not significant for ChatGPT-4.0 vs. Bard (MD:0.64, CI:-0.46,1.74; p = 0.24). 
Egger's test was significant in text generation for FRE and FKGL (p < 0.01 / 
p < 0.01) and in subgroup ChatGPT-4.0 vs. Bard and ChatGPT-4.0 vs. Gemini 
(p < 0.01 / p = 0.02) for FRE as well as in subgroups ChatGPT-3.5 vs. Bard and 
ChatGPT-4.0 vs. Gemini for FKGL (p < 0.01 / p < 0.01).
CONCLUSION: Readability of spontaneously generated texts by Bard/Gemini was 
slightly superior compared to ChatGPT-3.5/-4.0 and readability of simplified 
texts by ChatGPT-3.5/-4.0 tended to be improved compared to Bard. Results are 
limited due study quality and publication bias. Standardized reporting could 
improve study quality and chatbot development.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-03035-2
PMCID: PMC12403948
PMID: 40890707 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


222. J Pediatr Orthop B. 2025 Aug 29. doi: 10.1097/BPB.0000000000001287. Online ahead 
of print.

From algorithms to answers: a comparative analysis of popular search engines and 
large language models on clubfoot patient education.

Pereira DE(1), Guisse NF, Siddabattula R, Perugini J, Hosseinzadeh P.

Author information:
(1)Department of Orthopaedics, Washington University School of Medicine, St. 
Louis, Missouri, USA.

This study evaluates Chat Generative Pre-Trained Transformer 4o's (ChatGPT-4o's) 
utility in clinical relevance and accuracy compared with Google for pediatric 
clubfoot treatment questions. Both were queried for the 15 most frequently asked 
questions related to pediatric clubfoot treatment, with Google as control. 
Questions were classified using the modified Rothwell criteria for online 
sources. Questions and answers were independently graded for clinical relevance 
(0 = not clinically relevance, 1 = some clinical relevance, 2 = very clinically 
relevant) and clinical accuracy (0 = inaccurate, 1 = somewhat accurate, 2 = 
accurate), respectively (D.E.P. and N.G.). Questions and answers were validated 
by an expert, board-certified pediatric orthopedic surgeon (P.H.), who also 
resolved any discrepancies in grading. Per modified Rothwell criteria, Google 
responses were most frequently classified as either 'notion' or 
'indications/management' while ChatGPT-4o responses were most likely addressed 
as 'notion' or 'longevity'. Google sources were primarily from academic and 
government platforms, while ChatGPT-4o exclusively used academic sources. 
ChatGPT-4o questions scored higher for clinical relevance (P = 0.006); however, 
clinical accuracy of answers was equivalent (P = 0.570). ChatGPT-4o provides 
clinically relevant questions, more so than Google with regard to pediatric 
clubfoot treatment. Furthermore, ChatGPT-4o uses a greater proportion of 
academic sources compared with Google. While both sources provided clinically 
accurate answers, large language models appeared to provide information that was 
more relevant and scholarly to patients' concerns regarding clubfoot; however, 
further validation and extensive testing are required to prevent the unnecessary 
spread of misinformation and its utilization in a clinical setting.

Copyright © 2025 Wolters Kluwer Health, Inc. All rights reserved.

DOI: 10.1097/BPB.0000000000001287
PMID: 40888791


223. Sci Rep. 2025 Aug 30;15(1):31993. doi: 10.1038/s41598-025-17256-y.

A model for epileptic EEG detection and recognition based on Multi-Attention 
mechanism and Spatiotemporal.

Su J(1), Huang Z(2), Ma Y(3), Shi H(4), Yang Y(5), Xi M(1), Li B(6).

Author information:
(1)Neurosurgery Department, Affiliate Children's Hospital of Xi'an Jiaotong 
University, Xi'an, 710003, Shaanxi Province, China.
(2)Xi'an Key Laboratory of High Pricision Industrial Intelligent Vision 
Measurement Technology, School of Electronic Information, Xijing University, 
Xi'an, 710123, China. huangzhentao168@163.com.
(3)Xi'an Key Laboratory of High Pricision Industrial Intelligent Vision 
Measurement Technology, School of Electronic Information, Xijing University, 
Xi'an, 710123, China. yahongma@sina.com.
(4)Neurosurgery Department, Affiliate Children's Hospital of Xi'an Jiaotong 
University, Xi'an, 710003, Shaanxi Province, China. shihangyu723@aliyun.com.
(5)Xi'an Key Laboratory of High Pricision Industrial Intelligent Vision 
Measurement Technology, School of Electronic Information, Xijing University, 
Xi'an, 710123, China.
(6)Neurology Department, Affiliate Children's Hospital of Xi'an Jiaotong 
University, Xi'an, 710003, Shaanxi Province, China.

In the field of neuroscience, epilepsy is a chronic non-communicable brain 
disease that affects approximately 50 million people worldwide. 
Electroencephalography (EEG) has become a key tool in detecting and 
characterizing human neurological diseases such as epilepsy. This rapid and 
accurate diagnosis allows doctors to provide timely and effective treatment to 
patients, significantly reducing the frequency of future seizures and the risk 
of related complications, which is crucial for ensuring the long-term health and 
quality of life of patients. Currently, deep learning technologies, particularly 
Convolutional Neural Networks (CNNs) and Long Short-Term Memory Networks 
(LSTMs), have demonstrated significant improvements in accuracy across various 
fields. However, CNNs exhibit limitations in perceiving global dependencies, 
while LSTMs face challenges such as gradient vanishing in long sequences. This 
paper proposes a novel EEG recognition model, the Epileptic EEG Detection and 
Recognition Model based on Multiple Attention Mechanisms and Spatiotemporal 
Feature Fusion (MASF). MASF consists of a hybrid attention mechanism, 
Transformer encoder, and dot-product attention mechanism, which directly 
interprets the epileptic state from the raw EEG signals, thereby eliminating the 
need for extensive data preprocessing and feature extraction. It is worth noting 
that our method achieved an accuracy of 94.19% and 72.50% on the CHB-MIT and 
Bonn University datasets, respectively, in ten-fold cross-validation tests. In 
conclusion, the MASF method for epileptic seizure ictal detection based on EEG 
signals demonstrates significant potential in accelerating diagnosis and 
improving patient prognosis, especially since it achieves high accuracy without 
the need for extensive data preprocessing or feature extraction. The source code 
and dataset can be obtained from 
https://github.com/Xhuangzhentao/MASF-Model-.git .

© 2025. The Author(s).

DOI: 10.1038/s41598-025-17256-y
PMCID: PMC12398548
PMID: 40885781 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


224. Child Abuse Negl. 2025 Nov;169(Pt 2):107653. doi: 10.1016/j.chiabu.2025.107653. 
Epub 2025 Aug 29.

Case reports unlocked: Leveraging retrieval-augmented generation with large 
language models to advance research on psychological child maltreatment.

Stoll D(1), Jud A(2), Wehrli S(3), Lätsch D(4), Steinmann S(5), Wallimann MS(6), 
Quehenberger J(7).

Author information:
(1)Institute of Psychology and Education, Ulm University, Germany; School of 
Social Work, ZHAW Zurich University of Applied Sciences, Switzerland. Electronic 
address: dragan.stoll@zhaw.ch.
(2)School of Social Work, ZHAW Zurich University of Applied Sciences, 
Switzerland; Child and Adolescent Psychiatry, Psychosomatics, and Psychotherapy, 
Ulm University Clinics, Ulm, Germany. Electronic address: andreas.jud2@zhaw.ch.
(3)School of Life Sciences and Facility Management, ZHAW Zurich University of 
Applied Sciences, Switzerland. Electronic address: samuel.wehrli@zhaw.ch.
(4)School of Social Work, ZHAW Zurich University of Applied Sciences, 
Switzerland.
(5)School of Social Work, ZHAW Zurich University of Applied Sciences, 
Switzerland. Electronic address: selina.steinmann@zhaw.ch.
(6)School of Social Work, ZHAW Zurich University of Applied Sciences, 
Switzerland. Electronic address: meret.wallimann@zhaw.ch.
(7)School of Social Work, ZHAW Zurich University of Applied Sciences, 
Switzerland. Electronic address: julia.quehenberger@zhaw.ch.

BACKGROUND: Research on psychological child maltreatment is impeded by a lack of 
high-quality structured data. Crucial information is often documented in child 
protective services (CPS) case files, but only in narrative form. Recent 
research on the utilization of retrieval-augmented generation (RAG) methods with 
large language models (LLMs) for extracting structured data from narratives has 
demonstrated significant potential. RAG methods can facilitate automated 
classification, thereby eliminating the need for laborious annotation.
OBJECTIVE: We aimed to extract structured data from narrative casework reports 
by utilizing RAG and LLMs to classify mentions of 24 CPS case factors. These 
factors encompass child maltreatment indicators, risk factors associated with 
parental, family, and child characteristics, CPS interventions, and their 
outcomes. We focused on examining the extraction of psychological abuse due to 
its complex nature and difficulty in assessing this phenomenon. The results were 
compared with parental lack of cooperation, a factor with a presumed medium 
level of recognition difficulty, and a more straightforward factor of parental 
alcohol abuse.
METHODS: We developed a four-stage workflow comprising of (1) case reports 
collection, (2) RAG based assessment of case factor mentions, (3) automated 
extraction of case factors from RAG assessments, and (4) case labeling. All CPS 
reports (N = 29,770) between 2008 and 2022 from Switzerland's largest CPS 
provider were collected. Model performance was evaluated compared against 
human-coded validation data on assessments. Two expert human reviewers 
independently classified weighted random samples of reports to validate the 
findings from which a consensus dataset was derived.
RESULTS: The model classified psychological abuse, lack of parental cooperation 
and parental alcohol abuse compared to a consensus dataset, with an accuracy of 
82 %, 83 %, and 95 %, respectively, surpassing the agreement rates between the 
two human reviewers (79 %, 80 %, and 93 %).
CONCLUSIONS: RAG based assessment can replicate human judgment even on complex 
CPS case factors. High accuracy and complete inter-rater agreement level was 
achieved for factors that are straightforward to classify, such as parental 
alcohol abuse. The effectiveness of these methods stems from the presence of 
contextual clues related to case factors within a few sentences across different 
sections of the text, rather than from characteristics inherent to the entire 
text. For case factors such as parental lack of cooperation, both supporting and 
refuting evidence needs to be assessed to achieve optimal accuracy. Careful 
consideration of potential biases and limitations in RAG methods is advised. 
These applications can serve as early warning systems, by identifying critical 
factors from extensive case notes that might otherwise be overlooked, supporting 
professionals in making informed decisions and improving outcomes for at-risk 
children.

Copyright © 2025 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.chiabu.2025.107653
PMID: 40884999 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no competing interests.


225. Medicina (Kaunas). 2025 Jul 25;61(8):1342. doi: 10.3390/medicina61081342.

Clinical Failure of General-Purpose AI in Photographic Scoliosis Assessment: A 
Diagnostic Accuracy Study.

Aydin C(1), Duygu OB(2), Karakas AB(3), Er E(4), Gokmen G(5), Ozturk AM(1), 
Govsa F(6).

Author information:
(1)Department of Orthopedics and Traumatology, Faculty of Medicine, Ege 
University, 35040 Izmir, Turkey.
(2)Department of Anatomy, Faculty of Medicine, Bakırcay University, 35660 Izmir, 
Turkey.
(3)Department of Anatomy, Faculty of Medicine, Kastamonu University, 37150 
Kastamonu, Turkey.
(4)Department of Visual Communication Design, Faculty of Fine Arts, Design and 
Architecture, Ege University, 35040 Izmir, Turkey.
(5)Faculty of Medicine, Dokuz Eylul University, 35220 Izmir, Turkey.
(6)Digital Imaging and 3D Modelling Laboratory, Department of Anatomy, Faculty 
of Medicine, Ege University, 35040 Izmir, Turkey.

Background and Objectives: General-purpose multimodal large language models 
(LLMs) are increasingly used for medical image interpretation despite lacking 
clinical validation. This study evaluates the diagnostic reliability of 
ChatGPT-4o and Claude 2 in photographic assessment of adolescent idiopathic 
scoliosis (AIS) against radiological standards. This study examines two critical 
questions: whether families can derive reliable preliminary assessments from 
LLMs through analysis of clinical photographs and whether LLMs exhibit cognitive 
fidelity in their visuospatial reasoning capabilities for AIS assessment. 
Materials and Methods: A prospective diagnostic accuracy study (STARD-compliant) 
analyzed 97 adolescents (74 with AIS and 23 with postural asymmetry). 
Standardized clinical photographs (nine views/patient) were assessed by two LLMs 
and two orthopedic residents against reference radiological measurements. 
Primary outcomes included diagnostic accuracy (sensitivity/specificity), Cobb 
angle concordance (Lin's CCC), inter-rater reliability (Cohen's κ), and 
measurement agreement (Bland-Altman LoA). Results: The LLMs exhibited hazardous 
diagnostic inaccuracy: ChatGPT misclassified all non-AIS cases (specificity 0% 
[95% CI: 0.0-14.8]), while Claude 2 generated 78.3% false positives. Systematic 
measurement errors exceeded clinical tolerance: ChatGPT overestimated thoracic 
curves by +10.74° (LoA: -21.45° to +42.92°), exceeding tolerance by >800%. Both 
LLMs showed inverse biomechanical concordance in thoracolumbar curves (CCC ≤ 
-0.106). Inter-rater reliability fell below random chance (ChatGPT κ = -0.039). 
Universal proportional bias (slopes ≈ -1.0) caused severe curve underestimation 
(e.g., 10-15° error for 50° deformities). Human evaluators demonstrated superior 
bias control (0.3-2.8° vs. 2.6-10.7°) but suboptimal specificity (21.7-26.1%) 
and hazardous lumbar concordance (CCC: -0.123). Conclusions: General-purpose 
LLMs demonstrate clinically unacceptable inaccuracy in photographic AIS 
assessment, contraindicating clinical deployment. Catastrophic false positives, 
systematic measurement errors exceeding tolerance by 480-1074%, and inverse 
diagnostic concordance necessitate urgent regulatory safeguards under frameworks 
like the EU AI Act. Neither LLMs nor photographic human assessment achieve 
reliability thresholds for standalone screening, mandating domain-specific 
algorithm development and integration of 3D modalities.

DOI: 10.3390/medicina61081342
PMCID: PMC12387722
PMID: 40870387 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no competing interests, 
financial or personal, that could have influenced the research or its reporting.


226. BMC Med Inform Decis Mak. 2025 Aug 27;25(1):315. doi: 
10.1186/s12911-025-03117-1.

Development and validation of a deep learning model for early detection and 
screening of diabetic retinopathy.

Cao F(1), Guo X(2), Li M(1), Li S(3), Peng X(4).

Author information:
(1)School of Management, Harbin Institute of Technology, Room 339, Building 2H, 
2 Yikuang Str. Nangang Dist. Harbin, Harbin, 150001, P.R. China.
(2)School of Management, Harbin Institute of Technology, Room 339, Building 2H, 
2 Yikuang Str. Nangang Dist. Harbin, Harbin, 150001, P.R. China. 
xitongguo@gmail.com.
(3)Heilongjiang Academy of Chinese Medicine Sciences, Harbin, P.R. China.
(4)Ningbo Municipal Hospital of TCM, Affiliated Hospital of Zhejiang Chinese 
Medical University, Ningbo, P.R. China.

Early diagnosis and screening of diabetic retinopathy (DR) are crucial for 
reducing medical burdens and conserving healthcare resources. This study 
introduces an advanced AI-assisted recognition system designed to enhance the 
detection of DR lesions through innovative automatic learning methods. Central 
to our approach are agnostic text instruction templates, which facilitate 
zero-shot DR detection by integrating text embeddings with visual information. 
Our system performs comprehensive lesion detection by leveraging similarity 
mapping at both the image and patch levels, enabling it to identify a wide range 
of diabetic retinopathy (DR) lesions without the need for extensive annotated 
data. This AI-assisted system distinguishes itself from traditional fully 
supervised models and few-shot learning approaches by addressing the 
complexities of DR image annotation and safeguarding patient privacy. To 
validate the system's effectiveness, we conducted extensive experiments across 
five internal and publicly available test sets, as well as an external test set 
captured using smartphone devices. Our evaluation involved performance analysis 
of various pre-training methods, including detailed patch-level visualizations 
and t-SNE clustering techniques to assess the quality of feature embeddings. The 
results of our zero-shot experiments reveal that our system outperforms 
conventional transfer learning-based DR detection methods. This superiority is 
evident in both the pre-training and testing phases, showcasing the system's 
ability to deliver accurate and reliable DR lesion detection while circumventing 
the limitations of traditional approaches.

Publisher: Not applicable.

© 2025. The Author(s).

DOI: 10.1186/s12911-025-03117-1
PMCID: PMC12382226
PMID: 40866960 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: No ethical approval was necessary for this study; all participants 
in this study consented to participation, and informed consent was obtained from 
all of the participants. The manuscripts reporting studies not involving human 
participants. All experiments in this study are based on public datasets and 
have no ethical implications. Our study does not adhere to the Declaration of 
Helsinki. Consent for publication: Not applicable. Competing interests: The 
authors declare no competing interests.


227. JMIR Form Res. 2025 Aug 27;9:e76097. doi: 10.2196/76097.

Evaluating the Quality and Understandability of Radiology Report Summaries 
Generated by ChatGPT: Survey Study.

Sunshine A(1), Honce GH(2), Callen AL(1), Zander DA(1), Tanabe JL(1), Pisani 
Petrucci SL(1), Lin CT(3), Honce JM(1).

Author information:
(1)Department of Radiology, University of Colorado Anschutz Medical Campus, 19th 
Ave. Mail Stop C278, Aurora, CO, 80045, United States, 1 303-724-3796, 1 
303-724-3795.
(2)Hartway Evaluation Group, Denver, CO, United States.
(3)Department of Medicine, University of Colorado Anschutz Medical Campus, 
Aurora, CO, United States.

BACKGROUND: Radiology reports convey critical medical information to health care 
providers and patients. Unfortunately, they are often difficult for patients to 
comprehend, causing confusion and anxiety, thereby limiting patient engagement 
in health care decision-making. Large language models (LLMs) like ChatGPT 
(OpenAI) can create simplified, patient-friendly report summaries to increase 
accessibility, albeit with errors.
OBJECTIVE: We evaluated the accuracy and clarity of ChatGPT-generated summaries 
compared to original radiologist-assessed radiology reports, assessed patients' 
understanding and satisfaction with the summaries compared to the original 
reports, and compared the readability of the original reports and summaries 
using validated readability metrics.
METHODS: We anonymized 30 radiology reports created by neuroradiologists at our 
institution (6 brain magnetic resonance imaging, 6 brain computed tomography, 6 
head and neck computed tomography angiography, 6 neck computed tomography, and 6 
spine computed tomography). These anonymized reports were processed by ChatGPT 
to produce patient-centric summaries. Four board-certified neuroradiologists 
evaluated the ChatGPT-generated summaries on quality and accuracy compared to 
the original reports, and 4 patient volunteers separately evaluated the reports 
and summaries on perceived understandability and satisfaction. Readability was 
assessed using word count and validated readability scales.
RESULTS: After reading the summary, patient confidence in understanding (98%, 
116/118 vs 26%, 31/118) and satisfaction regarding the level of 
jargon/terminology (91%, 107/118 vs 8%, 9/118) and time taken to understand the 
content (97%, 115/118 vs 23%, 27/118) substantially improved. Ninety-two percent 
(108/118) of responses indicated the summary clarified patients' questions about 
the report, and 98% (116/118) of responses indicated patients would use the 
summary if available, with 67% (79/118) of responses indicating they would want 
access to both the report and summary, while 26% (31/118) of responses indicated 
only wanting the summary. Eighty-three percent (100/120) of radiologist 
responses indicated the summary represented the original report "extremely well" 
or "very well," with only 5% (6/120) of responses indicating it did so "slightly 
well" or "not well at all." Five percent (6/120) of responses indicated there 
was missing relevant medical information in the summary, 12% (14/120) reported 
instances of overemphasis of nonsignificant findings, and 18% (22/120) reported 
instances of underemphasis of significant findings. No fabricated findings were 
identified. Overall, 83% (99/120) of responses indicated that the summary would 
definitely/probably not lead patients to incorrect conclusions about the 
original report, with 10% (12/120) of responses indicating the summaries may do 
so.
CONCLUSIONS: ChatGPT-generated summaries could significantly improve perceived 
comprehension and satisfaction while accurately reflecting most key information 
from original radiology reports. Instances of minor omissions and 
under-/overemphasis were noted in some summaries, underscoring the need for 
ongoing validation and oversight. Overall, these artificial 
intelligence-generated, patient-centric summaries hold promise for enhancing 
patient-centered communication in radiology.

© Alexis Sunshine, Grace H Honce, Andrew L Callen, David A Zander, Jody L 
Tanabe, Samantha L Pisani Petrucci, Chen-Tan Lin, Justin M Honce. Originally 
published in JMIR Formative Research (https://formative.jmir.org).

DOI: 10.2196/76097
PMCID: PMC12385610
PMID: 40864905 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


228. Dent J (Basel). 2025 Aug 12;13(8):366. doi: 10.3390/dj13080366.

Charting New Territory: AI Applications in Dental Caries Detection from 
Panoramic Imaging.

Hung M(1)(2)(3)(4), Yevseyevich D(1), Khazana M(1), Schwartz C(1)(5)(6), Lipsky 
MS(1)(7).

Author information:
(1)College of Dental Medicine, Roseman University of Health Sciences, South 
Jordan, UT 84095, USA.
(2)Division of Public Health, University of Utah, Salt Lake City, UT 84108, USA.
(3)School of Business, University of Utah, Salt Lake City, UT 84112, USA.
(4)Primary Children's Hospital, Salt Lake City, UT 84113, USA.
(5)Library, Roseman University of Health Sciences, South Jordan, UT 84095, USA.
(6)Library, Noorda College of Osteopathic Medicine, Provo, UT 84606, USA.
(7)Institute on Aging, Portland State University, Portland, OR 97207, USA.

Introduction: Dental caries remains a public health concern, and early detection 
prevents its progression and complications. Panoramic radiographs are essential 
diagnostic tools, yet the interpretation of panoramic X-rays varies among 
practitioners. Artificial intelligence (AI) presents a promising approach to 
enhance diagnostic accuracy in detecting dental caries. This scoping review 
examines the current literature on the use of AI programs to analyze panoramic 
radiographs for the diagnosis of dental caries. Methods: This scoping review 
searched PubMed, Scopus, Web of Science, and Dentistry and Oral Sciences Source, 
adhering to PRISMA guidelines. The review included peer-reviewed, original 
research published in English that investigated the use of AI to diagnose dental 
caries. Data were extracted on the AI model characteristics, advantages, 
disadvantages, and diagnostic performance. Results: Seven studies met the 
inclusion criteria. The Deep Learning Model achieved the highest performance 
(specificity 0.9487, accuracy 0.9789, F1 score 0.9245), followed by Diagnocat 
and Tooth Type Enhanced Transformer. Models such as CranioCatch and CariSeg 
showed moderate performance, while the Dental Caries Detection Network 
demonstrated the lowest. Benefits included improved diagnostic support and 
workflow efficiency, while limitations involved dataset biases, interpretability 
challenges, and computational demands. Conclusions: Applying AI technologies to 
panoramic X-rays demonstrates the potential for enhancing caries diagnosis, with 
some models achieving near-expert performance. However, future research must 
address the generalizability, transparency, and integration of AI models into 
clinical practice. Future research should focus on diverse training datasets, 
explainable AI development, clinical validation, and incorporating AI training 
into dental education and training.

DOI: 10.3390/dj13080366
PMCID: PMC12385533
PMID: 40863069

Conflict of interest statement: The authors declare no conflicts of interest.


229. J Med Internet Res. 2025 Aug 25;27:e65523. doi: 10.2196/65523.

Nurse Researchers' Experiences and Perceptions of Generative AI: Qualitative 
Semistructured Interview Study.

Kang R(#)(1), Xuan Z(#)(1), Tong L(1), Wang Y(1), Jin S(1), Xiao Q(1).

Author information:
(1)School of Nursing, Capital Medical University, No.10 Xi-tou-tiao, You-an-men 
Wai, Feng-tai District, Beijing, 100069, China.
(#)Contributed equally

BACKGROUND: With the rapid development and iteration of generative artificial 
intelligence, the growing popularity of such groundbreaking tools among nurse 
researchers, represented by ChatGPT (OpenAI), is receiving passionate debate and 
intrigue. Although there has been qualitative research on generative artificial 
intelligence in other fields, little is known about the experiences and 
perceptions of nurse researchers; this study seeks to report on the topic.
OBJECTIVE: This study aimed to describe the experiences and perceptions of 
generative artificial intelligence among Chinese nurse researchers, as well as 
provide a reference for the application of generative artificial intelligence in 
nursing research in the future.
METHODS: Semistructured interviews were used to collect data in this qualitative 
study. Researchers mainly conducted interviews on the cognition, experience, and 
future expectations of nurse researchers regarding the use of generative 
artificial intelligence. Twenty-seven nurse researchers were included in the 
study. Through purposive sampling and snowball sampling, there were 7 nursing 
faculty researchers, 10 nursing graduate students, and 10 clinical nurse 
researchers. Data were analyzed using inductive content analysis.
RESULTS: Five themes and 12 subthemes were categorized from 27 original 
interview documents as follows: (1) diverse reflections on human-machine 
symbiosis, which includes the interplay between substitution and assistance, 
researchers shaping the potential of generative artificial intelligence, and 
acceptance of generative artificial intelligence with alacrity; (2) multiple 
factors of the usage experience, including individual characteristics and 
various usage scenarios; (3) research paradigm reshaping in the infancy stage, 
which involves full-process groundbreaking assistive tools and emergence of new 
research paths; (4) application risks of generative artificial intelligence, 
including intrinsic limitations of generative artificial intelligence and 
academic integrity and medical ethics; and (5) the co-improvement of technology 
and literacy, which concerns reinforcement needs for generative artificial 
intelligence literacy, development of nursing research generative artificial 
intelligence and urgent need for artificial intelligence-generated content 
detection tools. In this context, the first 4 themes form the rocket of the 
human-machine symbiosis journey. Only when humans fully leverage the advantages 
of machines (generative artificial intelligence) and overcome their shortcomings 
can this human-machine symbiosis journey reach the correct future direction 
(fifth theme).
CONCLUSIONS: This study explored the experiences and perceptions of nurse 
researchers interacting with generative artificial intelligence, which was a 
"symbiotic journey" full of twists and turns, and provides a reference and basis 
for achieving harmonious coexistence between nurse researchers and generative 
artificial intelligence in the future. Nurse researchers, policy makers, and 
application developers can use the conclusions of this study to further promote 
the application of generative artificial intelligence in nursing research, 
policy making, and product development.

© Ruifu Kang, Zehui Xuan, Ling Tong, Yanling Wang, Shuai Jin, Qian Xiao. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/65523
PMCID: PMC12377238
PMID: 40853413 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


230. Insights Imaging. 2025 Aug 23;16(1):184. doi: 10.1186/s13244-025-02068-5.

Predicting pediatric age from chest X-rays using deep learning: a novel 
approach.

Li M(#)(1), Zhao J(#)(2), Liu H(1), Jin B(1), Cui X(1), Wang D(3).

Author information:
(1)Department of Radiology, Xinhua Hospital Affiliated to Shanghai Jiao Tong 
University School of Medicine, Shanghai, China.
(2)Department of Radiology, Shanghai Tenth People's Hospital, Tongji University, 
Shanghai, China.
(3)Department of Radiology, Xinhua Hospital Affiliated to Shanghai Jiao Tong 
University School of Medicine, Shanghai, China. wangdengbin@xinhuamed.com.cn.
(#)Contributed equally

OBJECTIVES: Accurate age estimation is essential for assessing pediatric 
developmental stages and for forensics. Conventionally, pediatric age is 
clinically estimated by bone age through wrist X-rays. However, recent advances 
in deep learning enable other radiological modalities to serve as a promising 
complement. This study aims to explore the effectiveness of deep learning for 
pediatric age estimation using chest X-rays.
MATERIALS AND METHODS: We developed a ResNet-based deep neural network model 
enhanced with Coordinate Attention mechanism to predict pediatric age from chest 
X-rays. A dataset comprising 128,008 images was retrospectively collected from 
two large tertiary hospitals in Shanghai. Mean Absolute Error (MAE) and Mean 
Absolute Percentage Error (MAPE) were employed as main evaluation metrics across 
age groups. Further analysis was conducted using Spearman correlation and 
heatmap visualizations.
RESULTS: The model achieved an MAE of 5.86 months for males and 5.80 months for 
females on the internal validation set. On the external test set, the MAE was 
7.40 months for males and 7.29 months for females. The Spearman correlation 
coefficient was above 0.98, indicating a strong positive correlation between the 
predicted and true age. Heatmap analysis revealed the deep learning model mainly 
focused on the spine, mediastinum, heart and great vessels, with additional 
attention given to surrounding bones.
CONCLUSIONS: We successfully constructed a large dataset of pediatric chest 
X-rays and developed a neural network model integrated with Coordinate Attention 
for age prediction. Experiments demonstrated the model's robustness and proved 
that chest X-rays can be effectively utilized for accurate pediatric age 
estimation.
CRITICAL RELEVANCE STATEMENT: By integrating pediatric chest X-rays with age 
data using deep learning, we can provide more support for predicting children's 
age, thereby aiding in the screening of abnormal growth and development in 
children.
KEY POINTS: This study explores whether deep learning could leverage chest 
X-rays for pediatric age prediction. Trained on over 120,000 images, the model 
shows high accuracy on internal and external validation sets. This method 
provides a potential complement for traditional bone age assessment and could 
reduce radiation exposure.

© 2025. The Author(s).

DOI: 10.1186/s13244-025-02068-5
PMCID: PMC12374921
PMID: 40848095

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was approved by the IRB of Xinhua Hospital Affiliated to 
Shanghai Jiao Tong University School of Medicine (approval ID: XHEC-D-2024-143) 
and Shanghai Tenth People’s Hospital Review Committee (approval ID: 
SHSY-IEC-5.0/24K135/P01). Consent for publication: All authors and the ethics 
committee consented to the publication of this manuscript. Informed consent was 
waived. Competing interests: The authors declare that they have no competing 
interests.


231. Sci Rep. 2025 Aug 21;15(1):30719. doi: 10.1038/s41598-025-15130-5.

Parents' information needs and perceptions of chatbots regarding self medicating 
their children.

Wei X(1), Chen X(2)(3)(4), Yue L(5), Liao P(5).

Author information:
(1)Xiangya School of Nursing, Central South University, Changsha, China.
(2)Teaching and Research Section of Clinical Nursing, Xiangya Hospital, Central 
South University, Changsha, China. chenxiuwen2020@163.com.
(3)National Clinical Research Center for Geriatric Disorders, Xiangya Hospital, 
Central South University, Changsha, China. chenxiuwen2020@163.com.
(4)Xiangya School of Nursing, Central South University, Changsha, China. 
chenxiuwen2020@163.com.
(5)Teaching and Research Section of Clinical Nursing, Xiangya Hospital, Central 
South University, Changsha, China.

This qualitative study aimed to better understand parents' information needs and 
perceptions of chatbots for self-medicating their children and developing 
chatbots for self-medication. A qualitative study was employed. Semi-structured 
interviews were conducted from October 2023 to December 2023. The interview data 
were analysed using a thematic analysis method. A total of 26 participants were 
interviewed by trained interviewers. Four key themes were identified: (1) 
Multiple information needs for self-medication; (2) Factors promoting the use of 
chatbots in self-medication; (3) Factors hindering the use of chatbots in 
self-medication; and (4) Expectations and suggestions for chatbots design. The 
parents' perspective provided important insights into the design of the chatbot 
conversational interfaces that could help improve parents' practice of 
self-medicating their children. Although most participants were unfamiliar with 
chatbots, they were generally optimistic about their convenience, reliability, 
and benefits that would reduce medical burdens and cross-infection. When 
designing chatbots in the future, we will focus on the privacy and financial 
security, information recognition and cost issues to design chatbots with 
various transmission forms, communication interaction, health tracking and 
feedback functions.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-15130-5
PMCID: PMC12370943
PMID: 40841815 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


232. J Med Internet Res. 2025 Aug 20;27:e70140. doi: 10.2196/70140.

Deep Phenotyping of Obesity: Electronic Health Record-Based Temporal Modeling 
Study.

Ruan X(1), Lu S(1), Wang L(2), Wen A(1), Murali S(3), Liu H(1).

Author information:
(1)Department of Health Data Science and AI, McWilliams School of Biomedical 
Informatics, The University of Texas Health Science Center at Houston, 7000 
Fannin St, Houston, TX, 77030, United States, 1 713-500-3924.
(2)Department of Clinical and Health Informatics, McWilliams School of 
Biomedical Informatics, The University of Texas Health Science Center at 
Houston, Houston, TX, United States.
(3)Department of Surgery, McGovern Medical School, University of Texas Health 
Science Center at Houston, Houston, TX, United States.

BACKGROUND: Obesity affects approximately 40% of adults and 15%-20% of children 
and adolescents in the United States, and poses significant economic and 
psychosocial burdens. Currently, patient responses to any single antiobesity 
medication (AOM) vary significantly, making obesity deep phenotyping and 
associated precision medicine important targets of investigation.
OBJECTIVE: This study aimed to evaluate the potential of electronic health 
records (EHR) as a primary data source for obesity deep phenotyping. We 
conducted an in-depth analysis of the data elements and quality available from 
obesity patients prior to pharmacotherapy and applied a multimodal longitudinal 
deep autoencoder to investigate the feasibility, data requirements, clustering 
patterns, and challenges associated with EHR-based obesity deep phenotyping.
METHODS: We analyzed 53,688 pre-AOM periods from 32,969 patients with obesity or 
overweight who underwent medium- to long-term AOM treatment. A total of 92 
laboratory and vital measurements, along with 79 ICD (International 
Classification of Diseases)-derived clinical classifications software (CCS) 
codes recorded within one year prior to AOM treatment, were used to train a 
gated recurrent unit with decay-based longitudinal autoencoder (GRU-D-AE) to 
generate dense embeddings for each pre-AOM record. Principal component analysis 
and Gaussian mixture modeling (GMM) were applied to identify clusters.
RESULTS: Our analysis identified at least 9 clusters, with 5 exhibiting distinct 
and explainable clinical relevance. Certain clusters show characteristics 
overlapping with phenotypes from traditional phenotyping strategy. Results from 
multiple training folds demonstrated stable clustering patterns in 2D space and 
reproducible clinical significance. However, challenges persist regarding the 
stability of missing data imputation across folds, maintaining consistency in 
input features, and effectively visualizing complex diseases in low-dimensional 
spaces.
CONCLUSIONS: In this proof-of-concept study, we demonstrated longitudinal EHR as 
a valuable resource for deep phenotyping the pre-AOM period at per patient visit 
level. Our analysis revealed the presence of clusters with distinct clinical 
significance, which could have implications in AOM treatment options. Further 
research using larger, independent cohorts is necessary to validate the 
reproducibility and clinical relevance of these clusters, uncover more detailed 
substructures and corresponding AOM treatment responses.

©Xiaoyang Ruan, Shuyu Lu, Liwei Wang, Andrew Wen, Sameer Murali, Hongfang Liu. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/70140
PMCID: PMC12373304
PMID: 40834423 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


233. Cyberpsychol Behav Soc Netw. 2025 Sep;28(9):623-629. doi: 
10.1177/21522715251371092. Epub 2025 Aug 25.

AI-Assisted Art Therapy: Enhancing Psychological Recovery in Work-Related 
Injuries Through Personalized Emotional Support.

Xu C(1), Zhang Y(2), Li H(2), Mao Z(2)(3).

Author information:
(1)Xi'an Jiaotong Liverpool University, Suzhou, China.
(2)Changzhou Maternity and Child Health Care Hospital, Changzhou Medical Center, 
Nanjing Medical University, Changzhou, China.
(3)Development and Related Diseases of Women and Children Key Laboratory of 
Sichuan Province, Sichuan, China.

Work-related injuries often trigger anxiety and Post-Traumatic Stress Disorder 
(PTSD), where art therapy shows promise but faces personalization limitations. 
This preregistered study investigated artificial intelligence (AI)-assisted art 
therapy, utilizing ChatGPT-4, to reduce anxiety and PTSD symptoms and improve 
emotional well-being in 113 participants recovering from work-related injuries, 
compared to traditional art therapy. The AI-assisted group received personalized 
feedback via an AI platform analyzing artwork. Results showed significantly 
greater reductions in anxiety and PTSD symptoms in the AI-assisted group 
(moderate effect sizes, Cohen's d = 0.55 and 0.62, respectively), but no 
significant difference in positive affect. Qualitative interviews revealed 
perceived emotional support from AI, enhancing engagement. These findings 
suggest AI augments art therapy by providing real-time, personalized support, 
though further research is needed to enhance positive emotional outcomes.

DOI: 10.1177/21522715251371092
PMID: 40853067 [Indexed for MEDLINE]


234. Eur Arch Otorhinolaryngol. 2025 Aug 23. doi: 10.1007/s00405-025-09630-3. Online 
ahead of print.

Does ChatGPT update itself? Accuracy of ChatGPT in tympanostomy tube guidance: A 
comparative analysis with current literature.

Durgut O(1), Dikici O(2).

Author information:
(1)Department of Otorhinolaryngology, Health Science University, Bursa City 
Hospital, T.C. Sağlık Bakanlığı Bursa Şehir Hastanesi Doğanköy Mahallesi, 
Nilüfer /Bursa, 16110, Turkey. durgutosman@yahoo.com.
(2)Department of Otorhinolaryngology, Health Science University, Bursa City 
Hospital, T.C. Sağlık Bakanlığı Bursa Şehir Hastanesi Doğanköy Mahallesi, 
Nilüfer /Bursa, 16110, Turkey.

OBJECTIVE: This study aims to evaluate the accuracy of ChatGPT-4.0 in providing 
information on tympanostomy tube indications in children, comparing its 
responses with established clinical guidelines and examining its ability to 
update itself over time.
METHODS: Sixteen clinical scenarios from the American Academy of 
Otolaryngology-Head and Neck Surgery Foundation (AAO-HNSF) guidelines were 
assessed using 18 specific questions. Responses were evaluated by two 
otolaryngologists and ChatGPT itself. The final validation was conducted by a 
senior otolaryngologist. Cohen's Kappa analysis was performed to assess 
inter-rater reliability.
RESULTS: ChatGPT-4.0 correctly answered 15.5 out of 16 scenarios (96.8%). The 
second-stage question of scenario 7 was evaluated as incorrect. When current 
literature was referenced, all responses reached 100% accuracy. Among the 
correct answers, 4 scenarios were not fully aligned with the guidelines. 
However, when responses were based on current literature, all of these answers 
were found to be fully compliant. The agreement among the three evaluators was 
perfect, as confirmed by Cohen's Kappa analysis. Despite using an updated 
version (ChatGPT-4.0) and over a year having passed, it was observed that 
ChatGPT-3.5 answered a previously incorrect scenario in the same incorrect 
manner. This suggests that the model may have limited capacity for self-updating 
over time. These findings are consistent with previous research, indicating that 
ChatGPT provides highly accurate responses regarding tympanostomy tube placement 
and largely aligns with existing guidelines.
CONCLUSION: ChatGPT-4.0 demonstrates high accuracy in providing guideline-based 
medical information, but its ability to update itself over time appears to be 
limited. However, when prompted to reference current literature, its accuracy 
improves significantly. These findings highlight the importance of structured 
prompting and critical evaluation of AI-generated medical guidance.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00405-025-09630-3
PMID: 40849397

Conflict of interest statement: Declarations. Conflict of interest: Osman 
Durgut, M.D. declares that he has no conflicts of interest. Oğuzhan Dikici, M.D. 
declares that he has no conflicts of interest.


235. Biomed Phys Eng Express. 2025 Sep 1;11(5). doi: 10.1088/2057-1976/adfde9.

DCE-UNet: a transformer-based fully automated segmentation network for multiple 
adolescent spinal disorders in x-ray images.

Xue Z(1), Deng S(1), Yue Y(1), Chen C(1), Li Z(1), Yang Y(1), Sun S(1), Liu 
Y(1).

Author information:
(1)Beijing Institute of Petrochemical Technology, Qingyuan North Road, No. 19, 
Daxing District, Beijing, 102617, People's Republic of China.

In recent years, spinal x-ray image segmentation has played a vital role in the 
computer-aided diagnosis of various adolescent spinal disorders. However, due to 
the complex morphology of lesions and the fact that most existing methods are 
tailored to single-disease scenarios, current segmentation networks struggle to 
balance local detail preservation and global structural understanding across 
different disease types. As a result, they often suffer from limited accuracy, 
insufficient robustness, and poor adaptability. To address these challenges, we 
propose a novel fully automated spinal segmentation network, DCE-UNet, which 
integrates the local modeling strength of convolutional neural networks (CNNs) 
with the global contextual awareness of Transformers. The network introduces 
several architectural and feature fusion innovations. Specifically, a 
lightweight Transformer module is incorporated in the encoder to model 
high-level semantic features and enhance global contextual understanding. In the 
decoder, a Rec-Block module combining residual convolution and channel attention 
is designed to improve feature reconstruction and multi-scale fusion during the 
upsampling process. Additionally, the downsampling feature extraction path 
integrates a novel DC-Block that fuses channel and spatial attention mechanisms, 
enhancing the network's ability to represent complex lesion structures. 
Experiments conducted on a self-constructed large-scale multi-disease adolescent 
spinal x-ray dataset demonstrate that DCE-UNet achieves a Dice score of 91.3%, a 
mean Intersection over Union (mIoU) of 84.1, and a Hausdorff Distance (HD) of 
4.007, outperforming several state-of-the-art comparison networks. Validation on 
real segmentation tasks further confirms that DCE-UNet delivers consistently 
superior performance across various lesion regions, highlighting its strong 
adaptability to multiple pathologies and promising potential for clinical 
application.

© 2025 IOP Publishing Ltd. All rights, including for text and data mining, AI 
training, and similar technologies, are reserved.

DOI: 10.1088/2057-1976/adfde9
PMID: 40840472 [Indexed for MEDLINE]


236. Laryngoscope. 2026 Feb;136(2):605-615. doi: 10.1002/lary.70063. Epub 2025 Aug 
20.

Surgeon, Trainee, or GPT? A Blinded Multicentric Study of AI-Augmented Operative 
Notes.

Hack S(1), Attal R(1), Locatelli G(2), Scotta G(3)(4), Maniaci A(5), Parisi 
FM(6), van der Poel N(7)(8), Van Daele M(7)(8), Garcia-Lliberos A(9), 
Rodriguez-Prado C(9), Chiesa-Estomba CM(10), Andueza-Guembe M(10), Cobb P(11), 
Zalzal HG(11), Saibene AM(2).

Author information:
(1)City St. George's University of London, School of Medicine, Program Delivered 
by University of Nicosia at the Chaim Sheba Medical Center, Ramat Gan, Israel.
(2)Otolaryngology Unit, Santi Paolo E Carlo Hospital, Department of Health 
Sciences, Università Degli Studi Di Milano, Milan, Italy.
(3)Department of Otolaryngology, Sheffield Teaching Hospitals, Sheffield, UK.
(4)ENT Department, Royal Hallamshire Hospital, Sheffield, UK.
(5)Department of Medicine and Surgery, University of Enna "Kore", Enna, Italy.
(6)Department of Medical and Surgical Sciences and Advanced Technologies "G.F. 
Ingrassia", ENT Section, University of Catania, Catania, Italy.
(7)Department of Otorhinolaryngology and Head and Neck Surgery, Antwerp 
University Hospital, Edegem, Belgium.
(8)Department of Translational Neuroscience, Faculty of Medicine and Health 
Science, University of Antwerp, Antwerp, Belgium.
(9)Department of Otolaryngology, Valencia University General Hospital, Valencia, 
Spain.
(10)Department of Otorhinolaryngology, Biodonostia Research Institute, 
Osakidetza, Donostia University Hospital, San Sebastian, Spain.
(11)Division of Otolaryngology-Head and Neck Surgery, Children's National 
Hospital, Washington, District of Columbia, USA.

OBJECTIVES: Clear, complete operative documentation is essential for surgical 
safety, continuity of care, and medico-legal standards. Large language models 
such as ChatGPT offer promise for automating clinical documentation; however, 
their performance in operative note generation, particularly in surgical 
subspecialties, remains underexplored. This study aimed to compare the quality, 
accuracy, and efficiency of operative notes authored by a surgical resident, 
attending surgeon, GPT alone, and an attending surgeon using GPT as a writing 
aid.
METHODS: Five publicly available otolaryngologic procedures were selected. For 
each procedure, four operative notes were generated, one by a resident, one by 
an attending, one by GPT alone, and one by a hybrid of attending plus GPT. Ten 
blinded otolaryngologists (five residents, five attendings) independently 
reviewed all 20 notes. Reviewers scored each note across eight domains using a 
five-point scale, assigned a final approval rating, and provided qualitative 
feedback. Writing time was recorded to assess documentation efficiency.
RESULTS: Hybrid notes written by an attending surgeon with GPT assistance 
received the highest average domain scores and the highest "as is" approval rate 
(79%), outperforming all other groups. GPT-only notes were the fastest to 
generate but had the lowest approval rate (23%) and the highest incidence of 
both omissions and overdocumentation. Writing time was significantly reduced in 
both AI-assisted groups compared to human-only authorship. Inter-rater 
reliability among reviewers was moderate to high across most domains.
CONCLUSION: In this limited dataset, hybrid human-AI collaboration outperformed 
both human-only and AI-only authorship in operative documentation. These 
findings support GPT-assisted documentation to improve operative note efficiency 
and consistency.
LEVEL OF EVIDENCE: N/A.

© 2025 The American Laryngological, Rhinological and Otological Society, Inc.

DOI: 10.1002/lary.70063
PMID: 40833187 [Indexed for MEDLINE]


237. medRxiv [Preprint]. 2025 Aug 12:2025.08.08.25333315. doi: 
10.1101/2025.08.08.25333315.

Detecting Stigmatizing Language in Clinical Notes with Large Language Models for 
Addiction Care.

Sethi R(1), Caskey J(2), Gao Y(3), Churpek MM(2), Miller TA(4), Mayampurath 
A(2), Afshar ES(5), Afshar M(2), Dligach D(1).

Author information:
(1)Loyola University Chicago, Chicago, IL, United States.
(2)School of Medicine and Public Health, University of Wisconsin, Madison, WI, 
United States.
(3)University of Colorado Anschutz Medical Campus, Aurora, CO, United States.
(4)Boston Children's Hospital and Harvard Medical School.
(5)Department of Family Medicina and Community Health, University of Wisconsin, 
Madison, WI, United States.

RATIONALE: Recent studies have found that stigmatizing terms can incline 
physicians to pursue punitive approaches to patient care. The intensive care 
unit (ICU) contains large volumes of progress notes that may contain 
stigmatizing language, which could perpetuate negative biases against patients 
and affect healthcare delivery. Patients with substance use disorders (alcohol, 
opioid, and non-opioid drugs) are particularly vulnerable to stigma. This study 
aimed to examine the performance of Large Language Models (LLMs) in the 
identification of stigmatizing language from ICU progress notes of patients with 
substance use disorders (SUD).
METHODS: Clinical notes were sampled from the Medical Information Mart for 
Intensive Care (MIMIC)-III, which contains 2,083,180 ICU notes. These 2,083,180 
notes were passed into a rule-based labeling approach followed by manual 
verification for more ambiguous cases. The labeling approach followed the NIH 
guidelines on stigma in SUD. The labeling process resulted in identifying 38,552 
stigmatizing encounters. To design our cohort, we randomly sampled an equivalent 
amount of non-stigmatizing encounters to create a dataset with 77,104 notes. 
This cohort was organized into train/development/test datasets (70/15/15). We 
utilized Meta's Llama-3 8B Instruct LLM to run the following experiments for 
stigma detection: (1) prompts with instructions that adhere to the NIH terms 
(Zero-Shot); (2) prompts with instructions and examples (in-context learning); 
(3) in-context learning with a selective retrieval system for the NIH terms 
(Retrieval Augmented Generation-RAG); and (4) supervised fine-tuning (SFT). We 
also created a baseline model using keyword search. Evaluation was performed on 
the held-out test set for accuracy, macro F1 score, and error analysis. The 
LLM-based approaches were prompted to provide their reasoning for label 
prediction. Additionally, all approaches were evaluated on an external 
validation dataset from the University of Wisconsin (UW) Health System with 
288,130 ICU notes.
RESULTS: SFT had the best performance with 97.2% accuracy, followed by 
in-context learning. The LLMs with in-context learning and SFT provided 
appropriate reasoning for false positives during human review. Both approaches 
identified clinical notes with stigmatizing language that were missed during 
annotation (10/93 false positives for SFT and 22/186 false positives for the 
in-context learning approach were considered valid after human review). SFT 
maintained its accuracy at 97.9% on a similarly balanced external validation 
dataset.
CONCLUSION: Our findings demonstrate that LLMs, particularly using SFT and 
in-context learning, effectively identify stigmatizing language in ICU notes 
with high accuracy while explaining their reasoning in an asynchronous fashion 
without needing rigorous and time-intensive manual verification involved in 
labeling. These models also demonstrated the ability to identify novel 
stigmatizing language not explicitly in training data nor existing guidelines. 
This study highlights the potential of LLMs in reducing stigma in clinical 
documentation, especially for patients with SUD. These LLMs enable 
identification of stigmatizing language in clinical notes that can perpetuate 
negative stigma towards patients and encourage rewriting of notes.

DOI: 10.1101/2025.08.08.25333315
PMCID: PMC12363688
PMID: 40832420

Conflict of interest statement: Competing Interests: All authors declare no 
financial or non-financial competing interests.


238. BMC Med Educ. 2025 Aug 19;25(1):1175. doi: 10.1186/s12909-025-07750-2.

Effectiveness of generative artificial intelligence-based teaching versus 
traditional teaching methods in medical education: a meta-analysis of randomized 
controlled trials.

Li J(#)(1)(2), Yin K(#)(1)(2), Wang Y(3), Jiang X(4)(5)(6), Chen D(7)(8).

Author information:
(1)Department of Anesthesiology, West China Second University Hospital, Sichuan 
University, Chengdu, China.
(2)Key Laboratory of Birth Defects and Related Diseases of Women and Children, 
Ministry of Education, Chengdu, China.
(3)Key Laboratory of BioResource and Eco-Environment of Ministry of Education, 
College of Life Science, Sichuan University, Chengdu, China.
(4)Department of Anesthesiology, West China Second University Hospital, Sichuan 
University, Chengdu, China. 1598862657jxq@scu.edu.cn.
(5)Key Laboratory of Birth Defects and Related Diseases of Women and Children, 
Ministry of Education, Chengdu, China. 1598862657jxq@scu.edu.cn.
(6)Department of Anesthesiology, Chengdu Hi-Tech Zone Hospital for Women and 
Children, Chengdu, China. 1598862657jxq@scu.edu.cn.
(7)Department of Anesthesiology, West China Second University Hospital, Sichuan 
University, Chengdu, China. scucdx@foxmail.com.
(8)Key Laboratory of Birth Defects and Related Diseases of Women and Children, 
Ministry of Education, Chengdu, China. scucdx@foxmail.com.
(#)Contributed equally

BACKGROUND: Artificial intelligence (AI) has demonstrated remarkable 
capabilities across diverse medical applications, potentially revolutionizing 
healthcare delivery systems. This systematic review and meta-analysis 
investigated the comparative effectiveness of generative artificial intelligence 
(GAI)-based teaching methodologies versus conventional pedagogical approaches on 
educational outcomes among medical students.
METHODS: We conducted a comprehensive literature search across multiple 
electronic databases, including PubMed, Cochrane Library, EMBASE, and Web of 
Science, encompassing studies published from January 2014 through January 2025. 
The review focused on randomized controlled trials (RCTs) that compared 
GAI-based teaching interventions with traditional instructional teaching methods 
in medical students.
RESULTS: The meta-analysis incorporated 11 eligible RCTs, comprising 786 medical 
students. Pooled analysis revealed no statistically significant difference in 
knowledge acquisition scores between GAI-based and traditional teaching 
approaches (standardized mean difference [SMD] 0.27, 95% confidence interval 
[CI] -0.31 to 0.85; p = 0.36). However, subgroup analysis indicated enhanced 
knowledge performance in the GAI group specifically for extended learning 
periods (exceeding one week) and practice-oriented courses. GAI-based 
instruction demonstrated superior outcomes in practical skill development 
compared to conventional methods (SMD 0.63, 95% CI 0.10-1.16; p = 0.02). 
Students in the GAI group reported significantly higher satisfaction scores with 
their learning experience.
CONCLUSION: While theoretical knowledge acquisition remains comparable between 
teaching modalities, the distinctive advantages of GAI-based approaches in 
practical skill development warrant their integration into medical curricula. 
Future research should focus on optimizing the integration of GAI-based teaching 
methods, standardizing implementation protocols, and evaluating long-term 
educational outcomes.
TRIAL REGISTRATION: This protocol was registered on the International Platform 
of Registered Systematic Review and Meta-analysis Protocols (INPLASY) with the 
registration number INPLASY202510006. Registered on 2 January 2025.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-07750-2
PMCID: PMC12362899
PMID: 40830476 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Ethical approval and patient consent were not required as all 
analyses were based on previously published studies. Consent for publication: 
Not applicable. Competing interests: The authors declare no competing interests.


239. Sci Rep. 2025 Aug 19;15(1):30404. doi: 10.1038/s41598-025-13818-2.

Lung adenocarcinoma subtype classification based on contrastive learning model 
with multimodal integration.

Wang C(1), Liu L(2), Fan C(3), Zhang Y(4), Mai Z(2), Li L(2), Liu Z(5), Tian 
Y(6), Hu J(7), Elazab A(8).

Author information:
(1)Shenzhen Research Institute of Big Data, Shenzhen, China.
(2)National Cancer Center, Chinese Academy of Medical Sciences and Peking Union 
Medical College, National Clinical Research Center for Cancer, Cancer Hospital 
and Shenzhen Hospital, Shenzhen, China.
(3)Zhejiang University of Finance and Economics, Hangzhou, China.
(4)Zhejiang University of Finance and Economics, Hangzhou, China. 
zyq@zufe.edu.cn.
(5)National Cancer Center, Chinese Academy of Medical Sciences and Peking Union 
Medical College, National Clinical Research Center for Cancer, Cancer Hospital 
and Shenzhen Hospital, Shenzhen, China. zhou_liu8891@yeah.net.
(6)The Second Affiliated Hospital, School of Medicine, The Chinese University of 
Hong Kong, Shenzhen, China.
(7)Hongqi Hospital Affiliated to Mudanjiang Medical University, Mudanjiang, 
China.
(8)School of Biomedical Engineering, Shenzhen University, Shenzhen, China.

Accurately identifying the stages of lung adenocarcinoma is essential for 
selecting the most appropriate treatment plans. Nonetheless, this task is 
complicated due to challenges such as integrating diverse data, similarities 
among subtypes, and the need to capture contextual features, making precise 
differentiation difficult. We address these challenges and propose a multimodal 
deep neural network that integrates computed tomography (CT) images, annotated 
lesion bounding boxes, and electronic health records. Our model first combines 
bounding boxes with precise lesion location data and CT scans, generating a 
richer semantic representation through feature extraction from regions of 
interest to enhance localization accuracy using a vision transformer module. 
Beyond imaging data, the model also incorporates clinical information encoded 
using a fully connected encoder. Features extracted from both CT and clinical 
data are optimized for cosine similarity using a contrastive language-image 
pre-training module, ensuring they are cohesively integrated. In addition, we 
introduce an attention-based feature fusion module that harmonizes these 
features into a unified representation to fuse features from different 
modalities further. This integrated feature set is then fed into a classifier 
that effectively distinguishes among the three types of adenocarcinomas. 
Finally, we employ focal loss to mitigate the effects of unbalanced classes and 
contrastive learning loss to enhance feature representation and improve the 
model's performance. Our experiments on public and proprietary datasets 
demonstrate the efficiency of our model, achieving a superior validation 
accuracy of 81.42% and an area under the curve of 0.9120. These results 
significantly outperform recent multimodal classification approaches. The code 
is available at https://github.com/fancccc/LungCancerDC .

© 2025. The Author(s).

DOI: 10.1038/s41598-025-13818-2
PMCID: PMC12365151
PMID: 40830161 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


240. Implement Sci Commun. 2025 Aug 18;6(1):87. doi: 10.1186/s43058-025-00775-0.

Beyond the surface of capacity building: a mixed-methods study of the core 
functions and forms of dissemination and implementation science consultations.

Swanson KN(1)(2), Stadnick NA(3)(4)(5)(6)(7), Aarons GA(3)(4)(5)(6)(7), 
Brookman-Frazee L(3)(4)(5)(6)(7), Bouchard I(8), Du Z(3), Brubaker AG(5)(6), 
Geremia C(4)(9), Cain K(4)(10), Conradi LR(5)(6), Sklar M(3)(5)(6)(7), Viglione 
C(10), Rabin BA(3)(4)(10).

Author information:
(1)Dissemination and Implementation Science Center, UC San Diego Altman Clinical 
and Translational Research Institute, University of California San Diego, La 
Jolla, CA, USA. knswanson@health.ucsd.edu.
(2)Herbert Wertheim School of Public Health and Human Longevity Science, 
University of California San Diego, La Jolla, CA, USA. 
knswanson@health.ucsd.edu.
(3)Dissemination and Implementation Science Center, UC San Diego Altman Clinical 
and Translational Research Institute, University of California San Diego, La 
Jolla, CA, USA.
(4)San Diego Center for AIDS Research Implementation Science Hub, University of 
California San Diego, La Jolla, CA, USA.
(5)UC San Diego ImplementatioN Science and Team Effectiveness in Practice (IN 
STEP) Children's Mental Health Research Center, University of California San 
Diego, La Jolla, CA, USA.
(6)Department of Psychiatry, University of California San Diego, La Jolla, CA, 
USA.
(7)Child and Adolescent Services Research Center, San Diego, CA, USA.
(8)California State University Northridge, Northridge, CA, USA.
(9)Department of Medicine, University of California San Diego, La Jolla, CA, 
USA.
(10)Herbert Wertheim School of Public Health and Human Longevity Science, 
University of California San Diego, La Jolla, CA, USA.

Update of
    Res Sq. 2025 May 13:rs.3.rs-6559185. doi: 10.21203/rs.3.rs-6559185/v1.

BACKGROUND: The number of Dissemination and Implementation Science (DIS) 
capacity building programs is increasing worldwide. These programs aim to 
enhance diverse DIS skills through a variety of activities. Our team's 
systematic review of DIS programs determined that DIS consultations were offered 
across 67% of programs, yet their specific roles in capacity development were 
not well defined. This mixed methods study aimed to identify and categorize the 
functions and forms of consultation activities across three DIS capacity 
building programs at the University of California San Diego that varied in 
content focus and infrastructure and then to map findings onto DIS competencies.
METHODS: Consultation notes from the three programs were extracted for content 
analysis to identify discussion topics, DIS guidance provided, and resources 
shared. Generative artificial intelligence (ChatGPT Plus) facilitated content 
analyses with multiple rounds of validation from program consultants to refine 
and finalize themes. Themes were categorized into consultation functions and 
forms. Forms were mapped onto DIS competencies and a gap analysis was conducted 
to identify areas for improvement. Program metrics were used to further 
contextualize findings.
RESULTS: A total of 108 consultation notes were analyzed across the three 
programs. The most common types of support requested related to grant proposals 
(54%) and ongoing D&I projects (25%). Consultation discussion topics most 
frequently involved applying implementation science principles (55%) grant 
development (31%), guidance centered on DIS methods (47%), and study/project 
design (37%). Consultation guidance was most often aligned with 
intermediate-level DIS competencies (58%), primarily in the design and analysis 
and practice-based considerations domains.
CONCLUSIONS: These findings highlight the nature of DIS consultation services, 
particularly among those seeking support for grant proposals and ongoing 
implementation projects. Consultations primarily addressed intermediate-level 
competencies within the domains of design, analysis, and practice-based 
implementation strategies, indicating a clear opportunity to strengthen support 
for both foundational and advanced skill development. These results suggest the 
need for scalable consultation frameworks, improved tracking systems, and tiered 
training resources to optimize the reach and impact of DIS capacity-building 
efforts.

© 2025. The Author(s).

DOI: 10.1186/s43058-025-00775-0
PMCID: PMC12359906
PMID: 40826438

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: Gregory Aarons is an Editorial Board Member for Implementation 
Science Communications. Borsika Rabin is part of the Editorial Board for 
Implementation Science Communications. The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


241. J Med Internet Res. 2025 Aug 18;27:e77893. doi: 10.2196/77893.

Magnitude and Impact of Hallucinations in Tabular Synthetic Health Data on 
Prognostic Machine Learning Models: Validation Study.

Pilgram L(1)(2)(3), El Kababji S(2), Liu D(1)(2), El Emam K(1)(2).

Author information:
(1)School of Epidemiology and Public Health, Faculty of Medicine, University of 
Ottawa, Ottawa, ON, Canada.
(2)CHEO Research Institute, Children's Hospital of Eastern Ontario, Ottawa, ON, 
Canada.
(3)Department of Nephrology and Medical Intensive Care, Charité - 
Universitaetsmedizin Berlin, Berlin, Germany.

BACKGROUND: Generative artificial intelligence (AI) for tabular synthetic data 
generation (SDG) has significant potential to accelerate health care research 
and innovation. A critical limitation of generative AI, however, is 
hallucinations. Although this has been commonly observed in text-generating 
models, it may also occur in tabular SDG.
OBJECTIVE: This study aims to investigate the magnitude of hallucinations in 
tabular synthetic data, whether their frequency increases with training data 
complexity, and the extent to which they impact the utility of synthetic data 
for downstream prognostic machine learning (ML) modeling tasks.
METHODS: On the basis of 12 large and high-dimensional real-world health care 
datasets, 6354 training datasets of different complexity were created by varying 
the subset of variables included in each dataset. Synthetic data were generated 
using 7 different SDG models. Hallucinations were defined as synthetic records 
that did not exist in the population, and the hallucination rate (HR) was the 
proportion of hallucinations in a synthetic dataset. Classification was the 
downstream prognostic modeling task, conducted via an ML approach (light 
gradient boosted machine) and an artificial neural network (multilayer 
perceptron). Mixed-effects models were fitted to examine the relationship 
between training data complexity and the HR and the HR and the predictive 
performance of AI and ML models when trained on the synthetic data.
RESULTS: The HR ranged from 0.3% to 100% (median 99.1%, IQR 98.5%-100.0%) and 
increased with training data complexity. However, in most SDG models, the HR did 
not affect AI and ML prognostic model performance. In the SDG models in which a 
significant association was detected, the estimated effect was very small, with 
a maximum decrease in the area under the receiver operating characteristic curve 
of -0.0002 (95% CI -0.0003 to -0.0002, P<.001) in light gradient boosting 
machine and -0.0001 (95% CI -0.0002 to -0.0001, P=.002) in multilayer 
perceptron.
CONCLUSIONS: These findings suggest that while hallucinations may be very common 
in synthetic tabular health data, they do not necessarily impair its utility for 
prognostic modeling.

©Lisa Pilgram, Samer El Kababji, Dan Liu, Khaled El Emam. Originally published 
in the Journal of Medical Internet Research (https://www.jmir.org), 18.08.2025.

DOI: 10.2196/77893
PMCID: PMC12402739
PMID: 40825542 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: At the time the study was 
conducted KEE was the scholar in residence at the office of the Information and 
Privacy Commissioner of Ontario and held shares in Aetion, which provided the 
sequential synthesis generative model software that was used in this study. At 
the time of publication both of these conflicts are no longer in effect. At the 
time of publication, KEE is the Editor-in-Chief of JMIR AI


242. Front Oncol. 2025 Aug 1;15:1613462. doi: 10.3389/fonc.2025.1613462. eCollection 
2025.

Pre-operative T-stage discrimination in gallbladder cancer using machine 
learning and DeepSeek-R1.

Chae J(1), Wang Z(1), Wu D(2), Zhang L(3), Tuzikov A(4), Madiyevich MT(5), Xu 
M(6), Yu D(6), Qin P(1).

Author information:
(1)Institute of Biopharmaceutical and Health Engineering, Shenzhen International 
Graduate School, Tsinghua University, Shenzhen, Guangdong, China.
(2)School of Communication Engineering and the Artificial Intelligence 
Institute, Hangzhou Dianzi University, Hangzhou, Zhejiang, China.
(3)The First Hospital of Hebei Medical University, Shijiazhuang, Hebei, China.
(4)United Institute of Informatics Problems, National Academy of Sciences of 
Belarus, Minsk, Belarus.
(5)Department of Biomedical Engineering & Tashkent State Technical University, 
Tashkent, Uzbekistan.
(6)Affiliated Fifth Hospital, Wenzhou Medical University, Wenzhou, 
Zhejiang, China.

BACKGROUND: Gallbladder cancer (GBC) frequently exhibits non-specific early 
symptoms, delaying diagnosis. This study (i) assessed whether routine blood 
biomarkers can distinguish early T stages via machine learning and (ii) compared 
the T-stage discrimination performance of a large language model (DeepSeek-R1) 
when supplied with (a) radiology-report text alone versus (b) radiology-report 
text plus blood-biomarker values.
METHODS: We retrospectively analyzed 232 pathologically confirmed GBC patients 
treated at Lishui Central Hospital between 2023 and 2024 (T1, n = 51; T2, n = 
181). Seven blood variables-neutrophil-to-lymphocyte ratio (NLR), 
monocyte-to-lymphocyte ratio (MLR), platelet-tolymphocyte ratio (PLR), 
carcino-embryonic antigen (CEA), carbohydrate antigen 19-9 (CA19-9), 
carbohydrate antigen 125 (CA125), and alpha-fetoprotein (AFP)-were used to train 
Random forest, Support Vector Machine (SVC), XGBoost, and LightGBM models. 
Synthetic Minority Over-sampling Technique (SMOTE) was applied only to the 
training folds in one setting and omitted in another. Model performance was 
evaluated on an independent test set (N = 47) by the area under the 
receiver-operating-characteristic curve (AUROC, 95% CI by 1 000-sample bootstrap 
confidence interval, CI); cross-validation (CV) accuracy served as a 
supplementary metric. DeepSeek-R1 was prompted in a zero-shot, chain-of-thought 
manner to classify T1 versus T2 using (a) the radiology report alone or (b) the 
report plus the patient's biomarker profile.
RESULTS: Biomarker-based machine-learning models yielded uniformly poor T-stage 
discrimination. Without SMOTE, individual models such as XGBoost achieved an 
AUROC of 0.508 on the independent test set, while recall for the T1 class 
remained low (e.g., 14.3% for some models), indicating performance near random 
chance. Applying SMOTE to the training data produced statistically significant 
gains in cross-validation (CV) accuracy for several models (e.g., XGBoost CV 
Acc. 0.71 → 0.80, p = 0.005; LGBM CV Acc. [No-SMOTE] → [SMOTE], p = 0.004). 
However, these improvements did not translate to better discrimination on the 
independent test set; for instance, XGBoost's AUROC decreased from 0.508 to 
0.473 after SMOTE application. Overall, the biomarker models failed to provide 
clinically meaningful T-stage differentiation. DeepSeek-R1 analyzing radiology 
text alone reached 89.6% accuracy on the full 232-patient cohort dataset, and 
consistently flagged T2 cases on phrases such as "gallbladder wall thickening." 
Supplying biomarker values did not change accuracy (89.6%).
CONCLUSIONS: The evaluated blood biomarkers did not independently aid early 
T-stage discrimination, and SMOTE offered no meaningful performance gain. 
Conversely, a radiologytext-driven large language model delivered high accuracy 
with interpretable rationale, highlighting its potential to guide surgical 
strategy in GBC. Prospective multi-center studies with larger cohorts are 
warranted to confirm these findings.

Copyright © 2025 Chae, Wang, Wu, Zhang, Tuzikov, Madiyevich, Xu, Yu and Qin.

DOI: 10.3389/fonc.2025.1613462
PMCID: PMC12355213
PMID: 40823085

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


243. Value Health. 2025 Nov;28(11):1655-1664. doi: 10.1016/j.jval.2025.08.002. Epub 
2025 Aug 13.

A4SLR: An Agentic Artificial Intelligence-Assisted Systematic Literature Review 
Framework to Augment Evidence Synthesis for Health Economics and Outcomes 
Research and Health Technology Assessment.

Lee K(1), Paek H(1), Ofoegbu N(1), Rube S(1), Higashi MK(2), Dawoud D(3), Xu 
H(4), Shi L(5), Wang X(6).

Author information:
(1)IMO Health, Rosemont, IL, USA.
(2)ISPOR, The Professional Society for Health Economics and Outcomes Research, 
Lawrenceville, NJ, USA.
(3)Cairo University, Cairo, Egypt.
(4)Yale University, New Haven, CT, USA.
(5)Tulane University, New Orleans, LA, USA.
(6)IMO Health, Rosemont, IL, USA; Tulane University, New Orleans, LA, USA. 
Electronic address: xw108@caa.columbia.edu.

OBJECTIVES: Systematic literature reviews (SLRs) are essential for synthesizing 
high-quality evidence in clinical research, health economics and outcomes 
research, and health technology assessments. However, the growing volume of 
published data has made SLRs time-consuming, labor-intensive, and costly. To 
address these challenges, we introduce A4SLR, an agentic 
artificial-intelligence-assisted SLR framework, which provides a flexible, 
extensible methodology for automating the entire SLR process-from initial query 
formulation to evidence synthesis-across various study fields.
METHODS: A4SLR comprises 8 modules integrated with specialized artificial 
intelligence agents powered by large language models: search, I/E criteria 
deployment, abstract/full-text screening, text/table pre-processing, data 
extraction, assessment, risk of bias analysis, and report. We implemented and 
validated this framework using 2 use cases, non-small cell lung cancer and 
perinatal mood and anxiety disorders. Performance of the assessment was 
evaluated quantitatively and qualitatively.
RESULTS: Our implementation demonstrated high accuracy in article screening (F1 
scores: 0.917-0.977), risk of bias assessment (Cohen's k: 0.8442-0.9064), and 
data extraction (F-scores: 0.96-0.998), including patient characteristics, 
safety and efficacy outcomes, economic model parameters, and cost-effectiveness 
data. Notably, the text/table preprocessing agent yielded comprehensive coverage 
of data elements, particularly in the challenging tasks of accurately matching 
outcome values to their corresponding study arms.
CONCLUSIONS: Our findings highlight the potential of the A4SLR framework to 
transform the evidence synthesis process by addressing the limitations of manual 
SLRs, thereby enhancing health economics and outcomes research and health 
technology assessments. Designed as a scalable, user-centric, extensible 
approach, A4SLR provides a robust solution for generating comprehensive 
up-to-date evidence to support researchers and decision makers across diverse 
clinical and therapeutic areas.

Copyright © 2025. Published by Elsevier Inc.

DOI: 10.1016/j.jval.2025.08.002
PMID: 40816692 [Indexed for MEDLINE]

Conflict of interest statement: Author Disclosures Author disclosure forms can 
be accessed below in the Supplemental Material section. Drs Dawoud and Shi are 
editors for Value in Health and had no role in the peer-review process of this 
article.


244. JMIR Form Res. 2025 Aug 15;9:e69892. doi: 10.2196/69892.

Exploring Young Adults' Experiences and Beliefs in Asthma Medication Management: 
Pilot Qualitative Study Comparing Human and Multiple AI Thematic Analysis.

Jeminiwa RN(1), Popielaski C(2), King A(1).

Author information:
(1)Department of Pharmacy Practice, Jefferson College of Pharmacy, Thomas 
Jefferson University, 901 Walnut Street, Philadelphia, PA, 19107, United States, 
1 2159551159.
(2)Long Island Jewish Medical Center, Northwell Health, New Hyde Park, NY, 
United States.

BACKGROUND: Young adults take their asthma maintenance medication 67% of the 
time or less. Understanding the specific needs and behaviors of young adults 
with asthma is essential for developing targeted interventions to improve 
disease self-management. Artificial intelligence (AI) has demonstrated its 
utility in summarizing and identifying patterns in qualitative research and may 
support or augment human coding efforts. However, there is pause literature to 
support this assertion.
OBJECTIVE: The objective of this study is to begin to explore the medication 
management-related needs of young adults with asthma via a pilot feasibility 
study. We aim to understand how to best assist young adults with asthma 
self-management and to identify potential areas where digital health 
interventions can provide support. We further aimed to understand the 
comparative outcome of human versus multiple AI platforms in performing thematic 
analysis.
METHODS: This study purposefully sampled young adults between the ages of 18 
years and 29 years who had a prescription for an inhaled corticosteroid (ICS) 
and were either students or staff of a large metropolitan university in the 
northeastern United States. Semistructured interviews lasting 40 minutes on 
average were conducted with 4 participants via a teleconferencing application to 
elicit young adults' opinions on the topic. Interviews were recorded and 
transcribed verbatim using Otter.ai (Otter.ai, Inc). Investigators listened to 
the recording to confirm the accuracy of transcriptions and to make corrections 
when necessary. After performing a second round of line-by-line coding, the 
codes were reviewed by investigators and grouped into broader, overarching 
themes. All investigators reviewed and discussed the final codes. Human 
qualitative data analyses were performed using NVivo 14 software (QSR 
International). After completing human analyses, the investigators performed 
thematic analysis with multiple AI platforms (Google Gemini, Microsoft Copilot, 
and OpenAI's ChatGPT) to compare the final themes with investigator-derived 
themes.
RESULTS: Human analysis yielded 4 themes: support from clinicians, social 
support, digital self-management support, and educational support. The AI-based 
analysis also generated similar themes with different labels. The level of 
overlap on the underlying concept between humans, Gemini, Copilot, and ChatGPT 
was high, accounting for the fact that, although the specific labels differed, 
they referred to the same concept.
CONCLUSIONS: Findings from our pilot exploratory study offer insights into the 
necessity for a holistic approach in supporting young adults with asthma. Based 
on the health belief model, if the identified multifaceted needs are addressed, 
health care systems may support medication adherence and improve health outcomes 
for this understudied patient population. Our pilot study also offers 
preliminary findings that artificial intelligence may be leveraged for 
successful thematic analysis of qualitative data with appropriate caution.

© Ruth Ndarake Jeminiwa, Caroline Popielaski, Amber King. Originally published 
in JMIR Formative Research (https://formative.jmir.org).

DOI: 10.2196/69892
PMCID: PMC12356605
PMID: 40815807 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


245. IEEE J Biomed Health Inform. 2025 Aug 15;PP. doi: 10.1109/JBHI.2025.3599643. 
Online ahead of print.

Enhancing Psychological Assessments with Open-Ended Questionnaires and Large 
Language Models: An ASD Case Study.

Altozano A, Minissi ME, Gomez-Zaragoza L, Maddalon L, Alcaniz M, Marin-Morales 
J.

Open-ended questionnaires allow respondents to express freely, capturing richer 
information than close-ended formats, but they are harder to analyze. Recent 
natural language processing advancements enable automatic assessment of 
open-ended responses, yet its use in psychological classification is 
underexplored. This study proposes a methodology using pre-trained large 
language models (LLMs) for automatic classification of open-ended 
questionnaires, applied to autism spectrum disorder (ASD) classification via 
parental reports. We compare multiple training strategies using transcribed 
responses from 51 parents (26 with typically developing children, 25 with ASD), 
exploring variations in model fine-tuning, input representation, and 
specificity. Subject-level predictions are derived by aggregating 12 individual 
question responses. Our best approach achieved 84% subject-wise accuracy and 1.0 
ROC-AUC using an OpenAI embedding model, per-question training, including 
questions in the input, and combining the predictions with a voting system. In 
addition, a zero-shot evaluation using GPT-4o was conducted, yielding comparable 
results, underscoring the potential of both compact, local models and large 
out-of-the-box LLMs. To enhance transparency, we explored interpretability 
methods. Proprietary LLMs like GPT-4o offered no direct explanation, and OpenAI 
embedding models showed limited interpretability. However, locally deployable 
LLMs provided the highest interpretability. This highlights a trade-off between 
proprietary models' performance and local models' explainability. Our findings 
validate LLMs for automatically classifying open-ended questionnaires, offering 
a scalable, cost-effective complement for ASD assessment. These results suggest 
broader applicability for psychological analysis of other conditions, advancing 
LLM use in mental health research.

DOI: 10.1109/JBHI.2025.3599643
PMID: 40815586


246. J Craniofac Surg. 2025 Aug 14. doi: 10.1097/SCS.0000000000011779. Online ahead 
of print.

Comparing GPT-4o and o1 in Otolaryngology: An Evaluation of Guideline Adherence 
and Accuracy.

Prasad S(1), DiStefano N, Khuu N, Deluca N, Thaller S.

Author information:
(1)Department of Otolaryngology, University of Miami Miller School of Medicine, 
Miami, FL.

Artificial-intelligence chatbots are gaining prominence in otolaryngology, yet 
their clinical safety depends on strict adherence to practice guidelines. The 
authors compared the accuracy of OpenAI's general-purpose GPT-4o model with the 
specialty-tuned o1 model on 100 otolaryngology questions drawn from national 
guidelines and common clinical scenarios spanning 7 subspecialty domains. 
Blinded otolaryngologists graded each answer as correct, partially correct, 
incorrect, or non-answer (scores 1, 0.5, 0, respectively), and paired 
statistical tests assessed performance differences. The o1 model delivered fully 
correct responses for 73% of questions, partially correct for 26%, and incorrect 
for 1%, yielding a mean accuracy score of 0.86. GPT-4o produced 64% correct and 
36% partially correct answers with no incorrect responses, for a mean score of 
0.82. The 4-point gap was not statistically significant (paired t test P=0.165; 
Wilcoxon P=0.157). Pediatric questions had the highest correctness (o1=92.9%, 
GPT-4o=78.6%). No domain showed systematic critical errors. Both models thus 
supplied predominantly guideline-concordant information, and specialty tuning 
conferred only a modest, nonsignificant benefit in this data set. These findings 
suggest contemporary large-language models may approach reliability thresholds 
suitable for supervised decision support in otolaryngology, but continual 
validation and oversight remain essential before routine deployment.

Copyright © 2025 by Mutaz B. Habal, MD.

DOI: 10.1097/SCS.0000000000011779
PMID: 40811069

Conflict of interest statement: The authors report no conflicts of interest.


247. J Eval Clin Pract. 2025 Aug;31(5):e70238. doi: 10.1111/jep.70238.

Is Readability a Proxy for Reliability? A Qualitative Evaluation of ChatGPT-4.0 
in Orthopaedic Trauma Communication.

Atahan MO(1), Üner Ç(2), Aydemir M(2), Uzun MF(3), Yalın M(4), Gölgelioğlu F(5).

Author information:
(1)Department of Orthopedics and Traumatology, Afyonkarahisar State Hospital, 
Afyonkarahisar, Turkey.
(2)Department of Orthopedics and Traumatology, Yozgat City Hospital, Yozgat, 
Turkey.
(3)Department of Orthopedics and Traumatology, Ceylanpınar State Hospital, 
Şanlıurfa, Turkey.
(4)Department of Orthopedics and Traumatology, Yalova University Faculty of 
Medicine, Yalova, Turkey.
(5)Department of Orthopedics and Traumatology, Bozok University Faculty of 
Medicine, Yozgat, Turkey.

AIM: This study aimed to evaluate the accuracy, readability, and safety of 
ChatGPT-4.0's responses to frequently asked questions (FAQs) related to 
orthopaedic trauma and to examine whether readability is associated with the 
quality and reliability of content.
METHODS: Ten common patient questions related to orthopaedic emergencies were 
submitted to ChatGPT-4.0. Each response was assessed independently by three 
orthopaedic trauma surgeons using a 4-point ordinal scale for accuracy, clinical 
appropriateness, and safety. Readability was calculated using the Flesch-Kincaid 
Grade Level (FKGL). Inter-rater agreement was analysed using intraclass 
correlation coefficients (ICC). The presence of disclaimers was also recorded.
RESULTS: ChatGPT-4.0's responses had a mean FKGL score of 10.5, indicating high 
school-level readability. Stratified analysis showed comparable readability 
scores across response quality categories: excellent (10.0), poor (9.8), and 
dangerous (10.1), suggesting that readability does not predict content 
reliability. Accuracy and safety scores varied considerably among responses, 
with the highest inter-rater agreement in clinical appropriateness (ICC = 0.81) 
and the lowest in safety assessments (ICC = 0.68). Notably, nine out of 10 
responses included a disclaimer indicating the nonprofessional nature of the 
content, with one omission observed in a high-risk clinical scenario.
CONCLUSION: Although ChatGPT-4.0 provides generally readable responses to 
orthopaedic trauma questions, readability does not reliably distinguish between 
accurate and potentially harmful information. These findings highlight the need 
for expert review when using AI-generated content in clinical communication.

© 2025 John Wiley & Sons Ltd.

DOI: 10.1111/jep.70238
PMID: 40810381 [Indexed for MEDLINE]


248. Front Public Health. 2025 Jul 30;13:1634638. doi: 10.3389/fpubh.2025.1634638. 
eCollection 2025.

Perceptions of large language models in medical education and clinical practice 
among pediatric emergency physicians in Saudi Arabia: a multiregional 
cross-sectional study.

AlGoraini Y(1), Alsayyali M(2), Alotaibi O(1), Almeshawi I(1), Alaifan F(1), 
Alrashed R(1).

Author information:
(1)Pediatric Emergency Department, King Fahad Medical City, Riyadh, Saudi 
Arabia.
(2)Pediatric Emergency Department, Armed Forced Hospital Alhada, Taif, Saudi 
Arabia.

BACKGROUND: Artificial intelligence (AI) is reshaping healthcare delivery and 
education, but little is known about its perceived value among pediatric 
emergency medicine (PEM) physicians in Saudi Arabia. This study aimed to assess 
the perceptions and experiences of PEM physicians in Saudi Arabia toward the use 
of AI, particularly ChatGPT, in clinical practice and medical education.
METHODS: A cross-sectional, web-based survey was conducted among 100 PEM 
physicians across various regions of Saudi Arabia. The questionnaire explored 
demographics, AI experience, perceived benefits and limitations, and the 
evaluation of ChatGPT-generated clinical and educational content.
RESULTS: Most participants (96%) believed that AI tools, such as ChatGPT, would 
play a significant role in the future of PEM. A high agreement was observed 
regarding AI's usefulness in medical education (91%) and clinical practice, 
particularly in differential diagnosis (77%) and documentation (78%). The 
ChatGPT-generated responses to a clinical scenario (croup) were rated highly for 
validity, reasoning, and educational value. However, 66% of them still preferred 
traditional textbooks for complex topics. The key concerns included accuracy 
(83%), patient safety (56%), and lack of regulatory guidance (52%).
CONCLUSION: Saudi PEM physicians show a strong interest in integrating AI tools, 
such as ChatGPT, into clinical and educational workflows. Although optimism is 
high, concerns about safety, ethics, and oversight highlight the need for 
regulatory frameworks and structured implementation strategies.

Copyright © 2025 AlGoraini, Alsayyali, Alotaibi, Almeshawi, Alaifan and 
Alrashed.

DOI: 10.3389/fpubh.2025.1634638
PMCID: PMC12343484
PMID: 40809779 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


249. Sensors (Basel). 2025 Aug 6;25(15):4845. doi: 10.3390/s25154845.

A Multi-Class Intrusion Detection System for DDoS Attacks in IoT Networks Using 
Deep Learning and Transformers.

Wahab SA(1)(2), Sultana S(1), Tariq N(3), Mujahid M(1), Khan JA(4), Mylonas 
A(4).

Author information:
(1)Department of Computing and Technology, H-9 Campus, Iqra University, 
Islamabad 44000, Pakistan.
(2)Department of Avionics Engineering, Main Campus PAF Complex E-9, Air 
University, Islamabad 44000, Pakistan.
(3)Department of Artificial Intelligence and Data Science, National University 
of Computer and Emerging Sciences, Islamabad 44000, Pakistan.
(4)Department of Computer Science, Cybersecurity and Computing Systems Research 
Group, University of Hertfordshire, Hertfordshire AL10 9AB, UK.

The rapid proliferation of Internet of Things (IoT) devices has significantly 
increased vulnerability to Distributed Denial of Service (DDoS) attacks, which 
can severely disrupt network operations. DDoS attacks in IoT networks disrupt 
communication and compromise service availability, causing severe operational 
and economic losses. In this paper, we present a Deep Learning (DL)-based 
Intrusion Detection System (IDS) tailored for IoT environments. Our system 
employs three architectures-Convolutional Neural Networks (CNNs), Deep Neural 
Networks (DNNs), and Transformer-based models-to perform binary, three-class, 
and 12-class classification tasks on the CiC IoT 2023 dataset. Data 
preprocessing includes log normalization to stabilize feature distributions and 
SMOTE-based oversampling to mitigate class imbalance. Experiments on the CIC-IoT 
2023 dataset show that, in the binary classification task, the DNN achieved 
99.2% accuracy, the CNN 99.0%, and the Transformer 98.8%. In three-class 
classification (benign, DDoS, and non-DDoS), all models attained near-perfect 
performance (approximately 99.9-100%). In the 12-class scenario (benign plus 12 
attack types), the DNN, CNN, and Transformer reached 93.0%, 92.7%, and 92.5% 
accuracy, respectively. The high precision, recall, and ROC-AUC values 
corroborate the efficacy and generalizability of our approach for IoT DDoS 
detection. Comparative analysis indicates that our proposed IDS outperforms 
state-of-the-art methods in terms of detection accuracy and efficiency. These 
results underscore the potential of integrating advanced DL models into IDS 
frameworks, thereby providing a scalable and effective solution to secure IoT 
networks against evolving DDoS threats. Future work will explore further 
enhancements, including the use of deeper Transformer architectures and 
cross-dataset validation, to ensure robustness in real-world deployments.

DOI: 10.3390/s25154845
PMCID: PMC12349258
PMID: 40808008

Conflict of interest statement: The authors declare no conflicts of interest.


250. Sensors (Basel). 2025 Aug 4;25(15):4790. doi: 10.3390/s25154790.

Dual-Branch Deep Learning with Dynamic Stage Detection for CT Tube Life 
Prediction.

Chen Z(1), Liu Y(2), Qin Z(3), Li H(1), Xie S(1), Fan L(1), Liu Q(1), Huang 
J(2).

Author information:
(1)Department of Medical Engineering, West China Hospital, Sichuan University, 
Chengdu 610041, China.
(2)Innovation Institute for Integration of Medicine and Engineering, West China 
Hospital, Sichuan University, Chengdu 610041, China.
(3)Chengdu Women's and Children's Central Hospital, School of Medicine, 
University of Electronic Science and Technology of China, Chengdu 611731, China.

CT scanners are essential tools in modern medical imaging. Sudden failures of 
their X-ray tubes can lead to equipment downtime, affecting healthcare services 
and patient diagnosis. However, existing prediction methods based on a single 
model struggle to adapt to the multi-stage variation characteristics of tube 
lifespan and have limited modeling capabilities for temporal features. To 
address these issues, this paper proposes an intelligent prediction architecture 
for CT tubes' remaining useful life based on a dual-branch neural network. This 
architecture consists of two specialized branches: a residual self-attention 
BiLSTM (RSA-BiLSTM) and a multi-layer dilation temporal convolutional network 
(D-TCN). The RSA-BiLSTM branch extracts multi-scale features and also enhances 
the long-term dependency modeling capability for temporal data. The D-TCN branch 
captures multi-scale temporal features through multi-layer dilated convolutions, 
effectively handling non-linear changes in the degradation phase. Furthermore, a 
dynamic phase detector is applied to integrate the prediction results from both 
branches. In terms of optimization strategy, a dynamically weighted triplet 
mixed loss function is designed to adjust the weight ratios of different 
prediction tasks, effectively solving the problems of sample imbalance and 
uneven prediction accuracy. Experimental results using leave-one-out 
cross-validation (LOOCV) on six different CT tube datasets show that the 
proposed method achieved significant advantages over five comparison models, 
with an average MSE of 2.92, MAE of 0.46, and R2 of 0.77. The LOOCV strategy 
ensures robust evaluation by testing each tube dataset independently while 
training on the remaining five, providing reliable generalization assessment 
across different CT equipment. Ablation experiments further confirmed that the 
collaborative design of multiple components is significant for improving the 
accuracy of X-ray tubes remaining life prediction.

DOI: 10.3390/s25154790
PMCID: PMC12349151
PMID: 40807955 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
declare that are relevant to the content of this article.


251. Med Phys. 2025 Aug;52(8):e18041. doi: 10.1002/mp.18041.

A novel image segmentation network with multi-scale and flow-guided attention 
for early screening of vaginal intraepithelial neoplasia (VAIN).

Li J(1), Xia W(1), Lu J(1), Liu J(2), Qiu L(3)(4), Shi Z(1), Song Y(1), Li Y(5), 
Zhang D(1), Yang H(1)(6), Fu L(7).

Author information:
(1)School of Optical Electrical and Computer Engineering, University of Shanghai 
for Science and Technology, Shanghai, China.
(2)School of Electronic and Electrical Engineering, Shanghai University of 
Engineering Science, Shanghai, China.
(3)Department of Obstetrics and Gynecology, School of Medicine, Ren Ji Hospital, 
Shanghai Jiao Tong University, Shanghai, China.
(4)Shanghai Key Laboratory of Gynecologic Oncology, Ren Ji Hospital, School of 
Medicine, Shanghai Jiao Tong University, Shanghai, China.
(5)Department of Obstetrics and Gynecology, The Third Hospital of Shanxi Medical 
University (Shanxi Bethune Hospital), Shanxi, China.
(6)Key Laboratory of Space Active Opto-electronics Technology, Chinese Academy 
of Sciences, Shanghai, China.
(7)Shanghai First Maternity and Infant Hospital, School of Medicine, Tongji 
University, Shanghai, China.

BACKGROUND: Vaginal intraepithelial neoplasia (VAIN) is a rare precancerous 
lesion, and early diagnosis is crucial for preventing its progression to 
invasive vaginal cancer. However, the subtle differences in morphology and color 
between VAIN lesions and normal vaginal tissue make the automatic segmentation 
of VAIN highly challenging. Existing methods struggle to achieve precise 
segmentation, impacting the efficiency of early screening.
PURPOSE: This study aims to develop a high-accuracy, robust deep learning image 
segmentation network to accurately and automatically segment VAIN lesions, 
thereby improving the efficiency and accuracy of early VAIN screening.
METHODS: We propose a multi-scale dilated attention flow network for VAIN image 
segmentation. This network improves upon the U-Net architecture by optimizing 
the designs of the encoder and decoder and incorporating skip connection 
modules. In the encoding stage, we introduce the dilated squeeze-and-excitation 
(DiSE) module and the flow field guided adaptive separation and enhancement 
(FGASE) module. The DiSE module integrates dilated convolutions with varying 
dilation rates and a channel attention mechanism, effectively extracting 
multi-scale contextual information and enhancing the model's ability to perceive 
VAIN lesions of different sizes. The FGASE module employs flow-guided techniques 
to dynamically separate the features of the main region (VAIN lesions) from the 
edge region and enhance them individually. In the decoding stage, we propose a 
depth wise enhanced pooling (DEP) module that combines deep convolutional layers 
with adaptive pooling strategies to improve local feature extraction 
capabilities and optimize global contextual information. The skip connection 
stage introduces a triple statistical attention (TSA) module that utilizes 
global average pooling, global max pooling, and global standard deviation 
pooling to effectively capture diverse feature information, thereby enhancing 
the model's ability to model long-range dependencies.
RESULTS: Experiments conducted on a VAIN image dataset comprising 1142 patients 
demonstrate that the proposed network significantly outperforms other medical 
image segmentation methods across six metrics: Mean intersection over union 
(MIoU), dice coefficient, accuracy, recall, precision, and mean absolute error 
(MAE). Specifically, this network achieved an MIoU of 0.8461 and a Dice 
coefficient of 0.9166, substantially higher than other comparative methods, with 
a faster convergence speed. Ablation studies further confirm the effectiveness 
of each module in enhancing the model's performance.
CONCLUSIONS: The proposed network exhibits exceptional performance and 
robustness in the task of VAIN image segmentation, effectively segmenting VAIN 
lesions and providing strong technical support for early VAIN screening and 
clinical diagnosis. This work has significant clinical application value.

© 2025 American Association of Physicists in Medicine.

DOI: 10.1002/mp.18041
PMID: 40804792 [Indexed for MEDLINE]


252. Front Dent Med. 2025 Jul 29;6:1634006. doi: 10.3389/fdmed.2025.1634006. 
eCollection 2025.

Evaluating the accuracy of generative artificial intelligence models in dental 
age estimation based on the Demirjian's method.

Abuabara A(1), do Nascimento TVPM(2), Trentini SM(1), Costa Gonçalves AM(1), 
Hueb de Menezes-Oliveira MA(3), Madalena IR(3), Beisel-Memmert S(4), Kirschneck 
C(4), Antunes LAA(5), Miranda de Araujo C(2), Baratto-Filho F(1)(2), Küchler 
EC(4).

Author information:
(1)Post-Graduation Program in Health and Environment, University from the 
Joinville Region - Univille, Joinville, Brazil.
(2)School of Dentistry, Tuiuti University of Paraná - UTP, Curitiba, Brazil.
(3)Department of Biomaterials, University of Uberaba - UNIUBE, Uberaba, Brazil.
(4)Department of Orthodontics, University Hospital Bonn, Medical Faculty, Bonn, 
Germany.
(5)Postgraduate Program in Dentistry, Health Institute of Nova Friburgo, 
Fluminense Federal University, Niterói, Rio de Janeiro, Brazil.

INTRODUCTION: Dental age estimation plays a key role in forensic identification, 
clinical diagnosis, treatment planning, and prognosis in fields such as 
pediatric dentistry and orthodontics. Large language models (LLM) are 
increasingly being recognized for their potential applications in Dentistry. 
This study aimed to compare the performance of currently available generative 
artificial intelligence LLM technologies in estimating dental age using the 
Demirjian's scores.
METHODS: Panoramic radiographs were analyzed using Demirjian's method (1973), 
with each left permanent mandibular tooth classified from stage A to H. 
Untrained LLM, ChatGPT (GPT-4-turbo), Gemini 2.0 Flash, and DeepSeek-V3 were 
tasked with estimating dental age based on the patient's Demirjian score for 
each tooth. Due to the probabilistic nature of ChatGPT, Gemini, and DeepSeek, 
which can produce varying responses to the same question, three responses were 
collected per case per day (three different computers) from each model on three 
separate days. The age estimates obtained from LLM were compared to the 
individuals' chronological ages. Intra- and inter-examiner reliability was 
assessed using the Intraclass Correlation Coefficient (ICC). Model performance 
was evaluated using Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), 
Coefficient of Determination (R 2), and Bias.
RESULTS: Thirty panoramic radiographs (40% female, 60% male; mean age 
10.4 ± 2.32 years) were included. Both intra- and inter-examiner ICC values 
exceeded 0.85. ChatGPT and DeepSeek exhibited comparable but suboptimal 
performance, with higher errors (MAE: 1.98-2.05 years; RMSE: 2.33-2.35 years), 
negative R 2 values (-0.069 to -0.049), and substantial overestimation biases 
(1.90-1.91 years), indicating poor model fit and systematic flaws. Gemini 
demonstrated intermediate results, with a moderate MAE (1.57 years) and RMSE 
(1.81 years), a positive R 2 (0.367), and a lower bias (1.32 years).
DISCUSSION: This study demonstrated that, although LLM like ChatGPT, Gemini, and 
DeepSeek can estimate dental age using Demirjian's scores, their performance 
remains inferior to the traditional method. Among them, DeepSeek-V3 showed the 
best results, but all models require task-specific training and validation 
before clinical application.

© 2025 Abuabara, do Nascimento, Trentini, Costa Gonçalves, Hueb de 
Menezes-Oliveira, Madalena, Beisel-Memmert, Kirschneck, Antunes, Miranda de 
Araujo, Baratto-Filho and Küchler.

DOI: 10.3389/fdmed.2025.1634006
PMCID: PMC12339434
PMID: 40800006

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


253. JAMIA Open. 2025 Aug 10;8(4):ooaf080. doi: 10.1093/jamiaopen/ooaf080. 
eCollection 2025 Aug.

Ensemble learning to enhance accurate identification of patients with glaucoma 
using electronic health records.

Mungle T(1), Naderalvojoud B(1), Andrews CA(2), An HS(2), Bicket A(2), Zhang 
A(2), Rosenthal J(2), Lee WS(3), Ludwig CA(3), Mekonnen B(3), Pershing S(3), 
Stein JD(2)(4), Hernandez-Boussard T(1); SOURCE Consortium.

Collaborators: Pershing S, Wang SY, Amin S, Edwards PA, Srikumaran D, Woreta F, 
Schultz JS, Shrivastava A, Ahmad B, Pasquale LP, Bryar P, French D, Ramachandran 
R, Vanderbeek BL, Ganapathy P, Deiner M, Sun C, Patnaik J, Subramanian P, Munir 
S, Munir W, Stein JD, De Lott L, Feldman R, Stagg BC, Wirostko B, McMillian B, 
Sheybani A, Liu J, Sarrapour S.

Author information:
(1)Department of Medicine, Stanford University, Stanford, CA 94305, United 
States.
(2)Department of Ophthalmology and Visual Sciences, University of Michigan, Ann 
Arbor, MI 48105, United States.
(3)Department of Ophthalmology, Byers Eye Institute, Stanford University, 
Stanford, CA 94305, United States.
(4)Department of Health Management and Policy, School of Public Health, 
University of Michigan, Ann Arbor, MI 48109, United States.

OBJECTIVES: Existing ophthalmology studies for clinical phenotypes 
identification in real-world datasets (RWD) rely exclusively on structured data 
elements (SDE). We evaluated the performance, generalizability, and fairness of 
multimodal ensemble models that integrate real-world SDE and free-text data 
compared to SDE-only models to identify patients with glaucoma.
MATERIALS AND METHODS: This is a retrospective cross-sectional study involving 2 
health systems- University of Michigan (UoM) and Stanford University (SU). It 
involves 1728 patients visiting eye clinics during 2012-2021. Free-text 
embeddings extracted using BioClinicalBERT were combined with SDE. 
EditedNearestNeighbor (ENN) undersampling and Borderline-Synthetic Minority 
Over-sampling Technique (bSMOTE) addressed class imbalance. Lasso Regression 
(LR), Random Forest (RF), Support Vector Classifier (SVC) models were trained on 
UoM imbalanced (imb) and resampled data along with bagging ensemble method. 
Models were externally validated with SU data. Fairness was assessed using 
equalized odds difference (EOD) and Target Probability Difference (TPD).
RESULTS: Among 900 and 828 patients from UoM and SU, 10% and 23% respectively 
had glaucoma as confirmed by ophthalmologists. At UoM, multimodal LRimb 
(F1 = 76.60 [61.90-88.89]; AUROC = 95.41 [87.01-99.63]) outperformed unimodal 
RFimb (F1 = 69.77 [52.94-83.64]; AUROC = 97.72 [95.95-99.18]) and ICD-coding 
method (F1 = 53.01 [39.51-65.43]; AUROC = 90.10 [84.59-93.93]). Bagging 
(BM = LRENN + LRbSMOTE) improved performance achieving an F1 of 83.02 
[70.59-92.86] and AUROC of 97.59 [92.98-99.88]. During external validation BM 
achieved the highest F1 (68.47 [62.61-73.75]), outperforming unimodal 
(F1 = 51.26 [43.80-58.13]) and multimodal LRimb (F1 = 62.46 [55.95-68.24]). BM 
EOD revealed lower disparities for sex (<0.1), race (<0.5) and ethnicity (<0.5), 
and had least uncertainty using TDP analysis as compared to traditional models.
DISCUSSION: Multimodal ensemble models integrating structured and unstructured 
EHR data outperformed traditional SDE models achieving fair predictions across 
demographic sub-groups. Among ensemble methods, bagging demonstrated better 
generalizability than stacking, particularly when training data is limited.
CONCLUSION: This approach can enhance phenotype discovery to enable future 
research studies using RWD, leading to better patient management and clinical 
outcomes.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamiaopen/ooaf080
PMCID: PMC12342940
PMID: 40799932

Conflict of interest statement: None declared.


254. Opt Express. 2025 Mar 24;33(6):13848-13863. doi: 10.1364/OE.558277.

Enhanced transformer method coupled with transfer learning for surface defect 
segmentation of myopia control spectacle lenses.

Wang R, Cheung CF, Wang B, Zhu Z, Tse DY.

Myopia requires visual correction. The complications associated with myopia 
affect a large population of schoolchildren around the world. Nanostructured 
myopia control spectacle lenses (NMCSLs) containing nano surface features are 
commonly used as a non-invasive approach for slowing down the progression of 
myopia. However, the effective segmentation of surface defects generated in the 
precision manufacturing of the NMCSL heavily relies on highly efficient and 
effective defect detection and characterization methods. As a result, this paper 
presents an enhanced transformer method coupled with the transfer learning 
(E2Trans) method, which combines the powerful feature extraction abilities of 
the transformer and the knowledge re-usage abilities of transfer learning to 
realize high-efficiency and high-accuracy defect segmentation. To further 
improve the segmentation performance, two auxiliary decoders are added to adjust 
the training loss. To validate the model's performance, a lens defect dataset is 
built, and a series of experiments are conducted. The results show that our 
proposed model can segment five lens defects, including notches, black spots, 
bubbles, fibers, and scratches with high segmentation accuracy and speed. In 
addition, a detection system is developed for real-time lens defect detection.

DOI: 10.1364/OE.558277
PMID: 40798189


255. J Crohns Colitis. 2025 Aug 12:jjaf134. doi: 10.1093/ecco-jcc/jjaf134. Online 
ahead of print.

Results of the 9th Scientific Workshop of the European Crohn's and Colitis 
Organisation (ECCO): Artificial Intelligence in medical management and precision 
medicine.

Kopylov U(1)(2), Verstockt B(3), Marigorta UM(4), Noviello D(5), Bossuyt P(6), 
Mookhoek A(7), Sinonque P(3), El-Hussuna A(8), Sahnan K(9), Baumgart DC(10), 
Noor NM(11), Allocca M(12), Carter D(1)(2), Ensari A(13), Iacucci M(14), Pellino 
G(15), Soriano A(16), de Laffolie J(17), Daperno M(18), Raine T(11), Cleynen 
I(19), Sebastian S(20)(21).

Author information:
(1)Department of Gastroenterology, Sheba Medical Center, Tel Hashomer, Israel.
(2)Faculty of Medicine, Tel Aviv University, Israel.
(3)Department of Gastroenterology, KU Leuven, Leuven, Belgium.
(4)Integrative Genomics Lab, Center for Cooperative Research in Biosciences (CIC 
bioGUNE), Basque Research and Technology Alliance (BRTA), Bizkaia Technology 
Park, Derio, Basque Country, and Ikerbasque, Basque Foundation for Science, 
48013 Bilbao, Bizkaia, Spain.
(5)Department of Pathophysiology and Transplantation, University of Milan, 
Milan, Italy.
(6)Department of Gastroenterology, Imelda General Hospital, Bonheiden, Belgium.
(7)Institute of Tissue Medicine and Pathology, University of Bern, Bern, 
Switzerland.
(8)OpenSourceResearch organisation, Aalborg, Denmark.
(9)St Marks Hospital, London, United Kingdom.
(10)Division of Gastroenterology and Hepatology, University of Alberta, 
Edmonton, Alberta, Canada.
(11)Department of Gastroenterology, Cambridge University Hospitals NHS 
Foundation Trust, Cambridge, United Kingdom.
(12)IRCCS Hospital San Raffaele and University Vita-Salute San Raffaele, 
Gastroenterology and Endoscopy, Milan, Italy.
(13)Department of Pathology, Ankara University, Ankara, Turkey.
(14)APC Microbiome Ireland, College of Medicine and Health, University College 
of Cork, Cork, Ireland.
(15)Department of Surgery, University of Campania "Luigi Vanvitelli," Naples, 
Italy.
(16)Department of Internal Medicine, Arcispedale Santa Maria Nuova, IRCCS, 
Reggio Emilia, Italy.
(17)Department General Paediatrics and Neonatology, University Giessen, Giessen, 
Germany.
(18)Gastroenterology Unit, AO Ordine Mauriziano, Turin, Italy.
(19)Department of Human Genetics, KU Leuven, Leuven, Belgium.
(20)IBD Unit, Department of Gastroenterology, Hull University Teaching Hospitals 
NHS Trust, Hull, United Kingdom.
(21)Hull York Medical School, Hull, United Kingdom.

BACKGROUND AND AIMS: Artificial intelligence (AI) is increasingly being applied 
in various fields of medicine, including Inflammatory Bowel Diseases (IBD). This 
systematic review, conducted as part of the ECCO 9th Scientific Workshop on AI 
in IBD, explores AI applications in multiomic precision medicine, large language 
models (LLMs) for textual tasks and utilisation of wearable and remote care 
technologies.
METHODS: A comprehensive systematic analysis of the literature was undertaken, 
emphasising three topics: multiomic predictive models in IBD; natural language 
processing (NLP) and LLMs for clinical practice, research and patient 
communication; and the role of remote monitoring and wearable devices.
RESULTS: Key areas of promise include the implementation of NLP and LLMs for 
case identification and differentiation, tracking disease activity, 
pharmacovigilance, quality assurance and patient support. Multiomic approaches, 
integrating genomics, transcriptomics, proteomics, metabolomics and 
metagenomics, show potential for developing more accurate diagnostic and risk 
prediction models and improving treatment response prediction and detection of 
actionable drug targets for future therapeutics. Wearables and remote monitoring 
technologies can transform IBD management from episodic assessments to 
continuous less biased tracking of patient-reported outcomes and physiological 
biomarkers.
CONCLUSIONS: While AI and multiomic approaches hold substantial promise for 
advancing IBD management and research, further refinement is necessary to ensure 
content validity and address safety concerns, thereby allowing integration of AI 
into clinical workflows and safeguarding of data privacy. Future research should 
prioritise the integration of diverse omic data, conduct of longitudinal studies 
and validation in large and diverse cohorts.

© The Author(s) 2025. Published by Oxford University Press on behalf of European 
Crohn’s and Colitis Organisation. All rights reserved. For commercial re-use, 
please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/ecco-jcc/jjaf134
PMID: 40796214


256. Annu Rev Biomed Data Sci. 2025 Aug;8(1):227-250. doi: 
10.1146/annurev-biodatasci-111224-124530.

Leveraging Unstructured Data in Electronic Health Records to Detect Adverse 
Events from Pediatric Drug Use: A Scoping Review.

Golder S(1), O'Connor K(2), Lopez-Garcia G(3), Tatonetti NP(3), 
Gonzalez-Hernandez G(3).

Author information:
(1)Department of Health Sciences, University of York, York, United Kingdom; 
email: su.golder@york.ac.uk.
(2)Department of Biostatistics, Epidemiology and Informatics, Perelman School of 
Medicine, University of Pennsylvania, Philadelphia, Pennsylvania, USA.
(3)Department of Computational Biomedicine, Cedars-Sinai Medical Center, Los 
Angeles, California, USA.

Update of
    medRxiv. 2025 Mar 20:2025.03.20.25324320. doi: 10.1101/2025.03.20.25324320.

Adverse drug events (ADEs) in pediatric populations pose significant public 
health challenges, yet research on their detection and monitoring remains 
limited. This scoping review evaluates the use of unstructured data from 
electronic health records (EHRs) to identify ADEs in children. We searched six 
databases, including MEDLINE, Embase, and IEEE Xplore, in September 2024. From 
984 records, only nine studies met our inclusion criteria, indicating a 
significant gap in research toward identifying ADEs in children. We found that 
unstructured data in EHRs can indeed be of value and enhance pediatric 
pharmacovigilance, although their use has been so far very limited. Traditional 
natural language processing methods have been employed to extract ADEs, but the 
approaches utilized face challenges in generalizability and context 
interpretation. These challenges could be addressed with recent advances in 
transformer-based models and large language models, unlocking the use of EHR 
data at scale for pediatric pharmacovigilance.

DOI: 10.1146/annurev-biodatasci-111224-124530
PMID: 40789734 [Indexed for MEDLINE]


257. Acad Radiol. 2025 Oct;32(10):5693-5702. doi: 10.1016/j.acra.2025.07.046. Epub 
2025 Aug 7.

Structured Report Generation for Breast Cancer Imaging Based on Large Language 
Modeling: A Comparative Analysis of GPT-4 and DeepSeek.

Chen K(1), Hou X(2), Li X(3), Xu W(3), Yi H(4).

Author information:
(1)Department of Nuclear Medicine, Zhejiang Cancer Hospital, Hangzhou Institute 
of Medicine (HIM), Chinese Academy of Sciences, Hangzhou, Zhejiang 310022, China 
(K.C., H.Y.).
(2)Department of Medical Imaging, Tianjin Children's Hospital (Tianjin 
University Children's Hospital), Tianjin, China (X.H.).
(3)Department of Molecular Imaging and Nuclear Medicine, Tianjin Medical 
University Cancer Institute and Hospital, National Clinical Research Center for 
Cancer, Tianjin, China (X.L., W.X.).
(4)Department of Nuclear Medicine, Zhejiang Cancer Hospital, Hangzhou Institute 
of Medicine (HIM), Chinese Academy of Sciences, Hangzhou, Zhejiang 310022, China 
(K.C., H.Y.). Electronic address: yihq@zjcc.org.cn.

RATIONALE AND OBJECTIVES: The purpose of this study is to compare the 
performance of GPT-4 and DeepSeek large language models in generating structured 
breast cancer multimodality imaging integrated reports from free-text radiology 
reports including mammography, ultrasound, MRI, and PET/CT.
MATERIALS AND METHODS: A retrospective analysis was conducted on 1358 free-text 
reports from 501 breast cancer patients across two institutions. The study 
design involved synthesizing multimodal imaging data into structured reports 
with three components: primary lesion characteristics, metastatic lesions, and 
TNM staging. Input prompts were standardized for both models, with GPT-4 using 
predesigned instructions and DeepSeek requiring manual input. Reports were 
evaluated based on physician satisfaction using a Likert scale, descriptive 
accuracy including lesion localization, size, SUV, and metastasis assessment, 
and TNM staging correctness according to NCCN guidelines. Statistical analysis 
included McNemar tests for binary outcomes and correlation analysis for 
multiclass comparisons with a significance threshold of P < .05.
RESULTS: Physician satisfaction scores showed strong correlation between models 
with r-values of 0.665 and 0.558 and P-values below .001. Both models 
demonstrated high accuracy in data extraction and integration. The mean accuracy 
for primary lesion features was 91.7% for GPT-4% and 92.1% for DeepSeek, while 
feature synthesis accuracy was 93.4% for GPT4 and 93.9% for DeepSeek. Metastatic 
lesion identification showed comparable overall accuracy at 93.5% for GPT4 and 
94.4% for DeepSeek. GPT-4 performed better in pleural lesion detection with 
94.9% accuracy compared to 79.5% for DeepSeek, whereas DeepSeek achieved higher 
accuracy in mesenteric metastasis identification at 87.5% vs 43.8% for GPT4. TNM 
staging accuracy exceeded 92% for T-stage and 94% for M-stage, with N-stage 
accuracy improving beyond 90% when supplemented with physical exam data.
CONCLUSION: Both GPT-4 and DeepSeek effectively generate structured breast 
cancer imaging reports with high accuracy in data mining, integration, and TNM 
staging. Integrating these models into clinical practice is expected to enhance 
report standardization and physician productivity.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.acra.2025.07.046
PMID: 40780984 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


258. JMIR Res Protoc. 2025 Aug 8;14:e73602. doi: 10.2196/73602.

Health Care Professionals' Experiences and Opinions About Generative AI and 
Ambient Scribes in Clinical Documentation: Protocol for a Scoping Review.

Garcia Sanchez C(1), Kharko A(1)(2), Hägglund M(1)(3), Riggare S(1), Blease 
C(1)(4).

Author information:
(1)Participatory eHealth and Health Data Research Group, Department of Women's 
and Children's Health, Uppsala University, Uppsala, Sweden.
(2)Centre for Primary Care and Health Services Research, University of 
Manchester, Manchester, United Kingdom.
(3)Uppsala MedTech Science and Innovation, Uppsala University Hospital, Uppsala, 
Sweden.
(4)Division of Digital Psychiatry, Harvard Medical School, Beth Israel Deaconess 
Medical Center, Boston, United States.

BACKGROUND: Generative artificial intelligence (GenAI) leverages large language 
models (LLMs) that are transforming health care. Specialized ambient GenAI 
tools, like Nuance Dax, Speke, and Tandem Health, "listen" to consultations and 
generate clinical notes. Medical-focused models, like Med-PaLM, provide tailored 
health care insights. GenAI's capability to summarize complex data and generate 
responses in various conversational styles or literacy levels makes it 
particularly valuable since it has the potential to alleviate the burden of 
clinical documentation on health care professionals (HCPs). While GenAI may 
prove to be helpful, offering novel benefits, it comes with its own set of 
challenges. The quality of the source data can introduce biases, leading to 
skewed recommendations or outright false information (so-called hallucinations). 
In addition, due to the conversational nature of chatbot responses, users may be 
susceptible to misinformation, posing risks to both safety and privacy. 
Therefore, careful implementation and rigorous oversight are essential to ensure 
accuracy, ethical integrity, and alignment with clinical standards. Despite 
these advances, currently, no review has investigated HCPs' experiences and 
opinions about GenAI in clinical documentation. Yet, such a perspective is 
crucial to better understand how these technologies can be safely and ethically 
adopted and implemented in clinical practice.
OBJECTIVE: We aim to present the protocol for a scoping review exploring HCPs' 
experiences and opinions about GenAI and ambient scribes in clinical 
documentation.
METHODS: This scoping review will be carried out following the methodological 
framework of Arksey and O'Malley and the PRISMA-ScR (Preferred Reporting Items 
for Systematic Reviews and Meta-Analyses for Scoping Reviews) checklist. 
Relevant papers will be searched for in PubMed, IEEE Xplore, APA PsycInfo, 
CINAHL, and Web of Science. The review will include studies published between 
January 2023 and September 2025. Studies will be included that represent 
original peer-reviewed work that explores HCPs' experiences and opinions about 
the use of GenAI or ambient scribes for clinical documentation. Data extraction 
will include publication type, country, sample characteristics, clinical 
setting, study aim, study design, research question, and key findings. Study 
quality will be assessed using the Mixed Methods Appraisal Tool.
RESULTS: The results will be presented as a narrative synthesis structured along 
the key themes of the evidence mapped. Data will be collated and presented in 
charts and tabular format. Findings will be reported in a peer-reviewed scoping 
review.
CONCLUSIONS: This will be the first scoping review that considers HCPs' 
experiences and opinions about GenAI and ambient scribes in clinical 
documentation. The results will clarify how HCPs use-or avoid using-GenAI in 
daily health care work. This insight will help address perceived benefits, 
risks, expectations, and uncertainties. It may also reveal key research gaps in 
the field.
INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): PRR1-10.2196/73602.

©Carolina Garcia Sanchez, Anna Kharko, Maria Hägglund, Sara Riggare, Charlotte 
Blease. Originally published in JMIR Research Protocols 
(https://www.researchprotocols.org), 08.08.2025.

DOI: 10.2196/73602
PMCID: PMC12374137
PMID: 40779760 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


259. Clin Transl Oncol. 2025 Aug 8. doi: 10.1007/s12094-025-04014-9. Online ahead of 
print.

Development and validation of a transformer-based deep learning model for 
predicting distant metastasis in non-small cell lung cancer using (18)FDG PET/CT 
images.

Hu N(#)(1), Luo Y(#)(2), Tang M(#)(1), Yan G(3), Yuan S(4), Li F(1), Lei P(5).

Author information:
(1)Department of Radiology, The Affiliated Hospital of Guizhou Medical 
University, No. 28, Guiyi Street, Yunyan District, Guiyang, 550004, China.
(2)Department of Anesthesiology, Guizhou Provincial People's Hospital, Guiyang, 
Guizhou Province, China.
(3)Department of Nuclear Medicine, The Affiliated Hospital of Guizhou Medical 
University, Guiyang, 550004, China.
(4)Department of Ultrasound Center, Affliated Hospital of Guizhou Medical 
University, Guiyang, 550004, China.
(5)Department of Radiology, The Affiliated Hospital of Guizhou Medical 
University, No. 28, Guiyi Street, Yunyan District, Guiyang, 550004, China. 
pingguilei@foxmail.com.
(#)Contributed equally

BACKGROUND: This study aimed to develop and validate a hybrid deep learning (DL) 
model that integrates convolutional neural network (CNN) and vision transformer 
(ViT) architectures to predict distant metastasis (DM) in patients with 
non-small cell lung cancer (NSCLC) using 18F-FDG PET/CT images.
METHODS: A retrospective analysis was conducted on a cohort of consecutively 
registered patients who were newly diagnosed and untreated for NSCLC. A total of 
167 patients with available PET/CT images were included in the analysis. DL 
features were extracted using a combination of CNN and ViT architectures, 
followed by feature selection, model construction, and evaluation of model 
performance using the receiver operating characteristic (ROC) and the area under 
the curve (AUC).
RESULTS: The ViT-based DL model exhibited strong predictive capabilities in both 
the training and validation cohorts, achieving AUCs of 0.824 and 0.830 for CT 
features, and 0.602 and 0.694 for PET features, respectively. Notably, the model 
that integrated both PET and CT features demonstrated a notable AUC of 0.882 in 
the validation cohort, outperforming models that utilized either PET or CT 
features alone. Furthermore, this model outperformed the CNN model (ResNet 50), 
which achieved an AUC of 0.752 [95% CI 0.613, 0.890], p < 0.05. Decision curve 
analysis further supported the efficacy of the ViT-based DL model.
CONCLUSION: The ViT-based DL developed in this study demonstrates considerable 
potential in predicting DM in patients with NSCLC, potentially informing the 
creation of personalized treatment strategies. Future validation through 
prospective studies with larger cohorts is necessary.

© 2025. The Author(s), under exclusive licence to Federación de Sociedades 
Españolas de Oncología (FESEO).

DOI: 10.1007/s12094-025-04014-9
PMID: 40779149

Conflict of interest statement: Declarations. Conflict of interest: The authors 
have no relevant financial or non-financial interests to disclose. Consent to 
publish: All participants consented to the release of their data to the journal. 
Ethical approval: This study was approved by the institutional review board of 
our institute. Informed consent: Written informed consent was not obtained from 
individual patients because Affiliated Hospital of Guizhou Medical University 
does not mandate informed consent for retrospective studies approved by the 
ethics committee.


260. Stud Health Technol Inform. 2025 Aug 7;329:1650-1651. doi: 10.3233/SHTI251146.

Leveraging State Machine Models to Develop a Knowledge Summary for Juvenile 
Sjögren's Disease.

Gomez GF(1)(2), Cummins J(2), Taylor D(2), Shen J(2), Madarapu S(1), Kunam 
A(1)(3), Sankuratri BYV(1), Vetter C(4), Thyvalikakath T(1)(2).

Author information:
(1)Indiana University School of Dentistry, Indiana University Indianapolis, IN, 
USA.
(2)Regenstrief Institute Inc., Indianapolis, IN, USA.
(3)Luddy School of Informatics, Computing, and Engineering, Indianapolis, IN, 
USA.
(4)Indiana University School of Medicine, Indiana University Indianapolis, IN, 
USA.

Juvenile Sjögren's disease (jSD) is a rare condition with no diagnostic 
criteria. The study aims to automate information extraction from biomedical 
texts by a rule-based state machine natural language processing (NLP) model to 
generate a knowledge summary on jSD. A state machine model was constructed to 
detect and classify keyterms within nDepth™. Preliminary results are based on 
processing 2,043 articles with 12,588,356 text lines based on a series of states 
and transition rules, requiring further validation.

DOI: 10.3233/SHTI251146
PMID: 40776163 [Indexed for MEDLINE]


261. Stud Health Technol Inform. 2025 Aug 7;329:1170-1174. doi: 10.3233/SHTI251023.

Enhancing Vaccine Safety Surveillance: Extracting Vaccine Mentions from 
Emergency Department Triage Notes Using Fine-Tuned Large Language Models.

Khademi S(1), Black J(2), Palmer C(1), Javed M(1), Clothier H(1), Buttery 
J(1)(3), Dimaguila GL(1).

Author information:
(1)Murdoch Children's Research Institute, Parkville, Australia.
(2)Department of Health, State Government of Victoria, Melbourne, Australia.
(3)Department of Paediatrics, The University of Melbourne, Parkville, Australia.

This study evaluates fine-tuned Llama 3.2 models for extracting vaccine-related 
information from emergency department triage notes to support near real-time 
vaccine safety surveillance. Prompt engineering was used to initially create a 
labeled dataset, which was then confirmed by human annotators. The performance 
of prompt-engineered models, fine-tuned models, and a rule-based approach was 
compared. The fine-tuned Llama 3 billion parameter model outperformed other 
models in its accuracy of extracting vaccine names. Model quantization enabled 
efficient deployment in resource-constrained environments. Findings demonstrate 
the potential of large language models in automating data extraction from 
emergency department notes, supporting efficient vaccine safety surveillance and 
early detection of emerging adverse events following immunization issues.

DOI: 10.3233/SHTI251023
PMID: 40776041 [Indexed for MEDLINE]


262. Stud Health Technol Inform. 2025 Aug 7;329:683-687. doi: 10.3233/SHTI250927.

Can Generative LLMs Help Classify Imbalanced Real-World Data? Exploring Rare 
Diseases on Social Media.

Le Priol E(1), Potier J(1), Burgun A(1)(2).

Author information:
(1)Clinical Bio-Informatics Laboratory, Université Paris Cité, INSERM UMR 1163, 
Imagine Institute, Paris, France.
(2)Department of Medical Informatics, Necker Hospital, AP-HP.

Developmental and Epileptic Encephalopathies (DEEs) are rare, severe conditions 
often discussed by families on social media, offering valuable insights into 
their experiences. Identifying these messages amidst unrelated content is 
crucial but challenging due to data imbalance. This study evaluates different 
uses of generative large language models (LLMs) for binary classification of 
DEE-related experiences within social media posts. Using CamemBERT as a 
baseline, we compared two strategies: zero-shot prompt-based classification and 
synthetic data generation for minority class augmentation. While zero-shot 
prompting underperformed, the addition of 2% synthetic data improved all metrics 
(macro/positive F1, precision and recall). Higher proportions of synthetic data 
led to decreased precision. These findings underscore the potential of hybrid 
approaches combining fine-tuning and domain-specific synthetic data for 
addressing data imbalance in rare disease contexts. Further validation across 
models and datasets is needed.

DOI: 10.3233/SHTI250927
PMID: 40775945 [Indexed for MEDLINE]


263. Stud Health Technol Inform. 2025 Aug 7;329:613-617. doi: 10.3233/SHTI250913.

A Novel Model for Generating Patient Laboratory Test Orders from Admission: 
Transformer Model Approach.

Hyohdoh Y(1), Nomura KY(1), Mitani K(1), Hatakeyama Y(1).

Author information:
(1)Center of Medical Information Science, Kochi Medical School, Kochi 
University.

There is a growing demand for medical pseudo-data that maintains statistical 
utility, enabling the analysis of a wide range of medical data without 
compromising patient privacy. Additionally, there is a growing need for 
effective sequence prediction in healthcare. The proposed model in this 
research, utilizes GPT-2, a Transformer model from natural language processing, 
to generate laboratory test order data for patients from admission to discharge 
while incorporating temporal processes. This research utilized patient data from 
Kochi Medical School Hospital, Japan, with specific inclusion criteria. The 
methodology involved converting patient profiles and order data into categorical 
formats and employing SentencePiece tokenization to create 8000 tokens for 
training the GPT-2 model. The model's performance was evaluated across three 
patterns, predicting laboratory test orders at various times during 
hospitalization. It was compared to a gradient boosting decision tree (GBDT) 
model using F-scores as the evaluation metric. Results indicated that while the 
GBDT model performed better based solely on admission information, the GPT-2 
model showed superior performance when using time-series information 
post-admission. Our research proposed a model that successfully considered 
temporal processes to generate laboratory test order data for patients from 
admission to discharge using GPT-2, a Transformer model developed in the field 
of natural language processing.

DOI: 10.3233/SHTI250913
PMID: 40775931 [Indexed for MEDLINE]


264. Neural Netw. 2025 Dec;192:107920. doi: 10.1016/j.neunet.2025.107920. Epub 2025 
Jul 29.

Family-based continual learning for multi-domain pattern analysis in federated 
frameworks with GCN and ViT.

Iqbal S(1), Zhong X(2), Khan MA(3), Wu Z(4), Alhammadi DA(5), Liu W(6).

Author information:
(1)College of Mechatronics and Control Engineering, Shenzhen University, 
Shenzhen, 518060, China. Electronic address: saeediqbalkhattak@gmail.com.
(2)College of Mechatronics and Control Engineering, Shenzhen University, 
Shenzhen, 518060, China. Electronic address: xzhong@szu.edu.cn.
(3)Center of AI, Prince Mohammad Bin Fahd University, Alkhobar, Kingdom of Saudi 
Arabia. Electronic address: attique.khan@ieee.org.
(4)College of Mechatronics and Control Engineering, Shenzhen University, 
Shenzhen, 518060, China. Electronic address: zzwu@szu.edu.cn.
(5)Department of Information Systems, College of Computer and Information 
Sciences, Princess Nourah bint Abdulrahman University, P.O.Box 84428, Riyadh, 
11671, Saudi Arabia. Electronic address: daalhammadi@pnu.edu.sa.
(6)College of Mechatronics and Control Engineering, Shenzhen University, 
Shenzhen, 518060, China. Electronic address: wxliu@szu.edu.cn.

Continual Learning (CL) and Federated Learning (FL) integration have attracted a 
lot of interest in dynamic and decentralized areas where client data 
distributions show a lot of unpredictability, such as industrial imaging, 
satellite images, medical imaging, and robotic vision. Catastrophic forgetting, 
non-IID data, and the requirement for effective model updates across clients 
with restricted data privacy are issues that traditional FL approaches find 
difficult to handle. These limits impede the development of robust models that 
can generalize across a range of applications and adjust to changing data and 
resource limitations. In this research, we offer a novel framework for FL 
(FedCL) that combines Graph Convolutional Networks (GCNs) and Vision 
Transformers (ViTs) with Family-based CL (FCL). Our approach reduces 
catastrophic forgetting and allows the model to be dynamically adjusted to 
various client data distributions by introducing a hierarchical, three-tiered 
model architecture made up of the Parent Model (Learning Model), Grandparent 
Model (Stable Model), and Child Model (Plastic Model). The system utilizes the 
power of GCN for capturing structural links in patient data and ViT's 
self-attention mechanism for fast feature extraction, assuring stable 
performance across varied datasets. Knowledge Distillation Loss (KDL) and 
surrogate ratios are added to the model to improve learning and facilitate 
efficient information transfer. We assess our proposed approach on several 
benchmark datasets, such as FashionMNIST, MedMNIST, and DigitMNIST, and validate 
it using the MVTeC AD and Vision dataset under several criteria, including 
F1-score (97.0 %), accuracy (97.6 %), precision (97.2 %), Learning Performance 
(LP - 97.3 %), and Anomaly Identification Performance (AIP - 96.5 %). Our 
findings show that the suggested FCL framework considerably lowers catastrophic 
forgetting across domains with different data properties while outperforming 
conventional FL techniques in terms of model adaptability, data privacy 
preservation, and computational efficiency. The suggested approach offers a 
viable path forward for the development of federated CL in intricate, practical 
applications. The code and data supporting the findings of this study are 
available at the GitHub link: FedViTCL.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.neunet.2025.107920
PMID: 40773783 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


265. J Med Internet Res. 2025 Aug 6;27:e74428. doi: 10.2196/74428.

Generative Artificial Intelligence in Primary Care: Qualitative Study of UK 
General Practitioners' Views.

Blease C(1), Garcia Sanchez C(1), Locher C(2), McMillan B(3), Gaab J(2), Torous 
J(4).

Author information:
(1)Participatory eHealth & Health data Research Group, Department of Women's and 
Children's Health, Uppsala University, MTC-huset, Dag Hammarskjölds väg 14B, 1 
tr, Uppsala, 752 37, Sweden, 46 184710000.
(2)Clinical Psychology and Psychosomatics, Faculty of Psychology, University of 
Basel, Basel, Switzerland.
(3)Centre for Primary Care and Health Services Research, University of 
Manchester, Manchester, United Kingdom.
(4)Department of Psychiatry, Beth Israel Deaconess Medical Center, Harvard 
Medical School, Boston, MA, United States.

BACKGROUND: The potential for generative artificial intelligence (GenAI) to 
assist with clinical tasks is the subject of ongoing debate within biomedical 
informatics and related fields.
OBJECTIVE: This study aimed to explore general practitioners' (GPs') opinions 
about GenAI on primary care.
METHODS: In January 2025, we conducted a web-based survey of 1005 UK GPs' 
experiences and opinions of GenAI in clinical practice. This study involved a 
qualitative inductive descriptive analysis of a written response ("comments") to 
an open-ended question in the survey. After analysis, the interpretation of 
themes was also informed by the technology acceptance model.
RESULTS: Out of 1005 respondents, 611 GPs (61%) provided written comments in 
response to the free text question, totaling 7990 words. Comments were 
classified into 3 major themes and 8 subthemes in relation to GenAI in clinical 
practice. The major themes were (1) unfamiliarity, (2) ambivalence and anxiety, 
and (3) role in clinical tasks. "Unfamiliarity" encompassed a lack of experience 
and knowledge, and the need for training on GenAI. "Ambivalence and anxiety" 
included mixed expectations among GPs in relation to these tools, beliefs about 
diminished human connection, and skepticism about AI accountability. Finally, 
commenting on the role of GenAI in clinical tasks, GPs believed it would help 
with documentation. However, respondents questioned AI's clinical judgment and 
raised concerns about operational uncertainty concerning these tools. Female GPs 
were more likely to leave comments than male GPs, with 53% (324/611) of female 
GPs providing feedback compared to 41.1% (162/394) who did not. Chi-square tests 
confirmed this difference ((χ²₂= 14.6, P=.001). In addition, doctors who left 
comments were significantly more likely to have used GenAI in clinical practice 
compared with those who did not. Among all respondents, 71.7% (438/611) had not 
used GenAI. However, noncommenters were even less likely to have used it, with 
80.7% (318/394) reporting no use. A chi-square test confirmed this difference 
(χ²₁=10.0, P=.002).
CONCLUSIONS: This study provides timely insights into UK GPs' perspectives on 
the role, impact, and limitations of GenAI in primary care. However, the study 
has limitations. The qualitative data analyzed originates from a self-selected 
subset of respondents who chose to provide free-text comments, and these 
participants were more likely to have used GenAI tools in clinical practice. 
However, the substantial number of comments offers valuable insights into the 
diverse views held by GPs regarding GenAI. Furthermore, the majority of our 
respondents reported limited experience and training with these tools; however, 
many GPs perceived potential benefits of GenAI and ambient AI for documentation. 
Notably, 2 years after the widespread introduction of GenAI, GPs' persistent 
lack of understanding and training remains a critical concern. More extensive 
qualitative work would provide a more in-depth understanding of GPs' views.

© Charlotte Blease, Carolina Garcia Sanchez, Cosima Locher, Brian McMillan, Jens 
Gaab, John Torous. Originally published in the Journal of Medical Internet 
Research (https://www.jmir.org).

DOI: 10.2196/74428
PMCID: PMC12327960
PMID: 40769193 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: CB is Associate Editor of 
JMIR Mental Health, and JT is Editor-in-Chief of JMIR Mental Health. The other 
authors declare no conflicts of interest.


266. Sci Rep. 2025 Aug 5;15(1):28541. doi: 10.1038/s41598-025-13586-z.

Father-inclusive chatbot-based prenatal education during COVID-19 pandemic 
enhances maternal-fetal attachment in Korean primigravida women across levels of 
partner support.

Chung K(1)(2)(3), Jhung K(4), Cho HY(5)(6), Park JY(7)(8)(9).

Author information:
(1)Institute of Behavioral Science in Medicine, Yonsei University College of 
Medicine, Yonsei University Health System, Seoul, Republic of Korea.
(2)Department of Psychiatry, Yongin Severance Hospital, Yonsei University 
College of Medicine, Yonsei University Health System, Dongbaekjukjeon-daero 361, 
Giheung-gu, Yongin, 16995, Republic of Korea.
(3)Center for Digital Health, Yongin Severance Hospital, Yonsei University 
College of Medicine, Yonsei University Health System, Yongin, Republic of Korea.
(4)Department of Psychiatry, International St. Mary's Hospital, Catholic 
Kwandong University College of Medicine, Incheon, Republic of Korea.
(5)Department of Obstetrics and Gynecology, Seoul National University Hospital, 
Seoul National University College of Medicine, Daehak-ro 101, Jongno-gu, Seoul, 
03080, Republic of Korea. hycho.md@gmail.com.
(6)Institute of Reproductive Medicine and Population, Medical Research Center, 
Seoul National University, Seoul, Republic of Korea. hycho.md@gmail.com.
(7)Institute of Behavioral Science in Medicine, Yonsei University College of 
Medicine, Yonsei University Health System, Seoul, Republic of Korea. 
empathy@yuhs.ac.
(8)Department of Psychiatry, Yongin Severance Hospital, Yonsei University 
College of Medicine, Yonsei University Health System, Dongbaekjukjeon-daero 361, 
Giheung-gu, Yongin, 16995, Republic of Korea. empathy@yuhs.ac.
(9)Center for Digital Health, Yongin Severance Hospital, Yonsei University 
College of Medicine, Yonsei University Health System, Yongin, Republic of Korea. 
empathy@yuhs.ac.

This study investigated whether a mobile chatbot-delivered intervention for 
prenatal education could enhance maternal-fetal attachment (MFA) among pregnant 
women with different levels of spousal support. In a quasi-experimental study 
conducted in January 2021, sixty volunteers who met all the eligibility criteria 
were enrolled: legally married, pregnant women aged 19 to 49 years, primigravida 
women from 21 to 32 weeks of gestation, and those in need of prenatal education. 
After completing a pre-questionnaire on pregnancy status, perception of prenatal 
education, and partner support, the women were asked to sequentially perform the 
given six tasks via a chatbot for two weeks, and then they filled out a 
post-questionnaire after completing the tasks. Both affective and 
cognitive-behavioral MFA outcomes were repeatedly measured before and after the 
intervention. The chatbot intervention enhanced affective MFA by increasing the 
intensity of prenatal education regardless of spousal support levels (p = .03): 
(1) low (n = 29; pre vs. post: p < .0042) and (2) high (n = 31; pre vs. post: 
p < .0042). In terms of cognitive-behavioral MFA, active paternal involvement in 
the chatbot-assisted baby talk helped women with low spousal support to 
differentiate themselves from the fetus (p = .02; pre vs. post: p < .0042) and 
to attribute characteristics and intention to the fetus (p = .011; pre vs. post: 
p < .0042), eliciting positive affect. This new mobile chatbot successfully 
served as a guide for pregnant couples starting prenatal education for the first 
time.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-13586-z
PMCID: PMC12325668
PMID: 40764632 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


267. PLoS One. 2025 Aug 1;20(8):e0324369. doi: 10.1371/journal.pone.0324369. 
eCollection 2025.

The role of music in ADHD: A multi-dimensional computational and theoretical 
analysis.

Achuthan K(1), Khobragade S(1).

Author information:
(1)Center for Cybersecurity Systems and Networks, Amrita Vishwa Vidyapeetham, 
Amritapuri, Kollam, Kerala, India.

Attention-Deficit Hyperactivity Disorder (ADHD), a neurodevelopmental disorder 
affecting children and adults worldwide, has seen a significant rise in 
diagnoses and medication prescriptions in recent decades. This trend has 
emphasized the need for non-pharmacological interventions such as music to aid 
ADHD management. This study explores the musical experiences of individuals with 
ADHD through a comprehensive analysis of user-generated content from the Reddit 
r/ADHD community between 2014-2024. Advanced computational techniques, including 
large language models such as Gemini 1.5 Pro and LLAMA 3.1 were employed for 
data extraction and categorization. Additionally, APIs from digital streaming 
platforms were utilized to analyze musical characteristics and lyrical content 
of 9,215 tracks across three distinct categories: focus music, stuck songs, and 
general purpose. Insights from selective attention, emotion arousal and mood 
congruence theories were used to interpret the findings. Statistical analysis 
revealed significant variations in musical characteristics, with 
instrumentalness showing the largest effect size across contexts, suggesting 
unique musical preferences among individuals with ADHD. Correlation analyses 
uncovered complex interrelationships between musical attributes, particularly in 
focus music, where energy, speechiness, and instrumental characteristics 
displayed distinctive patterns. The sentiment and popularity analysis of lyrics 
further illuminated the emotional landscape of music in ADHD experiences, 
revealing a strategic approach to musical selection as a potential cognitive and 
emotional self-regulation mechanism.

Copyright: © 2025 Achuthan, Khobragade. This is an open access article 
distributed under the terms of the Creative Commons Attribution License, which 
permits unrestricted use, distribution, and reproduction in any medium, provided 
the original author and source are credited.

DOI: 10.1371/journal.pone.0324369
PMCID: PMC12316199
PMID: 40748879 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that they have no known 
competing financial interests or personal relationships that could have appeared 
to influence the work reported in this article.


268. JMIR Form Res. 2025 Jul 31;9:e71966. doi: 10.2196/71966.

Use of a Medical Communication Framework to Assess the Quality of Generative 
Artificial Intelligence Replies to Primary Care Patient Portal Messages: Content 
Analysis.

Lee NS(1), Richards N(1), Grandominico J(1), Cronin RM(1), Hendricks AK(2), 
Tripathi RS(2), Jonas DE(1).

Author information:
(1)Department of Internal Medicine, College of Medicine, The Ohio State 
University, 2050 Kenny Rd, suite 2400, Columbus, OH, 43221, United States, 1 
614-814-1361.
(2)Health System Informatics, The Ohio State University Wexner Medical Center, 
Columbus, OH, United States.

BACKGROUND: There is growing interest in applying generative artificial 
intelligence (GenAI) to respond to electronic patient portal messages, 
particularly in primary care where message volumes are highest. However, 
evaluations of GenAI as an inbox communication tool are limited. Qualitative 
analysis of when and how often GenAI responses achieve communication goals can 
inform estimates of impact and guide continuous improvement.
OBJECTIVE: This study aims to evaluate GenAI responses to primary care messages 
using a medical communication framework.
METHODS: This was a descriptive quality improvement study of 201 GenAI replies 
to a purposively sampled, diverse pool of real primary care patient messages in 
a large midwestern academic medical center. Two physician reviewers (NSL and NR) 
used a hybrid deductive-inductive approach to qualitatively identify and define 
themes, guided by constructs from the "best practice" medical communication 
framework. After achieving thematic saturation, the reviewers assessed the 
presence or absence of identified communication themes, both independently and 
collaboratively. Discrepant observations were reconciled via discussion. 
Frequencies of identified themes were tallied.
RESULTS: Themes in strengths and limitations emerged across 5 communication 
domains. In the domain of rapport building, expressing respect and restating key 
phrases were strengths, while inappropriate or inadequate rapport building 
statements were limitations. For information gathering, questions that built 
toward a plan or elicited patient needs were strengths, while questions that 
were out of place or redundant were limitations. For information delivery, 
accurate content delivered clearly and professionally was a strength, but 
delivery of inaccurate content was an observed limitation. GenAI responses could 
facilitate next steps by outlining choices or providing instruction, but 
sometimes those next steps were inappropriate or premature. Finally, in 
responding to emotion, strengths were that emotions were named and validated, 
while inadequate or absent acknowledgment of emotion was a limitation. Overall, 
26.4% (53/201) of all messages displayed communication strengths without 
limitations, 27.4% (55/201) had limitations without strengths, and the remaining 
46.3% (93/201) had both. Strengths outnumbered limitations in rapport building 
(87/201, 43.3% vs 35/201, 17.4%) and facilitating next steps (73/201, 36.3% vs 
39/201, 19.4%). Limitations outnumbered strengths in the remaining domains of 
information delivery (89/201, 44.3% vs 43/201, 21.4%), information gathering 
(60/201, 29.9% vs 43/201, 21.4%), and responding to emotion (7/201, 8.5% vs 
9/201, 4.5%).
CONCLUSIONS: GenAI response quality on behalf of primary care physicians and 
advanced practice providers may vary by communication function. Expressions of 
respect or descriptions of common next steps may be appropriate, but gathering 
and delivering appropriate information, or responding to emotion, may be 
limited. While communication standards were often met, they were also often 
compromised. Understanding these strengths and limitations can inform decisions 
about whether, when, and how to apply GenAI as a tool for primary care inbox 
communication.

© Natalie S Lee, Nathan Richards, Jodi Grandominico, Robert M Cronin, Amanda K 
Hendricks, Ravi S Tripathi, Daniel E Jonas. Originally published in JMIR 
Formative Research (https://formative.jmir.org).

DOI: 10.2196/71966
PMCID: PMC12313158
PMID: 40743559 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


269. J Med Internet Res. 2025 Jul 31;27:e72984. doi: 10.2196/72984.

Large Language Model Symptom Identification From Clinical Text: Multicenter 
Study.

McMurry AJ(1)(2), Phelan D(1), Dixon BE(3)(4), Geva A(1)(5), Gottlieb D(1)(6), 
Jones JR(1), Terry M(1), Taylor DE(4), Callaway H(4), Manoharan S(4), Miller 
T(1)(2), Olson KL(1)(2), Mandl KD(1)(2).

Author information:
(1)Computational Health Informatics Program, Boston Children's Hospital, 401 
Park Drive, LM5506, Mail Stop BCH3187, Boston, MA, 02215, United States, 1 
617-355-4145.
(2)Department of Pediatrics, Harvard Medical School, Boston, MA, United States.
(3)Department of Health Policy and Management, Fairbanks School of Public 
Health, Indiana University, Indianapolis, IN, United States.
(4)Center for Biomedical Informatics, Regenstrief Institute, Indianapolis, IN, 
United States.
(5)Department of Anesthesia, Harvard Medical School, Boston, MA, United States.
(6)Department of Biomedical Informatics, Harvard Medical School, Boston, MA, 
United States.

BACKGROUND: Recognizing patient symptoms is fundamental to medicine, research, 
and public health. However, symptoms are often underreported in coded formats 
even though they are routinely documented in physician notes. Large language 
models (LLMs), noted for their generalizability, could help bridge this gap by 
mimicking the role of human expert chart reviewers for symptom identification.
OBJECTIVE: The primary objective of this multisite study was to measure the 
accurate identification of infectious respiratory disease symptoms using LLMs 
instructed to follow chart review guidelines. The secondary objective was to 
evaluate LLM generalizability in multisite settings without the need for 
site-specific training, fine-tuning, or customization.
METHODS: Four LLMs were evaluated: GPT-4, GPT-3.5, Llama2 70B, and Mixtral 8×7B. 
LLM prompts were instructed to take on the role of chart reviewers and follow 
symptom annotation guidelines when assessing physician notes. Ground truth 
labels for each note were annotated by subject matter experts. Optimal LLM 
prompting strategies were selected using a development corpus of 103 notes from 
the emergency department at Boston Children's Hospital. The performance of each 
LLM was measured using a test corpus with 202 notes from Boston Children's 
Hospital. The performance of an International Classification of Diseases, Tenth 
Revision (ICD-10)-based method was also measured as a baseline. Generalizability 
of the most performant LLM was then measured in a validation corpus of 308 notes 
from 21 emergency departments in the Indiana Health Information Exchange.
RESULTS: Symptom identification accuracy was superior for every LLM tested for 
each infectious disease symptom compared to an ICD-10-based method 
(F1-score=45.1%). GPT-4 was the highest scoring (F1-score=91.4%; P<.001) and was 
significantly better than the ICD-10-based method, followed by GPT-3.5 
(F1-score=90.0%; P<.001), Llama2 (F1-score=81.7%; P<.001), and Mixtral 
(F1-score=83.5%; P<.001). For the validation corpus, performance of the 
ICD-10-based method decreased (F1-score=26.9%), while GPT-4 increased 
(F1-score=94.0%), demonstrating better generalizability using GPT-4 (P<.001).
CONCLUSIONS: LLMs significantly outperformed an ICD-10-based method for 
respiratory symptom identification in emergency department electronic health 
records. GPT-4 demonstrated the highest accuracy and generalizability, 
suggesting that LLMs may augment or replace traditional approaches. LLMs can be 
instructed to mimic human chart reviewers with high accuracy. Future work should 
assess broader symptom types and health care settings.

© Andrew J McMurry, Dylan Phelan, Brian E Dixon, Alon Geva, Daniel Gottlieb, 
James R Jones, Michael Terry, David E Taylor, Hannah Callaway, Sneha Manoharan, 
Timothy Miller, Karen L Olson, Kenneth D Mandl. Originally published in the 
Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/72984
PMCID: PMC12313083
PMID: 40743494 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


270. AIDS Care. 2025 Oct;37(10):1718-1731. doi: 10.1080/09540121.2025.2518731. Epub 
2025 Jul 31.

"Un amigo secreto": perceived acceptability of a chatbot for depression 
education, self-help and linkage to care among adolescents living with HIV in 
Peru.

Rupani N(1), Vasquez DH(2), Contreras C(2)(3), Menacho L(4), Tapia M(2), Kolevic 
L(5)(6), Franke MF(7), Galea JT(3).

Author information:
(1)Morsani College of Medicine, University of South Florida, Tampa, USA.
(2)Socios En Salud Sucursal Peru, Lima, Peru.
(3)School of Social Work, College of Behavioral and Community Sciences, 
University of South Florida, Tampa, USA.
(4)Instituto de Medicina Tropical Alexander Von Humboldt, Universidad Peruana 
Cayetano Heredia, Lima, Peru.
(5)Instituto Nacional de Salud del Niño, Lima, Peru.
(6)Universidad Nacional Mayor de San Marcos, Lima, Peru.
(7)Department of Global Health and Social Medicine, Harvard Medical School, 
Boston, USA.

Adolescents living with HIV (ALWH) in Peru face significant mental health 
challenges, including a high prevalence of depression, exacerbated by limited 
access to mental health resources. This study explores the perceived 
acceptability of chatbots to support depression education, self-care, and 
referral among ALWH. Focus groups and interviews were conducted with 28 
adolescents, 5 caregivers, and 5 HIV care personnel. A semi-structured interview 
guide assessed seven core chatbot acceptability constructs: affective attitude, 
ethics, burden, intervention coherence, opportunity costs, perceived 
effectiveness, and self-efficacy. Most participants expressed positive attitudes 
towards a potential mental health chatbot, highlighting the possibilities it 
could offer as a non-threatening and accessible source of information and 
support, offering immediate responses and facilitating a safe space for ALWH to 
communicate their needs in moments of isolation. Participants understood a 
potential chatbot's three main purposes and had confidence in their abilities to 
use one successfully. However, concerns were raised regarding a potential 
chatbot's inability to provide human empathy and potential privacy issues 
related to data security. Despite some reservations, participants generally 
believed that chatbots could effectively deliver educational content, promote 
self-help strategies, and serve as a bridge to professional care.

DOI: 10.1080/09540121.2025.2518731
PMID: 40743294 [Indexed for MEDLINE]


271. Front Digit Health. 2025 Jul 16;7:1658635. doi: 10.3389/fdgth.2025.1658635. 
eCollection 2025.

Correction: Diagnostic efficacy of large language models in the pediatric 
emergency department: a pilot study.

Del Monte F(#)(1), Barolo R(#)(2), Circhetta M(3), Delmonaco AG(1), Castagno 
E(1), Pivetta E(4), Bergamasco L(3)(5), Franco M(6), Olmo G(3), Bondone C(1).

Author information:
(1)Department of Pediatric Emergency, Regina Margherita Children's 
Hospital-A.O.U. Città Della Salute e Della Scienza di Torino, Turin, Italy.
(2)Department of Public Health and Pediatrics, Postgraduate School of 
Pediatrics, University of Turin, Turin, Italy.
(3)Department of Control and Computer Engineering, Politecnico di Torino, Turin, 
Italy.
(4)Division of Emergency Medicine and High Dependency Unit, Department of 
Medical Sciences, Città Della Salute e Della Scienza di Torino and University of 
Turin, Turin, Italy.
(5)LINKS Foundation, Turin, Italy.
(6)Department of Clinical and Biological Sciences, University of Turin, Turin, 
Italy.
(#)Contributed equally

Erratum for
    Front Digit Health. 2025 Jul 01;7:1624786. doi: 10.3389/fdgth.2025.1624786.

[This corrects the article DOI: 10.3389/fdgth.2025.1624786.].

© 2025 Del Monte, Barolo, Circhetta, Delmonaco, Castagno, Pivetta, Bergamasco, 
Franco, Olmo and Bondone.

DOI: 10.3389/fdgth.2025.1658635
PMCID: PMC12308998
PMID: 40741323


272. ArXiv [Preprint]. 2025 Jul 22:arXiv:2507.17009v1.

Multi-Label Classification with Generative AI Models in Healthcare: A Case Study 
of Suicidality and Risk Factors.

Huang M(1), Li Z(1), Hu Y(1), Wang W(1), Wen A(1), Lane S(2), Selek S(2), 
Shahani L(2), Machado-Vieira R(2), Soares J(2), Xu H(3), Liu H(1)(2).

Author information:
(1)McWilliams School of Biomedical Informatics, The University of Texas Health 
Science Center at Houston, TX, USA.
(2)Faillace Department of Psychiatry & Behavioral Sciences, McGovern Medical 
School, The University of Texas Health Science at Houston, Houston, TX, USA.
(3)Department of Biomedical Informatics and Data Science, School of Medicine, 
Yale University, New Haven, CT, USA.

Suicide remains a pressing global health crisis, with over 720,000 deaths 
annually and millions more affected by suicide ideation (SI) and suicide 
attempts (SA). Early identification of suicidality-related factors (SrFs), 
including SI, SA, exposure to suicide (ES), and non-suicidal self-injury (NSSI), 
is critical for timely intervention. While prior studies have applied AI to 
detect SrFs in clinical notes, most treat suicidality as a binary classification 
task, overlooking the complexity of co-occurring risk factors. This study 
explores the use of generative large language models (LLMs), specifically 
GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs from 
psychiatric electronic health records (EHRs). We present a novel end-to-end 
generative MLC pipeline and introduce advanced evaluation methods, including 
label-set-level metrics and a multi-label confusion matrix for error analysis. 
Fine-tuned GPT-3.5 achieved top performance with 0.94 partial-match accuracy and 
0.91 F1 score, while GPT-4.5 with guided prompting showed superior performance 
across label sets, including rare or minority label sets, indicating a more 
balanced and robust performance. Our findings reveal systematic error patterns, 
such as the conflation of SI and SA, and highlight the models' tendency toward 
cautious over-labeling. This work not only demonstrates the feasibility of using 
generative AI for complex clinical classification tasks but also provides a 
blueprint for structuring unstructured EHR data to support large-scale clinical 
research and evidence-based medicine.

PMCID: PMC12310136
PMID: 40740509


273. Front Public Health. 2025 Jul 16;13:1590689. doi: 10.3389/fpubh.2025.1590689. 
eCollection 2025.

Explainable machine learning prediction of internet addiction among Chinese 
primary and middle school children and adolescents: a longitudinal study based 
on positive youth development data (2019-2022).

Liu J(#)(1), Chen L(#)(2), Chen Y(#)(3), Luo J(3), Yu K(3), Fan L(4), Yong C(4), 
He H(4), Liao S(5), Ge Z(1), Jiang L(4)(6).

Author information:
(1)AIM for Health Lab, Monash University, Melbourne, VIC, Australia.
(2)School of Mathematics and Statistics, University of Melbourne, Melbourne, 
VIC, Australia.
(3)Jockey Club School of Public Health and Primary Care, The Chinese University 
of Hong Kong, Hong Kong, Hong Kong SAR, China.
(4)West China School of Public Health and West China Fourth Hospital, Sichuan 
University, Chengdu, China.
(5)School of Nursing, Capital Medical University, Peking, China.
(6)General Practice Ward, International Medical Center Ward, General Practice 
Medical Center, West China Hospital, Sichuan University, Chengdu, China.
(#)Contributed equally

BACKGROUND: Internet Addiction (IA) has emerged as a critical concern, 
especially among school age children and adolescents, potentially stalling their 
physical and mental development. Our study aimed to examine the risk factors 
associated with IA among Chinese children and adolescents and leverage 
explainable machine learning (ML) algorithms to predict IA status at the time of 
assessment, based on Young's Internet Addiction Test.
METHODS: The longitudinal data consisting of 8,824 schoolchildren from the 
Chengdu Positive Child Development (CPCD) survey were analyzed, where 33.3% of 
participants were identified with IA (Age: 10.97 ± 2.31, Male: 51.73%). IA was 
defined using Young's Internet Addiction Test (IAT ≥ 40). Demographic variables 
such as age, gender, and grade level, along with key variables including scores 
of Cognitive Behavioral Competencies (CBC), Prosocial Attributes (PA), Positive 
Identity (PI), General Positive Youth Development Qualities (GPYDQ), Life 
Satisfaction (LS), Delinquent Behavior (DB), Non-Suicidal Self-Injury (NSSI), 
Depression (DP), Anxiety (AX), Family Function Disorders (FF), Egocentrism (EG), 
Empathy (EP), Academic Intrinsic Value (IV), and Academic Utility Value (UV) 
were examined. Chi-square and Mann-Whitney U tests were employed to validate the 
significance of the mentioned predictors of IA. We applied six ML models: Extra 
Random Forest, XGBoost, Logistic Regression, Bernoulli Naïve Bayes, Multi-Layer 
Perceptron (MLP), and Transformer Encoder. Performance was evaluated via 10-fold 
cross-validation and held-out test sets across survey waves. Feature selection 
and SHapley Additive exPlanations (SHAP) analysis were utilised for model 
improvement and interpretability, respectively.
RESULTS: ExtraRFC achieved the best performance (Test AUC = 0.854, 
Accuracy = 0.798, F1 = 0.659), outperforming all other models across most 
metrics and external validations. Key predictors included grade level, 
delinquent behavior, anxiety, family function, and depression scores. SHAP 
analysis revealed consistent and interpretable feature contributions across 
individuals.
CONCLUSION: Depression, anxiety, and family dynamics are significant factors 
influencing IA in children. The Extra Random Forest model proves most effective 
in predicting IA, emphasising the importance of addressing these factors to 
promote healthy digital habits in children. This study presents an effective 
SHAP-based explainable ML framework for IA prediction in children and 
adolescents.

Copyright © 2025 Liu, Chen, Chen, Luo, Yu, Fan, Yong, He, Liao, Ge and Jiang.

DOI: 10.3389/fpubh.2025.1590689
PMCID: PMC12307306
PMID: 40740384 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


274. Singapore Med J. 2025 Jul 29. doi: 10.4103/singaporemedj.SMJ-2024-173. Online 
ahead of print.

Advances, reception and potential of ChatGPT as a tool for healthcare delivery 
and research: a systematic review.

Singh N(1), Neubronner S(1), Kanayan S(2), Illanes S(2), Choolani M(2), Kemp 
MW(2)(3)(4)(5).

Author information:
(1)Department of Obstetrics and Gynaecology, National University Hospital, 
Singapore.
(2)Department of Obstetrics and Gynaecology, NUS Yong Loo Lin School of 
Medicine, National University of Singapore, Singapore.
(3)Centre for Perinatal and Neonatal Medicine, Tohoku University Hospital, 
Sendai, Japan.
(4)Division of Obstetrics and Gynaecology, University of Western Australia, 
Perth, Australia.
(5)Women and Infants Research Foundation, King Edward Memorial Hospital, 
Subiaco, Western Australia, Australia.

ChatGPT gained widespread attention for its capabilities in natural language 
processing, enabling machines to assess human language inputs and generate 
complex, yet evolving answers. As large language models (LLMs) continue to 
develop, clear guidelines are needed to help healthcare providers and educators 
maximise their benefits while mitigating potential risks. This review assessed 
the utility and accuracy of applying ChatGPT in healthcare assistance, 
specifically in understanding clinical knowledge and guiding clinical practice 
and research. A search on PubMed/MEDLINE for ChatGPT-related articles from 30 
November 2022 (ChatGPT's release date) to 14 March 2024 yielded 2690 articles. 
After screening and reviewing, 2141 articles were deemed relevant to the 
clinical and research domains. Of the articles, 60.3% were supportive of 
ChatGPT, highlighting its immense potential for automating routine tasks, 
enhancing decision-making processes and addressing complex challenges in health 
care. However, 0.9% were not supportive of ChatGPT's utilisation in its current 
form, given the unresolved ethical implications and concerns regarding accuracy, 
bias, privacy and legal. Additionally, 38.8% had an equivocal stance, suggesting 
for further research to fully understand the rapidly evolving capabilities and 
potential impacts of ChatGPT in healthcare. This review presents a newly created 
conceptual framework, the 'ABCD model', to facilitate a systematic approach for 
researchers and healthcare practitioners to navigate ChatGPT's strengths and 
limitations. The model aims to align the development and deployment of ChatGPT 
by providing guiding principles, which ChatGPT and other emerging LLMs should 
incorporate into further developments to ensure their suitable application in 
health care.

Copyright © 2025 Singapore Medical Journal.

DOI: 10.4103/singaporemedj.SMJ-2024-173
PMID: 40734218


275. Int J Gynaecol Obstet. 2026 Feb;172(2):1104-1113. doi: 10.1002/ijgo.70407. Epub 
2025 Jul 29.

Leveraging artificial intelligence for evidence-based recommendations in uterine 
fibroid therapy: Addressing the unmet need in German healthcare-A clinical 
trial.

Psilopatis I(1), Lotz L(2), Sipulina N(2), Heindl F(2), Levidou G(3), Emons 
J(2).

Author information:
(1)Department of Gynecology and Obstetrics, Universitätsspital Basel, Basel, 
Switzerland.
(2)Department of Gynecology and Obstetrics, Universitätsklinikum Erlangen, 
Comprehensive Cancer Center Erlangen-EMN (CCC ER-EMN), 
Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Erlangen, Germany.
(3)Department of Pathology, Nuremberg Clinic, Paracelsus Medical University, 
Nuremberg, Germany.

OBJECTIVE: To evaluate the potential of an artificial intelligence (AI)-driven 
large language model, ChatGPT 4.0, to provide personalized, evidence-based 
treatment recommendations for uterine fibroids.
METHODS: ChatGPT 4.0 was trained using evidence-based data from Uptodate and 
German medical literature. The algorithm generated individualized 
recommendations based on clinical characteristics and patient preferences. 
Usability and quality were assessed through questionnaires completed by 40 
gynecologists and 45 women with fibroids.
RESULTS: Most gynecologists found the algorithm user-friendly and comprehensive, 
with 15 expressing a willingness to integrate it into practice and 24 
acknowledging its potential to enhance healthcare efficiency. Although only half 
believed it would improve patient outcomes, the tool was generally well 
received. Patients found the algorithm easy to understand and helpful for 
exploring treatment options, with the majority feeling it empowered informed 
discussions with their healthcare providers. A minority expressed 
dissatisfaction with usability or helpfulness.
CONCLUSION: ChatGPT 4.0 offers a promising AI-driven tool for personalized 
fibroid management in the absence of formal guidelines. Although not a 
substitute for official recommendations, it could support clinical decision 
making and enhance patient education. Further integration with standardized 
guidelines and prospective trials is needed to optimize its clinical utility.

© 2025 The Author(s). International Journal of Gynecology & Obstetrics published 
by John Wiley & Sons Ltd on behalf of International Federation of Gynecology and 
Obstetrics.

DOI: 10.1002/ijgo.70407
PMCID: PMC12790660
PMID: 40728151 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no conflicts of interest.


276. Appl Neuropsychol Child. 2025 Jul 28:1-11. doi: 10.1080/21622965.2025.2535017. 
Online ahead of print.

Wavelet-Attention deep model for pediatric ADHD diagnosis via EEG.

Masoudi B(1).

Author information:
(1)Department of Information Technology, Payamenoor University (PNU), Tehran, 
Islamic Republic of Iran.

Attention-deficit/hyperactivity disorder (ADHD) is a prevalent 
neurodevelopmental disorder in children, impacting academic performance, social 
interactions, and overall well-being. Early and accurate diagnosis is crucial, 
yet current methods rely heavily on subjective assessments. This study presents 
a novel Wavelet-Attention deep model for objective ADHD diagnosis using 
electroencephalography signals. The model integrates a wavelet transform for 
feature extraction with a deep residual network (ResNet) augmented by an 
attention mechanism to enhance focus on salient features. Rigorous 
preprocessing, including Independent Component Analysis for artifact removal, 
was applied to a publicly available dataset of 121 children. To ensure a robust 
and clinically relevant evaluation that avoids data leakage, a strict 
Leave-One-Subject-Out cross-validation protocol was employed. The proposed model 
demonstrated strong diagnostic performance, achieving an accuracy of 96.69%, a 
sensitivity of 95.08%, and a specificity of 98.33% in distinguishing between 
children with ADHD and healthy controls. Furthermore, model-agnostic 
interpretability analysis revealed that features derived from frontal lobe 
channels and low-frequency wavelet coefficients were most critical for the 
model's decisions, aligning with established neurophysiological markers of ADHD. 
The results suggest that this approach holds significant potential for 
developing a reliable and objective diagnostic tool for ADHD, facilitating 
earlier and more personalized interventions.

DOI: 10.1080/21622965.2025.2535017
PMID: 40720204


277. Diabet Med. 2025 Nov;42(11):e70115. doi: 10.1111/dme.70115. Epub 2025 Jul 27.

The usability and feasibility of a self-compassion chatbot (COMPASS) for youth 
living with type 1 diabetes.

Boggiss AL(1)(2), Babbott K(1)(3), Milford Ā(1), Ellett S(1), Consedine N(1), 
Reid S(4), Cao N(5), Cavadino A(6), Hopkins S(7), Jefferies C(8)(9), de Bock 
M(10), Serlachius A(1).

Author information:
(1)Department of Psychological Medicine, Faculty of Medical and Health Sciences, 
The University of Auckland, Auckland, Aotearoa New Zealand.
(2)Rio Tinto Children's Diabetes Centre, The Kids Research Institute Australia, 
Perth Children's Hospital, Perth, Western Australia, Australia.
(3)Acute Pain Service, Te Whatu Ora Waitematā, Auckland, Aotearoa New Zealand.
(4)Health Literacy New Zealand, Auckland, Aotearoa New Zealand.
(5)Le Va, Auckland, Aotearoa New Zealand.
(6)Epidemiology and Biostatistics, University of Auckland, Auckland, Aotearoa 
New Zealand.
(7)Centre for eResearch, University of Auckland, Auckland, Aotearoa New Zealand.
(8)Starship Child Health, Te Whatu Ora Te Toka Tumai Auckland, Auckland, 
Aotearoa New Zealand.
(9)Liggins Institute, University of Auckland, Auckland, Aotearoa New Zealand.
(10)Department of Paediatrics, University of Otago, Christchurch, Aotearoa New 
Zealand.

AIM: Although it is well established that youth with type 1 Diabetes (T1D) 
experience high rates of distress, current clinical care is often 
under-resourced and unable to provide sufficient or timely psychological 
support. The current study was designed to evaluate the safety, usability and 
feasibility of 'COMPASS,' a self-compassion chatbot intervention.
METHODS: Forty adolescents (aged 12-16 years) living with T1D participated in a 
12-week, single-group feasibility study using a mixed methods approach. 
Usability was assessed via qualitative feedback and interviews, feasibility 
through app analytics and recruitment data, and safety by utilisation of 
distress risk management protocols. Psychosocial measures (including diabetes 
distress, strengths and resilience, self-efficacy, self-compassion, self-care 
behaviours and emotional well-being) were collected at baseline, 6- and 12-week 
follow-ups, alongside HbA1c at baseline and 12 weeks.
RESULTS: Of 40 participants recruited, 32 (80%) completed the study. On average, 
participants completed 95% of modules started. Follow-up assessments showed 
promising reductions in diabetes distress, and improvements in resilience, 
self-efficacy, self-compassion and emotional well-being. Qualitative analysis 
highlighted strengths in engagement and relevance of information covered. Areas 
for improvement included increasing decision-making support to help alleviate 
the felt burden of diabetes.
CONCLUSIONS: The COMPASS chatbot was demonstrated to be safe, feasible and 
acceptable to adolescents living with T1D. Future research aims to conduct a 
hybrid type 1 effectiveness-implementation trial that will leverage updates in 
artificial intelligence to increase the flexibility and tailoring of responding.

© 2025 The Author(s). Diabetic Medicine published by John Wiley & Sons Ltd on 
behalf of Diabetes UK.

DOI: 10.1111/dme.70115
PMCID: PMC12535353
PMID: 40717280 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


278. Chin Med J (Engl). 2026 Jan 5;139(1):108-117. doi: 10.1097/CM9.0000000000003575. 
Epub 2025 Jul 25.

Development and validation of deep learning for predicting the growth of ovarian 
cancer organoids.

Wu H(1), Ma L(2)(3)(4), Wang L(2)(3)(4), Zhu X(2)(3)(4), Luo X(1), Zhang C(5), 
Ha C(6), Dang Y(7)(8), Wang H(2)(3)(4), Zou D(2)(3)(4).

Author information:
(1)Bioengineering College, Chongqing University, Chongqing 400044, China.
(2)Department of Gynecologic Oncology, Chongqing University Cancer Hospital & 
Chongqing Cancer Institute & Chongqing Cancer Hospital, Chongqing 400030, China.
(3)Chongqing Specialized Medical Research Center of Ovarian Cancer, Chongqing 
400030, China.
(4)Organoid Transformational Research Center, Chongqing Key Laboratory of 
Translational Research for Cancer Metastasis and Individualized Treatment, 
Chongqing University Cancer Hospital, Chongqing 400030, China.
(5)School of Medicine, Chongqing University, Chongqing 400044, China.
(6)Gynecology Department, General Hospital of Ningxia Medical University, 
Yinchuan, Ningxia 750003, China.
(7)Gansu Provincial Maternity and Child Care Hospital, Lanzhou, Gansu 730050, 
China.
(8)Institute of Pathology, School of Basic Medical Sciences, Lanzhou University, 
Lanzhou, Gansu 730000, China.

BACKGROUND: Organoids have attracted enormous interest in disease modeling, drug 
screening, and precision medicine. However, developing robust patient-derived 
organoids (PDOs) was time-consuming, costly, and had low success rates for 
certain cancer types, which limited their clinical utility. This study aimed to 
develop an interpretable deep learning-based model to predict the cultivation 
outcome of ovarian cancer organoids in advance.
METHODS: Longitudinal microscopy images of 517 ovarian cancer organoid droplets 
were divided into training ( n = 325), validation ( n = 88), and test ( n = 104) 
sets. Subsequently, growth prediction models were developed based on four neural 
network backbones (ResNet18, VGG11, ConvNeXt v2, and Swin Transformer v2), and 
specific optimization methods were designed for better prediction. Finally, 179 
samples from multiple centers were collected for prospective validation, and the 
gradient-weighted class activation mapping (Grad-CAM) method was used for 
interpretability analysis of the deep model to reveal the basis of the model's 
decisions.
RESULTS: The test set showed that the deep learning models could achieve 
high-performance prediction at the third stage with area under the curve (AUC) 
values greater than 0.8 for all four models. The homogeneous transfer learning 
optimization method improved the AUC from 0.833 to 0.884 ( P = 0.0039). In 
prospective validation, the optimized model achieved an AUC of 0.832, a Brier 
score of 0.1919 in the calibration curve, and a greater net benefit in the 
decision curve. Interpretability analysis revealed that the area where organoids 
are being formed and have already formed is important for prediction.
CONCLUSIONS: Our developed models achieved satisfactory results in predicting 
the growth of ovarian cancer organoids. There is potential for further 
development of the model toward process automation.

Copyright © 2025 The Chinese Medical Association, produced by Wolters Kluwer, 
Inc. under the CC-BY-NC-ND license.

DOI: 10.1097/CM9.0000000000003575
PMCID: PMC12768030
PMID: 40709801 [Indexed for MEDLINE]

Conflict of interest statement: None.


279. Int Nurs Rev. 2025 Sep;72(3):e70076. doi: 10.1111/inr.70076.

Enhancing Work Efficiency With Generative Artificial Intelligence: Experience 
and Training Insights From School Nurses Through Focus Groups and Surveys.

Chang LC(1)(2)(3), Huang CH(2)(3), Yu HY(2)(3)(4), Hung LL(5), Lin 
HL(1)(2)(3)(6), Liao LL(7)(8), Lai IJ(9).

Author information:
(1)School of Nursing, Chang Gung University of Science and Technology, Taoyuan, 
Taiwan, R.O.C.
(2)Department of Nursing, Linkou Branch, Chang Gung Memorial Hospital, Taoyuan, 
Taiwan, R.O.C.
(3)School of Nursing, College of Medicine, Chang Gung University, Tao-Yuan 
County, Taiwan, R.O.C.
(4)Department of Nursing, New Taipei Municipal Tu-Cheng Hospital, New Taipei, 
Taiwan, R.O.C.
(5)Department of Optometry, Hsin Sheng College of Medical Care and Management, 
Taoyuan City, Taiwan, R.O.C.
(6)Taipei Medical University, Taipei, Taiwan, R.O.C.
(7)Department of Public Health, College of Health Science, Kaohsiung Medical 
University, Kaohsiung City, Taiwan, R.O.C.
(8)Department of Medical Research, Kaohsiung Medical University Hospital, 
Kaohsiung City, Taiwan, R.O.C.
(9)Department of Nutrition, I-Shou University, Kaohsiung City, Taiwan, R.O.C.

AIM: This study examined the use of generative artificial intelligence (GAI), 
factors related to its application, and training expectations among school 
nurses who enrolled as seed instructors in a GAI training program.
BACKGROUND: GAI, such as ChatGPT, is reshaping professional environments by 
simulating human-like interactions. Enhancing user proficiency and reducing the 
risk of misuse is essential for practical applications.
METHODS: This study employed an explanatory sequential mixed-methods design 
using a purposive sample of 45 school nurses from primary, secondary, and higher 
education institutions in Taiwan. An online survey assessed attitudes and 
behaviors related to GAI, followed by six 90-minute focus groups that explored 
practical use and training needs.
FINDINGS: The survey confirmed the universal adoption of ChatGPT among 
participants, primarily for information retrieval. Perceived usefulness emerged 
as a key factor affecting workplace integration. Focus group discussions, guided 
by quantitative insights, highlighted ChatGPT's role in enhancing work 
efficiency through personalized educational materials, content creation, and 
summarization. Concerns included accuracy, ethical risks, and privacy. 
Participants expected improvements in workflow automation, statistical 
capabilities, and prompting to optimize AI integration in nursing tasks. 
However, misaligned expectations regarding automation and statistical 
capabilities were evident. These findings underscore the need for effective 
prompting, reliable information retrieval, and targeted AI training to support 
workplace adoption and critical evaluation.
CONCLUSION: ChatGPT was mainly used as a GAI tool for health education and 
content creation. It was valued for its personalized support but raised concerns 
about accuracy. Perceived usefulness significantly influenced its use, and 
participants sought training to improve their prompting skills.
IMPLICATIONS FOR NURSING PRACTICE AND HEALTH POLICY: Future AI training should 
integrate ethical principles alongside practical applications to minimize 
misuse, particularly in nursing. Specialized GAI training for nurses should 
emphasize hands-on application exercises to enhance task-specific efficiency and 
meet the evolving demands of healthcare.

© 2025 International Council of Nurses.

DOI: 10.1111/inr.70076
PMID: 40709744 [Indexed for MEDLINE]


280. J Biophotonics. 2025 Nov;18(11):e202500181. doi: 10.1002/jbio.202500181. Epub 
2025 Jul 25.

Deep Learning for Autonomous Surgical Guidance Using 3-Dimensional Images From 
Forward-Viewing Endoscopic Optical Coherence Tomography.

Ly S(1), Badré A(1), Brandt P(1), Wang C(2), Calle P(1), Reynolds J(1), Zhang 
Q(2), Fung KM(3)(4), Cui H(1), Yu Z(3)(5), Patel SG(6), Liu Y(1), Bradley NA(6), 
Tang Q(2)(4), Pan C(1)(2).

Author information:
(1)School of Computer Science, University of Oklahoma, Norman, Oklahoma, USA.
(2)Stephenson School of Biomedical Engineering, University of Oklahoma, Norman, 
Oklahoma, USA.
(3)Department of Pathology, University of Oklahoma Health Sciences Center, 
Oklahoma City, Oklahoma, USA.
(4)Stephenson Cancer Center, University of Oklahoma Health Sciences Center, 
Oklahoma City, Oklahoma, USA.
(5)Children's Hospital, University of Oklahoma Health Sciences Center, Oklahoma 
City, Oklahoma, USA.
(6)Department of Urology, University of Oklahoma Health Sciences Center, 
Oklahoma City, Oklahoma, USA.

A three-dimensional convolutional neural network (3D-CNN) was developed for the 
analysis of volumetric optical coherence tomography (OCT) images to enhance 
endoscopic guidance during percutaneous nephrostomy. The model was 
performance-benchmarked using a 10-fold nested cross-validation procedure and 
achieved an average test accuracy of 90.57% across a dataset of 10 porcine 
kidneys. This performance significantly exceeded that of 2D-CNN models that 
attained average test accuracies ranging from 85.63% to 88.22% using 1, 10, or 
100 radial sections extracted from the 3D OCT volumes. The 3D-CNN (~12 million 
parameters) was benchmarked against three state-of-the-art volumetric 
architectures: the 3D Vision Transformer (3D-ViT, ~45 million parameters), 
3D-DenseNet121 (~12 million parameters), and the Multi-plane and Multi-slice 
Transformer (M3T, ~29 million parameters). While these models achieved 
comparable inferencing accuracy, the 3D-CNN exhibited lower inference latency 
(33 ms) than 3D-ViT (86 ms), 3D-DenseNet121 (58 ms), and M3T (93 ms), 
representing a critical advantage for real-time surgical guidance applications. 
These results demonstrate the 3D-CNN's capability as a powerful and practical 
tool for computer-aided diagnosis in OCT-guided surgical interventions.

© 2025 Wiley‐VCH GmbH.

DOI: 10.1002/jbio.202500181
PMCID: PMC12718128
PMID: 40709742 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest The authors declare no 
conflicts of interest.


281. NPJ Precis Oncol. 2025 Jul 24;9(1):257. doi: 10.1038/s41698-025-01014-4.

Role of large language models in the multidisciplinary decision-making process 
for patients with renal cell carcinoma: a pilot experience.

Bertolo R(1), De Bon L(2), Caudana F(2), Pettenuzzo G(2), Malandra S(2), 
Casolani C(2), Zivi A(3), Fantinel E(3), Borsato A(4), Negrelli R(4), Salah El 
Din Tantawy E(5), Volpi G(5), Brunelli M(6), Veccia A(2), Cerruto MA(2), 
Antonelli A(2); AOUI Verona Uro-Oncology Multi-Disciplinary Team.

Author information:
(1)Department of Surgery, Dentistry, Pediatrics and Gynecology, Urology Unit, 
Azienda Ospedaliera Universitaria Integrata, University of Verona, Verona, 
Italy. riccardogiuseppe.bertolo@univr.it.
(2)Department of Surgery, Dentistry, Pediatrics and Gynecology, Urology Unit, 
Azienda Ospedaliera Universitaria Integrata, University of Verona, Verona, 
Italy.
(3)Department of Medicine, Section of Oncology, Azienda Ospedaliera 
Universitaria Integrata, Verona, Italy.
(4)Unit of Radiology, Azienda Ospedaliera Universitaria Integrata, Verona, 
Italy.
(5)Unit of Radiation Oncology, Azienda Ospedaliera Universitaria Integrata 
Verona, Verona, Italy.
(6)Department of Diagnostic and Public Health, Section of Pathology, University 
of Verona, Verona, Italy.

We evaluated an AI chatbot's ability to suggest diagnostic and therapeutic 
pathways for renal cell carcinoma (RCC) in a multidisciplinary tumor board 
(MDT). A retrospective analysis of 103 cases (2023-2024) found 62.1% agreement 
with MDT decisions (κ = 0.44, p< 0.001). Concordance was highest in when 
follow-up imaging was suggested (p = 0.001), with disease status influencing 
agreement (p = 0.004). These results suggest AI could assist in RCC case 
assessments, warranting further research.

© 2025. The Author(s).

DOI: 10.1038/s41698-025-01014-4
PMCID: PMC12289931
PMID: 40707576

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


282. JMIR Med Inform. 2025 Jul 24;13:e76128. doi: 10.2196/76128.

Comparative Analysis of Generative Artificial Intelligence Systems in Solving 
Clinical Pharmacy Problems: Mixed Methods Study.

Li L(1), Du P(1), Huang X(1), Zhao H(1), Ni M(1), Yan M(1), Wang A(1).

Author information:
(1)Department of Pharmacy, Fuwai Central China Cardiovascular Hospital, 1 Fuwai 
Road, Zhengdong New District, Zhengzhou, China, 86 18538298379.

BACKGROUND: Generative artificial intelligence (AI) systems are increasingly 
deployed in clinical pharmacy; yet, systematic evaluation of their efficacy, 
limitations, and risks across diverse practice scenarios remains limited.
OBJECTIVE: This study aims to quantitatively evaluate and compare the 
performance of 8 mainstream generative AI systems across 4 core clinical 
pharmacy scenarios-medication consultation, medication education, prescription 
review, and case analysis with pharmaceutical care-using a multidimensional 
framework.
METHODS: Forty-eight clinically validated questions were selected via stratified 
sampling from real-world sources (eg, hospital consultations, clinical case 
banks, and national pharmacist training databases). Three researchers 
simultaneously tested 8 different generative AI systems (ERNIE Bot, Doubao, 
Kimi, Qwen, GPT-4o, Gemini-1.5-Pro, Claude-3.5-Sonnet, and DeepSeek-R1) using 
standardized prompts within a single day (February 20, 2025). A double-blind 
scoring design was used, with 6 experienced clinical pharmacists (≥5 years 
experience) evaluating the AI responses across 6 dimensions: accuracy, rigor, 
applicability, logical coherence, conciseness, and universality, scored 0-10 per 
predefined criteria (eg, -3 for inaccuracy and -2 for incomplete rigor). 
Statistical analysis used one-way ANOVA with Tukey Honestly Significant 
Difference (HSD) post hoc testing and intraclass correlation coefficients (ICC) 
for interrater reliability (2-way random model). Qualitative thematic analysis 
identified recurrent errors and limitations.
RESULTS: DeepSeek-R1 (DeepSeek) achieved the highest overall performance (mean 
composite score: medication consultation 9.4, SD 1.0; case analysis 9.3, SD 
1.0), significantly outperforming others in complex tasks (P<.05). Critical 
limitations were observed across models, including high-risk decision errors-75% 
omitted critical contraindications (eg, ethambutol in optic neuritis) and a lack 
of localization-90% erroneously recommended macrolides for drug-resistant 
Mycoplasma pneumoniae (China's high-resistance setting), while only DeepSeek-R1 
aligned with updated American Academy of Pediatrics (AAP) guidelines for 
pediatric doxycycline. Complex reasoning deficits: only Claude-3.5-Sonnet 
detected a gender-diagnosis contradiction (prostatic hyperplasia in female); no 
model identified diazepam's 7-day prescription limit. Interrater consistency was 
lowest for conciseness in case analysis (ICC=0.70), reflecting evaluator 
disagreement on complex outputs. ERNIE Bot (Baidu) consistently underperformed 
(case analysis: 6.8, SD 1.5; P<.001 vs DeepSeek-R1).
CONCLUSIONS: While generative AI shows promise as a pharmacist assistance tool, 
significant limitations-including high-risk errors (eg, contraindication 
omissions), inadequate localization, and complex reasoning gaps-preclude 
autonomous clinical decision-making. Performance stratification highlights 
DeepSeek-R1's current advantage, but all systems require optimization in dynamic 
knowledge updating, complex scenario reasoning, and output interpretability. 
Future deployment must prioritize human oversight (human-AI co-review), ethical 
safeguards, and continuous evaluation frameworks.

©Lulu Li, Pengqiang Du, Xiaojing Huang, Hongwei Zhao, Ming Ni, Meng Yan, Aifeng 
Wang. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org).

DOI: 10.2196/76128
PMCID: PMC12288765
PMID: 40705654 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


283. JMIR Pediatr Parent. 2025 Jul 24;8:e73579. doi: 10.2196/73579.

Leveraging AI to Investigate Child Maltreatment Text Narratives: Promising 
Benefits and Addressable Risks.

Lukmanjaya W(1), Butler T(1), Cox S(2), Perez-Concha O(3), Bromfield L(2), 
Karystianis G(1).

Author information:
(1)School of Population Health, University of New South Wales Sydney, F25, 
Samuels Building, Samuel Terry Ave, Kensington, Sydney, 2033, Australia, 61 
293853136, 61 93136185.
(2)Australian Center for Child Protection, University of South Australia, 
Adelaide, Australia.
(3)Centre for Big Data Resarch in Health, University of New South Wales Sydney, 
Sydney, Australia.

The trove of information contained in child maltreatment narratives represents 
an opportunity to strengthen the evidence base for policy reform in this area, 
yet it remains underutilized by researchers and policy makers. Current research 
into child maltreatment often involves the use of qualitative methodologies or 
structured survey data that are either too broad or not representative, thereby 
limiting the development of effective policy responses and intervention 
strategies. Artificial intelligence (AI) approaches such as large language 
models (AI models that understand and generate language) can analyze large 
volumes of child maltreatment narratives by extracting population-level insights 
on factors of interest such as mental health and treatment needs. However, when 
applying such methods, it is useful to have a framework on which to base 
approaches to the data. We propose a seven step framework: (1) data governance; 
(2) researcher vetting; (3) data deidentification; (4) data access; (5) 
feasibility testing of baseline methods; (6) large-scale implementation of black 
box algorithms; and (7) domain expert result validation for such exercises to 
ensure careful execution and limit the risk of privacy and security breaches, 
bias, and unreliable conclusions.

© Wilson Lukmanjaya, Tony Butler, Sarah Cox, Oscar Perez-Concha, Leah Bromfield, 
George Karystianis. Originally published in JMIR Pediatrics and Parenting 
(https://pediatrics.jmir.org).

DOI: 10.2196/73579
PMCID: PMC12288702
PMID: 40705434

Conflict of interest statement: Conflicts of Interest: None declared.


284. BMJ Ment Health. 2025 Jul 22;28(1):e301762. doi: 10.1136/bmjment-2025-301762.

Development and evaluation of prompts for a large language model to screen 
titles and abstracts in a living systematic review.

Homiar A(1)(2), Thomas J(3), Ostinelli EG(4), Kennett J(4), Friedrich C(4), 
Cuijpers P(5), Harrer M(6), Leucht S(7), Miguel C(8), Rodolico A(9), Kataoka 
Y(10), Takayama T(11), Yoshimura K(11), So R(12), Tsujimoto Y(11), Yamagishi 
Y(13), Takagi S(14), Sakata M(15), Bašić Đ(5), Karyotaki E(8), Potts J(16), 
Salanti G(17), Furukawa TA(18), Cipriani A(4).

Author information:
(1)Department of Psychiatry, University of Oxford, Oxford, UK 
avahomiar@gmail.com.
(2)Division of Clinical Informatics, Beth Israel Deaconess Medical Center, 
Boston, Massachusetts, USA.
(3)University College London, London, UK.
(4)Department of Psychiatry, University of Oxford, Oxford, UK.
(5)Vrije Universiteit Amsterdam, Amsterdam, The Netherlands.
(6)Psychology & Digital Mental Health Care, Technical University of Munich, 
Munchen, Germany.
(7)Psychiatry and Psychotherapy, Technical University of Munich School of 
Medicine, Munich, Germany.
(8)Department of Clinical, Neuro and Developmental Psychology, Vrije 
Universiteit Amsterdam, Amsterdam, The Netherlands.
(9)Technical University of Munich, Munich, Germany.
(10)Kyoto Min-iren Asukai Hospital, Kyoto, Japan.
(11)Kyoto University, Kyoto, Japan.
(12)Okayama Psychiatric Medical Center, Okayama, Japan.
(13)The University of Tokyo, Bunkyo, Tokyo, Japan.
(14)Independent Researcher, Kyoto, Japan.
(15)Nagoya City University Graduate School of Medical Sciences and Medical 
School, Nagoya, Japan.
(16)NIHR Oxford Health Biomedical Research Centre, Oxford, UK.
(17)University of Bern, Bern, Switzerland.
(18)Kyoto University Graduate School of Medicine Faculty of Medicine, Kyoto, 
Japan.

BACKGROUND: Living systematic reviews (LSRs) maintain an updated summary of 
evidence by incorporating newly published research. While they improve review 
currency, repeated screening and selection of new references make them 
labourious and difficult to maintain. Large language models (LLMs) show promise 
in assisting with screening and data extraction, but more work is needed to 
achieve the high accuracy required for evidence that informs clinical and policy 
decisions.
OBJECTIVE: The study evaluated the effectiveness of an LLM (GPT-4o) in title and 
abstract screening compared with human reviewers.
METHODS: Human decisions from an LSR on prodopaminergic interventions for 
anhedonia served as the reference standard. The baseline search results were 
divided into a development and a test set. Prompts guiding the LLM's eligibility 
assessments were refined using the development set and evaluated on the test set 
and two subsequent LSR updates. Consistency of the LLM outputs was also 
assessed.
RESULTS: Prompt development required 1045 records. When applied to the remaining 
baseline 11 939 records and two updates, the refined prompts achieved 100% 
sensitivity for studies ultimately included in the review after full-text 
screening, though sensitivity for records included by humans at the title and 
abstract stage varied (58-100%) across updates. Simulated workload reductions of 
65-85% were observed. Prompt decisions showed high consistency, with minimal 
false exclusions, satisfying established screening performance benchmarks for 
systematic reviews.
CONCLUSIONS: Refined GPT-4o prompts demonstrated high sensitivity and moderate 
specificity while reducing human workload. This approach shows potential for 
integrating LLMs into systematic review workflows to enhance efficiency.

© Author(s) (or their employer(s)) 2025. Re-use permitted under CC BY. Published 
by BMJ Group.

DOI: 10.1136/bmjment-2025-301762
PMCID: PMC12306261
PMID: 40701625 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: AH, JT, JK, CF, PC, CM, AR, 
YK, KY, YY, ST, ĐB, EK, JP and GS: None. EGO received research and consultancy 
fees from Angelini Pharma. MH is a part-time employee of Get. On Institut 
GmbH/HelloBetter, a company that implements digital therapeutics into routine 
care. SL in the last three years has received honoraria for advising/consulting 
and/or for lectures and/or for educational material from Angelini, Apsen, 
Boehringer Ingelheim, Eisai, Ekademia, GedeonRichter, Janssen, Karuna, Kynexis, 
Lundbeck, Medichem, Medscape, Mitsubishi, Neurotorium, Otsuka, NovoNordisk, 
Recordati, Rovi and Teva. TT is a part-time employee of Fitting Cloud, outside 
of the submitted work. RS is an employee of CureApp. RS reports grants from 
Osake-no-Kagaku Foundation, the Mental Health Okamoto Memorial Foundation, 
Kobayashi Magobe Memorial Medical Foundation, personal fees from Otsuka 
Pharmaceutical, Nippon Shinyaku, Takeda Pharmaceutical and Sumitomo Pharma 
outside this work; In addition, RS has a patent JP2022049590A, US20220084673A1 
pending, a patent JP2022178215A pending, a patent JP2022070086 pending and a 
patent JP2023074128A pending. YT reports grants from Japan Society for the 
Promotion of Science, Kyoto University and Pfizer Foundation, outside of the 
submitted work. In addition, YT is a board member of Cochrane Japan and works as 
a physician at Oku Medical Clinic. MS is employed in the department of 
neurodevelopmental disorders, Nagoya City University Graduate School of 
Medicine, which is an endowment department supported by the City of Nagoya and 
has received a personal fee from SONY outside of the submitted work. TAF reports 
personal fees from Boehringer-Ingelheim, Daiichi Sankyo, DT Axis, Micron, 
Shionogi, SONY and UpToDate, and a grant from DT Axis and Shionogi, outside of 
the submitted work. In addition, TAF has a patent 7448125 and a pending patent 
2022-082495 and has licensed intellectual properties for Kokoro-app to DT Axis. 
AC has received research, educational and consultancy fees from the Italian 
Network for Paediatric Trials, CARIPLO Foundation, Lundbeck and Angelini Pharma, 
outside of the submitted work.


285. Best Pract Res Clin Obstet Gynaecol. 2025 Sep;102:102640. doi: 
10.1016/j.bpobgyn.2025.102640. Epub 2025 Jul 18.

Current and emerging data sources for assessment of ovarian toxicity in 
children, adolescents and young adults with cancer.

Strohl HB(1), Do NT(2), Irene Su H(3).

Author information:
(1)Department of Obstetrics, Gynecology, and Reproductive Sciences and Moores 
Cancer Center, University of California, 3855 Health Sciences Dr, Dept 0901, La 
Jolla, San Diego, CA, 92037, USA. Electronic address: hstrohl@health.ucsd.edu.
(2)Department of Obstetrics, Gynecology, and Reproductive Sciences and Moores 
Cancer Center, University of California, 3855 Health Sciences Dr, Dept 0901, La 
Jolla, San Diego, CA, 92037, USA. Electronic address: ntd005@health.ucsd.edu.
(3)Department of Obstetrics, Gynecology, and Reproductive Sciences and Moores 
Cancer Center, University of California, 3855 Health Sciences Dr, Dept 0901, La 
Jolla, San Diego, CA, 92037, USA. Electronic address: hisu@health.ucsd.edu.

One in 20 cancers occurs in children, adolescents, and young adults, with some 
treatments leading to infertility or premature ovarian insufficiency. Cancer 
survivors and clinicians seek to estimate reproductive risks to guide fertility 
preservation and manage ovarian health post-treatment. Available data focus more 
on surrogate outcomes like amenorrhea and ovarian reserve markers than clinical 
outcomes such as ovarian insufficiency. Tools like the Cancer-Related 
Infertility Score Predictor (CRISP) and FDA-recommended data sources, including 
the FDA Adverse Event Reporting System, provide guidance on known ovarian 
toxicity risks. However, many novel and current cancer treatments lack 
comprehensive data. Emerging strategies include using real-world administrative 
data linked with lab results to estimate risks, large language models to 
streamline systematic reviews, regulatory guidance requiring ovarian toxicity 
data in new drug trials, and ex vivo ovary models for testing. This review 
highlights the need for improved methods and consistent assessment to support 
the reproductive health of young cancer survivors.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.bpobgyn.2025.102640
PMID: 40700834 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest None.


286. J Am Med Inform Assoc. 2025 Sep 1;32(9):1445-1453. doi: 10.1093/jamia/ocaf121.

Explaining alerts from a pediatric risk prediction model using clinical text.

Nycklemoe S(1), Devarapu S(1), Gao Y(2), Carey K(3), Kuehnel N(4), Munjal N(5), 
Jani P(6), Churpek M(1)(7), Dligach D(8), Afshar M(1)(7), Mayampurath A(1)(7).

Author information:
(1)Department of Biostatistics & Medical Informatics, University of 
Wisconsin-Madison, Madison, WI 53726, United States.
(2)Department of Biomedical Informatics, University of Colorado Anschutz Medical 
Campus, Aurora, CO 80045, United States.
(3)Department of Medicine, University of Chicago, Chicago, IL 60637, United 
States.
(4)Department of Emergency Medicine, University of Wisconsin-Madison, Madison, 
WI 53726, United States.
(5)Department of Pediatrics, University of Wisconsin-Madison, Madison, WI 53726, 
United States.
(6)Department of Pediatrics, University of Chicago, Chicago, IL 60637, United 
States.
(7)Department of Medicine, University of Wisconsin-Madison, Madison, WI 53726, 
United States.
(8)Department of Computer Science, Loyola University, Chicago, IL 60626, United 
States.

OBJECTIVE: Risk prediction models are used in hospitals to identify pediatric 
patients at risk of clinical deterioration, enabling timely interventions and 
rescue. The objective of this study was to develop a new explainer algorithm 
that uses a patient's clinical notes to generate text-based explanations for 
risk prediction alerts.
MATERIALS AND METHODS: We conducted a retrospective study of 39 406 patient 
admissions to the American Family Children's Hospital at the University of 
Wisconsin-Madison (2009-2020). The pediatric Calculated Assessment of Risk and 
Triage (pCART) validated risk prediction model was used to identify children at 
risk for deterioration. A transformer model was trained to use clinical notes 
from the 12-hour period preceding each pCART score to predict whether a patient 
was flagged as at risk. Then, label-aware attention highlighted text phrases 
most important to an at-risk alert. The study cohort was randomly split into 
derivation (60%) and validation (20%) data, and a separate test (20%) was used 
to evaluate the explainer's performance.
RESULTS: Our pCART Explainer algorithm performed well in discriminating at-risk 
pCART alert vs no alert (c-statistic 0.805). Sample explanations from pCART 
Explainer revealed clinically important phrases such as "rapid breathing," "fall 
risk," "distension," and "grunting," thereby demonstrating excellent face 
validity.
DISCUSSION: The pCART Explainer could quickly orient clinicians to the patient's 
condition by drawing attention to key phrases in notes, potentially enhancing 
situational awareness and guiding decision-making.
CONCLUSION: We developed pCART Explainer, a novel algorithm that highlights text 
within clinical notes to provide medically relevant context about deterioration 
alerts, thereby improving the explainability of the pCART model.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf121
PMCID: PMC12361859
PMID: 40700686 [Indexed for MEDLINE]

Conflict of interest statement: COMPETING INTEREST STATEMENT The authors declare 
no competing interests. Dr. Churpek has a patent (#11,410,777) for risk 
stratification algorithms for hospitalized patients and receives royalties from 
the University of Chicago for this intellectual property


287. PLOS Digit Health. 2025 Jul 23;4(7):e0000919. doi: 10.1371/journal.pdig.0000919. 
eCollection 2025 Jul.

Using large language models to extract information from pediatric clinical 
reports.

Danhauser K(1), Wang Y(1), Klein C(1), Tacke U(2), Mantoan L(1), Ritter LA(1), 
Heinen F(1), Nobile C(1), Tacke M(1).

Author information:
(1)Department of Pediatrics, LMU University Hospital, Munich, Germany.
(2)University Children's Hospital, Basel, Switzerland.

Most medical documentation, including clinical reports, exists in unstructured 
formats, which hinder efficient data analysis and integration into 
decision-making systems for patient care and research. Both fields could profit 
significantly from a reliable automatic analysis of these documents. Current 
methods for data extraction from these documents are labor-intensive and 
inflexible. Large Language Models (LLMs) offer a promising alternative for 
transforming unstructured medical documents into structured data in a flexible 
manner. This study assesses the performance of large language models (LLMs) in 
extracting structured data from pediatric clinical reports. Nine different LLMs 
were assessed. The results demonstrate that both commercial and open-source LLMs 
can achieve high accuracy in identifying patient-specific information, with 
top-performing models achieving over 90% accuracy in key tasks.

Copyright: © 2025 Danhauser et al. This is an open access article distributed 
under the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pdig.0000919
PMCID: PMC12286318
PMID: 40700460

Conflict of interest statement: The authors have declared that no competing 
interests exist.


288. J Med Internet Res. 2025 Jul 23;27:e71418. doi: 10.2196/71418.

Perceptions and Attitudes of Chinese Oncologists Toward Endorsing AI-Driven 
Chatbots for Health Information Seeking Among Patients with Cancer: 
Phenomenological Qualitative Study.

Zeng L(1), Li Q(1), Zuo Y(2)(3)(4), Zhang Y(5), Li Z(6).

Author information:
(1)Division of Abdominal Tumor Multimodality Treatment, Cancer Center, West 
China Hospital, Sichuan University, Chengdu, China.
(2)Department of Gynecology and Obstetrics Nursing, West China Second University 
Hospital, Sichuan University, Chengdu, China.
(3)West China School of Nursing, Sichuan University, Chengdu, China.
(4)Key Laboratory of Birth Defects and Related Disease of Women and Children, 
Sichuan University, Chengdu, China.
(5)Division of Internal Medicine, Institute of Integrated Traditional Chinese 
and Western Medicine, West China Hospital, Sichuan University, Chengdu, China.
(6)Department of Radiation Oncology, Hainan Affiliated Hospital of Hainan 
Medical University (Hainan General Hospital), #19 Xiuhua Road, Xiuying District, 
Haikou, 570311, China, 86 18898963083.

BACKGROUND: Chatbots driven by large language model artificial intelligence (AI) 
have emerged as potential tools to enhance health information access for 
patients with cancer. However, their integration into patient education raises 
concerns among oncologists. Limited literature has examined the perceptions and 
attitudes of oncologists in terms of endorsing AI-driven chatbots for health 
information.
OBJECTIVE: This study aims to explore the perceptions and attitudes of Chinese 
oncologists toward endorsing AI-driven chatbots to patients with cancer.
METHODS: In this phenomenological qualitative study, we purposively sampled 
oncologists from 4 hospitals in Southwest and East China and conducted 
semistructured interviews with 24 participants between November 19, 2024, and 
December 21, 2024. The data saturation principle was observed to determine the 
end point of data collection. Data were analyzed using the Colaizzi method.
RESULTS: The participants were aged 42.0 (range 29-53) years on average, 
including 9 (37%) female and 15 (62%) male participants. The participants had an 
average of 8.8 (range 1-25) years in oncology. Of the participants, 7 (29%) had 
recommended AI chatbots to patients. Three key themes were revealed from 
analysis of interview transcriptions, including perceived benefits, significant 
concerns, and impacts on doctor-patient dynamics. Benefits included enhanced 
accessibility and potential support for chronic condition management. Concerns 
centered on liability, misinformation, lack of personalization, privacy and data 
security risks, and patient readiness and education. Oncologists stressed a dual 
impact of AI chatbots on doctor-patient dynamics, recognizing the potential for 
improved communication and risks of trust erosion due to overreliance on AI.
CONCLUSIONS: While recognizing the potential of AI-driven chatbots to enhance 
accessibility of health information and chronic disease management, Chinese 
oncologists report significant concerns, including liability, misinformation, 
lack of personalization, privacy and data security risks, and patient readiness. 
Addressing the challenges requires comprehensive solutions, such as clear 
policies and guidelines, rigorous testing and validation, institutional 
endorsement, and robust patient and provider education. Future efforts should 
focus on resolving the barriers while leveraging the strengths of AI technology 
to support patient-centered care in a safe, effective, and ethical manner.

© Lijuan Zeng, Qiaoqi Li, Qiaoqi Li, Ying Zhang, Zhaojun Li. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/71418
PMCID: PMC12309621
PMID: 40699917 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


289. Int Ophthalmol. 2025 Jul 23;45(1):314. doi: 10.1007/s10792-025-03690-4.

Advances in machine learning for ABCA4-related retinopathy: segmentation and 
phenotyping.

Shwetar YJ(1), Brooks BP(2), Jeffrey BG(2), Solomon BD(3), Haendel MA(4).

Author information:
(1)Joint Department of Biomedical Engineering, University of North Carolina and 
North Carolina State University, Chapel Hill, NC, USA. 
yousif_shwetar@med.unc.edu.
(2)Ophthalmic Genetics and Visual Function Branch, National Eye Institute, NIH, 
Bethesda, MD, USA.
(3)Medical Genetics Branch, National Human Genome Research Institute, NIH, 
Bethesda, MD, USA.
(4)Department of Genetics, University of North Carolina, Chapel Hill, NC, USA.

PURPOSE: Stargardt disease, also called ABCA4-related retinopathy (ABCA4R), is 
the most common form of juvenile-onset macular dystrophy and yet lacks an FDA 
approved treatment. Substantial progress has been made through landmark studies 
like that of the Progression of Atrophy Secondary to Stargardt Disease 
(ProgStar), but tasks like image segmentation and phenotyping still pose major 
challenges in terms of monitoring disease progression and categorizing patient 
subgroups. Furthermore, these methods are subjective and laborious. Recent 
advancements in machine learning (ML) and deep learning show considerable 
promise in automating these processes.
METHODS: This scoping review explores ML applications in ABCA4R, with a focus on 
segmentation and phenotyping. Following the Preferred Reporting Items for 
Systematic Reviews and Meta-Analysis (PRISMA) methodology, 15 articles were 
selected from 264, with 12 focused on the task of segmenting atrophic lesions, 
retinal flecks, retinal layer boundaries, or en-face imaging. Three studies 
addressed phenotyping based on electroretinography (ERG), visual acuity, and 
microperimetry.
RESULTS: Several effective approaches were implemented in these studies, 
including ensemble modeling, self-attention mechanisms, soft-label approaches, 
and dynamic frameworks that consider extent of tissue damage. Excellent model 
performance includes segmentation DICE performances of 0.99 and ERG phenotyping 
accuracies 90% and greater. Smaller datasets and variable presentations present 
as significant challenges, while advanced methods like Monte Carlo dropout and 
active learning improve pipeline efficiency and performance.
CONCLUSION: ML techniques are well on their way to automate key steps in ABCA4R 
evaluation with excellent performance. These emerging methods have the potential 
to expedite therapeutic innovation and enhance our understanding of ABCA4R.

© 2025. The Author(s).

DOI: 10.1007/s10792-025-03690-4
PMCID: PMC12287237
PMID: 40699379 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare no competing interests. Ethical approval: No ethics approval required. 
Consent to participate: No consent to participate required. Consent for 
publication: No consent to publish required.


290. Int Endod J. 2025 Jul 22. doi: 10.1111/iej.14277. Online ahead of print.

Endodontic diagnostics training in undergraduate dental education: An 
observational pilot study on AI-driven virtual patient e-learning.

Prinz M(1), Schäfer E(2), Bürklein S(2), Donnermeyer D(1)(3).

Author information:
(1)Department of Periodontology and Operative Dentistry, University of Münster, 
Münster, Germany.
(2)Central Interdisciplinary Ambulance in the School of Dentistry, University of 
Münster, Münster, Germany.
(3)Department of Restorative, Preventive and Pediatric Dentistry, School of 
Dental Medicine, University of Bern, Bern, Switzerland.

AIM: To develop and evaluate an e-learning tool utilizing a generative 
pre-trained transformer (GPT), a form of artificial intelligence (AI), to allow 
for realistic conversation on virtual patients when undergoing training on how 
to diagnose diseases of endodontic origin, and to evaluate improvements in 
self-perceived skills.
METHODOLOGY: A web app consisting of three components [website for user access, 
database server with patient case information, GPT-4-turbo model (OpenAI)] was 
designed to serve as the e-learning platform. Undergraduate students from 4th 
and 5th year at the dental school of the University of Münster, Germany, were 
asked to solve eight cases of virtual patients presenting with pain from 
endodontic or periodontal origin. Before, a questionnaire applying a 5-point 
Likert scale served to evaluate the current self-perceived state regarding the 
education, experience and skills in endodontic diagnostics and emergency 
treatment. After a 3-month timeframe of working with the programme individually, 
the students were asked to answer a second questionnaire which focused on their 
experience and self-perceived skills improvement after using the training 
software.
RESULTS: Ninety-two students participated in the first questionnaire and 72 
students finished the second questionnaire, resulting in a drop-out rate of 
21.7%. Students in the 5th-year reported more experience in dealing with 
emergency patients. Initially, both cohorts mainly did not feel confident to 
perform endodontic diagnostics independently. The evaluation of confidence to 
perform endodontic diagnostics independently, both by 4th- and 5th-year 
students, seemed improved after the training on the e-learning tool. The tool 
was recommended to be available for endodontic education by 72.2% of the 
students, who strongly agreed to such a statement. 76.4% of the participants 
strongly agreed to recommend the use of the tool to other students.
CONCLUSIONS: AI-based interactive e-learning programmes allowing for complex 
conversational patient encounters present a possibility to improve diagnostic 
and interactive skills of undergraduate students.

© 2025 The Author(s). International Endodontic Journal published by John Wiley & 
Sons Ltd on behalf of British Endodontic Society.

DOI: 10.1111/iej.14277
PMID: 40692414


291. Sci Rep. 2025 Jul 21;15(1):26378. doi: 10.1038/s41598-025-09862-7.

MDNCT: a multi-domain neurocognitive transformer architecture approach for early 
prediction of autism spectrum disorders.

Alsubai S(1)(2).

Author information:
(1)Department of Computer Science, College of Computer Engineering and Sciences, 
Prince Sattam bin Abdulaziz University, P.O. Box 151, 11942, Al-Kharj, Saudi 
Arabia. Sa.alsubai@psau.edu.sa.
(2)King Salman Center for Disability Research, Riyadh, 11614, Saudi Arabia. 
Sa.alsubai@psau.edu.sa.

Intellectual disability (ID) refers to a disorder involving intelligence and 
adaptive behavior that meets specific criteria involving deviance from the norm 
in terms of degree. ID is more common in males than females, and the causes can 
be genetic or environmental. This population has historically been characterized 
by significantly decreased life expectancy because they have not been diagnosed 
and treated for such diseases as cardiovascular and respiratory ones. However, 
medical progress in the last few years has slightly narrowed this gap, 
highlighting that understanding ID requires its consideration as a comorbidity 
to neurodevelopmental and cognitive diseases like Autism Spectrum Disorder 
(ASD), dementia, or learning disability. Thus, this work proposes the 
multi-domain NeuroCognitive Transformer (MDNCT) suitable for different 
prediction tasks on different datasets. Therefore, MDNCT obtains high 
performance based on the adequate preprocessing level according to the domain 
data's specific characteristics, more advanced feature extraction methods, and 
the use of Transformer-based neural networks. The structure of the framework 
incorporates common means to align multiple features across modalities and also 
other state-of-the-art features like multi-head self-attention and residual 
connections for learning. The use of the MDNCT includes important domains, 
including early dementia diagnostics for health purposes, social media comments 
toward learning disabilities, and effective identification of ASD in toddlers.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-09862-7
PMCID: PMC12280032
PMID: 40691484 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no conflict of interests.


292. Inj Prev. 2025 Jul 21:ip-2025-045655. doi: 10.1136/ip-2025-045655. Online ahead 
of print.

Large language models in public health: opportunity or threat? The case of 
button battery injuries.

Lorenzoni G(1), Gregori D(2).

Author information:
(1)Unit of Biostatistics, Epidemiology and Public Health, Department of Cardiac, 
Thoracic Vascular Sciences and Public Health, University of Padova, Padova, 
Italy.
(2)Unit of Biostatistics, Epidemiology and Public Health, Department of Cardiac, 
Thoracic Vascular Sciences and Public Health, University of Padova, Padova, 
Italy dario.gregori@unipd.it.

BACKGROUND: Button battery (BB) injuries in children represent a severe and 
growing public health burden. The literature on the topic is extensive; however, 
there is a notable lack of structured public health initiatives addressing the 
problem. The present study aimed to test the feasibility of using large language 
models (LLMs) to draft recommendations for preventing and managing BB ingestion 
in children.
METHODS: A set of questions was generated and submitted to ChatGPT-4o and 
ChatGPT-o1-preview. Questions were based on statements and websites of 
scientific societies and not-for-profit organisations and were developed to 
produce comprehensive recommendations that provided information on BB risks, 
primary and secondary prevention, clinical management and follow-up, and general 
public health initiatives. Two independent reviewers rated the accuracy and 
readability of the questions submitted to the LLMs. The accuracy was rated using 
a four-level scale, while the readability was assessed using two established 
readability tools, the Flesch Reading Ease (FRE) and the Flesch-Kincaid Grade 
Level (FKGL).
RESULTS: None of the answers provided by the LLMs were rated as completely 
incorrect or partially incorrect. ChatGPT-o1-preview outperformed ChatGPT-4o in 
accuracy, with eight answers rated as accurate and complete. Both models showed 
similar readability levels, with high FKGL and FRE scores indicating 
college-level comprehension.
DISCUSSION: LLM demonstrated a strong performance in this study, with no 
responses rated as incorrect or partially incorrect, showing its great potential 
and feasibility for use in public health.
CONCLUSIONS: The present findings suggested the potential feasibility of LLMs in 
public health for preventing paediatric injuries.

© Author(s) (or their employer(s)) 2025. No commercial re-use. See rights and 
permissions. Published by BMJ Group.

DOI: 10.1136/ip-2025-045655
PMID: 40691011

Conflict of interest statement: Competing interests: None declared.


293. Int J Med Inform. 2025 Nov;203:106015. doi: 10.1016/j.ijmedinf.2025.106015. Epub 
2025 Jun 19.

Toward responsible AI governance: Balancing multi-stakeholder perspectives on AI 
in healthcare.

Rozenblit L(1), Price A(2), Solomonides A(3), Joseph AL(4), Koski E(5), 
Srivastava G(6), Labkoff S(7), Bray D(8), Lopez-Gonzalez M(9), Singh R(10), 
deBronkart D(11), Barr PJ(12), Szolovits P(13), Dattani K(14), Jaffe C(15), 
Fridsma D(16), Baris R(17), Leftwich R(18), Stolper R(19), Weiner MG(20), Pastor 
N(21), Luque US(21), Lin B(22), Thuy Bui TT(23), Oladimeji B(24), Williams 
T(10), Jackson GP(25), Hsueh PS(26), Quintana Y(27).

Author information:
(1)Q.E.D. Institute, New Haven, CT, USA; Division of Clinical Informatics, Beth 
Israel Deaconess Medical Center, Boston, MA, USA; Yale School of Management, New 
Haven, CT, USA.
(2)The Dartmouth Institute for Health Policy and Clinical Practice, Geisel 
School of Medicine at Dartmouth, Hanover, NH, USA; BMJ, London, UK.
(3)Research Institute, Endeavor Health, Evanston, IL, USA.
(4)Division of Clinical Informatics, Beth Israel Deaconess Medical Center, 
Boston, MA, USA; School of Health Information Science, University of Victoria, 
Victoria, BC, Canada; Homewood Research Institute, Guelph, Ontario, Canada.
(5)IBM Research, Yorktown Heights, NY, USA.
(6)Division of Clinical Informatics, Beth Israel Deaconess Medical Center, 
Boston, MA, USA.
(7)Division of Clinical Informatics, Beth Israel Deaconess Medical Center, 
Boston, MA, USA; Quantori, Boston, MA, USA.
(8)Loomis Innovation Council, Stimson Center, Washington, DC, USA; Business 
Executives for National Security, Washington, DC, USA.
(9)Cognitive Insights for Artificial Intelligence, Baltimore, MD, USA.
(10)American Medical Informatics Association, Washington, DC, USA.
(11)Society for Participatory Medicine, Boston, MA, USA; HL7® Patient 
Empowerment Workgroup, USA; The BMJ's Patient Advisory Panel, London, UK.
(12)The Dartmouth Institute for Health Policy and Clinical Practice, Geisel 
School of Medicine at Dartmouth, Hanover, NH, USA.
(13)Department of Electrical Engineering and Computer Science, Massachusetts 
Institute of Technology, Cambridge, MA, USA.
(14)Google Cloud Healthcare and Life Sciences, New York, NY, USA.
(15)Health Level 7 International, Ann Arbor, MI, USA.
(16)Health Universe, San Francisco, CA, USA.
(17)eLumindata, Westport, CT, USA.
(18)Department of Biomedical Informatics, Vanderbilt University, Nashville, TN, 
USA.
(19)IQVIA, Boston, MA, USA.
(20)Department of Population Health Sciences, Weill Cornell Medicine, New York, 
NY, USA.
(21)HumanITcare, Barcelona, Spain.
(22)Departments of Neuroscience and Systems Biology, Columbia University Irving 
Medical Center, New York, NY, USA.
(23)Division of Clinical Informatics, Beth Israel Deaconess Medical Center, 
Boston, MA, USA; Massachusetts College of Pharmacy and Health Sciences, Boston, 
MA, USA.
(24)UnitedHealth Group, Minnesota, MN, USA.
(25)Intuitive Surgical, Sunnyvale, CA, USA; Pediatrics and Biomedical 
Informatics, Vanderbilt University Medical Center, Nashville, TN, USA.
(26)Pfizer Inc., New York, NY, USA.
(27)Division of Clinical Informatics, Beth Israel Deaconess Medical Center, 
Boston, MA, USA; Homewood Research Institute, Guelph, Ontario, Canada; 
Department of Medicine, Harvard Medical School, Boston, MA, USA. Electronic 
address: yquintan@bidmc.harvard.edu.

INTRODUCTION: The rapid integration of artificial intelligence (AI) into 
healthcare presents significant governance challenges, requiring balanced 
approaches that safeguard safety, efficacy, equity, and trust (SEET). This study 
proposes a cognitive framework to guide AI governance, addressing tradeoffs 
between speed, scope, and capability.
OBJECTIVE: To develop a structured governance model that harmonizes stakeholder 
perspectives, focusing on multi-dimensional challenges and ethical principles 
essential for AI in healthcare.
METHODS: A multidisciplinary team convened at the Blueprints for Trust 
conference, organized by the American Medical Informatics Association (AMIA), 
and the Division of Clinical Informatics at Beth Israel Deaconess Medical 
Center. Following extensive discussions with 190 participants across sectors, 
three governance models were identified to address specific domains: (1) 
Clinical Decision Support (CDS), (2) Real-World Evidence (RWE), (3) Consumer 
Health (CH).
RESULTS: Three governance models emerged, tailored to CDS, RWE, and CH domains. 
Key recommendations include establishing a Health AI Consumer Consortium for 
patient-centered oversight, initiating voluntary accreditation and certification 
frameworks, and piloting risk-level-based standards. These models balance rapid 
adaptation with SEET-focused safeguards through transparency, inclusivity, and 
ongoing learning.
CONCLUSION: A proactive, constraint-based governance framework is critical for 
responsible AI integration in healthcare. This structured, multi-stakeholder 
approach provides a roadmap for ethical, transparent governance that can evolve 
with technological advancements, enhancing trust and safety in healthcare AI 
applications.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106015
PMID: 40680319 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: Steven Labkoff is an employee of 
Quantori, LLC. Eileen Koski is employed by and owns stock in IBM. David Bray is 
the Principal and CEO at LeadDoAdapt, consultant to Health Level Seven's AI 
Taskforce, and is a periodic keynote speaker on Healthcare, AI, and related 
issues. Douglas Fridsma is an employee of Health Universe. Russell Leftwich is 
an employee of InterSystems Corporation. Bilikis Oladimeji is an employee and 
own stocks/stock options of UnitedHealth Group. Gretchen Purcell Jackson is an 
employee of Intuitive Surgical. Gretchen owns stock and/or stock options for 
IBM, Kyndryl, and Intuitive Surgical. Gretchen was the Past President and Past 
Board Chair of the American Medical Informatics Association. Pei-Yun Sabrina 
Hsueh is an employee of Pfizer Inc. Sabrina owns stock and/or stock options for 
IBM, Bayesian Health, and Pfizer. Sabrina is on the Practitioners Board of 
Association for Computing Machinery (ACM). The authors had no other conflicts or 
competing interests to disclose. All opinions are the authors’ and not the view 
of my employer or affiliated organizations.


294. JMIR Form Res. 2025 Jul 18;9:e74411. doi: 10.2196/74411.

Future Me, a Prospection-Based Chatbot to Promote Mental Well-Being in Youth: 
Two Exploratory User Experience Studies.

Dechant M(1), Lash E(2), Shokr S(1), O'Driscoll C(1).

Author information:
(1)University College London, 1-19 Torrington Place, London, WC1E 7HB, United 
Kingdom, +44 (0) 20 7679 1897.
(2)University of Oxford, Oxford, United Kingdom.

BACKGROUND: Digital interventions have been proposed as a solution to meet the 
growing demand for mental health support. Large language models (LLMs) have 
emerged as a promising technology for creating more personalized and adaptive 
mental health chatbots. While LLMs generate responses based on statistical 
patterns in training data rather than through conscious reasoning, they can be 
designed to support important psychological processes. Prospection-the ability 
to envision and plan for future outcomes-represents a transdiagnostic process 
altered across various mental health conditions that could be effectively 
targeted through such interventions. We designed "Future Me," an LLM-powered 
chatbot designed to facilitate future-oriented thinking and promote goal pursuit 
using evidence-based interventions including visualization, implementation 
intentions, and values clarification.
OBJECTIVE: This study aims to understand how users engage with Future Me, 
evaluate its effectiveness in supporting future-oriented thinking, and assess 
its acceptability across different populations, with particular attention to 
postgraduate students' stress management needs. We also seek to identify design 
improvements that could enhance the chatbot's ability to support users' mental 
well-being.
METHODS: In total, 2 complementary studies were conducted. Study 1 (n=20) 
examined how postgraduate students used Future Me during a single guided 
session, followed by semistructured interviews. Study 2 (n=14) investigated how 
postgraduate students interacted with Future Me over a 1-week period, with 
interviews before and after usage. Both studies analyzed conversation 
transcripts and interview data using thematic analysis to understand usage 
patterns, perceived benefits, and limitations.
RESULTS: Across both studies, participants primarily engaged with Future Me to 
discuss career or education goals, personal obstacles, and relationship 
concerns. Users valued Future Me's ability to provide clarity around 
goal-setting (85% of participants), its nonjudgmental nature, and its 24/7 
accessibility (58%). Future Me effectively facilitated self-reflection (80%) and 
offered new perspectives (70%), particularly for broader future-oriented 
concerns. However, both studies revealed limitations in the chatbot's ability to 
provide personalized emotional support during high-stress situations, with 
participants noting that responses sometimes felt formulaic (50%) or lacked 
emotional depth. Postgraduate students specifically emphasized the need for 
greater context awareness during periods of academic stress (58%). Overall, 57% 
of requests occurred outside office hours, dropping from 40 on day 1 to 12 by 
day 7.
CONCLUSIONS: Future Me demonstrates promise as an accessible tool for promoting 
prospection skills and supporting mental well-being through future-oriented 
thinking. However, effectiveness appears context-dependent, with prospection 
techniques more suitable for broader life decisions than acute stress 
situations. Future development should focus on creating more adaptive systems 
that can adjust their approach based on the user's emotional state and immediate 
needs. Rather than attempting to replicate human therapy entirely, chatbots like 
Future Me may be most effective when designed as complementary tools within 
broader support ecosystems, offering immediate guidance while facilitating 
connections to human support when needed.

© Martin Dechant, Eva Lash, Sarah Shokr, Ciarán O'Driscoll. Originally published 
in JMIR Formative Research (https://formative.jmir.org).

DOI: 10.2196/74411
PMCID: PMC12296204
PMID: 40680255 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


295. Eur J Pediatr. 2025 Jul 18;184(8):491. doi: 10.1007/s00431-025-06318-y.

Comparative evaluation of ChatGPT and LLaMA for reliability, quality, and 
accuracy in familial Mediterranean fever.

Uzun Bektaş A(1), Bora B(2), Ünsal E(2).

Author information:
(1)Department of Pediatric Rheumatology, Dokuz Eylul University, Izmir, Turkey. 
aslihan.uzun@deu.edu.tr.
(2)Department of Pediatric Rheumatology, Dokuz Eylul University, Izmir, Turkey.

Comment in
    Eur J Pediatr. 2025 Nov 12;184(12):750. doi: 10.1007/s00431-025-06434-9.

Familial Mediterranean fever (FMF) is the most common monogenic autoinflammatory 
disease. Large language models (LLMs) offer rapid access to medical information. 
This study evaluated and compared the reliability, quality, and accuracy of 
ChatGPT-4o and LLaMA-3.1 for FMF. Single Hub and Access Point for Paediatric 
Rheumatology in Europe (SHARE) and European League Against Rheumatism (EULAR) 
guidelines were used for question generation and also for answer validation. 
Thirty-one questions were developed from a clinician's perspective based on the 
related guidelines. Two pediatric rheumatologists with over 20 years of FMF 
experience independently and blindly evaluated the responses. Reliability, 
quality, and accuracy were assessed using the modified DISCERN Scale, Global 
Quality Score, and the guidelines, respectively. Readability was assessed using 
multiple established indices. Statistical analyses included the Shapiro-Wilk 
test to assess normality, followed by paired t-tests for normally distributed 
scores, and Wilcoxon signed-rank tests for non-normally distributed scores. Both 
models demonstrated moderate reliability and high response quality. In terms of 
alignment with the guidelines, LLaMA provided fully aligned, complete, and 
accurate responses to 51.6% (16/31) of the questions, whereas ChatGPT provided 
such responses to 80.6% (25/31). While 9.7% (4/31) of LLaMA's responses were 
entirely contradictory to the guidelines, ChatGPT did not produce any such 
responses. ChatGPT outperformed LLaMA in terms of accuracy, quality, and 
reliability, with statistical significance. Readability assessments showed that 
both LLMs required college-level understanding.
CONCLUSION: While LLMs show great promise, current limitations in accuracy and 
guideline adherence mean they should supplement, not replace, clinical 
expertise.
CLINICAL TRIAL REGISTRATION: This study does not involve clinical trials; 
therefore, no clinical trial registration is required.
WHAT IS KNOWN: •FMF is the most common hereditary autoinflammatory disease. LLMs 
are increasingly used to provide clinical information.
WHAT IS NEW: •This is the first study to assess two different LLMs in the 
context of FMF, evaluating their reliability, quality, and alignment with 
clinical guidelines. ChatGPT-4o outperformed LLaMA-3.1 in reliability,quality, 
and guideline alignment for FMF. However, both models showed informational gaps 
that may limit their clinical use.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00431-025-06318-y
PMID: 40679644 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval: Ethical approval 
was not required, as the study did not involve human or animal data. Consent to 
participate: This study does not involve human participants; therefore, informed 
consent for participation is not applicable. Consent for publication: This study 
does not involve human participants; therefore, informed consent for publication 
is not applicable. Competing interests: The authors declare no competing 
interests.


296. Int Nurs Rev. 2025 Sep;72(3):e70060. doi: 10.1111/inr.70060.

Exploring the Role and Potential of Chatbots in Learning From the Perspective of 
Nursing Students: A Systematic Review of Qualitative Studies.

Zhang S(1), Yang X(1), Zhang L(1), Liu H(1), Chen X(1), Yang X(1), Hu Y(1), Liu 
Q(1), He Y(1).

Author information:
(1)Women and Children's Hospital of Chongqing Medical University, Chongqing, 
China.

AIM: This study explores nursing students' experiences and needs regarding 
chatbot-assisted learning, while evaluating the opportunities and challenges 
they encounter.
BACKGROUND: As artificial intelligence (AI) advances, chatbots are increasingly 
integrated into nursing education, offering real-time feedback and personalized 
support. While chatbots show promise in enhancing theoretical knowledge and 
clinical skills, little is known about students' actual experiences and 
expectations. Gaining insight into students' perspectives is essential for 
optimizing chatbots' implementation in nursing education.
METHODS: This study conducts a systematic search of several electronic 
databases, including PubMed, Web of Science, Cochrane Library, China National 
Knowledge Infrastructure (CNKI), and Wanfang, using search terms related to 
nursing students, chatbots, experiences, perceptions, and qualitative research. 
A thematic synthesis approach is employed to extract and analyze the relevant 
studies.
RESULTS: Nine studies were included in this review. Thematic synthesis revealed 
four major themes: perceived availability, role participation, potential 
challenges, and student needs. Nursing students highlighted chatbots' technical 
and interactive availability-valuing timely responses, ease of use, and 
personalized support. Chatbots were seen as active participants in learning, 
facilitating bidirectional communication, clinical simulation, information 
retrieval, academic assistance, and decision-making. However, concerns were 
raised regarding information accuracy, ethical risks, over-reliance, and 
misalignment with nursing values. Students expressed clear expectations for 
chatbot updates, curriculum integration, and regulatory oversight to ensure safe 
and effective usage.
CONCLUSION: Chatbots not only offer substantial benefits for nursing education 
but also present notable challenges. Students prefer more specialized chatbots 
tailored to their diverse learning needs. They also emphasize the need for 
better regulation and training to improve chatbots' effectiveness.
IMPLICATIONS FOR PRACTICE AND POLICY: Educators should integrate chatbot 
training into curricula to enhance student readiness and ethical use. 
Policymakers must prioritize regulation that ensures data protection, content 
accuracy, and equitable access. Developing nursing-specific chatbot systems and 
institutional guidelines is essential for responsible adoption and sustainable 
innovation.

© 2025 International Council of Nurses.

DOI: 10.1111/inr.70060
PMID: 40677016 [Indexed for MEDLINE]


297. BMC Oral Health. 2025 Jul 17;25(1):1188. doi: 10.1186/s12903-025-06590-2.

Ensemble learning for microbiome-based caries diagnosis: multi-group modeling 
and biological interpretation from salivary and plaque metagenomic data.

Wei F(#)(1), Wu Z(#)(2), Li G(2), Sun X(1), Shi X(1), Tan L(2), Ai T(2), Qu 
L(3), Zheng S(4).

Author information:
(1)Department of Preventive Dentistry, Peking University School and Hospital of 
Stomatology & National Center for Stomatology & National Clinical Research 
Center for Oral Diseases & National Engineering Research Center of Oral 
Biomaterials and Digital Medical Devices, Beijing, PR China.
(2)China Telecom eSurfing Cloud, Dongcheng District, Beijing, PR China.
(3)China Telecom eSurfing Cloud, Dongcheng District, Beijing, PR China. 
qulong@chinatelecom.cn.
(4)Department of Preventive Dentistry, Peking University School and Hospital of 
Stomatology & National Center for Stomatology & National Clinical Research 
Center for Oral Diseases & National Engineering Research Center of Oral 
Biomaterials and Digital Medical Devices, Beijing, PR China. 
kqzsg86@bjmu.edu.cn.
(#)Contributed equally

BACKGROUND: Oral microbiota is a major etiological factor in the development of 
dental caries. Next-generation sequencing techniques have been widely used, 
generating vast amounts of data which is underexplored. The advancement of 
artificial intelligence (AI) technologies has made it possible to mine 
information from these large datasets. This study aimed to develop AI-driven 
diagnostic models and identify key microbial features for caries.
METHODS: We collected raw metagenomic and full-length 16 S rRNA gene sequencing 
data from previous studies on saliva and plaque to construct a caries AI 
training dataset comprising nearly 600 samples. Samples were grouped based on 
age, sequencing and sampling method. Through systematic comparison of seven 
machine learning architectures, including Logistic Regression, Random Forest, 
Support Vector Machines, Gradient Boosting, Convolutional Neural Networks, 
Feedforward Neural Networks, and Transformer models, we developed 
subgroup-specific caries diagnostic models, with subsequent ensemble learning 
integration to enhance generalizability.
RESULTS: The caries diagnostic model achieved a maximum AUC value of 1 (accuracy 
of 100%) for children under 6 years old in both saliva and plaque groups. The 
consistency of top features (species and metabolic pathways) contributing to the 
models was demonstrated through intra- and inter-group analyses. Key 
caries-associated species included Streptococcus salivarius, Streptococcus 
parasanguinis and Veillonella dispar. Veillonella parvula exhibits higher 
abundance in caries plaque samples, while being elevated in healthy saliva 
samples. Metabolic pathways like geranylgeranyl diphosphate and fructan 
biosynthesis were enriched in caries, whereas Bifidobacterium shunt and 
peptidoglycan biosynthesis were depleted.
CONCLUSION: The current work provided reliable diagnostic models for early 
childhood caries, and established a robust computational framework for AI-driven 
microbiome analysis. This study, by focusing on the characteristics of the oral 
microbiome, offers novel perspectives for data mining and validation of existing 
data through the application of AI modelling.

© 2025. The Author(s).

DOI: 10.1186/s12903-025-06590-2
PMCID: PMC12272970
PMID: 40676575 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study involved a secondary analysis of publicly available, 
de-identified metagenomic and full-length 16S rRNA gene sequencing data. 
Therefore, ethical approval from our institutional review board was not required 
for this specific study. We confirm that all original studies from which the 
data were sourced obtained the necessary ethical approvals and informed consent 
from participants prior to data deposition in public repositories. Consent for 
publication: Not applicable. Competing interests: The authors declare no 
competing interests. Clinical trial number: Not applicable.


298. Commun Med (Lond). 2025 Jul 17;5(1):296. doi: 10.1038/s43856-025-01015-1.

Transformer-based structural connectivity networks for ADHD-related connectivity 
alterations.

Shi L(#)(1), Shi L(#)(1), Cui Z(2), Lin C(3), Zhang R(4), Zhang J(4), Zhu Y(4), 
Shi W(4), Wang J(1), Wang Y(1), Wang D(5), Liu H(2), Gao X(6)(7)(8).

Author information:
(1)Department of Radiology, Zhejiang Cancer Hospital, Hangzhou Institute of 
Medicine (HIM), Chinese Academy of Sciences, Hangzhou, China.
(2)Beijing Children's Hospital, Capital Medical University, National Center for 
Children's Health, Beijing, China.
(3)The Second School of Clinical Medicine, Zhejiang Chinese Medical University, 
Hangzhou, China.
(4)Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of 
Science, Suzhou, China.
(5)Department of Neurology, The Second Affiliated Hospital of Soochow 
University, Suzhou, China.
(6)Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of 
Science, Suzhou, China. xingaosam@163.com.
(7)School of Disaster and Emergency Medicine, Tianjin University, Tianjin, 
China. xingaosam@163.com.
(8)Jinan Guoke Medical Engineering and Technology Development Co., Ltd., Jinan, 
China. xingaosam@163.com.
(#)Contributed equally

BACKGROUND: Attention-deficit/hyperactivity disorder (ADHD) is a common 
neurodevelopmental disorder that affects behavior, attention, and learning. 
Current diagnoses rely heavily on subjective assessments, underscoring the need 
for objective imaging-based methods. This study aims to explore whether 
structural connectivity networks derived from MRI can reveal alterations 
associated with ADHD and support data-driven understanding.
METHODS: We collected brain MRI data from 947 individuals (aged 7-26 years; 590 
males, 356 females, 1 unspecified) across eight centers, sourced from the Neuro 
Bureau ADHD-200 preprocessed dataset. Transformer-based deep learning models 
were used to learn relationships between different brain regions and construct 
structural connectivity networks. To prepare input for the model, each region 
was transformed into a standardized data sequence using four different 
strategies. The strength of connectivity between brain regions was then measured 
to identify structural differences related to ADHD. Five-fold cross-validation 
and statistical analyses were used to evaluate model robustness and group 
differences, respectively.
RESULTS: Here we show that the proposed method performs well in distinguishing 
ADHD individuals from healthy controls, with accuracy reaching 71.9 percent and 
an area under curve of 0.74. The structural networks also reveal significant 
differences in connectivity patterns (paired t-test: P = 0.81 × 10-6), 
particularly involving regions responsible for motor and executive function. 
Notably, the importance rankings of several brain regions, including the 
thalamus and caudate, differ markedly between groups.
CONCLUSIONS: This study shows that ADHD may be associated with connectivity 
alterations in multiple brain regions. Our findings suggest that brain 
structural connectivity networks built using Transformer-based methods offer a 
promising tool for both diagnosis and further research into brain structure.

Plain Language Summary: Attention-deficit/hyperactivity disorder (ADHD) is a 
brain condition that affects attention, behavior and learning, especially in 
children. Diagnosing ADHD is currently based on interviews and observations, 
which can be subjective and vary between doctors. This study explored whether 
artificial intelligence models could detect brain structure differences in 
people with ADHD using MRI scans. Researchers used a deep learning method called 
a Transformer to analyze brain images from 947 individuals. The model was able 
to spot patterns that helped distinguish people with ADHD from those without. It 
also highlighted brain areas that may be especially important in ADHD. These 
findings suggest that artificial intelligence could help build more accurate and 
objective tools for ADHD diagnosis, supporting better understanding and care in 
the future.

© 2025. The Author(s).

DOI: 10.1038/s43856-025-01015-1
PMCID: PMC12271470
PMID: 40676171

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


299. PCN Rep. 2025 Jul 15;4(3):e70159. doi: 10.1002/pcn5.70159. eCollection 2025 Sep.

Potential of ChatGPT in youth mental health emergency triage: Comparative 
analysis with clinicians.

Thotapalli S(1), Yilanli M(1)(2), McKay I(1)(2), Leever W(1)(2), Youngstrom 
E(1)(2), Harvey-Nuckles K(1)(2), Lowder K(1)(2), Schweitzer S(2), Sunderland 
E(2), Jackson DI(3), Sezgin E(1)(3).

Author information:
(1)Department of Psychiatry and Behavioral Health The Ohio State University 
Columbus Ohio USA.
(2)Nationwide Children's Hospital Columbus Ohio USA.
(3)The Abigail Wexner Research Institute, Nationwide Children's Hospital 
Columbus Ohio USA.

AIM: Large language models, such as GPT-4, are increasingly integrated into 
healthcare to support clinicians in making informed decisions. Given ChatGPT's 
potential, it is necessary to explore such applications as a support tool, 
particularly within mental health telephone triage services. This study 
evaluates whether GPT Models can accurately triage psychiatric emergency 
vignettes and compares its performance to that of clinicians.
METHODS: A cross-sectional study was performed to assess the performance of 
three different GPT-4 models (GPT-4o, GPT-4o Mini, and GPT-4 Legacy) in 
psychiatric emergency triage. Twenty-two psychiatric emergency vignettes, 
intended to represent realistic prehospital triage scenarios, were initially 
drafted using ChatGPT and subsequently reviewed and refined by the research team 
to ensure clinical accuracy and relevance. The GPT-4 models independently 
generated clinical responses to the vignettes over three iterations to ensure 
consistency. Thereafter, two advanced practice nurse practitioners independently 
assessed these responses utilizing a 3-point Likert-type scale for the main 
triage criteria: risk level (Low = 1 to High = 3), necessity of hospital 
admission (Yes = 1; No = 2), and urgency of clinical evaluation (Low = 1 to 
High = 3). Additionally, the nurse practitioners provided their clinical 
judgments independently for each vignette. Interrater reliability was evaluated 
by comparing responses generated by the GPT Models with the independent clinical 
assessments of nurse practitioners, and agreement was evaluated using Cohen's 
Kappa. The clinical expert committee (n = 3) conducted qualitative analyses of 
responses from both GPT Models using a systematic coding method to evaluate 
triage accuracy, clarity, completeness, and total score. The evaluation of 
responses focused on three key triage criteria: risk (Low = 1 to High = 3), 
admission necessity (Yes = 1; No = 2), and urgency of clinical evaluation 
(Low = 1 to High = 3).
RESULTS: GPT Models had an average admission score of 1.73 (standard deviation 
[SD] = 0.45; scale: Yes = 1, No = 2), indicating a general trend toward 
recommending against hospital admission. Risk (mean = 2.12, SD = 0.83) and 
urgency (mean = 2.27, SD = 0.44) assessments suggested moderate-to-high 
perceived risk and urgency (scale: Low = 1, High = 3), reflecting conservative 
decision-making. Interrater reliability between clinicians and GPT-4 models was 
substantial, with Cohen's Kappa values of 0.77 (admission), 0.78 (risk), and 
0.76 (urgency). GPT Models' responses tended toward slight over-triage, 
indicated by four false-positive admission recommendations and zero false 
negatives. Substantial interrater reliability was observed between clinicians 
and GPT-4 responses across the three triage criteria (Cohen's Kappa: 
admission = 0.77; risk = 0.78; urgency = 0.76).The mean scores for triage 
criteria responses between GPT-4 models and clinicians exhibited consistent 
patterns with minimal variability. Overall, GPT Models had a tendency to 
over-triage patients as indicated by four total false positives and zero false 
negatives for admissions.
CONCLUSION: This study indicates that GPT Models may serve as supportive 
decision-support tools in mental health telephone triage, particularly for 
psychiatric emergencies. Although response variability across iterations was 
minimal, most discrepancies in admission decisions were identified as false 
positives, reflecting that GPT Models may have a tendency to over-triage 
relative to clinician judgment. Further investigation is needed to establish 
robust structure to increase alignment with clinical decisions and response 
relevance in clinical practice.

© 2025 The Author(s). Psychiatry and Clinical Neurosciences Reports published by 
John Wiley & Sons Australia, Ltd on behalf of Japanese Society of Psychiatry and 
Neurology.

DOI: 10.1002/pcn5.70159
PMCID: PMC12264314
PMID: 40673126

Conflict of interest statement: The authors declare no conflicts of interest.


300. J Med Internet Res. 2025 Jul 16;27:e71977. doi: 10.2196/71977.

Menstrual Health Education Using a Specialized Large Language Model in India: 
Development and Evaluation Study of MenstLLaMA.

Adhikary PK(1), Motiyani I(1), Oke G(1), Joshi M(1), Pathak K(1), Singh SM(1), 
Chakraborty T(1)(2).

Author information:
(1)Department of Electrical Engineering, Indian Institute of Technology Delhi, 
Room: 3B-7 (Block III 3rd Floor), Hauz Khas, New Delhi, 110016, India, 91 
26591076 ext 011.
(2)Yardi School of Artificial Intelligence, Indian Institute of Technology 
Delhi, New Delhi, India.

BACKGROUND: The quality and accessibility of menstrual health education (MHE) in 
low- and middle-income countries, including India, remain inadequate due to 
persistent challenges (eg, poverty, social stigma, and gender inequality). While 
community-driven initiatives have sought to raise awareness, artificial 
intelligence offers a scalable and efficient solution for disseminating accurate 
information. However, existing general-purpose large language models (LLMs) are 
often ill-suited for this task, tending to exhibit low accuracy, cultural 
insensitivity, and overly complex responses. To address these limitations, we 
developed MenstLLaMA-a specialized LLM tailored to the Indian context and 
designed to deliver MHE empathetically, supportively, and accessibly.
OBJECTIVE: We aimed to develop and evaluate MenstLLaMA-a specialized LLM 
tailored to deliver accurate, culturally sensitive MHE-and assess its 
effectiveness in comparison to existing general-purpose models.
METHODS: We curated MENST-a novel, domain-specific dataset comprising 23,820 
question-answer pairs aggregated from medical websites, government portals, and 
health education resources. This dataset was systematically annotated with 
metadata capturing age groups, regions, topics, and sociocultural contexts. 
MenstLLaMA was developed by fine-tuning Meta-LLaMA-3-8B-Instruct, using 
parameter-efficient fine-tuning with low-rank adaptation to achieve domain 
alignment while minimizing computational overhead. We benchmarked MenstLLaMA 
against 9 state-of-the-art general-purpose LLMs, including GPT-4o, Claude-3, 
Gemini 1.5 Pro, and Mistral. The evaluation followed a multilayered framework: 
(1) automatic evaluation using standard natural language processing metrics 
(BLEU [Bilingual Evaluation Understudy], METEOR [Metric for Evaluation of 
Translation with Explicit Ordering], ROUGE-L [Recall-Oriented Understudy for 
Gisting Evaluation-Longest Common Subsequence], and BERTScore [Bidirectional 
Encoder Representations from Transformers Score]); (2) evaluation by clinical 
experts (N=18), who rated 200 expert-curated queries for accuracy and 
appropriateness; (3) medical practitioner interaction through the ISHA 
(Intelligent System for Menstrual Health Assistance) interactive chatbot, 
assessing qualitative dimensions (eg, relevance, understandability, preciseness, 
correctness, and context sensitivity); and (4) a user study with volunteer 
participants (N=200), who evaluated MenstLLaMA in 15- to 20-minute randomized 
sessions, rating the system across 7 qualitative user satisfaction metrics.
RESULTS: MenstLLaMA achieved the highest scores in BLEU (0.059) and BERTScore 
(0.911), outperforming GPT-4o (BLEU: 0.052, BERTScore: 0.896) and Claude-3 
(BERTScore: 0.888). Clinical experts preferred MenstLLaMA's responses over 
gold-standard answers in several culturally sensitive cases. In medical 
practitioners' evaluations using the ISHA-the chat interface powered by 
MenstLLaMA-the model scored 3.5 in relevance, 3.6 in understandability, 3.1/5 in 
preciseness, 3.5/5 in correctness, and 4.0/5 in context sensitivity. User 
evaluations indicated even stronger results, with ratings of 4.7/5 for 
understandability, 4.3/5 for relevance, 4.28/5 for preciseness, 4.1/5 for 
correctness, 4.6/5 for tone, 4.2/5 for flow, and 3.9/5 for context sensitivity.
CONCLUSIONS: MenstLLaMA demonstrates exceptional accuracy, empathy, and user 
satisfaction within the domain of MHE, bridging critical gaps left by 
general-purpose LLMs. Its potential for integration into broader health 
education platforms positions it as a transformative tool for menstrual 
well-being. Future research could explore its long-term impact on public 
perception and menstrual hygiene practices, while expanding demographic 
representation, enhancing context sensitivity, and integrating multimodal and 
voice-based interactions to improve accessibility across diverse user groups.

© Prottay Kumar Adhikary, Isha Motiyani, Gayatri Oke, Maithili Joshi, Kanupriya 
Pathak, Salam Michael Singh, Tanmoy Chakraborty. Originally published in the 
Journal of Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/71977
PMCID: PMC12286563
PMID: 40669074 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


301. Front Digit Health. 2025 Jul 1;7:1624786. doi: 10.3389/fdgth.2025.1624786. 
eCollection 2025.

Diagnostic efficacy of large language models in the pediatric emergency 
department: a pilot study.

Del Monte F(#)(1), Barolo R(#)(2), Circhetta M(3), Delmonaco AG(1), Castagno 
E(1), Pivetta E(4), Bergamasco L(3)(5), Franco M(6), Olmo G(3), Bondone C(1).

Author information:
(1)Department of Pediatric Emergency, Regina Margherita Children's 
Hospital-A.O.U. Città Della Salute e Della Scienza di Torino, Turin, Italy.
(2)Department of Public Health and Pediatrics, Postgraduate School of 
Pediatrics, University of Turin, Turin, Italy.
(3)Department of Control and Computer Engineering, Politecnico di Torino, Turin, 
Italy.
(4)Division of Emergency Medicine and High Dependency Unit, Department of 
Medical Sciences, Città Della Salute e Della Scienza di Torino and University of 
Turin, Turin, Italy.
(5)LINKS Foundation, Turin, Italy.
(6)Department of Clinical and Biological Sciences, University of Turin, 
Orbassano, Turin, Italy.
(#)Contributed equally

Erratum in
    Front Digit Health. 2025 Jul 16;7:1658635. doi: 10.3389/fdgth.2025.1658635.

BACKGROUND: The Pediatric Emergency Department (PED) faces significant 
challenges, such as high patient volumes, time-sensitive decisions, and complex 
diagnoses. Large Language Models (LLMs) have the potential to enhance patient 
care; however, their effectiveness in supporting the diagnostic process remains 
uncertain, with studies showing mixed results regarding their impact on clinical 
reasoning. We aimed to assess LLM-based chatbots performance in realistic PED 
scenarios, and to explore their use as diagnosis-making assistants in pediatric 
emergency.
METHODS: We evaluated the diagnostic effectiveness of 5 LLMs (ChatGPT-4o, Gemini 
1.5 Pro, Gemini 1.5 Flash, Llama-3-8B, and ChatGPT-4o mini) compared to 23 
physicians (including 10 PED physicians, 6 PED residents, and 7 Emergency 
Medicine residents). Both LLMs and physicians had to provide one primary 
diagnosis and two differential diagnoses for 80 real-practice pediatric clinical 
cases from the PED of a tertiary care Children's Hospital, with three different 
levels of diagnostic complexity. The responses from both LLMs and physicians 
were compared to the final diagnoses assigned upon patient discharge; two 
independent experts evaluated the answers using a five-level accuracy scale. 
Each physician or LLM received a total score out of 80, based on the sum of all 
answer points.
RESULTS: The best performing chatbots were ChatGPT-4o (score: 72.5) and Gemini 
1.5 Pro (score: 62.75), the first performing better (p < 0.05) than PED 
physicians (score: 61.88). Emergency Medicine residents performed worse (score: 
43.75) than both the other physicians and chatbots (p < 0.01). Chatbots' 
performance was inversely proportional to case difficulty, but ChatGPT-4o 
managed to match the majority of the correct answers even for highly difficult 
cases.
DISCUSSION: ChatGPT-4o and Gemini 1.5 Pro could be a valid tool for ED 
physicians, supporting clinical decision-making without replacing the 
physician's judgment. Shared protocols for effective collaboration between AI 
chatbots and healthcare professionals are needed.

© 2025 Del Monte, Barolo, Circhetta, Delmonaco, Castagno, Pivetta, Bergamasco, 
Franco, Olmo and Bondone.

DOI: 10.3389/fdgth.2025.1624786
PMCID: PMC12259579
PMID: 40666107

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


302. Sci Rep. 2025 Jul 15;15(1):25493. doi: 10.1038/s41598-025-08342-2.

Integrating vision transformer-based deep learning model with kernel extreme 
learning machine for non-invasive diagnosis of neonatal jaundice using 
biomedical images.

Eliazer M(1), Amaran S(1), Sreekumar K(1), Vikram A(2), Joshi GP(3), Cho W(4).

Author information:
(1)Department of Computing Technologies, SRM Institute of Science and 
Technology, Kattankulathur, Chennai, 603203, India.
(2)Department of Computer Science and Engineering, Aditya University, 
Surampalem, Andhra Pradesh, 533437, India.
(3)Department of Electronic and AI System Engineering, Kangwon National 
University, Samcheok, 25913, Republic of Korea. joshi@kangwon.ac.kr.
(4)Department of Electronic and AI System Engineering, Kangwon National 
University, Samcheok, 25913, Republic of Korea. wcho@kangwon.ac.kr.

Birth complications, particularly jaundice, are one of the leading causes of 
adolescent death and disease all over the globe. The main severity of these 
illnesses may diminish if scholars study more about their sources and progress 
toward effective treatment. Assured developments were prepared, but they are 
inadequate. Newborns repeatedly have jaundice as their primary medical concern. 
A raised level of bilirubin is a symbol of jaundice. Generally, in newborns, 
hyperbilirubinemia peaks in the initial post-delivery week. The inability to 
perceive issues early is sufficient for quick treatment, and the resemblance of 
indications might lead to misdiagnosis. Therefore, appropriate technologies are 
instantly required. Nowadays, researchers have begun to implement an 
image-processing model for analyzing jaundice. Paediatricians can detect and 
classify neonatal jaundice with machine learning (ML) and deep learning (DL) 
techniques. This study proposes an Early Diagnosis of Neonatal Jaundice Image 
Classification Using Kernel Extreme Learning Machine (EDNJIC-KELM) approach in 
the Healthcare Sector. The main intention of the EDNJIC-KELM approach is to 
build an effective system for diagnosing neonatal jaundice based on advanced 
methods. Initially, the image pre-processing stage applies the Wiener filtering 
(WF) method to improve the quality of an image and make it more suitable for 
analysis by removing the noise. In addition, the vision transformer (ViT) method 
is employed for the feature extraction process. Furthermore, the EDNJIC-KELM 
method employs the kernel extreme learning machine (KELM) method for the 
jaundice image classification. Finally, the enhanced coati optimization 
algorithm (ECOA) method is implemented for the hyperparameter tuning of the KELM 
method, which results in a higher classification process. The experimental 
analysis of the EDNJIC-KELM technique is examined using the Jaundice Image data. 
The performance validation of the EDNJIC-KELM technique portrayed a superior 
accuracy value of 96.97% over existing models.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-08342-2
PMCID: PMC12264047
PMID: 40665120 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests. Ethical approval: This article contains no 
studies with human participants performed by any authors. Consent to 
participate: Not applicable. Informed consent: Not applicable.


303. Int J Med Inform. 2025 Nov;203:106047. doi: 10.1016/j.ijmedinf.2025.106047. Epub 
2025 Jul 12.

AI in primary care: Comparing ChatGPT and family physicians on patient queries.

İnan M(1), Suvak Ö(2), Aypak C(2).

Author information:
(1)Department of Family Medicine, Ankara Etlik City Hospital, Ankara, Türkiye. 
Electronic address: muhammedinan.md@gmail.com.
(2)Department of Family Medicine, Ankara Etlik City Hospital, Ankara, Türkiye.

OBJECTIVE: The integration of artificial intelligence (AI) in medicine has led 
to growing interest in its applications for primary care. This study evaluates 
and compares the responses of ChatGPT-4o and family physicians to 200 commonly 
asked clinical questions in family medicine.
METHODS: This was a comparative, observational, cross-sectional study was 
conducted using a dataset of 200 primary care-related questions generated 
through literature review and expert validation. Three experienced family 
physicians and ChatGPT-4o independently provided responses. The responses were 
anonymized and randomly assessed by three independent family medicine experts. 
Evaluations were based on Likert scales for appropriateness (1-6), accuracy 
(1-6), comprehensiveness (1-3), and empathy (1-5). Word counts were also 
recorded.
RESULTS: ChatGPT-4o outperformed family physicians across all evaluation metrics 
(p < 0.01). ChatGPT-4o received higher scores for appropriateness (mean 
5.8 ± 0.5 vs. 4.3 ± 1.0), accuracy (5.8 ± 0.5 vs. 4.5 ± 1.1), comprehensiveness 
(2.4 ± 0.6 vs. 1.4 ± 0.7). and empathy (4.8 ± 0.4 vs. 4.0 ± 0.8). The average 
word count of ChatGPT's responses (298.8 ± 82.3 words) was significantly longer 
than that of physicians (106.1 ± 95.0 words). In topic-specific analysis, 
ChatGPT-4o outperformed physicians, except in General Consultation and Child 
Infections (p = 0.07, 0.08 respectively).
CONCLUSION: The findings suggest that ChatGPT-4o has the potential to enhance 
patient education, medical training, and clinical decision support. Future 
research should explore AI's real-world clinical impact, its role in improving 
medical education, and strategies to refine AI-generated responses for 
conciseness and cultural relevance.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijmedinf.2025.106047
PMID: 40664020 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


304. Jpn J Radiol. 2025 Nov;43(11):1788-1802. doi: 10.1007/s11604-025-01832-3. Epub 
2025 Jul 15.

Vision transformer and complex network analysis for autism spectrum disorder 
classification in T1 structural MRI.

Gao X(1), Xu Y(2)(3).

Author information:
(1)School of Mathematics and Statistics, Suzhou University of Technology, 
Changshu, 215500, China.
(2)School of Nuclear Science and Technology, University of South China, Hengyang 
City, 421001, Hunan, China. yxu40@hotmail.com.
(3)GE Healthcare, No. 1 Huatuo Road, Shanghai, 201210, China. yxu40@hotmail.com.

BACKGROUND: Autism spectrum disorder (ASD) affects social interaction, 
communication, and behavior. Early diagnosis is important as it enables timely 
intervention that can significantly improve long-term outcomes, but current 
diagnostic, which rely heavily on behavioral observations and clinical 
interviews, are often subjective and time-consuming. This study introduces an 
AI-based approach that uses T1-weighted structural MRI (sMRI) scans, network 
analysis, and vision transformers to automatically diagnose ASD.
METHODS: sMRI data from 79 ASD patients and 105 healthy controls were obtained 
from the Autism Brain Imaging Data Exchange (ABIDE) database. Complex network 
analysis (CNA) features and ViT (Vision Transformer) features were developed for 
predicting ASD. Five models were developed for each type of features: logistic 
regression, support vector machine (SVM), gradient boosting (GB), K-nearest 
neighbors (KNN), and neural network (NN). 25 models were further developed by 
federating the two sets of 5 models. Model performance was evaluated using 
accuracy, area under the receiver operating characteristic curve (AUC-ROC), 
sensitivity, and specificity via fivefold cross-validation.
RESULTS: The federate model CNA(KNN)-ViT(NN) achieved highest performance, with 
accuracy 0.951 ± 0.067, AUC-ROC 0.980 ± 0.020, sensitivity 0.963 ± 0.050, and 
specificity 0.943 ± 0.047. The performance of the ViT-based models exceeds that 
of the complex network-based models on 80% of the performance metrics. By 
federating CNA models, the ViT models can achieve better performance.
CONCLUSION: This study demonstrates the feasibility of using CNA and ViT models 
for the automated diagnosis of ASD. The proposed CNA(KNN)-ViT(NN) model achieved 
better accuracy in ASD classification based solely on T1 sMRI images. The 
proposed method's reliance on widely available T1 sMRI scans highlights its 
potential for integration into routine clinical examinations, facilitating more 
efficient and accessible ASD screening.

© 2025. The Author(s) under exclusive licence to Japan Radiological Society.

DOI: 10.1007/s11604-025-01832-3
PMID: 40663220 [Indexed for MEDLINE]


305. Bioinformatics. 2025 Jul 1;41(Supplement_1):i21-i29. doi: 
10.1093/bioinformatics/btaf196.

Automated assignment grading with large language models: insights from a 
bioinformatics course.

Poličar PG(1), Špendl M(1), Curk T(1), Zupan B(1)(2).

Author information:
(1)Faculty of Computer and Information Science, University of Ljubljana, Večna 
pot 113, 1000 Ljubljana, Slovenia.
(2)Department of Education, Innovation and Technology, Baylor College of 
Medicine, 1 Baylor Plz, Houston, TX 77030, United States.

MOTIVATION: Providing students with individualized feedback through assignments 
is a cornerstone of education that supports their learning and development. 
Studies have shown that timely, high-quality feedback plays a critical role in 
improving learning outcomes. However, providing personalized feedback on a large 
scale in classes with large numbers of students is often impractical due to the 
significant time and effort required. Recent advances in natural language 
processing and large language models (LLMs) offer a promising solution by 
enabling the efficient delivery of personalized feedback. These technologies can 
reduce the workload of course staff while improving student satisfaction and 
learning outcomes. Their successful implementation, however, requires thorough 
evaluation and validation in real classrooms.
RESULTS: We present the results of a practical evaluation of LLM-based graders 
for written assignments in the 2024/25 iteration of the Introduction to 
Bioinformatics course at the University of Ljubljana. Over the course of the 
semester, more than 100 students answered 36 text-based questions, most of which 
were automatically graded using LLMs. In a blind study, students received 
feedback from both LLMs and human teaching assistants (TAs) without knowing the 
source, and later rated the quality of the feedback. We conducted a systematic 
evaluation of six commercial and open-source LLMs and compared their grading 
performance with human TAs. Our results show that with well-designed prompts, 
LLMs can achieve grading accuracy and feedback quality comparable to human 
graders. Our results also suggest that open-source LLMs perform as well as 
commercial LLMs, allowing schools to implement their own grading systems while 
maintaining privacy.

© The Author(s) 2025. Published by Oxford University Press.

DOI: 10.1093/bioinformatics/btaf196
PMCID: PMC12261420
PMID: 40662811 [Indexed for MEDLINE]


306. Worldviews Evid Based Nurs. 2025 Aug;22(4):e70059. doi: 10.1111/wvn.70059.

Chatbot-Delivered Interventions for Improving Mental Health Among Young People: 
A Systematic Review and Meta-Analysis.

Li J(1), Li Y(1), Hu Y(1), Ma DCF(1), Mei X(1), Chan EA(1), Yorke J(1).

Author information:
(1)School of Nursing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong.

BACKGROUND: The characteristics, application, and effectiveness of chatbots in 
improving the mental health of young people have yet to be confirmed through 
systematic review and meta-analysis.
AIM: This systematic review aims to evaluate the effectiveness of 
chatbot-delivered interventions for improving mental health among young people, 
identify factors influencing effectiveness, and examine feasibility and 
acceptability.
METHODS: To identify eligible interventional studies, we systematically searched 
11 databases and search engines covering a publication period of January 2014 to 
September 2024. Meta-analyses and subgroup analyses were performed on randomized 
controlled trials to investigate the effectiveness of chatbot-delivered 
interventions and potential influencing factors. Narrative syntheses were 
conducted to summarize the feasibility and acceptability of these interventions 
in all the included studies.
RESULTS: We identified 29 eligible interventional studies, 13 of which were 
randomized controlled trials. The meta-analysis indicated that chatbot-delivered 
interventions significantly reduced distress (Hedge's g = -0.28, 95% CI [-0.46, 
-0.10]), but did not have a significant effect on psychological well-being 
(Hedge's g = 0.13, 95% CI [-0.16, 0.41]). The observed treatment effects were 
influenced by factors including sample type, delivery platform, interaction 
mode, and response generation approach. Overall, this review demonstrates that 
chatbot-delivered interventions were feasible and acceptable.
LINKING EVIDENCE TO ACTION: This review demonstrated that chatbot-delivered 
interventions had positive effects on psychological distress among young people. 
Chatbot-delivered interventions have the potential to supplement existing mental 
health services provided by multidisciplinary healthcare professionals. Future 
recommendations include using instant messenger platforms for delivery, 
enhancing chatbots with multiple communication methods to improve interaction 
quality, and refining language processing, accuracy, privacy, and security 
measures.

© 2025 The Author(s). Worldviews on Evidence‐Based Nursing published by Wiley 
Periodicals LLC on behalf of Sigma Theta Tau International.

DOI: 10.1111/wvn.70059
PMCID: PMC12261465
PMID: 40662463 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


307. Med Phys. 2025 Jul;52(7):e17918. doi: 10.1002/mp.17918.

Denoising pediatric cardiac photon-counting CT data with sparse coding and 
data-adaptive, self-supervised deep learning.

Clark DP(1)(2), Cao JY(3), Badea CT(1).

Author information:
(1)Quantitative Imaging and Analysis Lab, Department of Radiology, Duke 
University, Durham, North Carolina, USA.
(2)Center for Virtual Imaging Trials, Carl E. Ravin Advanced Imaging 
Laboratories, Department of Radiology, Duke University, Durham, North Carolina, 
USA.
(3)Department of Radiology, Division of Pediatric Radiology, Duke University, 
Durham, North Carolina, USA.

BACKGROUND: The judicious use of CT in pediatric cardiac applications is 
warranted because young patients face the need for repeated imaging and 
increased lifetime cancer risk after ionizing radiation exposure. The quality of 
pediatric cardiac CT scans is variable because of limited protocols 
optimizations for pediatric patients, the common presence of metallic implants 
following treatment, and disparities in denoising algorithm performance between 
adult and pediatric scans. Two recent technological developments promise to 
improve the average quality of pediatric CT scans at fixed or reduced dose: 
clinical photon-counting CT (PCCT) and deep learning (DL) algorithms for CT 
image denoising. Given advancements to accommodate variable image quality, these 
technologies will deliver improved spatial resolution, noise performance, and 
contrast resolution for pediatric cardiac CT imaging.
PURPOSE: To advance self-supervised DL denoising methods to accommodate variable 
image quality in pediatric cardiac CT data.
METHODS: Starting with the popular Vision Transformer (ViT) DL architecture, two 
targeted architectural changes were made: (1) the multi-layer perceptrons (MLPs) 
were modified to allow cross-token recombination of encoded image data following 
attention computations (parallels patch-wise weighting and averaging in 
non-local means [NLM]), and (2) the network head was replaced with the 
equivalent of an overcomplete dictionary to perform dictionary sparse coding 
(SC). This modified, 3D ViT (mViT) was then trained in a dynamic fashion: the 
balance between data fidelity and representation sparsity was adjusted during 
training such that the average fidelity error remained consistent with localized 
estimates of image noise. To demonstrate the newly proposed method, the mViT was 
trained with pediatric cardiac photon-counting x-ray CT data with variable 
levels of image noise (NAEOTOM Alpha PCCT scanner; retrospective data from 20 
patients scanned at Duke University; ages: 1-18 years; iterative reconstruction 
noise level in the left ventricle: 20-55 HU). Data from one patient with the 
highest levels of noise was reserved for validation. Testing data included Alpha 
data from three additional Duke patients (2 < 1 year old) and a murine cardiac 
PCCT data set acquired on a preclinical system.
RESULTS: The validation denoising results demonstrate that SC with the mViT 
preserves anatomic structures relevant to the diagnosis and treatment of 
congenital heart defects (coronary artery origins; valve leaflets; left 
ventricle boundaries) while achieving similar intensity bias and lower intensity 
variance values than competing denoising methods (bilateral filtration [BF], 
NLM, dictionary SC, block matching 4D, orthogonal matching pursuit, Noise2Void). 
Applying the trained mViT network to preclinical PCCT demonstrated robust 
generalization performance to high levels of image noise (∼230 HU) and differing 
image contrast; however, applying the network to clinical PCCT data in younger 
patients (< 1 year old) demonstrated some smoothing of image details in data 
already heavily denoised during reconstruction.
CONCLUSIONS: This work demonstrates robust, self-supervised denoising of 
pediatric cardiac PCCT data through data adaptation during network training 
based on local noise estimates. The trained network generalizes to data sets 
with high levels of noise and differing image contrast relative to the training 
data, suggesting that self-supervised fine tuning may allow the trained network 
to address related CT denoising problems.

© 2025 American Association of Physicists in Medicine.

DOI: 10.1002/mp.17918
PMCID: PMC12416440
PMID: 40660927 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest Statement: Joseph Cao has 
received speaker honoraria from Siemens Healthineers.


308. BMJ Open. 2025 Jul 13;15(7):e098734. doi: 10.1136/bmjopen-2025-098734.

Chatbot-delivered structured psychological intervention (SPI-Bot) for teenagers 
with adolescent idiopathic scoliosis in Hong Kong: protocol for a pilot 
randomised controlled trial.

Li J(1), Zhang T(2), Cheung JPY(2), Meng N(2), Wu Z(2), Ma DCF(1), Chan EA(1), 
Yu L(3), Cao Y(4), Li Y(5).

Author information:
(1)School of Nursing, Hong Kong Polytechnic University, Hong Kong, People's 
Republic of China.
(2)Department of Orthopaedics and Traumatology, The University of Hong Kong, 
Hong Kong, People's Republic of China.
(3)Department of Applied Social Sciences, The Hong Kong Polytechnic University, 
Hong Kong, People's Republic of China.
(4)Department of Social Work, The University of Hong Kong, Hong Kong, People's 
Republic of China.
(5)School of Nursing, Hong Kong Polytechnic University, Hong Kong, People's 
Republic of China Yan-nursing.li@polyu.edu.hk.

INTRODUCTION: Adolescent idiopathic scoliosis (AIS) often imposes a significant 
psychological burden on teenagers. Cognitive and behavioural strategies have 
demonstrated the potential to alleviate these challenges. Chatbots, with their 
real-time interaction capabilities, provide a promising and accessible approach 
to delivering psychological interventions for young people.
METHOD AND ANALYSIS: This pilot trial will evaluate a chatbot-delivered, 
structured psychological intervention (SPI-Bot) incorporating cognitive and 
behavioural strategies for teenagers with AIS. Conducted as a single-centre, 
parallel-group randomised controlled trial, the study aims to assess the 
feasibility, acceptability and preliminary effectiveness of SPI-Bot. Fifty-two 
teenagers with AIS and mild to moderate psychological symptoms will be 
block-randomised into two groups. The intervention group will complete an 
8-week, evidence-based SPI-Bot programme, while the control group will engage in 
casual conversations with another chatbot that does not include specific 
psychological health content. Assessments will be conducted at baseline, 
postintervention (8 weeks) and follow-up (12 weeks). Primary outcomes include 
feasibility and acceptability, measured through recruitment rates, adherence 
rates, attrition rates, engagement, working alliance, usability, user experience 
and adverse events. Secondary outcomes focus on effectiveness, including 
psychological distress, psychological well-being, perceived social support and 
quality of life. Participants in the intervention group will be purposively 
sampled for semistructured interviews to explore their perceptions of the 
intervention process.
ETHICS AND DISSEMINATION: This study has been approved by the Institutional 
Review Board of the Hong Kong Polytechnic University (Reference: 
HSEARS20240919007). The results of this pilot study will be disseminated through 
peer-reviewed journals and conference presentations.
TRIAL REGISTRATION NUMBER: NCT06698952.

© Author(s) (or their employer(s)) 2025. Re-use permitted under CC BY-NC. No 
commercial re-use. See rights and permissions. Published by BMJ Group.

DOI: 10.1136/bmjopen-2025-098734
PMCID: PMC12258280
PMID: 40659392 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: None declared.


309. IEEE Open J Eng Med Biol. 2025 Mar 5;6:459-464. doi: 10.1109/OJEMB.2025.3548002. 
eCollection 2025.

MVBNSleepNet: A Multi-View Brain Network-Based Convolutional Neural Network for 
Neonatal Sleep Staging.

Zhou L(1), Liu M(1), Hu X(2), Wang L(3), Xu Y(3), Chen C(4)(2), Chen W(5).

Author information:
(1)School of Information Science and TechnologyFudan University Shanghai 200433 
China.
(2)Human Phenome InstituteFudan University Shanghai 201203 China.
(3)Department of NeonatologyChildren's Hospital of Fudan University, National 
Children's Medical Center Shanghai 201102 China.
(4)Center for Medical Research and InnovationShanghai Pudong Hospital Shanghai 
201203 China.
(5)School of Biomedical EngineeringUniversity of Sydney Sydney NSW 2006 
Australia.

Goal: To develop a high-performance and robust solution for neonatal sleep 
staging that incorporates spatial topological information and functional 
connectivity of the brain, which are often overlooked in existing approaches. 
Methods: We propose MVBNSleepNet, a multi-view brain network-based convolutional 
neural network. The framework integrates a multi-view brain network (MVBN) to 
characterize brain functional connectivity from linear temporal correlation, 
information-theoretic, and phase-dynamics perspectives, providing comprehensive 
spatial topological information. A masking mechanism is employed to enhance 
model robustness by simulating random dropout or low-quality signal conditions. 
Additionally, an attention mechanism focuses on key regions of the brain network 
and reveals structural brain connectivity during sleep, while a CNN module 
extracts spatial features from brain networks and classifies them into specific 
sleep stages. The model was validated on a clinical dataset of 64 neonatal EEG 
recordings using a leave-one-subject-out validation strategy. Results: 
MVBNSleepNet achieved an accuracy of 83.9% in the two-stage sleep task (sleep 
and wakefulness) and 76.4% in the three-stage task (active sleep, quiet sleep, 
and wakefulness), outperforming state-of-the-art methods. Conclusions: The 
proposed MVBNSleepNet provides a robust and accurate solution for neonatal sleep 
staging and offers valuable insights into the functional connectivity of the 
early neural system.

© 2025 The Authors.

DOI: 10.1109/OJEMB.2025.3548002
PMCID: PMC12250969
PMID: 40657057

Conflict of interest statement: The authors declare that there are no conflicts 
of interest associated with this study or its publication.


310. Digit Health. 2025 Jul 10;11:20552076251353731. doi: 10.1177/20552076251353731. 
eCollection 2025 Jan-Dec.

The effect of chatbot-based exercise interventions on physical activity, 
exercise habits, and sedentary behavior: A systematic review and meta-analysis 
of randomized controlled trials.

Wang Q(1), Yang W(2), Xu K(3), Lam LT(1)(4)(5).

Author information:
(1)Faculty of Medicine, Macau University of Science and Technology, Macau SAR, 
China.
(2)Jiangsu Research Institute of Sports Science, Nanjing, China.
(3)Nanjing Sport Institute, Nanjing, China.
(4)Faculty of Medicine and Health, The University of Sydney, Sydney, Australia.
(5)Faculty of Health, University of Technology Sydney, Sydney, Australia.

OBJECTIVES: Chatbots, transcending the limitations of space and time while 
reducing user resistance, offer a personalized and autonomous approach 
potentially enhancing digital health interventions. This systematic review aims 
to quantitatively evaluate the effectiveness of chatbot-based exercise 
interventions (EIs) in randomized controlled trials (RCTs), emphasizing physical 
activity (PA), exercise habits (EH), and sedentary behavior (SB).
METHODS: A comprehensive search was carried out across Embase, Web of Science, 
PubMed, and Cochrane databases for studies published from January 2010 to 
October 2024. The characteristics of the included studies and the employed 
chatbots were analyzed. A meta-analysis was performed to synthesize the impact 
of chatbot-based EIs on PA, EH, and SB. We carried out subgroup analyses (SAs) 
for investigating potential modifying effects.
RESULTS: This review includes 12 studies conducted between 2013 and 2024, 
encompassing 2446 participants and 240 parent-child dyads, with participant ages 
ranging from 8 to 71 years. The chatbot-based EIs significantly enhanced PA 
compared to control groups (SMD = 0.20, 95% CI = 0.04-0.37, p = 0.02; I² = 51%, 
p = 0.04). However, these interventions did not significantly impact EH 
(SMD = 0.29, 95% CI = -0.48-1.06, p = 0.46) or reduce SB (SMD = 0.32, 95% 
CI = -0.54-1.19, p = 0.46). Notably, SAs revealed a small but significant 
short-term increase in EH (SMD = 0.29, 95% CI = 0.08-0.50, p = 0.006).
CONCLUSION: Chatbot-based EIs appear promising in improving PA, particularly 
moderate to vigorous PA (MVPA). Nonetheless, further RCTs with diverse chatbot 
designs, larger sample sizes, low-resource settings, objective measures of SB 
(e.g., accelerometers), and extended follow-ups are essential to corroborate 
these findings. Future research should also explore the adaptation of chatbots 
to various socioeconomic and cultural contexts and the integration of 
technologies like ChatGPT into EIs.Trial Registration: Meta-analysis PROSPERO: 
CRD42024609852.

© The Author(s) 2025.

DOI: 10.1177/20552076251353731
PMCID: PMC12254675
PMID: 40656859

Conflict of interest statement: The authors declared no potential conflicts of 
interest with respect to the research, authorship, and/or publication of this 
article.


311. Cureus. 2025 Jun 11;17(6):e85767. doi: 10.7759/cureus.85767. eCollection 2025 
Jun.

ChatGPT as a Learning Tool for Medical Students: Results From a Randomized 
Controlled Trial.

Kalam KA(1), Masoud FD(1), Muntaser A(2), Ranga R(1), Geng X(3), Goyal M(4).

Author information:
(1)Department of Medicine, Georgetown University School of Medicine, Washington, 
DC, USA.
(2)Department of Emergency Medicine, Wayne State University, Detroit, USA.
(3)Department of Biostatistics, Bioinformatics and Biomathematics, Georgetown 
University, Washington, DC, USA.
(4)Department of Emergency Medicine, MedStar Washington Hospital Center, 
Washington, DC, USA.

IMPORTANCE: As artificial intelligence (AI) tools like ChatGPT become 
increasingly widespread in medical education, it is essential to evaluate the 
effectiveness of AI tools in enhancing students' academic performance and 
retention compared to traditional educational resources such as lecture 
materials and textbooks.
OBJECTIVE: We aim to determine whether ChatGPT-4.0 improves medical students' 
short-term academic performance compared to internal institutional resources and 
publicly available online materials.
DESIGN: This study was a single-center, prospective, randomized controlled trial 
conducted over a two-week period in April 2025.
SETTING: The research took place at Georgetown University School of Medicine.
PARTICIPANTS: A total of 198 first-year medical students were invited to 
participate, with 33 students enrolling in the study. Participants (N = 33) 
were assigned to one of three groups: Group A (ChatGPT-4.0), Group B (external 
resources, including publicly available online materials such as Google, PubMed, 
and third-party educational websites, but excluding AI-assisted tools), and 
Group C (institutional resources such as electronic textbooks and lecture 
materials).
INTERVENTIONS:  Individuals (N = 33) were randomly assigned to one of three 
groups: Group A (ChatGPT-4.0; N = 10, 30.3%), Group B (external resources; N = 
12, 36.4%), and Group C (institutional resources; N = 11, 33.3%). Participants 
completed an initial multiple-choice quiz using their assigned resources, 
followed by a post-quiz survey. One week later, they retook the same quiz 
without access to any resources to assess retention. Primary outcome(s) and 
measure(s): The primary outcome was the initial quiz score. The secondary 
outcome was the retention score, evaluated through the second quiz without 
resources.
RESULTS: Initial quiz scores (Week 1) were significantly higher in Group A (N = 
10, mean = 9.60 ± 0.52) and Group B (N = 12, mean = 9.08 ± 0.79) compared to 
Group C (N = 11, mean = 6.64 ± 1.57) (p < 0.001). However, retention scores one 
week later (Week 2) showed no significant differences among the groups: Group A 
(N = 10, mean = 6.20 ± 1.93), Group B (N = 12, mean = 5.58 ± 2.07), and Group C 
(N = 11, mean = 4.36 ± 2.01) (p = 0.118).
CONCLUSIONS AND RELEVANCE: ChatGPT-4.0 improves short-term academic performance 
but does not provide a short-term retention advantage over institutional or 
external online educational resources. These findings demonstrate the potential 
of AI tools to enhance short-term learning outcomes while emphasizing the need 
for further research to evaluate their long-term effectiveness in educational 
settings.

Copyright © 2025, Kalam et al.

DOI: 10.7759/cureus.85767
PMCID: PMC12248138
PMID: 40656250

Conflict of interest statement: Human subjects: Consent for treatment and open 
access publication was obtained or waived by all participants in this study. 
Georgetown University Institutional Review Board issued approval N/A. This study 
was reviewed and granted exemption by the Georgetown University Institutional 
Review Board (IRB ID: STUDY00007141) on February 1, 2024. The study qualified 
for exemption under Category (2)(ii), which applies to research involving the 
use of educational tests, survey procedures, interview procedures, or 
observation of public behavior involving minimal risk. Animal subjects: All 
authors have confirmed that this study did not involve animal subjects or 
tissue. Conflicts of interest: In compliance with the ICMJE uniform disclosure 
form, all authors declare the following: Payment/services info: All authors have 
declared that no financial support was received from any organization for the 
submitted work. Financial relationships: All authors have declared that they 
have no financial relationships at present or within the previous three years 
with any organizations that might have an interest in the submitted work. Other 
relationships: All authors have declared that there are no other relationships 
or activities that could appear to have influenced the submitted work.


312. Eur J Pediatr. 2025 Jul 12;184(8):478. doi: 10.1007/s00431-025-06320-4.

Leveraging ChatGPT to strengthen pediatric healthcare systems: a systematic 
review.

Douma H(1), McNamara C(2), Bakola M(3), Stuckler D(4).

Author information:
(1)University of Texas, Austin, TX, USA. hugodouma@utexas.edu.
(2)Newcastle University, England, UK.
(3)University of Patras, Patras, Greece.
(4)Bocconi University, Milan, Italy.

This systematic review is the first to investigate ChatGPT's applications in 
pediatric healthcare systems by assessing its accuracy and readability, with a 
focus on its impact across key areas such as clinical decision-making, clinical 
documentation, patient education, and training. The primary question guiding 
this review is: How does ChatGPT impact pediatric healthcare systems, for 
example, in terms of improving patient education, enhancing providers' 
efficiency, and assisting with clinical decision-making? A systematic review was 
conducted using PubMed, EMBASE, and Web of Science (February 16, 2025). 
Inclusion criteria encompassed peer-reviewed studies evaluating ChatGPT in 
pediatric healthcare (ages 0-18 and guardians). Of 475 screened articles, 58 met 
eligibility criteria. Two independent reviewers extracted data on study 
characteristics, intervention types, outcomes, and results. ChatGPT's primary 
applications were patient education (n = 38), clinical decision-making (n = 12), 
and clinical documentation (n = 5). Accuracy was highest in patient education, 
where it generated educational materials and answered FAQs, though readability 
was often at a high school level, necessitating adaptation. Clinical 
documentation benefits included improved efficiency in drafting notes and 
discharge instructions. However, clinical decision-making and training (n = 3) 
showed mixed accuracy, particularly in management recommendations and patient 
care plans.
CONCLUSION: ChatGPT demonstrates potential in enhancing physician efficiency and 
tailoring patient education in pediatric healthcare. However, most studies 
relied on observational designs, with only one quasi-experimental study. Further 
experimental research is required to evaluate AI's impact on pediatric care 
system effectiveness and patient outcomes.
WHAT IS KNOWN: • AI has been increasingly integrated into pediatric healthcare, 
particularly in imaging, diagnostics, and decision support. • Prior studies have 
explored LLMs like ChatGPT in medical education and training.
WHAT IS NEW: • This is the first systematic review assessing ChatGPT's broader 
applications in pediatric healthcare, including decision-making, patient 
education, clinical documentation, and training. • ChatGPT shows high accuracy 
in patient education and documentation but variable performance in 
decision-making and training, emphasizing the need for medical supervision.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00431-025-06320-4
PMID: 40650728 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare no competing interests.


313. Sensors (Basel). 2025 Jun 24;25(13):3928. doi: 10.3390/s25133928.

MAK-Net: A Multi-Scale Attentive Kolmogorov-Arnold Network with BiGRU for 
Imbalanced ECG Arrhythmia Classification.

Zhao C(1), Lai B(1), Xu Y(2)(3), Wang Y(2)(3), Dong H(4)(5).

Author information:
(1)Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), 
Shenzhen 518107, China.
(2)Key Laboratory of Optoelectronic Devices and Systems of Ministry of Education 
and Guangdong Province, College of Physics and Optoelectronic Engineering, 
Shenzhen University, Shenzhen 518060, China.
(3)Shenzhen Key Laboratory of Photonic Devices and Sensing Systems for Internet 
of Things, Guangdong and Hong Kong Joint Research Centre for Optical Fiber 
Sensors, Shenzhen University, Shenzhen 518060, China.
(4)Individualized Interdisciplinary Program, The Hong Kong University of Science 
and Technology, Hong Kong, China.
(5)System Hub, The Hong Kong University of Science and Technology (Guangzhou), 
Guangzhou 511400, China.

Accurate classification of electrocardiogram (ECG) signals is vital for reliable 
arrhythmia diagnosis and informed clinical decision-making, yet real-world 
datasets often suffer severe class imbalance that degrades recall and F1-score. 
To address these limitations, we introduce MAK-Net, a hybrid deep learning 
framework that combines: (1) a four-branch multiscale convolutional module for 
comprehensive feature extraction across diverse waveform morphologies; (2) an 
efficient channel attention mechanism for adaptive weighting of clinically 
salient segments; (3) bidirectional gated recurrent units (BiGRU) to capture 
long-range temporal dependencies; and (4) Kolmogorov-Arnold Network (KAN) layers 
with learnable spline activations for enhanced nonlinear representation and 
interpretability. We further mitigate imbalance by synergistically applying 
focal loss and the Synthetic Minority Oversampling Technique (SMOTE). On the 
MIT-BIH arrhythmia database, MAK-Net attains state-of-the-art performance-0.9980 
accuracy, 0.9888 F1-score, 0.9871 recall, 0.9905 precision, and 0.9991 
specificity-demonstrating superior robustness to imbalanced classes compared 
with existing methods. These findings validate the efficacy of multiscale 
feature fusion, attention-guided learning, and KAN-based nonlinear mapping for 
automated, clinically reliable arrhythmia detection.

DOI: 10.3390/s25133928
PMCID: PMC12252100
PMID: 40648186 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


314. J Med Internet Res. 2025 Jul 11;27:e71916. doi: 10.2196/71916.

Implementing Large Language Models in Health Care: Clinician-Focused Review With 
Interactive Guideline.

Li H(1)(2), Fu JF(3)(4)(5), Python A(1)(6)(7).

Author information:
(1)Center for Data Science, Zhejiang University, Hangzhou, China.
(2)School of Mathematical Sciences, Zhejiang University, Hangzhou, China.
(3)School of Medicine, Children's Hospital of Zhejiang University, Hangzhou, 
China.
(4)National Clinical Research Center for Child Health, Hangzhou, China.
(5)National Regional Center for Children's Health, Hangzhou, China.
(6)School of Medicine, Zhejiang University, Hangzhou, China.
(7)Centre for Human Genetics, Nuffield Department of Medicine, University of 
Oxford, Oxford, United Kingdom.

BACKGROUND: Large language models (LLMs) can generate outputs understandable by 
humans, such as answers to medical questions and radiology reports. With the 
rapid development of LLMs, clinicians face a growing challenge in determining 
the most suitable algorithms to support their work.
OBJECTIVE: We aimed to provide clinicians and other health care practitioners 
with systematic guidance in selecting an LLM that is relevant and appropriate to 
their needs and facilitate the integration process of LLMs in health care.
METHODS: We conducted a literature search of full-text publications in English 
on clinical applications of LLMs published between January 1, 2022, and March 
31, 2025, on PubMed, ScienceDirect, Scopus, and IEEE Xplore. We excluded papers 
from journals below a set citation threshold, as well as papers that did not 
focus on LLMs, were not research based, or did not involve clinical 
applications. We also conducted a literature search on arXiv within the same 
investigated period and included papers on the clinical applications of 
innovative multimodal LLMs. This led to a total of 270 studies.
RESULTS: We collected 330 LLMs and recorded their application frequency in 
clinical tasks and frequency of best performance in their context. On the basis 
of a 5-stage clinical workflow, we found that stages 2, 3, and 4 are key stages 
in the clinical workflow, involving numerous clinical subtasks and LLMs. 
However, the diversity of LLMs that may perform optimally in each context 
remains limited. GPT-3.5 and GPT-4 were the most versatile models in the 5-stage 
clinical workflow, applied to 52% (29/56) and 71% (40/56) of the clinical 
subtasks, respectively, and they performed best in 29% (16/56) and 54% (30/56) 
of the clinical subtasks, respectively. General-purpose LLMs may not perform 
well in specialized areas as they often require lightweight prompt engineering 
methods or fine-tuning techniques based on specific datasets to improve model 
performance. Most LLMs with multimodal abilities are closed-source models and, 
therefore, lack of transparency, model customization, and fine-tuning for 
specific clinical tasks and may also pose challenges regarding data protection 
and privacy, which are common requirements in clinical settings.
CONCLUSIONS: In this review, we found that LLMs may help clinicians in a variety 
of clinical tasks. However, we did not find evidence of generalist clinical LLMs 
successfully applicable to a wide range of clinical tasks. Therefore, their 
clinical deployment remains challenging. On the basis of this review, we propose 
an interactive online guideline for clinicians to select suitable LLMs by 
clinical task. With a clinical perspective and free of unnecessary technical 
jargon, this guideline may be used as a reference to successfully apply LLMs in 
clinical settings.

©HongYi Li, Jun-Fen Fu, Andre Python. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org), 11.07.2025.

DOI: 10.2196/71916
PMCID: PMC12299950
PMID: 40644686 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


315. Acta Psychol (Amst). 2025 Aug;258:105270. doi: 10.1016/j.actpsy.2025.105270. 
Epub 2025 Jul 9.

Large language models' knowledge of children's memory and suggestibility: 
Evaluating model predictions of prior experimental results.

Santtila P(1), Sun Y(2), Kask K(3), Järvilehto L(4), Xiu J(5).

Author information:
(1)New York University Shanghai, Shanghai, China; Shanghai Frontiers Science 
Center of Artificial Intelligence and Deep Learning, New York University 
Shanghai, Shanghai, China.
(2)New York University Shanghai, Shanghai, China; The School of Psychology and 
Cognitive Science, East China Normal University, Shanghai, China. Electronic 
address: ys6261@nyu.edu.
(3)School of Natural Sciences and Health, Tallinn University, Estonia.
(4)Åbo Akademi University, Turku, Finland; Forensic Psychology Center for 
Children and Adolescents, Helsinki University.
(5)Zhengzhou University, Henan, China.

BACKGROUND: Accurately predicting children's memory and suggestibility in 
forensic contexts, such as child sexual abuse (CSA) investigations, remains 
challenging for human professionals. Large Language Model (LLM), as an advanced 
natural language processing tool, has demonstrated excellent capabilities in 
both medical decision-making and assisted investigation.
OBJECTIVE: The present benchmark study evaluated the responses of four 
LLMs-GPT-4, Claude 3.5 Sonnet, Llama-3-405b, and Erniebot-in predicting 
children's performance in memory and suggestibility studies, comparing their 
accuracy to human professionals.
SETTING: Four state-of-the-art LLMs were tested in a controlled setting, 
generating 4800 predictions across 30 questions from eight published studies.
METHODS: The models were assessed on their prediction accuracy, with overall and 
model-specific performance analyzed using chi-square tests.
RESULTS: LLMs achieved a 65 % accuracy rate, outperforming the 57 % accuracy of 
human professionals in prior studies. Claude 3.5 Sonnet, GPT-4, and Llama-3-405b 
outperformed Erniebot, particularly excelling in tasks related to false memory 
implantation.
CONCLUSIONS: LLMs show potential in supporting CSA interviews by providing 
accurate predictions of children's behavior, although further research is needed 
to validate their use in real-world forensic settings.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.actpsy.2025.105270
PMID: 40639181 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
indicate no conflict of interest in this study.


316. JMIR Med Inform. 2025 Jul 10;13:e60900. doi: 10.2196/60900.

Leveraging AI to Drive Timely Improvements in Patient Experience Feedback: 
Algorithm Validation.

Khanbhai M(1)(2), Carenzo C(1)(2), Aryasinghe S(1)(2), Manton D(2), Mayer 
E(1)(2)(3).

Author information:
(1)iCARE Digital Collaboration Space & Secure Data Environment, NIHR Imperial 
Biomedical Research Centre, 1a Sheldon Square, London, W2 6PY, United Kingdom, 
44 20 7589 5111.
(2)Department of Surgery & Cancer, Faculty of Medicine, Imperial College London, 
London, United Kingdom.
(3)Imperial College Healthcare NHS Trust, London, United Kingdom.

BACKGROUND: Understanding and improving patient care is pivotal for health care 
providers. With increasing volumes of the Friends and Family Test (FFT) data in 
England, manual analysis of this patient feedback poses challenges for many 
health care organizations. This underscores the importance of automated text 
analysis, particularly in predicting sentiments and themes in real time.
OBJECTIVE: Leveraging machine learning and natural language processing, this 
study explores the utility of a supervised algorithm to systematically test and 
refine the algorithm's cross-contextual performance in diverse health care 
settings, addressing variations in population characteristics, geographical 
locations, and care settings, ultimately driving improvements based on patient 
feedback.
METHODS: The text analytics algorithm initially developed in a large acute trust 
in London was further tested in 9 health care organizations with diverse care 
settings across England. These trusts varied in technical capacity and resource, 
population demographics, and FFT free text datasets. Testing and validation of 
the algorithm were performed, including manual coding of a subset of 
retrospective comments. Technical infrastructure, including coding environments 
and packages for algorithm testing and deployment, was optimized. The algorithm 
was iteratively trained using bag of words from anonymized data, tailored to 
accommodate contextual variations, and tested for change in algorithm 
performance while simultaneously rectifying issues identified.
RESULTS: The algorithm demonstrated satisfactory overall accuracy (>75%) in 
predicting themes and sentiments embedded within free-text responses across a 
variety of care settings and population demographics. While the algorithm 
yielded strong and reusable models in relatively stable environments, such as 
adult inpatient care settings, the initial accuracy was notably lower in 
organizations providing services such as pediatrics and mental health. However, 
the accuracy of our algorithm significantly improved when individual trust 
coding templates were applied. Thematic saturation was reached after the fifth 
organization was recruited, and no further coding was required for the last 4 
organizations. Subsequently, a framework and pipeline for deployment of the 
algorithm were developed to provide a standardized approach for implementation 
and analysis of FFT free text, ensuring ease of use.
CONCLUSIONS: This study represents a significant step forward in leveraging 
free-text FFT data for valuable insights in diverse health care settings through 
the testing and development of a robust supervised learning text analytics 
algorithm. The disparity in some care settings was anticipated, given that the 
lexicon and phraseology used was inherently different from those prevalent in 
adult inpatient care (where the algorithm was developed). However, these 
challenges were addressed with further coding and testing. This approach 
enhanced the accuracy and reliability of the algorithm, encouraged inter- and 
intraorganizational collaboration, and shared learning.

© Mustafa Khanbhai, Catalina Carenzo, Sarindi Aryasinghe, Dave Manton, Erik 
Mayer. Originally published in JMIR Medical Informatics 
(https://medinform.jmir.org).

DOI: 10.2196/60900
PMCID: PMC12270031
PMID: 40638776 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


317. NPJ Digit Med. 2025 Jul 9;8(1):421. doi: 10.1038/s41746-025-01833-6.

Transformer optimization with meta learning on pathology images for breast 
cancer lymph node micrometastasis.

Huang J(1), Wang J(2), Shi J(1), Ni H(3)(4), Xu S(5), Wu P(5), Ren Y(5)(6), Bian 
L(4), Su C(1), Xu Y(1), He X(1), Chen X(7)(8), Li J(9)(10)(11).

Author information:
(1)Department of Pathology, Soochow Medical College, Soochow University, Suzhou, 
China.
(2)School of Electronics and Information Engineering, Soochow University, 
Suzhou, China.
(3)Department of Pathology, Children's Hospital, Soochow University, Suzhou, 
China.
(4)Department of Pathology, Sun Yat-sen Memorial Hospital, Sun Yat-sen 
University, Guangzhou, China.
(5)Department of Pathology and Institute of Molecular Pathology, Jiangxi 
Provincial Key Laboratory for Precision Pathology and Intelligent Diagnosis, The 
First Affiliated Hospital, Jiangxi Medical College, Nanchang University, 
Nanchang, China.
(6)Department of Pathology, The Third Affiliated Hospital, Sun Yat-sen 
University, Guangzhou, China.
(7)School of Electronics and Information Engineering, Soochow University, 
Suzhou, China. xjchen@suda.edu.cn.
(8)State Key Laboratory of Radiation Medicine and Protection, Soochow 
University, Suzhou, China. xjchen@suda.edu.cn.
(9)Department of Pathology, Soochow Medical College, Soochow University, Suzhou, 
China. lijming3@sysu.edu.cn.
(10)Department of Pathology, Sun Yat-sen Memorial Hospital, Sun Yat-sen 
University, Guangzhou, China. lijming3@sysu.edu.cn.
(11)The MOE Basic Research and Innovation Center for the Targeted Therapeutics 
of Solid Tumors, Department of Pathology and Institute of Molecular Pathology, 
Jiangxi Provincial Key Laboratory for Precision Pathology and Intelligent 
Diagnosis, The First Affiliated Hospital, Jiangxi Medical College, Nanchang 
University, Nanchang, China. lijming3@sysu.edu.cn.

Lymph node micro-metastasis represents the initial stage of breast cancer spread 
or metastasis. However, the limited size of these hidden lesions restricts 
dataset expansion, presenting a significant challenge for manual examination and 
conventional deep learning techniques. By harnessing the power of meta-learning 
on limited datasets, we developed a novel network named MetaTrans, equipped with 
a 34-category dataset (MT-MCD) to effectively pinpoint micro-metastases in lymph 
nodes from pathological images. MetaTrans demonstrated superior performance on 
two different multi-center datasets and excelled in the 0-shot task for 
intraoperative frozen section diagnosis. Beyond breast cancer, MetaTrans 
efficiently identifies micro-metastases in thyroid and colorectal cancers and 
can be directly applied to recognize images captured by digital cameras under a 
microscope. Across all clinical validation scenarios, our method surpasses 
state-of-the-art baselines, exhibiting robust cross-domain adaptation and 
task-specific reliability, which highlight its translational potential in 
diverse pathological settings.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01833-6
PMCID: PMC12241324
PMID: 40634485

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


318. PLoS One. 2025 Jul 9;20(7):e0327564. doi: 10.1371/journal.pone.0327564. 
eCollection 2025.

Clinical prediction of intravenous immunoglobulin-resistant Kawasaki disease 
based on interpretable Transformer model.

Chen G(1), Yang Z(1).

Author information:
(1)Department of pediatrics, Affiliated Hospital of North Sichuan Medical 
College, Nanchong, Sichuan, China.

Intravenous immunoglobulin (IVIG) has been established as the first-line therapy 
for Kawasaki disease (KD). However, approximately 10%-20% of pediatric patients 
exhibit IVIG resistance. Current machine learning (ML) models demonstrate 
suboptimal predictive performance in KD treatment response prediction, primarily 
due to their limited ability to effectively process categorical variables and 
interpret tabular clinical data. This study aims to develop and interpretable 
transformer-based clinical prediction model for IVIG resistant KD and validate 
its clinical utility. This retrospective study analyzed clinical records of KD 
patients from the Affiliated Hospital of North Sichuan Medical College 
(Nanchong, China) between January 1, 2014 and December 31, 2024. A cohort of 
1,578 pediatric KD cases was systematically divided into training and validation 
sets. Six machine learning algorithms - Random Forest (RF), AdaBoost, Light 
Gradient Boosting Machine (LightGBM), eXtreme Gradient Boosting (XGBoost), 
Categorical Boosting (CatBoost), and Tabular Prior-data Fitted Network version 
2.0 (TabPFN-V2) - were implemented with five-fold cross-validation to optimize 
model hyperparameters. Model performance was rigorously evaluated using seven 
metrics: accuracy, precision, recall, F1-score, Matthews correlation coefficient 
(MCC), area under the receiver operating characteristic (ROC-AUC), and area 
under the precision-recall curve (PR-AUC). The top-performing model was 
subsequently subjected to interpretability analysis through Shapley Additive 
Explanations (SHAP) to elucidate feature contributions. The transformer-based 
TabPFN-V2 model demonstrated superior predictive performance in KD analysis, 
achieving an impressive validation set accuracy of 0.97. Comprehensive 
evaluation metrics confirmed its robust performance: precision 0.98, recall 
0.97, F1-score 0.98, MCC 0.95, ROC-AUC 0.99, and PR-AUC 0.99. Global 
interpretability analysis through kernel SHAP methodology identified the ten 
most influential predictive features ranked by significance: Coronary artery 
lesions (CAL), Aspartate aminotransferase (AST), C-reactive protein (CRP), 
whether it was incomplete KD (KDtype), Neutrophil count (N), Platelet count 
(PLT), Albumin (ALB), age, White blood cell count (WBC) and Hemoglobin (Hb). 
Local interpretability analysis revealed distinct correlation patterns with IVIG 
resistance:AST, CRP, and N demonstrated significant positive correlations, where 
elevated values corresponded to increased IVIG resistance risk; PLT and ALB 
showed negative correlations, with higher levels associated with reduced 
resistance probability. Notably, age and WBC parameters demonstrated threshold 
effects, where optimal cutoff values enabled re-calibration of single-variable 
predictive scores. This threshold-dependent relationship suggests potential 
clinical utility in risk stratification protocols.The TabPFN-V2 model, 
leveraging an interpretable transformer architecture, demonstrates dual clinical 
utilities in KD management: (1) accurate prediction of IVIG resistance risk, and 
(2) data-driven support for personalized therapeutic decision-making. This 
framework enables probabilistic estimation of treatment resistance likelihood 
while providing transparent feature contribution analyses essential for 
developing patient-specific management protocols.

Copyright: © 2025 Chen, Yang. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0327564
PMCID: PMC12240358
PMID: 40632807 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


319. JMIR Cardio. 2025 Jul 8;9:e68817. doi: 10.2196/68817.

Improving the Readability of Institutional Heart Failure-Related Patient 
Education Materials Using GPT-4: Observational Study.

King RC(1), Samaan JS(2), Haquang J(1), Bharani V(1), Margolis S(3), Srinivasan 
N(4), Peng Y(5), Yeo YH(2), Ghashghaei R(1).

Author information:
(1)Department of Medicine, Division of Cardiology, University of California, 
Irvine Medical Center, 101 The City Dr S, Orange, CA, 92868, United States, 1 
714-456-7890.
(2)Department of Medicine, Karsh Division of Gastroenterology and Hepatology, 
Cedars-Sinai Medical Center, Los Angeles, CA, United States.
(3)David Geffen School of Medicine, University of California, Los Angeles, Los 
Angeles, CA, United States.
(4)Keck School of Medicine, University of Southern California, Los Angeles, CA, 
United States.
(5)School of Mathematics and Statistics, Xi'an Jiaotong University, Xi'an, 
China.

BACKGROUND: Heart failure management involves comprehensive lifestyle 
modifications such as daily weights, fluid and sodium restriction, and blood 
pressure monitoring, placing additional responsibility on patients and 
caregivers, with successful adherence often requiring extensive counseling and 
understandable patient education materials (PEMs). Prior research has shown PEMs 
related to cardiovascular disease often exceed the American Medical 
Association's fifth- to sixth-grade recommended reading level. The large 
language model (LLM) ChatGPT may be a useful tool for improving PEM readability.
OBJECTIVE: We aim to assess the readability of heart failure-related PEMs from 
prominent cardiology institutions and evaluate GPT-4's ability to improve these 
metrics while maintaining accuracy and comprehensiveness.
METHODS: A total of 143 heart failure-related PEMs were collected from the 
websites of the top 10 institutions listed on the 2022-2023 US News & World 
Report for "Best Hospitals for Cardiology, Heart & Vascular Surgery." PEMs were 
individually entered into GPT-4 (version updated July 20, 2023), preceded by the 
prompt, "Please explain the following in simpler terms." Readability was 
assessed using the Flesch Reading Ease score, Flesch-Kincaid Grade Level (FKGL), 
Gunning Fog Index, Coleman-Liau Index, Simple Measure of Gobbledygook Index, and 
Automated Readability Index. The accuracy and comprehensiveness of revised GPT-4 
PEMs were assessed by a board-certified cardiologist.
RESULTS: For 143 institutional heart failure-related PEMs analyzed, the median 
FKGL was 10.3 (IQR 7.9-13.1; high school sophomore) compared to 7.3 (IQR 
6.1-8.5; seventh grade) for GPT-4's revised PEMs (P<.001). Of the 143 
institutional PEMs, there were 13 (9.1%) below the sixth-grade reading level, 
which improved to 33 (23.1%) after revision by GPT-4 (P<.001). No revised GPT-4 
PEMs were graded as less accurate or less comprehensive compared to 
institutional PEMs. A total of 33 (23.1%) GPT-4 PEMs were graded as more 
comprehensive.
CONCLUSIONS: GPT-4 significantly improved the readability of institutional heart 
failure-related PEMs. The model may be a promising adjunct resource in addition 
to care provided by a licensed health care professional for patients living with 
heart failure. Further rigorous testing and validation is needed to investigate 
its safety, efficacy, and impact on patient health literacy.

© Ryan C King, Jamil S Samaan, Joseph Haquang, Vishnu Bharani, Samuel Margolis, 
Nitin Srinivasan, Yuxin Peng, Yee Hui Yeo, Roxana Ghashghaei. Originally 
published in JMIR Cardio (https://cardio.jmir.org).

DOI: 10.2196/68817
PMCID: PMC12263092
PMID: 40627825 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: RG is a consultant for 
Pfizer, Alnylam, and AstraZeneca. None of the other authors have interests to 
disclose.


320. Diagn Interv Radiol. 2025 Jul 8. doi: 10.4274/dir.2025.253445. Online ahead of 
print.

Foundation models for radiology: fundamentals, applications, opportunities, 
challenges, risks, and prospects.

Akinci D'Antonoli T(1)(2), Bluethgen C(3), Cuocolo R(4), Klontzas ME(5)(6)(7), 
Ponsiglione A(8), Kocak B(9).

Author information:
(1)University Hospital Basel, Department of Diagnostic and Interventional 
Neuroradiology, Basel, Switzerland.
(2)University Children's Hospital Basel, Department of Pediatric Radiology, 
Basel, Switzerland.
(3)University Hospital Zurich, Institute for Diagnostic and Interventional 
Radiology, Zurich, Switzerland.
(4)Department of Medicine, Surgery and Dentistry, University of Salerno, 
Baronissi, Italy.
(5)University of Crete School of Medicine, Artificial Intelligence and 
Translational Imaging (ATI) Lab, Department of Radiology, Crete, Greece.
(6)Foundation for Research and Technology (ICS-FORTH), Institute of Computer 
Science, Computational Biomedicine Lab, Crete, Greece.
(7)Karolinska Institute, Department of Clinical Science, Intervention and 
Technology (CLINTEC), Division of Radiology, Stockholm, Sweden.
(8)University of Naples Federico II, Department of Advanced Biomedical Sciences, 
Naples, Italy.
(9)Başakşehir Çam and Sakura City Hospital, Department of Radiology, İstanbul, 
Türkiye.

Foundation models (FMs) represent a significant evolution in artificial 
intelligence (AI), impacting diverse fields. Within radiology, this evolution 
offers greater adaptability, multimodal integration, and improved 
generalizability compared with traditional narrow AI. Utilizing large-scale 
pre-training and efficient fine-tuning, FMs can support diverse applications, 
including image interpretation, report generation, integrative diagnostics 
combining imaging with clinical/laboratory data, and synthetic data creation, 
holding significant promise for advancements in precision medicine. However, 
clinical translation of FMs faces several substantial challenges. Key concerns 
include the inherent opacity of model decision-making processes, environmental 
and social sustainability issues, risks to data privacy, complex ethical 
considerations, such as bias and fairness, and navigating the uncertainty of 
regulatory frameworks. Moreover, rigorous validation is essential to address 
inherent stochasticity and the risk of hallucination. This international 
collaborative effort provides a comprehensive overview of the fundamentals, 
applications, opportunities, challenges, and prospects of FMs, aiming to guide 
their responsible and effective adoption in radiology and healthcare.

DOI: 10.4274/dir.2025.253445
PMID: 40626693


321. Acad Pediatr. 2025 Nov-Dec;25(8):102887. doi: 10.1016/j.acap.2025.102887. Epub 
2025 Jul 5.

Caregiver and Pediatric Clinician Perspectives on Artificial Intelligence for 
Language Services.

Ficerai-Garland G(1), Groves P(2), Puccio EA(3), Bruno S(4), Hoffman H(5), 
Villegas NP(6), Dancisin MCQ(7), Khan A(8), Lion KC(9), Chaves-Gnecco D(10), 
Diab M(11), Ragavan MI(12).

Author information:
(1)University of Pittsburgh School of Medicine (G Ficerai-Garland, P 
Groves, EA Puccio, S Bruno, and H Hoffman), Pittsburgh, Pa. Electronic address: 
Ficerai-Garland.Gabriella@medstudent.pitt.edu.
(2)University of Pittsburgh School of Medicine (G Ficerai-Garland, P 
Groves, EA Puccio, S Bruno, and H Hoffman), Pittsburgh, Pa. Electronic address: 
Groves.Peyton@medstudent.pitt.edu.
(3)University of Pittsburgh School of Medicine (G Ficerai-Garland, P 
Groves, EA Puccio, S Bruno, and H Hoffman), Pittsburgh, Pa. Electronic address: 
Puccio.Elena@medstudent.pitt.edu.
(4)University of Pittsburgh School of Medicine (G Ficerai-Garland, P 
Groves, EA Puccio, S Bruno, and H Hoffman), Pittsburgh, Pa. Electronic address: 
Bruno.Sabrina@medstudent.pitt.edu.
(5)University of Pittsburgh School of Medicine (G Ficerai-Garland, P 
Groves, EA Puccio, S Bruno, and H Hoffman), Pittsburgh, Pa. Electronic address: 
Hoffman.Henry@medstudent.pitt.edu.
(6)Division of General Academic Pediatrics, University of Pittsburgh School of 
Medicine (NP Villegas, D Chaves-Gnecco, and MI Ragavan), Pittsburgh, Pa. 
Electronic address: NCP33@pitt.edu.
(7)UPMC Children's Hospital of Pittsburgh (MCQ Dancisin), Pittsburgh, Pa. 
Electronic address: Mariacecilia.Dancisin@chp.edu.
(8)Division of General Pediatrics, Boston Children's Hospital (A Khan), Boston, 
Mass; Department of Pediatrics, Harvard Medical School (A Khan), Boston, Mass. 
Electronic address: Alisa.Khan@childrens.harvard.edu.
(9)Department of Pediatrics, University of Washington School of Medicine (KC 
Lion), Seattle, Wash; Center for Child Health, Behavior and Development, Seattle 
Children's Research Institute (KC Lion), Seattle, Wash. Electronic address: 
casey.lion@seattlechildrens.org.
(10)Division of General Academic Pediatrics, University of Pittsburgh School of 
Medicine (NP Villegas, D Chaves-Gnecco, and MI Ragavan), Pittsburgh, Pa. 
Electronic address: diego.chaves-gnecco@chp.edu.
(11)Language Technologies Institute, Carnegie Mellon University (M Diab), 
Pittsburgh, Pa. Electronic address: mdiab@andrew.cmu.edu.
(12)Division of General Academic Pediatrics, University of Pittsburgh School of 
Medicine (NP Villegas, D Chaves-Gnecco, and MI Ragavan), Pittsburgh, Pa. 
Electronic address: ragavanm@chp.edu.

OBJECTIVE: Children and caregivers who use languages other than English (LOE) 
for pediatric health care experience inequities and poorer overall health. 
Advancement of large language models has raised questions about the future use 
of artificial intelligence (AI) for language access. This study explored the 
perspectives of caregivers who use LOE and pediatric clinicians on using AI to 
address unmet needs in health care translation and interpretation.
METHODS: We conducted semistructured individual interviews with caregivers of 
pediatric patients who use LOE and pediatric clinicians about the use of AI 
language technologies in health care. A phone interpreter or fluent team member 
was used for LOE interviews. Recordings were transcribed, translated as needed, 
coded, and analyzed using inductive thematic analysis.
RESULTS: Twenty caregivers using 11 different LOE and 22 clinicians 
participated. Themes were consistent across participants, though clinicians were 
more familiar with the concept of AI. Participants reported use of technologies 
(eg, Google Translate) for written communication and situations where verbal 
interpretation was perceived to be inadequate. They were concerned about AI 
accuracy, privacy, and loss of empathy, but hoped it could provide real-time 
document translation and more convenient verbal communication. Ease of use, 
validation, and equitable creation and access were critical for use.
CONCLUSIONS: Caregivers and clinicians were open to using AI to fill gaps in 
translation and interpretation; however, robust validation of AI technology and 
complementary use with human interpreters and translators is needed. Future 
research, practice, and policy should focus on integrating AI while investing in 
human translation and interpretation.

Copyright © 2025 Academic Pediatric Association. Published by Elsevier Inc. All 
rights reserved.

DOI: 10.1016/j.acap.2025.102887
PMID: 40623593 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
have no conflicts of interest or corporate sponsors to disclose.


322. NPJ Digit Med. 2025 Jul 5;8(1):410. doi: 10.1038/s41746-025-01715-x.

A systematic literature review on integrating AI-powered smart glasses into 
digital health management for proactive healthcare solutions.

Wang B(1)(2), Zheng Y(3), Han X(4), Kong L(4), Xiao G(5), Xiao Z(6)(7), Chen 
S(8)(9).

Author information:
(1)Beijing Xiaotangshan Hospital, Beijing, China. boyuan422@foxmail.com.
(2)Department of Biomedical Sciences, City University of Hong Kong, Hong Kong, 
China. boyuan422@foxmail.com.
(3)School of Mathematical Sciences, Capital Normal University, Beijing, China.
(4)National Institute of Hospital Administration (NIHA), Beijing, China.
(5)National Institute of Hospital Administration (NIHA), Beijing, China. 
gexin_xiao@sina.com.
(6)The First Affiliated Hospital of Hunan University of Medicine, Huaihua, 
China. hyfyxzx@163.com.
(7)Hunan University of Medicine, Huaihua, China. hyfyxzx@163.com.
(8)The First Affiliated Hospital of Hunan University of Medicine, Huaihua, 
China. 1506376330@qq.com.
(9)Hunan Primary Digital Engineering Technology Research Center for Medical 
Prevention and Treatment, Huaihua, China. 1506376330@qq.com.

AI-powered smart glasses are emerging as a highly promising advancement in the 
field of digital health management, owing to their capabilities in real-time 
monitoring, chronic disease management, and personalized treatment planning. To 
comprehensively understand the current state of development, we systematically 
searched multiple databases, including Web of Science, PubMed, and IEEE Xplore, 
to collect relevant literature. This paper provides a systematic analysis of the 
current applications of smart glasses in healthcare, focusing on their potential 
benefits and limitations. Key issues discussed include user engagement, 
treatment adherence, data privacy, standardization, battery efficiency, clinical 
validation, and medical ethics. Our findings suggest that, supported by emerging 
clinical evidence, smart glasses have demonstrated significant improvements in 
areas such as assisted medical services, health management, anxiety alleviation 
in children, and telemedicine. By integrating multi-modal sensors, these devices 
are capable of accurately tracking certain physiological indicators and 
synchronizing real-time visual input, thereby enhancing the accuracy and 
timeliness of health interventions and medical services. Notably, some 
cutting-edge smart glasses have adopted advanced artificial intelligence 
algorithms, particularly large language models (LLMs) with context awareness and 
human-like interaction capabilities. These AI-powered glasses can offer 
real-time, personalized dietary and health management recommendations tailored 
to users' daily life scenarios. Building on these findings, this study further 
proposes a conceptual framework for proactive health management using smart 
glasses and explores future directions in technological development and 
practical applications. Overall, AI-enhanced smart glasses show great potential 
as a critical interface between healthcare providers and patients, poised to 
play a vital role in the future of personalized medicine and continuous health 
management.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01715-x
PMCID: PMC12228729
PMID: 40617964

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


323. Front Plant Sci. 2025 Jun 19;16:1522510. doi: 10.3389/fpls.2025.1522510. 
eCollection 2025.

Few-shot object detection for pest insects via features aggregation and 
contrastive learning.

He S(1)(2), Jin B(1)(2), Sun X(2), Jiang W(1)(2), Gu J(3), Gu F(4).

Author information:
(1)School of Information Science and Technology, Hainan Normal University, 
Haikou, Hainan, China.
(2)Hainan Provincial Engineering Research Center for Artificial Intelligence and 
Equipment for Monitoring Tropical Biodiversity and Ecological Environment, 
Hainan Normal University, Haikou, Hainan, China.
(3)College of Computer Science and Technology, Zhejiang University, Hangzhou, 
Zhejiang, China.
(4)Spice and Beverage Research Institute, Chinese Academy of Tropical 
Agricultural Sciences, Wanning, Hainan, China.

Accurate detection of pest insects is critical for agricultural pest management 
and crop yield protection, yet traditional detection methods struggle due to the 
vast diversity of pest species, significant individual differences, and limited 
labeled data. These challenges are compounded by the typically small size of 
pest targets and complex environmental conditions. To address these limitations, 
this study proposes a novel few-shot object detection (FSOD) method leveraging 
feature aggregation and supervised contrastive learning (SCL) within the Faster 
R-CNN framework. Our methodology involves multi-scale feature extraction using a 
Feature Pyramid Network (FPN), enabling the capture of rich semantic information 
across various scales. A Feature Aggregation Module (FAM) with an attention 
mechanism is designed to effectively fuse contextual features from support and 
query images, enhancing representation capabilities for multi-scale and 
few-sample pest targets. Additionally, supervised contrastive learning is 
employed to strengthen intra-class similarity and inter-class dissimilarity, 
thereby improving discriminative power. To manage class imbalance and enhance 
the focus on challenging samples, focal loss and class weights are integrated 
into the model's comprehensive loss function. Experimental validation on the 
PestDet20 dataset, consisting of diverse tropical pest insects, demonstrates 
that the proposed method significantly outperforms existing approaches, 
including YOLO, TFA, VFA, and FSCE. Specifically, our model achieves superior 
mean Average Precision (mAP) results across different few-shot scenarios 
(3-shot, 5-shot, and 10-shot), demonstrating robustness and stability. Ablation 
studies confirm that each component of our method substantially contributes to 
performance improvement. This research provides a practical and efficient 
solution for pest detection under challenging conditions, reducing dependency on 
large annotated datasets and improving detection accuracy for minority pest 
classes. While computational complexity remains higher than real-time frameworks 
like YOLO, the significant gains in detection accuracy justify the trade-off for 
critical pest management applications.

Copyright © 2025 He, Jin, Sun, Jiang, Gu and Gu.

DOI: 10.3389/fpls.2025.1522510
PMCID: PMC12222325
PMID: 40612615

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


324. Acad Radiol. 2025 Sep;32(9):5364-5377. doi: 10.1016/j.acra.2025.06.010. Epub 
2025 Jul 3.

A Novel Two-step Classification Approach for Differentiating Bone Metastases 
From Benign Bone Lesions in SPECT/CT Imaging.

Xie W(1), Wang X(2), Liu M(3), Mai L(1), Shangguan H(4), Pan X(1), Zhan Y(1), 
Zhang J(2), Wu X(2), Dai Y(2), Pei Y(2), Zhang G(2), Yao Z(1), Wang Z(5).

Author information:
(1)College of Medicine and Biological Information Engineering, Northeastern 
University, Shenyang, Liaoning 110167, China (W.X., L.M., X.P., Y.Z., Z.Y.); 
Department of Nuclear Medicine, General Hospital of Northern Theater Command, 
Shenyang, Liaoning 110016, China (W.X., X.W., M.L., L.M., X.P., Y.Z., J.Z., 
X.W., Y.D., Y.P., G.Z., Z.Y., Z.W.).
(2)Department of Nuclear Medicine, General Hospital of Northern Theater Command, 
Shenyang, Liaoning 110016, China (W.X., X.W., M.L., L.M., X.P., Y.Z., J.Z., 
X.W., Y.D., Y.P., G.Z., Z.Y., Z.W.).
(3)Department of Nuclear Medicine, General Hospital of Northern Theater Command, 
Shenyang, Liaoning 110016, China (W.X., X.W., M.L., L.M., X.P., Y.Z., J.Z., 
X.W., Y.D., Y.P., G.Z., Z.Y., Z.W.); Department of Nuclear Medicine, Xijing 
Hospital, Fourth Military Medical University, Xi'an, Shaanxi 710000, China 
(M.L.).
(4)School of Computer Science and Engineering, Northeastern University, 
Shenyang, Liaoning 110167, China (H.S.).
(5)Department of Nuclear Medicine, General Hospital of Northern Theater Command, 
Shenyang, Liaoning 110016, China (W.X., X.W., M.L., L.M., X.P., Y.Z., J.Z., 
X.W., Y.D., Y.P., G.Z., Z.Y., Z.W.). Electronic address: wangzhiguo5778@163.com.

RATIONALE AND OBJECTIVES: This study aims to develop and validate a novel 
two-step deep learning framework for the automated detection, segmentation, and 
classification of bone metastases in SPECT/CT imaging, accurately distinguishing 
malignant from benign lesions to improve early diagnosis and facilitate 
personalized treatment planning.
MATERIALS AND METHODS: A segmentation model, BL-Seg, was developed to 
automatically segment lesion regions in SPECT/CT images, utilizing a multi-scale 
attention fusion module and a triple attention mechanism to capture metabolic 
variations and refine lesion boundaries. A radiomics-based ensemble learning 
classifier was subsequently applied to integrate metabolic and texture features 
for benign-malignant differentiation. The framework was trained and evaluated 
using a proprietary dataset of SPECT/CT images collected from our institution. 
Performance metrics, including Dice coefficient, sensitivity, specificity, and 
AUC, were compared against conventional methods.
RESULTS: The study utilized a dataset of SPECT/CT cases from our institution, 
divided into training and test sets acquired on Siemens SPECT/CT scanners with 
minor protocol differences. BL-Seg achieved a Dice coefficient of 0.8797, 
surpassing existing segmentation models. The classification model yielded an AUC 
of 0.8502, with improved sensitivity and specificity compared to traditional 
approaches.
CONCLUSION: The proposed framework, with BL-Seg's automated lesion segmentation, 
demonstrates superior accuracy in detecting, segmenting, and classifying bone 
metastases, offering a robust tool for early diagnosis and personalized 
treatment planning in metastatic bone disease.

Copyright © 2025 The Association of University Radiologists. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.acra.2025.06.010
PMID: 40610298 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


325. J Med Internet Res. 2025 Jul 3;27:e53332. doi: 10.2196/53332.

Exploring Inflammatory Bowel Disease Discourse on Reddit Throughout the COVID-19 
Pandemic Using OpenAI's GPT-3.5 Turbo Model: Classification Model Validation and 
Case Study.

Babinski T(1)(2), Karley S(3), Cooper M(4), Shaik S(#)(2), Wang YK(#)(2).

Author information:
(1)Division of Gastroenterology, Hepatology, and Nutrition, Children's Hospital 
of Philadelphia, Philadelphia, PA, United States.
(2)Division of Management and Education, University of Pittsburgh at Bradford, 
Bradford, PA, United States.
(3)Division of Gastroenterology and Hepatology, University of Pennsylvania, 
Philadelphia, PA, United States.
(4)Department of Child and Adolescent Psychiatry and Behavioral Sciences, 
Children's Hospital of Philadelphia, Philadelphia, PA, United States.
(#)Contributed equally

BACKGROUND: Inflammatory bowel disease (IBD) is a chronic autoimmune disorder 
with an increasing prevalence in the general population. Internet-based 
communities have become vital for communication among patients with IBD, 
especially throughout the COVID-19 pandemic. However, these internet-based 
patient-to-patient communications remain largely underexplored.
OBJECTIVE: This study aims to analyze community posts from 3 of the largest IBD 
support groups on Reddit between March 1, 2020, and December 31, 2022, using a 
pretrained transformer model, and to validate the classification system's 
results via comparison to human scoring.
METHODS: We collected posts (N=53,333) from subreddits r/CrohnsDisease, 
r/UlcerativeColitis, and r/IBD and classified them using OpenAI's GPT-3.5 Turbo 
model to determine sentiment, categorize topics, and identify demographic 
information and mentions of the COVID-19 pandemic. A subset of posts (n=397) was 
manually scored to measure interrater agreement between human raters and the 
GPT-3.5 Turbo model.
RESULTS: Fleiss κ and Gwet AC1 coefficients indicated a high level of agreement 
between raters, with values ranging from 0.53 to 0.91. The raters demonstrated 
almost perfect agreement on the classification of gender, with a Fleiss κ of 
0.91 (P<.001). Medications (14,909/53,333) and symptoms (14,939/53,333) emerged 
as the most discussed topics, and most posts conveyed a neutral sentiment. While 
most users did not disclose their age, those who did primarily belonged to the 
20-29 years (2392/4828) and 30-39 years (859/4828) age groups. Based on 
self-reported gender, we identified 1509 men and 1502 women among our IBD Reddit 
users. When comparing the users on the IBD subreddits to the general IBD 
population, there was a significant difference in gender distribution 
(N=3,090,011; χ22=69.53; P<.001; φ<0.001). After an initial spike in posts 
within the first month, most posts did not reference the COVID-19 pandemic.
CONCLUSIONS: Our study showcases the potential of generative pretrained 
transformer models in processing and extracting insights from medical social 
media data. Future research can benefit from further subanalyses of our 
validated dataset or use OpenAI's model to analyze social media data for other 
conditions, particularly those for which patient experiences are challenging to 
collect.

©Tyler Babinski, Sara Karley, Marita Cooper, Salma Shaik, Y Ken Wang. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org), 
03.07.2025.

DOI: 10.2196/53332
PMCID: PMC12271966
PMID: 40607732 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


326. Brain Behav. 2025 Jul;15(7):e70648. doi: 10.1002/brb3.70648.

Dark Future: Development and Initial Validation of Artificial Intelligence 
Conspiracy Beliefs Scale (AICBS).

Lin CY(1)(2)(3)(4), Brailovskaia J(5)(6), Üztemur S(7), Gökalp A(8), Değirmenci 
N(9), Huang PC(10), Chen IH(11), Griffiths MD(12), Pakpour AH(13).

Author information:
(1)Institute of Allied Health Sciences, College of Medicine, National Cheng Kung 
University, Tainan, Taiwan.
(2)Department of Public Health, College of Medicine, National Cheng Kung 
University, Tainan, Taiwan.
(3)Biostatistics Consulting Center, National Cheng Kung University Hospital, 
College of Medicine, National Cheng Kung University, Tainan, Taiwan.
(4)School of Nursing, College of Nursing, Kaohsiung Medical University, 
Kaohsiung, Taiwan.
(5)Mental Health Research and Treatment Center, Ruhr-Universität Bochum, Bochum, 
Germany.
(6)German Center for Mental Health (DZPG), partner site Bochum-Marburg, Bochum, 
Germany.
(7)Department of Turkish and Social Sciences Education, Faculty of Education, 
Anadolu University, Eskişehir, Türkiye.
(8)Department of Educational Sciences, Gaziantep University, Gaziantep, Türkiye.
(9)Institute of Educational Sciences, Gazi University, Ankara, Türkiye.
(10)Department of Physiotherapy, School of Nursing and Health Sciences, Hong 
Kong Metropolitan University, Hong Kong, China.
(11)Chinese Academy of Education Big Data, Faculty of Education, Qufu Normal 
University, Qufu, China.
(12)International Gaming Research Unit, Psychology Department, Nottingham Trent 
University, Nottingham, UK.
(13)Department of Nursing, School of Health and Welfare, Jönköping University, 
Hälsohögskolan, Jönköping, Sweden.

BACKGROUND: In the past few years, the rapid development of artificial 
intelligence (AI) and its success in many areas of everyday life have attracted 
global attention. Some discussions have noted that generative AI tools can make 
decisions on their own with the potential to improve themselves. Subsequently, 
conspiracy theories have emerged concerning the future implications of AI. In 
the present study, the Artificial Intelligence Conspiracy Beliefs Scale (AICBS) 
was developed to assess conspiracy beliefs concerning AI, andits psychometric 
properties were examined.
METHODS: A cross-sectional survey was conducted with 788 Turkish participants 
(Mage = 25.10 years, 56% female). The sample was split to carry out an 
exploratory factor analysis (EFA; n = 423) and a confirmatory factor analysis 
(CFA; n = 365), resulting in a 30-item scale comprising five subdimensions.
RESULTS: The five-factor structure explained 62.58% of the total variance. The 
CFA showed acceptable model fit indices and confirmed the EFA's five-factor 
structure. Based on the EFA's factor loadings, a short five-item version of the 
AICBS (AICBS-5) was developed with one item from each subdimension (which 
explained 45.28% of the variance). The CFA confirmed the unidimensional 
structure of the AICBS-5. The internal consistency coefficients of the AICBS, 
its subdimensions, and the AICBS-5 demonstrated very good reliability. 
Correlation analyses with external criterion measures (AI Anxiety Scale, Generic 
Conspiracist Beliefs Scale-5, and Anomie) supported the concurrent validity of 
the AICBS, its subdimensions, and the AICBS-5.
CONCLUSION: The findings demonstrate that both AICBS and AICBS-5 are valid and 
reliable psychometric instruments to assess AI conspiracy beliefs.

© 2025 The Author(s). Brain and Behavior published by Wiley Periodicals LLC.

DOI: 10.1002/brb3.70648
PMCID: PMC12224044
PMID: 40607584 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


327. J Clin Neurophysiol. 2025 Jul 2;42(6):493-504. doi: 
10.1097/WNP.0000000000001186.

Artificial Intelligence and Machine Learning in Neuromodulation for Epilepsy.

Ervin B(1), Arya R(1)(2)(3).

Author information:
(1)Comprehensive Epilepsy Center, Division of Neurology, Cincinnati Children's 
Hospital Medical Center, Cincinnati, Ohio, U.S.A.
(2)Department of Pediatrics, University of Cincinnati College of Medicine, 
Cincinnati, Ohio, U.S.A. ; and.
(3)Department of Computer Science, University of Cincinnati, Cincinnati, Ohio, 
U.S.A .

Recent advances in artificial intelligence (AI) and machine learning (ML) can 
revolutionize neuromodulation therapies for drug-resistant epilepsy. Successful 
incorporation of AI/ML methods into the management of epilepsy can guide 
treatment decisions, enable interventions to adapt to dynamic epileptic 
networks, and hopefully improve patient outcomes. We introduce some common 
concepts in ML, focusing on neural networks, particularly convolutional and 
recurrent neural networks, and support vector machines, because these methods 
have been commonly applied to epilepsy neuromodulation. We discuss current AI/ML 
applications in neuromodulation, encompassing vagus nerve stimulation, 
responsive neurostimulation, and deep brain stimulation, for the treatment of 
epilepsy. We consider how AI/ML methods leverage large data sets to enhance 
patient-specific epileptic network analysis, optimize stimulation targets, and 
refine closed-loop systems for real-time seizure detection and termination. 
AI/ML applications extend to recognizing autonomic and behavioral seizure 
surrogates, detecting interictal epileptiform activity, and forecasting seizures 
for preemptive interventions. Furthermore, AI-powered neuroimaging analysis can 
enhance segmentation accuracy for precise electrode placement, which can improve 
neuromodulation outcomes. We review which AI/ML tools have been applied to each 
problem, as well as their relative performance. Challenges remain, however, in 
translating AI/ML models into clinical settings due to interpatient variability 
and limited real-world validation. Future directions include integrating 
behavioral signals, developing AI-assisted clinical decision tools, and refining 
energy-efficient neurostimulation designs. Large language models and generative 
AI hold promise for optimizing patient-specific neuromodulation strategies. 
However, further research is required to validate AI/ML applications in clinical 
practice, enhance model generalizability, and address ethical concerns 
surrounding data privacy and AI-driven decision making.

Copyright © 2025 by the American Clinical Neurophysiology Society.

DOI: 10.1097/WNP.0000000000001186
PMID: 40601969 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no funding or conflicts of 
interest to disclose.


328. BMC Neurol. 2025 Jul 1;25(1):264. doi: 10.1186/s12883-025-04280-8.

Evaluating ChatGPT and DeepSeek in postdural puncture headache management: a 
comparative study with international consensus guidelines.

Deng J(#)(1)(2), Qiu X(#)(1)(2), Dong C(1)(2), Xu L(1)(2), Dong X(3)(4), Yang 
S(5), Li Q(1)(2), Mei T(1)(2), Chen S(1)(2), Wu Y(1)(2), Sun J(1)(2), He F(6), 
Wang H(7)(8), Yu L(9)(10).

Author information:
(1)The Fourth Clinical School of Medicine, Zhejiang Chinese Medical 
University，Hangzhou First People's Hospital, Hangzhou, China.
(2)Department of Pain, The Affiliated Hangzhou First People's Hospital, Westlake 
University School of Medicine, Hangzhou, China.
(3)Department of Neurology, Shanghai General Hospital, Shanghai Jiao Tong 
University School of Medicine, No.86, Wujin Road, Shanghai, 200080, China.
(4)Program in Developmental and Stem Cell Biology, The Hospital for Sick 
Children, 686 Bay Street, Toronto, ON, M5G 0A4, Canada.
(5)Department of Anaesthesiology, First Affiliated Hospital of Soochow 
University, Suzhou, Jiangsu, China.
(6)Department of Pain Management, Center for Intracranial Hypotension 
Management, Sir Run Run Shaw Hospital, Medical College of Zhejiang University, 
Hangzhou, China. hefeifang@zju.edu.cn.
(7)The Fourth Clinical School of Medicine, Zhejiang Chinese Medical 
University，Hangzhou First People's Hospital, Hangzhou, China. 
wanghanbin@hospital.westlake.edu.cn.
(8)Department of Pain, The Affiliated Hangzhou First People's Hospital, Westlake 
University School of Medicine, Hangzhou, China. 
wanghanbin@hospital.westlake.edu.cn.
(9)The Fourth Clinical School of Medicine, Zhejiang Chinese Medical 
University，Hangzhou First People's Hospital, Hangzhou, China. 
yuliang@hospital.westlake.edu.cn.
(10)Department of Pain, The Affiliated Hangzhou First People's Hospital, 
Westlake University School of Medicine, Hangzhou, China. 
yuliang@hospital.westlake.edu.cn.
(#)Contributed equally

OBJECTIVE: To evaluate the use of ChatGPT and DeepSeek in clinical practice to 
provide healthcare professionals with accurate information on the prevention, 
diagnosis, and management of post-dural puncture headache (PDPH), in particular 
to evaluate ChatGPT-4o, ChatGPT-4o mini, DeepSeek-V3 and DeepSeek with Deep 
Think(R1)'s responses with consensus practice guidelines for headache after 
dural puncture.
BACKGROUND: Post-dural puncture headache (PDPH) is a common complication of 
dural puncture. Currently, there is a lack of evidence-based guidance on the 
prevention, diagnosis and management of PDPH. The 2023 Consensus guidelines 
provide comprehensive information. With the development and popularization of 
AI, more and more people are using ai models, including patients and doctors. 
However, the quality of the answers provided by ai has not yet been tested.
METHODS: Responses from ChatGPT-4o, ChatGPT-4o mini, DeepSeek-V3, and 
DeepSeek-R1 were evaluated against PDPH guidelines using four dimensions: 
Accuracy (guideline adherence), Overconclusiveness (unjustified 
recommendations), Supplementary information (additional relevant details), and 
Incompleteness (omission of critical guidelines). A 5-point Likert scale further 
assessed response accuracy and completeness.
RESULTS: All four models show high accuracy and completeness.Of the 10 clinical 
guidelines evaluated,ChatGPT-4o, ChatGPT-4o mini, DeepSeek-V3 and DeepSeek-R1 
all showed 100% accuracy in responses (10/10)(p = 1). None of the four models 
showed overly conclusive results(p = 1). In terms of supplementary information, 
ChatGPT-4o,ChatGPT-4o mini and DeepSeek-R1 are 100% (10/10), DeepSeek-V3 is 90% 
(9/10)(p = 1). In terms of incompleteness, ChatGPT-4o is 80%(8/10), DeepSeek-R1 
is 70%(7/10), ChatGPT-4o mini and DeepSeek-V3 are 60% (6/10) (p = 0.729).
CONCLUSION: All four AI models demonstrate clinical validity, with ChatGPT-4o 
and DeepSeek-R1 showing stronger guideline alignment. Though largely accurate, 
their responses achieve only 60-80% completeness relative to medical guidelines. 
Healthcare professionals must exercise caution when using AI tools and should 
critically evaluate outputs before clinical application. While promising, their 
partial guideline coverage requires careful human oversight. Further validation 
research is essential before these models can reliably support clinical 
decision-making for complex conditions like PDPH.

© 2025. The Author(s).

DOI: 10.1186/s12883-025-04280-8
PMCID: PMC12211737
PMID: 40597769 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


329. Sci Rep. 2025 Jul 1;15(1):21402. doi: 10.1038/s41598-025-05839-8.

BGATT-GR: accurate identification of glucocorticoid receptor antagonists based 
on data augmentation combined with BiGRU-attention.

Shoombuatong W(1), Mookdarsanit P(2), Schaduangrat N(3), Mookdarsanit L(4).

Author information:
(1)Center for Research Innovation and Biomedical Informatics, Faculty of Medical 
Technology, Mahidol University, Bangkok, 10700, Thailand. 
watshara.sho@mahidol.ac.th.
(2)Computer Science and Artificial Intelligence, Faculty of Science, 
Chandrakasem Rajabhat University, Bangkok, 10900, Thailand.
(3)Center for Research Innovation and Biomedical Informatics, Faculty of Medical 
Technology, Mahidol University, Bangkok, 10700, Thailand.
(4)Business Information System, Faculty of Management Science, Chandrakasem 
Rajabhat University, Bangkok, 10900, Thailand. lawankorn.s@chandra.ac.th.

The glucocorticoid receptor (GR) is a critical nuclear receptor that regulates a 
broad spectrum of physiological functions, including stress adaptation, immune 
response, and metabolism. Given the association between aberrant GR signaling 
and various pathological conditions, this pathway represents a promising 
therapeutic target. Several GR antagonists have been developed to block 
glucocorticoid binding to the receptor, showing therapeutic potential in 
disorders characterized by heightened or dysregulated glucocorticoid signaling. 
Therefore, this study proposes an innovative deep learning-based hybrid 
framework (termed BGATT-GR) that leverages a data augmentation method, a 
bidirectional gated recurrent unit (BiGRU), and a self-attention mechanism (ATT) 
to attain more accurate identification of GR antagonists. In BGATT-GR, we first 
employed AP2D, CDKExt, KR, Morgan, and RDKIT to extract molecular descriptors of 
GR antagonists and combined these molecular descriptors to generate multi-view 
features. Second, we adopted a data augmentation method that combined both 
random under-sampling (RUS) and the synthetic minority over-sampling technique 
(SMOTE) to address the issue of class imbalance. Third, the BGATT architecture 
was constructed to enhance the utility of the multi-view features by generating 
informative feature embeddings. Finally, we applied principal component analysis 
(PCA) to reduce the dimensionality of these feature embeddings and fed the 
processed feature vectors into the final classifier. Extensive experimental 
results showed that BGATT-GR provided more stable performance in both 
cross-validation and independent tests. Furthermore, the independent test 
results revealed that BGATT-GR attained superior predictive performance compared 
with several conventional ML models, with a balanced accuracy of 0.957, an MCC 
of 0.853, and an AUPR of 0.962. In summary, our experimental results provide 
strong evidence to suggest that BGATT-GR is highly accurate and effective for 
identifying GR antagonists.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-05839-8
PMCID: PMC12218296
PMID: 40595974 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


330. Sci Rep. 2025 Jul 1;15(1):20511. doi: 10.1038/s41598-025-07956-w.

Advanced cloud intrusion detection framework using graph based features 
transformers and contrastive learning.

Govindarajan V(#)(1), Muzamal JH(#)(2).

Author information:
(1)Colorado State University, Seattle, USA. vijay.govindarajan91@gmail.com.
(2)National University of Computer and Emerging Sciences, Lahore, Pakistan.
(#)Contributed equally

This paper presents a modular and scalable intrusion detection framework that 
combines graph-based feature extraction, Transformer-based autoencoding, and 
contrastive learning to improve detection accuracy in cloud environments. 
Network flows are modeled as graphs to capture relational patterns among IP 
addresses and services, and a Graph Neural Network (GNN) is used to extract 
structured embeddings. These embeddings are refined through a Transformer-based 
autoencoder to preserve contextual information, while contrastive learning 
enforces clear class separation during classification. The system is evaluated 
on NSL-KDD and CIC-IDS2018 datasets under both binary and multi-class scenarios. 
Experimental results show an average accuracy of 99.97%, with high precision and 
recall across all attack types, including minority classes such as U2R and R2L. 
The model achieves low false-positive rates and demonstrates real-time inference 
performance with modest resource requirements. Key contributions include an 
interpretable pipeline using SHAP for feature attribution, a strategy for 
mitigating class imbalance, and validation across datasets with detailed 
security and generalizability analyses. These results support the practical 
applicability of the proposed approach in high-throughput, cloud-based network 
environments.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-07956-w
PMCID: PMC12215497
PMID: 40595172

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


331. Sci Rep. 2025 Jul 2;15(1):22488. doi: 10.1038/s41598-025-05888-z.

Optimizing the early diagnosis of neurological disorders through the application 
of machine learning for predictive analytics in medical imaging.

Sadu VB(1), Bagam S(2), Naved M(3), Andluru SKR(4), Ramineni K(5), Alharbi 
MG(6), Sengan S(7), Khadhar Moideen R(8).

Author information:
(1)Department of Mechanical Engineering, Jawaharlal Nehru Technological 
University, Kakinada, 533003, Andhra Pradesh, India.
(2)Software Development Team Lead at Paycom, Master of Computer Science, 
Oklahoma Christian University, Edmond, Oklahoma, 73013, USA.
(3)Department of Business Analytics, Jaipuria Institute of Management, Noida, 
Uttar Pradesh, 201309, India.
(4)Department of Computer Science and Engineering, Koneru Lakshmaiah Education 
Foundation, Hyderabad, 500075, Telangana, India.
(5)School of Engineering, Anurag University, Hyderabad, Telangana, 500088, 
India.
(6)Department of Computer Science, College of Computer Engineering and Sciences, 
Prince Sattam Bin Abdulaziz University, Al-Kharj, 16278, Saudi Arabia.
(7)Department of Computer Science and Engineering, PSN College of Engineering 
and Technology, Tirunelveli, 627451, Tamil Nadu, India. sudhasengan@gmail.com.
(8)Department of Artificial Intelligence and Data Science, Mahendra Engineering 
College, Mallasamudram, Namakkal, Tamil Nadu, 637503, India. 
ksrahmaan2204@gmail.com.

Early diagnosis of Neurological Disorders (ND) such as Alzheimer's disease (AD) 
and Brain Tumors (BT) can be highly challenging since these diseases cause minor 
changes in the brain's anatomy. Magnetic Resonance Imaging (MRI) is a vital tool 
for diagnosing and visualizing these ND; however, standard techniques contingent 
upon human analysis can be inaccurate, require a long-time, and detect 
early-stage symptoms necessary for effective treatment. Spatial Feature 
Extraction (FE) has been improved by Convolutional Neural Networks (CNN) and 
hybrid models, both of which are changes in Deep Learning (DL). However, these 
analysis methods frequently fail to accept temporal dynamics, which is 
significant for a complete test. The present investigation introduces the 
STGCN-ViT, a hybrid model that integrates CNN + Spatial-Temporal Graph 
Convolutional Networks (STGCN) + Vision Transformer (ViT) components to address 
these gaps. The model causes the reference to EfficientNet-B0 for FE in space, 
STGCN for FE in time, and ViT for FE using AM. By applying the Open Access 
Series of Imaging Studies (OASIS) and Harvard Medical School (HMS) benchmark 
datasets, the recommended approach proved effective in the investigations, with 
Group A attaining an accuracy of 93.56%, a precision of 94.41% and an Area under 
the Receiver Operating Characteristic Curve (AUC-ROC) score of 94.63%. Compared 
with standard and transformer-based models, the model attains better results for 
Group B, with an accuracy of 94.52%, precision of 95.03%, and AUC-ROC score of 
95.24%. Those results support the model's use in real-time medical applications 
by providing proof of the probability of accurate but early-stage ND diagnosis.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-05888-z
PMCID: PMC12217182
PMID: 40594215 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


332. Sci Rep. 2025 Jul 2;15(1):22921. doi: 10.1038/s41598-025-04858-9.

Research on fault diagnosis method for variable condition planetary gearbox 
based on SKN attention mechanism and deep transfer learning.

Huang NQ(1), Song MM(2), Tang YH(1), Huang LX(3), Chen ZW(4).

Author information:
(1)College of Mechanical and Electrical Engineering, Ningde Normal University, 
Ningde, China.
(2)College of Mechanical and Electrical Engineering, Ningde Normal University, 
Ningde, China. t2122@ndnu.edu.cn.
(3)College of Mechanical and Electrical Engineering, Ningde Normal University, 
Ningde, China. hlx2601014@163.com.
(4)Department of Nature Science and Computer, Ganzhou Teachers College, Gan 
Zhou, China.

Deep learning network models are widely applied to fault diagnosis of planetary 
gearboxes. However, the multi-coupling fault characteristics, accompanied by 
data fuzziness and distribution differences, present certain challenges to 
diagnostic research. Under variable operating conditions, the fault data to be 
diagnosed becomes more prominently inconsistent in distribution, leading to 
suboptimal fault recognition rates in diagnostic models. A deep transfer 
learning method for planetary gearbox fault diagnosis based on a Selective 
Kernel Networks (SKN) attention mechanism is proposed. First, an input dataset 
is constructed through overlapping sampling, and a deep neural network 
diagnostic model is established to automatically learn features and perform 
diagnostics on the data. Second, a dynamic selection mechanism for convolution 
kernels is embedded in the deep neural network, enabling each neuron to 
adaptively adjust its receptive field size based on multi-scale input 
information. This mechanism extracts common features between the source and 
target domains, enhancing the network's feature extraction capability. Third, 
the Local Maximum Mean Discrepancy (LMMD) is used to perform sub-domain 
adaptation on the features of the source and target domains, reducing the 
distribution discrepancy between the two domains and constructing an end-to-end 
transfer adaptation model. This enables deep transfer learning fault diagnosis 
of planetary gearboxes under varying operating conditions. Finally, through 
experimental analysis and validation of 8 variable operating condition tasks, 
the fault identification accuracy of the diagnostic method proposed in this 
paper reached an average of 92.9%. Compared with traditional deep transfer 
learning diagnostic methods, it demonstrates higher diagnostic precision.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-04858-9
PMCID: PMC12216679
PMID: 40592955

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


333. JMIR Pediatr Parent. 2025 Jul 1;8:e70409. doi: 10.2196/70409.

Exploring the Acceptance and Opportunities of Using a Specific Generative AI 
Chatbot to Assist Parents in Managing Pediatric Rheumatological Chronic Health 
Conditions: Mixed Methods Study.

Lau CWY(1), Kupiec K(2)(3), Livermore P(2)(3)(4).

Author information:
(1)UCL Interaction Centre, Department of Computer Science and Division of 
Psychology and Language Sciences, University College London, 20 Bedford Way, 
London, WC1H 0AL, United Kingdom, 44 2074059200.
(2)2, Rheumatology Department, Great Ormond Street Hospital for Children NHS 
Foundation Trust, London, United Kingdom.
(3)Infection, Immunity, and Inflammation Research and Teaching Department, Great 
Ormond Street Institute of Child Health, University College London, London, 
United Kingdom.
(4)NIHR Great Ormond Street Hospital Biomedical Research Centre, UCL Great 
Ormond Street Institute of Child Health, London, United Kingdom.

BACKGROUND: Health care chatbots can be used to support patients and their 
families with everyday decision-making. While there is some research on 
integrating artificial intelligence into pediatric care, no study has focused on 
the opportunity of implementing a generative artificial intelligence chatbot for 
pediatric rheumatology. Pediatric rheumatology conditions require intense family 
input, which can often leave families struggling to navigate disease flares, 
pain, fatigue, medication side effects and adherence, and support of their 
child, often when pediatric rheumatology departments are shut. Understanding how 
we can support families better, without the need for increased personnel, will 
have implications for the health care systems.
OBJECTIVE: The study aimed to explore parental and children and young people's 
acceptance of chatbot use in a pediatric context, and understand how a chatbot 
could be specifically used for managing a child's chronic health condition.
METHODS: This study was a mixed methods design, using both a family workshop and 
a subsequent questionnaire.
RESULTS: In total, 22 participants contributed to the qualitative design using 
the world café methodology at a workshop, and 47 participants (36 parents and 11 
children and young people) completed quantitative data via a questionnaire. 
Participants expressed their likelihood of using chatbot technology, including 
ChatGPT, due to its accessibility. However, participants had significantly 
greater intention (parents: P<.001; children and young people: P=.006) to use a 
specific chatbot over ChatGPT, due to increased trust, credibility, and 
specificity in design. Children and young people and parents should be 
distinguished as 2 user groups in chatbot design, reflecting their specific 
needs in chatbot features and personalization.
CONCLUSIONS: Overall, the study reinforced the need for a specialized and 
trusted chatbot designed with input from health professionals to assist families 
in managing complex chronic health conditions to support families in between 
appointments and complement existing face-to-face care. Future research should 
evaluate users' engagement with a functional prototype to investigate its 
usefulness and explore its implementation into families' everyday lives. 
Importantly, the current findings have broader implications for the field of 
pediatric health care, as similarly tailored chatbot interventions could benefit 
families who are managing other chronic health conditions.

© Cheryl W Y Lau, Klaudia Kupiec, Polly Livermore. Originally published in JMIR 
Pediatrics and Parenting (https://pediatrics.jmir.org).

DOI: 10.2196/70409
PMCID: PMC12236638
PMID: 40591524

Conflict of interest statement: Conflicts of Interest: PL is currently receiving 
a personal fellowship award from the National Institute for Health and Care 
Research (NIHR) fellowship for nonmedical health care professionals (ACAF 
reference 302864), and as PPIE colead for the NIHR Great Ormond Street Hospital 
for Children (GOSH) BRC, she receives some salary support from the NIHR 
Biomedical Research Center at GOSH. PL is also a senior center affiliate at the 
Center for Adolescent Rheumatology Versus Arthritis at University College London 
(UCL) University College London Hospitals (UCLH), and GOSH, which is supported 
by Versus Arthritis (21593).


334. Comput Biol Med. 2025 Sep;195:110681. doi: 10.1016/j.compbiomed.2025.110681. 
Epub 2025 Jun 29.

A novel speech signal feature extraction technique to detect speech impairment 
in children accurately.

Manoswini M(1), Sahoo B(1), Swetapadma A(2).

Author information:
(1)School of Computer Engineering, KIIT Deemed to Be University, Bhubaneswar, 
751024, India.
(2)School of Computer Engineering, KIIT Deemed to Be University, Bhubaneswar, 
751024, India. Electronic address: aleena.swetapadmafcs@kiit.ac.in.

Speech signal processing and extracting useful information from speech signal is 
necessary for speech language impairment (SLI) detection in children. Although 
different features has been suggested for SLI detection, there is still a scope 
exist for exploration of other methods. A comparative study of different 
techniques for feature extraction can be done to find the optimal feature 
extraction technique. In this work, a study has been carried out to obtain 
optimal feature extraction technique for SLI detection. Inputs used for SLI 
detection here are the speech signals recorded from children. Features are first 
extracted from the recorded speech signals using various feature extraction 
techniques. The feature extraction techniques that has been implemented are 
relative spectral transform - perceptual linear prediction (RASTA), wavelet 
packet transform (WPT), linear predictive coding (LPC), perceptual linear 
prediction (PLP), Mel-Frequency cepstral coefficients (MFCC), complex 
quantization cepstral coefficient (CQCC), perceptual noise cepstral coefficients 
(PNCC). The features extracted are then given to deep learning models namely 
transformer, temporal convolutional networks (TCN) and TabNet for SLI detection. 
The result obtained has highest accuracy of 100.00 % using PNCC feature combined 
with TabNet method. The novelty of the method is that the PNCC features has not 
been suggested for SLI detection previously. The proposed method can be used for 
speech impairment detection and monitoring by therapist and doctors.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.compbiomed.2025.110681
PMID: 40587935 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest Authors do not 
have any competing interest.


335. PLoS One. 2025 Jun 30;20(6):e0326966. doi: 10.1371/journal.pone.0326966. 
eCollection 2025.

Enhancing student success prediction in higher education with swarm optimized 
enhanced efficientNet attention mechanism.

Alazmi M(1), Ayub N(2).

Author information:
(1)College of Computer Science and Engineering, University of Ha'il, Ha'il, 
Saudi Arabia.
(2)Department of Creative Technoloiges, Air University Islamabad, Islamabad, 
Pakistan.

Predicting student performance is crucial for providing personalized support and 
enhancing academic performance. Advanced machine-learning approaches are being 
used to understand student performance variables as educational data grows. A 
big dataset from several Chinese institutions and high schools is used to 
develop a credible student performance prediction technique. Moreover, the 
dataset includes 80 features and 200,000 records, and consequently, it 
represents one of the most extensive data collections available for educational 
research. Initially, data is passed through preprocessing to address outliers 
and missing values. In addition, we developed a novel hybrid feature selection 
model that combined correlation filtering with mutual information, 
Cross-Validation (CV) along with Recursive Feature Eliminatio (RFE) (R, and 
stability selection to identify the most impactful features. Moreover, This 
study develops the proposed EffiXNet, a more refined version of EfficientNet 
augmented with self-attention mechanisms, dynamic convolutions, improved 
normalization methods, and Sparrow Search Optimization Algorithm for 
hyperparameter optimization. The developed model was tested using an 80/20 
train-test split, where 160,000 records were used for training and 40,000 for 
testing. The results reported, including accuracy, precision, recall, and 
F1-score, are based on the full test dataset. However, for better visualization, 
the confusion matrices display only a representative subset of test results. 
Furthermore, the EffiXNet value of AUC amounting to 0.99, a 25% reduction of 
logarithmic loss relative to the baseline models, precision of 97.8%, F1-score 
of 98.1%, and reliable optimization of memory usage. Significantly, the 
developed model showed a consistently high-performance level demonstrated by 
various metrics, which indicates that it is proficient in capturing intricate 
data patterns. The key insights the current research provides are the necessity 
of early intervention and directed training support in the educational domain. 
The EffiXNet framework offers a robust, scalable, and efficient solution for 
predicting student performance, with potential applications in academic 
institutions worldwide.

Copyright: © 2025 Alazmi, Ayub. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0326966
PMCID: PMC12208463
PMID: 40587447 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


336. Bioimpacts. 2025 Apr 12;15:30849. doi: 10.34172/bi.30849. eCollection 2025.

A hybrid transformer-based approach for early detection of Alzheimer's disease 
using MRI images.

Wu Q(1), Wang Y(2), Zhang X(1), Zhang H(3), Che K(4).

Author information:
(1)Department of Psychiatry, The Third People's Hospital of Lanzhou, Lanzhou 
730030, Gansu, China.
(2)Department of Pediatric Psychiatry, The Third People's Hospital of Lanzhou, 
Lanzhou 730030, Gansu, China.
(3)Department of Traditional Chinese Medicine, The Third People's Hospital of 
Lanzhou, Lanzhou 730030, Gansu, China.
(4)Department of Magnetic Resonance Imaging, The First People's Hospital of 
Lanzhou, Lanzhou 730030, Gansu, China.

INTRODUCTION: Alzheimer's disease (AD) is a progressive neurodegenerative 
disorder that poses significant challenges for early detection. Advanced 
diagnostic methods leveraging machine learning techniques, particularly deep 
learning, have shown great promise in enhancing early AD diagnosis. This paper 
proposes a multimodal approach combining transfer learning, Transformer 
networks, and recurrent neural networks (RNNs) for diagnosing AD, utilizing MRI 
images from multiple perspectives to capture comprehensive features.
METHODS: Our methodology integrates MRI images from three distinct perspectives: 
sagittal, coronal, and axial views, ensuring the capture of rich local and 
global features. Initially, ResNet50 is employed for local feature extraction 
using transfer learning, which improves feature quality while reducing model 
complexity. The extracted features are then processed by a Transformer encoder, 
which incorporates positional embeddings to maintain spatial relationships. 
Finally, 2D convolutional layers combined with LSTM networks are used for 
classification, enabling the model to capture sequential dependencies in the 
data.
RESULTS: The proposed framework was rigorously tested on the Alzheimer's Disease 
Neuroimaging Initiative (ADNI) dataset. Our approach achieved an impressive 
accuracy of 96.92% on test data and 98.12% on validation data, significantly 
outperforming existing methods in the field. The integration of Transformer and 
LSTM models led to enhanced feature representation and improved diagnostic 
performance.
CONCLUSION: This study demonstrates the effectiveness of combining transfer 
learning, Transformer networks, and LSTMs for AD diagnosis. The proposed 
framework provides a comprehensive analysis that improves classification 
accuracy, offering a valuable tool for early detection and intervention in 
clinical practice. These findings highlight the potential for advancing 
neuroimaging analysis and supporting future research in AD diagnostics.

© 2025 The Author(s).

DOI: 10.34172/bi.30849
PMCID: PMC12204811
PMID: 40584904

Conflict of interest statement: Authors declare no conflict of interests.


337. Front Med (Lausanne). 2025 Jun 13;12:1565275. doi: 10.3389/fmed.2025.1565275. 
eCollection 2025.

Standardized clinical assessments and advanced AI-driven instruments used to 
evaluate neurofunctional deficits, including within biomarker based framework, 
in Parkinson's disease - human intelligence made vs. AI models - systematic 
review.

Anghelescu A(#)(1)(2), Munteanu C(#)(2)(3), Spinu A(4), Ciobanu V(5), Popescu 
C(4), Cioca IE(6), Andone I(4), Stoica SI(1)(2), Mandu M(4), Rebedea A(2), 
Giuvara S(2), Malaelea AD(2), Vladulescu-Trandafir AI(4), Morcov MV(6)(7), Onose 
G(4).

Author information:
(1)Department of Specific Discipline, Faculty of Midwifery and Nursing, 
University of Medicine and Pharmacy "Carol Davila", Bucharest, Romania.
(2)Teaching Emergency Hospital "Bagdasar-Arseni", Bucharest, Romania.
(3)Department of Biomedical Sciences, Faculty of Medical Bioengineering, 
University of Medicine and Pharmacy "Grigore T. Popa", Iaşi, Romania.
(4)Department of Clinical Education, Physical and Rehabilitation Medicine, 
Faculty of Medicine, University of Medicine and Pharmacy "Carol Davila", 
Bucharest, Romania.
(5)Department of Computer Science, Politehnica University of Bucharest, 
Bucharest, Romania.
(6)Department of Basic Medical Sciences, Faculty of Midwifery and Nursing, 
University of Medicine and Pharmacy "Carol Davila", Bucharest, Romania.
(7)National Teaching Centre for Neuro-psycho-motor Rehabilitation in Children 
"Dr. N. Robanescu", Bucharest, Romania.
(#)Contributed equally

Erratum in
    Front Med (Lausanne). 2025 Jul 14;12:1649641. doi: 
10.3389/fmed.2025.1649641.

INTRODUCTION: Considering the extensive development of artificial intelligence 
(AI) facilities, like Generative Pre-Trained Transformer (ChatGPT) 4.o and 
ChatGPT Scholar, we explored their abilities to conduct a systematic literature 
review. Using as a specific domain, an attempt to frame/methodize clinical 
assessment instruments used to evaluate neuro-functional deficits in Parkinson's 
disease (PD) - including framed through the ICF(-DH) paradigm - for the 
above-mentioned comparison between human intelligence (HI) and AI, this paper is 
as well, a follow-up regarding the most actual subject matter of the AI's 
capabilities evolution in this respect. As well-known 
clinical-/paraclinical-/functional evaluations, using assessment quantitative 
(as much as possible) instruments, are basic endeavors for rehabilitation, as 
they enable setting of appropriate and realistic therapeutic-rehabilitative 
specific goals.
METHODS: Within the actual work, we have first achieved a narrative synthesis of 
the main molecular mechanisms involved in PD pathophysiology, underpinning its 
clinical appearance and evolution. To fundament our knowledge on an up-to-date 
information regarding the clinical-functional evaluation tools practiced in PD, 
we systematically reviewed the literature in this domain, published in the last 
6 years, through a PRISMA type method for filtering/selecting the related 
bibliographic resources. The same keywords combinations/syntaxes have been used 
contextually, also to dialogize with ChatGPT4.o and ChatGPT.
RESULTS: Scholar Applying PRISMA type methodology (HI achieved), we have 
selected, matching the filtering criteria, 24 articles. Interrogating the two AI 
above-mentioned models, we obtained quite difficult to be availed/useful - 
comparative to our HI obtained - outcomes. Thus, when interrogating ChatGPT4.o, 
ChatGPT Scholar repeatedly, they provided - partially diverse - inappropriate 
related answers, including ones pending on the interrogator's IP, although they 
claimed to have this capacity.
DISCUSSION: We consider, regarding their capabilities to achieve systematic 
literature reviews, that neither ChatGPT 4.o nor ChatGPT Scholar still cannot 
succeed this (yet, they keep improving lately). Additionally, we have 
consistently extended, including within a narrative related literature review, 
our 'dialogue" with these two AI facilities regarding their availability to 
enhance the related evaluation instruments accuracy on neurofunctional 
assessments within biomarker-based frameworks. So, our research aimed basically 
to emphasize the main topical data regarding these two important paradigms of 
knowledge (based on HI and on AI) acquirements - considering the impetuous 
development of the latter - and thus, possibly to contribute inclusively at 
improving the actual performances to achieve Systematic Literature Reviews 
through the PRISMA type method - for the moment still better served by HI.

Copyright © 2025 Anghelescu, Munteanu, Spinu, Ciobanu, Popescu, Cioca, Andone, 
Stoica, Mandu, Rebedea, Giuvara, Malaelea, Vladulescu-Trandafir, Morcov and 
Onose.

DOI: 10.3389/fmed.2025.1565275
PMCID: PMC12202485
PMID: 40584716

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


338. Entropy (Basel). 2025 Jun 9;27(6):614. doi: 10.3390/e27060614.

LSE-CVCNet: A Generalized Stereoscopic Matching Network Based on Local 
Structural Entropy and Multi-Scale Fusion.

Yang W(1)(2), Zhao Y(1)(3), Gu Y(4), Huang L(1), Li J(1), Zhao J(1)(5).

Author information:
(1)State Key Laboratory of Public Big Data, College of Computer Science and 
Technology, Guizhou University, Guiyang 550025, China.
(2)School of Mathematical Sciences, Minzu Normal University of Xingyi, Xingyi 
562400, China.
(3)Electronic and Computer Engineering School, Shenzhen Graduate School of 
Peking University, Shenzhen 518055, China.
(4)College of Big Data and Internet, Shenzhen Technology University, Shenzhen 
518000, China.
(5)School of Mathematics and Big Data, Guizhou Education University, Guiyang 
550000, China.

This study presents LSE-CVCNet, a novel stereo matching network designed to 
resolve challenges in dynamic scenes, including dynamic feature misalignment 
caused by texture variability and contextual ambiguity from occlusions. By 
integrating three key innovations-local structural entropy (LSE) to quantify 
structural uncertainty in disparity maps and guide adaptive attention, a 
cross-image attention mechanism (CIAM-T) to asymmetrically extract features from 
left/right images for improved feature alignment, and multi-resolution cost 
volume fusion (MRCV-F) to preserve fine-grained details through multi-scale 
fusion-LSE-CVCNet enhances disparity estimation accuracy and cross-domain 
generalization. The experimental results demonstrate robustness under varying 
lighting, occlusions, and complex geometries, outperforming state-of-the-art 
methods across multiple data sets. Ablation studies validate each module's 
contribution, while cross-domain tests confirm generalization in unseen 
scenarios. This work establishes a new paradigm for adaptive stereo matching in 
dynamic environments.

DOI: 10.3390/e27060614
PMCID: PMC12191594
PMID: 40566201

Conflict of interest statement: The authors declare no potential conflicts of 
interests.


339. Diagnostics (Basel). 2025 Jun 6;15(12):1444. doi: 10.3390/diagnostics15121444.

A Hybrid Artificial Intelligence Approach for Down Syndrome Risk Prediction in 
First Trimester Screening.

Yalçın E(1), Aslan S(2), Toğaçar M(3), Demir SC(1).

Author information:
(1)Department of Obstetrics and Gynecology, Division of Perinatology, Cukurova 
University School of Medicine, 01330 Adana, Turkey.
(2)Department of Computer Engineering, Faculty of Engineering and Natural 
Sciences, Malatya Turgut Ozal University, 44210 Malatya, Turkey.
(3)Department of Management Information Systems, Faculty of Economics and 
Administrative Sciences, Fırat University, 23119 Elazig, Turkey.

Background/Objectives: The aim of this study is to develop a hybrid artificial 
intelligence (AI) approach to improve the accuracy, efficiency, and reliability 
of Down Syndrome (DS) risk prediction during first trimester prenatal screening. 
The proposed method transforms one-dimensional (1D) patient data-including 
features such as nuchal translucency (NT), human chorionic gonadotropin (hCG), 
and pregnancy-associated plasma protein A (PAPP-A)-into two-dimensional (2D) 
Aztec barcode images, enabling advanced feature extraction using 
transformer-based deep learning models. Methods: The dataset consists of 958 
anonymous patient records. Each record includes four first trimester screening 
markers, hCG, PAPP-A, and NT, expressed as multiples of the median. The DS risk 
outcome was categorized into three classes: high, medium, and low. Three 
transformer architectures-DeiT3, MaxViT, and Swin-are employed to extract 
high-level features from the generated barcodes. The extracted features are 
combined into a unified set, and dimensionality reduction is performed using two 
feature selection techniques: minimum Redundancy Maximum Relevance (mRMR) and 
RelieF. Intersecting features from both selectors are retained to form a compact 
and informative feature subset. The final features are classified using machine 
learning algorithms, including Bagged Trees and Naive Bayes. Results: The 
proposed approach achieved up to 100% classification accuracy using the Naive 
Bayes classifier with 1250 features selected by RelieF and 527 intersecting 
features from mRMR. By selecting a smaller but more informative subset of 
features, the system significantly reduced hardware and processing demands while 
maintaining strong predictive performance. Conclusions: The results suggest that 
the proposed hybrid AI method offers a promising and resource-efficient solution 
for DS risk assessment in first trimester screening. However, further 
comparative studies are recommended to validate its performance in broader 
clinical contexts.

DOI: 10.3390/diagnostics15121444
PMCID: PMC12192227
PMID: 40564765

Conflict of interest statement: The authors declare that there is no conflict of 
interest related to this paper.


340. Acta Radiol. 2025 Nov;66(11):1141-1148. doi: 10.1177/02841851251349495. Epub 
2025 Jun 23.

Fine-tuned large language model for classifying CT-guided interventional 
radiology reports.

Yasaka K(1), Nishimura N(1), Fukushima T(1), Kubo T(1), Kiryu S(2), Abe O(1).

Author information:
(1)Department of Radiology, The University of Tokyo Hospital, Tokyo, Japan.
(2)Department of Radiology, International University of Health and Welfare 
Narita Hospital, Chiba, Japan.

BackgroundManual data curation was necessary to extract radiology reports due to 
the ambiguities of natural language.PurposeTo develop a fine-tuned large 
language model that classifies computed tomography (CT)-guided interventional 
radiology reports into technique categories and to compare its performance with 
that of the readers.Material and MethodsThis retrospective study included 
patients who underwent CT-guided interventional radiology between August 2008 
and November 2024. Patients were chronologically assigned to the training 
(n = 1142; 646 men; mean age = 64.1 ± 15.7 years), validation (n = 131; 83 men; 
mean age = 66.1 ± 16.1 years), and test (n = 332; 196 men; mean age = 
66.1 ± 14.8 years) datasets. In establishing a reference standard, reports were 
manually classified into categories 1 (drainage), 2 (lesion biopsy within fat or 
soft tissue density tissues), 3 (lung biopsy), and 4 (bone biopsy). The 
bi-directional encoder representation from the transformers model was fine-tuned 
with the training dataset, and the model with the best performance in the 
validation dataset was selected. The performance and required time for 
classification in the test dataset were compared between the best-performing 
model and the two readers.ResultsCategories 1/2/3/4 included 309/367/270/196, 
30/42/40/19, and 75/124/78/55 patients for the training, validation, and test 
datasets, respectively. The model demonstrated an accuracy of 0.979 in the test 
dataset, which was significantly better than that of the readers (0.922-0.940) 
(P ≤0.012). The model classified reports within a 49.8-53.5-fold shorter time 
compared to readers.ConclusionThe fine-tuned large language model classified 
CT-guided interventional radiology reports into four categories demonstrating 
high accuracy within a remarkably short time.

DOI: 10.1177/02841851251349495
PMID: 40551586 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of conflicting interestsThe authors 
declared no potential conflicts of interest with respect to the research, 
authorship, and/or publication of this article.


341. Comput Biol Med. 2025 Sep;195:110518. doi: 10.1016/j.compbiomed.2025.110518. 
Epub 2025 Jun 22.

Detection of pre-ictal epileptic events using a self-attention based neural 
network from raw Neonatal EEG data.

Tejaswi K(1), Vikas M(1), Praharsha H(1), Mandal P(1), Chakraborty S(1), 
Wolkenhauer O(2), Bej S(3).

Author information:
(1)Indian Institute of Science Education and Research, Thiruvananthapuram, 
India.
(2)Institute of Computer Science, University of Rostock, Germany; 
Leibniz-Institute for Food Systems Biology, Technical University of Munich, 
Freising, Germany; Stellenbosch Institute for Advanced Study, South Africa.
(3)Indian Institute of Science Education and Research, Thiruvananthapuram, 
India. Electronic address: sbej7042@iisertvm.ac.in.

Epileptic seizures can occur unpredictably, making real-time monitoring and 
early warning systems critical, especially in neonatal patients, where timely 
intervention can significantly improve outcomes. Neonatal seizures are often 
subtle and difficult to detect, increasing the need for automated, early 
prediction methods to aid clinical decision-making. While machine learning 
models have been widely used for seizure detection, their application in 
preemptive seizure warning remains underexplored. In this study, we propose a 
self-attention-based neural network that processes raw EEG data to detect 
pre-ictal signals, enabling early seizure prediction. A key challenge in using 
attention mechanisms for EEG analysis is the computational burden of handling 
high-frequency, long-duration signals. To address this, we introduce a 
second-wise summary statistics-based embedding that significantly reduces the 
input sequence length while retaining essential features. We validate our model 
using a publicly available dataset of 79 neonatal patients with 
physician-annotated EEG recordings. Our classifier achieves a maximum accuracy 
of 91 percent in distinguishing pre-ictal and ictal events from non-ictal 
signals. Notably, we evaluate our model on completely unseen patients, 
demonstrating its potential for real-world applicability in neonatal seizure 
prediction. This study provides a proof-of-concept for a preemptive seizure 
warning system, paving the way for AI-driven neonatal epilepsy management and 
broader clinical applications in seizure detection.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.compbiomed.2025.110518
PMID: 40550202 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest There are no 
competing interests among the authors


342. J Med Internet Res. 2025 Jun 23;27:e72398. doi: 10.2196/72398.

AI in Medical Questionnaires: Scoping Review.

Luo X(#)(1), Li Y(#)(1), Xu J(#)(1), Zheng Z(1), Ying F(2)(3), Huang G(1)(4).

Author information:
(1)Faculty of Humanities and Arts, Macau University of Science and Technology, 
Macau, China.
(2)Industrial and Manufacturing Engineering, European Academy of Engineering, 
Gothenburg, Sweden.
(3)College of Computer Science and Technology, Zhejiang University, Hangzhou, 
China.
(4)Zhuhai M.U.S.T. Science and Technology Research Institute, Zhuhai, China.
(#)Contributed equally

Erratum in
    J Med Internet Res. 2025 Aug 11;27:e80644. doi: 10.2196/80644.

This systematic review aimed to explore the current applications, potential 
benefits, and issues of artificial intelligence (AI) in medical questionnaires, 
focusing on its role in 3 main functions: assessment, development, and 
prediction. The global mental health burden remains severe. The World Health 
Organization reports that >1 billion people worldwide experience mental 
disorders, with the prevalence of depression and anxiety among children and 
adolescents at 2.6% and 6.5%, respectively. However, commonly used clinical 
questionnaires such as the Hamilton Depression Rating Scale and the Beck 
Depression Inventory suffer from several problems, including the high degree of 
overlap of symptoms of depression with those of other psychiatric disorders and 
a lack of professional supervision during administration of the questionnaires, 
which often lead to inaccurate diagnoses. In the wake of the COVID-19 pandemic, 
the health care system is facing the dual challenges of a surge in patient 
numbers and the complexity of mental health issues. AI technology has now been 
shown to have great promise in improving diagnostic accuracy, assisting clinical 
decision-making, and simplifying questionnaire development and data analysis. To 
systematically assess the value of AI in medical questionnaires, this study 
searched 5 databases (PubMed, Embase, Cochrane Library, Web of Science, and 
China National Knowledge Infrastructure) for the period from database inception 
to September 2024. Of 49,091 publications, a total of 14 (0.03%) studies met the 
inclusion criteria. AI technologies showed significant advantages in assessment, 
such as distinguishing myalgic encephalomyelitis or chronic fatigue syndrome 
from long COVID-19 with 92.18% accuracy. In questionnaire development, natural 
language processing using generative models such as ChatGPT was used to 
construct culturally competent scales. In terms of disease prediction, one study 
had an area under the curve of 0.790 for cataract surgery risk prediction. 
Overall, 24 AI technologies were identified, covering traditional algorithms 
such as random forest, support vector machine, and k-nearest neighbor, as well 
as deep learning models such as convolutional neural networks, Bidirectional 
Encoder Representations From Transformers, and ChatGPT. Despite the positive 
findings, only 21% (3/14) of the studies had entered the clinical validation 
phase, whereas the remaining 79% (11/14) were still in the exploratory phase of 
research. Most of the studies (10/14, 71%) were rated as being of moderate 
methodological quality, with major limitations including lack of a control 
group, incomplete follow-up data, and inadequate validation systems. In summary, 
the integrated application of AI in medical questionnaires has significant 
potential to improve diagnostic efficiency, accelerate scale development, and 
promote early intervention. Future research should pay more attention to model 
interpretability, system compatibility, validation standardization, and ethical 
governance to effectively address key challenges such as data privacy, clinical 
integration, and transparency.
BACKGROUND: The World Health Organization reports that >1 billion people 
worldwide experience mental disorders, with the prevalence of depression and 
anxiety among children and adolescents at 2.6% and 6.5%, respectively. However, 
commonly used clinical questionnaires such as the Hamilton Depression Rating 
Scale and the Beck Depression Inventory suffer from several problems, including 
the high degree of overlap of symptoms of depression with those of other 
psychiatric disorders and a lack of professional supervision during 
administration of the questionnaires, which often lead to inaccurate diagnoses. 
In the wake of the COVID-19 pandemic, the health care system is facing the dual 
challenges of a surge in patient numbers and the complexity of mental health 
issues. Artificial Intelligence (AI) technology has now been shown to have great 
promise in improving diagnostic accuracy, assisting clinical decision-making, 
and simplifying questionnaire development and data analysis.
OBJECTIVE: This review aimed to explore the current applications, potential 
benefits, and issues of AI in medical questionnaires, focusing on its role in 3 
main functions: assessment, development, and prediction. The global mental 
health burden remains severe.
METHODS: The review included peer-reviewed studies that applied AI technologies 
to medical, psychological, or physiological questionnaires and reported 
measurable outcomes; non–peer-reviewed, non-English/Chinese, ethically 
noncompliant, or AI-unrelated studies were excluded. Five databases (PubMed, 
Embase, Cochrane Library, Web of Science, and CNKI) were searched from inception 
through September 2024. Three independent reviewers conducted data extraction, 
quality appraisal using the Joanna Briggs Institute tools, and narrative 
synthesis of AI applications across questionnaire assessment, development, and 
prediction tasks.
RESULTS: Of 49,091 publications, a total of 14 (0.03%) studies met the inclusion 
criteria. AI technologies showed advantages in assessment, such as 
distinguishing myalgic encephalomyelitis or chronic fatigue syndrome from long 
COVID-19 with 92.18% accuracy. In questionnaire development, natural language 
processing using generative models such as ChatGPT was used to construct 
culturally competent scales. In terms of disease prediction, one study had an 
area under the curve of 0.790 for cataract surgery risk prediction. Overall, 24 
AI technologies were identified, covering traditional algorithms such as random 
forest, support vector machine, and k-nearest neighbor, as well as deep learning 
models such as convolutional neural networks, Bidirectional Encoder 
Representations From Transformers, and ChatGPT. Despite the positive findings, 
only 21% (3/14) of the studies had entered the clinical validation phase, 
whereas the remaining 79% (11/14) were still in the exploratory phase of 
research. Most of the studies (10/14, 71%) were rated as being of moderate 
methodological quality, with major limitations including lack of a control 
group, incomplete follow-up data, and inadequate validation systems.
CONCLUSIONS: In summary, the integrated application of AI in medical 
questionnaires has significant potential to improve diagnostic efficiency, 
accelerate scale development, and promote early intervention. Future research 
should pay more attention to model interpretability, system compatibility, 
validation standardization, and ethical governance to effectively address key 
challenges such as data privacy, clinical integration, and transparency.

©Xuexing Luo, Yiyuan Li, Jing Xu, Zhong Zheng, Fangtian Ying, Guanghui Huang. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 23.06.2025.

DOI: 10.2196/72398
PMCID: PMC12235208
PMID: 40549427 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


343. Front Psychiatry. 2025 Jun 6;16:1585702. doi: 10.3389/fpsyt.2025.1585702. 
eCollection 2025.

The effect of Artificial Intelligence Health Education Accurately Linking System 
on childhood asthma: study protocol for a pilot randomized controlled trial.

Wang HF(#)(1), Li YH(#)(2), Zhang Q(3), Han L(3), Wang L(3), Zhang L(4), Bai 
X(1), Cheng M(1), Zhang T(1), Zhao F(1), Li H(#)(5), Wang XY(#)(6).

Author information:
(1)Nursing Department, Inner Mongolia Maternal and Child Health Care Hospital, 
Hohhot, China.
(2)College of Education, Chengdu College of Arts and Sciences, Chengdu, China.
(3)Pediatric Internal Medicine Department, Inner Mongolia Maternal and Child 
Health Care Hospital, Hohhot, China.
(4)Pediatric Outpatient Department, Inner Mongolia Maternal and Child Health 
Care Hospital, Hohhot, China.
(5)Nursing Department, Shandong Provincial Hospital Affiliated to Shandong First 
Medical University, Jinan, China.
(6)Pediatric Department, Inner Mongolia Maternal and Child Health Care Hospital, 
Hohhot, China.
(#)Contributed equally

BACKGROUND: Childhood asthma is a prevalent chronic disease that affects 
millions of children worldwide. Managing this disease demands not only medical 
treatment but also the long-term self-management efforts of both children and 
their parents. Conventional self-management education typically depends on 
face-to-face approaches, often failing to take into account the personalized 
requirements and ongoing support needed. Nevertheless, with the evolution of 
artificial intelligence (AI) technology, fresh prospects have emerged to boost 
the effectiveness of self-management for childhood asthma. Based on it, we have 
designed an AI Health Education Accurately Linkage System (AI-HEALS) to explore 
whether AI-driven interventions can improve self-management capabilities of 
families with asthmatic children, thereby helping them control the disease and 
reduce medical costs.
METHODS: This research is a pilot single-blind randomized controlled trial (RCT) 
intended to gauge the efficacy of the AI-HEALS intervention delivered via the 
WeChat platform in enhancing the self-management abilities of families with 
asthmatic children. Participants will be recruited from eligible families whose 
children have been diagnosed with asthma and randomly allocated to either the 
intervention group or the control group. The control group will receive standard 
treatment, whereas the intervention group will receive both standard treatment 
and the AI-HEALS intervention. The intervention includes an AI-enabled, 
voice-activated interactive question-and-answer system, as well as monitoring 
and recording of physiological indicators, regular reminders, and customized 
educational articles. All components of the intervention will mainly be provided 
through a WeChat official account named "Children's Asthma Health Management 
Expert." AI-HEALS will construct its knowledge base according to pediatric 
asthma treatment guidelines to enhance the accuracy and reliability of the 
information it offers. The primary outcome measure is the alteration in asthma 
symptom control levels, while secondary outcomes comprise a variety of other 
physiological indicators related to asthma, parents' self-management behaviors, 
and mental health conditions.
DISCUSSION: This study combines AI and mobile health technology to develop the 
AI-HEALS system, with the aim of assisting families of children with asthma in 
controlling the disease symptoms. The primary objective is to evaluate whether 
the intervention can improve asthma symptom control.
CLINICAL TRIAL REGISTRATION: The study is scheduled to begin in April 2025 and 
is expected to conclude in December 2026. This research protocol is the first 
version and was registered with the China Clinical Trial Registration Center on 
February 14, 2025 (Registration Number: ChiCTR2500097233).

Copyright © 2025 Wang, Li, Zhang, Han, Wang, Zhang, Bai, Cheng, Zhang, Zhao, Li 
and Wang.

DOI: 10.3389/fpsyt.2025.1585702
PMCID: PMC12179151
PMID: 40547124

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


344. Lancet Digit Health. 2025 Jun;7(6):100869. doi: 10.1016/j.landig.2025.03.001. 
Epub 2025 Jun 20.

Artificial intelligence-assisted detection of nasopharyngeal carcinoma on 
endoscopic images: a national, multicentre, model development and validation 
study.

Shi Y(1), Li Z(2), Wang L(1), Wang H(3), Liu X(4), Gu D(5), Chen X(1), Liu X(1), 
Gong W(1), Jiang X(6), Li W(7), Lin Y(8), Liu K(9), Luo D(10), Peng T(11), Peng 
X(12), Tong M(13), Zheng H(14), Zhou X(15), Wu J(16), El Fakhri G(4), Chang 
M(17), Liao J(18), Li J(19), Wang D(20), Ye J(21), Qu S(22), Jiang W(23), Liu 
Q(24), Sun X(24), Zheng Y(25), Yu H(26).

Author information:
(1)ENT Institute and Department of Otorhinolaryngology, Eye and ENT Hospital, 
Fudan University, Shanghai, China.
(2)ENT Institute and Department of Otorhinolaryngology, Eye and ENT Hospital, 
Fudan University, Shanghai, China; Department of Radiology and Biomedical 
Imaging, Yale University, New Haven, CT, USA.
(3)School of Life Science and Technology, Xi'an Jiaotong University, Xi'an, 
China; Jarvis Research Center, Tencent YouTu Lab, Shenzhen, China.
(4)Department of Radiology and Biomedical Imaging, Yale University, New Haven, 
CT, USA.
(5)Institute of Otorhinolaryngology, Clinical Research Center, Eye and ENT 
Hospital, Fudan University, Shanghai, China.
(6)Department of Otorhinolaryngology, Huazhong University of Science and 
Technology Union Shenzhen Hospital, Shenzhen, China.
(7)Department of Otorhinolaryngology Head and Neck Surgery, The Second 
Affiliated Hospital of Soochow University, Suzhou, China.
(8)Department of Otorhinolaryngology, Affiliated Xiaoshan Hospital, Hangzhou 
Normal University, Hangzhou, China.
(9)Department of Otorhinolaryngology Head and Neck Surgery, The Second 
Affiliated Hospital of Nanchang University, Nanchang, China.
(10)Department of Otorhinolaryngology Head and Neck Surgery, Nanchong Central 
Hospital, Nanchong, Sichuan, China.
(11)Department of Otorhinolaryngology Head and Neck Surgery, Hunan Provincial 
People's Hospital, First Affiliated Hospital of Hunan Normal University, 
Changsha, China.
(12)Department of Otorhinolaryngology Head and Neck Surgery, Zigong Fourth 
People's Hospital, Zigong, China.
(13)Department of Otolaryngology Head and Neck Surgery, Shaoxing City Yuecheng 
District People's Hospital, Shaoxing, China.
(14)Department of Otolaryngology Head and Neck Surgery, Wenzhou People's 
Hospital, Wenzhou, China.
(15)Department of Otorhinolaryngology Head and Neck Surgery, Shandong Provincial 
Hospital affiliated to Shandong First Medical University, Jinan, Shandong.
(16)Tencent Healthcare, Shenzhen, China.
(17)Department of Otorhinolaryngology Head and Neck Surgery, The People's 
Hospital of Jiangmen, Jiangmen, China.
(18)Department of Otorhinolaryngology Head and Neck Surgery, Quanzhou First 
Hospital Affiliated to Fujian Medical University, Quanzhou, China.
(19)Department of Otorhinolaryngology, Head and Neck Surgery, First Affiliated 
Hospital of Guangxi Medical University, Nanning, China.
(20)Department of Otorhinolaryngology, Head and Neck Surgery, Fujian Medical 
University Union Hospital, Fuzhou, Fujian, China.
(21)Department of Otorhinolaryngology, Head and Neck Surgery, The First 
Affiliated Hospital of Nanchang University, Nanchang, China.
(22)Department of Otorhinolaryngology, The People's Hospital of Guangxi Zhuang 
Autonomous Region, Nanning, China.
(23)Department of Otorhinolaryngology Head and Neck Surgery, Xiangya Hospital, 
Central South University, Changsha, China.
(24)ENT Institute and Department of Otorhinolaryngology, Eye and ENT Hospital, 
Fudan University, Shanghai, China; Research Units of New Technologies of 
Endoscopic Surgery in Skull Base Tumor, Chinese Academy of Medical Sciences, 
Shanghai, China.
(25)Jarvis Research Center, Tencent YouTu Lab, Shenzhen, China; Medical 
Artificial Intelligence Lab, Westlake University, Hangzhou, China. Electronic 
address: yefengzheng@tencent.com.
(26)ENT Institute and Department of Otorhinolaryngology, Eye and ENT Hospital, 
Fudan University, Shanghai, China; Research Units of New Technologies of 
Endoscopic Surgery in Skull Base Tumor, Chinese Academy of Medical Sciences, 
Shanghai, China. Electronic address: hongmengyush@163.com.

BACKGROUND: Nasopharyngeal carcinoma is highly curable when diagnosed early. 
However, the nasopharynx's obscure anatomical position and the similarity of 
local imaging manifestations with those of other nasopharyngeal diseases often 
lead to diagnostic challenges, resulting in delayed or missed diagnoses. Our aim 
was to develop a deep learning algorithm to enhance an otolaryngologist's 
diagnostic capabilities by differentiating between nasopharyngeal carcinoma, 
benign hyperplasia, and normal nasopharynx during endoscopic examination.
METHODS: In this national, multicentre, model development and validation study, 
we developed a Swin Transformer-based Nasopharyngeal Diagnostic (STND) system to 
identify nasopharyngeal carcinoma, benign hyperplasia, and normal nasopharynx. 
STND was developed with 27 362 nasopharyngeal endoscopic images (10 693 
biopsy-proven nasopharyngeal carcinoma, 7073 biopsy-proven benign hyperplasia, 
and 9596 normal nasopharynx) sourced from eight prominent nasopharyngeal 
carcinoma centres (stage 1), and externally validated with 1885 prospectively 
acquired images from ten comprehensive hospitals with a high incidence of 
nasopharyngeal carcinoma (stage 2). Furthermore, we did a fully crossed, 
multireader, multicase study involving four expert otolaryngologists from four 
regional leading nasopharyngeal carcinoma centres, and 24 general 
otolaryngologists from 24 geographically diverse primary hospitals. This study 
included 400 images to evaluate the diagnostic capabilities of the experts and 
general otolaryngologists both with and without the aid of the STND system in a 
real-world environment.
FINDINGS: Endoscopic images used in the internal study (Jan 1, 2017, to Jan 31, 
2023) were from 15 521 individuals (9033 [58·2%] men and 6488 [41·8%] women; 
mean age 47·6 years [IQR 38·4-56·8]). Images from 945 participants (538 [56·9%] 
men and 407 [43·1%] women; mean age 45·2 years [IQR 35·2- 55·2]) were used in 
the external validation. STND in the internal dataset discriminated normal 
nasopharynx images from abnormalities (benign hyperplasia and nasopharyngeal 
carcinoma) with an area under the curve (AUC) of 0·99 (95% CI 0·99-0·99) and 
malignant images (ie, nasopharyngeal carcinoma) from non-malignant images (ie, 
benign hyperplasia and normal nasopharynx) with an AUC of 0·99 (95% CI 
0·98-0·99). In the external validation, the system had an AUC for the detection 
of nasopharyngeal carcinoma of 0·95 (95% CI 0·94-0·96), a sensitivity of 91·6% 
(95% CI 89·3-93·5), and a specificity of 86·1% (95% CI 84·1-87·9). In the 
multireader, multicase study, the artificial intelligence (AI)-assisted strategy 
enhanced otolaryngologists' diagnostic accuracy by 7·9%, increasing from 83·4% 
(95% CI 80·1-86·7, without AI assistance) to 91·2% (95% CI 88·6-93·9, with AI 
assistance; p<0·0001) for primary care otolaryngologists. Reading time per image 
decreased with the aid of the AI model (mean 5·0 s [SD 2·5] vs 6·7 s [6·0] 
without the model; p=0·034).
INTERPRETATION: Our deep learning system has shown significant clinical 
potential for the practical application of nasopharyngeal carcinoma diagnosis 
through endoscopic images in real-world settings. The system offers substantial 
benefits for adoption in primary hospitals, aiming to enhance specificity, avoid 
additional biopsies, and reduce missed diagnoses.
FUNDING: New Technologies of Endoscopic Surgery in Skull Base Tumor: CAMS 
Innovation Fund for Medical Science; Shanghai Science and Technology Committee 
Foundation; Shanghai Municipal Key Clinical Specialty; National Key Clinical 
Specialty Program; and the Young Elite Scientists Sponsorship Program.

Copyright © 2025 The Author(s). Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.landig.2025.03.001
PMID: 40544083 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of interests We declare no competing 
interests.


345. World J Gastroenterol. 2025 Jun 7;31(21):107601. doi: 
10.3748/wjg.v31.i21.107601.

Classification of pediatric video capsule endoscopy images for small bowel 
abnormalities using deep learning models.

Huang YH(1), Lin Q(1), Jin XY(2), Chou CY(3), Wei JJ(1), Xing J(1), Guo HM(1), 
Liu ZF(1), Lu Y(4).

Author information:
(1)Department of Gastroenterology, Children's Hospital of Nanjing Medical 
University, Nanjing 210008, Jiangsu Province, China.
(2)School of Electronic Science and Engineering, Nanjing University, Nanjing 
210023, Jiangsu Province, China.
(3)College of Medicine, National Taiwan University, Taipei 100, Taiwan.
(4)Department of Gastroenterology, Children's Hospital of Nanjing Medical 
University, Nanjing 210008, Jiangsu Province, China. luyan_cpu@163.com.

BACKGROUND: Video capsule endoscopy (VCE) is a noninvasive technique used to 
examine small bowel abnormalities in both adults and children. However, manual 
review of VCE images is time-consuming and labor-intensive, making it crucial to 
develop deep learning methods to assist in image analysis.
AIM: To employ deep learning models for the automatic classification of small 
bowel lesions using pediatric VCE images.
METHODS: We retrospectively analyzed VCE images from 162 pediatric patients who 
underwent VCE between January 2021 and December 2023 at the Children's Hospital 
of Nanjing Medical University. A total of 2298 high-resolution images were 
extracted, including normal mucosa and lesions (erosions/erythema, ulcers, and 
polyps). The images were split into training and test datasets in a 4:1 ratio. 
Four deep learning models: DenseNet121, Visual geometry group-16, ResNet50, and 
vision transformer were trained using 5-fold cross-validation, with 
hyperparameters adjusted for optimal classification performance. The models were 
evaluated based on accuracy, precision, recall, F1-score, and area under the 
receiver operating curve (AU-ROC). Lesion visualization was performed using 
gradient-weighted class activation mapping.
RESULTS: Abdominal pain was the most common indication for VCE, accounting for 
62% of cases, followed by diarrhea, vomiting, and gastrointestinal bleeding. 
Abnormal lesions were detected in 93 children, with 38 diagnosed with 
inflammatory bowel disease. Among the deep learning models, DenseNet121 and 
ResNet50 demonstrated excellent classification performance, achieving accuracies 
of 90.6% [95% confidence interval (CI): 89.2-92.0] and 90.5% (95%CI: 89.9-91.2), 
respectively. The AU-ROC values for these models were 93.7% (95%CI: 92.9-94.5) 
for DenseNet121 and 93.4% (95%CI: 93.1-93.8) for ResNet50.
CONCLUSION: Our deep learning-based diagnostic tool developed in this study 
effectively classified lesions in pediatric VCE images, contributing to more 
accurate diagnoses and increased diagnostic efficiency.

©The Author(s) 2025. Published by Baishideng Publishing Group Inc. All rights 
reserved.

DOI: 10.3748/wjg.v31.i21.107601
PMCID: PMC12175855
PMID: 40538507 [Indexed for MEDLINE]

Conflict of interest statement: Conflict-of-interest statement: The authors 
declare that they have no conflict of interest.


346. J Surg Educ. 2025 Dec;82(12):103566. doi: 10.1016/j.jsurg.2025.103566. Epub 2025 
Jun 18.

Artificial Intelligence-Assisted Narratives: Analysis of Surgical Residency 
Personal Statements.

Subillaga O(1), Coulter AP(2), Tashjian D(2), Seymour N(2), Hubbs D(2).

Author information:
(1)Department of Surgery, UMass Chan Medical School-Baystate, Springfield, 
Massachusetts. Electronic address: Oswaldo.subillaga@baystatehealth.org.
(2)Department of Surgery, UMass Chan Medical School-Baystate, Springfield, 
Massachusetts.

OBJECTIVE: Personal statements (PSs) express applicants' personal 
characteristics and motivations informing pursuit of a surgical career. 
Generative artificial intelligence (AI) is a revolutionary technology. There are 
currently no data to suggest how and to what extent AI is used in surgical 
residency applications. We examined the prevalence of AI use and applicant pool 
characteristics in PSs submitted to a surgical residency.
DESIGN: PSs from US MD and DO applicants to an academic general surgery program 
were collected for both the 2022-23 and 2023-24 NRMP Match cycles. PSs were 
analyzed using 2 AI-detection tools: GPTZero and Copyleaks. Data were analyzed 
using T-test and Fisher's Exact Test.
SETTING: UMass Chan Medical School-Baystate general surgery residency program in 
Springfield, Massachusetts.
PARTICIPANTS: There were 1332 applications during 2022-23 NRMP Match cycle and 
1221 for 2023-24. After excluding international medical graduates and incomplete 
applications, 1490 PSs were analyzed.
RESULTS: 1490 PS were included (758 [50.9%] for 2022-23; 732 [49.1%] for 
2023-24). Demographic characteristics did not differ between the 2 cycles. 
GPTZero identified AI use in 77 (10.2%) PSs in 2022-23 and 268 (36.6%) in 
2023-24 (p < 0.001). Copyleaks identified AI use in 20 (2.6%) PSs in 2022-23 and 
165 (22.5%) in 2023-24 (p < 0.001). Concordance in AI detection with both tools 
was observed in 13 (1.7% of total PSs) for 2022-23 and 155 (21.2%) for 2023-24 
(p < 0.001). Subgroup analysis of concordance in 2023-24 showed increased 
non-English native language characteristics (38.7% vs 19.6%; p < 0.001), a lower 
average personal statement word count (597.3 vs 645.9; p < 0.001) and shorter 
average sentence (10.0 vs 10.4 words; p < 0.001) in the AI group.
CONCLUSIONS: Although AI-detection tools are imperfect, demonstration of 
increased AI use in personal statement preparation is compelling. Implications 
of AI use in residency applications are unknown, and programs must develop 
policies anticipating ongoing and potentially increased use of AI in the 
upcoming application cycles.

Copyright © 2025 Association of Program Directors in Surgery. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jsurg.2025.103566
PMID: 40537315 [Indexed for MEDLINE]


347. Eur Radiol Exp. 2025 Jun 19;9(1):61. doi: 10.1186/s41747-025-00600-2.

Data extraction from free-text stroke CT reports using GPT-4o and Llama-3.3-70B: 
the impact of annotation guidelines.

Wihl J(1), Rosenkranz E(1), Schramm S(1), Berberich C(1), Griessmair M(1)(2), 
Woźnicki P(3), Pinto F(3), Ziegelmayer S(4), Adams LC(4), Bressem KK(5), 
Kirschke JS(1), Zimmer C(1), Wiestler B(1)(6), Hedderich D(1), Kim SH(7).

Author information:
(1)Department of Diagnostic and Interventional Neuroradiology, TUM University 
Hospital, School of Medicine and Health, Technical University of Munich, Munich, 
Germany.
(2)Department of Diagnostic, Interventional and Pediatric Radiology, Inselspital 
Bern, University of Bern, Bern, Switzerland.
(3)Smart Reporting GmbH, Munich, Germany.
(4)Department of Diagnostic and Interventional Radiology, TUM University 
Hospital, School of Medicine and Health, Technical University of Munich, Munich, 
Germany.
(5)Department of Cardiovascular Radiology and Nuclear Medicine, German Heart 
Center Munich, School of Medicine and Health, Technical University of Munich, 
Munich, Germany.
(6)AI for Image-Guided Diagnosis and Therapy, School of Medicine and Health, 
Technical University of Munich, Munich, Germany.
(7)Department of Diagnostic and Interventional Neuroradiology, TUM University 
Hospital, School of Medicine and Health, Technical University of Munich, Munich, 
Germany. suhwan.kim@tum.de.

BACKGROUND: To evaluate the impact of an annotation guideline on the performance 
of large language models (LLMs) in extracting data from stroke computed 
tomography (CT) reports.
METHODS: The performance of GPT-4o and Llama-3.3-70B in extracting ten imaging 
findings from stroke CT reports was assessed in two datasets from a single 
academic stroke center. Dataset A (n = 200) was a stratified cohort including 
various pathological findings, whereas dataset B (n = 100) was a consecutive 
cohort. Initially, an annotation guideline providing clear data extraction 
instructions was designed based on a review of cases with inter-annotator 
disagreements in dataset A. For each LLM, data extraction was performed under 
two conditions: with the annotation guideline included in the prompt and without 
it.
RESULTS: GPT-4o consistently demonstrated superior performance over 
Llama-3.3-70B under identical conditions, with micro-averaged precision ranging 
from 0.83 to 0.95 for GPT-4o and from 0.65 to 0.86 for Llama-3.3-70B. Across 
both models and both datasets, incorporating the annotation guideline into the 
LLM input resulted in higher precision rates, while recall rates largely 
remained stable. In dataset B, the precision of GPT-4o and Llama-3-70B improved 
from 0.83 to 0.95 and from 0.87 to 0.94, respectively. Overall classification 
performance with and without the annotation guideline was significantly 
different in five out of six conditions.
CONCLUSION: GPT-4o and Llama-3.3-70B show promising performance in extracting 
imaging findings from stroke CT reports, although GPT-4o steadily outperformed 
Llama-3.3-70B. We also provide evidence that well-defined annotation guidelines 
can enhance LLM data extraction accuracy.
RELEVANCE STATEMENT: Annotation guidelines can improve the accuracy of LLMs in 
extracting findings from radiological reports, potentially optimizing data 
extraction for specific downstream applications.
KEY POINTS: LLMs have utility in data extraction from radiology reports, but the 
role of annotation guidelines remains underexplored. Data extraction accuracy 
from stroke CT reports by GPT-4o and Llama-3.3-70B improved when well-defined 
annotation guidelines were incorporated into the model prompt. Well-defined 
annotation guidelines can improve the accuracy of LLMs in extracting imaging 
findings from radiological reports.

© 2025. The Author(s).

DOI: 10.1186/s41747-025-00600-2
PMCID: PMC12179022
PMID: 40536631 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This retrospective study was approved by the Institutional Review 
Board of the TUM. Consent for publication: The need for informed consent was 
waived (2024-125-S-NP, May 29, 2024). Competing interests: FP is a full-time 
employee of Smart Reporting GmbH, a provider of radiology reporting software. PW 
is a consultant for Smart Reporting GmbH.


348. Med Educ. 2025 Dec;59(12):1280-1289. doi: 10.1111/medu.15746. Epub 2025 Jun 18.

The impact of generative AI on health professional education: A systematic 
review in the context of student learning.

Pham TD(1), Karunaratne N(1), Exintaris B(1), Liu D(2), Lay T(1), Yuriev E(1), 
Lim A(1)(3).

Author information:
(1)Faculty of Pharmacy and Pharmaceutical Sciences, Monash University, 
Parkville, VIC, Australia.
(2)Office of the Deputy Vice-Chancellor (Education), The University of Sydney, 
Camperdown, NSW, Australia.
(3)Murdoch Childrens Research Institute, Royal Children's Hospital, Parkville, 
VIC, Australia.

BACKGROUND: Generative Artificial Intelligence (GenAI) is increasingly 
integrated into health professions education (HPE), offering new opportunities 
for student learning. However, current research lacks a comprehensive 
understanding of how HPE students actually use GenAI in practice. Laurillard's 
Conversational Framework outlines six learning types-acquisition, inquiry, 
practice, production, discussion and collaboration-commonly used to categorise 
learning activities supported by conventional and digital technologies. Gaining 
insight into how GenAI aligns with these six learning types could assist HPE 
academics in integrating it more thoughtfully and effectively into teaching and 
learning.
PURPOSE: This systematic review investigates how HPE students utilise GenAI and 
examines how these uses align with Laurillard's six learning types compared to 
conventional and digital technologies.
MATERIAL AND METHODS: A systematic review searching five major databases-ERIC, 
Education Database, Ovid Medline, Ovid Embase and Scopus including articles on 
HPE students' use of GenAI until 15th September 2024. Studies were included if 
they were conducted within formal HPE training programs in HPE and specifically 
mentioned how students interact with GenAI. Data were mapped to the six learning 
modes of the Laurillard's Framework. Study quality was assessed using the 
Medical Education Research Study Quality Instrument (MERSQI).
RESULTS: Thirty-three studies met inclusion criteria. GenAI supported learning 
most frequently in practice (73%), inquiry (70%), production (67%) and 
acquisition (55%). These studies highlight GenAI's varied educational 
applications, from clarifying complex concepts to simulating clinical scenarios 
and generating practice materials. Discussion and collaboration were less 
represented (12% each), suggesting a shift toward more individualised learning 
with GenAI. The findings highlight benefits such as efficiency and 
accessibility, alongside concerns about critical thinking, academic integrity 
and reduced peer interaction.
CONCLUSION: This review has provided insights into HPE students' learning 
aligned with Laurillard's existing six learning types. Although GenAI supports 
personalised and self-directed learning, its role in collaborative modes is 
under-explored.

© 2025 The Author(s). Medical Education published by Association for the Study 
of Medical Education and John Wiley & Sons Ltd.

DOI: 10.1111/medu.15746
PMCID: PMC12686775
PMID: 40533396 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


349. J Med Internet Res. 2025 Jun 18;27:e69929. doi: 10.2196/69929.

Performance of ChatGPT-4o and Four Open-Source Large Language Models in 
Generating Diagnoses Based on China's Rare Disease Catalog: Comparative Study.

Zhong W(1), Liu Y(1), Liu Y(1), Yang K(1), Gao H(1), Yan H(1), Hao W(1), Yan 
Y(#)(1), Yin C(#)(1).

Author information:
(1)Department of Prenatal Diagnosis, Beijing Obstetrics and Gynecology Hospital, 
Capital Medical University, Beijing Maternal and Child Health Care Hospital, No. 
251 Yaojiayuan Road, Chaoyang District, Beijing, China, 8618810963279.
(#)Contributed equally

BACKGROUND: Diagnosing rare diseases remains challenging due to their inherent 
complexity and limited physician knowledge. Large language models (LLMs) offer 
new potential to enhance diagnostic workflows.
OBJECTIVE: This study aimed to evaluate the diagnostic accuracy of ChatGPT-4o 
and 4 open-source LLMs (qwen2.5:7b, Llama3.1:8b, qwen2.5:72b, and Llama3.1:70b) 
for rare diseases, assesses the language effect on diagnostic performance, and 
explore retrieval augmented generation (RAG) and chain-of-thought (CoT) 
reasoning.
METHODS: We extracted clinical manifestations of 121 rare diseases from China's 
inaugural rare disease catalog. ChatGPT-4o generated a primary and 5 
differential diagnoses, while 4 LLMs were assessed in both English and Chinese 
contexts. The lowest-performing model underwent RAG and CoT re-evaluation. 
Diagnostic accuracy was compared via the McNemar test. A survey evaluated 11 
clinicians' familiarity with rare diseases.
RESULTS: ChatGPT-4o demonstrated the highest diagnostic accuracy with 90.1%. 
Language effects varied across models: qwen2.5:7b showed comparable performance 
in Chinese (51.2%) and English (47.9%; χ²1=0.32, P=.57), whereas Llama3.1:8b 
exhibited significantly higher English accuracy (67.8% vs 31.4%; χ²1=40.20, 
P<.001). Among larger models, qwen2.5:72b maintained cross-lingual consistency 
considering the odds ratio (OR; Chinese: 82.6% vs English: 83.5%; OR 0.88, 95% 
CI 0.27-2.76,P=1.000), contrasting with Llama3.1:70b's language-dependent 
variation (Chinese: 80.2% vs English: 90.1%; OR 0.29,95% CI 0.08-0.83, P=.02). 
Cross-model comparisons revealed Llama3.1:8b underperformed qwen2.5:7b in 
Chinese (χ²1=13.22,P<.001) but surpassed it in English (χ²1=13.92,P<.001). No 
significant differences were observed between qwen2.5:72b and Llama3.1:70b 
(English: OR 0.33, P=.08; Chinese: OR 1.5, 95% CI 0.48-5.12,P=.07); qwen2.5:72b 
matched ChatGPT-4o's performance in both languages (English: OR 0.33, P=.08; 
Chinese: OR 0.44, P=.09); Llama3.1:70b mirrored ChatGPT-4o's English accuracy 
(OR 1, P=1.000) but lagged in Chinese (OR 0.33; P=.02). RAG implementation 
enhanced qwen2.5:7b's accuracy to 79.3% (χ²1=31.11, P<.001) with 85.9% retrieval 
precision. The distilled model Deepseek-R1:7b markedly underperformed (9.9% vs 
qwen2.5:7b; χ²1=42.19, P<.001). Clinician surveys revealed significant knowledge 
gaps in rare disease management.
CONCLUSIONS: ChatGPT-4o demonstrated superior diagnostic performance for rare 
diseases. While Llama3.1:8b demonstrates viability for localized deployment in 
resource-constrained English diagnostic workflows, Chinese applications require 
larger models to achieve comparable diagnostic accuracy. This urgency is 
heightened by the release of open-source models like DeepSeek-R1, which may see 
rapid adoption without thorough validation. Successful clinical implementation 
of LLMs requires 3 core elements: model parameterization, user language, and 
pretraining data. The integration of RAG significantly enhanced open-source LLM 
accuracy for rare disease diagnosis, although caution remains warranted for 
low-parameter reasoning models showing substantial performance limitations. We 
recommend hospital IT departments and policymakers prioritize language relevance 
in model selection and consider integrating RAG with curated knowledge bases to 
enhance diagnostic utility in constrained settings, while exercising caution 
with low-parameter models.

© Wei Zhong, YiFan Liu, Yan Liu, Kai Yang, HuiMin Gao, HuiHui Yan, WenJing Hao, 
YouSheng Yan, ChengHong Yin. Originally published in the Journal of Medical 
Internet Research (https://www.jmir.org).

DOI: 10.2196/69929
PMCID: PMC12192912
PMID: 40532199 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


350. Nurs Open. 2025 Jun;12(6):e70254. doi: 10.1002/nop2.70254.

'Conducting Research Amidst Hustle and Bustle': A Qualitative Study of the 
Research Challenges Faced by Part-Time Nursing Postgraduates.

Xu J(1), Lv F(2), Zhang J(3), Luo C(1), Yang Z(4), Wu J(1).

Author information:
(1)School of Medicine, Jiangsu University, Zhenjiang, China.
(2)Faculty of Life Sciences and Technology, Jiangsu University Jingjiang 
College, Zhenjiang, China.
(3)Affiliated Hospital of Jiangsu University, Zhenjiang, China.
(4)The First Affiliated Hospital of Wannan Medical College, Wuhu, China.

AIM: To explore the research challenges and experiences faced by part-time 
postgraduate nursing students and reveal the underlying reasons behind these 
challenges.
DESIGN: A descriptive qualitative study design was applied in this study.
METHODS: Fourteen part-time postgraduate nursing students were recruited using a 
combination of purposeful sampling and snowball sampling. Data were collected 
via in-depth semi structured interviews and analysed using reflexive thematic 
analysis. Consolidated criteria for reporting qualitative studies (COREQ): 
32-item checklist was used to report this study.
RESULTS: Two main themes and six subthemes were identified. The first theme 
revealed that part-time postgraduate nursing students 'conducting research 
amidst hustle and bustle'. Conflicting commitments, time and space constraints 
that weakened interpersonal research support, and difficulty in meditating on 
research were the main challenges they faced in the research process. The second 
theme described part-time postgraduate nursing students' experiences of 
'perplexity' about nursing research, including differences from expectations, 
self-doubt and a painful process. Compared to full-time postgraduate nursing 
students, part-time postgraduate nursing students faced unique research 
challenges that impacted their physical, mental and academic completion. 
Students, nursing educators and healthcare facility administrators should be 
aware of these challenges and experiences, and all three should work together to 
address them. Part-time postgraduate nursing students need to improve their time 
management and other skills to ensure learning engagement and efficiency. 
Nursing educators should guide students through transformative learning at 
critical junctures for research challenges. Collaboration between schools and 
healthcare organisations to create supportive environments for part-time 
postgraduate nursing students needs to be further explored.
PATIENT OR PUBLIC CONTRIBUTION: One interviewee participated in the design of 
the interview guide, and 14 part-time postgraduate nursing students expressed 
their challenges and experiences in the nursing research process. Thanks to them 
for sharing, enabling researchers to access the data and analyse it.

© 2025 The Author(s). Nursing Open published by John Wiley & Sons Ltd.

DOI: 10.1002/nop2.70254
PMCID: PMC12173147
PMID: 40526804 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


351. BMJ Oncol. 2025 May 15;4(1):e000759. doi: 10.1136/bmjonc-2025-000759. 
eCollection 2025.

Large language models in oncology: a review.

Chen D(1)(2), Parsa R(1)(3), Swanson K(4), Nunez JJ(5)(6), Critch A(7), 
Bitterman DS(8)(9)(10), Liu FF(11)(12), Raman S(1)(2)(13).

Author information:
(1)Radiation Medicine Program, Princess Margaret Hospital Cancer Centre, 
Toronto, Ontario, Canada.
(2)Temerty Faculty of Medicine, University of Toronto, Toronto, Ontario, Canada.
(3)Michael G. DeGroote School of Medicine, McMaster University, Stockholm, 
Ontario, Canada.
(4)Department of Medicine, University of California-San Francisco, San 
Francisco, California, USA.
(5)Department of Psychiatry, University of British Columbia, Vancouver, British 
Columbia, Canada.
(6)BC Cancer Agency, Vancouver, British Columbia, Canada.
(7)Center for Human-Compatible Artificial Intelligence, Department of Electrical 
Engineering and Computer Sciences, UC Berkeley, Berkeley, California, USA.
(8)Artificial Intelligence in Medicine (AIM) Program, Mass General Brigham, 
Harvard Medical School, Boston, Massachusetts, USA.
(9)Department of Radiation Oncology, Dana-Farber Cancer Institute, Boston, 
Massachusetts, USA.
(10)Computational Health Informatics Program, Boston Children's Hospital, 
Boston, Massachusetts, USA.
(11)Radiation Medicine Program, Princess Margaret Hospital, Toronto, Ontario, 
Canada.
(12)Department of Radiation Oncology, University of Toronto, Toronto, Ontario, 
Canada.
(13)Radiation Oncology, BC Cancer - Vancouver, Vancouver, British Columbia, 
Canada.

Large language models (LLMs) have demonstrated emergent human-like capabilities 
in natural language processing, leading to enthusiasm about their integration in 
healthcare environments. In oncology, where synthesising complex, multimodal 
data is essential, LLMs offer a promising avenue for supporting clinical 
decision-making, enhancing patient care, and accelerating research. This 
narrative review aims to highlight the current state of LLMs in medicine; 
applications of LLMs in oncology for clinicians, patients, and translational 
research; and future research directions. Clinician-facing LLMs enable clinical 
decision support and enable automated data extraction from electronic health 
records and literature to inform decision-making. Patient-facing LLMs offer the 
potential for disseminating accessible cancer information and psychosocial 
support. However, LLMs face limitations that must be addressed before clinical 
adoption, including risks of hallucinations, poor generalisation, ethical 
concerns, and scope integration. We propose the incorporation of LLMs within 
compound artificial intelligence systems to facilitate adoption and efficiency 
in oncology. This narrative review serves as a non-technical primer for 
clinicians to understand, evaluate, and participate as active users who can 
inform the design and iterative improvement of LLM technologies deployed in 
oncology settings. While LLMs are not intended to replace oncologists, they can 
serve as powerful tools to augment clinical expertise and patient-centred care, 
reinforcing their role as a valuable adjunct in the evolving landscape of 
oncology.

Copyright © Author(s) (or their employer(s)) 2025. Re-use permitted under CC 
BY-NC. No commercial re-use. See rights and permissions. Published by BMJ Group.

DOI: 10.1136/bmjonc-2025-000759
PMCID: PMC12164365
PMID: 40519217

Conflict of interest statement: None declared.


352. Eur J Cardiovasc Nurs. 2025 Sep 5;24(6):885-895. doi: 10.1093/eurjcn/zvaf074.

Artificial intelligence-generated podcasts open new doors to make science 
accessible: a mixed-method evaluation of quality and the potential for science 
communication.

Desmedt CP(1)(2), Budts W(2)(3), De Vos M(4)(5), Moons P(1)(6)(7).

Author information:
(1)KU Leuven Department of Public Health and Primary Care, KU Leuven - 
University of Leuven, Kapucijnenvoer 7, PB7001, Leuven B-3000, Belgium.
(2)Department of Cardiovascular Sciences, KU Leuven, Herestraat 49 box 911, 
Leuven 3000, Belgium.
(3)Congenital and Structural Cardiology, UZ Leuven, Herestraat 49, Leuven 3000, 
Belgium.
(4)STADIUS Center for Dynamical Systems Signal Processing and Data Analytics, KU 
Leuven, Kasteelpark Arenberg 10, bus 2446, Leuven B-3001, Belgium.
(5)Department of Electrical Engineering, KU Leuven, Kasteelpark Arenberg 10 - 
box 2440, Leuven 3001, Belgium.
(6)Gothenburg Center for Person-Centered Care, University of Gothenburg, Arvid 
Wallgrens backe 1, Gothenburg 413 46, Sweden.
(7)Department of Paediatrics and Child Health, University of Cape Town, 
Klipfontein Rd, Rondebosch, Cape Town 7700, South Africa.

AIMS: Science podcasts have proved to be valuable mediums for medical education 
and science dissemination. Tools adopting rapidly evolving technologies such as 
generative artificial intelligence (GenAI) now enable us to create podcasts in a 
matter of minutes (e.g. NotebookLM, Jellypod). However, GenAI entails 
challenges, such as hallucinations, which could compromise the trustworthiness 
of generated content. Therefore, this study aimed to explore the quality of 
AI-generated podcasts and their potential for science communication.
METHODS AND RESULTS: We conducted a mixed-method evaluation of 10 AI-generated 
podcasts for articles published in the European Journal of Cardiovascular 
Nursing. Participants were asked to complete a questionnaire and were invited 
for a video interview. They were not informed of the AI-nature of the podcast 
prior to evaluation. Only half of them were able to identify this aspect. The 
fact that the podcast was able to summarize key findings in an easily 
understandable and engaging manner was found to be a great asset. However, 
participants also indicated that the American style of the podcast took away 
from its credibility. Moreover, some podcasts contained inaccuracies, incorrect 
use of medical terms and mispronunciations, thereby compromising 
trustworthiness. Podcasts were found to be most appropriate for patients and the 
public but could be useful for researchers and healthcare professionals as well 
if they were tailored accordingly. Rigorous evaluation and transparency about 
the AI-generated nature of the podcast, referencing the original article and 
author acknowledgement were recommended.
CONCLUSION: AI-generated podcasts could be relevant additions to scientific 
journal articles and valuable alternatives for traditional science podcasts.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
European Society of Cardiology.

DOI: 10.1093/eurjcn/zvaf074
PMID: 40515580 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of interest: P.M. is Editor-in-Chief of 
the European Journal of Cardiovascular Nursing. Hence, in line with the 
journal’s conflict of interest policy, this paper was handled by Jeroen 
Hendriks.


353. J Pediatr Urol. 2025 Dec;21(6):1643-1649. doi: 10.1016/j.jpurol.2025.05.030. 
Epub 2025 Jun 2.

Annual updates of the European Association of Urology - European Society for 
Pediatric Urology (EAU-ESPU) paediatric urology guidelines: Are large-language 
models (LLM) better than the usual structured methodology?

't Hoen LA(1), van Uitert A(2), Bussmann M(3), Bezuidenhout C(4), Ribal M(5), 
Canfield S(6), Yuan Y(7), Omar MI(8), Castagnetti M(9), Burgu B(10), O'Kelly 
F(11), Quaedackers J(12), Rawashdeh Y(13), Silay S(14), Bujons A(15), Bogaert 
G(16), Pakkasjarvi N(17), Skott M(13), Kennedy U(18), Gnech M(19), Radmayr 
C(20).

Author information:
(1)Department of Pediatric Urology, Erasmus Medical Center, Rotterdam, the 
Netherlands. Electronic address: l.thoen@erasmusmc.nl.
(2)Department of Urology, Radboud University Medical Centre, Nijmegen, the 
Netherlands.
(3)M. Bussman, Helmholtz-Zentrum Dresden-Rossendorf, Dresden, Germany.
(4)Guidelines Office, European Association of Urology, Arnhem, the Netherlands.
(5)Uro-Oncology Unit, Hospital Clinic, University of Barcelona, Barcelona, 
Spain.
(6)Division of Urology, University of Texas McGovern Medical School, Houston, 
TX, USA.
(7)Department of Medicine, London Health Science Centre, London, Ontario, 
Canada; Department of Medicine, McMaster University, Hamilton, Ontario, Canada.
(8)Academic Urology Unit, University of Aberdeen, Aberdeen, UK.
(9)Department of Surgical, Oncological and Gastroenterological Sciences, 
University of Padova, Padua, Italy; Pediatric Urology Unit, Bambino Gesù 
Children Hospital and Research Center, Rome, Italy.
(10)Department of Pediatric Urology, Ankara University School of Medicine, 
Ankara, Turkey.
(11)Division of Paediatric Urology, Beacon Hospital Dublin & University College 
Dublin, Ireland.
(12)Department of Urology, University Medical Center Groningen, Groningen, the 
Netherlands.
(13)Department of Urology, Section of Pediatric Urology, Aarhus, Denmark.
(14)Division of Pediatric Urology, Department of Urology, Biruni University, 
Istanbul, Turkey.
(15)Division of Pediatric Urology, Urology Department Fundació Puigvert 
Universitat Autonòma Barcelona, Spain.
(16)Department of Urology, University of Leuven, Leuven, Belgium.
(17)Dept of Pediatric Surgery, New Children's Hospital, Helsinki University 
Hospital, Helsinki, Finland; Finland Dept of Pediatric Surgery, Section of 
Urology, University Children's Hospital, Uppsala, Sweden.
(18)Department of Pediatric Urology, University Children's Hospital Zurich, 
Zurich, Switzerland.
(19)Department of Paediatric Urology, Fondazione IRCCS Ca' Granda, Ospedale 
Maggiore Policlinico, Milan, Italy.
(20)Pediatric Urology, Medical University of Innsbruck, Innsbruck, Austria.

INTRODUCTION: The European Association for Urology - European Society for 
Pediatric Urology (EAU-ESPU) guidelines comprise a comprehensive publication of 
evidence based clinical guidelines for the field of Pediatric urology. The goal 
is to produce recommendations to optimize patient care and provide an assessment 
of benefits and harms and possible alternative treatment options. Artificial 
intelligence (AI) has immensely evolved and is often used in urology. With the 
emergence of Chat Generative Pre-trained Transformer (ChatGPT) and CoPilot, a 
new dimension in AI was reached and more widespread use of AI became possible. 
ChatGPT and CoPilot are both large language models (LLMs).
OBJECTIVES: The aim of the current study was to test the ability of LLMs to 
provide a trustworthy update of two of the chapters of the EAU-ESPU Pediatric 
Urology Guideline.
STUDY DESIGN: Three LLM's (Chat-GPT 3.5, Chat-GPT 4.0 and CoPilot) were asked to 
perform a systematic update of the hydrocele and varicocele chapters. For both 
chapters two standard conversations were written; one humane dialogue and one 
conversation in which we included minor prompt engineering, i.e. few-shot 
prompting. All conversations were performed five times by an independent 
researcher and outcomes were scored for accuracy, consistency and reliability, 
using several predefined criteria by two reviewers.
RESULTS: A total of sixty conversations were analyzed. All three LLMs were 
unable to update the guidelines with the recent relevant literature because of 
the lack of access to the correct scientific databases. Furthermore, a high 
variability was seen in the responses provided by the LLMs, although the input 
text was similar every time. The use of basic prompting in the structured 
conversations compared to the humane responses improved the consistency of the 
responses. The reproducibility, consistency, and reliability of the updates 
provided by the LLMs were assessed to be inadequate, despite the use of basic 
prompting.
DISCUSSION: Development of AI and specific plug-ins for LLMs are developing at a 
very fast pace. A specific follow-up project would be to create specific 
plug-ins and advanced prompt engineering in cooperation with AI experts for 
existing LLMs to update the guidelines with access to the relevant databases and 
correct instructions to follow the handbook of the guidelines.
CONCLUSION: At the moment LLMs cannot replace the panel members of the EAU 
Guidelines panel in their work to update the clinical guidelines. They have 
demonstrated inadequate consistency, reliability, accuracy, and are not able to 
incorporate new literature.

Copyright © 2025 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.jpurol.2025.05.030
PMID: 40514273 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of interest No conflict of interest.


354. Digit Health. 2025 Jun 9;11:20552076251348850. doi: 10.1177/20552076251348850. 
eCollection 2025 Jan-Dec.

Answering real-world clinical questions using large language model, 
retrieval-augmented generation, and agentic systems.

Low YS(1), Jackson ML(1), Hyde RJ(1), Brown RE(1), Sanghavi NM(1), Baldwin 
JD(1), Pike CW(1), Muralidharan J(1), Hui G(1)(2), Alexander N(3), Hassan 
H(4)(5), Nene RV(6), Pike M(7), Pokrzywa CJ(8), Vedak S(9), Yan AP(3), Yao 
DH(10), Zipursky AR(3), Dinh C(1), Ballentine P(1), Derieg DC(1), Polony V(1), 
Chawdry RN(1), Davies J(1), Hyde BB(1), Shah NH(1)(9), Gombar S(1)(11).

Author information:
(1)Atropos Health, New York, NY, USA.
(2)Department of Medicine, University of California, Los Angeles, CA, USA.
(3)Department of Pediatrics, The Hospital for Sick Children, Toronto, Ontario, 
Canada.
(4)Division of Hematology/Oncology, The Hospital for Sick Children, Toronto 
Ontario, Canada.
(5)Program in Child Health Evaluative Sciences, Peter Gilgan Centre for Research 
and Learning, The Hospital for Sick Children, Toronto, Ontario, Canada.
(6)Department of Emergency Medicine, University of California, San Diego, CA, 
USA.
(7)Department of Emergency Medicine, University of Michigan, Ann Arbor, MI, USA.
(8)Department of Surgery, Columbia University, New York, NY, USA.
(9)Division of Clinical Informatics, Stanford University, Stanford, CA, USA.
(10)Department of Emergency Medicine, Stanford University, Stanford, CA, USA.
(11)Department of Pathology, Stanford University, Stanford, CA, USA.

OBJECTIVE: The practice of evidence-based medicine can be challenging when 
relevant data are lacking or difficult to contextualize for a specific patient. 
Large language models (LLMs) could potentially address both challenges by 
summarizing published literature or generating new studies using real-world 
data.
MATERIALS AND METHODS: We submitted 50 clinical questions to five LLM-based 
systems: OpenEvidence, which uses an LLM for retrieval-augmented generation 
(RAG); ChatRWD, which uses an LLM as an interface to a data extraction and 
analysis pipeline; and three general-purpose LLMs (ChatGPT-4, Claude 3 Opus, 
Gemini 1.5 Pro). Nine independent physicians evaluated the answers for 
relevance, quality of supporting evidence, and actionability (i.e., sufficient 
to justify or change clinical practice).
RESULTS: General-purpose LLMs rarely produced relevant, evidence-based answers 
(2-10% of questions). In contrast, RAG-based and agentic LLM systems, 
respectively, produced relevant, evidence-based answers for 24% (OpenEvidence) 
to 58% (ChatRWD) of questions. OpenEvidence produced actionable results for 48% 
of questions with existing evidence, compared to 37% for ChatRWD and <5% for the 
general-purpose LLMs. ChatRWD provided actionable results for 52% of questions 
that lacked existing literature compared to <10% for other LLMs.
DISCUSSION: Special-purpose LLM systems greatly outperformed general-purpose 
LLMs in producing answers to clinical questions. Retrieval-augmented 
generation-based LLM (OpenEvidence) performed well when existing data were 
available, while only the agentic ChatRWD was able to provide actionable answers 
when preexisting studies were lacking.
CONCLUSION: Synergistic systems combining RAG-based evidence summarization and 
agentic generation of novel evidence could improve the availability of pertinent 
evidence for patient care.

© The Author(s) 2025.

DOI: 10.1177/20552076251348850
PMCID: PMC12159471
PMID: 40510193

Conflict of interest statement: The authors declared the following potential 
conflicts of interest with respect to the research, authorship, and/or 
publication of this article: ChatRWD, the LLM system evaluated in this study, is 
developed by Atropos Health where many of the authors are employed. NHS is not 
an Atropos Health employee but sits on its board. OpenEvidence, another LLM 
system evaluated here, is provided by OpenEvidence whom we consulted during the 
writing of this manuscript. Non-Atropos employees NA, HH, RVN, MP, CJP, SV, APY, 
D-HY, and ARZ, have nothing to disclose.


355. Healthcare (Basel). 2025 May 27;13(11):1271. doi: 10.3390/healthcare13111271.

AI Chatbots in Pediatric Orthopedics: How Accurate Are Their Answers to Parents' 
Questions on Bowlegs and Knock Knees?

Kamal AH(1).

Author information:
(1)Division of Orthopedics, Department of Surgery, College of Medicine, King 
Faisal University, Al-Ahsa 31982, Saudi Arabia.

BACKGROUND/OBJECTIVES: Large-language modules facilitate accessing health 
information instantaneously. However, they do not provide the same level of 
accuracy or detail. In pediatric orthopedics, where parents have urgent concerns 
regarding knee deformities (bowlegs and knock knees), the accuracy and 
dependability of these chatbots can affect parent decisions to seek treatment. 
The goal of this study was to analyze how AI chatbots addressed parental 
concerns regarding pediatric knee deformities.
METHODS: A set of twenty standardized questions, consisting of ten questions 
each on bowlegs and knock knees, were designed through literature reviews and 
through analysis of parental discussion forums and expert consultations. Each of 
the three chatbots (ChatGPT, Gemini, and Copilot) was asked the same set of 
questions. Five pediatric orthopedic surgeons were then asked to rate each 
response for accuracy, clarity, and comprehensiveness, along with the degree of 
misleading information provided, on a scale of 1-5. The reliability among raters 
was calculated using intraclass correlation coefficients (ICCs), while 
differences among the chatbots were assessed using a Kruskal-Wallis test with 
post hoc pairwise comparisons.
RESULTS: All three chatbots displayed a moderate-to-good score for inter-rater 
reliability. ChatGPT and Gemini's scores were higher for accuracy and 
comprehensiveness than Copilot's (p < 0.05). However, no notable differences 
were found in clarity or in the likelihood of giving incorrect answers. Overall, 
more detailed and precise responses were given by ChatGPT and Gemini, while, 
with regard to clarity, Copilot performed comparably but was less thorough.
CONCLUSIONS: There were notable discrepancies in performance across the AI 
chatbots in providing pediatric orthopedic information, which demonstrates 
indications of evolving potential. In comparison to Copilot, ChatGPT and Gemini 
were relatively more accurate and comprehensive. These results highlight the 
persistent requirement for real-time supervision and stringent validation when 
employing chatbots in the context of pediatric healthcare.

DOI: 10.3390/healthcare13111271
PMCID: PMC12154324
PMID: 40508883

Conflict of interest statement: The author declare no conflicts of interest.


356. JMIR AI. 2025 Jun 12;4:e65481. doi: 10.2196/65481.

AI-Powered Drug Classification and Indication Mapping for Pharmacoepidemiologic 
Studies: Prompt Development and Validation.

Ogorek B(1), Rhoads T(1), Finkelman E(1), Rodriguez-Chavez IR(1)(2).

Author information:
(1)Spencer Health Solutions, Inc, Morrisville, NC, United States.
(2)4Biosolutions Consulting, Rockville, MD, United States.

BACKGROUND: Pharmacoepidemiologic studies, which promote rational drug use and 
improve health outcomes, often require Anatomical Therapeutic Chemical 
Classification System (ATC) drug classification within real-world data (RWD) 
sources. Existing classification tools are expensive, brittle, or have 
restrictive terms of service, and lack context that may inform classification 
itself.
OBJECTIVE: This study sought to establish large language models (LLMs) as an 
assisting technology in the drug classification task. This included developing 
artificial intelligence prompts that reason about drugs using RWD and showing 
that the resulting accuracy, efficiency, and effectiveness are favorable to 
alternative methods.
METHODS: A prompt was constructed to classify aspirin as either an analgesic or 
antithrombotic and evaluated within 12,294 anonymized daily dose strings from a 
polychronic population residing in the United States and Canada. The patients 
used a smart medication dispenser called "spencer" and consented to the use of 
their data for research. The LLM prompt requested that the best and next-best 
second-level ATC code be returned, and grading was performed on a 3-point scale. 
After success in a pilot sample of 20, an inference sample of 200 was taken 
without replacement. Finite population inference was carried out on the 
proportion of outputs receiving 1 of the top 2 grades. As a benchmark, Google's 
Programmable Search Engine was used to query the drug name plus "ATC code" 
followed by regex-based extraction of ATC codes. All imperfect results were 
reviewed.
RESULTS: The population consisted of 12,294 daily dose strings from 86.26% 
(2908/3371) patients residing in Canada and 13.73% (463/3371) residing in the 
United States. A prompt using the chain-of-thought reasoning was able to 
distinguish between aspirin's analgesic versus antithrombotic therapeutic uses 
and performed well in the pilot sample. In the inferential sample, 87.5% 
(175/200) were graded as perfect, 5% (10/200) had a minor issue, and 7.5% 
(15/200) had a major issue. The estimate of the proportion of at least mostly 
correct classification was 92.5% (185/200, 80% CI 90.1%-94.9%). For the 
search-based algorithm, 82.5% (165/200) were deemed acceptable. The 
chain-of-thought reasoning was most helpful with supplements (eg, folic acid) 
when high doses indicated antianemic preparations. The problem formulation of 
daily dose inputs and multiple ATC outputs was sometimes incompatible with the 
drug (eg, pregabalin, calcitriol, and methotrexate).
CONCLUSIONS: GPT-4o offers cost-effective drug classification from RWD without 
violating any terms of service. Using a chain-of-thought prompting technique, 
GPT-4o can reason about drug dosages that affect the class. The wide 
accessibility of LLMs gives every research team the ability to classify drugs at 
scale, a key prerequisite of pharmacoepidemiologic research.

©Benjamin Ogorek, Thomas Rhoads, Eric Finkelman, Isaac R Rodriguez-Chavez. 
Originally published in JMIR AI (https://ai.jmir.org), 12.06.2025.

DOI: 10.2196/65481
PMCID: PMC12203024
PMID: 40505126

Conflict of interest statement: Conflicts of Interest: The authors are all 
current or former employees or contractors of Spencer Health Solutions, Inc, the 
developer of the spencer platform.


357. JMIR Infodemiology. 2025 Jun 12;5:e68469. doi: 10.2196/68469.

Availability and Use of Digital Technology Among Women With Polycystic Ovary 
Syndrome: Scoping Review.

Wright PJ(1), Burts C(1), Harmon C(1), Corbett CF(1).

Author information:
(1)University of South Carolina, 1601 Greene Street, Columbia, SC, 29208, United 
States, 1 803-777-6039.

BACKGROUND: Polycystic ovary syndrome (PCOS) is a common endocrinopathy among 
women that requires self-management to improve mental and physical health 
outcomes and reduce risk of comorbidity. Digital technology has rapidly emerged 
as a valuable self-management tool for people with chronic health conditions. 
However, little is known about the digital technology available for and used by 
women with PCOS.  .
OBJECTIVE: The purpose of this scoping review was to identify what is known 
about digital technology currently available and used by women with PCOS for 
PCOS-specific knowledge, self-management, or social support.
METHODS: The databases PubMed, Embase, CINAHL, and Compendex were searched using 
Medical Subject Headings terms for PCOS, digital technology, health knowledge, 
self-management, and social support. Inclusion criteria were full-text, 
peer-reviewed publications of primary research from 2010 to 2025 in English 
about digital technology used for PCOS-specific knowledge, self-management, or 
social support by women aged 18 years and older with PCOS. Exclusion criteria 
were articles about pediatric populations and digital technology used for 
intervention recruitment or by health care providers to diagnose or treat 
patients.
RESULTS: In total, 34 full-text articles met the inclusion criteria. Given the 
scope of digital technology, eligible studies were grouped into 7 domains: 
mobile apps (n=14), internet-based programs (eg, Google; n=6), social media 
(n=6), SMS text message (n=2), machine learning (n=2), artificial intelligence 
(eg, ChatGPT [OpenAI]; n=3), and web-based intervention platforms (n=1). 
Findings highlighted participants' varied perceptions of technology usefulness 
based on reliability of health care information, application features, accuracy 
of PCOS or fertility prediction, social group engagement, user-friendly 
interfaces, cultural sensitivity, and accessibility.
CONCLUSIONS: There is potential for digital technology to transform PCOS 
self-management, but further design and development are needed to optimize the 
technologies for women with PCOS. Future research should focus on including end 
users during the design phase of digital technology, refining predictive models, 
improving app inclusivity, conducting frequent reliability testing, and 
enhancing user engagement and support via additional features to promote more 
comprehensive self-management of PCOS.   .

© Pamela J Wright, Charlotte Burts, Carolyn Harmon, Cynthia F Corbett. 
Originally published in JMIR Infodemiology (https://infodemiology.jmir.org).

DOI: 10.2196/68469
PMCID: PMC12178569
PMID: 40505084 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


358. Med Teach. 2025 Dec;47(12):1995-2005. doi: 10.1080/0142159X.2025.2513419. Epub 
2025 Jun 12.

Advancing medical education in cervical cancer control with large language 
models for multiple-choice question generation.

Chen M(1), Ma J(2), Cui X(3), Dai Q(4), Hu H(5), Wu Y(1), Husaiyin S(6), Wu 
A(7), Qiao Y(1).

Author information:
(1)School of Population Medicine and Public Health, Chinese Academy of Medical 
Sciences and Peking Union Medical College, Beijing, China.
(2)Tencent Sustainable Social Value Inclusive Health Lab, Tencent, Beijing, 
China.
(3)Department of Gynecologic Oncology, Cancer Hospital of China Medical 
University, Liaoning Cancer Hospital & Institute, Shenyang, Liaoning Province, 
China.
(4)Department of Diagnosis and Treatment for Cervical Diseases, Chengdu Women's 
and Children's Central Hospital, School of Medicine, University of Electronic 
Science and Technology of China, Chengdu, Sichuan Province, China.
(5)Department of Gynecology, Shenzhen Maternity and Child Healthcare Hospital, 
Southern Medical University, Shenzhen, Guangdong Province, China.
(6)Department of Gynecology, People's Hospital of Xinjiang Uygur Autonomous 
Region, Urumqi, China.
(7)Wuxi Maternity and Child Health Care Hospital, Wuxi School of Medicine, 
Jiangnan University, Wuxi, Jiangsu Province, China.

OBJECTIVE: To explore the feasibility of using large language models (LLMs) to 
generate multiple-choice questions (MCQs) for cervical cancer control education 
and compare them with those created by clinicians.
METHODS: GPT-4o and Baichuan4 generated 40 MCQs each with iteratively refined 
prompts. Clinicians generated 40 MCQs for comparison. 120 MCQs were evaluated by 
12 experts across five dimensions (correctness, clarity and specificity, 
cognitive level, clinical relevance, explainability) using a 5-point Likert 
scale. Difficulty and discriminatory power were tested by practitioners. 
Participants were asked to identify the source of each MCQ.
RESULTS: Automated MCQs were similar to clinician-generated ones in most 
dimensions. However, clinician-generated MCQs had a higher cognitive level 
(4.00±1.08) than those from GPT-4o (3.68±1.07) and Baichuan4 (3.7±1.13). Testing 
with 312 practitioners revealed no significant differences in difficulty or 
discriminatory power among clinicians (59.51±24.50, 0.38±0.14), GPT-4o 
(61.89±25.36, 0.30±0.19), and Baichuan4 (59.79±26.25, 0.33±0.15). Recognition 
rates for LLM-generated MCQs ranged from 32% to 50%, with experts outperforming 
general practitioners in identifying the question setters.
CONCLUSIONS: LLMs can generate MCQs comparable to clinician-generated ones with 
engineered prompts, though clinicians outperformed in cognitive level. 
LLM-assisted MCQ generation could enhance efficiency but requires rigorous 
validation to ensure educational quality.

DOI: 10.1080/0142159X.2025.2513419
PMID: 40504493 [Indexed for MEDLINE]


359. JMIR Mhealth Uhealth. 2025 Jun 11;13:e66403. doi: 10.2196/66403.

Natural Language Processing Chatbot-Based Interventions for Improvement of Diet, 
Physical Activity, and Tobacco Smoking Behaviors: Systematic Review.

Chen J(#)(1), Hu RZ(#)(2), Zhuang YX(2), Zhang JQ(2), Shan R(1), Yang Y(1), Liu 
Z(1)(2).

Author information:
(1)Department of Maternal and Child Health, School of Public Health, Peking 
University, No. 38, Xueyuan Road, Haidian District, Beijing, 100191, China.
(2)Department of Preventive Medicine, School of Public Health, Peking 
University, Beijing, China.
(#)Contributed equally

BACKGROUND: The rapid development of artificial intelligence technology has 
enabled chatbots to increasingly promote health-related behaviors, addressing 
the high demand for human resources in traditional interventions. Several 
systematic reviews have been conducted in this area. However, the existing 
reviews have not focused on the rigorously designed randomized trials of the 
state-of-the-art chatbots (interacting with users through unconstrained natural 
language), thus calling for an updated review.
OBJECTIVE: We aimed to explore the effects of natural language processing (NLP) 
chatbot-based interventions on improving diet, physical activity, and tobacco 
smoking behaviors in the general population and to evaluate the chatbot use 
behaviors during the implementation process.
METHODS: We comprehensively searched 12 databases or registers for eligible 
studies published from January 1, 2010, until July 16, 2024, and obtained a 
total of 6301 studies. We included randomized controlled trials (RCTs) that used 
NLP-chatbots to promote diet, physical activity, or tobacco smoking behaviors 
among adults or children. Due to considerable heterogeneity across the included 
studies, we adopted the synthesis without meta-analysis guidelines and 
summarized the effectiveness of NLP chatbot-based interventions. We used the new 
evidence-mapping method (bubble plot) to visualize the results. We also 
described the results related to the changes in diet, physical activity, or 
tobacco smoking behaviors (eg, change of BMI and stage of change). To evaluate 
the implementation process of the intervention, we summarized users' interaction 
with NLP-chatbots and their feelings (eg, satisfaction) about NLP-chatbot use. 
Additionally, we assessed the risk of bias of studies using the RoB 2.0 (Risk of 
Bias; The Cochrane Collaboration) tools.
RESULTS: We finally included 7 RCTs. Concerning dietary and physical activity 
behaviors, the effectiveness of NLP chatbot-based interventions was inconsistent 
among adults, while no evidence of effect was observed among children. 
Concerning tobacco smoking behaviors, the included studies showed consistent 
evidence of improving this behavior among adults. Regarding the risk of bias of 
the changes in diet, physical activity, and tobacco smoking behaviors, 2 of 3, 2 
of 4, and 1 of 2 studies had a high risk of bias, respectively, while the 
remaining had a low risk of bias. Concerning the interactions with NLP-chatbots, 
studies showed an overall high percentage of general interaction between users 
and NLP-chatbots, but not a satisfactorily high percentage of interactions 
specific to health behaviors. Concerning feelings about NLP-chatbot use, users 
showed a positive impression of NLP-chatbot use, feeling it was useful, 
credible, and financially feasible.
CONCLUSIONS: NLP chatbot-based interventions were beneficial for adults' tobacco 
smoking behaviors, but no such evidence was found on diet or physical activity 
behaviors among adults or children. More RCTs with larger samples and lower risk 
of bias are urgently needed to enhance our findings in the future.

©Jing Chen, Run-Ze Hu, Yu-Xuan Zhuang, Jia-Qi Zhang, Rui Shan, Yang Yang, Zheng 
Liu. Originally published in JMIR mHealth and uHealth 
(https://mhealth.jmir.org).

DOI: 10.2196/66403
PMCID: PMC12175970
PMID: 40503914 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


360. BMC Med Educ. 2025 Jun 11;25(1):849. doi: 10.1186/s12909-025-07446-7.

Digital learning resource use among Swedish medical students: insights from a 
nationwide survey.

Bjurström MF(1)(2), Lundkvist E(3), Sturesson LW(4), Borgquist O(4), Lundén 
R(5), Fagerlund MJ(6), Lipcsey M(7), Kander T(4).

Author information:
(1)Department of Surgical Sciences, Clinical Pain Research, Uppsala University, 
Uppsala, Sweden. martin.flores.bjurstrom@uu.se.
(2)Multidisciplinary Pain Center / Department of Anesthesiology and Intensive 
Care, Uppsala University Hospital; Department of Surgical Sciences, Uppsala 
University Hospital, Ingång 79, Våning 2, Uppsala, 751 85, Sweden. 
martin.flores.bjurstrom@uu.se.
(3)Department of Pharmacy, Clinical Pharmacy and Pharmacotherapy and Unit for 
Academic Teaching and Learning, Uppsala University, Uppsala, Sweden.
(4)Department of Clinical Sciences Lund, Anesthesiology and Intensive Care 
Medicine, Lund University, Lund, Sweden.
(5)Department of Anesthesiology and Intensive Care, Skåne University Hospital, 
Lund, Sweden.
(6)Department of Physiology and Pharmacology, Section for Anesthesiology and 
Intensive Care Medicine, Karolinska Institute, Stockholm, Sweden.
(7)Department of Surgical Sciences, Anesthesiology and Intensive Care, Uppsala 
University, Uppsala, Sweden.

BACKGROUND: Medical students navigate a complex landscape of digital tools with 
potential to enhance learning. The main objectives of the current study were to 
investigate which digital resources are being used, which background factors are 
associated with utilization, perceived advantages and disadvantages of different 
digital resources, and explore future directions.
METHODS: Cross-sectional, nationwide, online 25-item multiple-choice question 
survey and one free-text question enabling qualitative data analysis. Medical 
students at all seven universities with medical school programs in Sweden were 
invited to participate. Data were collected October - December 2024.
RESULTS: One thousand seven hundred sixty-six students responded to the survey, 
with an average response rate of 20.2% across sites. The five most frequently 
used digital resources were (percentage using at least on a weekly basis): 
University study platform (75.3%), videos (68.0%), flashcards (66.4%), student 
notes (53.4%) and external study platforms (47.3%). Flashcards were perceived to 
have a large to very large positive impact on development and maintenance of 
theoretical knowledge by 63.7% of students. Younger age (≤ 25 years) was 
strongly associated with higher use of flashcards (OR 1.98 (95% CI 1.54-2.54)) 
and generative artificial intelligence (AI) (OR 1.66 (1.29-2.15)), whereas 
having children at home was associated with more frequent use of videos (OR 2.32 
(1.32-4.08)) and university digital platforms (OR 2.62 (1.26-5.45)), in 
multivariable logistic regression analyses. Most students (74.8%) reported 
finding their digital resources based on recommendations from more senior 
medical students. Perceived key advantages of digital resources in general were 
availability (90.9%), flexibility (80.6%), and more effective learning compared 
to traditional modalities (59.0%), while possible disadvantages included risk 
for distraction (49.6%) and uncertainty regarding reliability of content 
(45.4%). Qualitative data highlighted several areas of interest, including calls 
for universities and lecturers to provide high-quality, updated video material 
and flashcard decks tailored to the curriculum, and review and recommend 
third-party digital resources (e.g., YouTube channels).
CONCLUSIONS: Medical students extensively use digital resources, with perceived 
large positive learning effects and benefits. Several background factors 
influence usage patterns. These data may support institutions, program directors 
and teachers in their efforts to guide and improve use of digital learning tools 
in medical schools.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-07446-7
PMCID: PMC12153187
PMID: 40500719 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The Swedish Ethical Review Authority presented no ethical 
objections to the project, deemed the study exempt from full review, and waived 
the need for consent to participate (Dnr 2024–03072-01). This study was 
conducted in accordance with the ethical standards of the Declaration of 
Helsinki. Consent for publication: Not applicable. Competing interests: The 
authors declare no competing interests.


361. Eur Radiol. 2025 Dec;35(12):8116-8125. doi: 10.1007/s00330-025-11721-y. Epub 
2025 Jun 11.

Non-enhanced CT deep learning model for differentiating lung adenocarcinoma from 
tuberculoma: a multicenter diagnostic study.

Zhang G(#)(1), Shang L(#)(2), Li S(#)(2)(3), Zhang J(#)(4), Zhang Z(5), Zhang 
X(6), Qian R(2), Yang K(2), Li X(2), Liu Y(2)(7), Wu Y(2)(8), Pu H(2), Cao Y(9), 
Man Q(10), Kong W(11).

Author information:
(1)Department of Radiology, Sichuan Provincial People's Hospital, University of 
Electronic Science and Technology of China, Chengdu, China. 
zhangguojin@med.uestc.edu.cn.
(2)Department of Radiology, Sichuan Provincial People's Hospital, University of 
Electronic Science and Technology of China, Chengdu, China.
(3)Department of Radiology, Lanzhou University Second Hospital, Lanzhou, China.
(4)Department of Radiology, The Fifth Affiliated Hospital of Zunyi Medical 
University, Zhuhai, China.
(5)Department of Radiological Sciences, School of Medicine, University of 
California Irvine, Irvine, USA.
(6)Department of Pharmaceuticals Diagnosis, GE Healthcare, Beijing, China.
(7)Department of Radiology, Chengdu Wenjiang District People's Hospital, 
Chengdu, China.
(8)Department of Radiology, The First People's Hospital of Yibin, Yibin, China.
(9)Department of Radiology, Qinghai University Affiliated Hospital, Xining, 
China. caoyt18@lzu.edu.cn.
(10)School of Pharmacy, Chengdu Medical College, Chengdu, China. gretam@163.com.
(11)Department of Radiology, Sichuan Provincial People's Hospital, University of 
Electronic Science and Technology of China, Chengdu, China. 
kongweifang@med.uestc.edu.cn.
(#)Contributed equally

OBJECTIVES: To develop and validate a deep learning model based on 
three-dimensional features (DL_3D) for distinguishing lung adenocarcinoma (LUAD) 
from tuberculoma (TBM).
MATERIALS AND METHODS: A total of 1160 patients were collected from three 
hospitals. A vision transformer network-based DL_3D model was trained, and its 
performance in differentiating LUAD from TBM was evaluated using validation and 
external test sets. The performance of the DL_3D model was compared with that of 
two-dimensional features (DL_2D), radiomics, and six radiologists. Diagnostic 
performance was assessed using the area under the receiver operating 
characteristic curves (AUCs) analysis.
RESULTS: The study included 840 patients in the training set (mean age, 54.8 
years [range, 19-86 years]; 514 men), 210 patients in the validation set (mean 
age, 54.3 years [range, 18-86 years]; 128 men), and 110 patients in the external 
test set (mean age, 54.7 years [range, 22-88 years]; 51 men). In both the 
validation and external test sets, DL_3D exhibited excellent diagnostic 
performance (AUCs, 0.895 and 0.913, respectively). In the test set, the DL_3D 
model showed better performance (AUC, 0.913; 95% CI: 0.854, 0.973) than the 
DL_2D (AUC, 0.804, 95% CI: 0.722, 0.886; p < 0.001), radiomics (AUC, 0.676, 95% 
CI: 0.574, 0.777; p < 0.001), and six radiologists (AUCs, 0.692 to 0.810; p 
value range < 0.001-0.035).
CONCLUSION: The DL_3D model outperforms expert radiologists in distinguishing 
LUAD from TBM.
KEY POINTS: Question Can a deep learning model perform in differentiating LUAD 
from TBM on non-enhanced CT images? Findings The DL_3D model demonstrated higher 
diagnostic performance than the DL_2D model, radiomics model, and six 
radiologists in differentiating LUAD and TBM. Clinical relevance The DL_3D model 
could accurately differentiate between LUAD and TBM, which can help clinicians 
make personalized treatment plans.

© 2025. The Author(s), under exclusive licence to European Society of Radiology.

DOI: 10.1007/s00330-025-11721-y
PMID: 40500528 [Indexed for MEDLINE]

Conflict of interest statement: Compliance with ethical standards. Guarantor: 
The scientific guarantor of this publication is Guojin Zhang. Conflict of 
interest: The authors declare that they have no conflict of interest. Statistics 
and biometry: G.J., Q.M., and X.Z. have significant statistical expertise. 
Informed consent: Written informed consent was waived by the Institutional 
Review Board. Ethical approval: This retrospective study was approved by the 
Institutional Review Board of Sichuan Provincial People’s Hospital (no. 
2024-497) and waived the requirement for informed consent. Study subjects or 
cohorts overlap: No study subjects or cohorts have been previously reported. 
Methodology: Retrospective Diagnostic study Multicenter study


362. J Med Internet Res. 2025 Jun 11;27:e72638. doi: 10.2196/72638.

Enhancing Pulmonary Disease Prediction Using Large Language Models With Feature 
Summarization and Hybrid Retrieval-Augmented Generation: Multicenter 
Methodological Study Based on Radiology Report.

Li R(#)(1), Mao S(#)(2), Zhu C(1), Yang Y(1), Tan C(3), Li L(4), Mu X(4), Liu 
H(#)(1), Yang Y(#)(2).

Author information:
(1)School of Biomedical Engineering, Capital Medical University, No. 10, 
Xitoutiao, You An Men, Fengtai District, Beijing, 100069, China, 86 
010-83911542.
(2)State Key Laboratory of Networking and Switching Technology, Beijing 
University of Posts and Telecommunications, Beijing, China.
(3)Department of Respiratory Medicine, Beijing Friendship Hospital, Capital 
Medical University, Beijing, China.
(4)Beijing Respiratory and Critical Care Medicine Department, Tsinghua Changgung 
Hospital, School of Clinical Medicine, Tsinghua University, Beijing, China.
(#)Contributed equally

BACKGROUND: The rapid advancements in natural language processing, particularly 
the development of large language models (LLMs), have opened new avenues for 
managing complex clinical text data. However, the inherent complexity and 
specificity of medical texts present significant challenges for the practical 
application of prompt engineering in diagnostic tasks.
OBJECTIVE: This paper explores LLMs with new prompt engineering technology to 
enhance model interpretability and improve the prediction performance of 
pulmonary disease based on a traditional deep learning model.
METHODS: A retrospective dataset including 2965 chest CT radiology reports was 
constructed. The reports were from 4 cohorts, namely, healthy individuals and 
patients with pulmonary tuberculosis, lung cancer, and pneumonia. Then, a novel 
prompt engineering strategy that integrates feature summarization (F-Sum), chain 
of thought (CoT) reasoning, and a hybrid retrieval-augmented generation (RAG) 
framework was proposed. A feature summarization approach, leveraging term 
frequency-inverse document frequency (TF-IDF) and K-means clustering, was used 
to extract and distill key radiological findings related to 3 diseases. 
Simultaneously, the hybrid RAG framework combined dense and sparse vector 
representations to enhance LLMs' comprehension of disease-related text. In 
total, 3 state-of-the-art LLMs, GLM-4-Plus, GLM-4-air (Zhipu AI), and GPT-4o 
(OpenAI), were integrated with the prompt strategy to evaluate the efficiency in 
recognizing pneumonia, tuberculosis, and lung cancer. The traditional deep 
learning model, BERT (Bidirectional Encoder Representations from Transformers), 
was also compared to assess the superiority of LLMs. Finally, the proposed 
method was tested on an external validation dataset consisted of 343 chest 
computed tomography (CT) report from another hospital.
RESULTS: Compared with BERT-based prediction model and various other prompt 
engineering techniques, our method with GLM-4-Plus achieved the best performance 
on test dataset, attaining an F1-score of 0.89 and accuracy of 0.89. On the 
external validation dataset, F1-score (0.86) and accuracy (0.92) of the proposed 
method with GPT-4o were the highest. Compared to the popular strategy with 
manually selected typical samples (few-shot) and CoT designed by doctors 
(F1-score=0.83 and accuracy=0.83), the proposed method that summarized disease 
characteristics (F-Sum) based on LLM and automatically generated CoT performed 
better (F1-score=0.89 and accuracy=0.90). Although the BERT-based model got 
similar results on the test dataset (F1-score=0.85 and accuracy=0.88), its 
predictive performance significantly decreased on the external validation set 
(F1-score=0.48 and accuracy=0.78).
CONCLUSIONS: These findings highlight the potential of LLMs to revolutionize 
pulmonary disease prediction, particularly in resource-constrained settings, by 
surpassing traditional models in both accuracy and flexibility. The proposed 
prompt engineering strategy not only improves predictive performance but also 
enhances the adaptability of LLMs in complex medical contexts, offering a 
promising tool for advancing disease diagnosis and clinical decision-making.

© Ronghao Li, Shuai Mao, Congmin Zhu, Yingliang Yang, Chunting Tan, Li Li, 
Xiangdong Mu, Honglei Liu, Yuqing Yang. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org).

DOI: 10.2196/72638
PMCID: PMC12176309
PMID: 40499132 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


363. Eur J Public Health. 2025 Dec 1;35(6):1082-1086. doi: 10.1093/eurpub/ckaf072.

Evaluation of a large language model (ChatGPT) versus human researchers in 
assessing risk-of-bias and community engagement levels: a systematic review 
use-case analysis.

Di Pumpo M(1)(2), Riccardi MT(3), De Vita V(1), Damiani G(1)(4).

Author information:
(1)Department of Life Sciences and Public Health, Università Cattolica del Sacro 
Cuore, Rome, Italy.
(2)Italian Society for Artificial Intelligence in Medicine (SIIAM-Società 
Italiana Intelligenza Artificiale in Medicina), Rome, Italy.
(3)Cancer Screening Unit, Local Health Unit Roma 2, Rome, Italy.
(4)Department of Woman and Child Health and Public Health, Fondazione 
Policlinico Universitario A. Gemelli IRCCS, Rome, Italy.

Large language models (LLMs) like OpenAI's ChatGPT (generative pretrained 
transformers) offer great benefits to systematic review production and quality 
assessment. A careful assessment and comparison with standard practice is highly 
needed. Two custom GPTs models were developed to compare a LLM's performance in 
"Risk-of-bias (ROB)" assessment and "Levels of engagement reached (LOER)" 
classification vs human judgments. Inter-rater agreement was calculated. ROB GPT 
classified a slightly higher "low risk" overall judgments (27.8% vs 22.2%) and 
"some concern" (58.3% vs 52.8%) than the research team, for whom "high risk" 
judgments were double (25.0% vs 13.9%). The research team classified slightly 
higher "low risk" total judgments (59.7% vs 55.1%) and almost double "high risk" 
(11.1% vs 5.6%) compared to "ROB GPT" (55.1%), which rated higher "some 
concerns" (39.4% vs 29.2%) (P = .366). With regards to LOER analysis, 91.7% vs 
25.0% were classified "Collaborate" level, 5.6% vs 61.1% as "Shared leadership", 
and 2.8% as "Involve" vs 13.9% by researchers, while no studies classified in 
the first two engagement level vs 8.3% and 13.9%, respectively, by researchers 
(P = .169). A mixed-effect ordinal logistic regression showed an odds ratio 
(OR) = 0.97 [95% confidence interval (CI) 0.647-1.446, P = .874] for ROB and an 
OR = 1.00 (95% CI = 0.397-2.543, P = .992) for LOER compared to researchers. 
Partial agreement on some judgments was observed. Further evaluation of these 
promising tools is needed to enable their effective yet reliable introduction in 
scientific practice.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
European Public Health Association.

DOI: 10.1093/eurpub/ckaf072
PMCID: PMC12707484
PMID: 40493519 [Indexed for MEDLINE]


364. JCO Clin Cancer Inform. 2025 Jun;9:e2500071. doi: 10.1200/CCI-25-00071. Epub 
2025 Jun 9.

Enhancing Patient-Trial Matching With Large Language Models: A Scoping Review of 
Emerging Applications and Approaches.

Chen H(1), Li X(1), He X(2)(3), Chen A(1), McGill J(4), Webber EC(5)(6), Xu 
H(7), Liu M(1), Bian J(2)(3).

Author information:
(1)Department of Health Outcomes & Biomedical Informatics, College of Medicine, 
University of Florida, Gainesville, FL.
(2)Department of Biostatistics and Health Data Science, School of Medicine, 
Indiana University, Indianapolis, IN.
(3)Regenstrief Institute, Indianapolis, IN.
(4)Indiana University Health, Indianapolis, IN.
(5)Riley Hospital for Children at Indiana University Health, Indianapolis, IN.
(6)Division of Pediatric Hospital Medicine, School of Medicine, Indiana 
University, Indianapolis, IN.
(7)Section of Biomedical Informatics and Data Science, School of Medicine, Yale 
University, Yale, NH.

PURPOSE: Patient recruitment remains a major bottleneck in clinical trial 
execution, with inefficient patient-trial matching often causing delays and 
failures. Recent advancements in large language models (LLMs) offer a promising 
avenue for automating and improving this process. This scoping review aims to 
provide a comprehensive synthesis of the emerging applications of LLMs in 
patient-trial matching.
METHODS: A comprehensive search was conducted in PubMed, Web of Science, and 
OpenAlex for literature published between December 1, 2022, and December 31, 
2024. Studies were included if they explicitly integrated LLMs into 
patient-trial matching systems. Data extraction focused on system architectures, 
patient data processing, eligibility criteria processing, matching techniques, 
evaluation metrics, and performance.
RESULTS: Of the 2,357 studies initially identified, 24 met the inclusion 
criteria. The majority (21/24) were published in 2024, highlighting the rapid 
adoption of LLMs in this domain. Most systems used patient-centric matching 
(17/24), with OpenAI's generative pretrained transformer models being the most 
commonly used LLM. Core components of these systems included eligibility 
criteria processing, patient data processing, and matching, with some 
incorporating retrieval algorithms to enhance computational efficiency. 
LLM-integrated approaches demonstrated improved accuracy and scalability in 
patient-trial matching, although challenges such as performance variability, 
interpretability, and reliance on synthetic data sets remain significant.
CONCLUSION: LLM-based patient-trial matching systems present a transformative 
opportunity to enhance the efficiency and accuracy of clinical trial 
recruitment. Despite current limitations related to model generalizability, 
explainability, and data constraints, future advancements in hybrid modeling 
strategies, domain-specific fine-tuning, and real-world data set integration 
could further optimize LLM-based trial matching. Addressing these challenges 
will be crucial to realizing the full potential of LLMs in streamlining patient 
recruitment and accelerating clinical trial execution.

DOI: 10.1200/CCI-25-00071
PMCID: PMC12169815
PMID: 40489722 [Indexed for MEDLINE]

Conflict of interest statement: The following represents disclosure information 
provided by authors of this manuscript. All relationships are considered 
compensated unless otherwise noted. Relationships are self-held unless noted. I 
= Immediate Family Member, Inst = My Institution. Relationships may not relate 
to the subject matter of this manuscript. For more information about ASCO's 
conflict of interest policy, please refer to www.asco.org/rwc or 
ascopubs.org/cci/author-center. Open Payments is a public database containing 
information reported by companies about payments made to US-licensed physicians 
(Open Payments). James McGill Employment: Lilly Leadership: Lilly Stock and 
Other Ownership Interests: Lilly Consulting or Advisory Role: Paratus Sciences 
Corporation, PromiseBio, Iterative Health Research Funding: Lilly Travel, 
Accommodations, Expenses: Versiti Emily C. Webber Leadership: Indiana University 
Health Travel, Accommodations, Expenses: American Board of Preventative Medicine 
Hua Xu Stock and Other Ownership Interests: More Health, Melax Tech Inc 
Consulting or Advisory Role: IMO Inc Patents, Royalties, Other Intellectual 
Property: Software license income from University of Texas Health Science Center 
at Houston No other potential conflicts of interest were reported.


365. Dent Traumatol. 2025 Dec;41(6):696-705. doi: 10.1111/edt.13078. Epub 2025 Jun 9.

Evaluation of the Performance of Artificial Intelligence Based Chatbots in 
Providing First Aid Information on Dental Trauma According to the ToothSOS 
Application.

Çege EE(1), Cömert H(2), Akal N(3), Ölmez A(3).

Author information:
(1)Department of Paediatric Dentistry, Faculty of Dentistry, Karabuk University, 
Karabük, Turkey.
(2)Department of Paediatric Dentistry, Faculty of Dentistry, Lokman Hekim 
University, Ankara, Turkey.
(3)Department of Paediatric Dentistry, Faculty of Dentistry, Gazi University, 
Ankara, Turkey.

AIM: The aim of this study was to evaluate the performance of ChatGPT-4o and 
Gemini Advanced artificial intelligence-based chatbots (AI-based chatbots) in 
providing emergency intervention recommendations for dental trauma with 
intraoral photographs of patients diagnosed with traumatic dental injuries, and 
to assess their compatibility with emergency intervention recommendations in the 
ToothSOS application.
MATERIAL AND METHODS: In this study, 80 intraoral photographs obtained from 
patients presenting with dental trauma were uploaded to two different AI-based 
chatbots (ChatGPT-4o and Gemini Advanced) and the responses generated by these 
systems were evaluated by four paediatric dentists. The evaluators scored the 
responses with a Modified Global Quality Score (GQS), referring to the English 
instructions of the ToothSOS application. In order to analyse the reliability of 
the responses, a total of three evaluation sessions were conducted 1 week apart.
RESULTS: The ChatGPT-4o performed better when all injury types were considered 
together (p = 0.012). It was found that the ChatGPT-4o performed much better in 
complicated crown fracture cases (p = 0.004) and the Gemini Advanced chatbot 
performed much better in critical dental injuries such as avulsion (p < 0.001).
CONCLUSIONS: AI-based chatbots can be a helpful tool in the assessment of dental 
trauma. However, further development and expert validation are needed to improve 
their accuracy and consistency, especially in complex cases. Incorporating the 
International Association of Dental Traumatology (IADT) guidelines into the 
databases of these systems could improve the reliability of their 
recommendations. In addition, given the widespread use of AI-based chatbots in 
many fields, particularly health, they could contribute to public health by 
supporting access to accurate information.

© 2025 The Author(s). Dental Traumatology published by John Wiley & Sons Ltd.

DOI: 10.1111/edt.13078
PMCID: PMC12605821
PMID: 40488605 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


366. Integr Med Res. 2025 Jun;14(2):101150. doi: 10.1016/j.imr.2025.101150. Epub 2025 
May 4.

Evaluating and ranking guidelines on traditional and integrative medicine 
globally: Establishment of the International STAR-TIM Committee.

Yu X(1)(2), Qin Y(3)(4)(5), Estill J(3)(6), Liu H(3)(4)(5), Wang X(7), Bian 
Z(1)(2), Chen Y(2)(3)(4)(5)(7); STAR Secretariat.

Author information:
(1)Chinese EQUATOR Centre, Hong Kong Baptist University, Hong Kong, China.
(2)Vincent V.C. Woo Chinese Medicine Clinical Research Institute, School of 
Chinese Medicine, Hong Kong Baptist University, Hong Kong, China.
(3)Research Unit of Evidence-Based Evaluation and Guidelines, Chinese Academy of 
Medical Sciences (2021RU017), School of Basic Medical Sciences, Lanzhou 
University, Lanzhou, China.
(4)Key Laboratory of Evidence Based Medicine of Gansu Province, Lanzhou, China.
(5)Institute of Health Data Science, Lanzhou University, Lanzhou, China.
(6)Institute of Global Health, University of Geneva, Geneva, Switzerland.
(7)Department of Pediatric Research Institute, Chevidence Lab of Child and 
Adolescent Health, Children's Hospital of Chongqing Medical University, 
Chongqing, China.

To further enhance the quality and transparency of practice guidelines and 
consensus statements, the Scientific, Transparent, and Applicable Rankings 
(STAR) working group was officially established and commenced its work in 2021. 
The STAR tool contains 39 items grouped into 11 domains, with each domain and 
item assigned to a weight that reflects its importance. In 2021-2022, a total of 
266 Chinese Traditional and Integrative Medicine (TIM) guidelines and consensus 
statements met the inclusion criteria for STAR evaluation. After evaluating 
these TIM guidelines and consensus statements, the highest STAR score achieved 
was 96.5 (out of 100), with a median score 23.3 and a mean score of 32.0. As the 
number of TIM guidelines and consensus statements published globally continues 
to increase, the STAR Secretariat aims to establish an International STAR 
Specialty Committee for TIM. Utilizing the integrated STAR evaluation system and 
large language models, the committee seeks to enhance the efficiency of 
evaluations without compromising accuracy. This initiative aims to conduct 
evaluations of international TIM guidelines and consensus statements, thereby 
improving their quality and transparency. Additionally, it will assess the 
importance and necessity of developing the International STAR-TIM Extension.

© 2025 Korea Institute of Oriental Medicine. Published by Elsevier B.V.

DOI: 10.1016/j.imr.2025.101150
PMCID: PMC12145758
PMID: 40487779


367. Eur Arch Otorhinolaryngol. 2025 Aug;282(8):4325-4334. doi: 
10.1007/s00405-025-09504-8. Epub 2025 Jun 6.

Clinical decision support using large language models in otolaryngology: a 
systematic review.

Filali Ansary R(1), Lechien JR(2)(3)(4)(5).

Author information:
(1)Department of Surgery, Faculty of Medicine, UMONS Research Institute for 
Health Sciences and Technology, University of Mons (UMons), University of Mons, 
6, Mons, B7000, Belgium.
(2)Department of Surgery, Faculty of Medicine, UMONS Research Institute for 
Health Sciences and Technology, University of Mons (UMons), University of Mons, 
6, Mons, B7000, Belgium. Jerome.Lechien@umons.ac.be.
(3)Department of Otolaryngology-Head and Neck Surgery, School of Medicine, Foch 
Hospital, University Paris Saclay, Paris, France. Jerome.Lechien@umons.ac.be.
(4)Department of Otolaryngology-Head and Neck Surgery, CHU Saint-Pierre, 
Brussels, Belgium. Jerome.Lechien@umons.ac.be.
(5)Department of Otolaryngology, Elsan Hospital, Paris, France. 
Jerome.Lechien@umons.ac.be.

OBJECTIVE: This systematic review evaluated the diagnostic accuracy of large 
language models (LLMs) in otolaryngology-head and neck surgery clinical 
decision-making.
DATA SOURCES: PubMed/MEDLINE, Cochrane Library, and Embase databases were 
searched for studies investigating clinical decision support accuracy of LLMs in 
otolaryngology.
REVIEW METHODS: Three investigators searched the literature for peer-reviewed 
studies investigating the application of LLMs as clinical decision support for 
real clinical cases according to the Preferred Reporting Items for Systematic 
Reviews and Meta-Analyses (PRISMA) guidelines. The following outcomes were 
considered: diagnostic accuracy, additional examination and treatment 
recommendations. Study quality was assessed using the modified Methodological 
Index for Non-Randomized Studies (MINORS).
RESULTS: Of the 285 eligible publications, 17 met the inclusion criteria, 
accounting for 734 patients across various otolaryngology subspecialties. 
ChatGPT-4 was the most evaluated LLM (n = 14/17), followed by Claude-3/3.5 
(n = 2/17), and Gemini (n = 2/17). Primary diagnostic accuracy ranged from 45.7 
to 80.2% across different LLMs, with Claude often outperforming ChatGPT. LLMs 
demonstrated lower accuracy in recommending appropriate additional examinations 
(10-29%) and treatments (16.7-60%), with substantial subspecialty variability. 
Treatment recommendation accuracy was highest in head and neck oncology (55-60%) 
and lowest in rhinology (16.7%). There was substantial heterogeneity across 
studies for the inclusion criteria, information entered in the application 
programming interface, and the methods of accuracy assessment.
CONCLUSIONS: LLMs demonstrate promising moderate diagnostic accuracy in 
otolaryngology clinical decision support, with higher performance in providing 
diagnoses than in suggesting appropriate additional examinations and treatments. 
Emerging findings support that Claude often outperforms ChatGPT. Methodological 
standardization is needed for future research.
LEVEL OF EVIDENCE: NA.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00405-025-09504-8
PMID: 40481345 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethic committee: Not required. 
Informed consent: Not required. Competing interests: The author Jerome R. 
Lechien was not involved with the peer review process of this article.


368. J Dent. 2025 Sep;160:105877. doi: 10.1016/j.jdent.2025.105877. Epub 2025 Jun 4.

Large language models for the screening step in systematic reviews in dentistry.

Rokhshad R(1), Bagherianlemraski M(2), Ehsani SS(3), Haghighat S(4), Schwendicke 
F(5).

Author information:
(1)Department of Pediatric Dentistry, Loma Linda School of Dentistry, Loma 
Linda, USA. Electronic address: Ratarokhshad@gmail.com.
(2)Department of Health Research Methods, Evidence, and Impact, McMaster 
University, Hamilton, Ontario, Canada; Topic Group Dental Diagnostics and 
Digital Dentistry, ITU/WHO Focus Group AI on Health, Berlin, Germany. Electronic 
address: baghem7@mcmaster.ca.
(3)Topic Group Dental Diagnostics and Digital Dentistry, ITU/WHO Focus Group AI 
on Health, Berlin, Germany.
(4)Department of Dentistry and Oral Health, Aarhus University, Aarhus, Denmark.
(5)Clinic for Conservative Dentistry and Periodontology, LMU Klinikum, Munich, 
Germany. Electronic address: falk.schwendicke@med.uni-muenchen.de.

OBJECTIVES: This study assessed the performance of chatbots in the screening 
step of a systematic review (SR) with an exemplary focus on tooth segmentation 
on dental radiographs using artificial intelligence (AI).
METHODS: A comprehensive systematic search was performed in December 2024 across 
seven databases: PubMed, Scopus, Web of Science, Embase, IEEE, Google Scholar, 
and arXiv. Five chatbots-ChatGPT-4, Claude 2 100k, Claude Instant 100k, Meta's 
LLaMA 3, and Gemini-were evaluated for their ability to screen articles on tooth 
segmentation on radiographs using AI. The evaluations took place from January to 
February 2025, focusing on performance metrics such as accuracy, precision, 
sensitivity, specificity, and F1-score for screening quality measured against 
expert reviewers' screening, as well as Cohen's Kappa for inter-rater agreement 
between different chatbots.
RESULTS: A total of 891 studies were screened. Significant variability in the 
number of included or excluded studies was observed (p < 0.001/Chi-square), with 
Claude-instant-100k having the highest inclusion rate (54.88 %) and ChatGPT-4 
the lowest (29.52 %). Gemini excluded the most studies (67.90 %), while 
ChatGPT-4 marked the highest number of studies for full-text review (5.39 %). 
Fleiss' Kappa (-0.147, p < 0.001) indicated systematic disagreement between 
chatbots worse than random chance. Performance metrics varied; ChatGPT-4 had the 
highest precision (24 %) and accuracy (75 %) measured against human expert 
reviewers, while Claude-instant-100k had the highest sensitivity (96 %) but the 
lowest precision (16 %).
CONCLUSION: Chatbots showed limited accuracy during study screening and low 
inter-rater agreement. There remains the need for human oversight during 
systematic reviewing.
CLINICAL SIGNIFICANCE: Theoretically, Chatbots can streamline SR tasks such as 
screening. However, human oversight remains critical to maintain the integrity 
of the review.

Copyright © 2025. Published by Elsevier Ltd.

DOI: 10.1016/j.jdent.2025.105877
PMID: 40480310 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declared no potential conflicts of interest with respect to the research, 
authorship, and/or publication of this article.


369. NPJ Digit Med. 2025 Jun 4;8(1):332. doi: 10.1038/s41746-025-01746-4.

Racial bias in AI-mediated psychiatric diagnosis and treatment: a qualitative 
comparison of four large language models.

Bouguettaya A(1)(2), Stuart EM(3), Aboujaoude E(4)(5).

Author information:
(1)Department of Biomedical Sciences, Cedars-Sinai Medical Center, Los Angeles, 
CA, USA.
(2)School of Nursing and Midwifery, Monash University, Melbourne, VIC, 
Australia.
(3)Jonathan Jaques Children's Cancer Institute, Miller Children's & Women's 
Hospital Long Beach, Long Beach, CA, USA.
(4)Department of Biomedical Sciences, Cedars-Sinai Medical Center, Los Angeles, 
CA, USA. elias.aboujaoude@cshs.org.
(5)Department of Psychiatry and Behavioral Sciences, Stanford University, 
Stanford, CA, USA. elias.aboujaoude@cshs.org.

Artificial intelligence (AI), particularly large language models (LLMs), is 
increasingly integrated into mental health care. This study examined racial bias 
in psychiatric diagnosis and treatment across four leading LLMs: Claude, 
ChatGPT, Gemini, and NewMes-15 (a local, medical-focused LLaMA 3 variant). Ten 
psychiatric patient cases representing five diagnoses were presented to these 
models under three conditions: race-neutral, race-implied, and race-explicitly 
stated (i.e., stating patient is African American). The models' diagnostic 
recommendations and treatment plans were qualitatively evaluated by a clinical 
psychologist and a social psychologist, who scored 120 outputs for bias by 
comparing responses generated under race-neutral, race-implied, and 
race-explicit conditions. Results indicated that LLMs often proposed inferior 
treatments when patient race was explicitly or implicitly indicated, though 
diagnostic decisions demonstrated minimal bias. NewMes-15 exhibited the highest 
degree of racial bias, while Gemini showed the least. These findings underscore 
critical concerns about the potential for AI to perpetuate racial disparities in 
mental healthcare, emphasizing the necessity of rigorous bias assessment in 
algorithmic medical decision support systems.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01746-4
PMCID: PMC12137607
PMID: 40467886

Conflict of interest statement: Competing interests: The authors declare no 
competing interests.


370. BMC Oral Health. 2025 Jun 3;25(1):905. doi: 10.1186/s12903-025-06070-7.

The potentials and challenges of integrating generative artificial intelligence 
(AI) in dental and orthodontic education: a systematic review.

Baxmann M(1), Kárpáti K(2), Baráth Z(3).

Author information:
(1)Department of Orthodontics, Faculty of Education and Research, DTMD 
University, Wiltz, 9516, Luxembourg. info@orthodentix.de.
(2)Department of Orthodontics and Pediatric Dentistry, Faculty of Dentistry, 
University of Szeged, Szeged, 6720, Hungary.
(3)Department of Prosthodontics, Faculty of Dentistry, University of Szeged, 
Szeged, 6720, Hungary.

BACKGROUND: Generative AI technologies offer significant opportunities to 
enhance orthodontic education by improving knowledge retention, clinical 
decision-making, and skills training. This systematic review aimed to evaluate 
the impact of generative AI tools in orthodontic education, focusing on 
knowledge retention, decision-making, and practical skills.
METHODS: A comprehensive literature search was conducted across PubMed, Cochrane 
Library, ERIC, CINAHL, and IEEE Xplore from January 2010 to December 2023. 
Studies evaluating the integration of generative AI in dental and orthodontic 
education were included. Seventeen studies met the inclusion criteria. Risk of 
bias was assessed using the Cochrane Risk of Bias Tool and the Newcastle-Ottawa 
Scale, with the GRADE approach used to evaluate evidence quality.
RESULTS: Generative AI improved knowledge retention and clinical decision-making 
through adaptive learning pathways and real-time feedback. Barriers included 
limited faculty training, technical infrastructure deficits, and educator 
resistance.
CONCLUSIONS: Generative AI holds transformative potential for orthodontic 
education but requires addressing practical and ethical challenges. Future 
research should focus on longitudinal studies to validate long-term impact and 
explore integration strategies.

© 2025. The Author(s).

DOI: 10.1186/s12903-025-06070-7
PMCID: PMC12135255
PMID: 40462083 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Not applicable. Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


371. BMC Public Health. 2025 Jun 2;25(1):2038. doi: 10.1186/s12889-025-23206-0.

Assessing the accuracy and explainability of using ChatGPT to evaluate the 
quality of health news.

Liu X(1), He L(2), Alanazi E(3), Liu E(4), Goss A(4), Gumireddy L(4).

Author information:
(1)College for Public Health and Social Justice, Saint Louis University, St. 
Louis, USA. xiaoyu.liu@slu.edu.
(2)Zilber College of Public Health, University of Wisconsin-Milwaukee, 
Milwaukee, USA.
(3)College of Health Sciences, Saudi Electronic University, Riyadh, Saudi 
Arabia.
(4)College for Public Health and Social Justice, Saint Louis University, St. 
Louis, USA.

BACKGROUND: With the growing prevalence of health misinformation online, there 
is an urgent need for tools that can reliably assist the public in evaluating 
the quality of health information. This study investigates the performance of 
GPT-3.5-Turbo, a representative and widely used large language model (LLM), in 
rating the quality of health news and providing explanatory justification for 
the rating assessment.
METHODS: We evaluated GPT-3.5-Turbo’s performance on 3222 health news articles 
from an expert-annotated dataset compiled by HealthNewsReview.org, which 
assesses the quality of health news across nine criteria. GPT-3.5-Turbo was 
prompted with standardized queries tailored to each criterion. We measured its 
rating performance using 95% confidence intervals for precision, recall, and F1 
scores in binary classification (satisfactory/not satisfactory). Additionally, 
linguistic complexity, readability, and the quality of GPT-3.5-Turbo’s 
explainability were assessed through both quantitative linguistic analysis and 
qualitative evaluation of consistency and contextual relevance.
RESULTS: GPT-3.5-Turbo’s rating performance varied across criteria, with the 
highest accuracy for the Cost criterion (F1 = 0.824) but lower accuracy for 
Benefit, Conflict, and Quality criteria (F1 < 0.5), underperforming compared to 
traditional supervised machine learning models. However, its explanations were 
clear, with readability suited to late high school or early college levels and 
scored highly for consistency (average score: 2.90/3) and contextual relevance 
(average score: 2.73/3). These findings highlight GPT-3.5-Turbo’s strength in 
providing understandable and contextually relevant explanations, despite that 
its rating accuracy is limited.
CONCLUSION: While GPT-3.5-Turbo’s rating accuracy requires improvement, its 
strength in offering comprehensible and contextually relevant explanations 
presents a valuable opportunity to enhance public understanding of health news 
quality. Leveraging LLMs as complementary tools for health literacy initiatives 
could help mitigate misinformation by facilitating non-expert audiences to 
interpret and assess health information.
SUPPLEMENTARY INFORMATION: The online version contains supplementary material 
available at 10.1186/s12889-025-23206-0.

DOI: 10.1186/s12889-025-23206-0
PMCID: PMC12128262
PMID: 40457340

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: No applicable. Consent for publication: No applicable. Competing 
interests: The authors declare no competing interests.


372. J Med Internet Res. 2025 Jun 2;27:e73233. doi: 10.2196/73233.

Enhancing the Accuracy of Human Phenotype Ontology Identification: Comparative 
Evaluation of Multimodal Large Language Models.

Zhong W(1), Sun M(2), Yao S(3), Liu Y(1), Peng D(4), Liu Y(1), Yang K(1), Gao 
H(1), Yan H(1), Hao W(1), Yan Y(#)(1), Yin C(#)(1).

Author information:
(1)Department of Prenatal Diagnosis, Beijing Obstetrics and Gynecology Hospital, 
Capital Medical University, Beijing Maternal and Child Health Care Hospital, 251 
Yaojiayuan Road, Chaoyang District, Beijing, 100020, China, 86 15572779093.
(2)Department of Reproductive Medicine, Shijiazhuang People's Hospital, Hebei 
Province, Shijiazhuang, China.
(3)Department of Gynecology and Obstetrics, Yijishan Hospital of Wannan Medical 
College, Anhui province, Wuhu, China.
(4)School of Medicine, South China University of Technology, Guangdong Province, 
Guangzhou, China.
(#)Contributed equally

BACKGROUND: Identifying Human Phenotype Ontology (HPO) terms is crucial for 
diagnosing and managing rare diseases. However, clinicians, especially junior 
physicians, often face challenges due to the complexity of describing patient 
phenotypes accurately. Traditional manual search methods using HPO databases are 
time-consuming and prone to errors.
OBJECTIVE: The aim of the study is to investigate whether the use of multimodal 
large language models (MLLMs) can improve the accuracy of junior physicians in 
identifying HPO terms from patient images related to rare diseases.
METHODS: In total, 20 junior physicians from 10 specialties participated. Each 
physician evaluated 27 patient images sourced from publicly available 
literature, with phenotypes relevant to rare diseases listed in the Chinese Rare 
Disease Catalogue. The study was divided into 2 groups: the manual search group 
relied on the Chinese Human Phenotype Ontology website, while the MLLM-assisted 
group used an electronic questionnaire that included HPO terms preidentified by 
ChatGPT-4o as prompts, followed by a search using the Chinese Human Phenotype 
Ontology. The primary outcome was the accuracy of HPO identification, defined as 
the proportion of correctly identified HPO terms compared to a standard set 
determined by an expert panel. Additionally, the accuracy of outputs from 
ChatGPT-4o and 2 open-source MLLMs (Llama3.2:11b and Llama3.2:90b) was evaluated 
using the same criteria, with hallucinations for each model documented 
separately. Furthermore, participating physicians completed an additional 
electronic questionnaire regarding their rare disease background to identify 
factors affecting their ability to accurately describe patient images using 
standardized HPO terms.
RESULTS: A total of 270 descriptions were evaluated per group. The MLLM-assisted 
group achieved a significantly higher accuracy rate of 67.4% (182/270) compared 
to 20.4% (55/270) in the manual group (relative risk 3.31, 95% CI 2.58-4.25; 
P<.001). The MLLM-assisted group demonstrated consistent performance across 
departments, whereas the manual group exhibited greater variability. Among 
standalone MLLMs, ChatGPT-4o achieved an accuracy of 48% (13/27), while the 
open-source models Llama3.2:11b and Llama3.2:90b achieved 15% (4/27) and 18% 
(5/27), respectively. However, MLLMs exhibited a high hallucination rate, 
frequently generating HPO terms with incorrect IDs or entirely fabricated 
content. Specifically, ChatGPT-4o, Llama3.2:11b, and Llama3.2:90b generated 
incorrect IDs in 57.3% (67/117), 98% (62/63), and 82% (46/56) of cases, 
respectively, and fabricated terms in 34.2% (40/117), 41% (26/63), and 32% 
(18/56) of cases, respectively. Additionally, a survey on the rare disease 
knowledge of junior physicians suggests that participation in rare disease and 
genetic disease training may enhance the performance of some physicians.
CONCLUSIONS: The integration of MLLMs into clinical workflows significantly 
enhances the accuracy of HPO identification by junior physicians, offering 
promising potential to improve the diagnosis of rare diseases and standardize 
phenotype descriptions in medical research. However, the notable hallucination 
rate observed in MLLMs underscores the necessity for further refinement and 
rigorous validation before widespread adoption in clinical practice.

© Wei Zhong, Mingyue Sun, Shun Yao, YiFan Liu, Dingchuan Peng, Yan Liu, Kai 
Yang, HuiMin Gao, HuiHui Yan, WenJing Hao, YouSheng Yan, ChengHong Yin. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org).

DOI: 10.2196/73233
PMCID: PMC12148245
PMID: 40456109 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


373. Autism Res. 2025 Aug;18(8):1568-1579. doi: 10.1002/aur.70062. Epub 2025 Jun 2.

The Language ENvironment Analysis (LENA) System in Toddlers With Early 
Indicators of Autism: Test-Retest Reliability and Convergent Validity With 
Clinical Language Assessments.

Nadwodny N(1), Yoder PJ(2), Ingersoll BR(3), Wainer AL(4), Stone WL(5), 
Eisenhower A(1), Carter AS(1); RISE Research Network.

Collaborators: Stone WL, Ingersoll BR, Wainer AL, Carter AS, Shedrick RC, 
Broder-Fingert S, Edmunds SR.

Author information:
(1)Department of Psychology, University of Massachusetts Boston, Boston, 
Massachusetts, USA.
(2)Vanderbilt Kennedy Center, Vanderbilt University, Nashville, Tennessee, USA.
(3)Department of Psychology, Michigan State University, East Lansing, Michigan, 
USA.
(4)Department of Psychiatry and Behavioral Sciences, Rush University, Chicago, 
Illinois, USA.
(5)Department of Psychology, University of Washington, Seattle, Washington, USA.

Clinical language assessments often influence the types of services that 
autistic children are eligible to receive. However, these assessments often take 
place outside of the child's natural language environment. In this study, we 
assess the potential of using naturalistic language processing technology, the 
Language ENvironment Analysis (LENA) system, in clinical research. Within a 
sample of caregivers and autistic toddlers aged 16-33 months (N = 100), the 
current study examined associations between all LENA-generated variables and two 
clinical assessments of language: the Vineland Adaptive Behavior Scales, Third 
Edition: Communication Domain and the MacArthur Bates Communicative Development 
Inventories: Vocabulary Checklist. We also evaluated LENA test-retest 
reliability in a subsample of participants (n = 81). Some LENA-generated 
variables-specifically, the Conversational Turn Count, Vocal Productivity, and 
Automated Vocalization Assessment-exhibited small-to-moderate significant 
positive correlations with clinical language assessment variables. Additionally, 
all LENA-generated variables demonstrated moderate-to-good test-retest 
reliability within a 2-week period. To our knowledge, this is the first study 
that examines the psychometric properties of all LENA-generated variables in a 
single large sample. Findings show promising evidence of LENA's utility as a 
source of naturalistic language data for research with autistic toddlers. Trial 
Registration: ClinicalTrials.gov identifier: NCT05114538 ("Improving the Part C 
Early Intervention Service Delivery System for Children with ASD").

© 2025 The Author(s). Autism Research published by International Society for 
Autism Research and Wiley Periodicals LLC.

DOI: 10.1002/aur.70062
PMCID: PMC12384741
PMID: 40452405 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


374. J Endod. 2025 Sep;51(9):1229-1234. doi: 10.1016/j.joen.2025.05.012. Epub 2025 
May 29.

Dental Trauma Evo - Development of an Artificial Intelligence-powered Chatbot to 
Support Professional Management of Dental Trauma.

Bubna DP(1), Felipe de Jesus Freitas P(2), Ferraz AX(3), Abuabara A(4), 
Baratto-Filho F(4), Marques de Mattos de Araujo B(5), Kuchler EC(6), Roskamp 
L(1), Deliga Schroder AG(2), Miranda de Araujo C(7).

Author information:
(1)School of Dentistry, Department of Endodontics, Tuiuti University of Paraná, 
Curitiba, PR, Brazil.
(2)School of Dentistry, Department of Endodontics, Tuiuti University of Paraná, 
Curitiba, PR, Brazil; Center for Artificial Intelligence in Health (NIAS), 
School of Dentistry, Curitiba, PR, Brazil.
(3)Center for Artificial Intelligence in Health (NIAS), School of Dentistry, 
Curitiba, PR, Brazil; Graduate Program in Human Communication Health, Tuiuti 
University of Paraná, Curitiba, PR, Brazil.
(4)School of Dentistry, University of the Region of Joinville (Univille), 
Joinville, Santa Catarina, Brazil.
(5)School of Dentistry, Department of Endodontics, Pontifícia Universidade 
Católica do Paraná, Curitiba, Paraná, Brazil.
(6)Department of Orthodontics, University Hospital Bonn, Bonn, North 
Rhine-Westphalia, Germany.
(7)School of Dentistry, Department of Endodontics, Tuiuti University of Paraná, 
Curitiba, PR, Brazil; Center for Artificial Intelligence in Health (NIAS), 
School of Dentistry, Curitiba, PR, Brazil. Electronic address: 
cristiano.m.araujo@hotmail.com.

INTRODUCTION: Dental trauma is a public health issue that requires proper 
clinical management to minimize complications. Artificial intelligence-based 
technologies can support decision-making by providing standardized guidance. 
This study developed and evaluated an artificial intelligence-powered chatbot to 
assist professionals in managing dental trauma.
METHODS: The Dental Trauma Evo chatbot was developed based on the guidelines of 
the International Association of Dental Traumatology, utilizing a rule-based 
system to ensure recommendations aligned with established clinical protocols. 
The chatbot was implemented in Python, integrated with the ChatGPT-4 API, and 
made available via Streamlit, enabling interactions in over 50 languages. 
Validation was conducted by specialists in endodontics and pediatric dentistry, 
who assessed the clarity and consistency of the responses. Performance was 
evaluated through 384 interactions covering 32 types of trauma, assessing the 
accuracy and completeness of the recommendations.
RESULTS: The chatbot achieved 100% accuracy in recommending appropriate clinical 
management. However, some initial responses did not include all treatment 
options outlined in the guidelines, particularly in cases of enamel, dentin, and 
pulp-exposed fractures in primary teeth, where only 53% of responses were 
complete. After adjustments to the code, the second round of testing showed 
significant improvements, reaching 100% completeness in most cases. Only the 
recommendations for the reimplantation of avulsed teeth with an open apex 
reached 93%, indicating the need for further refinements.
CONCLUSIONS: The artificial intelligence-powered chatbot demonstrated high 
performance in standardizing the management of dental trauma, proving its 
potential to assist professionals in decision-making by providing quick and 
precise responses aligned with international guidelines.

Copyright © 2025 American Association of Endodontists. Published by Elsevier 
Inc. All rights reserved.

DOI: 10.1016/j.joen.2025.05.012
PMID: 40449881 [Indexed for MEDLINE]


375. Med Image Anal. 2025 Aug;104:103614. doi: 10.1016/j.media.2025.103614. Epub 2025 
May 26.

ScanAhead: Simplifying standard plane acquisition of fetal head ultrasound.

Men Q(1), Zhao H(2), Drukker L(3), Papageorghiou AT(4), Noble JA(5).

Author information:
(1)Institute of Biomedical Engineering, Department of Engineering Science, 
University of Oxford, Oxford, OX3 7DQ, United Kingdom; School of Engineering 
Mathematics and Technology, University of Bristol, Bristol, BS8 1TW, United 
Kingdom. Electronic address: qianhui.men@bristol.ac.uk.
(2)Institute of Biomedical Engineering, Department of Engineering Science, 
University of Oxford, Oxford, OX3 7DQ, United Kingdom; Department of Eye and 
Vision Sciences, University of Liverpool, Liverpool, L69 7ZB, United Kingdom.
(3)Nuffield Department of Women's & Reproductive Health, University of Oxford, 
Oxford, OX3 9DU, United Kingdom; Department of Obstetrics and Gynecology, 
Tel-Aviv University, Tel Aviv, Ramat Aviv, 69978, Israel.
(4)Nuffield Department of Women's & Reproductive Health, University of Oxford, 
Oxford, OX3 9DU, United Kingdom.
(5)Institute of Biomedical Engineering, Department of Engineering Science, 
University of Oxford, Oxford, OX3 7DQ, United Kingdom.

The fetal standard plane acquisition task aims to detect an Ultrasound (US) 
image characterized by specified anatomical landmarks and appearance for 
assessing fetal growth. However, in practice, due to variability in human 
operator skill and possible fetal motion, it can be challenging for a human 
operator to acquire a satisfactory standard plane. To support a human operator 
with this task, this paper first describes an approach to automatically predict 
the fetal head standard plane from a video segment approaching the standard 
plane. A transformer-based image predictor is proposed to produce a high-quality 
standard plane by understanding diverse scales of head anatomy within the US 
video frame. Because of the visual gap between the video frames and standard 
plane image, the predictor is equipped with an offset adaptor that performs 
domain adaption to translate the off-plane structures to the anatomies that 
would usually appear in a standard plane view. To enhance the anatomical details 
of the predicted US image, the approach is extended by utilizing a second 
modality, US probe movement, that provides 3D location information. Quantitative 
and qualitative studies conducted on two different head biometry planes 
demonstrate that the proposed US image predictor produces clinically plausible 
standard planes with superior performance to comparative published methods. The 
results of dual-modality solution show an improved visualization with enhanced 
anatomical details of the predicted US image. Clinical evaluations are also 
conducted to demonstrate the consistency between the predicted echo textures and 
the expected echo patterns seen in a typical real standard plane, which 
indicates its clinical feasibility for improving the standard plane acquisition 
process.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.media.2025.103614
PMID: 40446481 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


376. Front Digit Health. 2025 May 15;7:1500220. doi: 10.3389/fdgth.2025.1500220. 
eCollection 2025.

Personalization variables in digital mental health interventions for depression 
and anxiety in adolescents and youth: a scoping review.

Wanniarachchi VU(1), Greenhalgh C(2), Choi A(1), Warren JR(1).

Author information:
(1)School of Computer Science, University of Auckland, Auckland, New Zealand.
(2)School of Computer Science, University of Nottingham, Nottingham, United 
Kingdom.

INTRODUCTION: The impact of personalization on user engagement and adherence in 
digital mental health interventions (DMHIs) has been widely explored. However, 
there is a lack of clarity regarding the prevalence of its application, as well 
as the dimensions and mechanisms of personalization within DMHIs for adolescents 
and youth.
METHODS: To understand how personalization has been applied in DMHIs for 
adolescents and young people, a scoping review was conducted. Empirical studies 
on DMHIs for adolescents and youth with depression and anxiety, published 
between 2013 and July 2024, were extracted from PubMed and Scopus. A total of 67 
studies were included in the review. Additionally, we expanded an existing 
personalization framework, which originally classified personalization into four 
dimensions (content, order, guidance, and communication) and four mechanisms 
(user choice, provider choice, rule-based, and machine learning), by 
incorporating non-therapeutic elements.
RESULTS: The adapted framework includes therapeutic and non-therapeutic content, 
order, guidance, therapeutic and non-therapeutic communication, interfaces 
(customization of non-therapeutic visual or interactive components), and 
interactivity (personalization of user preferences), while retaining the 
original mechanisms. Half of the interventions studied used only one 
personalization dimension (51%), and more than two-thirds used only one 
personalization mechanism. This review found that personalization of therapeutic 
content (51% of the interventions) and interfaces (25%) were favored. User 
choice was the most prevalent personalization mechanism, present in 60% of 
interventions. Additionally, machine learning mechanisms were employed in a 
substantial number of cases (30%), but there were no instances of generative 
artificial intelligence (AI) among the included studies.
DISCUSSION: The findings of the review suggest that although personalization 
elements of the interventions are reported in the articles, their impact on 
younger people's experience with DMHIs and adherence to mental health protocols 
is not thoroughly addressed. Future interventions may benefit from incorporating 
generative AI, while adhering to standard clinical research practices, to 
further personalize user experiences.

© 2025 Wanniarachchi, Greenhalgh, Choi and Warren.

DOI: 10.3389/fdgth.2025.1500220
PMCID: PMC12119569
PMID: 40444184

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


377. BMJ Open Qual. 2025 May 28;14(2):e003330. doi: 10.1136/bmjoq-2025-003330.

How does QI work? A trust-building framework in African healthcare: primary 
evidence from Kenya and Malawi.

Adam MB(1)(2), Makobu NW(3), Mate K(4)(5), Newman T(6), Donelson AJ(7).

Author information:
(1)Head of Research AIC Kijabe Hospital, Kijabe, Kenya mary.b.adam@gmail.com.
(2)Africa Consortium for Quality Improvement Research in Frontline Healthcare 
(ACQUIRE), Nairobi, Kenya.
(3)Maternal Newborn Community Health, AIC Kijabe Hospital, Kijabe, Kenya.
(4)Institute for Healthcare Improvement, Cambridge, Massachusetts, USA.
(5)Medicine, Weill Cornell Medical School, New York, New York, USA.
(6)Santa Cruz River Analytics, Tucson, Arizona, USA.
(7)Donelson Consulting, Tucson, Arizona, USA.

Trust is fundamental to the effective functioning of healthcare systems, 
influencing access, utilisation and adherence to evidence-based practices. While 
quality improvement (QI) processes are widely recognised for addressing 
technical challenges, their role in fostering trust and relationships within 
health systems remains underexplored. This study examines the relationship 
dynamics in QI teams and how trust-building frameworks align with adaptive 
processes in healthcare settings. We conducted a qualitative study involving 30 
healthcare workers from six African countries, recruited through the Africa 
Consortium for Quality Improvement in Frontline Healthcare. Data were collected 
through semistructured interviews, transcribed and analysed using both inductive 
and deductive methods. Deductive analysis was guided by a published 
trust-building framework, while insights from a large language model were 
incorporated in addition to a traditional analysis to provide an unbiased 
perspective. Results identified three theoretically described dimensions of 
trust-building within QI teams: common goals, self-interest and 
gratitude/indebtedness. Common goals fostered teamwork, multidisciplinary 
collaboration and effective communication, while self-interest motivated 
personal and professional growth. Gratitude and recognition reinforced team 
cohesion and sustained motivation. Participants highlighted the importance of 
trust in achieving project success, noting that robust relationships within 
teams correlated with improved outcomes. The study underscores the dual nature 
of QI processes, which simultaneously address technical improvements and 
adaptive challenges, including trust and relationship-building. Trust-building, 
framed as an iterative process of aligning common goals, recognising 
contributions and addressing individual interests, complements technical QI 
methodologies like Plan-Do-Study-Act cycles. These findings support expanding QI 
frameworks to emphasise relational dynamics, contributing to more sustainable 
and impactful healthcare improvements. Further research should continue to 
explore the adaptive dimensions of QI, integrating recent research on culturally 
relevant frameworks prioritising kindness in healthcare systems, to enhance 
trust and collaboration within healthcare systems.

© Author(s) (or their employer(s)) 2025. Re-use permitted under CC BY-NC. No 
commercial re-use. See rights and permissions. Published by BMJ Group.

DOI: 10.1136/bmjoq-2025-003330
PMCID: PMC12121567
PMID: 40441735 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: None of the authors have 
any competing interests. They have completed the unfiled competing interest 
statement from ICJME.


378. Front Microbiol. 2025 May 14;16:1510126. doi: 10.3389/fmicb.2025.1510126. 
eCollection 2025.

Construction of a predictive model for rebleeding risk in upper gastrointestinal 
bleeding patients based on clinical indicators such as Helicobacter pylori 
infection.

Zang W(1), Lin Z(2), Zhao Y(2), Jia T(2), Zhang X(3).

Author information:
(1)Department of Radiology, ShengJing Hospital of China Medical University, 
Shenyang, Liaoning, China.
(2)The Second Clinical College of China Medical University, Shenyang, Liaoning, 
China.
(3)The Fourth Affiliated Hospital of China Medical University, Shenyang, 
Liaoning, China.

BACKGROUND: The annual incidence of upper gastrointestinal hemorrhage (UGIB) is 
about 60 cases/100,000 people, and about 40% of UGIB patients have hemorrhagic 
ulcers. Ulcer formation is often associated with Helicobacter pylori (H. pylori) 
infection, non-steroidal anti-inflammatory drugs (NSAIDs) use and other factors, 
so ulcerative disease is the main cause of upper gastrointestinal bleeding. H. 
pylori induces chronic superficial gastritis with neutrophils infiltrating into 
the mucosa, so it is assumed that H. pylori infection is the basis of bleeding 
lesions. H. pylori infection is widespread worldwide, with about 50% of the 
population carrying the bacteria. Mortality during hospitalization is higher in 
patients with UGIB because rebleeding significantly increases the risk of death, 
especially if timely intervention is not provided. Rebleeding may also lead to 
severe complications such as shock and multiple organ failure. At present, the 
commonly used clinical scores for UGIB patients mainly include Rockall score 
(RS), AIMS65 score and Glasgow-Blatchford score (GBS). Because some hospitals 
are limited by local medical and health conditions, they lack timely and 
accurate endoscopic diagnosis and treatment equipment, and it is difficult to 
make accurate and timely judgments on patients.
METHOD: In this experiment, 254 patients with upper digestive tract hemorrhage 
from Shengjing Hospital affiliated to China Medical University were collected, 
and the clinical indicators and information of H. pylori infection, age, shock 
state, concomitant disease, H. pylori infection degree, systolic blood pressure, 
blood urea nitrogen, hemoglobin, pulse, black stool, syncope, liver disease and 
other patients were finally collected. We analyzed the correlation between 
various clinical indicators and rebleeding in hospitalized patients. Based on 
the collected clinical information and laboratory indicators, this study 
constructed a deep learning model, the data is divided into four categories 
(clinical information, vital signs, laboratory examination items, stool 
examination) as input, and Transformer is used as feature extractor. KAN as a 
classifier to predict the risk of rebleeding in patients with upper 
gastrointestinal bleeding. The model uses five-fold cross validation and 
calculates key metrics such as accuracy to evaluate its performance. In 
addition, the deep learning model was compared with a variety of machine 
learning methods (decision tree, random forest, logistic regression, K-nearest 
neighbor) and common clinical risk scores (Rockall score, AIMS65 score, 
Glasgow-Blatchford score) to verify its effectiveness and advantages. In order 
to highlight the importance of H. pylori infection degree to the model 
performance, we conducted a comparative experiment to observe the role of H. 
pylori infection degree in the model.
RESULTS: In the correlation analysis between rebleeding and clinical data and 
related indicators, the risk of rebleeding in men (62.5%) was higher than that 
in women (43.47%), and the risk of rebleeding in patients with concurrent 
diseases (60.37%) was higher than that in patients without concurrent diseases. 
In the analysis of the correlation between the degree of infection and the 
laboratory test items, the hemoglobin level of patients will also change with 
the change of the degree of infection of patients (p < 0.05 in the above 
correlation analysis, all had statistical significance). The rebleeding 
detection rates of Rockall score, AIMS65 score and Glasgow Blatchford score were 
16.14%, 0 and 77.17%, respectively. Of the four machine learning models, Random 
Forest (RF) had the highest accuracy on the test set at 0.68. The accuracy of 
the deep learning model on the verification set is the highest of 0.9750, and 
the accuracy of the test set is the highest of 0.9615. In addition, by exploring 
the influence of infection on the model prediction, it was found that the 
prediction accuracy of rebleeding in the non-H. pylori infection group (0.8989) 
was lower than that in the H. pylori infection group (0.9636), and other 
evaluation parameters were also lower than that in the infection group. In 
addition, by adding irrelevant random noise to mask the influence of infection 
degree on model output, it is found that the model prediction accuracy (0.7992) 
is significantly reduced.
CONCLUSION: Based on the degree of H. pylori infection in patients with upper 
gastrointestinal bleeding, combined with a number of clinical laboratory tests 
and clinical data, we developed a clinical model for predicting the risk of 
rebleeding in patients with upper gastrointestinal bleeding. It provides an 
early prediction of rebleeding during a patient's hospitalization and optimizes 
early intervention for patients to a certain extent. It provides a more concise, 
convenient and effective guidance scheme for small and medium-sized hospitals to 
make clinical decisions for UGIB patients.

Copyright © 2025 Zang, Lin, Zhao, Jia and Zhang.

DOI: 10.3389/fmicb.2025.1510126
PMCID: PMC12116659
PMID: 40438212

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


379. Sci Rep. 2025 May 28;15(1):18655. doi: 10.1038/s41598-025-99659-5.

An interpretable deep learning framework using FCT-SMOTE and BO-TabNet 
algorithms for reservoir water sensitivity damage prediction.

He YB(1)(2), Sheng KM(3)(4), Du ML(3)(4), Jiang GC(5)(6)(7), Dong TF(3)(4), Guo 
L(8), Xu BT(8).

Author information:
(1)National Key Laboratory of Petroleum Resources and Engineering, China 
University of Petroleum (Beijing), Beijing, 102249, China. heyb@cup.edu.cn.
(2)MOE Key Laboratory of Petroleum Engineering, China University of Petroleum 
(Beijing), Beijing, 102249, China. heyb@cup.edu.cn.
(3)National Key Laboratory of Petroleum Resources and Engineering, China 
University of Petroleum (Beijing), Beijing, 102249, China.
(4)MOE Key Laboratory of Petroleum Engineering, China University of Petroleum 
(Beijing), Beijing, 102249, China.
(5)National Key Laboratory of Petroleum Resources and Engineering, China 
University of Petroleum (Beijing), Beijing, 102249, China. 
m15600263100_1@163.com.
(6)MOE Key Laboratory of Petroleum Engineering, China University of Petroleum 
(Beijing), Beijing, 102249, China. m15600263100_1@163.com.
(7)National Engineering Research Center of Oil & Gas Drilling and Completion 
Technology, Beijing, 102249, China. m15600263100_1@163.com.
(8)China Oilfield Services Limited, Sanhe, 065201, Hebei Province, China.

This study proposes an interpretable deep learning framework to address the 
high-dimensional and inherently unpredictable challenges associated with oil and 
gas drilling and completion operations. By comparing TabNet, Tab Transformer, 
Hopular, and TabDDPM through computational experiments under identical 
conditions, TabNet was selected as the optimal approach. The framework 
integrates Bayesian optimization (BO) with TabNet to model complex oilfield 
tabular datasets. Fair Cut Tree (FCT) and Synthetic Minority Over-sampling 
Technique (SMOTE) are incorporated to mitigate data missingness and imbalance, 
thereby enhancing dataset integrity and robustness. Empirical validation was 
conducted using 270 data entries collected from 15 distinct oil fields, 
specifically focusing on reservoir water sensitivity damage in natural core 
samples. The proposed framework exhibited superior predictive accuracy for the 
water sensitivity index on an independent test set, achieving a mean absolute 
percentage error (MAPE) of 4.4495% and a root mean square error (RMSE) of 4.05, 
underscoring its strong generalization capability. Moreover, this methodological 
approach enables a quantitative assessment of the influence of critical factors, 
including reservoir water salinity, initial permeability, and the mineralogical 
composition of rock formations, on water sensitivity predictions. This 
represents a significant advancement from traditional qualitative analyses to a 
more rigorous quantitative factor analysis, with the interpretability findings 
corroborating established mechanistic insights. The proposed framework offers a 
versatile and reliable solution for precise predictive modeling in complex 
drilling and completion scenarios reliant on tabular data, thereby providing a 
robust theoretical foundation and algorithmic support for accurate forecasting 
in the oil and gas industry.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-99659-5
PMCID: PMC12119900
PMID: 40437186

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


380. Sci Rep. 2025 May 27;15(1):18584. doi: 10.1038/s41598-025-03604-5.

Smart intrusion detection model to identify unknown attacks for improved road 
safety and management.

Alshammari F(1)(2), Alsaleh A(3).

Author information:
(1)Department of Educational Studies, College of Education, Majmaah University, 
Majmaah, Saudi Arabia.
(2)School of Education, University of Queensland, Brisbane, QLD, Australia.
(3)Department of Computer Engineering, College of Computer and Information 
Sciences, Majmaah University, Majmaah, Saudi Arabia. alsaleh@mu.edu.sa.

The Internet of Vehicles (IoV) has emerged as a transformative technology for 
intelligent transportation systems, enabling real-time communication between 
vehicles, infrastructure and external networks. However, this connectivity also 
introduces significant cybersecurity risks, such as spoofing, injection and 
denial of service (DoS) attacks, which threaten operational safety and system 
reliability. To address these challenges, this study proposes the adaptive 
CNN-based intrusion detection system (ACIDS), a robust and scalable framework 
designed to enhance intrusion detection in IoV environments. ACIDS integrates 
convolutional neural networks (CNN) for hierarchical feature extraction, the 
synthetic minority over-sampling technique (SMOTE) to address class imbalance 
and an open-set classification framework to detect novel attack patterns. The 
model was evaluated on the AWID dataset, achieving an accuracy of 94%, a perfect 
detection rate of 100% and a low false alarm rate of 3%. Additional validation 
on the NSL-KDD dataset confirmed its generalizability, with an accuracy of 91.7% 
and a detection rate of 98.3%. These results significantly outperform baseline 
models, including support vector machines (SVM) and random forests (RF), as well 
as recent methods such as transformer-based and hybrid RNN-CNN approaches. Key 
parameters used for benchmarking include accuracy, detection rate, false alarm 
rate, precision, F1-Score and AUC-ROC, demonstrating the model's balanced 
performance and computational efficiency. By addressing critical issues such as 
class imbalance, adaptability to novel threats and real-time scalability, ACIDS 
offers a practical solution for securing IoV systems. Its low computational 
overhead and ability to operate on resource-constrained edge devices further 
emphasize its suitability for real-world deployments.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-03604-5
PMCID: PMC12116748
PMID: 40425772

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


381. Depress Anxiety. 2025 May 19;2025:9943590. doi: 10.1155/da/9943590. eCollection 
2025.

From the p-Factor to Cognitive Content: Detection and Discrimination of 
Psychopathologies Based on Explainable Artificial Intelligence.

Eyrikaya E(1), Dağ İ(2).

Author information:
(1)Department of Clinical Psychology, Graduate School of Social Science, Ankara 
University, Ankara, Türkiye.
(2)Department of Clinical Psychology, Faculty of Letters, Hacettepe University, 
Ankara, Türkiye.

Background and Aims: Differentiating psychopathologies is challenging due to 
shared underlying mechanisms, such as the p-factor. Nevertheless, recent 
methodological advances suggest that distinct linguistic markers can help detect 
and differentiate these conditions. This study aimed to use cognitive content 
analysis with advanced natural language processing (NLP) and machine learning 
(ML) to (Study 1) distinguish among control, depression, anxiety, and 
depressive-anxiety groups and (Study 2) detect general psychopathology. Methods: 
Data from 1901 participants (retained from 2551 respondents aged 18-43 years who 
completed the Beier sentence completion test [BSCT]) were analyzed. For Study 1, 
groups were formed using the Depression, Anxiety, and Stress Scale (DASS-21); 
negative mood was assessed via the Positive and Negative Affect Schedule 
(PANAS). For Study 2, the Brief Symptom Inventory (BSI) categorized general 
psychopathology and self-reported diagnostic status served as external 
validation. Two analytic approaches were employed: (1) textual analysis with a 
bidirectional encoder representations from transformers (BERT) model and (2) 
subscale-score analysis using a support vector machine (SVM). SHapley Additive 
exPlanations (SHAP) interpreted the ML models. Results: In Study 1, the models 
distinguished control, depression, anxiety, and depressive-anxiety groups. 
Anxiety was marked by positive content, hope, and I-Talk, whereas depression 
involved negative, hopeless content. Depressive-anxiety combined features of 
anxiety with a pronounced negative outlook, suggesting a transitional phase 
where diminishing hope may bridge anxiety to depression. In Study 2, the models 
performed high in distinguishing the self-reported pathology diagnosis group 
(area under the curve [AUC]: 0.81 [BERT], 0.85 [SVM]) from subclinical samples 
but failed to differentiate the self-reported past diagnosis (AUC: 0.53 [BERT], 
0.57 [SVM]) group from controls. This implies that cognitive changes in 
psychopathology may share a consistent underlying structure like p-factor. 
Conclusion: These pioneer findings demonstrate that integrating advanced 
computational techniques can identify key linguistic markers and guide the 
development of language-based diagnostic tools, potentially transforming mental 
health diagnostics.

Copyright © 2025 Erkan Eyrikaya and İhsan Dağ. Depression and Anxiety published 
by John Wiley & Sons Ltd.

DOI: 10.1155/da/9943590
PMCID: PMC12105905
PMID: 40421470 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


382. J Med Imaging (Bellingham). 2025 May;12(3):034501. doi: 
10.1117/1.JMI.12.3.034501. Epub 2025 May 21.

Convolutional variational auto-encoder and vision transformer hybrid approach 
for enhanced early Alzheimer's detection.

Fonseka H(1), Varastehpour S(1), Shakiba M(1), Golkar E(2), Tien D(3).

Author information:
(1)Unitec Institute of Technology, School of Computing, Electrical and Applied 
Technology, Auckland, New Zealand.
(2)Thomas Jefferson University, Department of Radiation Oncology, Philadelphia, 
Pennsylvania, United States.
(3)Charles Sturt University, Bathurst, New South Wales, Australia.

PURPOSE: Alzheimer's disease (AD) is becoming more prevalent among the elderly, 
with projections indicating that it will affect a significantly large population 
in the future. Regardless of substantial research efforts and investments 
focused on exploring the underlying biological factors, a definitive cure has 
yet to be discovered. The currently available treatments are only effective in 
slowing disease progression if it is identified in the early stages of the 
disease. Therefore, early diagnosis has become critical in treating AD.
APPROACH: Recently, the use of deep learning techniques has demonstrated 
remarkable improvement in enhancing the precision and speed of automatic AD 
diagnosis through medical image analysis. We propose a hybrid model that 
integrates a convolutional variational auto-encoder (CVAE) with a vision 
transformer (ViT). During the encoding phase, the CVAE captures key features 
from the MRI scans, whereas the decoding phase reduces irrelevant details in 
MRIs. These refined inputs enhance the ViT's ability to analyze complex patterns 
through its multihead attention mechanism.
RESULTS: The model was trained and evaluated using 14,000 structural MRI samples 
from the ADNI and SCAN databases. Compared with three benchmark methods and 
previous studies with Alzheimer's classification techniques, our approach 
achieved a significant improvement, with a test accuracy of 93.3%.
CONCLUSIONS: Through this research, we identified the potential of the CVAE-ViT 
hybrid approach in detecting minor structural abnormalities related to AD. 
Integrating unsupervised feature extraction via CVAE can significantly enhance 
transformer-based models in distinguishing between stages of cognitive 
impairment, thereby identifying early indicators of AD.

© 2025 Society of Photo-Optical Instrumentation Engineers (SPIE).

DOI: 10.1117/1.JMI.12.3.034501
PMCID: PMC12094909
PMID: 40415866


383. JAMA Netw Open. 2025 May 1;8(5):e2511922. doi: 
10.1001/jamanetworkopen.2025.11922.

Large Language Models and Text Embeddings for Detecting Depression and Suicide 
in Patient Narratives.

Lho SK(1), Park SC(2), Lee H(1)(3), Oh DY(1), Kim H(1), Jang S(1), Jung 
HY(1)(4), Yoo SY(1)(4), Park SM(5), Lee JY(1)(3)(4).

Author information:
(1)Department of Psychiatry, Seoul Metropolitan Government-Seoul National 
University Boramae Medical Center, Seoul, Republic of Korea.
(2)Institute for Biomaterials, Korea University, Seoul, Republic of Korea.
(3)Interdisciplinary Program in Cognitive Science, Seoul National University, 
Seoul, Republic of Korea.
(4)Department of Psychiatry, Seoul National University College of Medicine, 
Seoul, Republic of Korea.
(5)Department of Counseling Psychology, Hannam University, Daejeon, Republic of 
Korea.

IMPORTANCE: Large language models (LLMs) and text-embedding models have shown 
potential in assessing mental health risks based on narrative data from 
psychiatric patients.
OBJECTIVE: To assess whether LLMs and text-embedding models can identify 
depression and suicide risk based on sentence completion test (SCT) narratives 
of psychiatric patients.
DESIGN, SETTING, AND PARTICIPANTS: This cross-sectional study, conducted at 
Seoul Metropolitan Government-Seoul National University Boramae Medical Center, 
analyzed SCT data collected from April 1, 2016, to September 30, 2021. 
Participants included psychiatric patients aged 18 to 39 years who completed SCT 
and self-assessments for depression (Beck Depression Inventory-II or Zung 
Self-Rating Depression Scale) and/or suicide (Beck Scale for Suicidal Ideation). 
Patients confirmed to have an IQ below 70 were excluded, leaving 1064 eligible 
SCT datasets (52 627 completed responses). Data processing with LLMs (GPT-4o, 
May 13, 2024, version; OpenAI [hereafter, LLM1]; gemini-1.0-pro, February 2024 
version; Google DeepMind [hereafter, LLM2]; and GPT-3.5-turbo-16k, January 25, 
2024, version; OpenAI) and text-embedding models (text-embedding-3-large, OpenAI 
[hereafter, text-embedding 1]; text-embedding3-small; OpenAI; and 
text-embedding-ada-002; OpenAI) was performed between July 4 and September 30, 
2024.
MAIN OUTCOMES AND MEASURES: Outcomes included the performance of LLMs and 
text-embedding models in detecting depression and suicide, as measured by the 
area under the receiver operating characteristic curve (AUROC), balanced 
accuracy, and macro F1-score. Performance was evaluated across concatenated 
narratives of SCT, including self-concept, family, gender perception, and 
interpersonal relations narratives.
RESULTS: Based on SCT narratives from 1064 patients (mean [SD] age, 25.4 [5.5] 
years; 673 men [63.3%]), LLM1 showed strong performance in zero-shot learning, 
with an AUROC of 0.720 (95% CI, 0.689-0.752) for depression and 0.731 (95% CI, 
0.704-0.762) for suicide risk using self-concept narratives. Few-shot learning 
for depression further improved the performance of LLM1 (AUROC, 0.754 [95% CI, 
0.721-0.784]) and LLM2 (AUROC, 0.736 [95% CI, 0.704-0.770]). The text-embedding 
1 model paired with extreme gradient boosting outperformed other models, 
achieving an AUROC of 0.841 (95% CI, 0.783-0.897) for depression and 0.724 (95% 
CI, 0.650-0.795) for suicide risk. Overall, self-concept narratives showed the 
most accurate detections across all models.
CONCLUSIONS AND RELEVANCE: This cross-sectional study of SCT narratives from 
psychiatric patients suggests that LLMs and text-embedding models may 
effectively detect depression and suicide risk, particularly using self-concept 
narratives. However, while these models demonstrated potential for detecting 
mental health risks, further improvements in performance and safety are 
essential before clinical application.

DOI: 10.1001/jamanetworkopen.2025.11922
PMCID: PMC12102709
PMID: 40408109 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest Disclosures: None reported.


384. Am J Speech Lang Pathol. 2025 Jul 10;34(4):2098-2114. doi: 
10.1044/2025_AJSLP-24-00464. Epub 2025 May 22.

Clinical Application of Large Language Models for Intervention Plan Development 
in Speech-Language Pathology.

Kim N(1), Homer M(1), Jang H(2).

Author information:
(1)California Baptist University, Riverside.
(2)Indiana University Indianapolis.

PURPOSE: This study investigates the speech and language intervention plan 
outputs generated by six different artificial intelligence (AI) tools powered by 
large language models (LLMs), currently available for clinical writing in the 
field of speech-language pathology. This study aims to evaluate the potential 
applications and limitations of these AI tools, as well as their ability to 
provide relevant and reliable information for developing intervention plans.
METHOD: Using a mixed design including both quantitative and qualitative 
analyses, this study compared the performance outputs of the six AI tools across 
three fictional clinical cases, each involving different types of speech and 
language disorders in 5-year-old children. Two types of command prompts, each 
with three levels of input specificity, were used to generate AI outputs.
RESULTS: Results revealed that the intervention plans generated by these AI 
tools were rated between Needs Improvement and Meets Expectations in terms of 
clinical knowledge and competency. Detailed and structured command prompts than 
general prompts yielded outputs with higher ratings, while the specificity of 
case information did not consistently influence the outputs. Each AI tool 
demonstrated unique strengths and limitations in supporting the development of 
intervention plans.
CONCLUSION: The results of this study may serve as foundational data to provide 
insights into how clinicians, educators, and students in the field of 
speech-language pathology can appropriately and responsibly utilize existing AI 
resources when implementing these technologies into the development of 
intervention plans.

DOI: 10.1044/2025_AJSLP-24-00464
PMID: 40403406 [Indexed for MEDLINE]


385. Int J Legal Med. 2025 Sep;139(5):2323-2337. doi: 10.1007/s00414-025-03497-z. 
Epub 2025 May 22.

An X-ray bone age assessment method for hands and wrists of adolescents in 
Western China based on feature fusion deep learning models.

Wang YH(1)(2)(3), Zhou HM(3)(4), Wan L(3)(5), Guo YC(6)(7), Li YZ(8), Liu TA(8), 
Guo JX(1)(2)(9), Li DY(3)(10)(11), Chen T(12)(13).

Author information:
(1)College of Forensic Medicine, Xi'an Jiaotong University Health Science 
Center, Xi'an, Shaanxi, 710061, PR China.
(2)NHC Key Laboratory of Forensic Science, Xi'an Jiaotong University, Xi'an, 
Shaanxi, 710061, PR China.
(3)Academy of Forensic Science, Shanghai Key Laboratory of Forensic Medicine 
(21DZ2270800), Shanghai Forensic Service Platform, Key Laboratory of Forensic 
Science, Ministry of Justice, 1347 GuangFu West Road, Shanghai, 200063, China.
(4)School of Forensic Medicine, Shanxi Medical University, Jinzhong, 030604, 
Shanxi, China.
(5)Department of Radiology, Shanghai Public Health Clinical Center, Fudan 
University, Shanghai, 201508, China.
(6)Key Laboratory of Shaanxi Province for Craniofacial Precision Medicine 
Research, College of Stomatology, Xi'an Jiaotong University, Xi'an, 710004, 
Shaanxi, PR China.
(7)Department of Orthodontics, College of Stomatology, Xi'an Jiaotong 
University, Xi'an, 710004, Shaanxi, PR China.
(8)Shanghai Shuzhiwei Information Technology Co., LTD, 333 WenHai Road, 
Shanghai, 200444, China.
(9)Department of Radiology, the First Affiliated Hospital of Xi'an Jiaotong 
University, Xi'an, 710061, Shaanxi, PR China.
(10)Academy of Medical Sciences, Shanxi Medical University, Taiyuan, 030000, 
China.
(11)School of Public Health, Shanxi Medical University, Taiyuan, 030000, China.
(12)College of Forensic Medicine, Xi'an Jiaotong University Health Science 
Center, Xi'an, Shaanxi, 710061, PR China. chenteng@mail.xjtu.edu.cn.
(13)NHC Key Laboratory of Forensic Science, Xi'an Jiaotong University, Xi'an, 
Shaanxi, 710061, PR China. chenteng@mail.xjtu.edu.cn.

The epiphyses of the hand and wrist serve as crucial indicators for assessing 
skeletal maturity in adolescents. This study aimed to develop a deep learning 
(DL) model for bone age (BA) assessment using hand and wrist X-ray images, 
addressing the challenge of classifying BA in adolescents. The results of this 
DL-based classification were then compared and analyzed with those obtained from 
manual assessment. A retrospective analysis was conducted on 688 hand and wrist 
X-ray images of adolescents aged 11.00-23.99 years from western China, which 
were randomly divided into training set, validation set and test set. The BA 
assessment results were initially analyzed and compared using four DL network 
models: InceptionV3, InceptionV3 + SE + Sex, InceptionV3 + Bilinear and 
InceptionV3 + Bilinear. + SE + Sex, to identify the DL model with the best 
classification performance. Subsequently, the results of the top-performing 
model were compared with those of manual classification. The study findings 
revealed that the InceptionV3 + Bilinear + SE + Sex model exhibited the best 
performance, achieving classification accuracies of 96.15% and 90.48% for the 
training and test set, respectively. Furthermore, based on the 
InceptionV3 + Bilinear + SE + Sex model, classification accuracies were 
calculated for four age groups (< 14.0 years, 14.0 years ≤ age < 16.0 years, 
16.0 years ≤ age < 18.0 years, ≥ 18.0 years), with notable accuracies of 100% 
for the age groups 16.0 years ≤ age < 18.0 years and ≥ 18.0 years. The BA 
classification, utilizing the feature fusion DL network model, holds significant 
reference value for determining the age of criminal responsibility of 
adolescents, particularly at the critical legal age boundaries of 14.0, 16.0, 
and 18.0 years.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00414-025-03497-z
PMID: 40402226 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethical approval: Ethical approval 
was granted by the ethics committee of Xi’an Jiaotong University. And informed 
consent was waived because of the retrospective nature. All procedures performed 
in studies involving human participants were in accordance with the ethical 
standards of the institutional and/or national research committee and with the 
1964 Helsinki declaration and its later amendments or comparable ethical 
standards. Research involving human participants and/or animals: Human 
participants. Conflict of interest: The authors have declared that no competing 
interests exist. Clinical trial number: Not applicable.


386. Cureus. 2025 Apr 21;17(4):e82705. doi: 10.7759/cureus.82705. eCollection 2025 
Apr.

ChatGPT and Gemini for Patient Education: A Comparative Analysis of Common 
Pediatric Exanthematous Conditions.

Reshi A(1), Arora N(2)(3), Singh T(4).

Author information:
(1)Psychiatry, Bhagawan Sri Balagangadharanatha Swamiji (BGS) Global Institute 
of Medical Sciences, Bengaluru, IND.
(2)Pediatrics, Government Medical College, Patiala, IND.
(3)Pediatrics, Guru Gobind Singh Medical College and Hospital, Faridkot, IND.
(4)Pediatric Medicine, The Doctor's Hub Polyclinic, Dubai, ARE.

INTRODUCTION: Varicella, hand, foot, and mouth disease (HFMD), and measles are 
some of the common causes of fever with rash in the pediatric age group. ChatGPT 
and Gemini are effective large language models (LLMs) for parents to understand 
their child's condition. Therefore, considering the growing popularity of 
artificial intelligence (AI), LLMs, and their ability to disseminate health 
information, assessing ChatGPT's (OpenAI, San Francisco, CA, USA) and Gemini's 
(Google LLC, Mountain View, CA, USA) quality and accuracy is essential.
MATERIALS AND METHODS: A cross-sectional study was conducted on responses 
generated using AI for common causes of fever with rash in the pediatric age 
group, namely varicella, HFMD, and measles. ChatGPT and Gemini were used for the 
generation of brochures for patient education. The responses generated were 
evaluated using the Flesch-Kincaid Calculator (Good Calculators: 
https://goodcalculators.com/), the QuillBot plagiarism tool (QuillBot, Chicago, 
IL, USA), and the modified DISCERN score. Statistical analysis was done using R 
version 4.3.2 (R Foundation for Statistical Computing, Vienna, Austria, 
https://www.R-project.org/), and unpaired t-tests were used to compare the 
various scores. A p-value of less than 0.05 was considered statistically 
significant.
RESULTS: It was found that ChatGPT generates a higher word count as compared to 
Gemini (p=0.047). Sentences, average words per sentence, average syllables per 
word, ease score, and grade level between the two AI tools were statistically 
insignificant (p>0.05). The mean reliability score was 3/5 in the case of Gemini 
versus 2.67/5 in ChatGPT, but the difference was statistically insignificant 
(p=0.725).
CONCLUSIONS: This study highlights that ChatGPT generates more word count than 
Gemini, and the finding was statistically significant (p=0.047). Additionally, 
there is no significant difference in the average ease score or grade score for 
common pediatric exanthematous conditions: varicella, HFMD, and measles. Future 
research should focus on improving AI-generated health content by incorporating 
real-time validation mechanisms, expert reviews, and structured patient 
feedback.

Copyright © 2025, Reshi et al.

DOI: 10.7759/cureus.82705
PMCID: PMC12094275
PMID: 40400811

Conflict of interest statement: Human subjects: All authors have confirmed that 
this study did not involve human participants or tissue. Animal subjects: All 
authors have confirmed that this study did not involve animal subjects or 
tissue. Conflicts of interest: In compliance with the ICMJE uniform disclosure 
form, all authors declare the following: Payment/services info: All authors have 
declared that no financial support was received from any organization for the 
submitted work. Financial relationships: All authors have declared that they 
have no financial relationships at present or within the previous three years 
with any organizations that might have an interest in the submitted work. Other 
relationships: All authors have declared that there are no other relationships 
or activities that could appear to have influenced the submitted work.


387. BMC Oral Health. 2025 May 21;25(1):756. doi: 10.1186/s12903-025-06163-3.

Customized GPT-4V(ision) for radiographic diagnosis: can large language model 
detect supernumerary teeth?

Aşar EM(1), İpek İ(2), Bi Lge K(3).

Author information:
(1)Department of Pediatric Dentistry, Faculty of Dentistry, Selçuk University, 
Konya, Turkey. enesmustafasar@gmail.com.
(2)Department of Pediatric Dentistry, Faculty of Dentistry, Firat University, 
Elazig, Turkey.
(3)Department of Restorative Dentistry, Faculty of Dentistry, Firat University, 
Elazig, Turkey.

BACKGROUND: With the growing capabilities of language models like ChatGPT to 
process text and images, this study evaluated their accuracy in detecting 
supernumerary teeth on periapical radiographs. A customized GPT-4V model 
(CGPT-4V) was also developed to assess whether domain-specific training could 
improve diagnostic performance compared to standard GPT-4V and GPT-4o models.
METHODS: One hundred eighty periapical radiographs (90 with and 90 without 
supernumerary teeth) were evaluated using GPT-4 V, GPT-4o, and a fine-tuned 
CGPT-4V model. Each image was assessed separately with the standardized prompt 
"Are there any supernumerary teeth in the radiograph above?" to avoid contextual 
bias. Three dental experts scored the responses using a three-point Likert scale 
for positive cases and a binary scale for negatives. Chi-square tests and ROC 
analysis were used to compare model performances (p < 0.05).
RESULTS: Among the three models, CGPT-4 V exhibited the highest accuracy, 
detecting supernumerary teeth correctly in 91% of cases, compared to 77% for 
GPT-4o and 63% for GPT-4V. The CGPT-4V model also demonstrated a significantly 
lower false positive rate (16%) than GPT-4V (42%). A statistically significant 
difference was found between CGPT-4V and GPT-4o (p < 0.001), while no 
significant difference was observed between GPT-4V and CGPT-4V or between GPT-4V 
and GPT-4o. Additionally, CGPT-4V successfully identified multiple supernumerary 
teeth in radiographs where present.
CONCLUSIONS: These findings highlight the diagnostic potential of customized GPT 
models in dental radiology. Future research should focus on multicenter 
validation, seamless clinical integration, and cost-effectiveness to support 
real-world implementation.

© 2025. The Author(s).

DOI: 10.1186/s12903-025-06163-3
PMCID: PMC12096622
PMID: 40399904 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: All procedures in this study were performed in accordance with the 
ethical standards of the Selcuk University Faculty of Dentistry Non-Invasive 
Clinical Research Ethics Committee in Turkey (Decision No:2024/68) and with the 
1964 Helsinki Declaration and its later amendments or comparable ethical 
standards. Consent for publication: Not applicable. Competing interests: The 
authors declare no competing interests.


388. Am J Perinatol. 2025 Dec;42(16):2094-2103. doi: 10.1055/a-2616-4182. Epub 2025 
May 20.

Application of Generative AI to Enhance Obstetrics and Gynecology Research.

Kawakita T(1), Wong MS(2)(3), Gibson KS(4), Gupta M(5)(6), Gimovsky AC(7), 
Moussa HN(8)(9), Hye HJ(10)(11); Society of Maternal-Fetal Medicine Clinical 
Informatics Committee.

Author information:
(1)Department of Obstetrics and Gynecology, Eastern Virginia Medical School, 
Norfolk, Virginia.
(2)Division of Maternal-Fetal Medicine, Department of Obstetrics and Gynecology, 
Cedars-Sinai Medical Center, Los Angeles, California.
(3)Division of Informatics, Department of Computational Biomedicine, 
Cedars-Sinai Medical Center, Los Angeles, California.
(4)Division of Maternal-Fetal Medicine, Department of Obstetrics and Gynecology, 
The MetroHealth System, Case Western Reserve University, Cleveland, Ohio.
(5)Department of Obstetrics and Gynecology, Beth Israel Deaconess Medical 
Center, Boston, Massachusetts.
(6)Department of Obstetrics, Gynecology and Reproductive Biology, Harvard 
Medical School, Boston, Massachusetts.
(7)Division of Maternal-Fetal Medicine, Department of Obstetrics and Gynecology, 
Warren Alpert School of Brown University, Women & Infants Hospital of Rhode 
Island, Providence, Rhode Island.
(8)Division of Maternal Fetal Medicine, Department of Obstetrics and Gynecology, 
University of Cincinnati, Cincinnati, Ohio.
(9)Kettering Health Maternal Fetal Medicine, Kettering, Ohio.
(10)Department of Obstetrics and Gynecology, NYU Grossman Long Island School of 
Medicine, Mineola, New York.
(11)Department of Health Informatics, NYU Langone Health, New York, New York.

The rapid evolution of large-language models such as ChatGPT, Claude, and Gemini 
is reshaping the methodological landscape of obstetrics and gynecology (OBGYN) 
research. This narrative review provides a comprehensive account of generative 
AI capabilities, key use cases, and recommended safeguards for investigators. 
First, generative AI expedites hypothesis generation, enabling researchers to 
interrogate vast corpora and surface plausible, overlooked questions. Second, it 
streamlines systematic reviews by composing optimized search strings, screening 
titles and abstracts, and identifying full-text discrepancies. Third, AI 
assistants can draft reproducible analytic code, perform preliminary descriptive 
or inferential analyses, and create publication-ready tables and figures. 
Fourth, the models support scholarly writing by suggesting journal-specific 
headings, refining prose, harmonizing references, and translating technical 
content for multidisciplinary audiences. Fifth, they augment peer-review and 
editorial workflows by delivering evidence-focused critiques. In educational 
settings, these models can create adaptive curricula and interactive simulations 
for trainees, fostering digital literacy and evidence-based practice early in 
professional development among clinicians. Integration into clinical 
decision-support pipelines is also foreseeable, warranting proactive governance. 
Notwithstanding these opportunities, responsible use demands vigilant oversight. 
Large-language models occasionally fabricate citations or misinterpret 
domain-specific data ("hallucinations"), potentially propagating misinformation. 
Outputs are highly prompt-dependent, creating a reliance on informed prompt 
engineering that may disadvantage less technical clinicians. Moreover, uploading 
protected health information or copyrighted text raises privacy, security, and 
intellectual property concerns. We outline best-practice recommendations: 
maintain human verification of all AI-generated content; cross-validate 
references with primary databases; employ privacy-preserving, on-premises 
deployments for sensitive data; document prompts for reproducibility; and 
disclose AI involvement transparently. In summary, generative AI offers a 
powerful adjunct for OBGYN scientists by accelerating topic formulation, 
evidence synthesis, data analysis, manuscript preparation, and peer review. When 
coupled with rigorous oversight and ethical safeguards, these tools can enhance 
productivity without compromising scientific integrity. Future studies should 
quantify accuracy, bias, and downstream patient impact. · Generative AI supports 
various research stages in OBGYN, such as hypothesis generation, systematic 
review assistance, data analysis, and scientific writing, demonstrating its 
potential to streamline research workflows and improve research efficiency.. · 
Generative AI has notable limitations, including the risk of generating 
inaccurate references ("hallucinations") and the need for careful supervision.. 
· Effective usage requires skill in prompt engineering, posing a challenge for 
those without technical expertise.. · Utilizing generative AI in sensitive 
fields like OBGYN raises privacy, security, and ethical concerns..

Thieme. All rights reserved.

DOI: 10.1055/a-2616-4182
PMID: 40393680 [Indexed for MEDLINE]

Conflict of interest statement: None declared.


389. JMIR Med Educ. 2025 May 20;11:e63353. doi: 10.2196/63353.

Assessing ChatGPT's Capability as a New Age Standardized Patient: Qualitative 
Study.

Cross J(#)(1), Kayalackakom T(#)(2), Robinson RE(3), Vaughans A(4), Sebastian 
R(4), Hood R(2), Lewis C(2), Devaraju S(2), Honnavar P(2), Naik S(2), Joseph 
J(2), Anand N(5), Mohammed A(6), Johnson A(6), Cohen E(6), Adeniji T(6), Nnenna 
Nnaji A(6), George JE(6).

Author information:
(1)Medical University of the Americas, PO Box 701, Charlestown, Saint Kitts and 
Nevis, 1 9788629500 ext 364.
(2)Department of Education Enhancement, College of Medicine, American University 
of Antigua, St Johns, Antigua and Barbuda.
(3)Department of Health Informatics, School of Professional Studies, 
Northwestern University, Evanston, IL, United States.
(4)Department of Biochemistry, Cell Biology and Genetics, College of Medicine, 
American University of Antigua, Basseterre, Antigua and Barbuda.
(5)Department of Medical Education, School of Medicine, University of Texas Rio 
Grande Valley, Edinburgh, TX, United States.
(6)School of Medicine, Xavier University, Orangestad, Aruba.
(#)Contributed equally

BACKGROUND: Standardized patients (SPs) have been crucial in medical education, 
offering realistic patient interactions to students. Despite their benefits, SP 
training is resource-intensive and access can be limited. Advances in artificial 
intelligence (AI), particularly with large language models such as ChatGPT, 
present new opportunities for virtual SPs, potentially addressing these 
limitations.
OBJECTIVES: This study aims to assess medical students' perceptions and 
experiences of using ChatGPT as an SP and to evaluate ChatGPT's effectiveness in 
performing as a virtual SP in a medical school setting.
METHODS: This qualitative study, approved by the American University of Antigua 
Institutional Review Board, involved 9 students (5 females and 4 males, aged 
22-48 years) from the American University of Antigua College of Medicine. 
Students were observed during a live role-play, interacting with ChatGPT as an 
SP using a predetermined prompt. A structured 15-question survey was 
administered before and after the interaction. Thematic analysis was conducted 
on the transcribed and coded responses, with inductive category formation.
RESULTS: Thematic analysis identified key themes preinteraction including 
technology limitations (eg, prompt engineering difficulties), learning efficacy 
(eg, potential for personalized learning and reduced interview stress), 
verisimilitude (eg, absence of visual cues), and trust (eg, concerns about AI 
accuracy). Postinteraction, students noted improvements in prompt engineering, 
some alignment issues (eg, limited responses on sensitive topics), maintained 
learning efficacy (eg, convenience and repetition), and continued verisimilitude 
challenges (eg, lack of empathy and nonverbal cues). No significant trust issues 
were reported postinteraction. Despite some limitations, students found ChatGPT 
as a valuable supplement to traditional SPs, enhancing practice flexibility and 
diagnostic skills.
CONCLUSIONS: ChatGPT can effectively augment traditional SPs in medical 
education, offering accessible, flexible practice opportunities. However, it 
cannot fully replace human SPs due to limitations in verisimilitude and prompt 
engineering challenges. Integrating prompt engineering into medical curricula 
and continuous advancements in AI are recommended to enhance the use of virtual 
SPs.

© Joseph Cross, Tarron Kayalackakom, Raymond E Robinson, Andrea Vaughans, Roopa 
Sebastian, Ricardo Hood, Courtney Lewis, Sumanth Devaraju, Prasanna Honnavar, 
Sheetal Naik, Jillwin Joseph, Nikhilesh Anand, Abdalla Mohammed, Asjah Johnson, 
Eliran Cohen, Teniola Adeniji, Aisling Nnenna Nnaji, Julia Elizabeth George. 
Originally published in JMIR Medical Education (https://mededu.jmir.org).

DOI: 10.2196/63353
PMCID: PMC12111480
PMID: 40393017 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


390. J Gastrointest Oncol. 2025 Apr 30;16(2):435-452. doi: 10.21037/jgo-2024-1024. 
Epub 2025 Apr 27.

Validation of multiple deep learning models for colorectal tumor differentiation 
with endoscopic ultrasound images: a dual-center study.

Men H(#)(1), Yan C(#)(1), Peng X(#)(2), Jin SQ(3), Du YH(4), Tang ZS(1), Li 
H(1), Ou-Yang T(5), Zhang S(6), Ding LS(2), Deng J(1), Xu Z(2), Li GB(7), Luo 
HY(8), Li Z(1), Xie F(5), Han S(1).

Author information:
(1)General Surgery Center, Zhujiang Hospital, Southern Medical University, 
Guangzhou, China.
(2)The Second Clinical College of Southern Medical University, Guangzhou, China.
(3)Department of Gastroenterology, Zhujiang Hospital, Southern Medical 
University, Guangzhou, China.
(4)School of Science and Engineering, The Chinese University of Hong Kong, 
Shenzhen, China.
(5)Department of Gastroenterology, Nanfang Hospital of Southern Medical 
University, Guangzhou, China.
(6)School of Instrument Science and Engineering, the State Key Laboratory of 
Digital Medical Engineering, the School of Biological Science and Medical 
Engineering, Southeast University, Nanjing, China.
(7)School of Computer Science and Engineering, Sun Yat-sen University, 
Guangzhou, China.
(8)Department of General Surgery, The Sixth People's Hospital of Huizhou, 
Huizhou, China.
(#)Contributed equally

BACKGROUND: Colorectal cancer (CRC) is one of the most common malignancies 
worldwide. Differentiating adenomas and cancers in colorectal lesions is 
essential for reducing morbidity and mortality associated with CRC. Endoscopic 
ultrasound (EUS) is crucial in the diagnosis of CRC, and artificial intelligence 
(AI) offers a promising approach for identifying colorectal lesions without the 
need for histopathological confirmation. The objective of this study was to 
validate the efficacy of EUS combined with AI for the diagnosis of colorectal 
adenoma and cancer and to compare it with that of conventional endoscopic 
diagnosis.
METHODS: This retrospective study included 554 patients (167 with CRC, 136 with 
adenomas, and 251 controls) from two independent centers. The dataset was 
randomly divided into training and test sets in a 2:1 ratio (360 for the 
training dataset; 194 for the testing dataset). A model was developed using a 
"feature extractor + multilayer perceptron (MLP) classifier" framework, 
incorporating Residual Network 50 (ResNet50), EfficientNet-B0, Visual Geometry 
Group 11_BN (VGG_11_BN), and Vision Transformer (ViT) as feature extractors. 
Four AI systems were trained and validated, and the model with the highest F1 
scores was subsequently compared to four endoscopists using the test dataset, 
and interobserver agreement measured by Fleiss' kappa.
RESULTS: The accuracies for three-category classification (CRC, adenoma and 
controls) were 70.62% for ResNet50, 68.56% for EfficientNet-B0, 63.4% for ViT, 
and 70.10% for VGG_11_BN. ResNet50 achieved the highest F1 scores (70.37%) and 
diagnostic accuracy and was selected for comparison with endoscopists. For CRC 
diagnosis, ResNet50 outperformed endoscopists with an accuracy of 80.93%, 
sensitivity of 72.88%, and specificity of 84.44%, which were significantly 
higher than those of all endoscopists (P<0.05). For adenoma diagnosis, ResNet50 
had a sensitivity of 47.92%, which was significantly higher than that of 
nonexpert endoscopists (P<0.05). The interobserver agreement was fair among AI 
systems (Fleiss' κ =0.674) and among experts (Fleiss' κ =0.557) and was slight 
among nonexperts (Fleiss' κ =0.284).
CONCLUSIONS: EUS-AI has high diagnostic accuracy for CRC and adenoma as compared 
to non-expert endoscopists. ResNet50 is a promising tool for enhancing 
diagnostic accuracy in clinical practice using EUS.

Copyright © 2025 AME Publishing Company. All rights reserved.

DOI: 10.21037/jgo-2024-1024
PMCID: PMC12078835
PMID: 40386596

Conflict of interest statement: Conflicts of Interest: All authors have 
completed the ICMJE uniform disclosure form (available at 
https://jgo.amegroups.com/article/view/10.21037/jgo-2024-1024/coif). H.M., X.P., 
H.L. and Z.L. report funding support from the Guangdong Provincial Science and 
Technology Innovation Project (grant No. pdjh2024b093). S.Q.J. and S.H. report 
funding support from the Southern Medical University Program (grant No. 
202412121303) and the Guangdong Provincial Science and Technology Innovation 
Project (grant No. pdjh2024b093). T.O.Y. reports funding support from the 
Guangdong Provincial Natural Science Foundation (grant No. 2023A1515010729). 
Z.X. reports funding support from the Southern Medical University Program (grant 
No. 202412121303). F.X. reports funding support from the National Natural 
Science Foundation of China (grant No. 82470539), and Guangdong Provincial 
Natural Science Foundation (grant No. 2023A1515010729). Z.L. and S.H. reports 
funding support from the Guangdong Provincial Science and Technology Innovation 
Strategy Special Funds (grant No. 2023A0505010038). The other authors have no 
conflicts of interest to declare.


391. Res Sq [Preprint]. 2025 May 8:rs.3.rs-6605863. doi: 10.21203/rs.3.rs-6605863/v1.

SleepBert: An Intelligent Clinical Encyclopaedia for Sleep Disorders Using Large 
Language Models.

Amala Ann KA, Vaidhehi V.

Diagnosis of sleep disorders is difficult owing to the nature of sleep 
microarchitecture and the heterogeneity of symptom presentation. Conventional 
analysis of Polysomnography (PSG)-the interpretation of EEG bandpower, sleep 
spindles, and K-complexes-is time-consuming, laborious, and subjective, 
restricting detection of infrequent co-occurrences of disorders and their link 
to neuro-cognitive and genetic disorders. To overcome these challenges, we 
present SleepBert, a hybrid Retrieval-Augmented Generation (RAG) model that 
combines structured PSG features with unstructured clinical narratives for 
holistic sleep disorder analysis. Constructed by fine-tuning ClinicalBERT on PSG 
data from the NCH (paediatric dataset) and ISRUC datasets, SleepBert has a 
PSG-specific knowledge retrieval layer to retrieve real-time evidence from 
medical databases such as PubMed. The model delivered 93.40% accuracy, outdoing 
ClinicalBERT (87.20%) and BERT (80.90%), with 90.1% accuracy in retrieving 
PubMed and response latency of 5.4 seconds. This system serves as an 
Encyclopaedia of sleep disorders, delivering evidence-based, correct insights 
and support for decision making to clinicians and researchers. The system 
supports the analysis of a large number of PSGs, speeds up data-driven 
discoveries, and allows access to rare neuro-cognitive and genetic markers. 
SleepBert is an extensible platform for pushing the frontier of sleep disorder 
research and enhancing clinical decision-making through quick, accurate 
interpretations of sophisticated PSG data.

DOI: 10.21203/rs.3.rs-6605863/v1
PMCID: PMC12083639
PMID: 40386411

Conflict of interest statement: Conflict of Interest: The authors declare that 
there are no conflicts of interest. Declaration of Conflicting Interests In 
relation to the research, writing, and/or publication of this work, the 
author(s) declared no conflicts of interest.


392. JMIR Cancer. 2025 May 15;11:e64697. doi: 10.2196/64697.

A Deep Learning-Enabled Workflow to Estimate Real-World Progression-Free 
Survival in Patients With Metastatic Breast Cancer: Study Using Deidentified 
Electronic Health Records.

Varma G(1), Yenukoti RK(1), Kumar M P(1), Ashrit BS(1), Purushotham K(1), Subash 
C(1), Ravi SK(1), Kurien V(1), Aman A(2), Manoharan M(1), Jaiswal S(2), Anand 
A(2), Barve R(2), Thiagarajan V(2), Lenehan P(3), Soefje SA(4), Soundararajan 
V(3).

Author information:
(1)Department of Clinical Sciences, Nference, 4th Floor, Indiqube, Golf View 
Campus Tower-2, 22, 3rd Cross Rd, Murugeshpalya, S R Layout, Bangalore, 560017, 
India, 91 8728831787.
(2)Department of Data Science and Engineering, Nference, Bangalore, India.
(3)Department of Clinical Sciences, Nference, Cambridge, MA, United States.
(4)Department of Pharmacy, Director of Pharmacy, Mayo Clinic, Rochester, MN, 
United States.

BACKGROUND: Progression-free survival (PFS) is a crucial endpoint in cancer drug 
research. Clinician-confirmed cancer progression, namely real-world PFS (rwPFS) 
in unstructured text (ie, clinical notes), serves as a reasonable surrogate for 
real-world indicators in ascertaining progression endpoints. Response evaluation 
criteria in solid tumors (RECIST) is traditionally used in clinical trials using 
serial imaging evaluations but is impractical when working with real-world data. 
Manual abstraction of clinical progression from unstructured notes remains the 
gold standard. However, this process is a resource-intensive, time-consuming 
process. Natural language processing (NLP), a subdomain of machine learning, has 
shown promise in accelerating the extraction of tumor progression from 
real-world data in recent years.
OBJECTIVES: We aim to configure a pretrained, general-purpose health care NLP 
framework to transform free-text clinical notes and radiology reports into 
structured progression events for studying rwPFS on metastatic breast cancer 
(mBC) cohorts.
METHODS: This study developed and validated a novel semiautomated workflow to 
estimate rwPFS in patients with mBC using deidentified electronic health record 
data from the Nference nSights platform. The developed workflow was validated in 
a cohort of 316 patients with hormone receptor-positive, human epidermal growth 
factor receptor-2 (HER-2) 2-negative mBC, who were started on palbociclib and 
letrozole combination therapy between January 2015 and December 2021. 
Ground-truth datasets were curated to evaluate the workflow's performance at 
both the sentence and patient levels. NLP-captured progression or a change in 
therapy line were considered outcome events, while death, loss to follow-up, and 
end of the study period were considered censoring events for rwPFS computation. 
Peak reduction and cumulative decline in Patient Health Questionnaire-8 (PHQ-8) 
scores were analyzed in the progressed and nonprogressed patient subgroups.
RESULTS: The configured clinical NLP engine achieved a sentence-level 
progression capture accuracy of 98.2%. At the patient level, initial progression 
was captured within ±30 days with 88% accuracy. The median rwPFS for the study 
cohort (N=316) was 20 (95% CI 18-25) months. In a validation subset (n=100), 
rwPFS determined by manual curation was 25 (95% CI 15-35) months, closely 
aligning with the computational workflow's 22 (95% CI 15-35) months. A 
subanalysis revealed rwPFS estimates of 30 (95% CI 24-39) months from radiology 
reports and 23 (95% CI 19-28) months from clinical notes, highlighting the 
importance of integrating multiple note sources. External validation also 
demonstrated high accuracy (92.5% sentence level; 90.2% patient level). 
Sensitivity analysis revealed stable rwPFS estimates across varying levels of 
missing source data and event definitions. Peak reduction in PHQ-8 scores during 
the study period highlighted significant associations between patient-reported 
outcomes and disease progression.
CONCLUSIONS: This workflow enables rapid and reliable determination of rwPFS in 
patients with mBC receiving combination therapy. Further validation across more 
diverse external datasets and other cancer types is needed to ensure broader 
applicability and generalizability.

© Gowtham Varma, Rohit Kumar Yenukoti, Praveen Kumar M, Bandlamudi Sai Ashrit, K 
Purushotham, C Subash, Sunil Kumar Ravi, Verghese Kurien, Avinash Aman, Mithun 
Manoharan, Shashank Jaiswal, Akash Anand, Rakesh Barve, Viswanathan Thiagarajan, 
Patrick Lenehan, Scott A Soefje, Venky Soundararajan. Originally published in 
JMIR Cancer (https://cancer.jmir.org).

DOI: 10.2196/64697
PMCID: PMC12097284
PMID: 40372953 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: GV, PK, KP, SC, AvA, MM, 
SJ, AkA, RB, VT, PL, and VS are current employees of Nference, inc and hold a 
minority stake in the company. RKY, BSA, and VK are past employees of Nference. 
SAS is affiliated with Mayo Clinic, Rochester. The authors declare no further 
competing interests in the findings of the study.


393. Technol Health Care. 2025 Sep;33(5):2266-2278. doi: 10.1177/09287329251341071. 
Epub 2025 May 15.

Assessing the impact of chatbots on health decision-making: A multifactorial 
experimental approach.

Xie Z(1)(2).

Author information:
(1)School of Media and Communication, Shanghai Jiao Tong University, Shanghai, 
China.
(2)Wee Kim Wee School of Communication and Information, Nanyang Technological 
University, Singapore, Singapore.

BackgroundChatbots are increasingly integrated into healthcare, offering 
personalized and accessible health advice. However, the impact of factors such 
as chatbot authority, health information type, and interaction style on users' 
decision-making remains unclear.ObjectiveThis study aims to investigate how 
these elements influence users' willingness to adopt health advice provided by 
chatbots.MethodsA 2 × 2 × 2 factorial experiment was conducted with 480 
university students to examine the effects of chatbot authority (authoritative 
vs. non-authoritative), health information type (preventive vs. 
treatment-related), and interaction style (formal vs. informal). Participants' 
willingness to adopt the health advice was measured before and after interacting 
with the chatbot.ResultsThe study found that a authoritative chatbot delivering 
treatment-related advice in a formal style significantly increased willingness 
to adopt the advice. Conversely, preventive information was more effective when 
presented informally by a non-authoritative chatbot. These results support the 
media evocation paradigm, which suggests that chatbots framed as authoritative 
figures evoke greater user engagement and trust in health contexts.ConclusionThe 
findings extend the media evocation paradigm by demonstrating that chatbot 
authority, information type, and interaction style should be aligned with the 
nature of health advice to maximize effectiveness. This study provides insights 
for designing chatbots that improve health decision-making by tailoring their 
communication strategies.

DOI: 10.1177/09287329251341071
PMID: 40371446 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of conflicting interestsThe author 
declared no potential conflicts of interest with respect to the research, 
authorship, and/or publication of this article.


394. Sensors (Basel). 2025 Apr 25;25(9):2725. doi: 10.3390/s25092725.

Research on Intrusion Detection Method Based on Transformer and CNN-BiLSTM in 
Internet of Things.

Zhang C(1), Li J(1), Wang N(1), Zhang D(1).

Author information:
(1)School of Cyberspace Security, Beijing University of Post and 
Telecommunications, Beijing 100876, China.

With the widespread deployment of Internet of Things (IoT) devices, their 
complex network environments and open communication modes have made them prime 
targets for cyberattacks. Traditional Intrusion Detection Systems (IDS) face 
challenges in handling complex attack types, data imbalance, and feature 
extraction difficulties in IoT environments. Accurately detecting abnormal 
traffic in IoT has become increasingly critical. To address the limitation of 
single models in comprehensively capturing the diverse features of IoT traffic, 
this paper proposes a hybrid model based on CNN-BiLSTM-Transformer, which better 
handles complex features and long-sequence dependencies in intrusion detection. 
To address the issue of data class imbalance, the Borderline-SMOTE method is 
introduced to enhance the model's ability to recognize minority class attack 
samples. To tackle the problem of redundant features in the original dataset, a 
comprehensive feature selection strategy combining XGBoost, Chi-square (Chi2), 
and Mutual Information is adopted to ensure the model focuses on the most 
discriminative features. Experimental validation demonstrates that the proposed 
method achieves 99.80% accuracy on the CIC-IDS 2017 dataset and 97.95% accuracy 
on the BoT-IoT dataset, significantly outperforming traditional intrusion 
detection methods, proving its efficiency and accuracy in detecting abnormal 
traffic in IoT environments.

DOI: 10.3390/s25092725
PMCID: PMC12074129
PMID: 40363165

Conflict of interest statement: The authors declare no conflicts of interest.


395. Sensors (Basel). 2025 Apr 24;25(9):2704. doi: 10.3390/s25092704.

MUF-Net: A Novel Self-Attention Based Dual-Task Learning Approach for Automatic 
Left Ventricle Segmentation in Echocardiography.

Lyu J(1), Meng J(2), Zhang Y(1), Ling SH(3).

Author information:
(1)College of Artificial Intelligence, Tianjin University of Science and 
Technology, Tianjin 300457, China.
(2)College of Light Industry Science and Engineering, Tianjin University of 
Science and Technology, Tianjin 300457, China.
(3)School of Electrical and Data Engineering, Faculty of Engineering and 
Information Technology, University of Technology Sydney, Sydney, NSW 2007, 
Australia.

Left ventricular ejection fraction (LVEF) is a critical indicator for assessing 
cardiac function and diagnosing heart disease. LVEF can be derived by estimating 
the left ventricular volume from end-systolic and end-diastolic frames through 
echocardiography segmentation. However, current algorithms either focus 
primarily on single-frame segmentation, neglecting the temporal and spatial 
correlations between consecutive frames, or often fail to effectively address 
the inherent challenges posed by the low-contrast and fuzzy edges characteristic 
of echocardiography, thereby resulting in suboptimal segmentation outcomes. In 
this study, we propose a novel self-attention-based dual-task learning approach 
for automatic left ventricle segmentation. First, we introduce a multi-scale 
edge-attention U-Net to achieve supervised semantic segmentation of 
echocardiography. Second, an optical flow network is developed to capture the 
changes in the optical flow fields between frames in an unsupervised manner. 
These two tasks are then jointly trained using a temporal consistency mechanism 
to extract spatio-temporal features across frames. Experimental results 
demonstrate that our model outperforms existing segmentation methods. Our 
proposed method not only enhances the performance of semantic segmentation but 
also improves the consistency of segmentation between consecutive frames.

DOI: 10.3390/s25092704
PMCID: PMC12074395
PMID: 40363141 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest. 
The funders had no role in the design of the study; in the collection, analyses, 
or interpretation of data; in the writing of the manuscript; or in the decision 
to publish the results.


396. BMC Psychiatry. 2025 May 12;25(1):478. doi: 10.1186/s12888-025-06912-2.

A controlled trial examining large Language model conformity in psychiatric 
assessment using the Asch paradigm.

Shoval DH(1)(2), Gigi K(3), Haber Y(3)(4), Itzhaki A(3)(5), Asraf K(6), Piterman 
D(7), Elyoseph Z(3)(7).

Author information:
(1)The Center for Psychobiological Research, Department of Psychology and 
Educational Counseling, Max Stern Yezreel Valley College, Yezreel Valley, 
Israel. dorith@yvc.ac.il.
(2)The Institute for Research and Development, The Artificial Third, Tel Aviv, 
Israel. dorith@yvc.ac.il.
(3)The Institute for Research and Development, The Artificial Third, Tel Aviv, 
Israel.
(4)The PhD Program of Hermeneutics & Cultural Studies, Interdisciplinary Unit, 
Bar-Ilan University, Ramat Gan, Israel.
(5)At time of research: Senior at Hakfar Hayarok High School, Ramat HaSharon, 
Israel.
(6)The Center for Psychobiological Research, Department of Psychology and 
Educational Counseling, Max Stern Yezreel Valley College, Yezreel Valley, 
Israel.
(7)Faculty of Education, School of Therapy, Counseling, and Human Development, 
University of Haifa, Haifa, Israel.

BACKGROUND: Despite significant advances in AI-driven medical diagnostics, the 
integration of large language models (LLMs) into psychiatric practice presents 
unique challenges. While LLMs demonstrate high accuracy in controlled settings, 
their performance in collaborative clinical environments remains unclear. This 
study examined whether LLMs exhibit conformity behavior under social pressure 
across different diagnostic certainty levels, with a particular focus on 
psychiatric assessment.
METHODS: Using an adapted Asch paradigm, we conducted a controlled trial 
examining GPT-4o's performance across three domains representing increasing 
levels of diagnostic uncertainty: circle similarity judgments (high certainty), 
brain tumor identification (intermediate certainty), and psychiatric assessment 
using children's drawings (high uncertainty). The study employed a 3 × 3 
factorial design with three pressure conditions: no pressure, full pressure 
(five consecutive incorrect peer responses), and partial pressure (mixed correct 
and incorrect peer responses). We conducted 10 trials per condition combination 
(90 total observations), using standardized prompts and multiple-choice 
responses. The binomial test and chi-square analyses assessed performance 
differences across conditions.
RESULTS: Under no pressure, GPT-4o achieved 100% accuracy across all domains. 
Under full pressure, accuracy declined systematically with increasing diagnostic 
uncertainty: 50% in circle recognition, 40% in tumor identification, and 0% in 
psychiatric assessment. Partial pressure showed a similar pattern, with 
maintained accuracy in basic tasks (80% in circle recognition, 100% in tumor 
identification) but complete failure in psychiatric assessment (0%). All 
differences between no pressure and pressure conditions were statistically 
significant (P <.05), with the most severe effects observed in psychiatric 
assessment (χ²₁=16.20, P <.001).
CONCLUSIONS: This study reveals that LLMs exhibit conformity patterns that 
intensify with diagnostic uncertainty, culminating in complete performance 
failure in psychiatric assessment under social pressure. These findings suggest 
that successful implementation of AI in psychiatry requires careful 
consideration of social dynamics and the inherent uncertainty in psychiatric 
diagnosis. Future research should validate these findings across different AI 
systems and diagnostic tools while developing strategies to maintain AI 
independence in clinical settings.
TRIAL REGISTRATION: Not applicable.

© 2025. The Author(s).

DOI: 10.1186/s12888-025-06912-2
PMCID: PMC12070653
PMID: 40355854 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was approved by the institutional review board of Max 
Stern Yezreel Valley College (approval number YVC EMEK 2023-77). The study 
protocol complied with all relevant institutional guidelines. As the study 
analyzed only LLM outputs without human participants, informed consent was not 
required. Consent for publication: Not applicable. Competing interests: The 
authors declare no competing interests.


397. J Med Internet Res. 2025 May 12;27:e65397. doi: 10.2196/65397.

Classifying the Information Needs of Survivors of Domestic Violence in Online 
Health Communities Using Large Language Models: Prediction Model Development and 
Evaluation Study.

Guan S(#)(1), Hui V(#)(2)(3), Stiglic G(4), Constantino RE(3), Lee YJ(3), Wong 
AKC(5).

Author information:
(1)Department of Electrical and Electronic Engineering, The Hong Kong 
Polytechnic University, Hung Hom, China (Hong Kong).
(2)Centre for Smart Health, School of Nursing, The Hong Kong Polytechnic 
University, Hung Hom, China (Hong Kong).
(3)Department of Health and Community Systems, School of Nursing, University of 
Pittsburgh, Pittsburgh, PA, United States.
(4)Faculty of Health Sciences, University of Maribor, Maribor, Slovenia.
(5)School of Nursing, The Hong Kong Polytechnic University, Hung Hom, China 
(Hong Kong).
(#)Contributed equally

BACKGROUND: Domestic violence (DV) is a significant public health concern 
affecting the physical and mental well-being of numerous women, imposing a 
substantial health care burden. However, women facing DV often encounter 
barriers to seeking in-person help due to stigma, shame, and embarrassment. As a 
result, many survivors of DV turn to online health communities as a safe and 
anonymous space to share their experiences and seek support. Understanding the 
information needs of survivors of DV in online health communities through 
multiclass classification is crucial for providing timely and appropriate 
support.
OBJECTIVE: The objective was to develop a fine-tuned large language model (LLM) 
that can provide fast and accurate predictions of the information needs of 
survivors of DV from their online posts, enabling health care professionals to 
offer timely and personalized assistance.
METHODS: We collected 294 posts from Reddit subcommunities focused on DV shared 
by women aged ≥18 years who self-identified as experiencing intimate partner 
violence. We identified 8 types of information needs: shelters/DV 
centers/agencies; legal; childbearing; police; DV report 
procedure/documentation; safety planning; DV knowledge; and communication. Data 
augmentation was applied using GPT-3.5 to expand our dataset to 2216 samples by 
generating 1922 additional posts that imitated the existing data. We adopted a 
progressive training strategy to fine-tune GPT-3.5 for multiclass text 
classification using 2032 posts. We trained the model on 1 class at a time, 
monitoring performance closely. When suboptimal results were observed, we 
generated additional samples of the misclassified ones to give them more 
attention. We reserved 184 posts for internal testing and 74 for external 
validation. Model performance was evaluated using accuracy, recall, precision, 
and F1-score, along with CIs for each metric.
RESULTS: Using 40 real posts and 144 artificial intelligence-generated posts as 
the test dataset, our model achieved an F1-score of 70.49% (95% CI 
60.63%-80.35%) for real posts, outperforming the original GPT-3.5 and GPT-4, 
fine-tuned Llama 2-7B and Llama 3-8B, and long short-term memory. On artificial 
intelligence-generated posts, our model attained an F1-score of 84.58% (95% CI 
80.38%-88.78%), surpassing all baselines. When tested on an external validation 
dataset (n=74), the model achieved an F1-score of 59.67% (95% CI 51.86%-67.49%), 
outperforming other models. Statistical analysis revealed that our model 
significantly outperformed the others in F1-score (P=.047 for real posts; P<.001 
for external validation posts). Furthermore, our model was faster, taking 19.108 
seconds for predictions versus 1150 seconds for manual assessment.
CONCLUSIONS: Our fine-tuned LLM can accurately and efficiently extract and 
identify DV-related information needs through multiclass classification from 
online posts. In addition, we used LLM-based data augmentation techniques to 
overcome the limitations of a relatively small and imbalanced dataset. By 
generating timely and accurate predictions, we can empower health care 
professionals to provide rapid and suitable assistance to survivors of DV.

©Shaowei Guan, Vivian Hui, Gregor Stiglic, Rose Eva Constantino, Young Ji Lee, 
Arkers Kwan Ching Wong. Originally published in the Journal of Medical Internet 
Research (https://www.jmir.org), 12.05.2025.

DOI: 10.2196/65397
PMCID: PMC12107195
PMID: 40354642 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


398. PLOS Digit Health. 2025 May 12;4(5):e0000849. doi: 10.1371/journal.pdig.0000849. 
eCollection 2025 May.

Artificial intelligence's contribution to biomedical literature search: 
revolutionizing or complicating?

Yip R(1)(2), Sun YJ(1)(2), Bassuk AG(3), Mahajan VB(1)(2)(4).

Author information:
(1)Molecular Surgery Laboratory, Stanford University, Palo Alto, California, 
United States of America.
(2)Department of Ophthalmology, Byers Eye Institute, Stanford University, Palo 
Alto, California, United States of America.
(3)Department of Pediatrics, University of Iowa, Iowa City, Iowa, United States 
of America.
(4)Veterans Affairs Palo Alto Health Care System, Palo Alto, California, United 
States of America.

There is a growing number of articles about conversational AI (i.e., ChatGPT) 
for generating scientific literature reviews and summaries. Yet, comparative 
evidence lags its wide adoption by many clinicians and researchers. We explored 
ChatGPT's utility for literature search from an end-user perspective through the 
lens of clinicians and biomedical researchers. We quantitatively compared basic 
versions of ChatGPT's utility against conventional search methods such as Google 
and PubMed. We further tested whether ChatGPT user-support tools (i.e., plugins, 
web-browsing function, prompt-engineering, and custom-GPTs) could improve its 
response across four common and practical literature search scenarios: (1) 
high-interest topics with an abundance of information, (2) niche topics with 
limited information, (3) scientific hypothesis generation, and (4) for newly 
emerging clinical practices questions. Our results demonstrated that basic 
ChatGPT functions had limitations in consistency, accuracy, and relevancy. 
User-support tools showed improvements, but the limitations persisted. 
Interestingly, each literature search scenario posed different challenges: an 
abundance of secondary information sources in high interest topics, and 
uncompelling literatures for new/niche topics. This study tested practical 
examples highlighting both the potential and the pitfalls of integrating 
conversational AI into literature search processes, and underscores the 
necessity for rigorous comparative assessments of AI tools in scientific 
research.

Copyright: This is an open access article, free of all copyright, and may be 
freely reproduced, distributed, transmitted, modified, built upon, or otherwise 
used by anyone for any lawful purpose. The work is made available under the 
Creative Commons CC0 public domain dedication.

DOI: 10.1371/journal.pdig.0000849
PMCID: PMC12068611
PMID: 40354425

Conflict of interest statement: The authors have declared that no competing 
interests exist.


399. Diagn Interv Radiol. 2025 May 12. doi: 10.4274/dir.2025.253101. Online ahead of 
print.

Evaluating the reference accuracy of large language models in radiology: a 
comparative study across subspecialties.

Güneş YC(1), Cesur T(2), Çamur E(3).

Author information:
(1)Kırıkkale Yüksek İhtisas Hospital, Clinic of Radiology, Kırıkkale, Türkiye.
(2)Mamak State Hospital, Clinic of Radiology, Ankara, Türkiye.
(3)Ankara 29 May State Hospital, Clinic of Radiology, Ankara, Türkiye.

PURPOSE: This study aimed to compare six large language models (LLMs) [Chat 
Generative Pre-trained Transformer (ChatGPT)o1-preview, ChatGPT-4o, ChatGPT-4o 
with canvas, Google Gemini 1.5 Pro, Claude 3.5 Sonnet, and Claude 3 Opus] in 
generating radiology references, assessing accuracy, fabrication, and 
bibliographic completeness.
METHODS: In this cross-sectional observational study, 120 open-ended questions 
were administered across eight radiology subspecialties (neuroradiology, 
abdominal, musculoskeletal, thoracic, pediatric, cardiac, head and neck, and 
interventional radiology), with 15 questions per subspecialty. Each question 
prompted the LLMs to provide responses containing four references with in-text 
citations and complete bibliographic details (authors, title, journal, 
publication year/month, volume, issue, page numbers, and PubMed Identifier). 
References were verified using Medline, Google Scholar, the Directory of Open 
Access Journals, and web searches. Each bibliographic element was scored for 
correctness, and a composite final score [(FS): 0-36] was calculated by summing 
the correct elements and multiplying this by a 5-point verification score for 
content relevance. The FS values were then categorized into a 5-point Likert 
scale reference accuracy score (RAS: 0 = fabricated; 4 = fully accurate). 
Non-parametric tests (Kruskal-Wallis, Tamhane's T2, Wilcoxon signed-rank test 
with Bonferroni correction) were used for statistical comparisons.
RESULTS: Claude 3.5 Sonnet demonstrated the highest reference accuracy, with 
80.8% fully accurate references (RAS 4) and a fabrication rate of 3.1%, 
significantly outperforming all other models (P < 0.001). Claude 3 Opus ranked 
second, achieving 59.6% fully accurate references and a fabrication rate of 
18.3% (P < 0.001). ChatGPT-based models (ChatGPT-4o, ChatGPT-4o with canvas, and 
ChatGPT o1-preview) exhibited moderate accuracy, with fabrication rates ranging 
from 27.7% to 52.9% and <8% fully accurate references. Google Gemini 1.5 Pro had 
the lowest performance, achieving only 2.7% fully accurate references and the 
highest fabrication rate of 60.6% (P < 0.001). Reference accuracy also varied by 
subspecialty, with neuroradiology and cardiac radiology outperforming pediatric 
and head and neck radiology.
CONCLUSION: Claude 3.5 Sonnet significantly outperformed all other models in 
generating verifiable radiology references, and Claude 3 Opus showed moderate 
performance. In contrast, ChatGPT models and Google Gemini 1.5 Pro delivered 
substantially lower accuracy with higher rates of fabricated references, 
highlighting current limitations in automated academic citation generation.
CLINICAL SIGNIFICANCE: The high accuracy of Claude 3.5 Sonnet can improve 
radiology literature reviews, research, and education with dependable 
references. The poor performance of other models, with high fabrication rates, 
risks misinformation in clinical and academic settings and highlights the need 
for refinement to ensure safe and effective use.

DOI: 10.4274/dir.2025.253101
PMID: 40351113


400. Spine J. 2025 Dec;25(12):2710-2716. doi: 10.1016/j.spinee.2025.05.026. Epub 2025 
May 8.

Human versus machine: deciding on high-stakes surgery in possible cauda equina 
syndrome.

Najjar E(1), Abdelazim Hassan A(2), Muscogliati R(3), Salem KM(1), Quraishi 
NA(1).

Author information:
(1)Centre for Spinal Studies and Surgery, Queens Medical Centre, Nottingham 
University Hospitals NHS Trust, Nottingham, United Kingdom.
(2)Centre for Spinal Studies and Surgery, Queens Medical Centre, Nottingham 
University Hospitals NHS Trust, Nottingham, United Kingdom; Department of 
Orthopedics and Trauma Surgery, Assiut University School of Medicine, Assiut, 
Egypt.
(3)Centre for Spinal Studies and Surgery, Queens Medical Centre, Nottingham 
University Hospitals NHS Trust, Nottingham, United Kingdom; Hull York Medical 
School, University of Hull, Hull, United Kingdom. Electronic address: 
rmuscogliati@gmail.com.

BACKGROUND CONTEXT: Cauda equina syndrome (CES) is a spine surgical urgency 
requiring prompt intervention to prevent neurological deficits. Accurate 
identification of CES cases needing urgent surgery is essential to avoid 
long-term sequelae.
PURPOSE: To evaluate the concordance between an AI language model (ChatGPT) and 
a spinal multidisciplinary team (MDT) in recommending surgical intervention for 
suspected CES cases.
STUDY DESIGN/SETTING: Retrospective concordance analysis comparing surgical 
recommendations between ChatGPT and a Spinal MDT.
PATIENT SAMPLE: Among 160 referrals presenting with red flags for possible CES, 
10 cases were used to calibrate ChatGPT to specific clinical and diagnostic 
parameters, with the remaining 150 cases included in the primary analysis. The 
average patient age was 50.6 years (range 18-87), with a male-to-female ratio of 
68:82.
OUTCOME MEASURES: The primary outcome was the concordance rate between ChatGPT 
and the MDT in recommending surgery, evaluated through agreement rates and 
statistical analysis.
METHODS: Each of the 150 cases was presented as standardized slides including 
clinical history, imaging, and examination findings. Both the MDT and ChatGPT 
assessed the need for urgent surgery. Discordant cases (n=17) were further 
reviewed by 3 spinal surgeons blinded to prior decisions.
RESULTS: ChatGPT and the MDT agreed on surgical recommendations in 133 out of 
150 cases, achieving an 88.7% concordance (Cohen's Kappa = 0.764, p<.001). 
ChatGPT recommended surgery more frequently in the 17 discordant cases, but this 
difference was not statistically significant (McNemar's test statistic = 1.23, 
p=.46). Review by 3 independent surgeons reached consensus on 11 of the 17 
discordant cases (64.7%), highlighting variability among experts; individual 
surgeons aligned with ChatGPT in 5 to 6 cases each (29.4%-35.3%).
CONCLUSIONS: Substantial agreement between ChatGPT and the MDT suggests 
ChatGPT's comparable sensitivity in detecting surgical candidates in CES cases. 
Variability among surgeons on discordant cases underscores subjectivity in CES 
triage. ChatGPT may be a valuable adjunct in high-stakes clinical 
decision-making, though further validation and refinement are needed.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.spinee.2025.05.026
PMID: 40348281 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have influenced the work reported in this paper.


401. Surgery. 2025 Jul;183:109393. doi: 10.1016/j.surg.2025.109393. Epub 2025 May 9.

A literature review on the role of artificial intelligence-based chatbots in 
patient education in colorectal surgery.

Garcia Garcia L(1), Emile SH(2), Linkeshwaran L(3), Wignakumar A(4), Wexner 
SD(5).

Author information:
(1)Servicio de Cirugía General y Digestiva. Complejo Hospitalario Universitario 
Materno Infantil de Gran Canaria, Las Palmas de Gran Canaria, Spain.
(2)Ellen Leifer Shulman and Steven Shulman Digestive Disease Center, Cleveland 
Clinic Florida, Weston, FL; Colorectal Surgery Unit, General Surgery Department, 
Mansoura University Hospitals, Mansoura, Egypt. Electronic address: 
https://twitter.com/dr_samehhany81.
(3)UCL Medical School, University College London, London, United Kingdom.
(4)Ellen Leifer Shulman and Steven Shulman Digestive Disease Center, Cleveland 
Clinic Florida, Weston, FL. Electronic address: https://twitter.com/AWignakumar.
(5)Ellen Leifer Shulman and Steven Shulman Digestive Disease Center, Cleveland 
Clinic Florida, Weston, FL. Electronic address: wexners@ccf.org.

INTRODUCTION: Artificial intelligence-based chatbots are becoming increasingly 
used in patient education, in the realm of colorectal diseases. Perhaps, not 
surprisingly, concerns about the appropriateness of chatbot answers have been 
raised by healthcare professionals. Numerous studies have explored the utility 
and accuracy of chatbots in providing information in several clinical 
disciplines. This review aimed to summarize the findings of published studies, 
highlighting the strengths and limitations of chatbots used in medical education 
for colorectal surgery.
METHODS: We searched MEDLINE via PubMed and Scopus in February 2025 for original 
articles evaluating artificial intelligence-based chatbots in patient education 
related to colorectal surgery, categorizing them into 3 groups: colorectal 
cancer, inflammatory bowel diseases, and other colorectal conditions.
RESULTS: We identified 15 studies, 9 assessed chatbot utility in patient 
education in colorectal cancer, 4 assessed their utility in inflammatory bowel 
diseases, 1 involved benign anal conditions, and another involved intestinal 
stomas. Our findings indicated that chatbots, particularly ChatGPT, can improve 
patient education by providing accessible information on common questions. 
However, we also identified several limitations of the ability of chatbots to 
address complex medical issues which underscored that these tools may complement 
rather than replace professional medical guidance.
CONCLUSION: Chatbots may be useful for patient education related to simple and 
basic information, but not in complex and patient-specific settings. Future 
research should focus on refining chatbot algorithms to enhance the accuracy and 
depth of their responses, ensuring they effectively support patient education 
while maintaining the crucial role of healthcare providers.

Copyright © 2025 The Author(s). Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.surg.2025.109393
PMID: 40347684 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest/Disclosure Dr Wexner is a 
consultant for ActivSurgical, Arthrex, Baxter, Becton, Dickinson and Co, Glaxo 
Smith Kline, Intuitive Surgical, OstomyCure, Takeda, and Virtual Ports; has 
consulting agreements with stock options for consulting with GI View, 
OstomyCure, and Virtual Ports; is a member of the Data Safety Monitoring Board 
of JSR/WCG/ACI (chair) and Polypoid (chair); and receives royalties from 
Intuitive Surgical, Karl Storz Endoscopy America Inc, and Unique Surgical 
Solutions, LLC. Dr Emile is a consultant for Becton, Dickinson and Co. None of 
the other authors report any conflicts of interest.


402. J Oral Rehabil. 2025 Sep;52(9):1377-1385. doi: 10.1111/joor.13986. Epub 2025 May 
9.

Natural Language Understanding to Assess Oral Health-Related Quality of Life: A 
Cross-Sectional Study Incorporating a Mixed Methods Approach.

Anweigi L(1), Naceur IB(1), Awad J(1), Ahmeda M(2), Barhom N(1), Tamimi F(1).

Author information:
(1)College of Dental Medicine, QU Health, Qatar University, Doha, Qatar.
(2)Dentistry, Newcastle University, Newcastle upon Tyne, UK.

BACKGROUND: Natural language understanding (NLU), a subfield of artificial 
intelligence, focuses on the computational understanding of human language. This 
technology offers an objective and quantitative approach to analysing interviews 
in qualitative research. This study hypothesises that NLU can assess the impact 
of oral health on quality of life by analysing semi-structured interviews.
OBJECTIVE: This study aimed to assess the utility of NLU in evaluating oral 
health-related quality of life by analysing semi-structured interviews with 
individuals diagnosed with hypodontia.
METHODS: A cross-sectional qualitative study was conducted on 10 participants 
(aged 16-25 years) suffering from hypodontia. Semi-structured interviews were 
transcribed and analysed using IBM Watson NLU text analysis. The analysis 
identified entities, keywords, sentiments (positive and negative) and emotions 
(joy, sadness, anger, fear and disgust) expressed in the interviews.
RESULTS: NLU analysis revealed a predominantly negative sentiment towards 
hypodontia and its management, with 93.2% of identified entities presenting a 
negative sentiment and only 6.8% showing a positive sentiment. Patient sentiment 
correlated inversely with age (R = -0.49), treatment waiting time (R = -0.22) 
and OHIP score (R = -20). Negative sentiments and sadness were most prominent 
when discussing the history of dental problems and feelings about their teeth, 
whereas joy and positive sentiments were expressed regarding successful dental 
work. Keywords associated with negative sentiment were primarily related to 
treatment length and delays.
CONCLUSION: NLU effectively identified patients' negative sentiments and 
emotional responses to oral health conditions, demonstrating its potential as a 
valuable tool in qualitative dental research.

© 2025 The Author(s). Journal of Oral Rehabilitation published by John Wiley & 
Sons Ltd.

DOI: 10.1111/joor.13986
PMCID: PMC12408959
PMID: 40346733 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


403. PLoS One. 2025 May 9;20(5):e0322280. doi: 10.1371/journal.pone.0322280. 
eCollection 2025.

A research on cross-age facial recognition technology based on AT-GAN.

Chen G(1), Peng X(1), Xu R(1).

Author information:
(1)School of Information Network, Zhejiang Police College, Hangzhou, Zhejiang 
Province, China.

Currently, predicting a person's facial appearance many years later based on 
early facial features remains a core technical challenge. In this paper, we 
propose a cross-age face prediction framework based on Generative Adversarial 
Networks (GANs). This framework extracts key features from early photos of the 
target individual and predicts their facial appearance at different ages in the 
future. Within our framework, we designed a GAN-based image restoration 
algorithm to enhance image deblurring capabilities and improve the generation of 
fine details, thereby increasing image resolution. Additionally, we introduced a 
semi-supervised learning algorithm called Multi-scale Feature Aggregation 
Scratch Repair (Semi-MSFA), which leverages both synthetic datasets and real 
historical photos to better adapt to the task of restoring old photographs. 
Furthermore, we developed a generative adversarial network incorporating a 
self-attention mechanism to predict age-progressed face images, ensuring the 
generated images maintain relatively stable personal characteristics across 
different ages. To validate the robustness and accuracy of our proposed 
framework, we conducted qualitative and quantitative analyses on open-source 
portrait databases and volunteer-provided data. Experimental results demonstrate 
that our framework achieves high prediction accuracy and strong generalization 
capabilities.

Copyright: © 2025 Chen et al. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0322280
PMCID: PMC12063864
PMID: 40343967 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


404. iScience. 2025 Mar 27;28(5):112288. doi: 10.1016/j.isci.2025.112288. eCollection 
2025 May 16.

Computer-aided assessment for enlarged fetal heart with deep learning model.

Nurmaini S(1), Sapitri AI(1), Roseno MT(2), Rachmatullah MN(1), Mirani P(3), 
Bernolian N(4), Darmawahyuni A(1), Tutuko B(1), Firdaus F(1), Islami A(1), Arum 
AW(1), Bastian R(1).

Author information:
(1)Intelligent System Research Group, Universitas Sriwijaya, Palembang, 
Indonesia.
(2)Computer Science Department, Universitas Sumatera Selatan, Palembang, 
Indonesia.
(3)Department of Obstetrics and Gynecology, Fetomaternal Division, Bunda 
Hospital, Palembang, Indonesia.
(4)Department of Obstetrics and Gynecology, Fetomaternal Division, Dr. Mohammad 
Hoesin General Hospital, Palembang, Indonesia.

Enlarged fetal heart conditions may indicate congenital heart diseases or other 
complications, making early detection through prenatal ultrasound essential. 
However, manual assessments by sonographers are often subjective, 
time-consuming, and inconsistent. This paper proposes a deep learning approach 
using the You Only Look Once (YOLO) architecture to automate fetal heart 
enlargement assessment. Using a set of ultrasound videos, YOLOv8 with a CBAM 
module demonstrated superior performance compared to YOLOv11 with 
self-attention. Incorporating the ResNeXtBlock-a residual network with 
cardinality-additionally enhanced accuracy and prediction consistency. The model 
exhibits strong capability in detecting fetal heart enlargement, offering a 
reliable computer-aided tool for sonographers during prenatal screenings. 
Further validation is required to confirm its clinical applicability. By 
improving early and accurate detection, this approach has the potential to 
enhance prenatal care, facilitate timely interventions, and contribute to better 
neonatal health outcomes.

© 2025 The Author(s).

DOI: 10.1016/j.isci.2025.112288
PMCID: PMC12059722
PMID: 40343273

Conflict of interest statement: The authors declare that they have no conflict 
of interest.


405. Int J Pediatr Otorhinolaryngol. 2025 Jul;194:112369. doi: 
10.1016/j.ijporl.2025.112369. Epub 2025 May 4.

Artificial intelligence in pediatric otolaryngology: A state-of-the-art review 
of opportunities and pitfalls.

Navarathna N(1), Kanhere A(1), Gomez C(2), Isaiah A(3).

Author information:
(1)Department of Otorhinolaryngology-Head and Neck Surgery, University of 
Maryland School of Medicine, Baltimore, MD, USA; University of Maryland 
Institute for Health Computing, Bethesda, MD, USA.
(2)Department of Otorhinolaryngology-Head and Neck Surgery, University of 
Maryland School of Medicine, Baltimore, MD, USA.
(3)Department of Otorhinolaryngology-Head and Neck Surgery, University of 
Maryland School of Medicine, Baltimore, MD, USA; University of Maryland 
Institute for Health Computing, Bethesda, MD, USA; Department of Diagnostic 
Radiology and Nuclear Medicine, University of Maryland School of Medicine, 
Baltimore, MD, USA; Department of Pediatrics, University of Maryland School of 
Medicine, Baltimore, MD, USA. Electronic address: AIsaiah@som.umaryland.edu.

BACKGROUND: Artificial Intelligence (AI) and machine learning (ML) have 
transformative potential in enhancing diagnostics, treatment planning, and 
patient management. However, their application in pediatric otolaryngology 
remains limited as the unique physiological and developmental characteristics of 
children require tailored AI applications, highlighting a gap in knowledge.
PURPOSE: To provide a narrative review of current literature on the application 
of AI in pediatric otolaryngology, highlighting knowledge gaps, associated 
challenges and future directions.
RESULTS: ML models have demonstrated efficacy in diagnosing conditions such as 
otitis media, adenoid hypertrophy, and pediatric obstructive sleep apnea through 
deep learning-based image analysis and predictive modeling. AI systems also show 
potential in surgical settings such as landmark identification during otologic 
surgery and prediction of middle ear effusion during tympanostomy tube 
placement. Telemedicine solutions and large language models have shown potential 
to improve accessibility to care and patient education. The principal challenges 
include flawed generalization of adult training data and the relative lack of 
pediatric data.
CONCLUSIONS: AI holds significant promise in pediatric otolaryngology. However, 
its widespread clinical integration requires addressing algorithmic bias, 
enhancing model interpretability, and ensuring robust validation across 
pediatric population. Future research should prioritize federated learning, 
developmental trajectory modeling, and psychosocial integration to create 
patient-centered solutions.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.ijporl.2025.112369
PMCID: PMC12332797
PMID: 40334638 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
have no conflicts of interest to declare.


406. Nurs Rep. 2025 Apr 14;15(4):130. doi: 10.3390/nursrep15040130.

Assessing the Accuracy, Completeness and Safety of ChatGPT-4o Responses on 
Pressure Injuries in Infants: Clinical Applications and Future Implications.

Soddu M(1), De Vito A(2), Madeddu G(2), Nicolosi B(3), Provenzano M(4), Ivziku 
D(5), Curcio F(2)(6).

Author information:
(1)University Hospital of Sassari, Viale San Pietro 10, 07100 Sassari, Italy.
(2)Department of Medicine, Surgery, and Pharmacy, University of Sassari, 07100 
Sassari, Italy.
(3)Department of Health Professions, AOU Meyer IRCCS, 50139 Florence, Italy.
(4)Unit of General Surgery, Santissima Trinità Hospital, 09121 Cagliari, Italy.
(5)Department of Health Professions, Fondazione Policlinico Universitario Campus 
Bio-Medico, 00128 Rome, Italy.
(6)Faculty of Medicine and Surgery, University of Sassari (UNISS), 07100 
Sassari, Italy.

Background/Objectives: The advent of large language models (LLMs), like 
platforms such as ChatGPT, capable of generating quick and interactive answers 
to complex questions, opens the way for new approaches to training healthcare 
professionals, enabling them to acquire up-to-date and specialised information 
easily. In nursing, they have proven to support clinical decision making, 
continuing education, the development of care plans and the management of 
complex clinical cases, as well as the writing of academic reports and 
scientific articles. Furthermore, the ability to provide rapid access to 
up-to-date scientific information can improve the quality of care and promote 
evidence-based practice. However, their applicability in clinical practice 
requires thorough evaluation. This study evaluated the accuracy, completeness 
and safety of the responses generated by ChatGPT-4 on pressure injuries (PIs) in 
infants. Methods: In January 2025, we analysed the responses generated by 
ChatGPT-4 to 60 queries, subdivided into 12 main topics, on PIs in infants. The 
questions were developed, through consultation of authoritative documents, based 
on their relevance to nursing care and clinical potential. A panel of five 
experts, using a 5-point Likert scale, assessed the accuracy, completeness and 
safety of the answers generated by ChatGPT. Results: Overall, over 90% of the 
responses generated by ChatGPT-4o received relatively high ratings for the three 
criteria assessed with the most frequent value of 4. However, when analysing the 
12 topics individually, we observed that Medical Device Management and 
Technological Innovation were the topics with the lowest accuracy scores. At the 
same time, Scientific Evidence and Technological Innovation had the lowest 
completeness scores. No answers for the three criteria analysed were rated as 
completely incorrect. Conclusions: ChatGPT-4 has shown a good level of accuracy, 
completeness and safety in addressing questions about pressure injuries in 
infants. However, ongoing updates and integration of high-quality scientific 
sources are essential for ensuring its reliability as a clinical 
decision-support tool.

DOI: 10.3390/nursrep15040130
PMCID: PMC12029477
PMID: 40333050

Conflict of interest statement: The authors declare no conflicts of interest. 
The funders had no role in the design of the study; in the collection, analyses 
or interpretation of data; in the writing of the manuscript; or in the decision 
to publish the results.


407. OTO Open. 2025 May 5;9(2):e70125. doi: 10.1002/oto2.70125. eCollection 2025 
Apr-Jun.

Domain-Specific Customization for Language Models in Otolaryngology: The ENT GPT 
Assistant.

Bicknell BT(1), Rivers NJ(2), Skelton A(1), Sheehan D(2), Hodges C(1), Fairburn 
SC(1), Greene BJ(2), Panuganti B(3).

Author information:
(1)UAB Heersink School of Medicine University of Alabama at Birmingham 
Birmingham Alabama USA.
(2)Department of Otolaryngology-Head and Neck Surgery University of Alabama at 
Birmingham Birmingham Alabama USA.
(3)Department of Otolaryngology-Head and Neck Surgery Washington University in 
St. Louis St. Louis Missouri USA.

OBJECTIVE: To develop and evaluate the effectiveness of domain-specific 
customization in large language models (LLMs) by assessing the performance of 
the ENT GPT Assistant (E-GPT-A), a model specifically tailored for 
otolaryngology.
STUDY DESIGN: Comparative analysis using multiple-choice questions (MCQs) from 
established otolaryngology resources.
SETTING: Tertiary care academic hospital.
METHODS: Two hundred forty clinical-vignette style MCQs were sourced from 
BoardVitals Otolaryngology and OTOQuest, covering a range of otolaryngology 
subspecialties (n = 40 for each). The E-GPT-A was developed using targeted 
instructions and customized to otolaryngology. The performance of E-GPT-A was 
compared against top-performing and widely used artificial intelligence (AI) 
LLMs, including GPT-3.5, GPT-4, Claude 2.0, and Claude 2.1. Accuracy was 
assessed across subspecialties, varying question difficulty tiers, and in 
diagnostics and management.
RESULTS: E-GPT-A achieved an overall accuracy of 74.6%, outperforming GPT-3.5 
(60.4%), Claude 2.0 (61.7%), Claude 2.1 (60.8%), and GPT-4 (68.3%). The model 
performed best in allergy and rhinology (85.0%) and laryngology (82.5%), whereas 
showing lower accuracy in pediatrics (62.5%) and facial plastics/reconstructive 
surgery (67.5%). Accuracy also declined as question difficulty increased. The 
average correct response percentage among otolaryngologists and otolaryngology 
trainees was 71.1% in the question set.
CONCLUSION: This pilot study using the E-GPT-A demonstrates the potential 
benefits of domain-specific customizations of language models for 
otolaryngology. However, further development, continuous updates, and continued 
real-world validation are needed to fully assess the capabilities of LLMs in 
otolaryngology.

© 2025 The Author(s). OTO Open published by Wiley Periodicals LLC on behalf of 
American Academy of Otolaryngology–Head and Neck Surgery Foundation.

DOI: 10.1002/oto2.70125
PMCID: PMC12051367
PMID: 40331108

Conflict of interest statement: The authors declare that there are no conflicts 
of interest.


408. Alzheimers Dement. 2025 May;21(5):e70132. doi: 10.1002/alz.70132.

Dual-stream algorithms for dementia detection: Harnessing structured and 
unstructured electronic health record data, a novel approach to prevalence 
estimation.

Collyer TA(1)(2), Liu M(1)(2), Beare R(1)(2)(3), Andrew NE(1)(2), Ung D(1)(2), 
Carver A(1)(2), Ilomaki J(4), Bell JS(4), Thrift AG(5), Rocca WA(6)(7)(8), St 
Sauver JL(6), Lu A(1)(2)(9), Siostrom K(2)(9), Moran C(1)(2), Roberts H(10), 
Chong TT(11), Murray A(12)(13), Ravipati T(1), O'Bree B(1)(2), Srikanth 
VK(1)(2)(9).

Author information:
(1)National Centre for Healthy Ageing, Frankston, Victoria, Australia.
(2)Peninsula Clinical School, School of Translational Medicine, Monash 
University, Frankston, Victoria, Australia.
(3)Developmental Imaging, Murdoch Children's Research Institute, Melbourne, 
Victoria, Australia.
(4)Centre for Medicine Use and Safety, Faculty of Pharmacy and Pharmaceutical 
Sciences, Monash University, Parkville, Victoria, Australia.
(5)Department of Medicine, School of Clinical Sciences at Monash Health, Monash 
University, Clayton, Victoria, Australia.
(6)Division of Epidemiology, Department of Quantitative Health Sciences, Mayo 
Clinic, Rochester, Minnesota, USA.
(7)Department of Neurology, Mayo Clinic, Rochester, Minnesota, USA.
(8)Women's Health Research Center, Mayo Clinic, Rochester, Minnesota, USA.
(9)Department of Geriatric Medicine, Peninsula Health, Frankston, Victoria, 
Australia.
(10)Department of Neurology, Monash Medical Centre, Clayton, Victoria, 
Australia.
(11)Turner Institute for Brain and Mental Health, School of Psychological 
Sciences, Monash University, Notting Hill, Victoria, Australia.
(12)Division of Geriatrics, Department of Medicine Hennepin HealthCare, Berman 
Centre for Outcomes and Clinical Research, Hennepin Healthcare Research 
Institute, Minneapolis, Minnesota, USA.
(13)Department of Neurology, University of Minnesota, Minneapolis, Minnesota, 
USA.

INTRODUCTION: Identifying individuals with dementia is crucial for prevalence 
estimation and service planning, but reliable, scalable methods are lacking. We 
developed novel set algorithms using both structured and unstructured electronic 
health record (EHR) data, applying Diagnostic and Statistical Manual of Mental 
Disorders criteria for dementia case identification.
METHODS: Our cohort (n = 1082) included individuals aged ≥ 60 with dementia 
identified through specialist clinics and a comparison group without dementia. 
Clinicians from Australia and the United States informed predictor selection. We 
developed algorithms through a biostatistics stream for structured data and a 
natural language processing (NLP) stream for text, synthesizing results via 
logistic regression.
RESULTS: The final structured model retained 16 variables (area under the 
receiver operating characteristic curve [AUC] 0.853, specificity 72.2%, 
sensitivity 80.6%). NLP classifiers (logistic regression, support vector 
machine, and random forest models) performed comparably. The final, combined 
model outperformed all others (AUC = 0.951, P < 0.001 for comparison to 
structured model).
DISCUSSION: Embedding text-derived insights within algorithms trained on 
structured medical data significantly enhances dementia identification capacity.
HIGHLIGHTS: Algorithmic tools for detection of individuals with dementia are 
available; however, previous work has used heterogeneous case definitions which 
are not clinically meaningful, and has relied on proxies such as diagnostic 
codes or medications for case ascertainment. We used a novel, dual-stream 
algorithmic development approach, simultaneously and separately modeling a 
clinically meaningful outcome (diagnosis of dementia according to specialized 
clinical impression) using structured and unstructured electronic health record 
datasets. Our clinically grounded case definition supported the inclusion of key 
structured variables (such as dementia International Classification of Disease 
codes and medications) as modeling predictors rather than outcomes. Our 
algorithms, published in detail to support validation and replication, represent 
a major step forward in the use of routinely collected data for detection of 
diagnosed dementia.

© 2025 The Author(s). Alzheimer's & Dementia published by Wiley Periodicals LLC 
on behalf of Alzheimer's Association.

DOI: 10.1002/alz.70132
PMCID: PMC12053150
PMID: 40325920 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no conflicts of interest 
pertaining to the content of this manuscript. Author disclosures are available 
in the supporting information.


409. J Am Med Inform Assoc. 2025 Jun 1;32(6):1050-1060. doi: 10.1093/jamia/ocaf068.

Development and validation of the provider documentation summarization quality 
instrument for large language models.

Croxford E(1), Gao Y(2), Pellegrino N(3), Wong K(3)(4), Wills G(4), First E(3), 
Schnier M(3), Burton K(4), Ebby C(5), Gorski J(4)(6), Kalscheur M(4)(7), Khalil 
S(4), Pisani M(7), Rubeor T(4), Stetson P(8), Liao F(4)(6), Goswami C(4), 
Patterson B(4)(6), Afshar M(1)(4)(7).

Author information:
(1)Department of Biostatistics and Medical Informatics, University of Wisconsin, 
Madison, WI 53792, United States.
(2)Department of Biomedical Informatics, University of Colorado-Anschutz 
Medical, Aurora, CO 80045, United States.
(3)Epic Systems, Verona, WI 53593, United States.
(4)UW Health, Madison, WI 53726, United States.
(5)Department of Pediatrics, University of Wisconsin, Madison, WI 53792, United 
States.
(6)BerbeeWalsh Department of Emergency Medicine, University of Wisconsin, 
Madison, WI 53792, United States.
(7)Department of Medicine, University of Wisconsin, Madison, WI 53792, United 
States.
(8)Memorial Sloan Kettering Cancer Center, New York, NY 10065, United States.

OBJECTIVES: As large language models (LLMs) are integrated into electronic 
health record (EHR) workflows, validated instruments are essential to evaluate 
their performance before implementation and as models and documentation 
practices evolve. Existing instruments for provider documentation quality are 
often unsuitable for the complexities of LLM-generated text and lack validation 
on real-world data. The Provider Documentation Summarization Quality Instrument 
(PDSQI-9) was developed to evaluate LLM-generated clinical summaries. This study 
aimed to validate the PDSQI-9 across key aspects of construct validity.
MATERIALS AND METHODS: Multi-document summaries were generated from real-world 
EHR data across multiple specialties using several LLMs (GPT-4o, Mixtral 8x7b, 
and Llama 3-8b). Validation included Pearson correlation analyses for 
substantive validity, factor analysis and Cronbach's α for structural validity, 
inter-rater reliability (ICC and Krippendorff's α) for generalizability, a 
semi-Delphi process for content validity, and comparisons of high- versus 
low-quality summaries for discriminant validity. Raters underwent standardized 
training to ensure consistent application of the instrument.
RESULTS: Seven physician raters evaluated 779 summaries and answered 8329 
questions, achieving over 80% power for inter-rater reliability. The PDSQI-9 
demonstrated strong internal consistency (Cronbach's α = 0.879; 95% CI, 
0.867-0.891) and high inter-rater reliability (ICC = 0.867; 95% CI, 
0.867-0.868), supporting structural validity and generalizability. Factor 
analysis identified a 4-factor model explaining 58% of the variance, 
representing organization, clarity, accuracy, and utility. Substantive validity 
was supported by correlations between note length and scores for Succinct (ρ = 
-0.200, P = .029) and Organized (ρ = -0.190, P = .037). The semi-Delphi process 
ensured clinically relevant attributes, and discriminant validity distinguished 
high- from low-quality summaries (P<.001).
DISCUSSION: The PDSQI-9 showed high inter-rater reliability, internal 
consistency, and a meaningful factor structure that reliably captured key 
dimensions of documentation quality. It distinguished between high- and 
low-quality summaries, supporting its practical utility for health systems 
needing an evaluation instrument for LLMs.
CONCLUSIONS: The PDSQI-9 demonstrates robust construct validity, supporting its 
use in clinical practice to evaluate LLM-generated summaries and facilitate 
safer, more effective integration of LLMs into healthcare workflows.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocaf068
PMCID: PMC12089781
PMID: 40323321 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
declare.


410. Nurse Educ Pract. 2025 May;85:104383. doi: 10.1016/j.nepr.2025.104383. Epub 2025 
Apr 28.

ChatGPT-driven interactive virtual reality communication simulation in obstetric 
nursing: A mixed-methods study.

Chen PJ(1), Liou WK(2).

Author information:
(1)Department of Nursing, College of Healthcare and Management, Asia Eastern 
University of Science and Technology, New Taipei City 220303, Taiwan. Electronic 
address: fk036@mail.aeust.edu.tw.
(2)Empower Vocational Education Research Center, National Taiwan University of 
Science and Technology, Taipei City 106335, Taiwan. Electronic address: 
d9314903@mail.ntust.edu.tw.

AIM: (1) Describe the development process for the ChatGPT-based Virtual Reality 
Obstetric Nursing Communication Simulation System (ChatVR-ONCS), (2) explore its 
effects on communication competence in nursing students and (3) determine the 
perceived usability of ChatVR-ONCS and learning experiences in nursing students.
BACKGROUND: Communication is crucial in obstetric nursing, yet traditional 
teaching methods often fail to enhance students' relevant skills due to their 
lack of realism. Therefore, developing a training tool that offers high 
usability, immersive experiences and real-time interactivity is vital.
DESIGN: Mixed research methods were applied involving pre- and post-tests within 
a single group of 52 nursing students at a Taiwanese science and technology 
university as well as a focus group.
METHODS: The Visual Analog Scale for Communication Self-Confidence, Maternal and 
Newborn Care Communication Assessment Form and System Usability Scale were 
administered before the intervention (T0), immediately after the intervention 
(T1) and 3 months after the intervention (T2). A focus group was conducted after 
T2 to explore their learning experience and obtain feedback on the proposed 
system.
RESULTS: The participants' communication self-confidence, communication skills 
and ratings on system usability significantly improved after the ChatVR-ONCS 
intervention, with the effects lasting for 3 months. The qualitative analysis 
demonstrated a high level of user satisfaction and the positive effects of the 
system in promoting communication skill development.
CONCLUSION: By integrating artificial intelligence and virtual reality 
technologies, ChatVR-ONCS not only significantly enhances the effectiveness of 
nurse-patient communication training but also provides empirical support for 
digital development in nursing education.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.nepr.2025.104383
PMID: 40315616 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


411. Brief Bioinform. 2025 May 1;26(3):bbaf171. doi: 10.1093/bib/bbaf171.

PSTP: accurate residue-level phase separation prediction using protein 
conformational and language model embeddings.

Feng M(1)(2), Liu L(1)(2), Xian ZN(3), Wei X(1), Li K(1)(2), Yan W(1)(2), Lu 
Q(1), Shi Y(1)(2), He G(1)(2).

Author information:
(1)Bio-X Institutes, Key Laboratory for the Genetics of Developmental and 
Neuropsychiatric Disorders, Shanghai Jiao Tong University, No. 1954 Huashan 
Road, Xuhui District, Shanghai 200030, China.
(2)Shanghai Institute of Medical Genetics, Shanghai Children's Hospital, 
Shanghai Jiao Tong University School of Medicine, No. 24 Lane 1400 West Beijing 
Road, Jing'an District, Shanghai 200040, China.
(3)School of Environmental Science & Engineering, Shanghai Jiao Tong University, 
No. 800 Dongchuan Road, Minhang District, Shanghai 200240, China.

Phase separation (PS) is essential in cellular processes and disease mechanisms, 
highlighting the need for predictive algorithms to analyze uncharacterized 
sequences and accelerate experimental validation. Current high-accuracy methods 
often rely on extensive annotations or handcrafted features, limiting their 
generalizability to sequences lacking such annotations and making it difficult 
to identify key protein regions involved in PS. We introduce Phase Separation's 
Transfer-learning Prediction (PSTP), which combines conformational embeddings 
with large language model embeddings, enabling state-of-the-art PS predictions 
from protein sequences alone. PSTP performs well across various prediction 
scenarios and shows potential for predicting novel-designed artificial proteins. 
Additionally, PSTP provides residue-level predictions that are highly correlated 
with experimentally validated PS regions. By analyzing 160 000+ variants, PSTP 
characterizes the strong link between the incidence of pathogenic variants and 
residue-level PS propensities in unconserved intrinsically disordered regions, 
offering insights into underexplored mutation effects. PSTP's sliding-window 
optimization reduces its memory usage to a few hundred megabytes, facilitating 
rapid execution on typical CPUs and GPUs. Offered via both a web server and an 
installable Python package, PSTP provides a versatile tool for decoding protein 
PS behavior and supporting disease-focused research.

© The Author(s) 2025. Published by Oxford University Press.

DOI: 10.1093/bib/bbaf171
PMCID: PMC12047702
PMID: 40315433 [Indexed for MEDLINE]


412. J Pediatr Surg. 2025 Jul;60(7):162359. doi: 10.1016/j.jpedsurg.2025.162359. Epub 
2025 Apr 29.

Generative Artificial Intelligence Accuracy in Interpreting Forest Plots in 
Pediatric Surgery Meta-analyses: A Perspective From Pediatric Surgery 
Meta-analysis Study Group (PESMA).

Azizoglu M(1), Escolino M(2), Kamci TO(3), Klyuev S(4), Perez Bertolez S(5), 
Risteski T(6), Elhalaby I(7), Borkar N(8), Esposito C(2), Okur MH(9), Lacher 
M(10), Mutanen A(11), Shehata S(12), Chiarenza F(13), Davenport M(14).

Author information:
(1)Istanbul Esenyurt Necmi Kadioglu State Hospital, Department of Pediatric 
Surgery, Istanbul, Turkey; Istinye University, Department of Stem Cell and 
Tissue Engineering, Istanbul, Turkey. Electronic address: mdmazizoglu@gmail.com.
(2)Pediatric Surgery Unit, Federico II University Naples, Naples, Italy.
(3)Tatvan State Hospital, Department of Pediatric Surgery, Bitlis, Turkey.
(4)AO GK MEDSI, Department of Pediatric Surgery, Moscow, Turkey.
(5)Pediatric Urology Unit, Department of Pediatric Surgery, Hospital Sant Joan 
de Déu, Universitat de Barcelona, Barcelona, Spain.
(6)Ss. Cyril and Methodius University of Skopje, Faculty of Medicine, Department 
of Pediatric Surgery, Skopje, Macedonia.
(7)Department of Pediatric Surgery, Faculty of Medicine, Tanta University, 
Tanta, Egypt.
(8)Department of Paediatric Surgery, AIIMS, Raipur, Chhattisgarh, India.
(9)Dicle University Medical School, Department of Pediatric Surgery, Diyarbakir, 
Turkey.
(10)Department of Pediatric Surgery, University Hospital Leipzig, Leipzig, 
Germany.
(11)Department of Pediatric Surgery, The New Children's Hospital, Helsinki 
University Hospital and University of Helsinki, Helsinki, Finland.
(12)Department of Pediatric Surgery, Faculty of Medicine, Alexandria University, 
Alexandria, Egypt.
(13)Department of Pediatric Surgery, Pediatric Minimally Invasive Surgery and 
New Technologies, San Bortolo Hospital, Vicenza, Italy.
(14)Department of Paediatric Surgery, Kings College Hospital, London, UK.

DOI: 10.1016/j.jpedsurg.2025.162359
PMID: 40312006

Conflict of interest statement: Conflict of interest None.


413. JAMA Netw Open. 2025 May 1;8(5):e258094. doi: 10.1001/jamanetworkopen.2025.8094.

Deep Learning Model of Primary Tumor and Metastatic Cervical Lymph Nodes From CT 
for Outcome Predictions in Oropharyngeal Cancer.

Song B(1), Leroy A(2), Yang K(3), Khalighi S(1), Pandav K(1), Dam T(1), Lee 
J(4), Stock S(4), Li XT(5), Sonuga J(1), Fu P(6), Koyfman S(3), Saba NF(7), 
Patel MR(8), Madabhushi A(1)(9).

Author information:
(1)Wallace H. Coulter Department of Biomedical Engineering, Georgia Institute of 
Technology and Emory University, Atlanta, Georgia.
(2)Therapanacea, Paris, France.
(3)Department of Radiation Oncology, Holden Comprehensive Cancer Center, Iowa 
Neuroscience Institute, University of Iowa, Iowa City.
(4)Diagnostics Institute, Cleveland Clinic, Cleveland, Ohio.
(5)Department of Radiology and Imaging Sciences, Emory University Hospital, 
Atlanta, Georgia.
(6)Department of Population and Quantitative Health Sciences, Case Western 
Reserve University, Cleveland, Ohio.
(7)Department of Hematology and Medical Oncology, Winship Cancer Institute, 
Atlanta, Georgia.
(8)Department of Otolaryngology, Winship Cancer Institute, Atlanta, Georgia.
(9)Atlanta Veterans Affairs Medical Center, Atlanta, Georgia.

IMPORTANCE: Primary tumor (PT) and metastatic cervical lymph node (LN) 
characteristics are highly associated with oropharyngeal squamous cell carcinoma 
(OPSCC) prognosis. Currently, there is a lack of studies to combine imaging 
characteristics of both regions for predictions of p16+ OPSCC outcomes.
OBJECTIVES: To develop and validate a computed tomography (CT)-based deep 
learning classifier that integrates PT and LN features to predict outcomes in 
p16+ OPSCC and to identify patients with stage I disease who may derive added 
benefit associated with chemotherapy.
DESIGN, SETTING, AND PARTICIPANTS: In this retrospective prognostic study, 
radiographic CT scans were analyzed of 811 patients with p16+ OPSCC treated with 
definitive radiotherapy or chemoradiotherapy from 3 independent cohorts. One 
cohort from the Cancer Imaging Archive (1998-2013) was used for model 
development and validation and the 2 remaining cohorts (2002-2015) were used to 
externally test the model performance. The Swin Transformer architecture was 
applied to fuse the features from both PT and LN into a multiregion imaging risk 
score (SwinScore) to predict survival outcomes across and within subpopulations 
at various stages. Data analysis was performed between February and July 2024.
EXPOSURES: Definitive radiotherapy or chemoradiotherapy treatment for patients 
with p16+ OPSCC.
MAIN OUTCOMES AND MEASURES: Hazard ratios (HRs), log-rank tests, concordance 
index (C index), and net benefit were used to evaluate the associations between 
multiregion imaging risk score and disease-free survival (DFS), overall survival 
(OS), and locoregional failure (LRF). Interaction tests were conducted to assess 
whether the association of chemotherapy with outcome significantly differs 
across dichotomized multiregion imaging risk score subgroups.
RESULTS: The total patient cohort comprised 811 patients with p16+ OPSCC (median 
age, 59.0 years [IQR, 47.4-70.6 years]; 683 men [84.2%]). In the external test 
set, the multiregion imaging risk score was found to be prognostic of DFS (HR, 
3.76 [95% CI, 1.99-7.10]; P < .001), OS (HR, 4.80 [95% CI, 2.22-10.40]; 
P < .001), and LRF (HR, 4.47 [95% CI, 1.43-14.00]; P = .01) among all patients 
with p16+ OPSCC. The multiregion imaging risk score, integrating both PT and LN 
information, demonstrated a higher C index (0.63) compared with models focusing 
solely on PT (0.61) or LN (0.58). Chemotherapy was associated with improved DFS 
only among patients with high scores (HR, 0.09 [95% CI, 0.02-0.47]; P = .004) 
but not those with low scores (HR, 0.83 [95% CI, 0.32-2.10]; P = .69).
CONCLUSIONS AND RELEVANCE: This prognostic study of p16+ OPSCC describes the 
development of a CT-based imaging risk score integrating PT and metastatic 
cervical LN features to predict recurrence risk and identify suitable candidates 
for treatment tailoring. This tool could optimize treatment modulations of p16+ 
OPSCC at a highly granular level.

DOI: 10.1001/jamanetworkopen.2025.8094
PMCID: PMC12046429
PMID: 40310642 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest Disclosures: Dr Leroy 
reported receiving grants from Therapanacea during the conduct of the study. Dr 
Fu reported receiving grants from Case Western Reserve University during the 
conduct of the study. Dr Koyfman reported receiving personal fees from Merck, 
BMS, Regeneron, Castle Biosciences, Galera Therapeutics, UpToDate, and Varian 
Medical Systems outside the submitted work. Dr Saba reported receiving personal 
fees from Astra Zeneca, Eisai Medical, Exelixis, Merck, EMD Serono, Pfizer, 
Kura, Vaccinex, Cue Biopharma, BionTech, GSK, Tosk, Seagen, Flamingo, Infinity, 
Inovio, Aveo, Medscape, Onclive, UpToDate, BMS, Cornerstone, Celldex, Surface 
Oncology, Astex, Imugene, Faron Pharmaceutical, Coherus, Adagene, Fulgent, 
Springer, Nanobiotix, and Taiho outside the submitted work and funding from BMS 
and Exelixis. Dr Patel reported receiving personal fees from Intuitive Surgical 
outside the submitted work. Prof Madabhushi reported being co-founder of and 
holding equity in Picture Health, Elucid Bioimaging, and Inspirata Inc; 
receiving grants from Bristol Myers-Squibb and Astrazeneca during the conduct of 
the study; and receiving personal fees from Simbiosys Inc, Takeda Inc, Aiforia 
Inc, Caris Inc, and Roche Inc outside the submitted work; in addition, Prof 
Madabhushi had a patent for USSN 9,767,555 issued. No other disclosures were 
reported.


414. J Neurodev Disord. 2025 Apr 30;17(1):24. doi: 10.1186/s11689-025-09612-w.

Automated extraction of functional biomarkers of verbal and ambulatory ability 
from multi-institutional clinical notes using large language models.

Kaster L(1), Hillis E(1), Oh IY(1), Aravamuthan BR(2), Lanzotti VC(3), Vickstrom 
CR(2); Brain Gene Registry Consortium; Gurnett CA(2), Payne PRO(1), Gupta A(4).

Collaborators: Wasserstein M, Chopra M, Sahin M, Wangler M, Schultz B, Izumi K, 
Bergner S, Gropman A, Smith-Hicks C, Abbeduto L, Hazlett H, Doherty D, German K, 
DaWalt L, Neul J, Constantino J, Baldridge D, Srivastava S, Molholm S, Walkley 
S, Storch E, Samaco R, Cohen J, Shankar S, Piven J, Mahida S, Sveden A, Dies K, 
Riggs ER, Savatt JM, Minor B.

Author information:
(1)Institute for Informatics, Data Science and Biostatistics, Washington 
University School of Medicine in St. Louis, St. Louis, MO, USA.
(2)Department of Neurology, Washington University School of Medicine in St. 
Louis, St. Louis, MO, USA.
(3)Department of Psychiatry, Washington University School of Medicine in St. 
Louis, St. Louis, MO, USA.
(4)Institute for Informatics, Data Science and Biostatistics, Washington 
University School of Medicine in St. Louis, St. Louis, MO, USA. 
agupta24@wustl.edu.

BACKGROUND: Functional biomarkers in neurodevelopmental disorders, such as 
verbal and ambulatory abilities, are essential for clinical care and research 
activities. Treatment planning, intervention monitoring, and identifying 
comorbid conditions in individuals with intellectual and developmental 
disabilities (IDDs) rely on standardized assessments of these abilities. 
However, traditional assessments impose a burden on patients and providers, 
often leading to longitudinal inconsistencies and inequities due to evolving 
guidelines and associated time-cost. Therefore, this study aimed to develop an 
automated approach to classify verbal and ambulatory abilities from EHR data of 
IDD and cerebral palsy (CP) patients. Application of large language models 
(LLMs) to clinical notes, which are rich in longitudinal data, may provide a 
low-burden pipeline for extracting functional biomarkers efficiently and 
accurately.
METHODS: Data from the multi-institutional National Brain Gene Registry (BGR) 
and a CP clinic cohort were utilized, comprising 3,245 notes from 125 
individuals and 5,462 clinical notes from 260 individuals, respectively. 
Employing three LLMs-GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4 Omni-we provided the 
models with a clinical note and utilized a detailed conversational format to 
prompt the models to answer: "Does the individual use any words?" and "Can the 
individual walk without aid?" These responses were evaluated against 
ground-truth abilities, which were established using neurobehavioral assessments 
collected for each dataset.
RESULTS: LLM pipelines demonstrated high accuracy (weighted-F1 scores > .90) in 
predicting ambulatory ability for both cohorts, likely due to the consistent use 
of Gross Motor Functional Classification System (GMFCS) as a consistent 
ground-truth standard. However, verbal ability predictions were more accurate in 
the BGR cohort, likely due to higher adherence between the prompt and 
ground-truth assessment questions. While LLMs can be computationally expensive, 
analysis of our protocol affirmed the cost effectiveness when applied to select 
notes from the EHR.
CONCLUSIONS: LLMs are effective at extracting functional biomarkers from EHR 
data and broadly generalizable across variable note-taking practices and 
institutions. Individual verbal and ambulatory ability were accurately 
extracted, supporting the method's ability to streamline workflows by offering 
automated, efficient data extraction for patient care and research. Future 
studies are needed to extend this methodology to additional populations and to 
demonstrate more granular functional data classification.

© 2025. The Author(s).

DOI: 10.1186/s11689-025-09612-w
PMCID: PMC12042395
PMID: 40307685 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: No participants were recruited specifically for this study. This 
work constitutes secondary use of data approved by the Washington University in 
St. Louis IRB (protocols #202010013 [Brain Gene Registry cohort] and #202309003 
[cerebral palsy cohort]). Consent for publication: Not applicable. Competing 
interests: The authors declare no competing interests.


415. Appl Clin Inform. 2025 Aug;16(4):1121-1135. doi: 10.1055/a-2597-2017. Epub 2025 
Apr 30.

Clinical Implementation of Artificial Intelligence Scribes in Health Care: A 
Systematic Review.

Hassan H(1)(2), Zipursky AR(2)(3), Rabbani N(4), You JG(5)(6), Tse G(7), 
Orenstein E(8)(9), Ray M(10), Parsons C(10), Shin S(8)(9), Lawton G(11), Jessa 
K(3), Sung L(1)(2), Yan AP(1)(2).

Author information:
(1)Division of Haematology/Oncology, The Hospital for Sick Children, Toronto, 
Canada.
(2)Program in Child Health Evaluative Sciences, Peter Gilgan Research Institute, 
The Hospital for Sick Children, Toronto, Canada.
(3)Department of Emergency Medicine, The Hospital for Sick Children, Toronto, 
Canada.
(4)Department of Pediatrics, Harvard Medical School, Boston, Massachusetts, 
United States.
(5)Department of Pathology, Massachusetts General Hospital, Boston, 
Massachusetts, United States.
(6)Department of Medicine, Brigham and Women's Hospital, Boston, Massachusetts, 
United States.
(7)Department of Pediatrics, Stanford University, Stanford, California, United 
States.
(8)Information Services and Technology, Children's Healthcare of Atlanta, 
Atlanta Georgia, United States.
(9)Department of Pediatrics, Emory University School of Medicine, Atlanta, 
Georgia, United States.
(10)Department of Pediatrics, Boston Children's Hospital, Boston, Massachusetts, 
United States.
(11)Department of Pediatrics, Children's Hospital of Philadelphia, Philadelphia, 
Pennsylvania, United States.

Artificial intelligence (AI) scribes use advanced speech recognition and natural 
language processing to automate clinical documentation and ease administrative 
burden. However, little is known about the effect of AI scribes on clinicians, 
patients, and organizations.This study aimed to (1) propose an evaluation 
framework to guide future AI scribe implementations, (2) describe the effect of 
AI scribes along the domains proposed in the developed evaluation framework, and 
(3) identify gaps in the AI scribe implementation literature to be evaluated in 
future studies.Databases including Embase, Embase Classic, and Ovid Medline were 
searched, and a manual review was conducted of the New England Journal of 
Medicine AI. Studies published after 2021 that reported on the implementation of 
AI scribes in health care were included. Descriptive analysis was undertaken. 
Quality assessment was undertaken using the Newcastle-Ottawa Scale. The nominal 
group technique was used to develop an evaluation framework.Eleven studies met 
the inclusion criteria, with 10 published in 2024. The most frequently used AI 
scribe was Dragon Ambient eXperience (n = 7, 64%). While clinicians often 
reported improved documentation quality, AI scribe accuracy varied, frequently 
requiring manual edits and raising occasional concerns about errors. Nine of 10 
studies reported improvements in at least one efficiency metric, and seven of 
ten studies highlighted positive effects on clinician wellness and burnout. 
Patient experience was assessed in three studies, all reporting favorable 
outcomes.AI scribes represent a promising tool for improving clinical efficiency 
and alleviating documentation burden. This systematic review highlights the 
potential benefits of AI scribes, including reduced documentation time and 
enhanced clinician satisfaction, while also identifying critical challenges such 
as variable adoption, performance limitations, and gaps in evaluation.

Thieme. All rights reserved.

DOI: 10.1055/a-2597-2017
PMCID: PMC12449105
PMID: 40306686 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no conflict of interest to 
report. M.R. has received consulting fees from Abridge, an artificial 
intelligence scribe company.


416. Front Med (Lausanne). 2025 Apr 15;12:1511487. doi: 10.3389/fmed.2025.1511487. 
eCollection 2025.

Automatic pelvic fracture segmentation: a deep learning approach and benchmark 
dataset.

Liu Y(1), Yibulayimu S(1), Zhu G(2), Shi C(1), Liang C(1), Zhao C(3), Wu X(3), 
Sang Y(2), Wang Y(1)(2).

Author information:
(1)Beijing Advanced Innovation Center for Biomedical Engineering, School of 
Biological Science and Medical Engineering, Beihang University, Beijing, China.
(2)Beijing Rossum Robot Technology Co., Ltd., Beijing, China.
(3)Department of Orthopaedics and Traumatology, Beijing Jishuitan Hospital, 
Beijing, China.

INTRODUCTION: Accurate segmentation of pelvic fractures from computed tomography 
(CT) is crucial for trauma diagnosis and image-guided reduction surgery. The 
traditional manual slice-by-slice segmentation by surgeons is time-consuming, 
experience-dependent, and error-prone. The complex anatomy of the pelvic bone, 
the diversity of fracture types, and the variability in fracture surface 
appearances pose significant challenges to automated solutions.
METHODS: We propose an automatic pelvic fracture segmentation method based on 
deep learning, which effectively isolates hipbone and sacrum fragments from 
fractured pelvic CT. The method employs two sequential networks: an anatomical 
segmentation network for extracting hipbones and sacrum from CT images, followed 
by a fracture segmentation network that isolates the main and minor fragments 
within each bone region. We propose a distance-weighted loss to guide the 
fracture segmentation network's attention on the fracture surface. Additionally, 
multi-scale deep supervision and smooth transition strategies are incorporated 
to enhance overall performance.
RESULTS: Tested on a curated dataset of 150 CTs, which we have made publicly 
available, our method achieves an average Dice coefficient of 0.986 and an 
average symmetric surface distance of 0.234 mm.
DISCUSSION: The method outperformed traditional max-flow and a transformer-based 
method, demonstrating its effectiveness in handling complex fracture.

Copyright © 2025 Liu, Yibulayimu, Zhu, Shi, Liang, Zhao, Wu, Sang and Wang.

DOI: 10.3389/fmed.2025.1511487
PMCID: PMC12039937
PMID: 40303367

Conflict of interest statement: GZ, YS, and YW were employed by Beijing Rossum 
Robot Technology Co., Ltd. The remaining authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


417. Comput Biol Med. 2025 Jun;192(Pt A):110150. doi: 
10.1016/j.compbiomed.2025.110150. Epub 2025 Apr 26.

Benchmarking HEp-2 cell segmentation methods in indirect immunofluorescence 
images - standard models to deep learning.

Iyer B(1), Deoghare S(2), Ranjan K(3), Aronow BJ(4), Prasath VBS(5).

Author information:
(1)Division of Biomedical Informatics, Cincinnati Children's Hospital Medical 
Center, Cincinnati, OH 45229, USA; Department of Electrical Engineering and 
Computer Science, University of Cincinnati, OH 45221, USA. Electronic address: 
iyerbs@mail.uc.edu.
(2)Division of Biomedical Informatics, Cincinnati Children's Hospital Medical 
Center, Cincinnati, OH 45229, USA; Department of Biomedical Informatics, College 
of Medicine, University of Cincinnati, OH 45267, USA.
(3)Division of Biomedical Informatics, Cincinnati Children's Hospital Medical 
Center, Cincinnati, OH 45229, USA.
(4)Division of Biomedical Informatics, Cincinnati Children's Hospital Medical 
Center, Cincinnati, OH 45229, USA; Department of Pediatrics, University of 
Cincinnati, USA; Department of Biomedical Informatics, College of Medicine, 
University of Cincinnati, OH 45267, USA.
(5)Division of Biomedical Informatics, Cincinnati Children's Hospital Medical 
Center, Cincinnati, OH 45229, USA; Department of Pediatrics, University of 
Cincinnati, USA; Department of Biomedical Informatics, College of Medicine, 
University of Cincinnati, OH 45267, USA; Department of Electrical Engineering 
and Computer Science, University of Cincinnati, OH 45221, USA.

Indirect Immunofluorescence (IIF) stained Human Epithelial (HEp-2) cells are 
considered the gold standard for detecting autoimmune diseases. Accurate cell 
segmentation, though often viewed as an intermediary step to downstream tasks 
like classification, significantly enhances overall performance when executed 
with precision. In this study, we conduct a systematic literature review of 
HEp-2 cell segmentation techniques, identifying 28 key papers utilizing 
traditional image processing, machine learning classifiers, deep convolutional 
neural networks (CNNs), and generative adversarial network (GAN) frameworks. 
Building on these insights, we benchmark 17 CNN models without pretraining and 8 
CNN models pretrained on ImageNet using both Frozen Encoder and Tunable Encoder 
strategies on the I3A dataset. Cross-validation (CV) and Benjamini-Hochberg (BH) 
significance correction were employed to ensure statistical rigor in model 
comparisons. Domain-Specific Pretraining (DSPT) experiments demonstrated 
performance improvements, particularly for underrepresented classes, while Data 
Augmentation strategies (DA-1 and DA-2) revealed distinct impacts across model 
categories. GAN-based segmentation experiments using the top-performing CNN 
architectures as generators within a Pix2Pix framework revealed performance 
degradation due to data limitations and adversarial training instabilities. 
Nonetheless, GANs displayed class-specific improvements in visual alignment of 
segmentation masks. Results were evaluated comprehensively across eight 
performance metrics, including Dice, IOU, Accuracy, Precision, Sensitivity, 
Specificity, AU-ROC and AU-PR. This work offers a robust benchmarking of 
state-of-the-art CNN, GAN, and Transformer-based models for HEp-2 cell 
segmentation, providing valuable insights for future research directions, 
including ensemble approaches, dynamic patch sampling, and diffusion models.

Copyright © 2025 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.compbiomed.2025.110150
PMID: 40288291 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


418. Int J Emerg Med. 2025 Apr 25;18(1):85. doi: 10.1186/s12245-025-00885-5.

Assessing artificial intelligence-generated patient discharge information for 
the emergency department: a pilot study.

De Rouck R(1)(2), Wille E(3), Gilbert A(4), Vermeersch N(5).

Author information:
(1)AZ Sint Maria Halle, Ziekenhuislaan 100, Halle, 1500, Belgium. 
ruben.de.rouck@vub.be.
(2)Research Group on Emergency and Disaster Medicine, Vrije Universiteit 
Brussel, Laarbeeklaan 103, Brussels, 1090, Belgium. ruben.de.rouck@vub.be.
(3)Department of Intensive Care Medicine, UZ Brussel, Laarbeeklaan 101, 
Brussels, 1090, Belgium.
(4)Chair of AI and Digital Medicine, University of Mons, Place du Parc 20, Mons, 
7000, Belgium.
(5)AZ Sint Maria Halle, Ziekenhuislaan 100, Halle, 1500, Belgium.

BACKGROUND: Effective patient discharge information (PDI) in emergency 
departments (EDs) is vital and often more crucial than the diagnosis itself. 
Patients who are well informed at discharge tend to be more satisfied and 
experience better health outcomes. The combination of written and verbal 
instructions tends to improve patient recall. However, creating written 
discharge materials is both time-consuming and costly. With the emergence of 
generative artificial intelligence (AI) and large language models (LMMs), there 
is potential for the efficient production of patient discharge documents. This 
study aimed to investigate several predefined key performance indicators (KPIs) 
of AI-generated patient discharge information.
METHODS: This study focused on three significant patients' complaints in the ED: 
nonspecific abdominal pain, nonspecific low back pain, and fever in children. To 
generate the brochures, we used an English query for ChatGPT using the GPT-4 LLM 
and DeepL software to translate the brochures to Dutch. Five KPIs were defined 
to assess these PDI brochures: quality, accessibility, clarity, correctness and 
usability. The brochures were evaluated for each KPI by 8 experienced emergency 
physicians using a rating scale from 1 (very poor) to 10 (excellent). To 
quantify the readability of the brochures, frequently used indices were 
employed: the Flesch Reading Ease, Flesch-Kincaid Grade Level, Simple Measure of 
Gobbledygook, and Coleman-Liau Index on the translated text.
RESULTS: The brochures generated by ChatGPT/GPT-4 were well received, scoring an 
average of 7 to 8 out of 10 across all evaluated aspects. However, the results 
also indicated a need for some revisions to perfect these documents. Readability 
analysis indicated that brochures require high school- to college-level 
comprehension, but this is likely an overestimation due to context-specific 
reasons as well as features inherent to the Dutch language.
CONCLUSION: Our findings indicate that AI tools such as LLM could represent a 
new opportunity to quickly produce patient discharge information brochures. 
However, human review and editing are essential to ensure accurate and reliable 
information. A follow-up study with more topics and validation in the intended 
population is necessary to assess their performance.

© 2025. The Author(s).

DOI: 10.1186/s12245-025-00885-5
PMCID: PMC12023409
PMID: 40281395

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Ethical committee approval was granted from the AZ Sint Maria 
hospital’s ethics board. Informed consent was obtained from all individual 
participants included in the study. Consent for publication: All authors have 
read and approved the manuscript for publication. All participants in the study 
consented to publication of their data before participating in the study. 
Competing interests: The authors declare no competing interests.


419. J Med Internet Res. 2025 Apr 25;27:e68427. doi: 10.2196/68427.

Enhancing Bidirectional Encoder Representations From Transformers (BERT) With 
Frame Semantics to Extract Clinically Relevant Information From German 
Mammography Reports: Algorithm Development and Validation.

Reichenpfader D(1)(2), Knupp J(3), von Däniken SU(4), Gaio R(5), Dennstädt F(5), 
Cereghetti GM(4), Sander A(6), Hiltbrunner H(4), Nairz K(4), Denecke K(1).

Author information:
(1)Institute for Patient-Centered Digital Health, School of Engineering and 
Computer Science, Bern University of Applied Sciences, Biel/Bienne, Switzerland.
(2)PhD School of Life Sciences, Faculty of Medicine, University of Geneva, 
Geneva, Switzerland.
(3)ID SUISSE AG, St. Gallen, Switzerland.
(4)Department of Diagnostic, Interventional, and Pediatric Radiology, Bern 
University Hospital, University of Bern, Bern, Switzerland.
(5)Department of Radiation Oncology, Bern University Hospital, University of 
Bern, Bern, Switzerland.
(6)ID BERLIN, Berlin, Germany.

BACKGROUND: Structured reporting is essential for improving the clarity and 
accuracy of radiological information. Despite its benefits, the European Society 
of Radiology notes that it is not widely adopted. For example, while structured 
reporting frameworks such as the Breast Imaging Reporting and Data System 
provide standardized terminology and classification for mammography findings, 
radiology reports still mostly comprise free-text sections. This variability 
complicates the systematic extraction of key clinical data. Moreover, manual 
structuring of reports is time-consuming and prone to inconsistencies. Recent 
advancements in large language models have shown promise for clinical 
information extraction by enabling models to understand contextual nuances in 
medical text. However, challenges such as domain adaptation, privacy concerns, 
and generalizability remain. To address these limitations, frame semantics 
offers an approach to information extraction grounded in computational 
linguistics, allowing a structured representation of clinically relevant 
concepts.
OBJECTIVE: This study explores the combination of Bidirectional Encoder 
Representations from Transformers (BERT) architecture with the linguistic 
concept of frame semantics to extract and normalize information from free-text 
mammography reports.
METHODS: After creating an annotated corpus of 210 German reports for 
fine-tuning, we generate several BERT model variants by applying 3 pretraining 
strategies to hospital data. Afterward, a fact extraction pipeline is built, 
comprising an extractive question-answering model and a sequence labeling model. 
We quantitatively evaluate all model variants using common evaluation metrics 
(model perplexity, Stanford Question Answering Dataset 2.0 [SQuAD_v2], seqeval) 
and perform a qualitative clinician evaluation of the entire pipeline on a 
manually generated synthetic dataset of 21 reports, as well as a comparison with 
a generative approach following best practice prompting techniques using the 
open-source Llama 3.3 model (Meta).
RESULTS: Our system is capable of extracting 14 fact types and 40 entities from 
the clinical findings section of mammography reports. Further pretraining on 
hospital data reduced model perplexity, although it did not significantly impact 
the 2 downstream tasks. We achieved average F1-scores of 90.4% and 81% for 
question answering and sequence labeling, respectively (best pretraining 
strategy). Qualitative evaluation of the pipeline based on synthetic data shows 
an overall precision of 96.1% and 99.6% for facts and entities, respectively. In 
contrast, generative extraction shows an overall precision of 91.2% and 87.3% 
for facts and entities, respectively. Hallucinations and extraction 
inconsistencies were observed.
CONCLUSIONS: This study demonstrates that frame semantics provides a robust and 
interpretable framework for automating structured reporting. By leveraging frame 
semantics, the approach enables customizable information extraction and supports 
generalization to diverse radiological domains and clinical contexts with 
additional annotation efforts. Furthermore, the BERT-based model architecture 
allows for efficient, on-premise deployment, ensuring data privacy. Future 
research should focus on validating the model's generalizability across external 
datasets and different report types to ensure its broader applicability in 
clinical practice.

©Daniel Reichenpfader, Jonas Knupp, Sandro Urs von Däniken, Roberto Gaio, Fabio 
Dennstädt, Grazia Maria Cereghetti, André Sander, Hans Hiltbrunner, Knud Nairz, 
Kerstin Denecke. Originally published in the Journal of Medical Internet 
Research (https://www.jmir.org), 25.04.2025.

DOI: 10.2196/68427
PMCID: PMC12064967
PMID: 40279645 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: At the time this study 
was conducted, JK was employed by ID SUISSE AG. AS is a board member of ID 
BERLIN and an affiliate of ID SUISSE AG.


420. J Intell. 2025 Apr 7;13(4):45. doi: 10.3390/jintelligence13040045.

Automated Graphic Divergent Thinking Assessment: A Multimodal Machine Learning 
Approach.

Zhang H(1), Dong H(1), Wang Y(1), Zhang X(1), Yu F(1), Ren B(1), Xu J(1)(2).

Author information:
(1)Faculty of Psychology, Beijing Normal University, Beijing 100875, China.
(2)Faculty of Arts and Sciences, Beijing Normal University, Zhuhai 519085, 
China.

This study proposes a multimodal deep learning model for automated scoring of 
image-based divergent thinking tests, integrating visual and semantic features 
to improve assessment objectivity and efficiency. Utilizing 708 Chinese high 
school students' responses from validated tests, we developed a system combining 
pretrained ResNet50 (image features) and GloVe (text embeddings), fused through 
a fully connected neural network with MSE loss and Adam optimization. The 
training set (603 images, triple-rated consensus scores) showed strong alignment 
with human scores (Pearson r = 0.810). Validation on 100 images demonstrated 
generalization capacity (r = 0.561), while participant-level analysis achieved 
0.602 correlation with total human scores. Results indicate multimodal 
integration effectively captures divergent thinking dimensions, enabling 
simultaneous evaluation of novelty, fluency, and flexibility. This approach 
reduces manual scoring subjectivity, streamlines assessment processes, and 
maintains cost-effectiveness while preserving psychometric rigor. The findings 
advance automated cognitive evaluation methodologies by demonstrating the 
complementary value of visual-textual feature fusion in creativity assessment.

DOI: 10.3390/jintelligence13040045
PMCID: PMC12028077
PMID: 40278054

Conflict of interest statement: The authors declare no conflicts of interest.


421. Cureus. 2025 Mar 25;17(3):e81145. doi: 10.7759/cureus.81145. eCollection 2025 
Mar.

An Academic Viewpoint (2025) on the Integration of Generative Artificial 
Intelligence in Medical Education: Transforming Learning and Practices.

Almansour M(1), Soliman M(1), Aldekhyyel R(1), Binkheder S(1), Temsah MH(2)(3), 
Malki KH(4).

Author information:
(1)Department of Medical Education, College of Medicine, King Saud University, 
Riyadh, SAU.
(2)Department of Pediatrics, College of Medicine, King Saud University, Riyadh, 
SAU.
(3)Pediatric Intensive Care Unit, Department of Pediatrics, King Saud University 
Medical City, Riyadh, SAU.
(4)Research Chair of Voice, Swallowing, and Communication Disorders, Department 
of Otolaryngology-Head and Neck Surgery, College of Medicine, King Saud 
University, Riyadh, SAU.

Generative artificial intelligence (GAI) has introduced a new era of medical 
education by offering innovative solutions to critical challenges in teaching, 
assessment, and clinical training. This expanded review explores the current and 
potential applications of GAI across multiple domains, including personalized 
tutoring, enhanced academic administrative efficiency, and improved preparedness 
for daily learning interactions. Utilizing a narrative review methodology 
combined with expert analysis, this study involved a structured literature 
search in January 2025 across PubMed, Scopus, and Google Scholar, followed by 
iterative brainstorming sessions and expert evaluations to assess the 
feasibility and impact of various GAI applications. Six domain experts then 
appraised the feasibility and impact of GAI technologies across educational 
settings, resulting in 10 identified domains of application: Quality and 
Administration, Curriculum Development, Teaching and Learning, Assessment and 
Evaluation, Clinical Training, Academic Guidance, Student Research, Student 
Affairs, Internship Management, and Student Activities. Our findings highlight 
how GAI supports personalized learning - through adaptive tutoring and automated 
performance dashboards - while optimizing administrative tasks such as course 
registration and policy oversight. In addition, immersive simulations and 
virtual patient encounters reinforce clinical decision-making and practical 
skills. GAI-driven tools also streamline research processes via automated 
literature reviews and proposal refinement, ultimately fostering greater 
efficiency across academic environments. Despite these opportunities, ethical 
considerations remain a priority. Issues pertaining to data privacy, algorithmic 
bias, and equitable access must be addressed through robust regulatory 
frameworks and institution-wide policies. Overall, by embracing targeted, 
ethically guided implementations, GAI has the evolving potential to enhance 
educational quality, improve operational effectiveness, and equip future 
healthcare professionals with the adaptive skills needed in a patient-centered 
clinical landscape.

Copyright © 2025, Almansour et al.

DOI: 10.7759/cureus.81145
PMCID: PMC12020443
PMID: 40276436

Conflict of interest statement: Conflicts of interest: In compliance with the 
ICMJE uniform disclosure form, all authors declare the following: 
Payment/services info: All authors have declared that no financial support was 
received from any organization for the submitted work. Financial relationships: 
All authors have declared that they have no financial relationships at present 
or within the previous three years with any organizations that might have an 
interest in the submitted work. Other relationships: All authors have declared 
that there are no other relationships or activities that could appear to have 
influenced the submitted work.


422. Psychiatry Res. 2025 Jun;348:116501. doi: 10.1016/j.psychres.2025.116501. Epub 
2025 Apr 15.

Assessing bias in AI-driven psychiatric recommendations: A comparative 
cross-sectional study of chatbot-classified and CANMAT 2023 guideline for 
adjunctive therapy in difficult-to-treat depression.

Chang Y(1), Liu YC(2), Huang SS(3), Hsu WY(4).

Author information:
(1)Department of Psychiatry, Changhua Christian Hospital, Changhua 500, Taiwan.
(2)Department of Psychiatry, Changhua Christian Hospital, Changhua 500, Taiwan; 
Department of Psychiatry, Changhua Christian Children's Hospital, Changhua 500, 
Taiwan; Department of Health Business Administration, Hungkuang University, 
Taichung City 433, Taiwan.
(3)Department of Psychiatry, Changhua Christian Hospital, Changhua 500, Taiwan; 
Post Baccalaureate Medicine, National Chung Hsing University, Taichung, 402202, 
Taiwan. Electronic address: 97278@cch.org.tw.
(4)Department of Psychiatry, Changhua Christian Hospital, Changhua 500, Taiwan; 
Post Baccalaureate Medicine, National Chung Hsing University, Taichung, 402202, 
Taiwan; School of Medicine, Chung Shan Medical University, Taichung 40201, 
Taiwan. Electronic address: cchhsuwenyu@gmail.com.

The integration of chatbots into psychiatry introduces a novel approach to 
support clinical decision-making, but biases in their recommendations pose 
significant concerns. This study investigates potential biases in 
chatbot-generated recommendations for adjunctive therapy in difficult-to-treat 
depression, comparing these outputs with the Canadian Network for Mood and 
Anxiety Treatments (CANMAT) 2023 guidelines. The analysis involved calculating 
Cohen's kappa coefficients to measure the overall level of agreement between 
chatbot-generated classifications and CANMAT guidelines. Differences between 
chatbot-generated and CANMAT classifications for each medication were assessed 
using the Wilcoxon signed-rank test. Results reveal substantial agreement for 
high-performing models, such as Google AI's Gemini 2.0 Flash, which achieved the 
highest Cohen's kappa value of 0.82 (SE = 0.052). In contrast, OpenAI's o1 model 
showed a lower agreement of 0.746 (SE = 0.057). Notable discrepancies were 
observed in the overestimation of medications such as quetiapine and lithium and 
the underestimation of modafinil and ketamine. Additionally, a distinct bias 
pattern was observed in OpenAI's chatbots, which demonstrated a tendency to 
over-recommend lithium and bupropion. Our study highlights both the promise and 
the challenges of employing AI tools in psychiatric practice, and advocates for 
multi-model approaches to mitigate bias and improve clinical reliability.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.psychres.2025.116501
PMID: 40267866 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


423. Hum Genet. 2025 Jun;144(6):605-614. doi: 10.1007/s00439-025-02743-z. Epub 2025 
Apr 23.

Harnessing genotype and phenotype data for population-scale variant 
classification using large language models and bayesian inference.

Manders TR(1), Tan CA(2), Kobayashi Y(2), Wahl A(2), Araya C(3)(4), Colavin 
A(3)(5), Facio FM(2)(6), Metz H(2), Reuter J(2), Frésard L(2), Padigepati SR(2), 
Stafford DA(2), Nussbaum RL(3)(7), Nykamp K(2)(6).

Author information:
(1)Labcorp Genetics Inc, 1400 16th Street, San Francisco, CA, 94103, USA. 
toby.manders@labcorp.com.
(2)Labcorp Genetics Inc, 1400 16th Street, San Francisco, CA, 94103, USA.
(3)Invitae Corporation, 1400 16th Street, San Francisco, CA, 94103, USA.
(4)Tapanti.org, PO Box #727, 836 Anacapa St, Santa Barbara, CA, 93102, USA.
(5)Present Address: Threshold Health Inc, 1638 Myrtle Ave, San Diego, CA, 92103, 
USA.
(6)Present Address: GeneDx, 205/207 Perry Parkway, Gaitherburg, MD, 20877, USA.
(7)Present Address: Division of Medical Genetics, Department of Pediatrics, 
University of California San Francisco, 1825 4th St, San Francisco, CA, 94158, 
USA.

Variants of Uncertain Significance (VUS) in genetic testing for hereditary 
diseases burden patients and clinicians, yet clinical data that could reduce VUS 
are underutilized due to a lack of scalable strategies. We assessed whether a 
machine learning approach using genotype and phenotype data could improve 
variant classification and reduce VUS. In this cohort study of a multi-step 
machine learning approach, patient data from test requisition forms were used to 
distinguish patients with molecular diagnoses from controls ("patient score"). A 
generative Bayesian model then used patient scores and variant classifications 
to infer variant pathogenicity ("variant score"). The study included 3.5 million 
patients referred for clinical genetic testing across various conditions. 
Primary outcomes were model- and gene-level discrimination, classification 
performance, probabilistic calibration, and concordance with orthogonal 
pathogenicity measures. Integration into a semi-quantitative classification 
framework was based on posterior pathogenicity probabilities matching 
PPV ≥ 0.99/NPV ≥ 0.95 thresholds, followed by expert review. We generated 1,334 
clinical variant models (CVMs); 595 showed high performance in both machine 
learning steps (AUROCpatient ≥ 0.8 and AUROCvariant ≥ 0.8) on held-out data. 
High-confidence predictions from these CVMs provided evidence for 5,362 VUS 
observed in 200,174 patients, representing 23.4% of all VUS observations in 
these genes. In 17 frequently tested genes, CVMs reclassified over 1,000 unique 
VUS, reducing VUS report rates by 9-49% per condition. In conclusion, a scalable 
machine learning approach using underutilized clinical data improved variant 
classification and reduced VUS.

© 2025. The Author(s).

DOI: 10.1007/s00439-025-02743-z
PMCID: PMC12170740
PMID: 40266329 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval: This study was 
performed in line with the principles of the Declaration of Helsinki. The use of 
de-identified genetic data was approved under the Western Institutional Review 
Board number CR-001-02 (Tracking ID 20161796). Consent to participate: Not 
applicable. Consent for publication: Not applicable. Competing interests: TM, 
CAT, YK, AW, LF, SP and DS are employees of Labcorp and are affiliated with 
Labcorp Genetics Inc. All authors are former employees of Invitae Corporation.


424. JAMA Netw Open. 2025 Apr 1;8(4):e256359. doi: 10.1001/jamanetworkopen.2025.6359.

Semantic Clinical Artificial Intelligence vs Native Large Language Model 
Performance on the USMLE.

Elkin PL(1)(2), Mehta G(1), LeHouillier F(1)(2), Resnick M(1)(2), Mullin S(3), 
Tomlin C(1), Resendez S(1), Liu J(1), Nebeker JR(2)(4), Brown SH(2)(5).

Author information:
(1)Department of Biomedical Informatics, Jacobs School of Medicine and 
Biomedical Sciences, University at Buffalo, Buffalo, New York.
(2)Clinical Informatics, Department of Veterans Affairs, Washington, DC.
(3)Roswell Park Comprehensive Cancer Center, Buffalo, New York.
(4)Department of Medicine, University of Utah, Salt Lake City.
(5)Department of Biomedical Informatics, Vanderbilt University Medical Center, 
Nashville, Tennessee.

IMPORTANCE: Large language models (LLMs) are being implemented in health care. 
Enhanced accuracy and methods to maintain accuracy over time are needed to 
maximize LLM benefits.
OBJECTIVE: To evaluate whether LLM performance on the US Medical Licensing 
Examination (USMLE) can be improved by including formally represented semantic 
clinical knowledge.
DESIGN, SETTING, AND PARTICIPANTS: This comparative effectiveness research study 
was conducted between June 2024 and February 2025 at the Department of 
Biomedical Informatics, Jacobs School of Medicine and Biomedical Sciences, 
University at Buffalo, Buffalo, New York, using sample questions from the USMLE 
Steps 1, 2, and 3.
INTERVENTION: Semantic clinical artificial intelligence (SCAI) was developed to 
insert formally represented semantic clinical knowledge into LLMs using 
retrieval augmented generation (RAG).
MAIN OUTCOMES AND MEASURES: The SCAI method was evaluated by comparing the 
performance of 3 Llama LLMs (13B, 70B, and 405B; Meta) with and without SCAI RAG 
on text-based questions from the USMLE Steps 1, 2, and 3. LLM accuracy for 
answering questions was determined by comparing the LLM output with the USMLE 
answer key.
RESULTS: The LLMs were tested on 87 questions in the USMLE Step 1, 103 in Step 
2, and 123 in Step 3. The 13B LLM enhanced by SCAI RAG was associated with 
significantly improved performance on Steps 1 and 3 but only met the 60% passing 
threshold on Step 3 (74 questions correct [60.2%]). The 70B and 405B LLMs passed 
all the USMLE steps with and without SCAI RAG. The SCAI RAG 70B model scored 80 
questions (92.0%) correctly on Step 1, 82 (79.6%) on Step 2, and 112 (91.1%) on 
Step 3. The SCAI RAG 405B model scored 79 (90.8%) correctly on Step 1, 87 
(84.5%) on Step 2, and 117 (95.1%) on Step 3. Significant improvements 
associated with SCAI RAG were found for the 13B model on Steps 1 and 3, the 70B 
model on Step 2, and the 405B parameter model on Step 3. The 70B model was 
significantly better than the 13B model, and the 405B model was not 
significantly better than the 70B model.
CONCLUSIONS AND RELEVANCE: In this comparative effectiveness research study, 
SCAI RAG was associated with significantly improved scores on the USMLE Steps 1, 
2, and 3. The 13B model passed Step 3 with RAG, and the 70B and 405B models 
passed and scored well on Steps 1, 2, and 3 with or without augmentation. New 
forms of reasoning by LLMs, like semantic reasoning, have potential to improve 
the accuracy of LLM performance on important medical questions. Improving LLM 
performance in health care with targeted, up-to-date clinical knowledge is an 
important step in LLM implementation and acceptance.

DOI: 10.1001/jamanetworkopen.2025.6359
PMCID: PMC12015668
PMID: 40261653 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of Interest Disclosures: Prof Elkin 
reported receiving grants from the National Institutes of Health (NIH) during 
the conduct of the study. Mr Mehta reported receiving grants from the NIH during 
the conduct of the study. Mr LeHouillier reported receiving grants from the 
National Library of Medicine (NLM) during the conduct of the study. Dr Mullin 
reported receiving a T15 postdoctoral training grant fellowship from the NLM 
during the conduct of the study. No other disclosures were reported.


425. Am J Hum Genet. 2025 May 1;112(5):1158-1172. doi: 10.1016/j.ajhg.2025.03.014. 
Epub 2025 Apr 16.

First steps toward building natural history of diseases computationally: Lessons 
learned from the Noonan syndrome use case.

Groza T(1), Rayabsri W(2), Gration D(2), Hariram H(3), Jamuar SS(4), Baynam 
G(5).

Author information:
(1)Rare Care Centre, Perth Children's Hospital, Nedlands, WA 6009, Australia; 
Bioinformatics Institute, Agency for Science, Technology and Research 
(A(∗)STAR), 30 Biopolis Street #07-01 Matrix, Singapore 138671, Singapore; 
SingHealth Duke-NUS Institute of Precision Medicine, 5 Hospital Drive Level 9, 
Singapore 169609, Singapore; School of Electrical Engineering, Computing and 
Mathematical Sciences, Curtin University, Kent Street, Bentley, WA 6102, 
Australia. Electronic address: tudor.groza@health.wa.gov.au.
(2)Western Australian Register of Developmental Anomalies, King Edward Memorial 
Hospital, 374 Bagot Road, Subiaco, WA 6008, Australia.
(3)Medical Student, Division of Medical Education, School of Medical Sciences, 
Faculty of Biology, Medicine and Health, The University of Manchester, 
Manchester M13 9PL, UK.
(4)SingHealth Duke-NUS Institute of Precision Medicine, 5 Hospital Drive Level 
9, Singapore 169609, Singapore; Genetics Service, Department of Paediatrics, KK 
Women's and Children's Hospital, 100 Bukit Timah Road, Singapore 229899, 
Singapore; SingHealth Duke-NUS Genomic Medicine Centre, 100 Bukit Timah Road, 
Singapore 229899, Singapore.
(5)Rare Care Centre, Perth Children's Hospital, Nedlands, WA 6009, Australia; 
Western Australian Register of Developmental Anomalies, King Edward Memorial 
Hospital, 374 Bagot Road, Subiaco, WA 6008, Australia; Faculty of Health and 
Medical Sciences, University of Western Australia, 35 Stirling Highway, Crawley, 
WA 6009, Australia.

Rare diseases (RDs) are conditions affecting fewer than 1 in 2,000 people, with 
over 7,000 identified, primarily genetic in nature, and more than half impacting 
children. Although each RD affects a small population, collectively, between 
3.5% and 5.9% of the global population, or 262.9-446.2 million people, live with 
an RD. Most RDs lack established treatment protocols, highlighting the need for 
proper care pathways addressing prognosis, diagnosis, and management. Advances 
in generative AI and large language models (LLMs) offer new opportunities to 
document the temporal progression of phenotypic features, addressing gaps in 
current knowledge bases. This study proposes an LLM-based framework to capture 
the natural history of diseases, specifically focusing on Noonan syndrome. The 
framework aims to document phenotypic trajectories, validate against RD 
knowledge bases, and integrate insights into care coordination using electronic 
health record (EHR) data from the Undiagnosed Diseases Program Singapore.

Crown Copyright © 2025. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.ajhg.2025.03.014
PMCID: PMC12120186
PMID: 40245863 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of interests The authors declare no 
competing interests.


426. J Med Internet Res. 2025 Apr 16;27:e70535. doi: 10.2196/70535.

Unveiling the Potential of Large Language Models in Transforming Chronic Disease 
Management: Mixed Methods Systematic Review.

Li C(1), Zhao Y(1), Bai Y(2), Zhao B(3), Tola YO(4), Chan CW(#)(5), Zhang 
M(#)(2), Fu X(#)(1).

Author information:
(1)The Department of Nursing, The Eighth Affiliated Hospital, Sun Yat-sen 
University, Shenzhen, China.
(2)The School of Nursing, Sun Yat-sen University, Guangzhou, China.
(3)The School of Artificial Intelligence, Sun Yat-sen University, Guangzhou, 
China.
(4)The Department of Clinical Research, Conestoga College, Kitchener, ON, 
Canada.
(5)The Nethersole School of Nursing, The Chinese University of Hong Kong, Hong 
Kong, China.
(#)Contributed equally

BACKGROUND: Chronic diseases are a major global health burden, accounting for 
nearly three-quarters of the deaths worldwide. Large language models (LLMs) are 
advanced artificial intelligence systems with transformative potential to 
optimize chronic disease management; however, robust evidence is lacking.
OBJECTIVE: This review aims to synthesize evidence on the feasibility, 
opportunities, and challenges of LLMs across the disease management spectrum, 
from prevention to screening, diagnosis, treatment, and long-term care.
METHODS: Following the PRISMA (Preferred Reporting Items for Systematic Reviews 
and Meta-Analysis) guidelines, 11 databases (Cochrane Central Register of 
Controlled Trials, CINAHL, Embase, IEEE Xplore, MEDLINE via Ovid, ProQuest 
Health & Medicine Collection, ScienceDirect, Scopus, Web of Science Core 
Collection, China National Knowledge Internet, and SinoMed) were searched on 
April 17, 2024. Intervention and simulation studies that examined LLMs in the 
management of chronic diseases were included. The methodological quality of the 
included studies was evaluated using a rating rubric designed for 
simulation-based research and the risk of bias in nonrandomized studies of 
interventions tool for quasi-experimental studies. Narrative analysis with 
descriptive figures was used to synthesize the study findings. Random-effects 
meta-analyses were conducted to assess the pooled effect estimates of the 
feasibility of LLMs in chronic disease management.
RESULTS: A total of 20 studies examined general-purpose (n=17) and 
retrieval-augmented generation-enhanced LLMs (n=3) for the management of chronic 
diseases, including cancer, cardiovascular diseases, and metabolic disorders. 
LLMs demonstrated feasibility across the chronic disease management spectrum by 
generating relevant, comprehensible, and accurate health recommendations (pooled 
accurate rate 71%, 95% CI 0.59-0.83; I2=88.32%) with retrieval-augmented 
generation-enhanced LLMs having higher accuracy rates compared to 
general-purpose LLMs (odds ratio 2.89, 95% CI 1.83-4.58; I2=54.45%). LLMs 
facilitated equitable information access; increased patient awareness regarding 
ailments, preventive measures, and treatment options; and promoted 
self-management behaviors in lifestyle modification and symptom coping. 
Additionally, LLMs facilitate compassionate emotional support, social 
connections, and health care resources to improve the health outcomes of chronic 
diseases. However, LLMs face challenges in addressing privacy, language, and 
cultural issues; undertaking advanced tasks, including diagnosis, medication, 
and comorbidity management; and generating personalized regimens with real-time 
adjustments and multiple modalities.
CONCLUSIONS: LLMs have demonstrated the potential to transform chronic disease 
management at the individual, social, and health care levels; however, their 
direct application in clinical settings is still in its infancy. A multifaceted 
approach that incorporates robust data security, domain-specific model 
fine-tuning, multimodal data integration, and wearables is crucial for the 
evolution of LLMs into invaluable adjuncts for health care professionals to 
transform chronic disease management.
TRIAL REGISTRATION: PROSPERO CRD42024545412; 
https://www.crd.york.ac.uk/PROSPERO/view/CRD42024545412.

©Caixia Li, Yina Zhao, Yang Bai, Baoquan Zhao, Yetunde Oluwafunmilayo Tola, 
Carmen WH Chan, Meifen Zhang, Xia Fu. Originally published in the Journal of 
Medical Internet Research (https://www.jmir.org), 16.04.2025.

DOI: 10.2196/70535
PMCID: PMC12044321
PMID: 40239198 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


427. Pediatr Nephrol. 2025 Sep;40(9):2899-2905. doi: 10.1007/s00467-025-06746-w. Epub 
2025 Apr 16.

Risks and benefits of ChatGPT in informing patients and families with rare 
kidney diseases: an explorative assessment by the European Rare Kidney Disease 
Reference Network (ERKNet).

van Eerde AM(1), Teixeira A(2), Galletti F(3), Maternik M(4), Capone V(5), 
Westland R(6), Mulder J(7)(8), Halbritter J(9), Osterholt T(10), Neukel V(11), 
Weber LT(12)(13), Liebau MC(12)(13)(14), Schaefer F(11), Kohl S(15).

Author information:
(1)Department of Genetics, University Medical Center, Utrecht, The Netherlands.
(2)Division of Pediatric Nephrology, Centro Materno-Infantil Do Norte, Unidade 
Local de Saúde de Santo António, Porto, Portugal.
(3)PKD International, Geneva, Switzerland.
(4)Department of Pediatrics, Nephrology, Hypertension, Medical University of 
Gdansk, Gdansk, Poland.
(5)Pediatric Nephrology, Dialysis and Transplant Unit, Fondazione IRCCS Ca' 
Granda-Ospedale Maggiore Policlinico, Milan, Italy.
(6)Department of Pediatric Nephrology, Emma Children'S Hospital - Amsterdam UMC, 
Location University of Amsterdam, Amsterdam, The Netherlands.
(7)Department of Pediatrics, Division of Pediatric Nephrology, Sophia Children's 
Hospital, Erasmus MC, Rotterdam, The Netherlands.
(8)Department of Pediatrics, Division of Pediatric Nephrology, Willem-Alexander 
Children'S Hospital, Leiden University Medical Center, Leiden, The Netherlands.
(9)Department of Nephrology and Medical Intensive Care, Charité 
Universitätsmedizin Berlin, Berlin, Germany.
(10)Department II of Internal Medicine and Center for Molecular Medicine 
Cologne, Faculty of Medicine, University Hospital of Cologne, University of 
Cologne, Cologne, Germany.
(11)Division of Pediatric Nephrology, Center for Pediatrics and Adolescent 
Medicine, University Hospital Heidelberg, Heidelberg, Germany.
(12)Department of Pediatrics, Faculty of Medicine, University Hospital Cologne 
and University of Cologne, Cologne, Germany.
(13)Center for Rare Diseases, University Hospital Cologne and Medical Faculty, 
University of Cologne, Cologne, Germany.
(14)Center for Molecular Medicine Cologne, Faculty of Medicine, University of 
Cologne, University Hospital Cologne, Cologne, Germany.
(15)Department of Pediatrics, Faculty of Medicine, University Hospital Cologne 
and University of Cologne, Cologne, Germany. stefan.kohl@uk-koeln.de.

BACKGROUND: Rare diseases affect fewer than 1 in 2000 individuals, but 
approximately 150 rare kidney diseases account for about 10% of the chronic 
kidney disease (CKD) population, impacting millions across Europe and globally. 
The scarcity of medical experts for these conditions results in an unmet need 
for accurate and helpful patient information. Large language models like ChatGPT 
may offer a technological solution to assist medical professionals in educating 
patients and improving doctor-patient communication. We hypothesized that 
ChatGPT could provide accurate responses to frequently asked basic questions 
from patients with rare kidney diseases.
METHODS: Medical professionals and members of European Patient Advocacy Groups 
(ePAGs) affiliated with the European Rare Kidney Disease Reference Network 
(ERKNet) simulated patient-ChatGPT interactions using a Microsoft forms 
questionnaire and ChatGPT 3.5 and 4.0. Participants selected any rare kidney 
disease for a structured conversation with ChatGPT 3.5 or 4.0. Responses were 
evaluated for accuracy and helpfulness.
RESULTS: Forty-six ERKNet experts and 12 ePAGs from 13 European countries 
participated in this study. ChatGPT provided scientifically accurate and helpful 
information on 28 randomly selected rare kidney diseases, including prognostic 
information and genetic testing guidance. Participants expressed neutral 
positions regarding ChatGPT's recommendations on alternative treatments, second 
opinions, and other information sources. While ChatGPT generally was perceived 
as helpful and empathetic, concerns about patient safety persisted.
CONCLUSIONS: ChatGPT exhibited substantial potential in addressing patient 
inquiries regarding rare kidney diseases in a real-world context. While it 
demonstrated resilience against misinformation in this application, careful 
human oversight remains essential and indispensable.

© 2025. The Author(s).

DOI: 10.1007/s00467-025-06746-w
PMCID: PMC12296756
PMID: 40237816 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare no competing interests. Disclaimer: No financial support, funding, or 
specific external influences have impacted the design, conduct, interpretation, 
or reporting of this research. The company “Open AI” was not involved in any 
aspect of this work, including data collection, analysis, or manuscript 
preparation.


428. medRxiv [Preprint]. 2025 Apr 3:2024.08.28.24312732. doi: 
10.1101/2024.08.28.24312732.

Integrating a host transcriptomic biomarker with a large language model for 
diagnosis of lower respiratory tract infection.

Van Phan H(1), Spottiswoode N(1), Lydon EC(1), Chu VT(2)(3), Cuesta A(1), 
Kazberouk AD(4), Richmond NL(1), Deosthale P(1), Calfee CS(5), Langelier 
CR(1)(3).

Author information:
(1)Department of Medicine, Division of Infectious Diseases, University of 
California San Francisco.
(2)Department of Pediatrics, Division of Infectious Diseases and Global Health, 
University of California San Francisco.
(3)Chan Zuckerberg Biohub San Francisco.
(4)Department of Medicine, University of California San Francisco.
(5)Department of Medicine, Division of Pulmonary, Critical Care, Allergy and 
Sleep Medicine, University of California San Francisco.

Update in
    Nat Commun. 2025 Dec 16;16(1):10882. doi: 10.1038/s41467-025-66218-5.

BACKGROUND: Lower respiratory tract infections (LRTIs) are a leading cause of 
mortality worldwide and can be difficult to diagnose in critically ill patients, 
as non-infectious causes of respiratory failure can present with similar 
clinical features.
METHODS: We developed a LRTI diagnostic method combining the pulmonary 
transcriptomic biomarker FABP4 with electronic medical record (EMR) text 
assessment using the large language model Generative Pre-trained Transformer 4 
(GPT-4). We evaluated this approach in a prospective cohort of critically ill 
adults with acute respiratory failure from whom tracheal aspirate FABP4 
expression was measured by RNA sequencing. Patients with LRTI or non-infectious 
conditions were identified using retrospective, multi-physician clinical 
adjudication. We then confirmed our findings by applying this method to an 
independent validation cohort of 115 adults with acute respiratory failure.
RESULTS: In the derivation cohort, a combined classifier incorporating FABP4 
expression and GPT-4-assisted EMR analysis achieved an AUC of 0.93 (±0.08) and 
an accuracy of 84%, outperforming FABP4 expression alone (AUC 0.84 ± 0.11) and 
GPT-4-based analysis alone (AUC 0.83 ± 0.07). By comparison, the primary medical 
team's admission diagnosis had an accuracy of 72%. In the validation cohort, the 
combined classifier yielded an AUC of 0.98 (±0.04) and an accuracy of 96%.
CONCLUSIONS: Integrating a host transcriptional biomarker with EMR text analysis 
using a large language model may offer a promising new approach to improving the 
diagnosis of LRTIs in critically ill adults.

DOI: 10.1101/2024.08.28.24312732
PMCID: PMC11998817
PMID: 40236397

Conflict of interest statement: Conflict of Interest Disclosures: No authors 
report conflicts of interest.


429. Discov Oncol. 2025 Apr 15;16(1):529. doi: 10.1007/s12672-025-02314-8.

Multi-objective deep learning for lung cancer detection in CT images: 
enhancements in tumor classification, localization, and diagnostic efficiency.

Abdulqader AF(1), Abdulameer S(2)(3), Bishoyi AK(4), Yadav A(5), Rekha MM(6), 
Kundlas M(7), Kavitha V(8), Aminov Z(9), Abdulali ZS(10), Alwan M(11), Jawad 
M(12), Mushtaq H(13), Farhood B(14).

Author information:
(1)College of Pharmacy, Alnoor University, Nineveh, Iraq.
(2)Department of Automobile Engineering, College of Engineering, Al-Musayab, 
University of Babylon, Babylon, Iraq.
(3)Ahl Al Bayt University, Kerbala, Iraq.
(4)Department of Microbiology, Faculty of Science, Marwadi University Research 
Center, Marwadi University, Rajkot, Gujarat, 360003, India.
(5)Department of Computer Engineering and Application, GLA University, Mathura, 
281406, India.
(6)Department of Chemistry and Biochemistry, School of Sciences, JAIN (Deemed to 
be University), Bangalore, Karnataka, India.
(7)Centre for Research Impact & Outcome, Chitkara University Institute of 
Engineering and Technology, Chitkara University, Rajpura, Punjab, 140401, India.
(8)Department of Chemistry, Sathyabama Institute of Science and Technology, 
Chennai, Tamil Nadu, India.
(9)Department of Public Health and Healthcare Management, Samarkand State 
Medical University, 18 Amir Temur Street, Samarkand, Uzbekistan.
(10)College of Health and Medical Technology, National University of Science and 
Technology, Nasiriyah, Dhi Qar, 64001, Iraq.
(11)Pharmacy College, Al-Farahidi University, Baghdad, Iraq.
(12)Department of Pharmacy, Al-Zahrawi University College, Karbala, Iraq.
(13)Gilgamesh Ahliya University, Baghdad, Iraq.
(14)Department of Medical Physics and Radiology, Faculty of Paramedical 
Sciences, Kashan University of Medical Sciences, Kashan, Iran. 
farhood-b@kaums.ac.ir.

OBJECTIVE: This study aims to develop and evaluate an advanced deep learning 
framework for the detection, classification, and localization of lung tumors in 
computed tomography (CT) scan images.
MATERIALS AND METHODS: The research utilized a dataset of 1608 CT scan images, 
including 623 cancerous and 985 non-cancerous cases, all carefully labeled for 
accurate tumor detection, classification (benign or malignant), and 
localization. The preprocessing involved optimizing window settings, adjusting 
slice thickness, and applying advanced data augmentation techniques to enhance 
the model's robustness and generalizability. The proposed model incorporated 
innovative components such as transformer-based attention layers, adaptive 
anchor-free mechanisms, and an improved feature pyramid network. These features 
enabled the model to efficiently handle detection, classification, and 
localization tasks. The dataset was split into 70% for training, 15% for 
validation, and 15% for testing. A multi-task loss function was used to balance 
the three objectives and optimize the model's performance. Evaluation metrics 
included mean average precision (mAP), intersection over union (IoU), accuracy, 
precision, and recall.
RESULTS: The proposed model demonstrated outstanding performance, achieving a 
mAP of 96.26%, IoU of 95.76%, precision of 98.11%, and recall of 98.83% on the 
test dataset. It outperformed existing models, including You Only Look Once 
(YOLO)v9 and YOLOv10, with YOLOv10 achieving a mAP of 95.23% and YOLOv9 
achieving 95.70%. The proposed model showed faster convergence, better 
stability, and superior detection capabilities, particularly in localizing 
smaller tumors. Its multi-task learning framework significantly improved 
diagnostic accuracy and operational efficiency.
CONCLUSION: The proposed model offers a robust and scalable solution for lung 
cancer detection, providing real-time inference, multi-task learning, and high 
accuracy. It holds significant potential for clinical integration to improve 
diagnostic outcomes and patient care.

© 2025. The Author(s).

DOI: 10.1007/s12672-025-02314-8
PMCID: PMC12000487
PMID: 40232589

Conflict of interest statement: Declarations. Ethics approval: The need for 
ethical approval was waived off by the ethical committee of Alnoor University, 
Mosul, Nineveh, Iraq. Consent to participate: None. Conflict of interest: The 
authors declare that there are no conflicts of interest.


430. Int J Eat Disord. 2025 Jul;58(7):1191-1195. doi: 10.1002/eat.24446. Epub 2025 
Apr 15.

All in the Name of Artificial Intelligence: A Commentary on Linardon (2025).

Burger P(1)(2)(3), Ghosh S(4).

Author information:
(1)GGz Eindhoven, Eindhoven, the Netherlands.
(2)Department of Pediatrics, Emma Children's Hospital, Amsterdam UMC, Amsterdam, 
the Netherlands.
(3)GGz Oost-Brabant, Boekel, the Netherlands.
(4)Department of Mathematics and Computer Science, Technical University of 
Eindhoven, Eindhoven, the Netherlands.

Comment in
    10.1002/eat.24394.

Artificial Intelligence (AI) is being rapidly integrated into healthcare, but 
Linardon et al. reveal a troubling gap between what AI actually is, its 
capabilities, and the patients' and clinicians' perceptions of it-equating AI 
solely with large language models. In this commentary, we discuss concerns over 
AI's black-box nature, its potential to perpetuate existing biases, and the 
blind trust some people place in its decisions, despite evidence that 
quantitative models outperform large language models in clinical decision-making 
tasks. While AI holds promise in eating disorder care, its integration requires 
a nuanced understanding of its capabilities, limitations, and the critical 
distinction between AI for administrative automation, clinical decision-making, 
and direct-to-patient AI. Poorly designed AI alerts risk becoming just another 
ignorable nuisance, while patient-facing AI could either empower individuals or 
drown them in notifications and misinformation. Before we anoint AI as 
healthcare's savior, it requires validation for accuracy, reliability, fairness, 
real-world usability, and its actual measurable impact on clinicians and 
patients. The real challenge is not whether AI will change healthcare but 
ensuring it does so responsibly-by integrating it thoughtfully into workflows, 
such that it is supporting rather than replacing clinical judgment, and 
maintaining accountability when things go wrong.

© 2025 The Author(s). International Journal of Eating Disorders published by 
Wiley Periodicals LLC.

DOI: 10.1002/eat.24446
PMCID: PMC12232355
PMID: 40231477 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


431. Medicine (Baltimore). 2025 Apr 11;104(15):e41593. doi: 
10.1097/MD.0000000000041593.

Public knowledge of food poisoning, risk perception and food safety practices in 
Saudi Arabia: A cross-sectional survey following foodborne botulism outbreak.

Alhuzaimi A(1)(2), Aljamaan F(1)(3)(4), Al-Ajwad FH(1)(5), Alabdulkareem 
W(1)(5)(6), Alshahrani FS(1)(7)(8), Altamimi I(1), Al-Eyadhy L(1), Bukhari A(9), 
BinOmair A(1)(5), Al-Subaie S(1)(5), Shayah Y(10), Alhaboob A(1)(5), Alanteet 
AA(11), Alanteet A(1)(5), Alharbi M(1), Jamal A(1)(12)(13), Barry M(1)(7)(14), 
Assiri RA(15), Alhasan K(1)(5)(16), Al-Tawfiq JA(17)(18)(19), Temsah 
MH(1)(5)(13).

Author information:
(1)College of Medicine, King Saud University, Riyadh, Saudi Arabia.
(2)Pediatric Cardiology Department, Cardiac Science Department, King Saud 
University Medical City, King Saud University, Riyadh, Saudi Arabia.
(3)Critical Care Department, College of Medicine, King Saud University Medical 
City, King Saud University, Riyadh, Saudi Arabia.
(4)Department of Medicine, Al Rayan Hospital, Dr. Sulaiman AL Habib Medical 
Group, Riyadh, Saudi Arabia.
(5)Pediatric Department, King Saud University Medical City, Riyadh, Saudi 
Arabia.
(6)Clinical Nutrition Department, King Saud University and King Saud University 
Medical City, Riyadh, Saudi Arabia.
(7)Department of Internal Medicine, Division of Infectious Diseases, College of 
Medicine, King Saud University and King Saud University Medical City, Riyadh, 
Saudi Arabia.
(8)Infection Prevention and Control Department, King Saud University Medical 
City, Riyadh, Saudi Arabia.
(9)College of Medicine, King Abdulaziz University, Rabigh, Saudi Arabia.
(10)College of Medicine, Alfaisal University, Riyadh, Saudi Arabia.
(11)Department of Pharmacology and Toxicology, College of Pharmacy, King Saud 
University, Riyadh, Saudi Arabia.
(12)Department of Family and Community Medicine, King Saud University Medical 
City, Riyadh, Saudi Arabia.
(13)Family and Community Medicine Department, Evidence-Based Health Care and 
Knowledge Translation Research Chair, College of Medicine, King Saud University, 
Riyadh, Saudi Arabia.
(14)Division of Infectious Diseases, Faculty of Medicine, University of Ottawa, 
Ottawa, Canada.
(15)Department of Basic Medical Sciences, College of Medicine, Princess Nourah 
bint Abdulrahman University, Riyadh, Saudi Arabia.
(16)Kidney & Pancreas Health Center, Organ Transplant Center of Excellence, King 
Faisal Specialist Hospital and Research Centre, Riyadh, Saudi Arabia.
(17)Specialty Internal Medicine and Quality Department, Johns Hopkins Aramco 
Healthcare, Dhahran, Saudi Arabia.
(18)Department of Medicine, Infectious Disease Division, Indiana University 
School of Medicine, Indianapolis, IN.
(19)Department of Medicine, Infectious Disease Division, Johns Hopkins 
University School of Medicine, Baltimore, MD.

To investigate food poisoning knowledge, risk perception and safe food handling 
practices among Saudi Arabian public following foodborne botulism outbreak. A 
cross-sectional survey targeting the Saudi Arabian public between May 6 to 20, 
2024, following the first foodborne botulism outbreak. Infectious disease and 
public health experts developed survey questions according to Saudi Public 
Health Authority and Ministry of Health (MOH) guidelines, and distributed 
surveys through social media. Of 3779 participants, 73.1% were female and 50.1% 
were aged 18 to 24 years. Almost one-third (30.2%) reported a previous food 
poisoning experience, with an incidence of 71.7 cases per 1000 person years. The 
most common perceived source of FP was restaurants foods (80.3%). The overall 
knowledge score of the participants regarding food poisoning was 3.42 ± 1.57 out 
of 7. The mean food safety practice score was 3.70 ± 1.42 out of 9. 
Multivariable regression analysis showed individuals aged 35 years or older 
(β = 0.205, P < .001), those who were married (β = 0.204, P = .003), 
participants with previous (FP) experience (β = 0.089, P = .009), and those who 
relied on information from the Ministry of Health or medical publications 
regarding FP (P < .001) exhibited significantly higher practice scores than 
other groups. The least adherence to safe practices were noted among the 
following: routine use of thermometer during cooking (2.7%), avoidance of 
washing raw chicken (13.7%) and washing hands after using cellphone during 
cooking (26.1%). The FP knowledge score did not correlate significantly with 
practice score (P = .065). This study highlights the significant knowledge gaps 
and inadequate food safety practices among the public in Saudi Arabia. Although 
certain groups, including adults (>35 years), married individuals, and those 
with previous food poisoning experience, showed greater adherence to safe food 
handling practices, adherence to specific preventive measures remained generally 
low. These findings highlight the need for targeted educational initiatives and 
interventions to improve food safety awareness and practices across diverse 
demographic groups in Saudi Arabia. The integration of generative AI tools, such 
as ChatGPT, as a public resource for food poisoning information, presents a new 
opportunity, but it requires further research and development to ensure accuracy 
and reliability.

Copyright © 2025 the Author(s). Published by Wolters Kluwer Health, Inc.

DOI: 10.1097/MD.0000000000041593
PMCID: PMC11999408
PMID: 40228247 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no funding and conflicts of 
interest to disclose.


432. Laryngoscope. 2025 Oct;135(10):3570-3578. doi: 10.1002/lary.32188. Epub 2025 Apr 
14.

Generative AI in Otolaryngology Residency Personal Statement Writing: A 
Mixed-Methods Analysis.

Wihlidal JGJ(1), Wolter NE(1)(2), Propst EJ(1)(2), Lin V(1)(3), Au M(4), Amin 
S(5), Siu JM(1)(2).

Author information:
(1)Department of Otolaryngology-Head and Neck Surgery, Temerty Faculty of 
Medicine, University of Toronto, Toronto, Ontario, Canada.
(2)Department of Otolaryngology-Head and Neck Surgery, Hospital for Sick 
Children, Toronto, Ontario, Canada.
(3)Department of Otolaryngology-Head and Neck Surgery, Sunnybrook Hospital, 
Toronto, Ontario, Canada.
(4)Department of Otolaryngology, McMaster Health Sciences, Hamilton, Ontario, 
Canada.
(5)Department of Otolaryngology, University of Washington, Seattle, Washington, 
USA.

OBJECTIVE: Generative Artificial Intelligence (GAI) interfaces have rapidly 
integrated into various societal domains. Widespread accessibility of GAI for 
drafting personal statements poses challenges for evaluators to gauge writing 
ability and personal insight. This study aims to compare the quality of 
GAI-generated personal statements to those written by successful applicants in 
OHNS residency programs, via integration of statistical and qualitative thematic 
analyses.
METHODS: Personal statements were collected from successful OHNS residency 
applicants. Characteristic extraction from submitted statements was used to 
generate GAI-written personal statements using ChatGPT 4.0. All statements were 
blindly reviewed by 21 experienced evaluators on a 10-point Likert scale of 
authenticity, readability, personability, and overall quality. Thematic analysis 
of qualitative reviewer comments was conducted to extract deeper insights into 
evaluators' perceptions. Quantitative results were compared using independent 
t-tests, while thematic coding was performed inductively using NVivo software.
RESULTS: GAI-generated personal statements significantly outperformed 
applicant-written statements in all assessed domains, including authenticity 
(7.67 vs. 7.05, p = 0.002), readability (8.03 vs. 7.49, p = 0.002), 
personability (7.33 vs. 6.72, p = 0.004), and overall score (7.49 vs. 6.90, 
p = 0.005). Thematic analysis revealed that GAI statements were seen as 
"well-constructed but generic," while applicant statements were often "verbose 
and lacked focus." Additionally, reviewers noted concerns regarding personal 
insight and engagement in AI-generated statements.
CONCLUSION: GAI-generated personal statements were rated more favorably across 
all domains, raising critical questions about the future of personal statements 
in the residency application process. While AI in medical education continues to 
evolve, clear guidelines on its ethical use in residency applications are 
essential.
LEVEL OF EVIDENCE: N/A.

© 2025 The Author(s). The Laryngoscope published by Wiley Periodicals LLC on 
behalf of The American Laryngological, Rhinological and Otological Society, Inc.

DOI: 10.1002/lary.32188
PMCID: PMC12475545
PMID: 40227955 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


433. Antimicrob Steward Healthc Epidemiol. 2025 Mar 31;5(1):e90. doi: 
10.1017/ash.2025.47. eCollection 2025.

Can we rely on artificial intelligence to guide antimicrobial therapy? A 
systematic literature review.

AlGain S(1)(2), Marra AR(3)(4), Kobayashi T(4)(5), Marra PS(6), Celeghini PD(3), 
Hsieh MK(4), Shatari MA(7), Althagafi S(8), Alayed M(1), Ranavaya JI(5), Boodhoo 
NA(9), Meade NO(5), Fu D(10), Sampson MM(2), Rodriguez-Nava G(2), Zimmet AN(2), 
Ha D(2), Alsuhaibani M(1), Huddleston BS(11), Salinas JL(2).

Author information:
(1)King Faisal Specialist Hospital and Research Center, Riyadh, Saudi Arabia.
(2)Division of Infectious Diseases & Geographic Medicine, Stanford University, 
Stanford, CA, USA.
(3)Hospital Israelita Albert Einstein, São Paulo, SP, Brazil.
(4)University of Iowa Hospitals and Clinics, Iowa City, IA, USA.
(5)Department of Internal Medicine, University of Kentucky, Lexington, KY, USA.
(6)School of Medicine, University of California, San Francisco, San Francisco, 
CA, USA.
(7)King Saud Medical City, Riyadh, Saudi Arabia.
(8)Pediatric Infectious Diseases, King Abdullah Specialized Children's Hospital, 
MNGHA, Jeddah, Saudi Arabia.
(9)Department of Epidemiology, University of Iowa College of Public Health, Iowa 
City, IA, USA.
(10)Pritzker School of Medicine, University of Chicago, Chicago, IL, USA.
(11)Lane Medical Library, Stanford University School of Medicine, Palo Alto, CA, 
USA.

BACKGROUND: Artificial intelligence (AI) has the potential to enhance clinical 
decision-making, including in infectious diseases. By improving antimicrobial 
resistance prediction and optimizing antibiotic prescriptions, these 
technologies may support treatment strategies and address critical gaps in 
healthcare. This study evaluates the effectiveness of AI in guiding appropriate 
antibiotic prescriptions for infectious diseases through a systematic literature 
review.
METHODS: We conducted a systematic review of studies evaluating AI (machine 
learning or large language models) used for guidance on prescribing appropriate 
antibiotics in infectious disease cases. Searches were performed in PubMed, 
CINAHL, Embase, Scopus, Web of Science, and Google Scholar for articles 
published up to October 25, 2024. Inclusion criteria focused on studies 
assessing the performance of AI in clinical practice, with outcomes related to 
antimicrobial management and decision-making.
RESULTS: Seventeen studies used machine learning as part of clinical decision 
support systems (CDSS). They improved prediction of antimicrobial resistance and 
optimized antimicrobial use. Six studies focused on large language models to 
guide antimicrobial therapy; they had higher prescribing error rates, patient 
safety risks, and needed precise prompts to ensure accurate responses.
CONCLUSIONS: AI, particularly machine learning integrated into CDSS, holds 
promise in enhancing clinical decision-making and improving antimicrobial 
management. However, large language models currently lack the reliability 
required for complex clinical applications. The indispensable role of infectious 
disease specialists remains critical for ensuring accurate, personalized, and 
safe treatment strategies. Rigorous validation and regular updates are essential 
before the successful integration of AI into clinical practice.

© The Author(s) 2025.

DOI: 10.1017/ash.2025.47
PMCID: PMC11986881
PMID: 40226293

Conflict of interest statement: Authors have no commercial or financial 
involvement with this manuscript.


434. Med Phys. 2025 Jul;52(7):e17828. doi: 10.1002/mp.17828. Epub 2025 Apr 11.

Generative evidential synthesis with integrated segmentation framework for 
MR-only radiation therapy treatment planning.

Mekki L(1), Ladra M(2), Acharya S(2), Lee J(2).

Author information:
(1)Department of Biomedical Engineering, Johns Hopkins University, Baltimore, 
Maryland, USA.
(2)Department of Radiation Oncology and Molecular Radiation Sciences, Johns 
Hopkins University, Baltimore, Maryland, USA.

BACKGROUND: Radiation therapy (RT) planning is a time-consuming process 
involving the contouring of target volumes and organs at risk, followed by 
treatment plan optimization. CT is typically used as the primary planning image 
modality as it provides electron density information needed for dose 
calculation. MRI is widely used for contouring after registration to CT due to 
its high soft tissue contrast. However, there exists uncertainties in 
registration, which propagate throughout treatment planning as contouring 
errors, and lead to dose inaccuracies. MR-only RT planning has been proposed as 
a solution to eliminate the need for CT scan and image registration, by 
synthesizing CT from MRI. A challenge in deploying MR-only planning in clinic is 
the lack of a method to estimate the reliability of a synthetic CT in the 
absence of ground truth. While methods have used sampling-based approaches to 
estimate model uncertainty over multiple inferences, such methods suffer from 
long run time and are therefore inconvenient for clinical use.
PURPOSE: To develop a fast and robust method for the joint synthesis of CT from 
MRI, estimation of model uncertainty related to the synthesis accuracy, and 
segmentation of organs at risk (OARs), in a single model inference.
METHODS: In this work, deep evidential regression is applied to MR-only brain RT 
planning. The proposed framework uses a multi-task vision transformer combining 
a single joint nested encoder with two distinct convolutional decoder paths for 
synthesis and segmentation separately. An evidential layer was added at the end 
of the synthesis decoder to jointly estimate model uncertainty in a single 
inference. The framework was trained and tested on a dataset of 119 (80 for 
training, 9 for validation, and 30 for test) paired T1-weighted MRI and CT scans 
with OARs contours.
RESULTS: The proposed method achieved mean ± SD SSIM of 0.820 ± 0.039, MAE of 
47.4 ± 8.49 HU, and PSNR of 23.4 ± 1.13 for the synthesis task and dice 
similarity coefficient of 0.799 ± 0.132 (lenses), 0.945 ± 0.020 (eyes), 
0.834 ± 0.059 (optic nerves), 0.679 ± 0.148 (chiasm), 0.947 ± 0.014 (temporal 
lobes), 0.849 ± 0.027 (hippocampus), 0.953 ± 0.024 (brainstem), 0.752 ± 0.228 
(cochleae) for segmentation-in a total run time of 6.71 ± 0.25 s. Additionally, 
experiments on challenging test cases revealed that the proposed evidential 
uncertainty estimation highlighted the same uncertain regions as Monte 
Carlo-based epistemic uncertainty, thus highlighting the reliability of the 
proposed method.
CONCLUSION: A framework leveraging deep evidential regression to jointly 
synthesize CT from MRI, predict the related synthesis uncertainty, and segment 
OARs in a single model inference was developed. The proposed approach has the 
potential to streamline the planning process and provide clinicians with a 
measure of the reliability of a synthetic CT in the absence of ground truth.

© 2025 American Association of Physicists in Medicine.

DOI: 10.1002/mp.17828
PMID: 40219601 [Indexed for MEDLINE]


435. BMC Infect Dis. 2025 Apr 12;25(1):513. doi: 10.1186/s12879-025-10817-6.

Remdesivir associated with reduced mortality in hospitalized COVID-19 patients: 
treatment effectiveness using real-world data and natural language processing.

Arribas López JR(1), Ruiz Seco MP(2), Fanjul F(3), Díaz Pollán B(4), González 
Ruano Pérez P(2), Ferre Beltrán A(3), De Miguel Buckley R(4), Portillo Horcajada 
L(5), De Álvaro Pérez C(6), Barroso Santos Carvalho PJ(6); Savana Research 
Group; Riera Jaume M(3).

Collaborators: Valdivieso JL, Fanjul V, Sarró E, Cabal-Hierro L, Menke S, 
Casadevall D, Marín-Corral J, Polo N, Taberna M.

Author information:
(1)Infectious Diseases Unit, Internal Medicine Service, Hospital Universitario 
La Paz, Hospital La Paz Institute for Health Research (IdiPAZ), Ciber 
Enfermedades Infecciosas (CIBERINFEC), Madrid, Spain. 
joser.arribas@salud.madrid.org.
(2)Internal Medicine Service, Hospital Universitario Infanta Sofía, Madrid, 
Spain.
(3)Infectious Diseases Unit, Internal Medicine Service, Hospital Universitario 
Son Espases, Fundació Institut de Investigació Sanitaria de Les Illes Balears 
(IdISBa), Palma de Mallorca, Spain.
(4)Infectious Diseases Unit, Internal Medicine Service, Hospital Universitario 
La Paz, Hospital La Paz Institute for Health Research (IdiPAZ), Ciber 
Enfermedades Infecciosas (CIBERINFEC), Madrid, Spain.
(5)Hospital Pharmacy, Hospital Universitario Infanta Sofía, Madrid, Spain.
(6)Gilead Sciences, Inc, Madrid, Spain.

BACKGROUND: Remdesivir (RDV) was the first antiviral approved for 
mild-to-moderate COVID-19 and for those patients at risk for progression to 
severe disease after clinical trials supported its association with improved 
outcomes. Real-world evidence (RWE) generated by artificial intelligence 
techniques could potentially expedite the validation of new treatments in future 
health crises. We aimed to use natural language processing (NLP) and machine 
learning (ML) to assess the impact of RDV on COVID19-associated outcomes 
including time to discharge and in-hospital mortality.
METHODS: Using EHRead®, an NLP technology including SNOMED-CT terminology that 
extracts unstructured clinical information from electronic health records (EHR), 
we retrospectively examined hospitalized COVID-19 patients with 
moderate-to-severe pneumonia in three Spanish hospitals between January 2021 and 
March 2022. Among RDV eligible patients, treated (RDV+) vs untreated (RDV‒) 
patients were compared after propensity score matching (PSM; 1:3.3 ratio) based 
on age, sex, Charlson comorbidity index, COVID-19 vaccination status, other 
COVID-19 treatment, hospital, and variant period. Cox proportional hazards 
models and Kaplan-Meier plots were used to assess statistical differences 
between groups.
RESULTS: Among 7,651,773 EHRs from 84,408 patients, 6,756 patients were detected 
with moderate-to-severe COVID-19 pneumonia during the study period. The study 
population was defined with 4,882 (72.3%) RDV eligible patients. The median age 
was 72 years and 57.3% were male. A total of 812 (16.6%) patients were 
classified as RDV+ and were matched to 2,703 RDV‒ patients (from a total of 
4,070 RDV‒). After PSM, all covariates had an absolute mean standardized 
difference of less than 10%. The hazard ratio for in-hospital mortality at 28 
days was 0.73 (95% confidence interval, CI, 0.56 to 0.96, p = 0.022) with RDV‒ 
as the reference group. Risk difference and risk ratio at 28 days was 2.7% and 
0.76, respectively, both favoring the RDV+ group. No differences were found in 
length of hospital stay since RDV eligibility between groups.
CONCLUSIONS: Using NLP and ML we were able to generate RWE on the effectiveness 
of RDV in COVID-19 patients, confirming the potential of using this methodology 
to measure the effectiveness of treatments in pandemics. Our results show that 
using RDV in hospitalized patients with moderate-to-severe pneumonia is 
associated with significantly reduced inpatient mortality. Adherence to clinical 
guideline recommendations has prognostic implications and emerging technologies 
in identifying eligible patients for treatment and avoiding missed opportunities 
during public health crises are needed.

© 2025. The Author(s).

DOI: 10.1186/s12879-025-10817-6
PMCID: PMC11992806
PMID: 40217145 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: This study was classified as a ‘non-interventional 
post-authorization study’ by the Spanish Agency of Medicines and Health Products 
and was reviewed and approved by the Drug Research Ethics Committee of the 
Balearic Islands (IB 4731/21 EOm) as the reference Ethics Committee. All methods 
and analysis followed legal and regulatory requirements and generally accepted 
research practices described in the latest edition of the Declaration of 
Helsinki, Good Pharmacoepidemiology Practices, and applicable local regulations. 
Informed consent was waived by the Drug Research Ethics Committee of the 
Balearic Islands, as data were retrospectively analyzed from patient EHRs, 
anonymized, and aggregated in an irreversibly dissociated manner. Data 
collection and assessment were obtained as part of routine clinical activity and 
performed in a blinded manner. Consent for publication: Not applicable. 
Competing interests: JRA reports personal fees from Viiv, Janssen, Gilead, MSD, 
and Aelix outside the submitted work. MPRS, LPH received personal fees from 
Gilead for patient review. RDMB reports personal fees (speaker fee and congress 
attendance grants) from Viiv, and Gilead. CDAP and PJBSC are employees of Gilead 
Sciences. FFL, BDP, PGRP, AFB, and MRJ declare no competing interests.


436. BMC Psychiatry. 2025 Apr 10;25(1):359. doi: 10.1186/s12888-025-06798-0.

Evaluating the agreement between ChatGPT-4 and validated questionnaires in 
screening for anxiety and depression in college students: a cross-sectional 
study.

Liu J(1), Gu J(1), Tong M(1), Yue Y(1), Qiu Y(1), Zeng L(1), Yu Y(1), Yang 
F(2)(3)(4), Zhao S(5).

Author information:
(1)School of Nursing, Hubei University of Chinese Medicine, No. 16 West 
Huangjiahu Road, Hongshan District, Wuhan, 430065, China.
(2)School of Nursing, Hubei University of Chinese Medicine, No. 16 West 
Huangjiahu Road, Hongshan District, Wuhan, 430065, China. fenyang@hbucm.edu.cn.
(3)Hubei Shizhen Laboratory, Wuhan, China. fenyang@hbucm.edu.cn.
(4)Nursing Department, Hubei Provincial Hospital of Traditional Chinese 
Medicine, No. 856 Luoyu Road, Hongshan District, Wuhan, Hubei, China. 
fenyang@hbucm.edu.cn.
(5)Nursing Department, Hubei Provincial Hospital of Traditional Chinese 
Medicine, No. 856 Luoyu Road, Hongshan District, Wuhan, Hubei, China. 
814270350@qq.com.

BACKGROUND: The Chat Generative Pre-trained Transformer (ChatGPT), an artificial 
intelligence-based web application, has demonstrated substantial potential 
across various knowledge domains, particularly in medicine. This cross-sectional 
study assessed the validity and possible usefulness of the ChatGPT-4 in 
assessing anxiety and depression by comparing two questionnaires.
METHODS: This study tasked ChatGPT-4 with generating a structured interview 
questionnaire based on the validated Patient Health Questionnaire-9 (PHQ-9) and 
Generalized Anxiety Disorder Scale-7 (GAD-7). These new measures were referred 
to as GPT-PHQ-9 and GPT-GAD-7. This study utilized Spearman correlation 
analysis, Intra-class correlation coefficients (ICC), Youden's index, receiver 
operating characteristic (ROC) and Bland-Altman plots to evaluate the 
consistency between scores from a ChatGPT-4 adapted questionnaire and those from 
a validated questionnaire.
RESULTS: A total of 200 college students participated. Cronbach's α indicated 
acceptable reliability for both GPT-PHQ-9 (α = 0.75) and GPT-GAD-7 (α = 0.76). 
ICC values were 0.80 for PHQ-9 and 0.70 for GAD-7. Spearman's correlation showed 
moderate associations with PHQ-9 (p = 0.63) and GAD-7 (p = 0.68). ROC curve 
analysis revealed optimal cutoffs of 9.5 for depressive symptoms and 6.5 for 
anxiety symptoms, both with high sensitivity and specificity.
CONCLUSIONS: The questionnaire adapted by ChatGPT-4 demonstrated good 
consistency with the validated questionnaire. Future studies should investigate 
the usefulness of the ChatGPT designed questionnaire in different populations.

© 2025. The Author(s).

DOI: 10.1186/s12888-025-06798-0
PMCID: PMC11983836
PMID: 40211256 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: Our study did not require further ethics committee approval as it 
did not involve animal or human clinical trials and was not unethical. In 
accordance with the ethical principles outlined in the Declaration of Helsinki, 
all participants provided informed consent before participating in the study. 
The anonymity and confidentiality of the participants were guaranteed, and 
participation was completely voluntary. Consent for publication: Not Applicable. 
Competing interests: The authors declare no competing interests.


437. Eur J Clin Pharmacol. 2025 Jun;81(6):875-883. doi: 10.1007/s00228-025-03838-2. 
Epub 2025 Apr 9.

Generative AI vs. human expertise: a comparative analysis of case-based rational 
pharmacotherapy question generation.

Güvel MC(1), Kıyak YS(2), Varan HD(3), Sezenöz B(4), Coşkun Ö(2), Uluoğlu C(5).

Author information:
(1)Department Medical Pharmacology, Gazi University Faculty of Medicine, Ankara, 
Turkey.
(2)Department of Medical Education and Informatics, Gazi University Faculty of 
Medicine, Ankara, Turkey.
(3)Department of Internal Medicine, Gazi University Faculty of Medicine, Ankara, 
Turkey.
(4)Department of Cardiology, Gazi University Faculty of Medicine, Ankara, 
Turkey.
(5)Department Medical Pharmacology, Gazi University Faculty of Medicine, Ankara, 
Turkey. culuoglu@yahoo.com.

PURPOSE: This study evaluated the performance of three generative AI 
models-ChatGPT- 4o, Gemini 1.5 Advanced Pro, and Claude 3.5 Sonnet-in producing 
case-based rational pharmacology questions compared to expert educators.
METHODS: Using one-shot prompting, 60 questions (20 per model) addressing 
essential hypertension and type 2 diabetes subjects were generated. A 
multidisciplinary panel categorized questions by usability (no revisions needed, 
minor or major revisions required, or unusable). Subsequently, 24 AI-generated 
and 8 expert-created questions were asked to 103 medical students in a 
real-world exam setting. Performance metrics, including correct response rate, 
discrimination index, and identification of nonfunctional distractors, were 
analyzed.
RESULTS: No statistically significant differences were found between 
AI-generated and expert-created questions, with mean correct response rates 
surpassing 50% and discrimination indices consistently equal to or above 0.20. 
Claude produced the highest proportion of error-free items (12/20), whereas 
ChatGPT exhibited the fewest unusable items (5/20). Expert revisions required 
approximately one minute per AI-generated question, representing a substantial 
efficiency gain over manual question preperation. Nonetheless, 19 out of 60 
AI-generated questions were deemed unusable, highlighting the necessity of 
expert oversight.
CONCLUSION: Large language models can profoundly accelerate the development of 
high-quality assessment questions in medical education. However, expert review 
remains critical to address lapses in reliability and validity. A hybrid model, 
integrating AI-driven efficiencies with rigorous expert validation, may offer an 
optimal approach for enhancing educational outcomes.

© 2025. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, 
part of Springer Nature.

DOI: 10.1007/s00228-025-03838-2
PMID: 40205076 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval: The study was 
performed in accordance with the ethical standards as laid down in the 2013 
Declaration of Helsinki. This study has been approved by Gazi University Ethics 
Commission (research number 2024–1707). Consent to participate: Informed consent 
was obtained from the data owners. Competing interests: The authors declare no 
competing interests.


438. J Am Coll Radiol. 2025 Aug;22(8):917-923. doi: 10.1016/j.jacr.2025.04.007. Epub 
2025 Apr 30.

Large Language Models for Global Health Clinics: Opportunities and Challenges.

Tripathi S(1), Alkhulaifat D(2), Muppuri M(3), Elahi A(4), Dako F(5).

Author information:
(1)Center for Global and Population Health Research in Radiology, Department of 
Radiology, Perelman School of Medicine at University of Pennsylvania, 
Philadelphia, Pennsylvania; Athinoula A. Martinos Center for Biomedical Imaging, 
Department of Radiology, Massachusetts General Hospital, Harvard Medical School, 
Boston, Massachusetts.
(2)Department of Radiology, Children's Hospital of Philadelphia, Philadelphia, 
Pennsylvania.
(3)Athinoula A. Martinos Center for Biomedical Imaging, Department of Radiology, 
Massachusetts General Hospital, Harvard Medical School, Boston, Massachusetts.
(4)Department of Information Services, University of Pennsylvania, Philadelphia, 
Pennsylvania; IS Application Manager, Penn Medicine.
(5)Director, Center for Global and Population Health Research in Radiology, 
Department of Radiology, Perelman School of Medicine at University of 
Pennsylvania, Philadelphia, Pennsylvania. Electronic address: 
farouk.dako@pennmedicine.upenn.edu.

Large language models (LLMs) have emerged as a new wave of artificial 
intelligence, and their applications could emerge as a pivotal resource capable 
of reshaping health care communication, research, and informed decision-making 
processes. These models offer unprecedented potential to swiftly disseminate 
critical health information and transcend linguistic barriers. However, their 
integration into health care systems presents formidable challenges, including 
inherent biases in training data, privacy vulnerabilities, and disparities in 
digital literacy. Despite these obstacles, LLMs possess unparalleled analytic 
prowess to inform evidence-based health care policies and clinical practices. 
Addressing these challenges necessitates the formulation of robust ethical 
frameworks, bias mitigation strategies, and educational initiatives to ensure 
equitable access to health care resources globally. By navigating these 
complexities with meticulous attention and foresight, LLMs stand poised to 
catalyze substantial advancements in global health outcomes, promoting health 
equity and improving population health worldwide.

Copyright © 2025. Published by Elsevier Inc.

DOI: 10.1016/j.jacr.2025.04.007
PMID: 40204164 [Indexed for MEDLINE]


439. BMC Nurs. 2025 Apr 7;24(1):386. doi: 10.1186/s12912-025-03044-6.

Neonatal nurses' experiences with generative AI in clinical decision-making: a 
qualitative exploration in high-risk nicus.

Alruwaili AN(1), Alshammari AM(2), Alhaiti A(3), Elsharkawy NB(2), Ali SI(4), 
Elsayed Ramadan OM(5).

Author information:
(1)College of Nursing, Nursing Administration and Education Department, Jouf 
University, Sakaka, 72388, Saudi Arabia. analrwili@ju.edu.sa.
(2)College of Nursing, Department of Maternity and Pediatric Health Nursing, 
Jouf University, Sakaka, 72388, Saudi Arabia.
(3)Department of Nursing, College of Applied Sciences, Almaarefa University, 
Diriyah, Riyadh, 13713, Saudi Arabia.
(4)College of Medicine, Department of Family and Community Medicine, King Faisal 
University, Alhssa, 31982, Saudi Arabia.
(5)Pediatric Nursing Department, Faculty of Nursing, Cairo University, Cairo, 
11562, Egypt. osama_ramadan_85@cu.edu.eg.

BACKGROUND: Neonatal nurses in high-risk Neonatal Intensive Care Units (NICUs) 
navigate complex, time-sensitive clinical decisions where accuracy and judgment 
are critical. Generative artificial intelligence (AI) has emerged as a 
supportive tool, yet its integration raises concerns about its impact on nurses' 
decision-making, professional autonomy, and organizational workflows.
AIM: This study explored how neonatal nurses experience and integrate generative 
AI in clinical decision-making, examining its influence on nursing practice, 
organizational dynamics, and cultural adaptation in Saudi Arabian NICUs.
METHODS: An interpretive phenomenological approach, guided by Complexity 
Science, Normalization Process Theory, and Tanner's Clinical Judgment Model, was 
employed. A purposive sample of 33 neonatal nurses participated in 
semi-structured interviews and focus groups. Thematic analysis was used to code 
and interpret data, supported by an inter-rater reliability of 0.88. Simple 
frequency counts were included to illustrate the prevalence of themes but were 
not used as quantitative measures. Trustworthiness was ensured through reflexive 
journaling, peer debriefing, and member checking.
RESULTS: Five themes emerged: (1) Clinical Decision-Making, where 93.9% of 
nurses reported that AI-enhanced judgment but required human validation; (2) 
Professional Practice Transformation, with 84.8% noting evolving role boundaries 
and workflow changes; (3) Organizational Factors, as 97.0% emphasized the 
necessity of infrastructure, training, and policy integration; (4) Cultural 
Influences, with 87.9% highlighting AI's alignment with family-centered care; 
and (5) Implementation Challenges, where 90.9% identified technical barriers and 
adaptation strategies.
CONCLUSIONS: Generative AI can support neonatal nurses in clinical 
decision-making, but its effectiveness depends on structured training, reliable 
infrastructure, and culturally sensitive implementation. These findings provide 
evidence-based insights for policymakers and healthcare leaders to ensure AI 
integration enhances nursing expertise while maintaining safe, patient-centered 
care.

© 2025. The Author(s).

DOI: 10.1186/s12912-025-03044-6
PMCID: PMC11977934
PMID: 40197527

Conflict of interest statement: Declarations. Institutional review board 
statement: The present study was conducted in accordance with institutional 
ethical standards and received approval from the Standing Committee of Bioethics 
Research, King Faisal University Ethics Committee (Protocol approval number: 
KFU-2024-ETHICS2926). Informed consent: Informed consent was obtained from all 
participants involved in the study. Consent for publication: Not applicable. 
Clinical trial number: Not applicable. Competing interests: The authors declare 
no competing interests.


440. J Dent. 2025 Jul;158:105733. doi: 10.1016/j.jdent.2025.105733. Epub 2025 Apr 5.

Chatbots for conducting systematic reviews in pediatric dentistry.

Rokhshad R(1), Mohammad FD(2), Nomani M(3), Mohammad-Rahimi H(4), Schwendicke 
F(5).

Author information:
(1)Department of Pediatric Dentistry, Loma Linda School of Dentistry, Loma 
Linda, United States. Electronic address: Ratarokhshad@gmail.com.
(2)Student Research Committee, Faculty of Dentistry, Mazandaran University of 
Medical Sciences (MAZUMS), Sari, Iran.
(3)Faculty of Dentistry, Kerman University of Medical Sciences, Kerman, Iran.
(4)Department of Dentistry and Oral Health, Aarhus University, Vennelyst 
Boulevard 9, Aarhus C, Aarhus 8000, Denmark; Clinic for Operative Dentistry and 
Periodontology, LMU Klinikum, Munich, Germany.
(5)Clinic for Operative Dentistry and Periodontology, LMU Klinikum, Munich, 
Germany.

OBJECTIVES: The performance of chatbots for discrete steps of a systematic 
review (SR) on artificial intelligence (AI) in pediatric dentistry was 
evaluated.
METHODS: Two chatbots (ChatGPT4/Gemini) and two non-expert reviewers were 
compared against two experts in a SR on AI in pediatric dentistry. Five tasks: 
(1) formulating a PICO question, (2) developing search queries for eight 
databases, (3) screening studies, (4) extracting data, and (5) assessing the 
risk of bias (RoB) were assessed. Chatbots and non-experts received identical 
prompts, with experts providing the reference standard. Performance was measured 
using accuracy, precision, sensitivity, specificity, and F1-score for search and 
screening tasks, Cohen's Kappa for risk of bias assessment, and a modified 
Global Quality Score (1-5) for PICO question formulation and data extraction 
quality. Statistical comparisons were performed using Kruskal-Wallis and Dunn's 
post-hoc tests.
RESULTS: In PICO formulation, ChatGPT outperformed Gemini slightly, while 
non-experts scored the lowest. Experts identified 1261 records, compared to 569 
(ChatGPT), 285 (Gemini), and 722 (non-experts). Screening showed chatbots having 
90 % sensitivity, >60 % specificity, <25 % precision, and F1-scores <40 %, 
versus non-experts' 84 % sensitivity, 91 % specificity, and 39 % F1-score, 
respectively. For data extraction, ChatGPT yielded a (mean±standard deviation) 
score of 31.6 ± 12.3 (max. was 45), Gemini 29.2 ± 12.3, and non-experts 30.4 ± 
11.3, respectively. For RoB, the agreement with experts was 49.4 % for ChatGPT, 
51.2 % for Gemini 48.8 % for non-experts (p > 0.05).
CONCLUSION: Chatbots could enhance SR efficiency, particularly for the study 
screening and data extraction steps. Human oversight remains critical for 
ensuring accuracy and completeness.
CLINICAL SIGNIFICANCE: Chatbots can streamline SR tasks like screening and data 
extraction, potentially accelerating evidence synthesis, though human oversight 
remains needed for reliability. The applicability of chatbots for SR steps was 
found dependent on the specific step, indicating reviewers need to make informed 
choices when employing chatbots for this purpose.

Copyright © 2025 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.jdent.2025.105733
PMID: 40194755 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declared no potential conflicts of interest with respect to the research, 
authorship, and/or publication of this article.


441. J Food Sci. 2025 Apr;90(4):e70186. doi: 10.1111/1750-3841.70186.

Mushroom species classification and implementation based on improved 
MobileNetV3.

Peng J(1), Li S(1), Li H(1), Lan Z(1), Ma Z(1), Xiang C(2), Li S(3).

Author information:
(1)College of Mechanical Engineering, Chongqing University of Technology, 
Chongqing, China.
(2)Chongqing Hualv Biotechnology Co., Ltd, Chongqing, China.
(3)College of Engineering and Technology, Southwest University, Chongqing, 
China.

Current methods for mushroom species classification face limitations in 
generalization ability and lack exploration of model deployment. To address 
these issues, this study systematically compares five models, including 
Transformer and common convolutional neural networks. MobileNetV3 was chosen as 
the model for this study, combining transfer learning with the adaptive hybrid 
optimizer (AHO) and dynamic cyclic learning rate strategies proposed in this 
research. The AHO merges Adam's fast convergence with stochastic gradient 
descent's stable fine-tuning. It adjusts the learning rate dynamically based on 
training progress, enabling quick convergence early on and precise adjustments 
later. The optimized model was trained, validated, and deployed on a dataset 
constructed in this study, which includes 3633 images covering three types of 
mushrooms. The model achieved a validation accuracy of 98.13% and an average 
test accuracy of 97.98%, with the smallest standard deviation of validation loss 
fluctuation (0.0343), confirming the model's stability. Notably, due to the 
slightly larger number of images in the Matsutake training subset (1412 images) 
compared to the other two categories (1148 and 1073 images), the test accuracy 
for Matsutake (99.28%) was slightly higher than that for Red mushroom (96.97%) 
and Beefsteak mushroom (97.69%), highlighting a minor limitation. However, the 
recall and F1 scores for each class are balanced, suggesting that the model 
exhibits robust performance in addressing interclass similarities, as 
corroborated by t-SNE visualization and Grad-CAM analysis. Additionally, the 
study confirmed the feasibility of practical application through deployment on 
PC, Android, and embedded platforms, providing a guiding solution for laboratory 
research, wild mushroom picking, and automated mushroom sorting. PRACTICAL 
APPLICATION: This study provides an AI model based on a lightweight neural 
network for identifying different mushroom species. It can be widely applied in 
scenarios such as mushroom harvesting, sorting, and research, helping farmers, 
consumers, and researchers easily and accurately identify mushroom varieties, 
thereby contributing to the development of the mushroom industry.

© 2025 Institute of Food Technologists.

DOI: 10.1111/1750-3841.70186
PMID: 40183742 [Indexed for MEDLINE]


442. Bioinform Adv. 2025 Mar 20;5(1):vbaf044. doi: 10.1093/bioadv/vbaf044. 
eCollection 2025.

Biological databases in the age of generative artificial intelligence.

Pop M(1), Attwood TK(2), Blake JA(3), Bourne PE(4), Conesa A(5), Gaasterland 
T(6), Hunter L(7), Kingsford C(8), Kohlbacher O(9), Lengauer T(10), Markel 
S(11), Moreau Y(12), Noble WS(13), Orengo C(14), Ouellette BFF(15), Parida 
L(16), Przulj N(17)(18)(19)(20), Przytycka TM(21), Ranganathan S(22), Schwartz 
R(8)(23), Valencia A(18)(19), Warnow T(24).

Author information:
(1)Center for Bioinformatics and Computational Biology, University of Maryland, 
College Park, MD 20742, United States.
(2)Department of Computer Science, The University of Manchester, Manchester M13 
9PL, United Kingdom.
(3)The Jackson Laboratory, Bar Harbor, ME 04609, United States.
(4)School of Data Science, The University of Virginia, Charlotesville, VA 22904, 
United States.
(5)Institute for Integrative Systems Biology, Spanish National Research Council, 
Paterna 46980, Spain.
(6)Bioinformatics & Systems Biology Graduate Program, La Jolla, CA 92093, United 
States.
(7)Department of Pediatrics, University of Chicago, Chicago, IL 60637, United 
States.
(8)Ray and Stephanie Lane Computational Biology Department, School of Computer 
Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States.
(9)Institute for Bioinformatics and Medical Informatics, University of Tübingen, 
Tübingen 72076, Germany.
(10)Max Planck Institute for Informatics and Saarland Informatics Campus, 
Saarbrücken 66123, Germany.
(11)Dassault Systèmes BIOVIA, San Diego, CA 92121, United States.
(12)Elektrotechniek ESAT-STADIUS, University of Leuven, Leuven 3000, Belgium.
(13)Department of Genome Sciences, University of Washington, Seattle, WA 98195, 
United States.
(14)Department of Structural and Molecular Biology, University College London, 
London WC1E 6BT, United Kingdom.
(15)Montreal, QC H2T 2W5, Canada.
(16)IBM T J Watson Research, Yorktown Heights, NY 10598, United States.
(17)Computational Biology Department, Mohamed bin Zayed University of Artificial 
Intelligence, Abu Dhabi SE45 05, United Arab Emirates.
(18)Barcelona Supercomputing Center, Barcelona 08034, Spain.
(19)Institución Catalana de Investigación y Estudios Avanzados (ICREA), 
Barcelona 08010, Spain.
(20)Department of Computer Science, University College London, London WC1E 6EA, 
United Kingdom.
(21)Computational Biology Branch, Division of Intramural Research, National 
Library of Medicine, Bethesda, MD 20894, United States.
(22)Department of Chemistry and Biomolecular Sciences, Macquarie University, 
Sydney, NSW 2109, Australia.
(23)Department of Biological Sciences, Carnegie Mellon University, Pittsburgh, 
PA 15213, United States.
(24)School of Computing and Data Science, University of Illinois 
Urbana-Champaign, Urbana, IL 61801, United States.

SUMMARY: Modern biological research critically depends on public databases. The 
introduction and propagation of errors within and across databases can lead to 
wasted resources as scientists are led astray by bad data or have to conduct 
expensive validation experiments. The emergence of generative artificial 
intelligence systems threatens to compound this problem owing to the ease with 
which massive volumes of synthetic data can be generated. We provide an overview 
of several key issues that occur within the biological data ecosystem and make 
several recommendations aimed at reducing data errors and their propagation. We 
specifically highlight the critical importance of improved educational programs 
aimed at biologists and life scientists that emphasize best practices in data 
engineering. We also argue for increased theoretical and empirical research on 
data provenance, error propagation, and on understanding the impact of errors on 
analytic pipelines. Furthermore, we recommend enhanced funding for the 
stewardship and maintenance of public biological databases.
AVAILABILITY AND IMPLEMENTATION: Not applicable.

© The Author(s) 2025. Published by Oxford University Press.

DOI: 10.1093/bioadv/vbaf044
PMCID: PMC11964588
PMID: 40177265

Conflict of interest statement: None declared.


443. JMIR Med Educ. 2025 Apr 2;11:e73698. doi: 10.2196/73698.

Authors' Reply: Citation Accuracy Challenges Posed by Large Language Models.

Temsah MH(1), Al-Eyadhy A(1), Jamal A(2), Alhasan K(1), Malki KH(3).

Author information:
(1)Pediatric Department, College of Medicine, King Saud University, King 
Abdullah Road, Riyadh, 11424, Saudi Arabia, 966 114692002.
(2)Department of Family and Community Medicine, King Saud University Medical 
City, Riyadh, Saudi Arabia.
(3)Research Chair of Voice, Swallowing, and Communication Disorders, Department 
of Otolaryngology-Head and Neck Surgery, College of Medicine, King Saud 
University, Riyadh, Saudi Arabia.

Comment in
    doi: 10.2196/72998.

DOI: 10.2196/73698
PMCID: PMC12037898
PMID: 40173373

Conflict of interest statement: None declared.


444. Children (Basel). 2025 Mar 14;12(3):359. doi: 10.3390/children12030359.

Conversational AI in Pediatric Mental Health: A Narrative Review.

Mansoor M(1), Hamide A(1), Tran T(1).

Author information:
(1)Edward Via College of Osteopathic Medicine-Louisiana Campus, Monroe, LA 
71203, USA.

BACKGROUND/OBJECTIVES: Mental health disorders among children and adolescents 
represent a significant global health challenge, with approximately 50% of 
conditions emerging before age 14. Despite substantial investment in services, 
persistent barriers such as provider shortages, stigma, and accessibility issues 
continue to limit effective care delivery. This narrative review examines the 
emerging application of conversational artificial intelligence (AI) in pediatric 
mental health contexts, mapping the current evidence base, identifying 
therapeutic mechanisms, and exploring unique developmental considerations 
required for implementation.
METHODS: We searched multiple electronic databases (PubMed/MEDLINE, PsycINFO, 
ACM Digital Library, IEEE Xplore, and Scopus) for literature published between 
January 2010 and February 2025 that addressed conversational AI applications 
relevant to pediatric mental health. We employed a narrative synthesis approach 
with thematic analysis to organize findings across technological approaches, 
therapeutic applications, developmental considerations, implementation contexts, 
and ethical frameworks.
RESULTS: The review identified promising applications for conversational AI in 
pediatric mental health, particularly for common conditions like anxiety and 
depression, psychoeducation, skills practice, and bridging to traditional care. 
However, most robust empirical research has focused on adult populations, with 
pediatric applications only beginning to receive dedicated investigation. Key 
therapeutic mechanisms identified include reduced barriers to self-disclosure, 
cognitive change, emotional validation, and behavioral activation. Developmental 
considerations emerged as fundamental challenges, necessitating age-appropriate 
adaptations across cognitive, emotional, linguistic, and ethical dimensions 
rather than simple modifications of adult-oriented systems.
CONCLUSIONS: Conversational AI has potential to address significant unmet needs 
in pediatric mental health as a complement to, rather than replacement for, 
human-delivered care. Future research should prioritize developmental 
validation, longitudinal outcomes, implementation science, safety monitoring, 
and equity-focused design. Interdisciplinary collaboration involving children 
and families is essential to ensure these technologies effectively address the 
unique mental health needs of young people while mitigating potential risks.

DOI: 10.3390/children12030359
PMCID: PMC11941195
PMID: 40150640

Conflict of interest statement: The authors declare no conflicts of interest.


445. IEEE Trans Ultrason Ferroelectr Freq Control. 2025 May;72(5):624-635. doi: 
10.1109/TUFFC.2025.3555447. Epub 2025 May 7.

Synthetic Lung Ultrasound Data Generation Using Autoencoder With Generative 
Adversarial Network.

Fatima N, Mento F, Afrakhteh S, Perrone T, Smargiassi A, Inchingolo R, Demi L.

Class imbalance is a significant challenge in medical image analysis, 
particularly in lung ultrasound (LUS), where severe patterns are often 
underrepresented. Traditional oversampling techniques, which simply duplicate 
original data, have limited effectiveness in addressing this issue. To overcome 
these limitations, this study introduces a novel supervised autoencoder 
generative adversarial network (SA-GAN) for data augmentation, leveraging 
advanced generative artificial intelligence (AI) to create high-quality 
synthetic samples for minority classes. In addition, the traditional data 
augmentation technique is used for comparison. The SA-GAN incorporates an 
autoencoder to develop a conditional latent space, effectively addressing weight 
clipping issues and ensuring higher quality synthetic data. The generated 
samples are evaluated using similarity metrics and expert analysis to validate 
their utility. Furthermore, state-of-the-art neural networks are used for 
multiclass classification, and their performance is compared when trained with 
GAN-based augmentation versus traditional data augmentation techniques. These 
contributions enhance the robustness and reliability of AI models in mitigating 
class imbalance in LUS analysis.

DOI: 10.1109/TUFFC.2025.3555447
PMID: 40146656 [Indexed for MEDLINE]


446. Nat Med. 2025 Jun;31(6):1882-1894. doi: 10.1038/s41591-025-03601-1. Epub 2025 
Mar 25.

AI-guided precision parenteral nutrition for neonatal intensive care units.

Phongpreecha T(#)(1)(2)(3)(4), Ghanem M(#)(1)(2)(3), Reiss JD(#)(2), Oskotsky 
TT(#)(5)(6), Mataraso SJ(1)(2)(3), De Francesco D(1)(2)(3), Reincke SM(1)(2)(3), 
Espinosa C(1)(2)(3), Chung P(1), Ng T(7), Costello JM(5), Sequoia JA(2), Razdan 
S(2)(8), Xie F(1)(2)(3), Berson E(1)(3)(4), Kim Y(1)(2)(3), Seong D(1)(2)(3), 
Szeto MY(2), Myers F(2), Gu H(2), Feister J(2), Verscaj CP(2), Rose LA(2), Sin 
LWY(1), Oskotsky B(5), Roger J(5), Shu CH(1)(2)(3), Shome S(1)(2)(3), Yang 
LK(1)(2)(3), Tan Y(4)(9), Levitte S(10), Wong RJ(2), Gaudillière B(1), Angst 
MS(1), Montine TJ(4), Kerner JA(2), Keller RL(6), Shaw GM(2), Sylvester KG(2), 
Fuerch J(2), Chock V(2), Gaskari S(7), Stevenson DK(2), Sirota M(5)(6), Prince 
LS(2), Aghaeepour N(11)(12)(13).

Author information:
(1)Department of Anesthesiology, Pain and Perioperative Medicine, Stanford 
University, Stanford, CA, USA.
(2)Department of Pediatrics, Stanford University, Stanford, CA, USA.
(3)Department of Biomedical Data Science, Stanford University, Stanford, CA, 
USA.
(4)Department of Pathology, Stanford University, Stanford, CA, USA.
(5)Bakar Computational Health Sciences Institute, University of California, San 
Francisco, CA, USA.
(6)Department of Pediatrics, University of California, San Francisco, CA, USA.
(7)Department of Pharmacy, Lucile Packard Children's Hospital, Palo Alto, CA, 
USA.
(8)Department of Pediatrics, Keck School of Medicine of USC, Division of 
Neonatal and Infant Critical Care Unit, Children's Hospital Los Angeles, Los 
Angeles, CA, USA.
(9)Department of Microbiology and Immunology, Stanford University, Stanford, CA, 
USA.
(10)Division of Pediatric Gastroenterology, Hepatology and Nutrition, Stanford 
University, Stanford, CA, USA.
(11)Department of Anesthesiology, Pain and Perioperative Medicine, Stanford 
University, Stanford, CA, USA. naghaeep@stanford.edu.
(12)Department of Pediatrics, Stanford University, Stanford, CA, USA. 
naghaeep@stanford.edu.
(13)Department of Biomedical Data Science, Stanford University, Stanford, CA, 
USA. naghaeep@stanford.edu.
(#)Contributed equally

Erratum in
    Nat Med. 2025 Jun;31(6):2070. doi: 10.1038/s41591-025-03691-x.

One in ten neonates are admitted to neonatal intensive care units, highlighting 
the need for precise interventions. However, the application of artificial 
intelligence (AI) in guiding neonatal care remains underexplored. Total 
parenteral nutrition (TPN) is a life-saving treatment for preterm neonates; 
however, implementation of the therapy in its current form is subjective, 
error-prone and resource-consuming. Here, we developed TPN2.0-a data-driven 
approach that optimizes and standardizes TPN using information collected 
routinely in electronic health records. We assembled a decade of TPN 
compositions (79,790 orders; 5,913 patients) at Stanford to train TPN2.0. In 
addition to internal validation, we also validated our model in an external 
cohort (63,273 orders; 3,417 patients) from a second hospital. Our algorithm 
identified 15 TPN formulas that can enable a precision-medicine approach 
(Pearson's R = 0.94 compared to experts), increasing safety and potentially 
reducing cost. A blinded study (n = 192) revealed that physicians rated TPN2.0 
higher than current best practice. In patients with high disagreement between 
the actual prescriptions and TPN2.0, standard prescriptions were associated with 
increased morbidities (for example, odds ratio = 3.33; P value = 0.0007 for 
necrotizing enterocolitis), while TPN2.0 recommendations were linked to reduced 
risk. Finally, we demonstrated that TPN2.0 employing a transformer architecture 
enabled guideline-adhering, physician-in-the-loop recommendations that allow 
collaboration between the care team and AI.

© 2025. The Author(s).

DOI: 10.1038/s41591-025-03601-1
PMCID: PMC12176641
PMID: 40133525 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: The methods described in 
this manuscript are covered in the US provisional Patent 63/268,689 
(WO2022256850A1; ‘Systems and methods to assess neonatal health risk and uses 
thereof’) approved in 2022. T.P. is a cofounder of Takeoff41. S.M. is a paid 
consultant for Danaher and Longitude Capital and receives a paid fellowship from 
Nucleate. J.H.F. is an advisor to Vitara, OvaryIt, Keriton, EmpoHealth, and 
Avanos; the consulting medical director of Novonate; and a cofounder for EMME. 
K.G.S. is a consultant for Avexegen Therapeutics, Infinant Health, mProbe and 
Mission Biocapital. M.S.A. is a member of the Scientific Advisory Board of 
Cytonics Inc. and AfaSci Research Laboratories and is a paid consultant for 
Syneos Health. D.K.S. is a member of the Clinical Advisory Board of Maternica 
Therapeutics. M.S. is a member of the Scientific Advisory Board of Exagen and 
Aria Pharmaceuticals and is a shareholder at Somnics. N.A. is a member of the 
Scientific Advisory Boards of January AI, Parallel Bio and WellSim Biomedical 
Technologies, is a cofounder of Takeoff41 and is a paid consultant for MaraBio 
Systems. The other authors declare no competing interests.


447. Telemed J E Health. 2025 Jul;31(7):821-828. doi: 10.1089/tmj.2024.0555. Epub 
2025 Mar 24.

Artificial Intelligence and Qualitative Analysis of Emergency Department 
Telemental Health Care Implementation Survey.

Keating C(1), Marcus SC(2)(3), Bowden CF(1), Worsley D(1), Doupnik SK(1)(4).

Author information:
(1)Division of General Pediatrics, Clinical Futures, and PolicyLab, Children's 
Hospital of Philadelphia, Philadelphia, Pennsylvania, USA.
(2)Leonard Davis Institute of Health Economics, University of Pennsylvania, 
Philadelphia, Pennsylvania, USA.
(3)Center for Mental Health, University of Pennsylvania, Philadelphia, 
Pennsylvania, USA.
(4)Department of Pediatrics, Vanderbilt University Medical Center, Nashville, 
Tennessee, USA.

Background: Implementation of telemental health care in emergency departments 
(EDs) in the United States (U.S.) has been increasing. Artificial intelligence 
(AI) can augment traditional qualitative research methods; little is known about 
its efficiency and accuracy. This study sought to understand ED directors' 
qualitative recommendations for improving telemental health care implementation 
and to understand how AI could facilitate analysis of qualitative survey 
responses. Methods: Directors at a nationally representative sample of 279 U.S. 
EDs that used telemental health care completed an open-ended survey question 
about improving telemental health care implementation between June 2022 and 
October 2023. Two groups of researchers completed independent qualitative coding 
of responses: one group used traditional qualitative methods, and one group used 
AI (ChatGPT 4.0) to facilitate analysis. Both groups independently developed a 
codebook, came to consensus on a combined codebook, and each group independently 
used it to code the survey responses. The two groups identified themes in ED 
directors' recommendations and compared codebooks and code application across 
traditional and AI approaches. Results: Themes included (1) recommendations for 
improving telemental health care directly and (2) recommendations for improving 
mental health care systems broadly to make telehealth more effective. ED 
directors' most common recommendation was enabling faster and more streamlined 
access to telemental health care. AI augmented human coding by identifying two 
valid codes not initially identified by human analysts. In codebook application, 
75% of responses were coded consistently across AI and human coders. Conclusions 
and Relevance: For US EDs using telemental health care, there is a need to 
improve timeliness and efficiency of access to telemental health care.

DOI: 10.1089/tmj.2024.0555
PMID: 40129004 [Indexed for MEDLINE]


448. Sci Rep. 2025 Mar 23;15(1):10013. doi: 10.1038/s41598-025-94779-4.

Design and implement of high accuracy HV-CCPS for high power discharge pumped 
excimer laser.

Shen Q(1)(2)(3), Liang X(4)(5)(6).

Author information:
(1)Science Island Branch of Graduate School, University of Science and 
Technology of China, Hefei, 230026, Anhui, China.
(2)Anhui Institute of Optics and Fine Mechanics, Hefei Institutes of Physical 
Sciences, Chinese Academy of Sciences, Hefei, 230031, China.
(3)Anhui Provincial Key Laboratory of Photonics Devices and Materials, Hefei, 
230031, China.
(4)Anhui Institute of Optics and Fine Mechanics, Hefei Institutes of Physical 
Sciences, Chinese Academy of Sciences, Hefei, 230031, China. 
liangxu@aiofm.ac.cn.
(5)Anhui Provincial Key Laboratory of Photonics Devices and Materials, Hefei, 
230031, China. liangxu@aiofm.ac.cn.
(6)Advanced Laser Technology Laboratory of Anhui Province, Hefei, 230037, China. 
liangxu@aiofm.ac.cn.

This paper presents the design of a 30 kV/2.03 A high-voltage capacitor charging 
power supply, centered around an enhanced resonant converter, for use in 
high-power excimer lasers and other related pulsed power applications. 
Typically, stray parameters of high-voltage high-frequency transformers 
negatively impact converter performance. Based on an analysis of the series 
resonant converter, an enhanced resonant converter is proposed, which leverages 
the stray parameters of the transformer to benefit the converter. Through 
calculations and simulations, it was found that a smaller resonant capacitor 
(Cp) enables dual-pulse rapid charging in the early stages of charging and 
single-pulse slow charging in the later stages. The transition point between 
dual and single pulses occurs later in the charging process, allowing for high 
power output while significantly improving charging accuracy and reducing the 
resonant current crest factor. Additionally, to address the underdamped 
discharge process in the downstream pulsed power applications, a protection 
network was designed, reducing the reverse peak voltage by approximately 80%. 
Finally, a prototype was tested on an excimer laser, achieving a peak power of 
60.9 kW and an accuracy of 0.22%, with a maximum laser output power of 304.2 W. 
The experimental results validate the proposed solution.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-94779-4
PMCID: PMC11930975
PMID: 40122961

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


449. Bioinformatics. 2025 Mar 29;41(4):btaf122. doi: 10.1093/bioinformatics/btaf122.

Clustering individuals using INMTD: a novel versatile multi-view embedding 
framework integrating omics and imaging data.

Li Z(1)(2)(3), Windels SFL(4), Malod-Dognin N(4), Weinberg SM(5)(6), Marazita 
ML(5)(6), Walsh S(7), Shriver MD(8), Fardo DW(9), Claes P(1)(2)(10)(11), Pržulj 
N(4)(12)(13), Van Steen K(1)(3).

Author information:
(1)Department of Human Genetics, KU Leuven, 3000 Leuven, Belgium.
(2)Medical Imaging Research Center, UZ Leuven, 3000 Leuven, Belgium.
(3)GIGA Molecular & Computational Biology, University of Liège, 4000 Liège, 
Belgium.
(4)Barcelona Supercomputing Center, 08034 Barcelona, Spain.
(5)Department of Oral and Craniofacial Sciences, Center for Craniofacial and 
Dental Genetics, University of Pittsburgh, Pittsburgh, PA 15260, United States.
(6)Department of Human Genetics, University of Pittsburgh, Pittsburgh, PA 15260, 
United States.
(7)Department of Biology, Indiana University Purdue University Indianapolis, 
Indianapolis, IN 46202, United States.
(8)Department of Anthropology, Pennsylvania State University, University Park, 
PA 16802, United States.
(9)Sanders-Brown Center on Aging, University of Kentucky, Lexington, KY 40506, 
United States.
(10)Department of Electrical Engineering, ESAT/PSI, KU Leuven, 3000 Leuven, 
Belgium.
(11)Murdoch Children's Research Institute, Parkville, VIC 3052, Australia.
(12)Department of Computer Science, University College London, London WC1E 6BT, 
United Kingdom.
(13)Catalan Institution for Research and Advanced Studies (ICREA), 08010 
Barcelona, Spain.

MOTIVATION: Combining omics and images can lead to a more comprehensive 
clustering of individuals than classic single-view approaches. Among the various 
approaches for multi-view clustering, nonnegative matrix tri-factorization 
(NMTF) and nonnegative Tucker decomposition (NTD) are advantageous in learning 
low-rank embeddings with promising interpretability. Besides, there is a need to 
handle unwanted drivers of clusterings (i.e. confounders).
RESULTS: In this work, we introduce a novel multi-view clustering method based 
on NMTF and NTD, named INMTD, which integrates omics and 3D imaging data to 
derive unconfounded subgroups of individuals. According to the adjusted Rand 
index, INMTD outperformed other clustering methods on a synthetic dataset with 
known clusters. In the application to real-life facial-genomic data, INMTD 
generated biologically relevant embeddings for individuals, genetics, and facial 
morphology. By removing confounded embedding vectors, we derived an unconfounded 
clustering with better internal and external quality; the genetic and facial 
annotations of each derived subgroup highlighted distinctive characteristics. In 
conclusion, INMTD can effectively integrate omics data and 3D images for 
unconfounded clustering with biologically meaningful interpretation.
AVAILABILITY AND IMPLEMENTATION: INMTD is freely available at 
https://github.com/ZuqiLi/INMTD.

© The Author(s) 2025. Published by Oxford University Press.

DOI: 10.1093/bioinformatics/btaf122
PMCID: PMC11978392
PMID: 40119919 [Indexed for MEDLINE]


450. J Cheminform. 2025 Mar 21;17(1):35. doi: 10.1186/s13321-025-00979-5.

An interpretable deep geometric learning model to predict the effects of 
mutations on protein-protein interactions using large-scale protein language 
model.

Zhang C(1), Sun Y(1)(2)(3), Hu P(4)(5)(6)(7)(8)(9).

Author information:
(1)Department of Computer Science, Western University, London, ON, Canada.
(2)Department of Computer Science, University of Manitoba, Winnipeg, MB, Canada.
(3)Department of Biochemistry, Western University, London, ON, Canada.
(4)Department of Computer Science, Western University, London, ON, Canada. 
phu49@uwo.ca.
(5)Department of Computer Science, University of Manitoba, Winnipeg, MB, Canada. 
phu49@uwo.ca.
(6)Department of Biochemistry, Western University, London, ON, Canada. 
phu49@uwo.ca.
(7)Department of Oncology, Western University, London, ON, Canada. phu49@uwo.ca.
(8)Department of Epidemiology and Biostatistics, Western University, London, ON, 
Canada. phu49@uwo.ca.
(9)The Children's Health Research Institute, Lawson Health Research Institute, 
London, ON, Canada. phu49@uwo.ca.

Protein-protein interactions (PPIs) are central to the mechanisms of signaling 
pathways and immune responses, which can help us understand disease etiology. 
Therefore, there is a significant need for efficient and rapid automated 
approaches to predict changes in PPIs. In recent years, there has been a 
significant increase in applying deep learning techniques to predict changes in 
binding affinity between the original protein complex and its mutant variants. 
Particularly, the adoption of graph neural networks (GNNs) has gained prominence 
for their ability to learn representations of protein-protein complexes. 
However, the conventional GNNs have mainly concentrated on capturing local 
features, often disregarding the interactions among distant elements that hold 
potential important information. In this study, we have developed a 
transformer-based graph neural network to extract features of the mutant segment 
from the three-dimensional structure of protein-protein complexes. By embracing 
both local and global features, the approach ensures a more comprehensive 
understanding of the intricate relationships, thus promising more accurate 
predictions of binding affinity changes. To enhance the representation 
capability of protein features, we incorporate a large-scale pre-trained protein 
language model into our approach and employ the global protein feature it 
provides. The proposed model is shown to be able to predict the mutation changes 
in binding affinity with a root mean square error of 1.10 and a Pearson 
correlation coefficient of near 0.71, as demonstrated by performance on test and 
validation cases. Our experiments on all five datasets, including both single 
mutant and multiple mutant cases, demonstrate that our model outperforms four 
state-of-the-art baseline methods, and the efficacy was subjected to 
comprehensive experimental evaluation. Our study introduces a transformer-based 
graph neural network approach to accurately predict changes in protein-protein 
interactions (PPIs). By integrating local and global features and leveraging 
pretrained protein language models, our model outperforms state-of-the-art 
methods across diverse datasets. The results of this study can provide new views 
for studying immune responses and disease etiology related to protein mutations. 
Furthermore, this approach may contribute to other biological or biochemical 
studies related to PPIs.Scientific contribution Our scientific contribution lies 
in the development of a novel transformer-based graph neural network tailored to 
predict changes in protein-protein interactions (PPIs) with excellent accuracy. 
By seamlessly integrating both local and global features extracted from the 
three-dimensional structure of protein-protein complexes, and leveraging the 
rich representations provided by pretrained protein language models, our 
approach surpasses existing methods across diverse datasets. Our findings may 
offer novel insights for the understanding of complex disease etiology 
associated with protein mutations. The novel tool can be applicable to various 
biological and biochemical investigations involving protein mutations.

© 2025. The Author(s).

DOI: 10.1186/s13321-025-00979-5
PMCID: PMC11927297
PMID: 40119464

Conflict of interest statement: Declarations. Completing interests: The authors 
declare no competing interests.


451. J Med Internet Res. 2025 Mar 21;27:e67967. doi: 10.2196/67967.

Large Language Model-Based Assessment of Clinical Reasoning Documentation in the 
Electronic Health Record Across Two Institutions: Development and Validation 
Study.

Schaye V(1)(2), DiTullio D(1), Guzman BV(3), Vennemeyer S(4), Shih H(4), 
Reinstein I(2), Weber DE(5)(6), Goodman A(5)(6), Wu DTY(4), Sartori DJ(1), 
Santen SA(7), Gruppen L(8), Aphinyanaphongs Y(3), Burk-Rafel J(1)(2).

Author information:
(1)Department of Medicine, NYU Grossman School of Medicine, New York, NY, United 
States.
(2)Institute for Innovations in Medical Education, NYU Grossman School of 
Medicine, New York, NY, United States.
(3)Division of Applied AI Technologies, NYU Langone Health, New York, NY, United 
States.
(4)Department of Biostatistics, Health informatics, and Data Sciences, 
University of Cincinnati College of Medicine, Cincinnati, OH, United States.
(5)Division of Hospital Medicine, Department of Pediatrics, Cincinnati 
Children's Hospital Medical Center, University of Cincinnati College of 
Medicine, Cincinnati, OH, United States.
(6)Division of Hospital Medicine, Department of Internal Medicine, University of 
Cincinnati College of Medicine, Cincinnati, OH, United States.
(7)Department of Emergency Medicine, University of Cincinnati College of 
Medicine, Cincinnati, OH, United States.
(8)Department of Learning Health Sciences, University of Michigan Medical 
School, Ann Arbor, MI, United States.

BACKGROUND: Clinical reasoning (CR) is an essential skill; yet, physicians often 
receive limited feedback. Artificial intelligence holds promise to fill this 
gap.
OBJECTIVE: We report the development of named entity recognition (NER), 
logic-based and large language model (LLM)-based assessments of CR documentation 
in the electronic health record across 2 institutions (New York University 
Grossman School of Medicine [NYU] and University of Cincinnati College of 
Medicine [UC]).
METHODS: The note corpus consisted of internal medicine resident admission notes 
(retrospective set: July 2020-December 2021, n=700 NYU and 450 UC notes and 
prospective validation set: July 2023-December 2023, n=155 NYU and 92 UC notes). 
Clinicians rated CR documentation quality in each note using a previously 
validated tool (Revised-IDEA), on 3-point scales across 2 domains: differential 
diagnosis (D0, D1, and D2) and explanation of reasoning, (EA0, EA1, and EA2). At 
NYU, the retrospective set was annotated for NER for 5 entities (diagnosis, 
diagnostic category, prioritization of diagnosis language, data, and linkage 
terms). Models were developed using different artificial intelligence 
approaches, including NER, logic-based model: a large word vector model 
(scispaCy en_core_sci_lg) with model weights adjusted with backpropagation from 
annotations, developed at NYU with external validation at UC, NYUTron LLM: an 
NYU internal 110 million parameter LLM pretrained on 7.25 million clinical 
notes, only validated at NYU, and GatorTron LLM: an open source 345 million 
parameter LLM pretrained on 82 billion words of clinical text, fined tuned on 
NYU retrospective sets, then externally validated and further fine-tuned at UC. 
Model performance was assessed in the prospective sets with F1-scores for the 
NER, logic-based model and area under the receiver operating characteristic 
curve (AUROC) and area under the precision-recall curve (AUPRC) for the LLMs.
RESULTS: At NYU, the NYUTron LLM performed best: the D0 and D2 models had 
AUROC/AUPRC 0.87/0.79 and 0.89/0.86, respectively. The D1, EA0, and EA1 models 
had insufficient performance for implementation (AUROC range 0.57-0.80, AUPRC 
range 0.33-0.63). For the D1 classification, the approach pivoted to a stepwise 
approach taking advantage of the more performant D0 and D2 models. For the EA 
model, the approach pivoted to a binary EA2 model (ie, EA2 vs not EA2) with 
excellent performance, AUROC/AUPRC 0.85/ 0.80. At UC, the NER, D-logic-based 
model was the best performing D model (F1-scores 0.80, 0.74, and 0.80 for D0, 
D1, D2, respectively. The GatorTron LLM performed best for EA2 scores 
AUROC/AUPRC 0.75/ 0.69.
CONCLUSIONS: This is the first multi-institutional study to apply LLMs for 
assessing CR documentation in the electronic health record. Such tools can 
enhance feedback on CR. Lessons learned by implementing these models at distinct 
institutions support the generalizability of this approach.

©Verity Schaye, David DiTullio, Benedict Vincent Guzman, Scott Vennemeyer, 
Hanniel Shih, Ilan Reinstein, Danielle E Weber, Abbie Goodman, Danny T Y Wu, 
Daniel J Sartori, Sally A Santen, Larry Gruppen, Yindalon Aphinyanaphongs, Jesse 
Burk-Rafel. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 21.03.2025.

DOI: 10.2196/67967
PMCID: PMC11971582
PMID: 40117575 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


452. J Med Internet Res. 2025 Apr 1;27:e70789. doi: 10.2196/70789.

Adoption of Large Language Model AI Tools in Everyday Tasks: Multisite 
Cross-Sectional Qualitative Study of Chinese Hospital Administrators.

Chen J(1), Liu Y(2), Liu P(1), Zhao Y(1), Zuo Y(3)(4)(5), Duan H(6).

Author information:
(1)Medical Services Management Department, Peking University People's Hospital, 
Beijing, China.
(2)Department of Obstetrics and Gynecology, Peking University Shenzhen Hospital, 
Shenzhen, China.
(3)Department of Gynecology and Obstetrics Nursing, West China Second University 
Hospital, Sichuan University, Chengdu, China.
(4)West China School of Nursing, Sichuan University, Chengdu, China.
(5)Key laboratory of Birth Defects and Related Disease of Women and Children 
(Sichuan University), Ministry of Education, Chengdu, China.
(6)School of Public Administration and Policy, Renmin University of China, 
Beijing, China.

BACKGROUND: Large language model (LLM) artificial intelligence (AI) tools have 
the potential to streamline health care administration by enhancing efficiency 
in document drafting, resource allocation, and communication tasks. Despite this 
potential, the adoption of such tools among hospital administrators remains 
understudied, particularly at the individual level.
OBJECTIVE: This study aims to explore factors influencing the adoption and use 
of LLM AI tools among hospital administrators in China, focusing on enablers, 
barriers, and practical applications in daily administrative tasks.
METHODS: A multicenter, cross-sectional, descriptive qualitative design was 
used. Data were collected through semistructured face-to-face interviews with 31 
hospital administrators across 3 tertiary hospitals in Beijing, Shenzhen, and 
Chengdu from June 2024 to August 2024. The Colaizzi method was used for thematic 
analysis to identify patterns in participants' experiences and perspectives.
RESULTS: Adoption of LLM AI tools was generally low, with significant 
site-specific variations. Participants with higher technological familiarity and 
positive early experiences reported more frequent use, while barriers such as 
mistrust in tool accuracy, limited prompting skills, and insufficient training 
hindered broader adoption. Tools were primarily used for document drafting, with 
limited exploration of advanced functionalities. Participants strongly 
emphasized the need for structured training programs and institutional support 
to enhance usability and confidence.
CONCLUSIONS: Familiarity with technology, positive early experiences, and 
openness to innovation may facilitate adoption, while barriers such as limited 
knowledge, mistrust in tool accuracy, and insufficient prompting skills can 
hinder broader use. LLM AI tools are now primarily used for basic tasks such as 
document drafting, with limited application to more advanced functionalities due 
to a lack of training and confidence. Structured tutorials and institutional 
support are needed to enhance usability and integration. Targeted training 
programs, combined with organizational strategies to build trust and improve 
accessibility, could enhance adoption rates and broaden tool use. Future 
quantitative investigations should validate the adoption rate and influencing 
factors.

©Jun Chen, Yu Liu, Peng Liu, Yiming Zhao, Yan Zuo, Hui Duan. Originally 
published in the Journal of Medical Internet Research (https://www.jmir.org), 
01.04.2025.

DOI: 10.2196/70789
PMCID: PMC12000786
PMID: 40116330 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


453. Law Hum Behav. 2025 Apr;49(2):163-172. doi: 10.1037/lhb0000590. Epub 2025 Mar 
20.

Automated question type coding of forensic interviews and trial testimony in 
child sexual abuse cases.

Szojka ZA(1), Yashraj S(1), Lyon TD(1).

Author information:
(1)Gould School of Law, University of Southern California.

OBJECTIVE: Question-type classification is widely used as a measure of interview 
quality. However, question-type coding is a time-consuming process when 
performed by manual coders. Reliable automated question-type coding approaches 
would facilitate the assessment of the quality of forensic interviews and court 
testimony involving victims of child abuse.
HYPOTHESES: We expected that the reliability achieved by the automated model 
would be comparable to manual coders.
METHOD: We examined whether a large language model (Robustly Optimized 
Bidirectional Encoder Representations from Transformers Approach) trained on 
questions (N = 351,920) asked in forensic interviews (n = 1,435) and trial 
testimony (n = 416) involving 3- to 17-year-old alleged victims of child sexual 
abuse could distinguish among (a) invitations, (b) wh-questions, (c) 
option-posing questions, and (d) nonquestions.
RESULTS: The model achieved high reliability (95% agreement; κ = .93). To 
determine whether disagreements were due to machine or manual errors, we recoded 
inconsistencies between the machine and manual codes. Manual coders erred more 
often than the machine, particularly by overlooking invitations and 
nonquestions. Correcting errors in the manual codes further increased the 
model's reliability (98% agreement; κ = .97).
CONCLUSIONS: Automated question-type coding can provide a time-efficient and 
highly accurate alternative to manual coding. We have made the trained model 
publicly available for use by researchers and practitioners. (PsycInfo Database 
Record (c) 2025 APA, all rights reserved).

DOI: 10.1037/lhb0000590
PMCID: PMC11975488
PMID: 40111820 [Indexed for MEDLINE]


454. Sci Rep. 2025 Mar 19;15(1):9404. doi: 10.1038/s41598-025-90315-6.

EEG detection and recognition model for epilepsy based on dual attention 
mechanism.

Huang Z(1)(2), Yang Y(1), Ma Y(3), Dong Q(4), Su J(5), Shi H(5), Zhang S(1), Hu 
L(6).

Author information:
(1)School of Electronic Information and Xi'an Key Laboratory of High Precision 
Industrial Intelligent Vision Measurement Technology, Xijing University, Xi'an, 
710123, China.
(2)College of Computer Science and Technology, Chongqing University of Posts and 
Telecommunications, Chongqing, 400065, China.
(3)School of Electronic Information and Xi'an Key Laboratory of High Precision 
Industrial Intelligent Vision Measurement Technology, Xijing University, Xi'an, 
710123, China. yahongma@sina.com.
(4)School of Mathematics and Statistics, Zhengzhou University, Zhengzhou, 
710003, Henan, China.
(5)Neurosurgery Department, Affiliate Children's Hospital of Xi'an Jiaotong 
University, Xi'an, 710003, Shaanxi Province, China.
(6)School of artificial intelligence, Chongqing University of Education, 
Chongqing, 400065, China. hull@cque.edu.cn.

In the field of clinical neurology, automated detection of epileptic seizures 
based on electroencephalogram (EEG) signals has the potential to significantly 
accelerate the diagnosis of epilepsy. This rapid and accurate diagnosis enables 
doctors to provide timely and effective treatment for patients, significantly 
reducing the frequency of future epileptic seizures and the risk of related 
complications, which is crucial for safeguarding patients' long-term health and 
quality of life. Presently, deep learning techniques, particularly Convolutional 
Neural Networks (CNNs) and Long Short-Term Memory networks (LSTMs), have 
demonstrated remarkable accuracy improvements across various domains. 
Consequently, researchers have utilized these methodologies in studies focused 
on recognizing epileptic signals through EEG analysis. However, current models 
based on CNN and LSTM still heavily rely on data preprocessing and feature 
extraction steps. Additionally, CNNs exhibit limitations in perceiving global 
dependencies, while LSTMs encounter challenges such as gradient vanishing in 
long sequences. This paper introduced an innovative EEG recognition model, that 
is the Spatio-temporal feature fusion epilepsy EEG recognition model with dual 
attention mechanism (STFFDA). STFFDA is comprised of a multi-channel framework 
that directly interprets epileptic states from raw EEG signals, thereby 
eliminating the need for extensive data preprocessing and feature extraction. 
Notably, our method demonstrates impressive accuracy results, achieving 95.18% 
and 77.65% on single-validation tests conducted on the datasets of CHB-MIT and 
Bonn University, respectively. Additionally, in the 10-fold cross-validation 
tests, their accuracy rates were 92.42% and 67.24%, respectively. In summary, it 
is demonstrated that the seizure detection method STFFD based on EEG signals has 
significant potential in accelerating diagnosis and improving patient prognosis, 
especially since it can achieve high accuracy rates without extensive data 
preprocessing or feature extraction.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-90315-6
PMCID: PMC11923361
PMID: 40108237 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


455. JMIRx Med. 2025 Mar 19;6:e65263. doi: 10.2196/65263.

Large Language Models for Pediatric Differential Diagnoses in Rural Health Care: 
Multicenter Retrospective Cohort Study Comparing GPT-3 With Pediatrician 
Performance.

Mansoor M(1), Ibrahim AF(2), Grindem D(3), Baig A(4).

Author information:
(1)Edward Via College of Osteopathic Medicine, 4408 Bon Aire Dr, Monroe, LA, 
71203, United States, 1 5045213500.
(2)Texas Tech University Health Sciences Center School of Medicine, Lubbock, TX, 
United States.
(3)Mayo Clinic, Rochester, MN, United States.
(4)Department of Radiology, Columbia University Medical Center, New York, NY, 
United States.

Update of
    doi: 10.1101/2024.08.09.24311777v1.

BACKGROUND: Rural health care providers face unique challenges such as limited 
specialist access and high patient volumes, making accurate diagnostic support 
tools essential. Large language models like GPT-3 have demonstrated potential in 
clinical decision support but remain understudied in pediatric differential 
diagnosis.
OBJECTIVE: This study aims to evaluate the diagnostic accuracy and reliability 
of a fine-tuned GPT-3 model compared to board-certified pediatricians in rural 
health care settings.
METHODS: This multicenter retrospective cohort study analyzed 500 pediatric 
encounters (ages 0-18 years; n=261, 52.2% female) from rural health care 
organizations in Central Louisiana between January 2020 and December 2021. The 
GPT-3 model (DaVinci version) was fine-tuned using the OpenAI application 
programming interface and trained on 350 encounters, with 150 reserved for 
testing. Five board-certified pediatricians (mean experience: 12, SD 5.8 years) 
provided reference standard diagnoses. Model performance was assessed using 
accuracy, sensitivity, specificity, and subgroup analyses.
RESULTS: The GPT-3 model achieved an accuracy of 87.3% (131/150 cases), 
sensitivity of 85% (95% CI 82%-88%), and specificity of 90% (95% CI 87%-93%), 
comparable to pediatricians' accuracy of 91.3% (137/150 cases; P=.47). 
Performance was consistent across age groups (0-5 years: 54/62, 87%; 6-12 years: 
47/53, 89%; 13-18 years: 30/35, 86%) and common complaints (fever: 36/39, 92%; 
abdominal pain: 20/23, 87%). For rare diagnoses (n=20), accuracy was slightly 
lower (16/20, 80%) but comparable to pediatricians (17/20, 85%; P=.62).
CONCLUSIONS: This study demonstrates that a fine-tuned GPT-3 model can provide 
diagnostic support comparable to pediatricians, particularly for common 
presentations, in rural health care. Further validation in diverse populations 
is necessary before clinical implementation.

© Masab Mansoor, Andrew F Ibrahim, David Grindem, Asad Baig. Originally 
published in JMIRx Med (https://med.jmirx.org).

DOI: 10.2196/65263
PMCID: PMC11939124
PMID: 40106452

Conflict of interest statement: Conflicts of Interest: None declared.


456. J Am Med Inform Assoc. 2025 May 1;32(5):876-884. doi: 10.1093/jamia/ocaf048.

Utilizing large language models for detecting hospital-acquired conditions: an 
empirical study on pulmonary embolism.

Cheligeer C(1)(2), Southern DA(1), Yan J(3), Wu G(1)(4), Pan J(1)(4), Lee 
S(1)(2), Martin EA(1)(2), Jafarpour H(3), Eastwood CA(1)(4), Zeng Y(3), Quan 
H(1)(4)(5).

Author information:
(1)Centre for Health Informatics, Cumming School of Medicine, University of 
Calgary, Calgary T2N 4N1, Canada.
(2)Provincial Research Data Services, Alberta Health Services, Calgary T2N 4N1, 
Canada.
(3)Concordia Institute for Information Systems Engineering, Concordia 
University, Montreal H3G 2W1, Canada.
(4)Department of Community Health Sciences, Cumming School of Medicine, 
University of Calgary, Calgary T2N 4N1, Canada.
(5)Libin Cardiovascular Institute, University of Calgary, Calgary T2N 4N1, 
Canada.

OBJECTIVES: Adverse event detection from Electronic Medical Records (EMRs) is 
challenging due to the low incidence of the event, variability in clinical 
documentation, and the complexity of data formats. Pulmonary embolism as an 
adverse event (PEAE) is particularly difficult to identify using existing 
approaches. This study aims to develop and evaluate a Large Language Model 
(LLM)-based framework for detecting PEAE from unstructured narrative data in 
EMRs.
MATERIALS AND METHODS: We conducted a chart review of adult patients (aged 
18-100) admitted to tertiary-care hospitals in Calgary, Alberta, Canada, between 
2017-2022. We developed an LLM-based detection framework consisting of three 
modules: evidence extraction (implementing both keyword-based and semantic 
similarity-based filtering methods), discharge information extraction (focusing 
on six key clinical sections), and PEAE detection. Four open-source LLMs 
(Llama3, Mistral-7B, Gemma, and Phi-3) were evaluated using positive predictive 
value, sensitivity, specificity, and F1-score. Model performance for 
population-level surveillance was assessed at yearly, quarterly, and monthly 
granularities.
RESULTS: The chart review included 10 066 patients, with 40 cases of PEAE 
identified (0.4% prevalence). All four LLMs demonstrated high sensitivity 
(87.5-100%) and specificity (94.9-98.9%) across different experimental 
conditions. Gemma achieved the highest F1-score (28.11%) using keyword-based 
retrieval with discharge summary inclusion, along with 98.4% specificity, 87.5% 
sensitivity, and 99.95% negative predictive value. Keyword-based filtering 
reduced the median chunks per patient from 789 to 310, while semantic filtering 
further reduced this to 9 chunks. Including discharge summaries improved 
performance metrics across most models. For population-level surveillance, all 
models showed strong correlation with actual PEAE trends at yearly granularity 
(r=0.92-0.99), with Llama3 achieving the highest correlation (0.988).
DISCUSSION: The results of our method for PEAE detection using EMR notes 
demonstrate high sensitivity and specificity across all four tested LLMs, 
indicating strong performance in distinguishing PEAE from non-PEAE cases. 
However, the low incidence rate of PEAE contributed to a lower PPV. The 
keyword-based chunking approach consistently outperformed semantic 
similarity-based methods, achieving higher F1 scores and PPV, underscoring the 
importance of domain knowledge in text segmentation. Including discharge 
summaries further enhanced performance metrics. Our population-based analysis 
revealed better performance for yearly trends compared to monthly granularity, 
suggesting the framework's utility for long-term surveillance despite dataset 
imbalance. Error analysis identified contextual misinterpretation, terminology 
confusion, and preprocessing limitations as key challenges for future 
improvement.
CONCLUSIONS: Our proposed method demonstrates that LLMs can effectively detect 
PEAE from narrative EMRs with high sensitivity and specificity. While these 
models serve as effective screening tools to exclude non-PEAE cases, their lower 
PPV indicates they cannot be relied upon solely for definitive PEAE 
identification. Further chart review remains necessary for confirmation. Future 
work should focus on improving contextual understanding, medical terminology 
interpretation, and exploring advanced prompting techniques to enhance precision 
in adverse event detection from EMRs.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocaf048
PMCID: PMC12012340
PMID: 40105654 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no competing interests.


457. J Pathol Inform. 2025 Feb 4;17:100422. doi: 10.1016/j.jpi.2025.100422. 
eCollection 2025 Apr.

A novel rotation and scale-invariant deep learning framework leveraging conical 
transformers for precise differentiation between meningioma and solitary fibrous 
tumor.

Azam MT(1), Balaha HM(1), Mistry A(2), Ali KM(3), Mobley BC(4), Leelatian N(4), 
Bhatia S(5), Gokden M(6), Lehman N(7), Ghazal M(8), El-Baz A(1), Gondim DD(9).

Author information:
(1)BioImaging Lab, Department of Bioengineering, University of Louisville, 
Louisville, KY, USA.
(2)Department of Neurosurgery, University of Louisville School of Medicine, 
Louisville, KY, USA.
(3)Department of Pathology, Faculty of Medicine, Mansoura University, Mansoura, 
Egypt.
(4)Department of Pathology, Vanderbilt University Medical Center, Nashville, TN, 
USA.
(5)Department of Neurosurgery, West Virginia University, Morgantown, WV, USA.
(6)Department of Pathology, University of Arkansas for Medical Sciences, Little 
Rock, AR, USA.
(7)Department of Pathology, Baylor Scott & White Hospital, Baylor College of 
Medicine, Temple, TX 76508, USA.
(8)Electrical, Computer, and Biomedical Engineering Department, Abu Dhabi 
University, Abu Dhabi, United Arab Emirates.
(9)Department of Pathology, University of Louisville School of Medicine, 
Louisville, KY, USA.

Meningiomas, the most prevalent tumors of the central nervous system, can have 
overlapping histopathological features with solitary fibrous tumors (SFT), 
presenting a significant diagnostic challenge. Accurate differentiation between 
these two diagnoses is crucial for optimal medical management. Currently, 
immunohistochemistry and molecular techniques are the methods of choice for 
distinguishing between them; however, these techniques are expensive and not 
universally available. In this article, we propose a rotational and 
scale-invariant deep learning framework to enable accurate discrimination 
between these two tumor types. The proposed framework employs a novel 
architecture of conical transformers to capture both global and local imaging 
markers from whole-slide images, accommodating variations across different 
magnification scales. A weighted majority voting schema is utilized to combine 
individual scale decisions, ultimately producing a complementary and more 
accurate diagnostic outcome. A dataset comprising 92 patients (46 with 
meningioma and 46 with SFT) was used for evaluation. The experimental results 
demonstrate robust performance across different validation methods. In 
train-test evaluation, the model achieved 92.27% accuracy, 87.77% sensitivity, 
97.55% specificity, and 92.46% F1-score. Performance further improved in 4-fold 
cross-validation, achieving 94.68% accuracy, 96.05% sensitivity, 93.11% 
specificity, and 95.07% F1-score. These findings highlight the potential of 
AI-based diagnostic approaches for precise differentiation between meningioma 
and SFT, paving the way for innovative diagnostic tools in pathology.

© 2025 The Authors.

DOI: 10.1016/j.jpi.2025.100422
PMCID: PMC11914819
PMID: 40104410

Conflict of interest statement: The authors declare that they have no known 
competing financial interests or personal relationships that could have appeared 
to influence the work reported in this article.


458. Sensors (Basel). 2025 Mar 3;25(5):1555. doi: 10.3390/s25051555.

IRFNet: Cognitive-Inspired Iterative Refinement Fusion Network for Camouflaged 
Object Detection.

Li G(1)(2), Wang J(1)(3), Wei J(1), Xu Z(1).

Author information:
(1)Shanghai Advanced Research Institute, Chinese Academy of Sciences, Shanghai 
201210, China.
(2)School of Electronic, Electrical and Communication Engineering, University of 
Chinese Academy of Sciences, Beijing 100049, China.
(3)School of Information Science and Technology, ShanghaiTech University, 
Shanghai 201210, China.

Camouflaged Object Detection (COD) aims to identify objects that are 
intentionally concealed within their surroundings through appearance, texture, 
or pattern adaptations. Despite recent advances, extreme object-background 
similarity causes existing methods struggle with accurately capturing 
discriminative features and effectively modeling multiscale patterns while 
preserving fine details. To address these challenges, we propose Iterative 
Refinement Fusion Network (IRFNet), a novel framework that mimics human visual 
cognition through progressive feature enhancement and iterative optimization. 
Our approach incorporates the following: (1) a Hierarchical Feature Enhancement 
Module (HFEM) coupled with a dynamic channel-spatial attention mechanism, which 
enriches multiscale feature representations through bilateral and trilateral 
fusion pathways; and (2) a Context-guided Iterative Optimization Framework 
(CIOF) that combines transformer-based global context modeling with iterative 
refinement through dual-branch supervision. Extensive experiments on three 
challenging benchmark datasets (CAMO, COD10K, and NC4K) demonstrate that IRFNet 
consistently outperforms fourteen state-of-the-art methods, achieving 
improvements of 0.9-13.7% across key metrics. Comprehensive ablation studies 
validate the effectiveness of each proposed component and demonstrate how our 
iterative refinement strategy enables progressive improvement in detection 
accuracy.

DOI: 10.3390/s25051555
PMCID: PMC11902440
PMID: 40096411

Conflict of interest statement: The authors declare no conflicts of interest.


459. Cleft Palate Craniofac J. 2025 Mar 17:10556656251327803. doi: 
10.1177/10556656251327803. Online ahead of print.

Readability of Online Patient Education Materials for Cleft Care: A Systematic 
Review and Meta-Analysis.

Nguyen AT(1), Li RA(2), Gosain AK(3), Galiano RD(2).

Author information:
(1)University of Rochester School of Medicine and Dentistry, Rochester, NY, USA.
(2)Department of Plastic and Reconstructive Surgery, Northwestern University 
Feinberg School of Medicine, Chicago, IL, USA.
(3)Division of Plastic Surgery, Ann & Robert H. Lurie Children's Hospital, 
Chicago, IL, USA.

ObjectiveTo evaluate the readability of online patient education materials 
(PEMs) for cleft lip and/or palate and assess their alignment with recommended 
readability levels.DesignThis study is a systematic review and 
meta-analysis.SettingLiterature search conducted in PubMed, Scopus, and Embase 
databases following PRISMA guidelines.MaterialsStudies evaluating online PEMs 
for cleft care with reported readability metrics, including Flesch-Kincaid Grade 
Level, Flesch Reading Ease, SMOG Index, or Gunning Fog 
Index.InterventionsAssessment of readability metrics of online PEMs and 
evaluation of artificial intelligence tools (eg, ChatGPT) for text 
simplification.Main Outcome Measure(s)Pooled readability estimates (eg, 
Flesch-Kincaid Grade Level, Flesch Reading Ease, SMOG Index, Gunning Fog Index), 
heterogeneity (I²), and confidence intervals (CIs).ResultsNine studies were 
included, consistently showing that PEMs exceed readability recommendations. 
Pooled estimates revealed a Flesch-Kincaid Grade Level of 9.48 (95% CI: 
8.51-10.45), Flesch Reading Ease score of 52.98 (95% CI: 42.62-63.34), SMOG 
Index of 9.27 (95% CI: 5.97-12.57), and Gunning Fog Index of 9.94 (95% CI: 
8.90-10.98). Heterogeneity was minimal (I² = 0%). Artificial intelligence tools 
like ChatGPT demonstrated potential in simplifying text to the recommended 
sixth-grade reading level but lacked usability and comprehension 
testing.ConclusionsOnline PEMs for cleft care are consistently written at 
reading levels too complex for the average caregiver, underscoring the need for 
improved readability and accessibility. Future research should focus on 
developing multimodal resources, conducting usability assessments, and including 
non-English materials to address global disparities in cleft care education.

DOI: 10.1177/10556656251327803
PMID: 40095963


460. Discov Oncol. 2025 Mar 17;16(1):334. doi: 10.1007/s12672-025-02111-3.

Advanced machine learning framework for enhancing breast cancer diagnostics 
through transcriptomic profiling.

Saadh MJ(1), Ahmed HH(2), Kareem RA(3), Yadav A(4), Ganesan S(5), Shankhyan 
A(6), Sharma GC(7), Naidu KS(8), Rakhmatullaev A(9), Sameer HN(10), Yaseen 
A(11), Athab ZH(12), Adil M(13), Farhood B(14).

Author information:
(1)Faculty of Pharmacy, Middle East University, Amman, 11831, Jordan.
(2)College of Pharmacy, Alnoor University, Mosul, Iraq.
(3)Ahl Al Bayt University, Kerbala, Iraq.
(4)Department of Computer Engineering and Application, GLA University, Mathura, 
281406, India.
(5)Department of Chemistry and Biochemistry, School of Sciences, JAIN (Deemed to 
Be University), Bangalore, Karnataka, India.
(6)Centre for Research Impact and Outcome, Chitkara University Institute of 
Engineering and Technology, Chitkara University, Rajpura, Punjab, 140401, India.
(7)Department of Applied Sciences-Chemistry, NIMS Institute of Engineering and 
Technology, NIMS University Rajasthan, Jaipur, India.
(8)Department of Chemistry, Raghu Engineering College, Visakhapatnam, Andhra 
Pradesh, 531162, India.
(9)Department of Faculty Pediatric Surgery, Tashkent Pediatric Medical 
Institute, Bogishamol Street 223, 100140, Tashkent, Uzbekistan.
(10)Collage of Pharmacy, National University of Science and Technology, Dhi Qar, 
64001, Iraq.
(11)Gilgamesh Ahliya University, Baghdad, Iraq.
(12)Department of Pharmacy, Al-Zahrawi University College, Karbala, Iraq.
(13)Pharmacy College, Al-Farahidi University, Baghdad, Iraq.
(14)Department of Medical Physics and Radiology, Faculty of Paramedical 
Sciences, Kashan University of Medical Sciences, Kashan, Iran. 
farhood-b@kaums.ac.ir.

PURPOSE: This study proposes an advanced machine learning (ML) framework for 
breast cancer diagnostics by integrating transcriptomic profiling with optimized 
feature selection and classification techniques.
MATERIALS AND METHODS: A dataset of 1759 samples (987 breast cancer patients, 
772 healthy controls) was analyzed using Recursive Feature Elimination, Boruta, 
and ElasticNet for feature selection. Dimensionality reduction techniques, 
including Non-Negative Matrix Factorization (NMF), Autoencoders, and 
transformer-based embeddings (BioBERT, DNABERT), were applied to enhance model 
interpretability. Classifiers such as XGBoost, LightGBM, ensemble voting, 
Multi-Layer Perceptron, and Stacking were trained using grid search and 
cross-validation. Model evaluation was conducted using accuracy, AUC, MCC, Kappa 
Score, ROC, and PR curves, with external validation performed on an independent 
dataset of 175 samples.
RESULTS: XGBoost and LightGBM achieved the highest test accuracies (0.91 and 
0.90) and AUC values (up to 0.92), particularly with NMF and BioBERT. The 
ensemble Voting method exhibited the best external accuracy (0.92), confirming 
its robustness. Transformer-based embeddings and advanced feature selection 
techniques significantly improved model performance compared to conventional 
approaches like PCA and Decision Trees.
CONCLUSION: The proposed ML framework enhances diagnostic accuracy and 
interpretability, demonstrating strong generalizability on an external dataset. 
These findings highlight its potential for precision oncology and personalized 
breast cancer diagnostics.

© 2025. The Author(s).

DOI: 10.1007/s12672-025-02111-3
PMCID: PMC11914415
PMID: 40095253

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests. Ethics approval and consent to participate: Not 
applicable. Consent for publication: Not applicable. Clinical trial number: Not 
applicable.


461. Comput Methods Programs Biomed. 2025 Jun;264:108717. doi: 
10.1016/j.cmpb.2025.108717. Epub 2025 Mar 13.

Linguistic-grammar profile of Polish patients with anorexia nervosa.

Maćkowska S(1), Rojewska K(2), Spinczyk D(3).

Author information:
(1)Department of Medical Informatics and Artificial Intelligence, Faculty of 
Biomedical Engineering, Silesian University of Technology, Roosevelt a 40, 
41-800 Zabrze, Poland.
(2)Central Laboratory of Clinical Psychology, Pediatric Endocrinology, Public 
Clinical Hospital No 1 in Zabrze, 41-800 Zabrze, Poland.
(3)Department of Medical Informatics and Artificial Intelligence, Faculty of 
Biomedical Engineering, Silesian University of Technology, Roosevelt a 40, 
41-800 Zabrze, Poland. Electronic address: dspinczyk@polsl.pl.

BACKGROUND AND OBJECTIVE: The process of diagnosing and treating anorexia is 
fraught with many challenges. Physiologically unstable patient status in the 
first period of treatment, the barrier between patient-therapist, and patient's 
resistance constitute an essential negative element in accurate diagnosis and 
appropriate therapy selection. For this reason, there was a need to create a 
tool using elements of natural language processing to support the psychologist's 
work in the diagnostic process to verify and validate the expert hypotheses.
METHODS: The research proposed that linguistic-grammatical profiles be created 
among the research and control groups using elements of natural language 
processing. After the general part of speech tagging, the rules for detailed 
analysis were developed for adjectives, verbs (including the verb "to be"), 
pronoun "I" and the possessive pronoun "my", cognitive words and characteristic 
terms related to body image. The choice of rules was dictated by the state of 
art and literature review. The obtained results were subjected to statistical 
analysis.
RESULTS: A detailed analysis showed a strong negative sentiment associated with 
body image among patients with anorexia. In the control group, the same analysis 
revealed opposite results. In this group, people are aware of their physical 
imperfections, but it does not distort their body image. Statistically 
significant differences were observed in all concept categories except for the 
noun group. Statistical analysis was not conducted for the following concept 
classes: personal pronoun "I", verb "to be" in the past form, verb "to be" in 
the future form, and general verbs in past form due to the insufficient number 
of occurrences of these concepts in the written notes.
CONCLUSION: The adopted NLP methods and the tools used in the designed 
projective method may be helpful in the psychological diagnosis of anorexia, due 
to the demonstrated differentiation between healthy and people with anorexia, 
providing detailed information about the patient and its required minimally 
invasive character.

Copyright © 2025 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.cmpb.2025.108717
PMID: 40086174 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


462. Recenti Prog Med. 2025 Mar;116(3):156-161. doi: 10.1701/4460.44555.

Towards artificial intelligence application in pain medicine.

Cascella M(1), Leoni MLG(2), Shariff MN(3), Varrassi G(4).

Author information:
(1)Anesthesia and Pain Medicine, Department of Medicine, Surgery and Dentistry 
Scuola Medica Salernitana, University of Salerno, Baronissi, Italy.
(2)Department of Medical and Surgical Sciences and Translational Medicine, 
Sapienza University of Rome, Italy.
(3)Department of AI&DS, Rajalakshmi Institute of Technology, Chennai, Tamil 
Nadu, India.
(4)Department of Research, Fondazione Paolo Procacci, Rome, Italy.

Pain is a complex, multidimensional experience involving significant challenges 
in both diagnosis and management. While acute pain serves as a critical warning 
mechanism, chronic pain encompasses intricate biological, psychological, and 
social components, complicating its assessment and treatment. Artificial 
intelligence (AI) technologies are revolutionizing medicine and healthcare. Here 
we present an overview of the recent advances in AI for pain medicine. For 
example, the emergence of automatic pain assessment (APA) methodologies offers 
promising avenues for more objective pain evaluation. For APA aims, AI 
technologies, including machine learning algorithms and deep learning 
architectures such as natural language processing systems, have shown potential 
in analyzing biosignals, facial expressions, and speech patterns related to 
pain. However, the integration of these objective measures with traditional 
self-reporting remains essential for a comprehensive approach to pain diagnosis. 
Notably, APA models can be implemented for pain diagnosis in newborn and 
non-communicative patients. Additionally, the application of AI extends beyond 
pain diagnosis to personalized treatment strategies, predict opioid use 
disorders, education and training, clinical trajectory definition, and 
telehealth and real-time. Despite the potential of these innovations, challenges 
such as validation, parameter selection, and ethical aspects of technical 
implementation must be addressed.

DOI: 10.1701/4460.44555
PMID: 40084580 [Indexed for MEDLINE]


463. Pediatr Transplant. 2025 May;29(3):e70068. doi: 10.1111/petr.70068.

The Potential of ChatGPT as a Source of Information for Kidney Transplant 
Recipients and Their Caregivers.

Demirbaş KC(1), Saygılı S(2), Yılmaz EK(2), Gülmez R(3), Ağbaş A(2), Taşdemir 
M(4), Canpolat N(2).

Author information:
(1)Department of Pediatrics, Istanbul University-Cerrahpaşa, Cerrahpaşa School 
of Medicine, Istanbul, Türkiye.
(2)Division of Pediatric Nephrology, Department of Pediatrics, Istanbul 
University-Cerrahpaşa, Cerrahpaşa School of Medicine, Istanbul, Türkiye.
(3)Division of Pediatric Nephrology, Department of Pediatrics, Istanbul Prof. 
Dr. Suleyman Yalcin Research and Training Hospital, Istanbul, Türkiye.
(4)Division of Pediatric Nephrology, Department of Pediatrics, Istinye 
University School of Medicine, Istanbul, Türkiye.

BACKGROUND: Education and enhancing the knowledge of adolescents who will 
undergo kidney transplantation are among the primary objectives of their care. 
While there are specific interventions in place to achieve this, they require 
extensive resources. The rise of large language models like ChatGPT-3.5 offers 
potential assistance for providing information to patients. This study aimed to 
evaluate the accuracy, relevance, and safety of ChatGPT-3.5's responses to 
patient-centered questions about pediatric kidney transplantation. The objective 
was to assess whether ChatGPT-3.5 could be a supplementary educational tool for 
adolescents and their caregivers in a complex medical context.
METHODS: A total of 37 questions about kidney transplantation were presented to 
ChatGPT-3.5, which was prompted to respond as a health professional would to a 
layperson. Five pediatric nephrologists independently evaluated the outputs for 
accuracy, relevance, comprehensiveness, understandability, readability, and 
safety.
RESULTS: The mean accuracy, relevancy, and comprehensiveness scores for all 
outputs were 4.51, 4.56, and 4.55, respectively. Out of 37 outputs, four were 
rated as completely accurate, and seven were completely relevant and 
comprehensive. Only one output had an accuracy, relevancy, and comprehensiveness 
score below 4. Twelve outputs were considered potentially risky, but only three 
had a risk grade of moderate or higher. Outputs that were considered risky had 
an accuracy and relevancy below the average.
CONCLUSION: Our findings suggest that ChatGPT could be a useful tool for 
adolescents or caregivers of individuals waiting for kidney transplantation. 
However, the presence of potentially risky outputs underscores the necessity for 
human oversight and validation.

© 2025 Wiley Periodicals LLC.

DOI: 10.1111/petr.70068
PMID: 40078030 [Indexed for MEDLINE]


464. Int Dent J. 2025 Apr;75(2):383-396. doi: 10.1016/j.identj.2025.02.005. Epub 2025 
Mar 11.

The Transformative Role of Artificial Intelligence in Dentistry: A Comprehensive 
Overview. Part 1: Fundamentals of AI, and its Contemporary Applications in 
Dentistry.

Samaranayake L(1), Tuygunov N(2), Schwendicke F(3), Osathanon T(4), Khurshid 
Z(5), Boymuradov SA(6), Cahyanto A(7).

Author information:
(1)Center of Excellence for Dental Stem Cell Biology, Faculty of Dentistry, 
Chulalongkorn University, Bangkok, Thailand; Faculty of Dentistry, University of 
Hong Kong, Sai Ying Pun, Hong Kong; Dr DY Patil Dental College and Hospital, Dr 
DY Patil Vidyapeeth, Pimpri, Pune, India. Electronic address: lakshman@hku.hk.
(2)Faculty of Dentistry, Kimyo International University in Tashkent, Tashkent, 
Uzbekistan. Electronic address: nozimtuygunov@gmail.com.
(3)Clinic for Conservative Dentistry and Periodontology, University Hospital of 
the Ludwig-Maximilians-University Munich, Munich, Germany.
(4)Center of Excellence for Dental Stem Cell Biology, Faculty of Dentistry, 
Chulalongkorn University, Bangkok, Thailand.
(5)Center of Excellence for Dental Stem Cell Biology, Faculty of Dentistry, 
Chulalongkorn University, Bangkok, Thailand; Department of Prosthodontics and 
Dental Implantology, College of Dentistry, King Faisal University, Al-Ahsa, 
Saudi Arabia.
(6)Department of oral and maxillofacial surgery, Faculty of Dentistry, Tashkent 
Medical Academy, Tashkent, Uzbekistan.
(7)Department of Restorative Dentistry, College of Dentistry, Ajman University, 
Ajman, United Arab Emirates; Centre of Medical and Bio-allied Health Sciences 
Research, Ajman University, Ajman, United Arab Emirates.

Artificial intelligence (AI) holds immense promise in revolutionising dentistry, 
spanning, diagnostics, treatment planning and educational realms. This narrative 
review, in two parts, explores the fundamentals and the multifaceted potential 
of AI in dentistry. The current article explores the profound impact of AI in 
dentistry, encompassing diagnostic tools, treatment planning, and patient care. 
The Part 2 of the article delves into the potential of AI in patient education, 
ethics and the FDI communique on AI in dentistry. The review begins by 
elucidating the historical context of AI, outlining its recent widespread use in 
various sectors, including medicine and dentistry. The narrative delves into the 
fundamental concepts of AI, which entails developing machines capable of 
executing tasks that typically necessitate human intellect. In the biomedical 
realm, AI has evolved from exploring computational models to constructing 
systems for clinical data processing and interpretation, aiming to enhance 
medical/dental decision-making. The discussion delves into the pivotal role of 
AI models in dentistry, such as Large Language Models (LLM), Large Vision Models 
(LVM), and Multimodality Models (MM), revolutionizing processes from clinical 
documentation to treatment planning. The narrative extends to the applications 
of AI in dental specialties such as periodontics, endodontics, oral medicine and 
pathology, restorative dentistry, prosthodontics, paediatric dentistry, forensic 
odontology, oral and maxillofacial surgery, orthodontics, and orofacial pain 
management. AI's role in improving treatment outcomes, diagnostic accuracy, and 
decision-making processes is evident across these specialties, showcasing its 
potential in transforming dental care. The review concludes by highlighting the 
need for continued validation, interdisciplinary collaboration, and regulatory 
frameworks to ensure the seamless integration of AI into dentistry, paving the 
way for enhanced patient outcomes and evidence-based practice in the field.

Copyright © 2025 The Authors. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.identj.2025.02.005
PMCID: PMC11976540
PMID: 40074616 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of interest All the authors declare no 
conflict of interest.


465. J Med Internet Res. 2025 Mar 11;27:e67488. doi: 10.2196/67488.

Accuracy of Large Language Models for Literature Screening in Thoracic Surgery: 
Diagnostic Study.

Dai ZY(1), Wang FQ(1), Shen C(1), Ji YL(1), Li ZY(1), Wang Y(1), Pu Q(1).

Author information:
(1)Department of Thoracic Surgery, West China Hospital of Sichuan University, 
Chengdu, China.

BACKGROUND: Systematic reviews and meta-analyses rely on labor-intensive 
literature screening. While machine learning offers potential automation, its 
accuracy remains suboptimal. This raises the question of whether emerging large 
language models (LLMs) can provide a more accurate and efficient approach.
OBJECTIVE: This paper evaluates the sensitivity, specificity, and summary 
receiver operating characteristic (SROC) curve of LLM-assisted literature 
screening.
METHODS: We conducted a diagnostic study comparing the accuracy of LLM-assisted 
screening versus manual literature screening across 6 thoracic surgery 
meta-analyses. Manual screening by 2 investigators served as the reference 
standard. LLM-assisted screening was performed using ChatGPT-4o (OpenAI) and 
Claude-3.5 (Anthropic) sonnet, with discrepancies resolved by Gemini-1.5 pro 
(Google). In addition, 2 open-source, machine learning-based screening tools, 
ASReview (Utrecht University) and Abstrackr (Center for Evidence Synthesis in 
Health, Brown University School of Public Health), were also evaluated. We 
calculated sensitivity, specificity, and 95% CIs for the title and abstract, as 
well as full-text screening, generating pooled estimates and SROC curves. LLM 
prompts were revised based on a post hoc error analysis.
RESULTS: LLM-assisted full-text screening demonstrated high pooled sensitivity 
(0.87, 95% CI 0.77-0.99) and specificity (0.96, 95% CI 0.91-0.98), with the area 
under the curve (AUC) of 0.96 (95% CI 0.94-0.97). Title and abstract screening 
achieved a pooled sensitivity of 0.73 (95% CI 0.57-0.85) and specificity of 0.99 
(95% CI 0.97-0.99), with an AUC of 0.97 (95% CI 0.96-0.99). Post hoc revisions 
improved sensitivity to 0.98 (95% CI 0.74-1.00) while maintaining high 
specificity (0.98, 95% CI 0.94-0.99). In comparison, the pooled sensitivity and 
specificity of ASReview tool-assisted screening were 0.58 (95% CI 0.53-0.64) and 
0.97 (95% CI 0.91-0.99), respectively, with an AUC of 0.66 (95% CI 0.62-0.70). 
The pooled sensitivity and specificity of Abstrackr tool-assisted screening were 
0.48 (95% CI 0.35-0.62) and 0.96 (95% CI 0.88-0.99), respectively, with an AUC 
of 0.78 (95% CI 0.74-0.82). A post hoc meta-analysis revealed comparable effect 
sizes between LLM-assisted and conventional screening.
CONCLUSIONS: LLMs hold significant potential for streamlining literature 
screening in systematic reviews, reducing workload without sacrificing quality. 
Importantly, LLMs outperformed traditional machine learning-based tools 
(ASReview and Abstrackr) in both sensitivity and AUC values, suggesting that 
LLMs offer a more accurate and efficient approach to literature screening.

©Zhang-Yi Dai, Fu-Qiang Wang, Cheng Shen, Yan-Li Ji, Zhi-Yang Li, Yun Wang, 
Qiang Pu. Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 11.03.2025.

DOI: 10.2196/67488
PMCID: PMC11937709
PMID: 40068152 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


466. Radiology. 2025 Mar;314(3):e241646. doi: 10.1148/radiol.241646.

Value of Using a Generative AI Model in Chest Radiography Reporting: A Reader 
Study.

Hong EK(1)(2), Roh B(3), Park B(3), Jo JB(4), Bae W(4), Soung Park J(5), Sung 
DW(6).

Author information:
(1)Department of Radiology, Mass General Brigham, Boston, Mass.
(2)Department of Radiology, Brigham & Women's Hospital, 75 Francis St, Boston, 
MA 02115.
(3)Kakaocorp, Seoul, South Korea.
(4)Soombit.ai, Seoul, South Korea.
(5)Department of Radiology, Soonchunhyang University College of Medicine, 
Cheonan, South Korea.
(6)Department of Radiology, Kyung Hee University School of Medicine, Seoul, 
South Korea.

Background Multimodal generative artificial intelligence (AI) technologies can 
produce preliminary radiology reports, and validation with reader studies is 
crucial for understanding the clinical value of these technologies. Purpose To 
assess the clinical value of the use of a domain-specific multimodal generative 
AI tool for chest radiograph interpretation by means of a reader study. 
Materials and Methods A retrospective, sequential, multireader, multicase reader 
study was conducted using 758 chest radiographs from a publicly available 
dataset from 2009 to 2017. Five radiologists interpreted the chest radiographs 
in two sessions: without AI-generated reports and with AI-generated reports as 
preliminary reports. Reading times, reporting agreement (RADPEER), and quality 
scores (five-point scale) were evaluated by two experienced thoracic 
radiologists and compared between the first and second sessions from October to 
December 2023. Reading times, report agreement, and quality scores were analyzed 
using a generalized linear mixed model. Additionally, a subset of 258 chest 
radiographs was used to assess the factual correctness of the reports, and 
sensitivities and specificities were compared between the reports from the first 
and second sessions with use of the McNemar test. Results The introduction of 
AI-generated reports significantly reduced average reading times from 34.2 
seconds ± 20.4 to 19.8 seconds ± 12.5 (P < .001). Report agreement scores 
shifted from a median of 5.0 (IQR, 4.0-5.0) without AI reports to 5.0 (IQR, 
4.5-5.0) with AI reports (P < .001). Report quality scores changed from 4.5 
(IQR, 4.0-5.0) without AI reports to 4.5 (IQR, 4.5-5.0) with AI reports (P < 
.001). From the subset analysis of factual correctness, the sensitivity for 
detecting various abnormalities increased significantly, including widened 
mediastinal silhouettes (84.3% to 90.8%; P < .001) and pleural lesions (77.7% to 
87.4%; P < .001). While the overall diagnostic performance improved, variability 
among individual radiologists was noted. Conclusion The use of a domain-specific 
multimodal generative AI model increased the efficiency and quality of radiology 
report generation. © RSNA, 2025 Supplemental material is available for this 
article. See also the editorial by Babyn and Adams in this issue.

DOI: 10.1148/radiol.241646
PMID: 40067108 [Indexed for MEDLINE]


467. Front Vet Sci. 2025 Feb 24;11:1504598. doi: 10.3389/fvets.2024.1504598. 
eCollection 2024.

Introducing AI-generated cases (AI-cases) & standardized clients (AI-SCs) in 
communication training for veterinary students: perceptions and adoption 
challenges.

Artemiou E(1), Hooper S(2)(3), Dascanio L(1), Schmidt M(1), Gilbert G(1).

Author information:
(1)Texas Tech University School of Veterinary Medicine, Amarillo, TX, United 
States.
(2)Department of Biomedical Sciences, Ross University School of Veterinary 
Medicine, St. Kitts, West Indies.
(3)Arkansas State University College of Veterinary Medicine, Jonesboro, AR, 
United States.

INTRODUCTION: The integration of Artificial Intelligence (AI) into medical 
education and healthcare has grown steadily over these past couple of years, 
though its application in veterinary education and practice remains relatively 
underexplored. This study is among the first to introduce veterinary students to 
AI-generated cases (AI-cases) and AI-standardized clients (AI-SCs) for teaching 
and learning communication skills. The study aimed to evaluate students' beliefs 
and perceptions surrounding the use of AI in veterinary education, with specific 
focus on communication skills training.
METHODS: Conducted at Texas Tech University School of Veterinary Medicine (TTU 
SVM) during the Spring 2024 semester, the study included pre-clinical veterinary 
students (n = 237), who participated in a 90-min communication skills laboratory 
activity. Each class was introduced to two AI-cases and two AI-SCs, developed 
using OpenAI's ChatGPT-3.5. The Calgary Cambridge Guide (CCG) served as the 
framework for practicing communication skills.
RESULTS: Results showed that although students recognized the widespread use of 
AI in everyday life, their familiarity, comfort and application of AI in 
veterinary education were limited. Notably, upper-year students were more 
hesitant to adopt AI-based tools, particularly in communication skills training.
DISCUSSION: The findings suggest that veterinary institutions should prioritize 
AI-literacy and further explore how AI can enhance and complement communication 
training, veterinary education and practice.

Copyright © 2025 Artemiou, Hooper, Dascanio, Schmidt and Gilbert.

DOI: 10.3389/fvets.2024.1504598
PMCID: PMC11892109
PMID: 40066104

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


468. J Dent. 2025 May;156:105668. doi: 10.1016/j.jdent.2025.105668. Epub 2025 Mar 8.

Assessment of CNNs, transformers, and hybrid architectures in dental image 
segmentation.

Schneider L(1), Krasowski A(2), Pitchika V(2), Bombeck L(3), Schwendicke F(4), 
Büttner M(2).

Author information:
(1)Department of Oral Diagnostics, Digital Health and Health Services Research, 
Charité - Universitätsmedizin, Berlin, Germany.
(2)Clinic for Conservative Dentistry and Periodontology, 
Ludwig-Maximilians-University, Munich, Germany.
(3)Department of Operative, Preventive and Pediatric Dentistry, Charité - 
Universitätsmedizin, Berlin, Germany.
(4)Clinic for Conservative Dentistry and Periodontology, 
Ludwig-Maximilians-University, Munich, Germany. Electronic address: 
falk.schwendicke@med.uni-muenchen.de.

OBJECTIVES: Convolutional Neural Networks (CNNs) have long dominated image 
analysis in dentistry, reaching remarkable results in a range of different 
tasks. However, Transformer-based architectures, originally proposed for Natural 
Language Processing, are also promising for dental image analysis. The present 
study aimed to compare CNNs with Transformers for different image analysis tasks 
in dentistry.
METHODS: Two CNNs (U-Net, DeepLabV3+), two Hybrids (SwinUNETR, UNETR) and two 
Transformer-based architectures (TransDeepLab, SwinUnet) were compared on three 
dental segmentation tasks on different image modalities. Datasets consisted of 
(1) 1881 panoramic radiographs used for tooth segmentation, (2) 1625 bitewings 
used for tooth structure segmentation, and (3) 2689 bitewings for caries lesions 
segmentation. All models were trained and evaluated using 5-fold 
cross-validation.
RESULTS: CNNs were found to be significantly superior over Hybrids and 
Transformer-based architectures for all three tasks. (1) Tooth segmentation 
showed mean±SD F1-Score of 0.89±0.009 for CNNs, 0.86±0.015 for Hybrids and 
0.83±0.22 for Transformer-based architectures. (2) In tooth structure 
segmentation CNNs also outperformed with 0.85±0.008 compared to Hybrids 
0.84±0.005 and Transformers 0.83±0.011. (3) Even more pronounced results were 
found for caries lesions segmentation; 0.49±0.031 for CNNs, 0.39±0.072 for 
Hybrids and 0.32±0.039 for Transformer-based architectures.
CONCLUSION: CNNs significantly outperformed Transformer-based architectures and 
their Hybrids on three segmentation tasks (teeth, tooth structures, caries 
lesions) on varying dental data modalities (panoramic and bitewing radiographs).
CLINICAL SIGNIFICANCE: As deep-learning-based image analysis is part of modern 
dentistry, practitioners and dental researchers should be aware of strength and 
limitations of modern model architectures for dental-image analysis. Models that 
demonstrate optimal performance in other domains do not necessarily constitute 
the optimal selection for the purpose of dental imaging.

Copyright © 2025 The Author(s). Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.jdent.2025.105668
PMID: 40064460 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare the following financial interests/personal relationships which may be 
considered as potential competing interests: Falk Schwendicke is co-founder of 
the startup dentalXrai GmbH. dentalXrai GmbH did not have any role in 
conceiving, conducting or reporting this study. The authors are solely 
responsible for the contents of this paper. If there are other authors, they 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


469. PLoS One. 2025 Mar 10;20(3):e0319463. doi: 10.1371/journal.pone.0319463. 
eCollection 2025.

Analysing how AI-powered chatbots influence destination decisions.

Orden-Mejía M(1), Carvache-Franco M(2), Huertas A(3), Carvache-Franco O(4), 
Carvache-Franco W(5).

Author information:
(1)Facultat de Turisme i Geografia, Universitat Rovira I Virgili, Carrer Joanot 
Martorell, Vila-seca, Spain.
(2)Universidad Bolivariana del Ecuador, Durán, Ecuador.
(3)Department of Communication, Universitat Rovira i Virgili, Tarragona, Spain.
(4)Universidad Espíritu Santo, Samborondón, Ecuador.
(5)Facultad de Ciencias Sociales y Humanísticas, Escuela Superior Politécnica 
del Litoral, ESPOL, Guayaquil, Ecuador.

This study aims to explore the role of destination chatbots as innovative tools 
in travel planning, focusing on their ability to enhance user experiences and 
influence decision-making processes. Based on the Technology Acceptance Model, 
Enterprise Content Management, and Information Systems Security models, the 
study examines the psychological, emotional, and technological factors that 
drive user satisfaction, continued use, and intention to visit a destination. 
Understanding these factors is crucial for improving chatbot design and 
optimizing their implementation in the tourism industry. A total of 312 
responses were collected from university students who regularly engage in 
tourism-related activities. The survey employed a structured questionnaire with 
items measuring information quality, user satisfaction, perceived enjoyment, 
usefulness, and behavioral intentions using a 7-point Likert scale. Structural 
equation modelling [SEM] was used to analyze the relationships between 
constructs, allowing us to evaluate the validity and reliability of the model. 
The results reveal that information quality positively enhances user 
satisfaction, perceived enjoyment, and perceived usefulness. Moreover, perceived 
enjoyment and usefulness are critical psychological and emotional drivers 
influencing users' decision to continue utilizing chatbots. Additionally, the 
analysis highlights the intention to continue using destination chatbots as a 
strong predictor of tourists' intention to visit the destination. The findings 
contribute to the theoretical understanding of technology acceptance and user 
behavior in tourism, while providing practical insights for destination managers 
and developers to enhance chatbot features and improve traveler engagement.

Copyright: © 2025 Orden-Mejía et al. This is an open access article distributed 
under the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.

DOI: 10.1371/journal.pone.0319463
PMCID: PMC11893117
PMID: 40063866 [Indexed for MEDLINE]

Conflict of interest statement: The authors have declared that no competing 
interests exist.


470. Expert Rev Med Devices. 2025 Apr;22(4):293-310. doi: 
10.1080/17434440.2025.2477601. Epub 2025 Mar 19.

An overview of utilizing artificial intelligence in localized prostate cancer 
imaging.

Stevenson E(1), Esengur OT(1), Zhang H(1), Simon BD(1)(2), Harmon SA(1), Turkbey 
B(1).

Author information:
(1)Molecular Imaging Branch, National Cancer Institute, National Institutes of 
Health, Bethesda, MD, USA.
(2)Institute of Biomedical Engineering, Department of Engineering Science, 
University of Oxford, Oxford, UK.

INTRODUCTION: Prostate cancer (PCa) is a leading cause of cancer-related deaths 
among men, and accurate diagnosis is critical for effective management. 
Multiparametric MRI (mpMRI) has become an essential tool in PCa diagnosis due to 
its superior spatial resolution which enables detailed anatomical, functional 
information and its resultant ability to detect clinically significant PCa. 
However, challenges such as subjective interpretation methods and high 
inter-reader variability remain. In recent years, artificial intelligence (AI) 
has emerged as a promising solution to enhance the diagnostic performance of 
mpMRI by automating key tasks such as prostate segmentation, lesion detection, 
classification.
AREAS COVERED: This review provides a comprehensive overview of the current AI 
applications in prostate mpMRI, discussing advancements in automated image 
analysis and how AI-driven models are developed to improve detection and risk 
stratification. A literature search was conducted to examine both machine 
learning and deep learning techniques applied in this field, highlighting key 
studies and future directions.
EXPERT OPINION: While AI models have shown significant promise, their clinical 
integration remains limited due to the need for larger, multi-institutional 
validation studies. As AI continues to evolve, multimodal approaches combining 
imaging with clinical data are likely to play pivotal role in personalized PCa 
diagnosis, treatment planning.

DOI: 10.1080/17434440.2025.2477601
PMCID: PMC12038709
PMID: 40056148 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Interests Baris Turkbey: 
Cooperative Research and Development Agreements with NVIDIA and Philips; 
royalties from the National Institutes of Health; patents in the field of 
artificial intelligence. Other authors have no relevant affiliations or 
financial involvement with any organization or entity with a financial interest 
in or financial conflict with the subject matter or materials discussed in the 
manuscript. This includes employment, consultancies, honoraria, stock ownership 
or options, expert testimony, grants or patents received or pending, or 
royalties.


471. NPJ Digit Med. 2025 Mar 7;8(1):149. doi: 10.1038/s41746-025-01542-0.

Red teaming ChatGPT in medicine to yield real-world insights on model behavior.

Chang CT(#)(1)(2), Farah H(#)(1), Gui H(#)(1)(3), Rezaei SJ(3), Bou-Khalil C(3), 
Park YJ(4), Swaminathan A(3), Omiye JA(1)(5), Kolluri A(6), Chaurasia A(7)(8), 
Lozano A(5), Heiman A(6), Jia AS(6), Kaushal A(9), Jia A(6), Iacovelli A(10), 
Yang A(5)(11), Salles A(6), Singhal A(7), Narasimhan B(6), Belai B(12), Jacobson 
BH(3), Li B(5), Poe CH(3), Sanghera C(6), Zheng C(3), Messer C(6), Kettud DV(6), 
Pandya D(6), Kaur D(3), Hla D(13), Dindoust D(6), Moehrle D(3), Ross D(14), Chou 
E(5), Lin E(15), Haredasht FN(8), Cheng G(5), Gao I(6), Chang J(5), Silberg 
J(5), Fries JA(8), Xu J(5), Jamison J(14), Tamaresis JS(5), Chen JH(2)(8)(16), 
Lazaro J(5), Banda JM(17), Lee JJ(10), Matthys KE(5), Steffner KR(18), Tian 
L(6), Pegolotti L(10), Srinivasan M(3), Manimaran M(19), Schwede M(16), Zhang 
M(14), Nguyen M(6), Fathzadeh M(20), Zhao Q(5), Bajra R(3), Khurana R(5), Azam 
R(6), Bartlett R(21), Truong ST(7), Fleming SL(5), Raj S(8), Behr S(22), Onyeka 
S(1), Muppidi S(6), Bandali T(6), Eulalio TY(5), Chen W(5), Zhou X(20), Ding 
Y(5)(23)(24), Cui Y(6), Tan Y(25), Liu Y(20), Shah N(3)(5), Daneshjou R(26)(27).

Author information:
(1)Department of Dermatology, Stanford University, Stanford, USA.
(2)Clinical Excellence Research Center, School of Medicine, Stanford University, 
Palo Alto, CA, USA.
(3)School of Medicine, Stanford University, Stanford, CA, USA.
(4)Temerty Faculty of Medicine, Toronto, ON, Canada.
(5)Department of Biomedical Data Science, Stanford University, Stanford, CA, 
USA.
(6)Stanford University, Stanford, CA, USA.
(7)Department of Computer Science, Stanford University, Stanford, CA, USA.
(8)Center for Biomedical Informatics Research, Stanford University, Stanford, 
CA, USA.
(9)Department of Bioengineering, Stanford University, Stanford, CA, USA.
(10)Department of Pediatrics, Stanford University, Stanford, CA, USA.
(11)Department of Mathematics and Statistics, McGill University, Montreal, QC, 
Canada.
(12)Department of Psychiatry, Stanford University, Stanford, CA, USA.
(13)Mayo Clinic Alix School of Medicine, Rochester, NY, USA.
(14)Department of Statistics, Stanford University, Stanford, CA, USA.
(15)Veterans Affairs Medical Center, Palo Alto, CA, USA.
(16)Department of Medicine, Stanford University, Stanford, CA, USA.
(17)Technology and Digital Solutions, Stanford Health Care, Palo Alto, CA, USA.
(18)Department of Anesthesiology, Stanford University, Stanford, CA, USA.
(19)Graduate School of Business, Stanford University, Stanford, CA, USA.
(20)Department of Epidemiology and Population Health, Stanford University, 
Stanford, CA, USA.
(21)Stanford BioDesign, Stanford University, Stanford, CA, USA.
(22)Department of Education and Psychology, Freie Universität Berlin, Berlin, 
Germany.
(23)Department of Genetics, Stanford School of Medicine, Stanford, CA, USA.
(24)Department of Clinical and Translational Science, Harvard Medical School, 
Boston, MA, USA.
(25)Department of Pathology, Stanford University, Stanford, CA, USA.
(26)Department of Dermatology, Stanford University, Stanford, USA. 
roxanad@stanford.edu.
(27)School of Medicine, Stanford University, Stanford, CA, USA. 
roxanad@stanford.edu.
(#)Contributed equally

Red teaming, the practice of adversarially exposing unexpected or undesired 
model behaviors, is critical towards improving equity and accuracy of large 
language models, but non-model creator-affiliated red teaming is scant in 
healthcare. We convened teams of clinicians, medical and engineering students, 
and technical professionals (80 participants total) to stress-test models with 
real-world clinical cases and categorize inappropriate responses along axes of 
safety, privacy, hallucinations/accuracy, and bias. Six medically-trained 
reviewers re-analyzed prompt-response pairs and added qualitative annotations. 
Of 376 unique prompts (1504 responses), 20.1% were inappropriate (GPT-3.5: 
25.8%; GPT-4.0: 16%; GPT-4.0 with Internet: 17.8%). Subsequently, we show the 
utility of our benchmark by testing GPT-4o, a model released after our event 
(20.4% inappropriate). 21.5% of responses appropriate with GPT-3.5 were 
inappropriate in updated models. We share insights for constructing red teaming 
prompts, and present our benchmark for iterative model assessments.

© 2025. The Author(s).

DOI: 10.1038/s41746-025-01542-0
PMCID: PMC11889229
PMID: 40055532

Conflict of interest statement: Competing interests: RD has served as an advisor 
to MDAlgorithms and Revea and received. consulting fees from Pfizer, L’Oreal, 
Frazier Healthcare Partners, and DWA, and research funding from UCB and declares 
no non-financial competing interests. All other authors declare no financial or 
non-financial competing interests.


472. JMIR Med Educ. 2025 Mar 3;11:e62779. doi: 10.2196/62779.

Detecting Artificial Intelligence-Generated Versus Human-Written Medical Student 
Essays: Semirandomized Controlled Study.

Doru B(1), Maier C(1), Busse JS(1), Lücke T(1), Schönhoff J(2), Enax-Krumova 
E(3), Hessler S(4), Berger M(#)(5), Tokic M(#)(6).

Author information:
(1)University Hospital of Paediatrics and Adolescent Medicine, St. 
Josef-Hospital, Ruhr University Bochum, Bochum, Germany.
(2)Departement of German Philology, General and Comparative Literary Studies, 
Ruhr University Bochum, Bochum, Germany.
(3)Department of Neurology, BG University Hospital Bergmannsheil gGmbH Bochum, 
Ruhr University Bochum, Bochum, Germany.
(4)German Department, German Linguistics, Ruhr University Bochum, Bochum, 
Germany.
(5)German Department, Digital Forensic Linguistics, Ruhr University Bochum, 
Bochum, Germany.
(6)Department for Medical Informatics, Biometry and Epidemiology, Ruhr 
University Bochum, Bochum, Germany.
(#)Contributed equally

BACKGROUND: Large language models, exemplified by ChatGPT, have reached a level 
of sophistication that makes distinguishing between human- and artificial 
intelligence (AI)-generated texts increasingly challenging. This has raised 
concerns in academia, particularly in medicine, where the accuracy and 
authenticity of written work are paramount.
OBJECTIVE: This semirandomized controlled study aims to examine the ability of 2 
blinded expert groups with different levels of content familiarity-medical 
professionals and humanities scholars with expertise in textual analysis-to 
distinguish between longer scientific texts in German written by medical 
students and those generated by ChatGPT. Additionally, the study sought to 
analyze the reasoning behind their identification choices, particularly the role 
of content familiarity and linguistic features.
METHODS: Between May and August 2023, a total of 35 experts (medical: n=22; 
humanities: n=13) were each presented with 2 pairs of texts on different medical 
topics. Each pair had similar content and structure: 1 text was written by a 
medical student, and the other was generated by ChatGPT (version 3.5, March 
2023). Experts were asked to identify the AI-generated text and justify their 
choice. These justifications were analyzed through a multistage, 
interdisciplinary qualitative analysis to identify relevant textual features. 
Before unblinding, experts rated each text on 6 characteristics: linguistic 
fluency and spelling/grammatical accuracy, scientific quality, logical 
coherence, expression of knowledge limitations, formulation of future research 
questions, and citation quality. Univariate tests and multivariate logistic 
regression analyses were used to examine associations between participants' 
characteristics, their stated reasons for author identification, and the 
likelihood of correctly determining a text's authorship.
RESULTS: Overall, in 48 out of 69 (70%) decision rounds, participants accurately 
identified the AI-generated texts, with minimal difference between groups 
(medical: 31/43, 72%; humanities: 17/26, 65%; odds ratio [OR] 1.37, 95% CI 
0.5-3.9). While content errors had little impact on identification accuracy, 
stylistic features-particularly redundancy (OR 6.90, 95% CI 1.01-47.1), 
repetition (OR 8.05, 95% CI 1.25-51.7), and thread/coherence (OR 6.62, 95% CI 
1.25-35.2)-played a crucial role in participants' decisions to identify a text 
as AI-generated.
CONCLUSIONS: The findings suggest that both medical and humanities experts were 
able to identify ChatGPT-generated texts in medical contexts, with their 
decisions largely based on linguistic attributes. The accuracy of identification 
appears to be independent of experts' familiarity with the text content. As the 
decision-making process primarily relies on linguistic attributes-such as 
stylistic features and text coherence-further quasi-experimental studies using 
texts from other academic disciplines should be conducted to determine whether 
instructions based on these features can enhance lecturers' ability to 
distinguish between student-authored and AI-generated work.

©Berin Doru, Christoph Maier, Johanna Sophie Busse, Thomas Lücke, Judith 
Schönhoff, Elena Enax- Krumova, Steffen Hessler, Maria Berger, Marianne Tokic. 
Originally published in JMIR Medical Education (https://mededu.jmir.org), 
03.03.2025.

DOI: 10.2196/62779
PMCID: PMC11914838
PMID: 40053752 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


473. Front Public Health. 2025 Feb 19;13:1512537. doi: 10.3389/fpubh.2025.1512537. 
eCollection 2025.

Deductively coding psychosocial autopsy interview data using a few-shot learning 
large language model.

Balt E(#)(1)(2), Salmi S(#)(1), Bhulai S(3), Vrinzen S(1), Eikelenboom M(1)(2), 
Gilissen R(1)(4), Creemers D(5)(6), Popma A(2), Mérelle S(1)(7).

Author information:
(1)Research Department, 113 Suicide Prevention, Amsterdam, Netherlands.
(2)Department of Psychiatry, Amsterdam University Medical Centers, Amsterdam, 
Netherlands.
(3)Department of Mathematics, VU University, Amsterdam, Netherlands.
(4)Department of Clinical Psychology, Leiden University, Leiden, Netherlands.
(5)Department of Child- and Youth Psychiatry, GGz Oost-Brabant, Boekel, 
Netherlands.
(6)Department of Psychiatry and Psychosocial Care, Radboud University Nijmegen, 
Nijmegen, Netherlands.
(7)Department of Child and Adolescent Psychiatry and Psychosocial Care, 
Amsterdam University Medical Center, Amsterdam, Netherlands.
(#)Contributed equally

BACKGROUND: Psychosocial autopsy is a retrospective study of suicide, aimed to 
identify emerging themes and psychosocial risk factors. It typically relies 
heavily on qualitative data from interviews or medical documentation. However, 
qualitative research has often been scrutinized for being prone to bias and is 
notoriously time- and cost-intensive. Therefore, the current study aimed to 
investigate if a Large Language Model (LLM) can be feasibly integrated with 
qualitative research procedures, by evaluating the performance of the model in 
deductively coding and coherently summarizing interview data obtained in a 
psychosocial autopsy.
METHODS: Data from 38 semi-structured interviews conducted with individuals 
bereaved by the suicide of a loved one was deductively coded by qualitative 
researchers and a server-installed LLAMA3 large language model. The model 
performance was evaluated in three tasks: (1) binary classification of coded 
segments, (2) independent classification using a sliding window approach, and 
(3) summarization of coded data. Intercoder agreement scores were calculated 
using Cohen's Kappa, and the LLM's summaries were qualitatively assessed using 
the Constant Comparative Method.
RESULTS: The results showed that the LLM achieved substantial agreement with the 
researchers for the binary classification (accuracy: 0.84) and the sliding 
window task (accuracy: 0.67). The performance had large variability across 
codes. LLM summaries were typically rich enough for subsequent analysis by the 
researcher, with around 80% of the summaries being rated independently by two 
researchers as 'adequate' or 'good.' Emerging themes in the qualitative 
assessment of the summaries included unsolicited elaboration and hallucination.
CONCLUSION: State-of-the-art LLMs show great potential to support researchers in 
deductively coding complex interview data, which would alleviate the investment 
of time and resources. Integrating models with qualitative research procedures 
can facilitate near real-time monitoring. Based on the findings, we recommend a 
collaborative model, whereby the LLM's deductive coding is complemented by 
review, inductive coding and further interpretation by a researcher. Future 
research may aim to replicate the findings in different contexts and evaluate 
models with a larger context size.

Copyright © 2025 Balt, Salmi, Bhulai, Vrinzen, Eikelenboom, Gilissen, Creemers, 
Popma and Mérelle.

DOI: 10.3389/fpubh.2025.1512537
PMCID: PMC11879832
PMID: 40046117 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


474. Precis Clin Med. 2025 Feb 8;8(1):pbaf003. doi: 10.1093/pcmedi/pbaf003. 
eCollection 2025 Mar.

Predictive model for daily risk alerts in sepsis patients in the ICU: 
visualization and clinical analysis of risk indicators.

Yang H(1)(2), Li J(3), Zhang C(4), Sierra AP(2), Shen B(4).

Author information:
(1)Information Center, West China Hospital of Sichuan University, Chengdu 
610041, China.
(2)Department of Computer Science and Information Technologies, Research Center 
for Information and Communications Technologies, University of A Coruña, 
Biomedical Research Institute of a Coruña, A Coruña 15071, Spain.
(3)Department of Clinical Laboratory Medicine, Jinniu Maternity and Child Health 
Hospital of Chengdu, Chengdu 610031, China.
(4)Joint Laboratory of Artificial Intelligence for Critical Care Medicine, 
Department of Critical Care Medicine and Institutes for Systems Genetics, 
Frontiers Science Center for Disease-related Molecular Network, West China 
Hospital, Sichuan University, Chengdu 610041, China.

This study introduces a novel Transformer-based time-series framework designed 
to revolutionize risk stratification in Intensive Care Units (ICUs) by 
predicting patient outcomes with high temporal precision. Leveraging sequential 
data from the eICU database, our two-stage architecture dynamically captures 
evolving health trajectories throughout a patient's ICU stay, enabling real-time 
identification of high-risk individuals and actionable insights for personalized 
interventions. The model demonstrated exceptional predictive power, achieving a 
progressive AUC increase from 0.87 (±0.021) on admission day to 0.92 (±0.009) by 
day 5, reflecting its capacity to assimilate longitudinal physiological 
patterns. Rigorous external validation across geographically diverse 
cohorts-including an 81.8% accuracy on Chinese sepsis data (AUC=0.73) and 76.56% 
accuracy on MIMIC-IV-3.1 (AUC=0.84)-confirmed robust generalizability. 
Crucially, SHAP-derived temporal heatmaps unveiled mortality-associated feature 
dynamics over time, bridging the gap between model predictions and clinically 
interpretable biomarkers. These findings establish a new paradigm for ICU 
prognostics, where data-driven temporal modeling synergizes with clinician 
expertise to optimize triage, reduce diagnostic latency, and ultimately improve 
survival outcomes in critical care.

© The Author(s) 2025. Published by Oxford University Press on behalf of the West 
China School of Medicine & West China Hospital of Sichuan University.

DOI: 10.1093/pcmedi/pbaf003
PMCID: PMC11878768
PMID: 40041421

Conflict of interest statement: There are no conflicts of interest related to 
this manuscript. In addition, as an Editorial Board Member of Precision Clinical 
Medicine, the corresponding author Bairong Shen was blinded from reviewing and 
making decisions on this manuscript.


475. Sci Rep. 2025 Feb 26;15(1):6932. doi: 10.1038/s41598-025-90450-0.

A labeled medical records corpus for the timely detection of rare diseases using 
machine learning approaches.

Rolando M(1), Raggio V(2), Naya H(1)(3), Spangenberg L(4)(5), Cagnina L(6)(7).

Author information:
(1)Bioinformatics Unit, Institut Pasteur de Montevideo, Montevideo, Uruguay.
(2)Departamento de Genética, Facultad de Medicina, Universidad de la República, 
Montevideo, Uruguay.
(3)Departamento de producción animal y pasturas, Facultad de Agronomía, 
Universidad de la República, Montevideo, Uruguay.
(4)Bioinformatics Unit, Institut Pasteur de Montevideo, Montevideo, Uruguay. 
lucia@pasteur.edu.uy.
(5)Departamento Básico de Medicina, Hospital de Clínicas, Universidad de la 
República, Montevideo, Uruguay. lucia@pasteur.edu.uy.
(6)Universidad Nacional de San Luis, San Luis, Argentina. lcagnina@unsl.edu.ar.
(7)Consejo Nacional de Investigaciones Científicas y Técnicas, CONICET, Buenos 
Aires, Argentina. lcagnina@unsl.edu.ar.

Rare diseases (RDs) are a group of pathologies that individually affect less 
than 1 in 2000 people but collectively impact around 7% of the world's 
population. Most of them affect children, are chronic and progressive, and have 
no specific treatment. RD patients face diagnostic challenges, with an average 
diagnosis time of 5 years, multiple specialist visits, and invasive procedures. 
This 'diagnostic odyssey' can be detrimental to their health. Machine learning 
(ML) has the potential to improve healthcare by providing more personalized and 
accurate patient management, diagnoses, and in some cases, treatments. 
Leveraging the MIMIC-III database and additional medical notes from different 
sources such as in-house data, PubMed and chatGPT, we propose a labeled dataset 
for early RD detection in hospital settings. Applying various supervised ML 
methods, including logistic regression, decision trees, support vector machine 
(SVM), deep learning methods (LSTM and CNN), and Transformers (BERT), we 
validated the use of the proposed resource, achieving 92.7% F-measure and a 96% 
AUC using SVM. These findings highlight the potential of ML in redirecting RD 
patients towards more accurate diagnostic pathways and presents a corpus that 
can be used for future development and refinements.

© 2025. The Author(s).

DOI: 10.1038/s41598-025-90450-0
PMCID: PMC11865299
PMID: 40011510 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Competing interests: The authors 
declare no competing interests.


476. Int J Environ Res Public Health. 2025 Jan 29;22(2):193. doi: 
10.3390/ijerph22020193.

Exploring Client Perceptions on Gaining Infant Feeding Information Through the 
Texas Women, Infants, and Children (WIC) Chatbot.

Baez K(1), Biediger-Friedman L(1), Johnson CM(1), Stubblefield E(1), Escalera 
L(1), Markides BR(2).

Author information:
(1)Nutrition and Foods Program, School of Family and Consumer Sciences, Texas 
State University, San Marcos, TX 78666, USA.
(2)Institute for Physical Activity and Nutrition, School of Exercise and 
Nutrition Sciences, Deakin University, Geelong, VIC 3216, Australia.

The modernization of the Special Supplemental Nutrition Program for Women, 
Infants, and Children (WIC) program is a priority. The Texas WIC chatbot, Maya, 
streamlines client interactions through dialog-based responses. This qualitative 
study explored client capabilities, motivations, and opportunities for seeking 
nutrition information about breastfeeding, formula feeding, infant feeding 
safety, adequacy of infant feeding, and complementary feeding via a chatbot. A 
team conducted in-depth semi-structured interviews with Texas WIC clients (n = 
19 women). All interviews were transcribed and subjected to a two-coder, 
four-phase process utilizing a theory-based codebook. Codes were compiled and 
thematically categorized. Identified themes included (1) motivations through 
necessity or resource availability, (2) client capabilities and Maya usability, 
and (3) opportunities for connection, support, and encouragement. Texas WIC 
clients that participated in this study expressed motivations, capabilities, and 
opportunities to engage with nutrition information through Maya. They described 
Maya as a favorable resource for behavior changes, and a trusted source of 
nutrition information, citing the credibility of WIC and reliability of the 
chatbot. The findings may inform future research and development of public 
health chatbots. Additional research is required to explore how different 
factors such as language and technology usage may impact client capabilities, 
motivations, and opportunities to seek nutrition information with regard to 
infant feeding.

DOI: 10.3390/ijerph22020193
PMCID: PMC11855084
PMID: 40003419 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare no conflicts of interest.


477. Radiology. 2025 Feb;314(2):e241051. doi: 10.1148/radiol.241051.

Assessing Completeness of Clinical Histories Accompanying Imaging Orders Using 
Adapted Open-Source and Closed-Source Large Language Models.

Larson DB(1)(2), Koirala A(1)(2), Cheuy LY(1)(2), Paschali M(1)(2), Van Veen 
D(3), Na HS(1)(2), Petterson MB(1), Fang Z(1)(2), Chaudhari AS(1)(2)(4).

Author information:
(1)Department of Radiology, Stanford University School of Medicine, 453 Quarry 
Rd, MC 5659, Stanford, CA 94304.
(2)AI Development and Evaluation Laboratory, Stanford University School of 
Medicine, Palo Alto, Calif.
(3)Department of Electrical Engineering, Stanford University, Stanford, Calif.
(4)Department of Biomedical Data Science, Stanford University, Stanford, Calif.

Background Incomplete clinical histories are a well-known problem in radiology. 
Previous dedicated quality improvement efforts focusing on reproducible 
assessments of the completeness of free-text clinical histories have relied on 
tedious manual analysis. Purpose To adapt and evaluate open-source and 
closed-source large language models (LLMs) for their ability to automatically 
extract clinical history elements within imaging orders and to use the 
best-performing adapted open-source model to assess the completeness of a large 
sample of clinical histories as a benchmark for clinical practice. Materials and 
Methods This retrospective single-site study used previously extracted 
information accompanying CT, MRI, US, and radiography orders from August 2020 to 
May 2022 at an adult and pediatric emergency department of a 613-bed tertiary 
academic medical center. Two open-source (Llama 2-7B [Meta], Mistral-7B [Mistral 
AI]) and one closed-source (GPT-4 Turbo [OpenAI]) LLMs were adapted using prompt 
engineering, in-context learning, and fine-tuning (open-source only) to extract 
the elements "past medical history," "what," "when," "where," and "clinical 
concern" from clinical histories. Model performance, interreader agreement using 
Cohen κ (none to slight, 0.01-0.20; fair, 0.21-0.40; moderate, 0.41-0.60; 
substantial, 0.61-0.80; almost perfect, 0.81-1.00), and semantic similarity 
between the models and the adjudicated manual annotations of two board-certified 
radiologists with 16 and 3 years of postfellowship experience, respectively, 
were assessed using accuracy, Cohen κ, and BERTScore, an LLM metric that 
quantifies how well two pieces of text convey the same meaning; 95% CIs were 
also calculated. The best-performing open-source model was then used to assess 
completeness on a large dataset of unannotated clinical histories. Results A 
total of 50 186 clinical histories were included (794 training, 150 validation, 
300 initial testing, 48 942 real-world application). Of the two open-source 
models, Mistral-7B outperformed Llama 2-7B in assessing completeness and was 
further fine-tuned. Both Mistral-7B and GPT-4 Turbo showed substantial overall 
agreement with radiologists (mean κ, 0.73 [95% CI: 0.67, 0.78] to 0.77 [95% CI: 
0.71, 0.82]) and adjudicated annotations (mean BERTScore, 0.96 [95% CI: 0.96, 
0.97] for both models; P = .38). Mistral-7B also rivaled GPT-4 Turbo in 
performance (weighted overall mean accuracy, 91% [95% CI: 89, 93] vs 92% [95% 
CI: 90, 94]; P = .31) despite being a smaller model. Using Mistral-7B, 26.2% 
(12 803 of 48 942) of unannotated clinical histories were found to contain all 
five elements. Conclusion An easily deployable fine-tuned open-source LLM 
(Mistral-7B), rivaling GPT-4 Turbo in performance, could effectively extract 
clinical history elements with substantial agreement with radiologists and 
produce a benchmark for completeness of a large sample of clinical histories. 
The model and code will be fully open-sourced. © RSNA, 2025 Supplemental 
material is available for this article.

DOI: 10.1148/radiol.241051
PMCID: PMC11868845
PMID: 39998369 [Indexed for MEDLINE]

Conflict of interest statement: Disclosures of conflicts of interest: D.B.L. 
Member of the Board of Chancellors of the American College of Radiology and 
Board of Trustees of the American Board of Radiology, shareholder in Bunkerhill 
Health; receives research funding from the Gordon and Betty Moore Foundation. 
A.K. No relevant relationships. L.Y.C. No relevant relationships. M.P. No 
relevant relationships. D.V.V. No relevant relationships. H.S.N. No relevant 
relationships. M.B.P. No relevant relationships. Z.F. No relevant relationships. 
A.S.C. Grants to institution from the NIH (R01HL167974, R01AR077604, 
R01EB002524, R01AR079431, and P41EB027060) and contracts (75N92020C00008 and 
75N92020C00021), ARPA-H, GE HealthCare, and Philips; royalties or licenses from 
LVIS though Stanford; consulting fees from Elucid Bioimaging, Patient Square 
Capital, and Chondrometrics; payment from Genentech for expert testimony; 
patents planned, issued or pending from GE HealthCare through Stanford; 
participation on an Advisory Board for Brain Key and Chondrometrics GmbH; stock 
or stock options in LVIS, Subtle Medical, and Brain Key; Microsoft Azure OpenAI 
credits.


478. Ann Intern Med. 2025 Mar;178(3):389-401. doi: 10.7326/ANNALS-24-02189. Epub 2025 
Feb 25.

Development of Prompt Templates for Large Language Model-Driven Screening in 
Systematic Reviews.

Cao C(1), Sang J(2), Arora R(3), Chen D(4), Kloosterman R(4), Cecere M(4), Gorla 
J(4), Saleh R(4), Drennan I(5), Teja B(6), Fehlings M(7), Ronksley P(8), Leung 
AA(9), Weisz DE(10), Ware H(11), Whelan M(11), Emerson DB(12), Arora RK(11), 
Bobrovitz N(13).

Author information:
(1)Temerty Faculty of Medicine, University of Toronto, Toronto, Ontario, and 
Centre for Health Informatics, Department of Community Health Sciences, 
University of Calgary, Calgary, Alberta, Canada (C.C.).
(2)Stripe, San Francisco, California (J.S.).
(3)Department of Biomedical Informatics, Harvard Medical School, Boston, 
Massachusetts (R.A.).
(4)Temerty Faculty of Medicine, University of Toronto, Toronto, Ontario, Canada 
(D.C., R.K., M.C., J.G., R.S.).
(5)Temerty Faculty of Medicine, University of Toronto, Department of Emergency 
Services and Sunnybrook Research Institute, Sunnybrook Health Sciences Centre, 
and Ornge Air Ambulance and Critical Care Transport, Toronto, Ontario, Canada 
(I.D.).
(6)Department of Anesthesiology and Pain Medicine, University of Toronto, and 
Department of Anesthesia and Critical Care Medicine, St. Michael's Hospital, 
Toronto, Ontario, Canada (B.T.).
(7)Department of Surgery, Temerty Faculty of Medicine, University of Toronto, 
Toronto, Ontario, Canada (M.F.).
(8)Department of Community Health Sciences, Cumming School of Medicine, 
University of Calgary, Calgary, Alberta, Canada (P.R.).
(9)Department of Medicine and Department of Community Health Sciences, Cumming 
School of Medicine, University of Calgary, Calgary, Alberta, Canada (A.A.L.).
(10)Department of Newborn and Developmental Paediatrics, Sunnybrook Health 
Sciences Centre, Toronto, Ontario, Canada (D.E.W.).
(11)Centre for Health Informatics, Department of Community Health Sciences, 
University of Calgary, Calgary, Alberta, Canada (H.W., M.W., R.K.A.).
(12)Vector Institute, Toronto, Ontario, Canada (D.B.E.).
(13)Centre for Health Informatics, Department of Community Health Sciences, and 
Department of Emergency Medicine, Cumming School of Medicine, University of 
Calgary, Calgary, Alberta, Canada (N.B.).

BACKGROUND: Systematic reviews (SRs) are hindered by the initial rigorous 
article screen, which delays access to reliable information synthesis.
OBJECTIVE: To develop generic prompt templates for large language model 
(LLM)-driven abstract and full-text screening that can be adapted to different 
reviews.
DESIGN: Diagnostic test accuracy.
SETTING: 48 425 citations were tested for abstract screening across 10 SRs. 
Full-text screening evaluated all 12 690 freely available articles from the 
original search. Prompt development used the GPT4-0125-preview model (OpenAI).
PARTICIPANTS: None.
MEASUREMENTS: Large language models were prompted to include or exclude articles 
based on SR eligibility criteria. Model outputs were compared with original SR 
author decisions after full-text screening to evaluate performance (accuracy, 
sensitivity, and specificity).
RESULTS: Optimized prompts using GPT4-0125-preview achieved a weighted 
sensitivity of 97.7% (range, 86.7% to 100%) and specificity of 85.2% (range, 
68.3% to 95.9%) in abstract screening and weighted sensitivity of 96.5% (range, 
89.7% to 100.0%) and specificity of 91.2% (range, 80.7% to 100%) in full-text 
screening across 10 SRs. In contrast, zero-shot prompts had poor sensitivity 
(49.0% abstract, 49.1% full-text). Across LLMs, Claude-3.5 (Anthropic) and GPT4 
variants had similar performance, whereas Gemini Pro (Google) and GPT3.5 
(OpenAI) models underperformed. Direct screening costs for 10 000 citations 
differed substantially: Where single human abstract screening was estimated to 
require more than 83 hours and $1666.67 USD, our LLM-based approach completed 
screening in under 1 day for $157.02 USD.
LIMITATIONS: Further prompt optimizations may exist. Retrospective study. 
Convenience sample of SRs. Full-text screening evaluations were limited to free 
PubMed Central full-text articles.
CONCLUSION: A generic prompt for abstract and full-text screening achieving high 
sensitivity and specificity that can be adapted to other SRs and LLMs was 
developed. Our prompting innovations may have value to SR investigators and 
researchers conducting similar criteria-based tasks across the medical sciences.
PRIMARY FUNDING SOURCE: None.

DOI: 10.7326/ANNALS-24-02189
PMID: 39993313 [Indexed for MEDLINE]

Conflict of interest statement: Disclosures: Disclosure forms are available with 
the article online.


479. J Biomed Inform. 2025 Apr;164:104796. doi: 10.1016/j.jbi.2025.104796. Epub 2025 
Feb 21.

Missing-modality enabled multi-modal fusion architecture for medical data.

Wang M(1), Fan S(1), Li Y(1), Xie Z(1), Chen H(2).

Author information:
(1)School of Biomedical Engineering, Capital Medical University, No.10, 
Xitoutiao, You An Men, Fengtai District, Beijing 100069, China; Beijing Key 
Laboratory of Fundamental Research on Biomechanics in Clinical Application, 
Capital Medical University, No.10, Xitoutiao, You An Men, Fengtai District, 
Beijing 100069, China.
(2)School of Biomedical Engineering, Capital Medical University, No.10, 
Xitoutiao, You An Men, Fengtai District, Beijing 100069, China; Beijing Key 
Laboratory of Fundamental Research on Biomechanics in Clinical Application, 
Capital Medical University, No.10, Xitoutiao, You An Men, Fengtai District, 
Beijing 100069, China. Electronic address: chenhui@ccmu.edu.cn.

BACKGROUND: Fusion of multi-modal data can improve the performance of deep 
learning models. However, missing modalities are common in medical data due to 
patient specificity, which is detrimental to the performance of multi-modal 
models in applications. Therefore, it is critical to adapt the models to missing 
modalities.
OBJECTIVE: This study aimed to develop an effective multi-modal fusion 
architecture for medical data that was robust to missing modalities and further 
improved the performance for clinical tasks.
METHODS: X-ray chest radiographs for the image modality, radiology reports for 
the text modality, and structured value data for the tabular data modality were 
fused in this study. Each modality pair was fused with a Transformer-based 
bi-modal fusion module, and the three bi-modal fusion modules were then combined 
into a tri-modal fusion framework. Additionally, multivariate loss functions 
were introduced into the training process to improve models' robustness to 
missing modalities during the inference process. Finally, we designed comparison 
and ablation experiments to validate the effectiveness of the fusion, the 
robustness to missing modalities, and the enhancements from each key component. 
Experiments were conducted on MIMIC-IV and MIMIC-CXR datasets with the 14-label 
disease diagnosis and patient in-hospital mortality prediction task The area 
under the receiver operating characteristic curve (AUROC) and the area under the 
precision-recall curve (AUPRC) were used to evaluate models' performance.
RESULTS: Our proposed architecture showed superior predictive performance, 
achieving the average AUROC and AUPRC of 0.916 and 0.551 in the 14-label 
classification task, 0.816 and 0.392 in the mortality prediction task. while the 
best average AUROC and AUPRC among the comparison methods were 0.876, 0.492 in 
the 14-label classification task and 0.806, 0.366 in the mortality prediction 
task. Both metrics decreased only slightly when tested with modal-incomplete 
data. Different levels of enhancements were achieved through three key 
components.
CONCLUSIONS: The proposed multi-modal fusion architecture effectively fused 
three modalities and showed strong robustness to missing modalities. This 
architecture holds promise for scaling up to more modalities to enhance the 
clinical practicality of the model.

Copyright © 2025 Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jbi.2025.104796
PMID: 39988001 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


480. Eur Arch Paediatr Dent. 2025 Jun;26(3):527-535. doi: 10.1007/s40368-025-01012-x. 
Epub 2025 Feb 22.

Evaluating the evidence-based potential of six large language models in 
paediatric dentistry: a comparative study on generative artificial intelligence.

Dermata A(1), Arhakis A(2), Makrygiannakis MA(3)(4), Giannakopoulos K(4), 
Kaklamanos EG(1)(4)(5).

Author information:
(1)School of Dentistry, Aristotle University of Thessaloniki, 54124, 
Thessaloniki, Greece.
(2)School of Dentistry, Aristotle University of Thessaloniki, 54124, 
Thessaloniki, Greece. arhakis@dent.auth.gr.
(3)School of Dentistry, National and Kapodistrian University of Athens, 11527, 
Athens, Greece.
(4)School of Dentistry, European University Cyprus, 2404, Nicosia, Cyprus.
(5)Hamdan Bin Mohammed College of Dental Medicine (HBMCDM), Mohammed Bin Rashid 
University of Medicine and Health Sciences (MBRU), Dubai, UAE.

PURPOSE: The use of large language models (LLMs) in generative artificial 
intelligence (AI) is rapidly increasing in dentistry. However, their reliability 
is yet to be fully founded. This study aims to evaluate the diagnostic accuracy, 
clinical applicability, and patient education potential of LLMs in paediatric 
dentistry, by evaluating the responses of six LLMs: Google AI's Gemini and 
Gemini Advanced, OpenAI's ChatGPT-3.5, -4o and -4, and Microsoft's Copilot.
METHODS: Ten open-type clinical questions, relevant to paediatric dentistry were 
posed to the LLMs. The responses were graded by two independent evaluators from 
0 to 10 using a detailed rubric. After 4 weeks, answers were reevaluated to 
assess intra-evaluator reliability. Statistical comparisons used Friedman's and 
Wilcoxon's and Kruskal-Wallis tests to assess the model that provided the most 
comprehensive, accurate, explicit and relevant answers.
RESULTS: Variations of results were noted. Chat GPT 4 answers were scored as the 
best (average score 8.08), followed by the answers of Gemini Advanced (8.06), 
ChatGPT 4o (8.01), ChatGPT 3.5 (7.61), Gemini (7,32) and Copilot (5.41). 
Statistical analysis revealed that Chat GPT 4 outperformed all other LLMs, and 
the difference was statistically significant. Despite variations and different 
responses to the same queries, remarkable similarities were observed. Except for 
Copilot, all chatbots managed to achieve a score level above 6.5 on all queries.
CONCLUSION: This study demonstrates the potential use of language models (LLMs) 
in supporting evidence-based paediatric dentistry. Nevertheless, they cannot be 
regarded as completely trustworthy. Dental professionals should critically use 
AI models as supportive tools and not as a substitute of overall scientific 
knowledge and critical thinking.

© 2025. The Author(s).

DOI: 10.1007/s40368-025-01012-x
PMCID: PMC12165978
PMID: 39987420 [Indexed for MEDLINE]


481. JMIR Med Educ. 2025 Feb 20;11:e63400. doi: 10.2196/63400.

Perceptions and Earliest Experiences of Medical Students and Faculty With 
ChatGPT in Medical Education: Qualitative Study.

Abouammoh N(1)(2), Alhasan K(1)(3)(4), Aljamaan F(1)(5), Raina R(6), Malki 
KH(1)(7), Altamimi I(1), Muaygil R(1)(8), Wahabi H(2)(9), Jamal A(1)(2)(9), 
Alhaboob A(1)(3), Assiri RA(10), Al-Tawfiq JA(11)(12)(13), Al-Eyadhy A(1)(3), 
Soliman M(1)(8), Temsah MH(1)(3)(9).

Author information:
(1)College of Medicine, King Saud University, Riyadh, Saudi Arabia.
(2)Department of Family and Community Medicine, King Saud University Medical 
City, King Saud University, Riyadh, Saudi Arabia.
(3)Pediatric Department, King Saud University Medical City, King Saud 
University, Riyadh, Saudi Arabia.
(4)Department of Kidney and Pancreas Transplant, Organ Transplant Center of 
Excellence, King Faisal Specialist Hospital & Research Centre, Riyadh, Saudi 
Arabia.
(5)Critical Care Department, King Saud University Medical City, King Saud 
University, Riyadh, Saudi Arabia.
(6)Department of Nephrology, Cleveland Clinic Akron General and Akron Children 
Hospital, Akron, OH, United States.
(7)Research Chair of Voice, Swallowing, and Communication Disorders, Department 
of Otolaryngology, College of Medicine, King Saud University, Riyadh, Saudi 
Arabia.
(8)Medical Education Department, King Saud University Medical City, King Saud 
University, Riyadh, Saudi Arabia.
(9)Evidence-Based Health Care & Knowledge Translation Research Chair, Family & 
Community Medicine Department, College of Medicine, King Saud University, 
Riyadh, Saudi Arabia.
(10)Department of Basic Medical Sciences, College of Medicine, Princess Nourah 
bint Abdulrahman University, Riyadh, Saudi Arabia.
(11)Specialty Internal Medicine and Quality Department, Johns Hopkins Aramco 
Healthcare, Dhahran, Saudi Arabia.
(12)Infectious Disease Division, Department of Medicine, Indiana University 
School of Medicine, Indianapolis, IN, United States.
(13)Infectious Disease Division, Department of Medicine, Johns Hopkins 
University School of Medicine, Baltimore, MD, United States.

BACKGROUND: With the rapid development of artificial intelligence technologies, 
there is a growing interest in the potential use of artificial 
intelligence-based tools like ChatGPT in medical education. However, there is 
limited research on the initial perceptions and experiences of faculty and 
students with ChatGPT, particularly in Saudi Arabia.
OBJECTIVE: This study aimed to explore the earliest knowledge, perceived 
benefits, concerns, and limitations of using ChatGPT in medical education among 
faculty and students at a leading Saudi Arabian university.
METHODS: A qualitative exploratory study was conducted in April 2023, involving 
focused meetings with medical faculty and students with varying levels of 
ChatGPT experience. A thematic analysis was used to identify key themes and 
subthemes emerging from the discussions.
RESULTS: Participants demonstrated good knowledge of ChatGPT and its functions. 
The main themes were perceptions of ChatGPT use, potential benefits, and 
concerns about ChatGPT in research and medical education. The perceived benefits 
included collecting and summarizing information and saving time and effort. 
However, concerns and limitations centered around the potential lack of critical 
thinking in the information provided, the ambiguity of references, limitations 
of access, trust in the output of ChatGPT, and ethical concerns.
CONCLUSIONS: This study provides valuable insights into the perceptions and 
experiences of medical faculty and students regarding the use of newly 
introduced large language models like ChatGPT in medical education. While the 
benefits of ChatGPT were recognized, participants also expressed concerns and 
limitations requiring further studies for effective integration into medical 
education, exploring the impact of ChatGPT on learning outcomes, student and 
faculty satisfaction, and the development of critical thinking skills.

©Noura Abouammoh, Khalid Alhasan, Fadi Aljamaan, Rupesh Raina, Khalid H Malki, 
Ibraheem Altamimi, Ruaim Muaygil, Hayfaa Wahabi, Amr Jamal, Ali Alhaboob, Rasha 
Assad Assiri, Jaffar A Al-Tawfiq, Ayman Al-Eyadhy, Mona Soliman, Mohamad-Hani 
Temsah. Originally published in JMIR Medical Education 
(https://mededu.jmir.org), 20.02.2025.

DOI: 10.2196/63400
PMCID: PMC11888024
PMID: 39977012 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: None declared.


482. Front Artif Intell. 2025 Feb 5;8:1533508. doi: 10.3389/frai.2025.1533508. 
eCollection 2025.

Large language models generating synthetic clinical datasets: a feasibility and 
comparative analysis with real-world perioperative data.

Barr AA(1), Quan J(1), Guo E(1), Sezgin E(2)(3).

Author information:
(1)Cumming School of Medicine, University of Calgary, Calgary, AB, Canada.
(2)The Abigail Wexner Research Institute, Nationwide Children's Hospital, 
Columbus, OH, United States.
(3)Department of Pediatrics, The Ohio State University College of Medicine, 
Columbus, OH, United States.

BACKGROUND: Clinical data is instrumental to medical research, machine learning 
(ML) model development, and advancing surgical care, but access is often 
constrained by privacy regulations and missing data. Synthetic data offers a 
promising solution to preserve privacy while enabling broader data access. 
Recent advances in large language models (LLMs) provide an opportunity to 
generate synthetic data with reduced reliance on domain expertise, computational 
resources, and pre-training.
OBJECTIVE: This study aims to assess the feasibility of generating realistic 
tabular clinical data with OpenAI's GPT-4o using zero-shot prompting, and 
evaluate the fidelity of LLM-generated data by comparing its statistical 
properties to the Vital Signs DataBase (VitalDB), a real-world open-source 
perioperative dataset.
METHODS: In Phase 1, GPT-4o was prompted to generate a dataset with qualitative 
descriptions of 13 clinical parameters. The resultant data was assessed for 
general errors, plausibility of outputs, and cross-verification of related 
parameters. In Phase 2, GPT-4o was prompted to generate a dataset using 
descriptive statistics of the VitalDB dataset. Fidelity was assessed using 
two-sample t-tests, two-sample proportion tests, and 95% confidence interval 
(CI) overlap.
RESULTS: In Phase 1, GPT-4o generated a complete and structured dataset 
comprising 6,166 case files. The dataset was plausible in range and correctly 
calculated body mass index for all case files based on respective heights and 
weights. Statistical comparison between the LLM-generated datasets and VitalDB 
revealed that Phase 2 data achieved significant fidelity. Phase 2 data 
demonstrated statistical similarity in 12/13 (92.31%) parameters, whereby no 
statistically significant differences were observed in 6/6 (100.0%) 
categorical/binary and 6/7 (85.71%) continuous parameters. Overlap of 95% CIs 
were observed in 6/7 (85.71%) continuous parameters.
CONCLUSION: Zero-shot prompting with GPT-4o can generate realistic tabular 
synthetic datasets, which can replicate key statistical properties of real-world 
perioperative data. This study highlights the potential of LLMs as a novel and 
accessible modality for synthetic data generation, which may address critical 
barriers in clinical data access and eliminate the need for technical expertise, 
extensive computational resources, and pre-training. Further research is 
warranted to enhance fidelity and investigate the use of LLMs to amplify and 
augment datasets, preserve multivariate relationships, and train robust ML 
models.

Copyright © 2025 Barr, Quan, Guo and Sezgin.

DOI: 10.3389/frai.2025.1533508
PMCID: PMC11836953
PMID: 39974356

Conflict of interest statement: The authors declare that the research was 
conducted in the absence of any commercial or financial relationships that could 
be construed as a potential conflict of interest.


483. Cureus. 2025 Feb 18;17(2):e79221. doi: 10.7759/cureus.79221. eCollection 2025 
Feb.

DeepSeek in Healthcare: Revealing Opportunities and Steering Challenges of a New 
Open-Source Artificial Intelligence Frontier.

Temsah A(1), Alhasan K(2)(3), Altamimi I(4), Jamal A(4), Al-Eyadhy A(2)(5), 
Malki KH(6), Temsah MH(2)(4)(5).

Author information:
(1)Software Engineering, Alfaisal University, Riyadh, SAU.
(2)Pediatric Department, College of Medicine, King Saud University, Riyadh, SAU.
(3)Kidney and Pancreas Health Center, Organ Transplant Center of Excellence, 
King Faisal Specialist Hospital and Research Center, Riyadh, SAU.
(4)Evidence-Based Research Chair, Family and Community Medicine, College of 
Medicine, King Saud University, Riyadh, SAU.
(5)Pediatric Intensive Care Unit, Pediatric Department, King Saud University 
Medical City, Riyadh, SAU.
(6)Research Chair of Voice, Swallowing, and Communication Disorders, Department 
of Otolaryngology-Head and Neck Surgery, College of Medicine, King Saud 
University, Riyadh, SAU.

Generative Artificial Intelligence (GAI) has driven several advancements in 
healthcare, with large language models (LLMs) such as OpenAI's ChatGPT, Google's 
Gemini, and Microsoft's Copilot demonstrating potential in clinical decision 
support, medical education, and research acceleration. However, their 
closed-source architecture, high computational costs, and limited adaptability 
to specialized medical contexts remained key barriers to universal adoption. 
Now, with the rise of DeepSeek's DeepThink (R1), an open-source LLM, gaining 
prominence since mid-January 2025, new opportunities and challenges emerge for 
healthcare integration and AI-driven research. Unlike proprietary models, 
DeepSeek fosters continuous learning by leveraging publicly available 
open-source datasets, possibly enhancing adaptability to the ever-evolving 
medical knowledge and scientific reasoning. Its transparent, community-driven 
approach may enable greater customization, regional specialization, and 
collaboration among data researchers and clinicians. Additionally, DeepSeek 
supports offline deployment, addressing some data privacy concerns. Despite 
these promising advantages, DeepSeek presents ethical and regulatory challenges. 
Users' data privacy worries have emerged, with concerns about user data 
retention policies and potential developer access to user-generated content 
without opt-out options. Additionally, when used in healthcare applications, its 
compliance with China's data-sharing regulations highlights the urgent need for 
clear international data privacy and governance. Furthermore, like other LLMs, 
DeepSeek may face limitations related to inherent biases, hallucinations, and 
output reliability, which warrants rigorous validation and human oversight 
before clinical application. This editorial explores DeepSeek's potential role 
in clinical workflows, medical education, and research while also highlighting 
its challenges related to security, accuracy, and responsible AI governance. 
With careful implementation, ethical considerations, and international 
collaboration, DeepSeek and similar LLMs could enhance healthcare innovation, 
providing cost-effective, scalable AI solutions while ensuring human expertise 
remains at the forefront of patient care.

Copyright © 2025, Temsah et al.

DOI: 10.7759/cureus.79221
PMCID: PMC11836063
PMID: 39974299

Conflict of interest statement: Conflicts of interest: In compliance with the 
ICMJE uniform disclosure form, all authors declare the following: 
Payment/services info: All authors have declared that no financial support was 
received from any organization for the submitted work. Financial relationships: 
All authors have declared that they have no financial relationships at present 
or within the previous three years with any organizations that might have an 
interest in the submitted work. Other relationships: All authors have declared 
that there are no other relationships or activities that could appear to have 
influenced the submitted work.


484. JMIR AI. 2025 Feb 18;4:e63701. doi: 10.2196/63701.

Predicting Satisfaction With Chat-Counseling at a 24/7 Chat Hotline for the 
Youth: Natural Language Processing Study.

Hornstein S(1), Lueken U(1)(2), Wundrack R(3), Hilbert K(4).

Author information:
(1)Department of Psychology, Humboldt-Universität zu Berlin, Berlin, Germany.
(2)German Center for Mental Health (DZPG), Partner site Berlin/Potsdam, Potsdam, 
Germany.
(3)Krisenchat gGmbH, Berlin, Germany.
(4)Department of Psychology, HMU Erfurt - Health and Medical University Erfurt, 
Erfurt, Germany.

BACKGROUND: Chat-based counseling services are popular for the low-threshold 
provision of mental health support to youth. In addition, they are particularly 
suitable for the utilization of natural language processing (NLP) for improved 
provision of care.
OBJECTIVE: Consequently, this paper evaluates the feasibility of such a use 
case, namely, the NLP-based automated evaluation of satisfaction with the chat 
interaction. This preregistered approach could be used for evaluation and 
quality control procedures, as it is particularly relevant for those services.
METHODS: The consultations of 2609 young chatters (around 140,000 messages) and 
corresponding feedback were used to train and evaluate classifiers to predict 
whether a chat was perceived as helpful or not. On the one hand, we trained a 
word vectorizer in combination with an extreme gradient boosting (XGBoost) 
classifier, applying cross-validation and extensive hyperparameter tuning. On 
the other hand, we trained several transformer-based models, comparing model 
types, preprocessing, and over- and undersampling techniques. For both model 
types, we selected the best-performing approach on the training set for a final 
performance evaluation on the 522 users in the final test set.
RESULTS: The fine-tuned XGBoost classifier achieved an area under the receiver 
operating characteristic score of 0.69 (P<.001), as well as a Matthews 
correlation coefficient of 0.25 on the previously unseen test set. The selected 
Longformer-based model did not outperform this baseline, scoring 0.68 (P=.69). A 
Shapley additive explanations explainability approach suggested that help 
seekers rating a consultation as helpful commonly expressed their satisfaction 
already within the conversation. In contrast, the rejection of offered exercises 
predicted perceived unhelpfulness.
CONCLUSIONS: Chat conversations include relevant information regarding the 
perceived quality of an interaction that can be used by NLP-based prediction 
approaches. However, to determine if the moderate predictive performance 
translates into meaningful service improvements requires randomized trials. 
Further, our results highlight the relevance of contrasting pretrained models 
with simpler baselines to avoid the implementation of unnecessarily complex 
models.
TRIAL REGISTRATION: Open Science Framework SR4Q9; https://osf.io/sr4q9.

©Silvan Hornstein, Ulrike Lueken, Richard Wundrack, Kevin Hilbert. Originally 
published in JMIR AI (https://ai.jmir.org), 18.02.2025.

DOI: 10.2196/63701
PMCID: PMC11888103
PMID: 39965198

Conflict of interest statement: Conflicts of Interest: SH and RW are employed by 
krisenchat, the organization that provided the data for this study. SH is also 
employed by Elona Health, a provider of digital health applications for mental 
health in Germany. KH is a scientific advisor and received virtual stock options 
from Mental Tech GmbH, which develops an artificial intelligence–based chatbot 
providing mental health support.


485. Reg Anesth Pain Med. 2025 Feb 16:rapm-2024-106358. doi: 
10.1136/rapm-2024-106358. Online ahead of print.

Human versus artificial intelligence: evaluating ChatGPT's performance in 
conducting published systematic reviews with meta-analysis in chronic pain 
research.

Purewal A(1), Fautsch K(2), Klasova J(2), Hussain N(3), D'Souza RS(4).

Author information:
(1)Department of Orthopedic Surgery and Rehabilitation Medicine, SUNY Downstate 
Health Sciences University, Brooklyn, New York, USA.
(2)Department of Anesthesiology and Perioperative Medicine, Mayo Clinic, 
Rochester, Minnesota, USA.
(3)The Ohio State University, Columbus, Ohio, USA.
(4)Department of Anesthesiology and Perioperative Medicine, Mayo Clinic, 
Rochester, Minnesota, USA DSouza.Ryan@mayo.edu.

INTRODUCTION: Artificial intelligence (AI), particularly large-language models 
like Chat Generative Pre-Trained Transformer (ChatGPT), has demonstrated 
potential in streamlining research methodologies. Systematic reviews and 
meta-analyses, often considered the pinnacle of evidence-based medicine, are 
inherently time-intensive and demand meticulous planning, rigorous data 
extraction, thorough analysis, and careful synthesis. Despite promising 
applications of AI, its utility in conducting systematic reviews with 
meta-analysis remains unclear. This study evaluated ChatGPT's accuracy in 
conducting key tasks of a systematic review with meta-analysis.
METHODS: This validation study used data from a published meta-analysis on 
emotional functioning after spinal cord stimulation. ChatGPT-4o performed 
title/abstract screening, full-text study selection, and data pooling for this 
systematic review with meta-analysis. Comparisons were made against 
human-executed steps, which were considered the gold standard. Outcomes of 
interest included accuracy, sensitivity, specificity, positive predictive value, 
and negative predictive value for screening and full-text review tasks. We also 
assessed for discrepancies in pooled effect estimates and forest plot 
generation.
RESULTS: For title and abstract screening, ChatGPT achieved an accuracy of 
70.4%, sensitivity of 54.9%, and specificity of 80.1%. In the full-text 
screening phase, accuracy was 68.4%, sensitivity 75.6%, and specificity 66.8%. 
ChatGPT successfully pooled data for five forest plots, achieving 100% accuracy 
in calculating pooled mean differences, 95% CIs, and heterogeneity estimates (I2 
score and tau-squared values) for most outcomes, with minor discrepancies in 
tau-squared values (range 0.01-0.05). Forest plots showed no significant 
discrepancies.
CONCLUSION: ChatGPT demonstrates modest to moderate accuracy in screening and 
study selection tasks, but performs well in data pooling and meta-analytic 
calculations. These findings underscore the potential of AI to augment 
systematic review methodologies, while also emphasizing the need for human 
oversight to ensure accuracy and integrity in research workflows.

© American Society of Regional Anesthesia & Pain Medicine 2025. Re-use permitted 
under CC BY-NC. No commercial re-use. Published by BMJ Group.

DOI: 10.1136/rapm-2024-106358
PMID: 39956557

Conflict of interest statement: Competing interests: RSD'S received 
investigator-initiated research grant funding from Nevro Corp and Saol 
Therapeutics paid to his institution. Other authors declare no conflicts of 
interest.


486. J Allergy Clin Immunol. 2025 Jul;156(1):81-87. doi: 10.1016/j.jaci.2025.02.004. 
Epub 2025 Feb 14.

Evaluating large language model performance to support the diagnosis and 
management of patients with primary immune disorders.

Rider NL(1), Li Y(2), Chin AT(3), DiGiacomo DV(4), Dutmer C(5), Farmer JR(6), 
Roberts K(7), Savova G(2), Ong MS(8).

Author information:
(1)Department of Health Systems and Implementation Science, Virginia Tech 
Carilion School of Medicine, Roanoke, Va; Section of Allergy and Immunology, 
Department of Medicine, Carilion Clinic, Roanoke, Va. Electronic address: 
nick70@vt.edu.
(2)Computational Health Informatics Program, Harvard Medical School and Boston 
Children's Hospital, Boston, Mass.
(3)Department of Pediatrics, Division of Immunology, Allergy, and Rheumatology, 
University of California, Los Angeles, Los Angeles, Calif.
(4)Department of Pediatrics, K. Hovnanian Children's Hospital, Jersey Shore 
University Medical Center, Neptune, NJ; Hackensack Meridian School of Medicine, 
Nutley, NJ.
(5)Department of Pediatrics, University of Colorado School of Medicine, Denver, 
Colo; Section of Allergy & Immunology, Children's Hospital Colorado, Denver, 
Colo.
(6)Clinical Immunodeficiency Program of Beth Israel Lahey Health, Division of 
Allergy and Immunology, Lahey Hospital & Medical Center, Burlington, Mass; 
Department of Medicine, UMass Chan Medical School, Burlington, Mass.
(7)McWilliams School of Biomedical Informatics, University of Texas Health 
Science Center at Houston, Houston, Tex.
(8)Department of Population Medicine, Harvard Pilgrim Health Care Institute, 
Harvard Medical School, Boston, Mass.

BACKGROUND: Generative artificial intelligence (GAI) is transforming health care 
in a variety of ways; however, the present utility of GAI for supporting 
clinicians who treat rare disease such as primary immune disorders (PIs) is not 
well studied. We evaluated the ability of 6 state-of-the-art large language 
models (LLMs) for providing clinical guidance about PIs.
OBJECTIVE: To quantitatively and qualitatively measure the utility of current, 
open-source LLMs for diagnosing and providing helpful clinical decision support 
about PIs.
METHODS: Five expert clinical immunologists each provided 5 real-world, 
anonymized PI case vignettes via multi-turn prompting to 6 LLMs (OpenAI GPT-4o, 
Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct, Mistral-7B-Instruct-v0.3, 
Mistral-Large-Instruct-2407, Mixtral-8x7B-Instruct-v0.1). We assessed the 
diagnostic accuracy of the LLMs and the quality of clinical reasoning using the 
Revised-IDEA (R-IDEA) score. Qualitative LLM assessment was made by immunologist 
narratives.
RESULTS: Performance accuracy (>88%) and R-IDEA scores (≥8) were superior for 3 
models (GPT-4o, Llama-3.1-70B-Instruct, Mistral-Large-Instruct-2407), with 
GPT-4o achieving the highest diagnostic accuracy (96.2%). Conversely, the 
remaining 3 models fell below acceptable accuracy rates near 60% or lower and 
had poor R-IDEA scores (≤0.55), with Mistral-7B-Instruct-v0.3 attaining the 
worst diagnostic accuracy (42.3%). Compared with the 3 best-performing LLMs, the 
3 worst-performing LLMs had a substantially lower median R-IDEA score (P < 
.001). Interclass correlation coefficient for R-IDEA score assignments varied 
substantially by LLM, ranging from good to poor agreement, and did not appear to 
correlate with either diagnostic accuracy or median R-IDEA score. Qualitatively, 
immunologists identified several themes (eg, correctness, differential diagnosis 
appropriateness, relative conciseness of explanations) of relevance to PIs.
CONCLUSIONS: LLM can support diagnosis and management of PIs; however, further 
tuning is needed to optimize LLMs for best practice recommendations.

Copyright © 2025 American Academy of Allergy, Asthma & Immunology. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jaci.2025.02.004
PMCID: PMC12229761
PMID: 39956279 [Indexed for MEDLINE]

Conflict of interest statement: Disclosure statement N.L.R. receives funding 
from National Institute of Allergy and Infectious Diseases (R21AI164100), the 
Centers for Disease Control and Prevention (NE11CD000002) via the Jeffrey Modell 
Foundation, and Takeda Pharmaceuticals. J.R.F. was supported by a faculty 
development award from the American Academy of Allergy, Asthma & Immunology and 
the National Institute on Minority Health and Health Disparities of the National 
Institutes of Health (NIH) (R01MD017816). Y.L. and G.S. receive funding from the 
NIH (R01GM114355). M.-S.O. receives funding from the NIH (R01MD017816) and 
Patient-Centered Outcomes Research Institute (PCORI) (ME-2022C2-25573). Research 
reported in this publication was partially funded through a PCORI award 
(ME-2022C2-25573). Disclosure of potential conflict of interest: N. L. Rider 
serves on scientific advisory committees for Takeda, Pharming, and Grifols. He 
receives royalties from Wolters Kluwer for contributions to UpToDate. J. R. 
Farmer is an ongoing consultant for Pharming and has received 
investigator-initiated research grants from Pfizer, Bristol Myers Squibb, and 
Pharming with no direct relation to the work presented. M.-S. Ong has received 
an investigator-initiated research grant from Pharming with no direct relation 
to the work presented. The rest of the authors declare that they have no 
relevant conflicts of interest.


487. J Autism Dev Disord. 2025 Feb 15. doi: 10.1007/s10803-025-06734-x. Online ahead 
of print.

Using Artificial Intelligence to Improve Empathetic Statements in Autistic 
Adolescents and Adults: A Randomized Clinical Trial.

Koegel LK(1), Ponder E(2), Bruzzese T(3), Wang M(3), Semnani SJ(3), Chi N(3), 
Koegel BL(2), Lin TY(2), Swarnakar A(3), Lam MS(3).

Author information:
(1)Department of Psychiatry and Behavioral Sciences, Stanford University School 
of Medicine, Stanford, CA, USA. Lynnk@Stanford.edu.
(2)Department of Psychiatry and Behavioral Sciences, Stanford University School 
of Medicine, Stanford, CA, USA.
(3)Department of Computer Science, Stanford University, Stanford, CA, USA.

Challenges with social communication and social interaction are a defining 
characteristic of autism spectrum disorder (ASD). These challenges frequently 
interfere with making friendships, securing and maintaining employment, and can 
lead to co-occurring conditions. While face-to-face clinical interventions with 
trained professionals can be helpful in improving social conversation, they can 
be costly and are unavailable to many, particularly given the high prevalence of 
ASD and lack of professional training. The purpose of this study was to assess 
whether an AI program using a Large Language Model (LLM) would improve verbal 
empathetic responses during social conversation. Autistic adolescents and 
adults, 11-35 years of age, who were able to engage in conversation but 
demonstrated challenges with empathetic responses participated in this study. A 
randomized clinical trial design was used to assess the effects of the AI 
program (Noora) compared to a waitlist control group. Noora asks participants to 
respond to leading statements and provides feedback on their answers. In this 
study, participants were asked to respond to 10 statements per day 5 days per 
week for 4 weeks for an expected total of 200 trials. Pre- and post-intervention 
conversation samples were collected to assess generalization during natural 
conversation. Additionally pre- and post-intervention questionnaires regarding 
each participant's comfort during social conversation and participants' 
satisfaction with the AI program were collected. The results of this study 
demonstrated that empathetic responses could be greatly improved by using an AI 
program for a short period of time. Participants in the experimental group 
showed statistically significant improvements in empathetic responses, which 
generalized to social conversation, compared to the waitlist control group. Some 
participants in the experimental group reported improved confidence in targeted 
areas and most reported high levels of satisfaction with the program. These 
findings suggest that AI using LLMs can be used to improve empathetic responses, 
thereby providing a time- and cost-efficient support program for improving 
social conversation in autistic adolescents and adults.

© 2025. The Author(s).

DOI: 10.1007/s10803-025-06734-x
PMID: 39954218

Conflict of interest statement: Declarations. Conflict of interest: Lynn K. 
Koegel is the editor of JADD. She was not involved in the editorial process. 
Lynn Koegel is a partner in the private company Koegel Autism Consultants, LLC. 
The other authors declare no conflicts of interest. Ethical Approval: This study 
was approved by the medical research ethics committee of Stanford School of 
Medicine and carried out according to the latest version of the Helsinki 
Declaration of 1975. All participants signed informed consent, parents of minor 
participants signed informed consents, and all minor participants signed 
informed assent.


488. J Orthop Surg Res. 2025 Feb 14;20(1):161. doi: 10.1186/s13018-025-05564-y.

Deep learning-assisted screening and diagnosis of scoliosis: segmentation of 
bare-back images via an attention-enhanced convolutional neural network.

Duan X(#)(1)(2), Ma X(#)(1)(2), Zhu M(1)(2), Wang L(1)(2), You D(3), Deng L(4), 
Niu N(5)(6).

Author information:
(1)Department of Orthopedics, General Hospital of Ningxia Medical University, 
Yinchuan, Ningxia, 750004, China.
(2)The First Clinical Medical College of Ningxia Medical University, Yinchuan, 
Ningxia, 750004, China.
(3)Department of Spinal Cord Surgery, Henan Provincial People's Hospital, 
Zhengzhou, Henan, 450000, China.
(4)Department of General Practice, Zhengzhou First People's Hospital, Zhengzhou, 
Henan, 450000, China.
(5)Department of Orthopedics, General Hospital of Ningxia Medical University, 
Yinchuan, Ningxia, 750004, China. niuningkui@163.com.
(6)Research Center for Prevention and Control of Bone and Joint Tuberculosis, 
General Hospital of Ningxia Medical University, Yinchuan, Ningxia, 750004, 
China. niuningkui@163.com.
(#)Contributed equally

BACKGROUND: Traditional diagnostic tools for scoliosis screening necessitate a 
substantial number of specialized personnel and equipment, leading to 
inconvenience that can result in missed opportunities for early diagnosis and 
optimal treatment. We have developed a deep learning-based image segmentation 
model to enhance the efficiency of scoliosis screening.
METHODS: A total of 350 patients with scoliosis and 108 healthy subjects were 
included in this study. The dataset was created using their bare back images and 
standing full-length anteroposterior spinal X-rays. An attention mechanism was 
incorporated into the original U-Net architecture to build a Dual AttentionUNet 
model for image segmentation. The entire dataset was divided into the training 
(321 cases), validation (46 cases), and test (91 cases) sets in a 7:1:2 ratio. 
The training set was used to train the Dual AttentionUNet model, and the 
validation set was used to fine-tune hyperparameters and prevent overfitting 
during training. The performance of the model was evaluated in the test set. 
After automatic segmentation of the back contour, a back asymmetry index was 
calculated via computer vision algorithms to classify scoliosis into different 
severities. The accuracy of classifications was statistically compared to those 
of three clinical experts.
RESULTS: Following the segmentation of bare back images and the application of 
computer vision algorithms, the Dual AttentionUNet model achieved an accuracy, 
precision, and recall rate of over 90% in predicting severe scoliosis. Notably, 
the model achieved an AUC value of 0.93 in identifying whether the subjects had 
scoliosis, which was higher than the 0.92 achieved by the deputy chief 
physician. In identifying severe scoliosis, their AUC values were 0.95 and 0.96, 
respectively.
CONCLUSION: The Dual AttentionUNet model, based on only bare back images, 
achieved accuracy and precision comparable to clinical physicians in determining 
scoliosis severity. Radiation-free, cost-saving, easy-to-operate and 
noninvasive, this model provides a novel option for large-scale scoliosis 
screening.

© 2025. The Author(s).

DOI: 10.1186/s13018-025-05564-y
PMCID: PMC11827350
PMID: 39953540 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The study was approved by the Ethics Review Committee of the 
General Hospital of Ningxia Medical University (No. KYLL20220133). All 
procedures were conducted in accordance with the ethical standards established 
by the Declaration of Helsinki and its subsequent amendments. Written informed 
consent was obtained from all participants. Consent for publication: Not 
applicable. Competing interests: The authors declare no competing interests.


489. Comput Biol Med. 2025 Apr;188:109787. doi: 10.1016/j.compbiomed.2025.109787. 
Epub 2025 Feb 12.

X-scPAE: An explainable deep learning model for embryonic lineage allocation 
prediction based on single-cell transcriptomics revealing key genes in embryonic 
cell development.

Liao K(1), Yan B(2), Ding Z(3), Huang J(4), Fan X(3), Wu S(5), Chen C(6), Li 
H(7).

Author information:
(1)The Central Laboratory of Birth Defects Prevention and Control, The 
Affiliated Women and Children's Hospital of Ningbo University, Ningbo, 315021, 
China; Ningbo Key Laboratory for the Prevention and Treatment of Embryogenic 
Diseases, The Affiliated Women and Children's Hospital of Ningbo University, 
Ningbo, 315021, China; Ningbo Key Laboratory of Genomic Medicine and Birth 
Defects Prevention, The Affiliated Women and Children's Hospital of Ningbo 
University, Ningbo, 315021, China; MOE Engineering Research Center of Gene 
Technology, School of Life Sciences, Fudan University, Shanghai, 200433, China.
(2)Institutes of Biomedical Sciences, Fudan University, Shanghai, 200433, China. 
Electronic address: 22111510005@m.fudan.edu.cn.
(3)Center for Reproductive Medicine, The Affiliated Women and Children's 
Hospital of Ningbo University, Ningbo, 315021, China.
(4)College of Computer Science, Chongqing University, Chongqing, 400044, China.
(5)Ningbo Key Laboratory of Genomic Medicine and Birth Defects Prevention, The 
Affiliated Women and Children's Hospital of Ningbo University, Ningbo, 315021, 
China.
(6)Ningbo Key Laboratory for the Prevention and Treatment of Embryogenic 
Diseases, The Affiliated Women and Children's Hospital of Ningbo University, 
Ningbo, 315021, China.
(7)The Central Laboratory of Birth Defects Prevention and Control, The 
Affiliated Women and Children's Hospital of Ningbo University, Ningbo, 315021, 
China; Ningbo Key Laboratory for the Prevention and Treatment of Embryogenic 
Diseases, The Affiliated Women and Children's Hospital of Ningbo University, 
Ningbo, 315021, China; Ningbo Key Laboratory of Genomic Medicine and Birth 
Defects Prevention, The Affiliated Women and Children's Hospital of Ningbo 
University, Ningbo, 315021, China. Electronic address: lihaibo-775@163.com.

In single-cell transcriptomics research, accurately predicting cell lineage 
allocation and identifying differences between lineages are crucial for 
understanding cell differentiation processes and reducing early pregnancy 
miscarriages in humans. This paper introduces an explainable PCA-based deep 
learning attention autoencoder model, X-scPAE (eXplained Single Cell PCA - 
Attention Auto Encoder), which is built on the Counterfactual Gradient 
Attribution (CGA) algorithm. The model is designed to predict lineage allocation 
in human and mouse single-cell transcriptomic data, while identifying and 
interpreting gene expression differences across lineages to extract key genes. 
It first reduces dimensionality using Principal Component Analysis (PCA) and 
ranks the importance of principal components. An autoencoder is then employed 
for feature extraction, integrating an attention mechanism to capture 
interactions between features. Finally, the Counterfactual Gradient Attribution 
algorithm calculates the importance of each feature. The model achieved an 
accuracy of 0.945 on the test set and 0.977 on the validation set, with other 
metrics such as F1-score, Precision, and Recall all reaching 0.94. It 
significantly outperformed both baseline algorithms (XGBoost, SVM, RF, and LR) 
and advanced approaches like F-Score-SVM, CV2-LR, scChrBin, and TripletCell. 
Notably, the explainability analysis uncovered key lineage predictor genes for 
both humans and mice and identified crucial genes distinguishing between 
developmental stages and lineages. A logistic regression model built using the 
extracted key genes still achieved an AUROC of 0.92, surpassing the performance 
of other feature extraction methods, including F-Score, CV2, PCA, random feature 
selection, and the interpretability method Shapley. Lastly, ablation studies 
demonstrated the effectiveness of each model component.

Copyright © 2025 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.compbiomed.2025.109787
PMID: 39946788 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


490. Diagnostics (Basel). 2025 Jan 25;15(3):285. doi: 10.3390/diagnostics15030285.

MammoViT: A Custom Vision Transformer Architecture for Accurate BIRADS 
Classification in Mammogram Analysis.

Al Mansour AGM(1), Alshomrani F(2), Alfahaid A(3), Almutairi ATM(4).

Author information:
(1)Radiology and Medical Imaging Department, College of Applied Medical 
Sciences, Prince Sattam Bin Abdulaziz University, Alkharj 11942, Saudi Arabia.
(2)Department of Diagnostic Radiology Technology, College of Applied Medical 
Science, Taibah University, Medinah 42353, Saudi Arabia.
(3)College of Computer Science and Engineering, Taibah University, Yanbu 46421, 
Saudi Arabia.
(4)Department of Computer, College of Science and Humanities, Shaqra University, 
Shaqra 11961, Saudi Arabia.

Background: Breast cancer screening through mammography interpretation is 
crucial for early detection and improved patient outcomes. However, the manual 
classification of mammograms using the BIRADS (Breast Imaging-Reporting and Data 
System) remains challenging due to subtle imaging features, inter-reader 
variability, and increasing radiologist workload. Traditional computer-aided 
detection systems often struggle with complex feature extraction and contextual 
understanding of mammographic abnormalities. To address these limitations, this 
study proposes MammoViT, a novel hybrid deep learning framework that leverages 
both ResNet50's hierarchical feature extraction capabilities and Vision 
Transformer's ability to capture long-range dependencies in images. Methods: We 
implemented a multi-stage approach utilizing a pre-trained ResNet50 model for 
initial feature extraction from mammogram images. To address the significant 
class imbalance in our four-class BIRADS dataset, we applied SMOTE (Synthetic 
Minority Over-sampling Technique) to generate synthetic samples for minority 
classes. The extracted feature arrays were transformed into non-overlapping 
patches with positional encodings for Vision Transformer processing. The Vision 
Transformer employs multi-head self-attention mechanisms to capture both local 
and global relationships between image patches, with each attention head 
learning different aspects of spatial dependencies. The model was optimized 
using Keras Tuner and trained using 5-fold cross-validation with early stopping 
to prevent overfitting. Results: MammoViT achieved 97.4% accuracy in classifying 
mammogram images across different BIRADS categories. The model's effectiveness 
was validated through comprehensive evaluation metrics, including a 
classification report, confusion matrix, probability distribution, and 
comparison with existing studies. Conclusions: MammoViT effectively combines 
ResNet50 and Vision Transformer architectures while addressing the challenge of 
imbalanced medical imaging datasets. The high accuracy and robust performance 
demonstrate its potential as a reliable tool for supporting clinical 
decision-making in breast cancer screening.

DOI: 10.3390/diagnostics15030285
PMCID: PMC11817779
PMID: 39941215

Conflict of interest statement: The authors declare no conflicts of interest.


491. J Pediatr Urol. 2025 Jun;21(3):639-643. doi: 10.1016/j.jpurol.2025.01.026. Epub 
2025 Jan 31.

Evaluating the utility of ChatGPT in enhancing parental education and clinical 
support in hypospadias care.

Adriansyah IA(1), Wahyudi I(2), Vallasciani S(3), Elhout Y(4), Bhat A(5), 
Priyadarshi S(6), Ansari MS(7), Raharja PAR(8), Abbas TO(9).

Author information:
(1)Department of Urology, Faculty of Medicine, Universitas Indonesia, Jakarta, 
Indonesia. Electronic address: iaadriansyah@gmail.com.
(2)Department of Urology, Faculty of Medicine, Universitas Indonesia, Jakarta, 
Indonesia. Electronic address: irf.wahyudi2011@gmail.com.
(3)Urology Division, Sidra Medicine, Doha, Qatar. Electronic address: 
svallasciani@sidra.org.
(4)Urology Division, Sidra Medicine, Doha, Qatar. Electronic address: 
yelhout@sidra.org.
(5)Bhat's Hypospadias and Reconstructive Urology Hospital and Research Centre, 
Jaipur, India. Electronic address: bhatamilal@gmail.com.
(6)SMS Medical College, Jaipur, Rajasthan, India. Electronic address: 
spriyadarshi64@gmail.com.
(7)Department of Urology and Renal Transplantation, Sanjay Gandhi Postgraduate 
Institute of Medical Sciences, Lucknow, Uttar Pradesh, India. Electronic 
address: ansarimsa@hotmail.com.
(8)Department of Urology, Faculty of Medicine, Universitas Indonesia, Jakarta, 
Indonesia. Electronic address: anggariskyraharja@gmail.com.
(9)Urology Division, Sidra Medicine, Doha, Qatar; College of Medicine, Qatar 
University, Doha, Qatar. Electronic address: tariq2c@hotmail.com.

INTRODUCTION: Hypospadias is a prevalent congenital anomaly that requires 
effective parental education. Current online resources often exceed recommended 
readability levels, potentially hindering understanding. This study evaluates 
the utility of ChatGPT in providing accurate, clear, and actionable information 
towards parental education about hypospadias.
METHODS: A structured set of questions was posed to ChatGPT 4.0 covering 
diagnosis, treatment options, and postoperative care. Responses were 
quantitatively assessed using the Patient Education Material Assessment Tool for 
Printable Materials (PEMAT-P) to measure understandability and actionability. 
Qualitative evaluations were conducted by six pediatric urologists who rated the 
information for accuracy on a scale from 1 (completely accurate) to 4 
(completely inaccurate). The Fleiss' Kappa statistic was calculated to assess 
inter-observer agreement among the urologists.
RESULTS: The quantitative assessment yielded understandability scores between 84 
% and 92 % (average 88 %), while actionability scores ranged from 37 % to 70 % 
(average 51 %). In the qualitative assessment, 41 % of responses were deemed 
completely accurate, with 35 % considered accurate but inadequate, and 24 % 
rated as inaccurate. The overall Kappa value was 0.607, indicating substantial 
agreement among reviewers regarding the accuracy of the information provided by 
ChatGPT.
CONCLUSION: ChatGPT can effectively convey information about hypospadias, but 
enhancing the actionability of its responses is crucial. Inaccuracy is still a 
main concern in using AI-generated search engine. Future updates should include 
more accurate and reliable responses and visual aids addition may support 
parents in navigating their child's care.

Copyright © 2025 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.jpurol.2025.01.026
PMID: 39939234 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of interest Authors also did not have 
any conflict of interest in making this paper.


492. Expert Rev Med Devices. 2025 Mar;22(3):233-241. doi: 
10.1080/17434440.2025.2466742. Epub 2025 Feb 16.

Effectiveness of mental health chatbots in depression and anxiety for 
adolescents and young adults: a meta-analysis of randomized controlled trials.

Chen TH(1), Chu G(2)(3), Pan RH(4)(5)(6), Ma WF(7)(8).

Author information:
(1)PhD Program for Health Science and Industry, China Medical University, 
Taichung, Taiwan.
(2)School of Nursing and Midwifery, College of Health, Medicine and Wellbeing, 
The University of Newcastle, New South Wales, Australia.
(3)College of Health, Medicine and Wellbeing, The University of Newcastle, New 
South Wales, Australia.
(4)Founder, La Vida Tec. Co. Ltd., Taichung, Taichung, Taiwan (R.O.C.).
(5)Institute of Hospital and Health Care Administration, National Yang Ming 
Chiao Tung University, Taipei, Taiwan (R.O.C.).
(6)Department of Information Management, Tunghai University, Taichung, Taiwan 
(R.O.C.).
(7)School of Nursing, China Medical University, Taichung, Taiwan.
(8)Department of Nursing, China Medical University Hospital, Taichung, Taiwan.

BACKGROUND: The mental health chatbot is dedicated to providing assistance to 
individuals grappling with the complexities of depression and anxiety.
OBJECTIVE: The study aimed to evaluate the effectiveness of the mental health 
chatbot in alleviating symptoms of depression and anxiety among adolescents and 
young adults.
METHODS: A systematic review framework was employed with a protocol 
pre-registered on Prospero (CRD42023418877). Databases were systematically 
searched, including PubMed, ACM Digital Library, Embase, Cochrane and IEEE. Data 
synthesis was conducted narratively, and meta-analysis was performed by pooling 
data from the original studies.
RESULTS: Ten randomized controlled trials focused on an acute population, mainly 
females and university students. Chatbots designed for daily conversations and 
mood monitoring, using cognitive behavioral therapy techniques, showed efficacy 
in treating depression (95% CI = -1.09 to -0.23; p = .003). However, it is 
essential to highlight that these interventions utilizing chatbots for mental 
health were not found to be efficacious in managing symptoms of anxiety (95% 
CI = -0.56 to 0.4; p = .74).
CONCLUSIONS: Evidence supports the effectiveness of mental health chatbots in 
treating depression, but further exploration and refinement are needed to 
optimize their efficacy in managing anxiety.

DOI: 10.1080/17434440.2025.2466742
PMID: 39935147 [Indexed for MEDLINE]


493. Commun Biol. 2025 Feb 10;8(1):209. doi: 10.1038/s42003-025-07479-0.

A deep learning-based system for automatic detection of emesis with high 
accuracy in Suncus murinus.

Lu Z(#)(1), Qiao Y(#)(2), Huang X(1), Cui D(1), Liu JYH(1), Ngan MP(1), Liu 
L(1), Huang Z(3), Li ZT(1), Yang L(1), Khalid A(1), Deng Y(1), Chan SW(4), Tu 
L(5), Rudd JA(6).

Author information:
(1)Emesis Research Group, School of Biomedical Sciences, Faculty of Medicine, 
The Chinese University of Hong Kong, Shatin, New Territories, Hong Kong.
(2)Department of Chemical and Biological Engineering, The Hong Kong University 
of Science and Technology, Clear Water Bay, Kowloon, Hong Kong.
(3)School of Health Sciences, University of Manchester, Manchester, UK.
(4)School of Health Sciences, Saint Francis University, Tseung Kwan O, New 
Territories, Hong Kong.
(5)Department of Pediatrics, USDA/ARS Children's Nutrition Research Center, 
Baylor College of Medicine, Houston, TX, USA.
(6)Emesis Research Group, School of Biomedical Sciences, Faculty of Medicine, 
The Chinese University of Hong Kong, Shatin, New Territories, Hong Kong. 
jar@cuhk.edu.hk.
(#)Contributed equally

Quantifying emesis in Suncus murinus (S. murinus) has traditionally relied on 
direct observation or reviewing recorded behaviour, which are laborious, 
time-consuming processes that are susceptible to operator error. With rapid 
advancements in deep learning, automated animal behaviour quantification tools 
with high accuracy have emerged. In this study, we pioneere the use of both 
three-dimensional convolutional neural networks and self-attention mechanisms to 
develop the Automatic Emesis Detection (AED) tool for the quantification of 
emesis in S. murinus, achieving an overall accuracy of 98.92%. Specifically, we 
use motion-induced emesis videos as training datasets, with validation results 
demonstrating an accuracy of 99.42% for motion-induced emesis. In our model 
generalisation and application studies, we assess the AED tool using various 
emetics, including resiniferatoxin, nicotine, copper sulphate, naloxone, U46619, 
cyclophosphamide, exendin-4, and cisplatin. The prediction accuracies for these 
emetics are 97.10%, 100%, 100%, 97.10%, 98.97%, 96.93%, 98.91%, and 98.41%, 
respectively. In conclusion, employing deep learning-based automatic analysis 
improves efficiency and accuracy and mitigates human bias and errors. Our study 
provides valuable insights into the development of deep learning neural network 
models aimed at automating the analysis of various behaviours in S. murinus, 
with potential applications in preclinical research and drug development.

© 2025. The Author(s).

DOI: 10.1038/s42003-025-07479-0
PMCID: PMC11811283
PMID: 39930110 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: The authors declare no 
competing interests. Ethical approval: All animal care and experimental 
procedures were conducted under license from the Government of the Hong Kong SAR 
and the Animal Experimentation Ethics Committee of The Chinese University of 
Hong Kong (Approved No.: 23-246-MIS). We have complied with all relevant ethical 
regulations for animal use.


494. J Cosmet Dermatol. 2025 Feb;24(2):e70022. doi: 10.1111/jocd.70022.

A Novel Technique for Eye Rejuvenation: A Case Series of the Combined Use of 
CO(2) Laser Blepharoplasty and Erbium: YAG Resurfacing and A Novel Artificial 
Intelligence Model to Quantify Laser Results.

Kesty CE(1)(2), Kesty KR(1)(2).

Author information:
(1)St. Petersburg Skin and Laser, St. Petersburg, Florida, USA.
(2)Kesty AI, St. Petersburg, Florida, USA.

BACKGROUND: Consumers are searching for a solution to rejuvenate the eye area. 
Surgical blepharoplasties are a common solution, but they lack improvement in 
skin quality.
AIMS: To present a novel procedure of a laser upper blepharoplasty in 
combination with erbium laser resurfacing of the lower eyelid for optimal 
rejuvenation and minimal complications.
METHODS: The authors present a laser upper blepharoplasty with the CO2 laser 
performed at the same time as lower eyelid resurfacing using an erbium laser. 
The authors used an artificial intelligence large language model to assess the 
patient before and after photographs to quantify cosmetic improvement.
RESULTS: After this novel procedure, patients demonstrated significant 
improvements in upper eyelid contour, reduced skin laxity, and smoother lower 
eyelid texture. Patient satisfaction was high, with each patient reporting an 
overall rejuvenated appearance and a more "awake" and youthful look. The 
artificial intelligence algorithm showed cosmetic improvement in line with the 
clinical evaluations by the patient and physician.
CONCLUSIONS: The combination of CO2 laser blepharoplasty and Er:YAG laser 
resurfacing addresses both upper eyelid dermatochalasis and lower eyelid 
wrinkles effectively while minimizing recovery time and the potential for 
complications. Artificial intelligence models were used to enhance this study 
and corroborate evaluator cosmetic improvement.

© 2025 The Author(s). Journal of Cosmetic Dermatology published by Wiley 
Periodicals LLC.

DOI: 10.1111/jocd.70022
PMCID: PMC11806243
PMID: 39921282 [Indexed for MEDLINE]

Conflict of interest statement: K.R.K. is the Founder of Kesty AI, which was 
used in this paper.


495. Urology. 2025 Apr;198:130-134. doi: 10.1016/j.urology.2025.01.054. Epub 2025 Feb 
4.

Quality of Information on Wilms Tumor From Artificial Intelligence Chatbots: 
What Are Your Patients and Their Families Reading?

Stapleton P(1), Santucci J(2), Cundy TP(2), Sathianathen N(2).

Author information:
(1)Grampian's Health, Ballarat, VIC, Australia; Grampian's Health Regional 
Research Organisation, GHRRO Urology, Ballarat, VIC, Australia. Electronic 
address: peter.stapleton@outlook.com.
(2)Grampian's Health, Ballarat, VIC, Australia; Melbourne University, Melbourne, 
VIC, Australia; Grampian's Health Regional Research Organisation, GHRRO Urology, 
Ballarat, VIC, Australia.

OBJECTIVE: To assess the ability of AI chatbots to deliver quality and 
understandable information on Wilms tumors to patients and their families.
METHODS: Google trends were used to evaluate the most asked questions related to 
Wilms tumor. Four AI chatbots (ChatGPT version 3.5, Perplexity, Chat Sonic, and 
Bing AI) were then used to assess these questions and their responses reviewed. 
Validated instruments were used to assess the quality (DISCERN instrument from 1 
low to 5 high), understandability and actionability (PEMAT, from 0% to 100%), 
the reading level of the information and whether there was misinformation 
compared to guidelines (5-point Likert scale).
RESULTS: All AI chat bots provided a high level of patient health information 
with a median DISCERN score of 4 (IQR 3-5). Additionally, there was little to no 
misinformation in outputs with a median of 1 (IQR 1-1). The median word count 
per output from the AIs was 275 (IQR 156-322), with an advanced ease of reading 
level comparable to a high school or college student, median Flesch-Kincaid 
Readability level of 46.7 (IQR 41.1-52.2). The overall PEMAT actionability was 
poor with a median of 40% (40-65), while the PEMAT understandability of the AI 
chatbot outputs was high, 83% (IQR 75-91.2).
CONCLUSION: AI chatbots provide generalized, understandable and accurate 
information regarding Wilms tumor. They can be reliably used as a source for 
patients and families when seeking further information. However, much of the 
information is reliant of medical professionals and not easily actionable by 
consumers but may act as a guide to help with discussions and understanding 
treatments.

Crown Copyright © 2025. Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.urology.2025.01.054
PMID: 39914668 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
have no conflict of interest to declare.


496. MedEdPublish (2016). 2025 Apr 28;14:245. doi: 10.12688/mep.20554.3. eCollection 
2024.

Utilisation of ChatGPT and other Artificial Intelligence tools among medical 
faculty in Uganda: a cross-sectional study.

Mukunya D(1), Nantale R(1), Kayemba F(1), Ajalo E(1), Pangholi K(1), Babuya 
J(1), Langoya Akuu S(2), Namiiro AM(3), Tweheyo R(1), Ekak S(4), Nakitto B(4), 
Nantongo K(3), Mpagi JL(5), Musaba MW(6), Oguttu F(1), Kuteesa J(7), Mubuuke 
AG(8), Munabi IG(9), Kiguli S(3).

Author information:
(1)Department of Community and Public Health, Busitema University-Mbale Campus, 
Tororo, Eastern Region, Uganda.
(2)Medicine, Mbarara University of Science and Technology Faculty of Medicine, 
Mbarara, Western Region, Uganda.
(3)Department of Pediatrics and Child Health, Makerere University School of 
Health Sciences, Kampala, Central Region, Uganda.
(4)Faculty of Health Sciences, Gulu University Faculty of Science, Gulu, 
Northern Region, Uganda.
(5)Department of Microbiology and Immunology, Busitema University-Mbale Campus, 
Mbale, Uganda.
(6)Obstetrics and Gynaecology, Busitema University -Mbale Campus, Mbale, Uganda.
(7)Department of Surgery, Mulago National Referral Hospital, Kampala, Central 
Region, Uganda.
(8)Department of Radiology, Makerere University School of Health Sciences, 
Kampala, Central Region, Uganda.
(9)Department of Human Anatomy, Makerere University School of Health Sciences, 
Kampala, Central Region, Uganda.

BACKGROUND: ChatGPT is a large language model that uses deep learning techniques 
to generate human-like texts. ChatGPT has the potential to revolutionize medical 
education as it acts as an interactive virtual tutor and personalized learning 
assistant. We assessed the use of ChatGPT and other Artificial Intelligence (AI) 
tools among medical faculty in Uganda.
METHODS: We conducted a descriptive cross-sectional study among medical faculty 
at four public universities in Uganda from November to December 2023. 
Participants were recruited consecutively. We used a semi-structured 
questionnaire to collect data on participants' socio-demographics and the use of 
AI tools such as ChatGPT. Our outcome variable was the use of ChatGPT and other 
AI tools. Data were analyzed in Stata version 17.0.
RESULTS: We recruited 224 medical faculty, majority [75% (167/224)] were male. 
The median age (interquartile range) was 41 years (34-50). Almost all medical 
faculty [90% (202/224)] had ever heard of AI tools such as ChatGPT. Over 63% 
(120/224) of faculty had ever used AI tools. The most commonly used AI tools 
were ChatGPT (56.3%) and Quill Bot (7.1%). Fifty-six faculty use AI tools for 
research writing, 37 for summarizing information, 28 for proofreading work, and 
28 for setting exams or assignments. Forty faculty use AI tools for non-academic 
purposes like recreation and learning new skills. Faculty older than 50 years 
were 40% less likely to use AI tools compared to those aged 24 to 35 years 
(Adjusted Prevalence Ratio (aPR):0.60; 95% Confidence Interval (CI): [0.45, 
0.80]).
CONCLUSION: The use of ChatGPT and other AI tools was high among medical faculty 
in Uganda. Older faculty (>50 years) were less likely to use AI tools compared 
to younger faculty. Training on AI use in education, formal policies, and 
guidelines are needed to adequately prepare medical faculty for the integration 
of AI in medical education.

Copyright: © 2025 Mukunya D et al.

DOI: 10.12688/mep.20554.3
PMCID: PMC11795020
PMID: 39911314

Conflict of interest statement: No competing interests were disclosed.


497. Eur Radiol. 2025 Aug;35(8):4461-4471. doi: 10.1007/s00330-025-11410-w. Epub 2025 
Feb 5.

PlaqueViT: a vision transformer model for fully automatic vessel and plaque 
segmentation in coronary computed tomography angiography.

Alvén J(1), Petersen R(1), Hagerman D(1), Sandstedt M(2)(3), Kitslaar P(4), 
Bergström G(5)(6), Fagman E(7)(8), Hjelmgren O(9)(10).

Author information:
(1)Department of Electrical Engineering, Chalmers University of Technology, 
Gothenburg, Sweden.
(2)Department of Radiology in Linköping, Linköping University, Linköping, 
Sweden.
(3)Department of Health, Medicine and Caring Sciences and Center for Medical 
Image Science and Visualization (CMIV), Linköping University, Linköping, Sweden.
(4)Medis Medical Imaging, Leiden, The Netherlands.
(5)Department of Molecular and Clinical Medicine, Institute of Medicine, 
University of Gothenburg, Gothenburg, Sweden.
(6)Department of Clinical Physiology, Queen Silvia Children's Hospital, 
Sahlgrenska University Hospital, Region Västra Götaland, Gothenburg, Sweden.
(7)Department of Radiology, Institute of Clinical Sciences, University of 
Gothenburg, Gothenburg, Sweden.
(8)Department of Radiology, Queen Silvia Children's Hospital, Sahlgrenska 
University Hospital, Region Västra Götaland, Gothenburg, Sweden.
(9)Department of Molecular and Clinical Medicine, Institute of Medicine, 
University of Gothenburg, Gothenburg, Sweden. ola.hjelmgren@wlab.gu.se.
(10)Pediatric Heart Centre, Queen Silvia Children's Hospital, Sahlgrenska 
University Hospital, Region Västra Götaland, Gothenburg, Sweden. 
ola.hjelmgren@wlab.gu.se.

OBJECTIVES: To develop and evaluate a deep learning model for segmentation of 
the coronary artery vessels and coronary plaques in coronary computed tomography 
angiography (CCTA).
MATERIALS AND METHODS: CCTA image data from the Swedish CardioPulmonary BioImage 
Study (SCAPIS) was used for model development (n = 463 subjects) and testing 
(n = 123) and for an interobserver study (n = 65). A dataset from Linköping 
University Hospital (n = 28) was used for external validation. The model's 
ability to detect coronary artery disease (CAD) was tested in a separate SCAPIS 
dataset (n = 684). A deep ensemble (k = 6) of a customized 3D vision transformer 
model was used for voxelwise classification. The Dice coefficient, the average 
surface distance, Pearson's correlation coefficient, analysis of segmented 
volumes by intraclass correlation coefficient (ICC), and agreement (sensitivity 
and specificity) were used to analyze model performance.
RESULTS: PlaqueViT segmented coronary plaques with a Dice coefficient = 0.55, an 
average surface distance = 0.98 mm and ICC = 0.93 versus an expert reader. In 
the interobserver study, PlaqueViT performed as well as the expert reader (Dice 
coefficient = 0.51 and 0.50, average surface distance = 1.31 and 1.15 mm, 
ICC = 0.97 and 0.98, respectively). PlaqueViT achieved 88% agreement 
(sensitivity 97%, specificity 76%) in detecting any coronary plaque in the test 
dataset (n = 123) and 89% agreement (sensitivity 95%, specificity 83%) in the 
CAD detection dataset (n = 684).
CONCLUSION: We developed a deep learning model for fully automatic plaque 
detection and segmentation that identifies and delineates coronary plaques and 
the arterial lumen with similar performance as an experienced reader.
KEY POINTS: Question A tool for fully automatic and voxelwise segmentation of 
coronary plaques in coronary CTA (CCTA) is important for both clinical and 
research usage of the CCTA examination. Findings Segmentation of coronary artery 
plaques by PlaqueViT was comparable to an expert reader's performance. Clinical 
relevance This novel, fully automatic deep learning model for voxelwise 
segmentation of coronary plaques in CCTA is highly relevant for large population 
studies such as the Swedish CardioPulmonary BioImage Study.

© 2025. The Author(s).

DOI: 10.1007/s00330-025-11410-w
PMCID: PMC12226657
PMID: 39909898 [Indexed for MEDLINE]

Conflict of interest statement: Compliance with ethical standards. Guarantor: 
The scientific guarantor of this publication is Dr Ola Hjelmgren. Conflict of 
interest: The authors of this manuscript declare relationships with the 
following companies: P.K. is employed by Medis Medical Imaging. The remaining 
authors declare no conflicts of interest. Statistics and biometry: Two of the 
authors have significant statistical expertise (J.A., O.H.). Informed consent: 
Written informed consent was obtained from all subjects (patients) in this 
study. Ethical approval: Institutional Review Board approval was obtained. Study 
subjects or cohorts overlap: Not applicable. Methodology: Retrospective 
Cross-sectional study, diagnostic study Performed at one institution


498. Comput Methods Biomech Biomed Engin. 2025 Feb 5:1-22. doi: 
10.1080/10255842.2025.2456990. Online ahead of print.

Dynamic feature selection and quantum representation for precise heart disease 
prediction: Quantum-HeartDiseaseNet approach.

Kunjachen LM(1), Kavitha R(2).

Author information:
(1)Department of Computer Science and Engineering, SRM Institute of Science and 
Technology, Ramapuram, Chennai, Tamil Nadu, India.
(2)Department of Information Technology, SRM Institute of Science and 
Technology, Ramapuram, Chennai, Tamil Nadu, India.

Cardiovascular disease is a leading cause of mortality, necessitating early and 
precise prediction for improved patient outcomes. This study proposes 
Quantum-HeartDiseaseNet, a novel heart disease risk prediction framework that 
integrates a Dynamic Opposite Pufferfish Optimization Algorithm for feature 
selection and a Quantum Attention-based Bidirectional Gated Recurrent Unit 
(QABiGRU) for accurate diagnosis. The feature selection method enhances 
diagnosis accuracy while reducing dimensionality, and Synthetic Minority 
Oversampling Technique (SMOTE) addresses data imbalance. Evaluated on three 
heart disease datasets, the proposed model achieved 98.87% accuracy, 98.74% 
precision, and 98.56% recall, outperforming conventional methods. Experimental 
results validate its effectiveness in early disease prediction.

DOI: 10.1080/10255842.2025.2456990
PMID: 39909079


499. Eur J Cancer. 2025 Mar 11;218:115273. doi: 10.1016/j.ejca.2025.115273. Epub 2025 
Jan 30.

Cellular morphometric biomarkers and large language model predict prognosis and 
treatment response in neuroblastoma patients: A retrospective and double-blind 
prospective single arm clinical study.

Wang X(1), Mao AW(2), Pan S(3), Wang D(4), He L(5), Vogel H(6), Mao JH(4), Weiss 
W(7), Li T(3), Chang H(8).

Author information:
(1)Department of Endocrinology, Children's Hospital of Nanjing Medical 
University, Nanjing, China.
(2)Department of Mathematics, University of California, Los Angeles, Los 
Angeles, United States.
(3)Department of Oncology, Children's Hospital of Nanjing Medical University, 
Nanjing, China.
(4)Biological Systems and Engineering Division, Lawrence Berkeley National 
Laboratory, Berkeley, CA, United States; Berkeley Biomedical Data Science 
Center, Lawrence Berkeley National Laboratory, Berkeley, CA, United States.
(5)Department of Pathology, Children's Hospital of Nanjing Medical University, 
Nanjing, China.
(6)Department of Pathology, Stanford University Medical Center, Stanford, CA, 
United States.
(7)Department of Neurology, Neurological Surgery, and Pediatrics, University of 
California, San Francisco, San Francisco, CA, United States.
(8)Biological Systems and Engineering Division, Lawrence Berkeley National 
Laboratory, Berkeley, CA, United States; Berkeley Biomedical Data Science 
Center, Lawrence Berkeley National Laboratory, Berkeley, CA, United States. 
Electronic address: hchang@lbl.gov.

BACKGROUND: The heterogeneity of Neuroblastoma (NB) leads to variation in 
response to treatment and outcomes. The aim of the current study is to discover 
AI-empowered cellular morphometric biomarkers (CMBs), to establish the 
corresponding CMB risk score (CMBRS), CMB risk group (CMBRG), large language 
model driven CMB risk score (CMB-LLM-RS), and large language model driven CMB 
risk group (CMB-LLM-RG), and to investigate and validate their prognostic and 
predictive power in NB.
METHODS: In this study, the retrospective cohort enrolled 84 primary NBs between 
1/2020 and 12/2021, followed up through 11/22/2024; the prospective cohort 
enrolled 67 primary NBs between 1/2022 and 7/2023, followed up through 
11/22/2024.
RESULTS: We identified 9 CMBs from a retrospective NB cohort, enabling the 
CMBRS, CMBRG, CMB-LLM-RS, and CMB-LLM-RG. Both CMBRG and CMB-LLM-RG are 
significantly associated with prognosis (p < 0.0001) and treatment response 
(p < 0.0001). Furthermore, we double-blindly validated the predictive power of 
CMBRG and CMB-LLM-RG in a prospective NB cohort, which confirms their potential 
value in real clinical settings. Importantly, CMBRG provides clinical value 
independent of the International Neuroblastoma Risk Group (INRG) classification 
system in both retrospective and prospective NB cohorts (p < 0.05); and the 
combination of CMBRG and INRG significantly increases prognostic and predictive 
performance for NB patients.
CONCLUSIONS: These findings suggest that CMBRG and CMB-LLM-RG have prognostic 
and predictive value for NB and warrants evaluation in larger multicenter 
cohorts.

Copyright © 2025 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.ejca.2025.115273
PMID: 39908653 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare no conflict of interest.


500. Med Image Anal. 2025 Apr;101:103462. doi: 10.1016/j.media.2025.103462. Epub 2025 
Jan 29.

DSAM: A deep learning framework for analyzing temporal and spatial dynamics in 
brain networks.

Thapaliya B(1), Miller R(2), Chen J(1), Wang YP(3), Akbas E(4), Sapkota R(1), 
Ray B(1), Suresh P(1), Ghimire S(5), Calhoun VD(6), Liu J(7).

Author information:
(1)Tri-Institutional Center for Translational Research in NeuroImaging and Data 
Science (TreNDS) - Georgia State, Georgia Tech and Emory, USA; Department of 
Computer Science, Georgia State University, Atlanta, USA.
(2)Los Alamos National Laboratory, NM, USA.
(3)Tulane School of Medicine, Tulane University, New Orleans, USA.
(4)Department of Computer Science, Georgia State University, Atlanta, USA.
(5)Department of Applied Sciences and Chemical Engineering, Tribhuvan 
University, Nepal.
(6)Tri-Institutional Center for Translational Research in NeuroImaging and Data 
Science (TreNDS) - Georgia State, Georgia Tech and Emory, USA; Department of 
Computer Science, Georgia State University, Atlanta, USA; School of Electrical 
and Computer Engineering, Georgia Institute of Technology, Atlanta, USA.
(7)Tri-Institutional Center for Translational Research in NeuroImaging and Data 
Science (TreNDS) - Georgia State, Georgia Tech and Emory, USA; Department of 
Computer Science, Georgia State University, Atlanta, USA. Electronic address: 
jliu75@gsu.edu.

Resting-state functional magnetic resonance imaging (rs-fMRI) is a noninvasive 
technique pivotal for understanding human neural mechanisms of intricate 
cognitive processes. Most rs-fMRI studies compute a single static functional 
connectivity matrix across brain regions of interest, or dynamic functional 
connectivity matrices with a sliding window approach. These approaches are at 
risk of oversimplifying brain dynamics and lack proper consideration of the goal 
at hand. While deep learning has gained substantial popularity for modeling 
complex relational data, its application to uncovering the spatiotemporal 
dynamics of the brain is still limited. In this study we propose a novel 
interpretable deep learning framework that learns goal-specific functional 
connectivity matrix directly from time series and employs a specialized graph 
neural network for the final classification. Our model, DSAM, leverages temporal 
causal convolutional networks to capture the temporal dynamics in both low- and 
high-level feature representations, a temporal attention unit to identify 
important time points, a self-attention unit to construct the goal-specific 
connectivity matrix, and a novel variant of graph neural network to capture the 
spatial dynamics for downstream classification. To validate our approach, we 
conducted experiments on the Human Connectome Project dataset with 1075 samples 
to build and interpret the model for the classification of sex group, and the 
Adolescent Brain Cognitive Development Dataset with 8520 samples for independent 
testing. Compared our proposed framework with other state-of-art models, results 
suggested this novel approach goes beyond the assumption of a fixed connectivity 
matrix, and provides evidence of goal-specific brain connectivity patterns, 
which opens up potential to gain deeper insights into how the human brain adapts 
its functional connectivity specific to the task at hand. Our implementation can 
be found on https://github.com/bishalth01/DSAM.

Copyright © 2025 The Authors. Published by Elsevier B.V. All rights reserved.

DOI: 10.1016/j.media.2025.103462
PMID: 39892220 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare no competing interests.


501. BMC Med Educ. 2025 Jan 31;25(1):159. doi: 10.1186/s12909-025-06753-3.

Empowering medical students with AI writing co-pilots: design and validation of 
AI self-assessment toolkit.

Khojasteh L(1), Kafipour R(2), Pakdel F(3), Mukundan J(4).

Author information:
(1)Department of English Language, School of Paramedical Sciences, Shiraz 
University of Medical Sciences, Shiraz, Iran.
(2)Department of English Language, School of Paramedical Sciences, Shiraz 
University of Medical Sciences, Shiraz, Iran. rezakafipour@gmail.com.
(3)Department of English Language, School of Paramedical Sciences, Shiraz 
University of Medical Sciences, Shiraz, Iran. ffpakdel@yahoo.com.
(4)School of Education, Taylors University, Lakeside Campus, Malaysia.

BACKGROUND AND OBJECTIVES: Assessing and improving academic writing skills is a 
crucial component of higher education. To support students in this endeavor, a 
comprehensive self-assessment toolkit was developed to provide personalized 
feedback and guide their writing improvement. The current study aimed to 
rigorously evaluate the validity and reliability of this academic writing 
self-assessment toolkit.
METHODS: The development and validation of the academic writing self-assessment 
toolkit involved several key steps. First, a thorough review of the literature 
was conducted to identify the essential criteria for authentic assessment. Next, 
an analysis of medical students' reflection papers was undertaken to gain 
insights into their experiences using AI-powered tools for writing feedback. 
Based on these initial steps, a preliminary version of the self-assessment 
toolkit was devised. An expert focus group discussion was then convened to 
refine the questions and content of the toolkit. To assess content validity, the 
toolkit was evaluated by a panel of 22 medical student participants. They were 
asked to review each item and provide feedback on the relevance and 
comprehensiveness of the toolkit for evaluating academic writing skills. Face 
validity was also examined, with the students assessing the clarity, wording, 
and appropriateness of the toolkit items.
RESULTS: The content validity evaluation revealed that 95% of the toolkit items 
were rated as highly relevant, and 88% were deemed comprehensive in assessing 
key aspects of academic writing. Minor wording changes were suggested by the 
students to enhance clarity and interpretability. The face validity assessment 
found that 92% of the items were rated as unambiguous, with 90% considered 
appropriate and relevant for self-assessment. Feedback from the students led to 
the refinement of a few items to improve their clarity in the context of the 
Persian language. The robust reliability testing demonstrated the consistency 
and stability of the academic writing self-assessment toolkit in measuring 
students' writing skills over time.
CONCLUSION: The comprehensive evaluation process has established the academic 
writing self-assessment toolkit as a robust and credible instrument for 
supporting students' writing improvement. The toolkit's strong psychometric 
properties and user-centered design make it a valuable resource for enhancing 
academic writing skills in higher education.

© 2025. The Author(s).

DOI: 10.1186/s12909-025-06753-3
PMCID: PMC11786331
PMID: 39891148 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The study was approved by the local ethics council of Shiraz 
University of Medical Sciences (decree code: IR.SUMS.REC.1403.240). We made 
every effort to adhere to ethical principles in our research design and 
implementation: Informed Consent: We secured written consent from all 
participants, ensuring they were fully informed about the study's objectives, 
methodology, and the intended use of the data collected. Voluntary 
Participation: Participants were informed of their right to withdraw from the 
study at any time without any negative consequences, thereby ensuring their 
autonomy throughout the process. Confidentiality and Anonymity: We maintained 
the anonymity and confidentiality of all participants. All data collected was 
de-identified prior to analysis to further protect participants' identities. 
Data Security: Data was stored securely and was accessible only to the research 
team to guarantee the privacy of participants. Minimal Risk: The study involved 
minimal risk to participants, as it focused exclusively on their feedback 
concerning the self-assessment toolkit, without introducing interventions or 
collecting sensitive personal data. Beneficence: Our research aimed to enhance 
academic writing assessment and instruction, ultimately benefitting future 
students and educators through improved educational practices. Justice: We 
ensured that all eligible participants had equal opportunities to contribute to 
the study, promoting equitable participation. Consent for publication: Not 
applicable. Competing interests: The authors declare no competing interests.


502. J Manag Care Spec Pharm. 2025 Feb;31(2-a Suppl):S2-S10. doi: 
10.18553/jmcp.2025.31.2-a.s2.

Emerging trends in managed care pharmacy: A mixed-method study.

Mattingly TJ 2nd(1), Happe LE(2)(3), Cranston L(4)(5).

Author information:
(1)Department of Pharmacotherapy, University of Utah College of Pharmacy, Salt 
Lake City.
(2)Department of Pharmaceutical Outcomes and Policy, University of Florida 
College of Pharmacy, Gainesville.
(3)Journal of Managed Care and Specialty Pharmacy, Academy of Managed Care 
Pharmacy, Alexandria, VA.
(4)Academy of Managed Care Pharmacy Foundation, Alexandria, VA.
(5)Cranston & Associates, LLC, Fairfax Station, VA.

BACKGROUND: Over the past 5 years, managed care pharmacy has been shaped by a 
global pandemic, advancements in generative artificial intelligence (AI), 
Medicare drug price negotiation policies, and significant therapeutic 
developments. Collective intelligence methods can be used to anticipate future 
developments in practice to help organizations plan and develop new strategies 
around those changes.
OBJECTIVE: To identify emerging trends in managed care pharmacy.
METHODS: In this sequential mixed-method study, we invited experts to 
participate in a multidisciplinary advisory panel to develop a survey with 5 
overarching domains. The qualitative analysis for our advisory panel meetings 
used a thematic analysis approach. To analyze the cross-sectional survey 
results, we used descriptive statistics and exploratory bivariate statistics to 
test for possible relationships with survey respondent demographics and 
likelihood predictions. To assess respondent opinions on the overall likelihood 
of an event occurring in the next 5 years, we combined "Highly likely/Somewhat 
likely" responses and compared with "Highly unlikely/Somewhat unlikely" 
responses.
RESULTS: Following our advisory panel focus groups, a total of 53 scenarios were 
developed for inclusion in the quantitative survey under the domains of (1) 
information technology, (2) therapeutics and diagnostics, (3) payment models, 
(4) pharmacy operations, and (5) public policy. A total of 1,238 individuals 
were invited to participate in the survey. Of eligible participants, 201 
complete survey responses were received for a final response rate of 16.2%. 
Survey participants rated increased use of glucagon-like peptide-1 receptor 
agonists by at least 25%, at least 1 major data breach, more than 10 new orphan 
drug approvals, and AI use in more than half of prior authorization reviews as 
the most likely scenarios to occur in the next 5 years. Respondents identified 
the following broad issues as those most likely to impact their organizations 
(employers) in the next 5 years: federal and state policy changes impacting 
managed care, cell and gene therapies, impact of AI on managed care operations, 
and emerging payment models.
CONCLUSIONS: This study provides valuable insights into the emerging trends that 
are expected to shape managed care pharmacy over the next 5 years. The 
integration of advanced technologies, such as AI, along with the increasing 
focus on specialty therapeutics, represents both opportunities and challenges 
for managed care organizations. However, areas with lower consensus highlight 
the need for caution in strategic planning.

DOI: 10.18553/jmcp.2025.31.2-a.s2
PMCID: PMC11785360
PMID: 39888855 [Indexed for MEDLINE]

Conflict of interest statement: Dr Mattingly reports grant support from the US 
Food and Drug Administration, National Institute on Minority Health and Health 
Disparities, and the Alzheimer Association and consulting fees in the past 24 
months from Arnold Ventures, Pharmaceutical Care Management Association (PCMA), 
and Pharmaceutical Research and Manufacturers of America (PhRMA) unrelated to 
this work. Dr Happe discloses consulting fees from the Academy of Managed Care 
Pharmacy unrelated to this work.


503. BMC Oral Health. 2025 Jan 29;25(1):153. doi: 10.1186/s12903-025-05431-6.

Enhancing furcation involvement classification on panoramic radiographs with 
vision transformers.

Zhang X(#)(1), Guo E(#)(2), Liu X(3), Zhao H(4), Yang J(3), Li W(5), Wu W(6), 
Sun W(7).

Author information:
(1)Department of Periodontics, Affiliated Hospital of Medical School, Nanjing 
Stomatological Hospital, Research Institute of Stomatology, Nanjing University, 
Nanjing, China. zhxuan2015@163.com.
(2)Division of Computer Science, University of Aizu, Aizu, Japan.
(3)Department of Periodontics, Affiliated Hospital of Medical School, Nanjing 
Stomatological Hospital, Research Institute of Stomatology, Nanjing University, 
Nanjing, China.
(4)The School of Computer Science and Technology, North University of China, 
Taiyuan, China.
(5)Department of Endodontics, Affiliated Hospital of Medical School, Nanjing 
Stomatological Hospital, Research Institute of Stomatology, Nanjing University, 
Nanjing, China.
(6)Department of Periodontics, Affiliated Hospital of Medical School, Nanjing 
Stomatological Hospital, Research Institute of Stomatology, Nanjing University, 
Nanjing, China. wuwenlei6812@163.com.
(7)Department of Periodontics, Affiliated Hospital of Medical School, Nanjing 
Stomatological Hospital, Research Institute of Stomatology, Nanjing University, 
Nanjing, China. wbsun@nju.edu.cn.
(#)Contributed equally

BACKGROUND: The severity of furcation involvement (FI) directly affected tooth 
prognosis and influenced treatment approaches. However, assessing, diagnosing, 
and treating molars with FI was complicated by anatomical and morphological 
variations. Cone-beam computed tomography (CBCT) enhanced diagnostic accuracy 
for detecting FI and measuring furcation defects. Despite its advantages, the 
high cost and radiation dose associated with CBCT equipment limited its 
widespread use. The aim of this study was to evaluate the performance of the 
Vision Transformer (ViT) in comparison with several commonly used traditional 
deep learning (DL) models for classifying molars with or without FI on panoramic 
radiographs.
METHODS: A total of 1,568 tooth images obtained from 506 panoramic radiographs 
were used to construct the database and evaluate the models. This study 
developed and assessed a ViT model for classifying FI from panoramic 
radiographs, and compared its performance with traditional models, including 
Multi-Layer Perceptron (MLP), Visual Geometry Group (VGG)Net, and GoogLeNet.
RESULTS: Among the evaluated models, the ViT model outperformed all others, 
achieving the highest precision (0.98), recall (0.92), and F1 score (0.95), 
along with the lowest cross-entropy loss (0.27) and the highest accuracy (92%). 
ViT also recorded the highest area under the curve (AUC) (98%), outperforming 
the other models with statistically significant differences (p < 0.05), 
confirming its enhanced classification capability. The gradient-weighted class 
activation mapping (Grad-CAM) analysis on the ViT model revealed the key areas 
of the images that the model focused on during predictions.
CONCLUSION: DL algorithms can automatically classify FI using readily accessible 
panoramic images. These findings demonstrate that ViT outperforms the tested 
traditional models, highlighting the potential of transformer-based approaches 
to significantly advance image classification. This approach is also expected to 
reduce both the radiation dose and the financial burden on patients while 
simultaneously improving diagnostic precision.

© 2025. The Author(s).

DOI: 10.1186/s12903-025-05431-6
PMCID: PMC11776184
PMID: 39881302 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval and consent to 
participate: The study was performed in accordance with the Declaration of 
Helsinki and approved by the Ethical Review Board at Nanjing University. Ethics 
committee approval number: NJSH-2022NL-069. All respondents were informed 
comprehensively and provided written informed consent. For children under the 
age of 16, the written informed consent was obtained from their parents/ 
guardians. Consent for publication: Not applicable. Competing interests: The 
authors declare no competing interests.


504. JMIR Public Health Surveill. 2025 Jan 29;11:e63809. doi: 10.2196/63809.

An Explainable Artificial Intelligence Text Classifier for Suicidality 
Prediction in Youth Crisis Text Line Users: Development and Validation Study.

Thomas J(1)(2)(3), Lucht A(3), Segler J(4), Wundrack R(3), Miché M(1), Lieb 
R(1), Kuchinke L(5), Meinlschmidt G(2)(6)(7)(8).

Author information:
(1)Division of Clinical Psychology and Epidemiology, Faculty of Psychology, 
University of Basel, Basel, Switzerland.
(2)Division of Clinical Psychology and Cognitive Behavioural Therapy, 
International Psychoanalytic University Berlin, Berlin, Germany.
(3)Department of Research, Analytics and Development, krisenchat gGmbH, Berlin, 
Germany.
(4)Division of Child and Adolescent Psychiatry/Psychotherapy, 
Universitätsklinikum Ulm, Ulm, Germany.
(5)Division of Methods and Statistics, International Psychoanalytic University 
Berlin, Berlin, Germany.
(6)Clinical Psychology and Psychotherapy, Methods and Approaches, Department of 
Psychology, Trier University, Trier, Germany.
(7)Department of Digital and Blended Psychosomatics and Psychotherapy, 
Psychosomatic Medicine, University Hospital and University of Basel, Basel, 
Switzerland.
(8)Department of Psychosomatic Medicine, University Hospital Basel, University 
of Basel, Basel, Switzerland.

BACKGROUND: Suicide represents a critical public health concern, and machine 
learning (ML) models offer the potential for identifying at-risk individuals. 
Recent studies using benchmark datasets and real-world social media data have 
demonstrated the capability of pretrained large language models in predicting 
suicidal ideation and behaviors (SIB) in speech and text.
OBJECTIVE: This study aimed to (1) develop and implement ML methods for 
predicting SIBs in a real-world crisis helpline dataset, using transformer-based 
pretrained models as a foundation; (2) evaluate, cross-validate, and benchmark 
the model against traditional text classification approaches; and (3) train an 
explainable model to highlight relevant risk-associated features.
METHODS: We analyzed chat protocols from adolescents and young adults (aged 
14-25 years) seeking assistance from a German crisis helpline. An ML model was 
developed using a transformer-based language model architecture with pretrained 
weights and long short-term memory layers. The model predicted suicidal ideation 
(SI) and advanced suicidal engagement (ASE), as indicated by composite 
Columbia-Suicide Severity Rating Scale scores. We compared model performance 
against a classical word-vector-based ML model. We subsequently computed 
discrimination, calibration, clinical utility, and explainability information 
using a Shapley Additive Explanations value-based post hoc estimation model.
RESULTS: The dataset comprised 1348 help-seeking encounters (1011 for training 
and 337 for testing). The transformer-based classifier achieved a macroaveraged 
area under the curve (AUC) receiver operating characteristic (ROC) of 0.89 (95% 
CI 0.81-0.91) and an overall accuracy of 0.79 (95% CI 0.73-0.99). This 
performance surpassed the word-vector-based baseline model (AUC-ROC=0.77, 95% CI 
0.64-0.90; accuracy=0.61, 95% CI 0.61-0.80). The transformer model demonstrated 
excellent prediction for nonsuicidal sessions (AUC-ROC=0.96, 95% CI 0.96-0.99) 
and good prediction for SI and ASE, with AUC-ROCs of 0.85 (95% CI 0.97-0.86) and 
0.87 (95% CI 0.81-0.88), respectively. The Brier Skill Score indicated a 44% 
improvement in classification performance over the baseline model. The Shapley 
Additive Explanations model identified language features predictive of SIBs, 
including self-reference, negation, expressions of low self-esteem, and 
absolutist language.
CONCLUSIONS: Neural networks using large language model-based transfer learning 
can accurately identify SI and ASE. The post hoc explainer model revealed 
language features associated with SI and ASE. Such models may potentially 
support clinical decision-making in suicide prevention services. Future research 
should explore multimodal input features and temporal aspects of suicide risk.

©Julia Thomas, Antonia Lucht, Jacob Segler, Richard Wundrack, Marcel Miché, 
Roselind Lieb, Lars Kuchinke, Gunther Meinlschmidt. Originally published in JMIR 
Public Health and Surveillance (https://publichealth.jmir.org), 29.01.2025.

DOI: 10.2196/63809
PMCID: PMC11822322
PMID: 39879608 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest: JT, AL, RW, and JS are or 
were employed and receive a salary from krisenchat. krisenchat did not influence 
aspects like data collection, analysis, and reporting of results. GM received 
funding from the Stanley Thomas Johnson Stiftung and Gottfried und Julia 
Bangerter-Rhyner-Stiftung (project PC 28/17 and PC 05/18), from 
Gesundheitsförderung Schweiz (project 18.191/K50001), from the Swiss Heart 
Foundation (project =FF21101), from the Research Foundation of the International 
Psychoanalytic University Berlin (project 5087 and 5217), from the Swiss 
National Science Foundation (SNSF; project 100014_135328), from the German 
Federal Ministry of Education and Research (budget item 68606) in the context of 
an evaluation project conducted among others in collaboration with krisenchat, 
from the Hasler Foundation (project 23004), in the context of a Horizon Europe 
project from the Swiss State Secretariat for Education, Research and Innovation 
(SERI; contract 22.00094), and from Wings Health in the context of a 
proof-of-concept study. GM is a cofounder, member of the board, and shareholder 
of Therayou AG, active in digital and blended mental health care. GM receives 
royalties from publishing companies as author, including a book published by 
Springer, and an honorarium from Lundbeck for speaking at a symposium. 
Furthermore, GM is compensated for providing psychotherapy to patients, acting 
as a supervisor, serving as a self-experience facilitator 
(“Selbsterfahrungsleiter”), and for postgraduate training of psychotherapists 
and supervisors.


505. J Clin Res Pediatr Endocrinol. 2025 Aug 22;17(3):278-287. doi: 
10.4274/jcrpe.galenos.2025.2024-8-13. Epub 2025 Jan 24.

Machine Learning-driven Identification of the Honeymoon Phase in Pediatric Type 
1 Diabetes and Optimizing Insulin Management.

R S(1).

Author information:
(1)Narasaraopeta Engineering College, Department of Computer Science and 
Engineering, Andhra Pradesh, India

OBJECTIVE: The honeymoon phase in type 1 diabetes (T1D) represents a temporary 
improvement in glycemic control but may complicate insulin management. The aim 
was to develop and validate a machine learning (ML)-driven method for accurately 
detecting this phase to optimize insulin therapy and prevent adverse outcomes.
METHODS: Data from pediatric T1D patients aged 6-17 years, including continuous 
glucose monitoring data, glucose management indicator (GMI) reports, hemoglobin 
A1c (HbA1c) values, and patient medical history, were used to train ML models 
including long short-term memory (LSTM) networks, transformer models, random 
forest, and gradient boosting machines (GBMs). These were designed to analyze 
glucose trends and identify the honeymoon phase in T1D patients.
RESULTS: The transformer model achieved the highest accuracy at 91%, followed by 
GBMs at 89%, LSTM at 88%, and random forest at 87%. Key features, such as 
glucose variability, insulin adjustments, GMI values, and HbA1c levels were 
critical to model performance. Accurate identification of the honeymoon phase 
enabled optimized insulin adjustments, enhancing glucose control and reducing 
hypoglycemia risk.
CONCLUSION: The ML-driven approach provides a robust method for detecting the 
honeymoon phase in T1D patients, demonstrating potential for improved 
personalized insulin management. The findings suggest significant benefits in 
patient outcomes, with future research focused on further validation and 
clinical integration.

©Copyright 2025 by Turkish Society for Pediatric Endocrinology and Diabetes / 
The Journal of Clinical Research in Pediatric Endocrinology published by Galenos 
Publishing House.

DOI: 10.4274/jcrpe.galenos.2025.2024-8-13
PMCID: PMC12372630
PMID: 39849669 [Indexed for MEDLINE]

Conflict of interest statement: Conflict of interest: None declared.


506. Med Biol Eng Comput. 2025 Jun;63(6):1571-1583. doi: 10.1007/s11517-025-03283-4. 
Epub 2025 Jan 22.

Automatic skeletal maturity grading from pelvis radiographs by deep learning for 
adolescent idiopathic scoliosis.

Zhao Y(1), Zhang J(2), Li H(3), Wang Q(3), Li Y(3), Wang Z(1).

Author information:
(1)School of Information, Yunnan University, East Outer Ring South Road, 
Kunming, 650504, China.
(2)School of Information, Yunnan University, East Outer Ring South Road, 
Kunming, 650504, China. jhzhang@ynu.edu.cn.
(3)Orthopedics Department, The First People's Hospital of Yunnan Province, 157 
Jinbi Road, Kunming, 650034, China.

Adolescent idiopathic scoliosis (AIS) is a three-dimensional spine deformity 
governed of the spine. A child's Risser stage of skeletal maturity must be 
carefully considered for AIS evaluation and treatment. However, there are 
intra-observer and inter-observer inaccuracies in the Risser stage manual 
assessment. A multi-task learning approach is proposed to address the low 
precision issue of manual assessment. With our developed multi-task learning 
approach, the iliac area is extracted and forwarded to the improved Swin 
Transformer for Risser stage assessment. The spatial and channel reconstruction 
convolutional Swin block is adapted to each stage of the Swin Transformer to 
achieve better performance. The Risser stage assessment based on iliac region 
extraction had an overall accuracy of 81.53%. The accuracy increased in 
comparison to ResNet50, ResNet101, Uni-former, Next-ViT, ConvNeXt, and the 
original Swin Transformer by 5.85%, 4.6%, 3.6%, 2.7%, 2.25%, and 1.8%, 
respectively. The Grad-CAM visualization is used to understand the 
interpretability of our proposed model. The results show that the proposed 
multi-task learning strategy performs well on the Risser stage assessment. Our 
proposed automatic Risser stage assessment method benefits the clinical 
evaluation of AIS. Project address: 
https://github.com/xyz911015/Risser-stage-assessment.

© 2025. International Federation for Medical and Biological Engineering.

DOI: 10.1007/s11517-025-03283-4
PMID: 39838221 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics approval: This work has 
been approved by The Committee on Human Subject Research and Ethics, Yunnan 
University. The approval number is YNUHM2024001. Conflict of interest: The 
authors declare no competing interests.


507. Ophthalmic Genet. 2025 Apr;46(2):180-185. doi: 10.1080/13816810.2025.2455576. 
Epub 2025 Jan 20.

Machine learning demonstrates clinical utility in distinguishing retinoblastoma 
from pseudo retinoblastoma with RetCam images.

Cruz-Abrams O(1), Dodds Rojas R(2), Abramson DH(1).

Author information:
(1)Department of Surgery, Memorial Sloan Kettering Cancer Center, New York, N.Y, 
US.
(2)DigITs, Memorial Sloan Kettering Cancer Center, New York, New York, USA.

BACKGROUND: Retinoblastoma is diagnosed and treated without biopsy based solely 
on appearance (with the indirect ophthalmoscope and imaging). More than 20 
benign ophthalmic disorders resemble retinoblastoma and errors in diagnosis 
continue to be made worldwide. A better noninvasive method for distinguishing 
retinoblastoma from pseudo retinoblastoma is needed.
METHODS: RetCam imaging of retinoblastoma and pseudo retinoblastoma from the 
largest retinoblastoma center in the U.S. (Memorial Sloan Kettering Cancer 
Center, New York, NY) were used for this study. We used several neural networks 
(ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152, and a Vision Image 
Transformer, or VIT), using 80% of images for training, 10% for validation, and 
10% for testing.
RESULTS: Two thousand eight hundred eighty-two RetCam images from patients with 
retinoblastoma at diagnosis, 1,970 images from pseudo retinoblastoma at 
diagnosis, and 804 normal pediatric fundus images were included. The highest 
sensitivity (98.6%) was obtained with a ResNet-101 model, as were the highest 
accuracy and F1 scores of 97.3% and 97.7%. The highest specificity (97.0%) and 
precision (97.0%) was attained with a ResNet-152 model.
CONCLUSION: Our machine learning algorithm successfully distinguished 
retinoblastoma from retinoblastoma with high specificity and sensitivity and if 
implemented worldwide will prevent hundreds of eyes from incorrectly being 
surgically removed yearly.

DOI: 10.1080/13816810.2025.2455576
PMCID: PMC12392977
PMID: 39834033 [Indexed for MEDLINE]

Conflict of interest statement: Disclosure statement No potential conflict of 
interest was reported by the author(s).


508. Radiol Med. 2025 Apr;130(4):508-523. doi: 10.1007/s11547-025-01949-5. Epub 2025 
Jan 20.

Deep learning-based MVIT-MLKA model for accurate classification of pancreatic 
lesions: a multicenter retrospective cohort study.

Liao H(#)(1), Huang C(#)(2), Liu C(3), Zhang J(4), Tao F(1), Liu H(1), Liang 
H(1), Hu X(4), Li Y(5), Chen S(#)(6), Li Y(#)(7).

Author information:
(1)Department of Radiology, The First Affiliated Hospital of Chongqing Medical 
University, Chongqing, 400016, China.
(2)College of Computer and Information Science, Southwest University, Chongqing, 
400715, China.
(3)Department of Radiology, Daping Hospital, Army Medical University, Chongqing, 
China.
(4)Department of Radiology, The Third Affiliated Hospital of Chongqing Medical 
University, Chongqing, China.
(5)Department of Radiology, The Third People's Hospital of Chengdu, Chengdu, 
China.
(6)College of Computer and Information Science, Southwest University, Chongqing, 
400715, China. csxpml@163.com.
(7)Department of Radiology, The First Affiliated Hospital of Chongqing Medical 
University, Chongqing, 400016, China. lymzhang70@163.com.
(#)Contributed equally

BACKGROUND: Accurate differentiation between benign and malignant pancreatic 
lesions is critical for effective patient management. This study aimed to 
develop and validate a novel deep learning network using baseline computed 
tomography (CT) images to predict the classification of pancreatic lesions.
METHODS: This retrospective study included 864 patients (422 men, 442 women) 
with confirmed histopathological results across three medical centers, forming a 
training cohort, internal testing cohort, and external validation cohort. A 
novel hybrid model, Multi-Scale Large Kernel Attention with Mobile Vision 
Transformer (MVIT-MLKA), was developed, integrating CNN and Transformer 
architectures to classify pancreatic lesions. The model's performance was 
compared with traditional machine learning methods and advanced deep learning 
models. We also evaluated the diagnostic accuracy of radiologists with and 
without the assistance of the optimal model. Model performance was assessed 
through discrimination, calibration, and clinical applicability.
RESULTS: The MVIT-MLKA model demonstrated superior performance in classifying 
pancreatic lesions, achieving an AUC of 0.974 (95% CI 0.967-0.980) in the 
training set, 0.935 (95% CI 0.915-0.954) in the internal testing set, and 0.924 
(95% CI 0.902-0.945) in the external validation set, outperforming traditional 
models and other deep learning models (P < 0.05). Radiologists aided by the 
MVIT-MLKA model showed significant improvements in diagnostic accuracy and 
sensitivity compared to those without model assistance (P < 0.05). Grad-CAM 
visualization enhanced model interpretability by effectively highlighting key 
lesion areas.
CONCLUSION: The MVIT-MLKA model efficiently differentiates between benign and 
malignant pancreatic lesions, surpassing traditional methods and significantly 
improving radiologists' diagnostic performance. The integration of this advanced 
deep learning model into clinical practice has the potential to reduce 
diagnostic errors and optimize treatment strategies.

© 2025. Italian Society of Medical Radiology.

DOI: 10.1007/s11547-025-01949-5
PMID: 39832039 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The authors 
declare that they have no competing interests. Ethical approval: This study and 
all its protocols were approved by the ethics committee of the first affiliated 
hospital of Chongqing medical university (approval number: no.2022-K63). This 
article adheres to Strengthening the Reporting of Cohort Studies in Surgery 
(STROCSS) guidelines. Informed consent: Written informed consent was not 
required for this study due to the retrospective nature. Research registration 
unique identifying number (UIN): Registry used: The study protocol registered on 
the ClinicalTrials.gov Registry. Unique Identifying number or registration ID: 
NCT06641947. Hyperlink to your specific registration (must be publicly 
accessible and will be checked): https://clinicaltrials.gov/study/NCT06641947


509. JHEP Rep. 2024 Aug 31;7(2):101198. doi: 10.1016/j.jhepr.2024.101198. eCollection 
2025 Feb.

Deep learning helps discriminate between autoimmune hepatitis and primary 
biliary cholangitis.

Gerussi A(1)(2), Saldanha OL(3)(4), Cazzaniga G(5), Verda D(6), Carrero ZI(3), 
Engel B(7)(8), Taubert R(7)(8), Bolis F(1)(2), Cristoferi L(1)(2), Malinverno 
F(1)(2), Colapietro F(9)(10), Akpinar R(9)(11), Di Tommaso L(9)(11), Terracciano 
L(9)(11), Lleo A(9)(10), Viganó M(12), Rigamonti C(13), Cabibi D(14), Calvaruso 
V(15), Gibilisco F(16)(17), Caldonazzi N(18), Valentino A(19), Ceola S(5), 
Canini V(5), Nofit E(1)(2), Muselli M(6), Calderaro J(20)(21)(22), Tiniakos 
D(23)(24), L'Imperio V(5), Pagni F(5), Zucchini N(5), Invernizzi P(1)(2), 
Carbone M(2)(25), Kather JN(3)(26)(27).

Author information:
(1)Division of Gastroenterology, Center for Autoimmune Liver Diseases, European 
Reference Network on Hepatological Diseases (ERN RARE-LIVER), Fondazione IRCCS 
San Gerardo dei Tintori, Monza, Italy.
(2)Department of Medicine and Surgery, University of Milano-Bicocca, Monza, 
Italy.
(3)Else Kroener Fresenius Center for Digital Health, Medical Faculty Carl Gustav 
Carus, Technical University Dresden, Dresden, Germany.
(4)Department of Diagnostic and Interventional Radiology, University Hospital 
RWTH Aachen, Aachen, Germany.
(5)Department of Medicine and Surgery, Pathology, Fondazione IRCCS San Gerardo 
dei Tintori, Università di Milano-Bicocca, Monza, Italy.
(6)Rulex Innovation Labs, Rulex Inc., Genoa, Italy.
(7)Department of Gastroenterology, Hepatology, Infectious Diseases and 
Endocrinology, Hannover Medical School, Hannover, Germany.
(8)European Reference Network on Hepatological Diseases (ERN RARE-LIVER), 
Hamburg, Germany.
(9)Department of Biomedical Sciences, Humanitas University, Pieve Emanuele, 
Milan, Italy.
(10)Division of Internal Medicine and Hepatology, Department of 
Gastroenterology, IRCCS Humanitas Research Hospital, Rozzano, Milan, Italy.
(11)Department of Pathology, IRCSS Humanitas Research Hospital, Rozzano-Milan, 
Italy.
(12)Gastroenterology Hepatology and Transplantation Unit, ASST Papa Giovanni 
XXIII, Bergamo, Italy.
(13)Department of Translational Medicine, Università del Piemonte Orientale, 
Division of Internal Medicine, AOU Maggiore della Carità, Novara, Italy.
(14)Pathology Institute, PROMISE, University of Palermo, Palermo, Italy.
(15)Gastrointestinal and Liver Unit, Department of Health Promotion Sciences, 
Maternal and Infantile Care, Internal Medicine and Medical Specialties, 
University of Palermo, Palermo, Italy.
(16)Department of Pathology, Hospital "Gravina e Santo Pietro", Caltagirone, 
Italy.
(17)Department of Medical and Surgical Sciences and Advanced Technologies, "G. 
F. Ingrassia", University of Catania, Catania, Italy.
(18)Department of Diagnostics and Public Health, Section of Pathology, 
University of Verona, Verona, Italy.
(19)Pathological Unit, Niguarda Hospital, Milan, Italy.
(20)Université Paris Est Créteil, INSERM, IMRB, Créteil, France.
(21)Assistance Publique-Hôpitaux de Paris, Henri Mondor-Albert Chenevier 
University Hospital, Department of Pathology, Créteil, France.
(22)Inserm, U955, Team 18, Créteil, France.
(23)Department of Pathology, Aretaieion Hospital, Medical School, National and 
Kapodistrian University of Athens, Athens, Greece.
(24)Translational and Clinical Research Institute, Faculty of Medical Sciences, 
Newcastle University, Newcastle upon Tyne, UK.
(25)Liver Unit, ASST Grande Ospedale Metropolitano Niguarda, Milan, Italy.
(26)Medical Oncology, National Center for Tumor Diseases (NCT), University 
Hospital Heidelberg, Heidelberg, Germany.
(27)Department of Medicine 1, University Hospital and Faculty of Medicine Carl 
Gustav Carus, Technische Universität Dresden, Dresden, Germany.

Erratum in
    JHEP Rep. 2025 Feb 17;7(3):101359. doi: 10.1016/j.jhepr.2025.101359.

BACKGROUND & AIMS: Biliary abnormalities in autoimmune hepatitis (AIH) and 
interface hepatitis in primary biliary cholangitis (PBC) occur frequently, and 
misinterpretation may lead to therapeutic mistakes with a negative impact on 
patients. This study investigates the use of a deep learning (DL)-based pipeline 
for the diagnosis of AIH and PBC to aid differential diagnosis.
METHODS: We conducted a multicenter study across six European referral centers, 
and built a library of digitized liver biopsy slides dating from 1997 to 2023. A 
training set of 354 cases (266 AIH and 102 PBC) and an external validation set 
of 92 cases (62 AIH and 30 PBC) were available for analysis. A novel DL model, 
the autoimmune liver neural estimator (ALNE), was trained on whole-slide images 
(WSIs) with H&E staining, without human annotations. The ALNE model was 
evaluated against clinico-pathological diagnoses and tested for interobserver 
variability among general pathologists.
RESULTS: The ALNE model demonstrated high accuracy in differentiating AIH from 
PBC, achieving an area under the receiver operating characteristic curve of 0.81 
in external validation. Attention heatmaps showed that ALNE tends to focus more 
on areas with increased inflammation, associating such patterns predominantly 
with AIH. A multivariate explainable ML model revealed that PBC cases 
misclassified as AIH more often had ALP values between 1 × upper limit of normal 
(ULN) and 2 × ULN, coupled with AST values above 1 × ULN. Inconsistency among 
general pathologists was noticed when evaluating a random sample of the same 
cases (Fleiss's kappa value 0.09).
CONCLUSIONS: The ALNE model is the first system generating a quantitative and 
accurate differential diagnosis between cases with AIH or PBC.
IMPACT AND IMPLICATIONS: This study demonstrates the significant potential of 
the autoimmune liver neural estimator model, a transformer-based deep learning 
system, in accurately distinguishing between autoimmune hepatitis and primary 
biliary cholangitis using digitized liver biopsy slides without human 
annotation. The scientific justification for this work lies in addressing the 
challenge of differentiating these conditions, which often present with 
overlapping features and can lead to therapeutic mistakes. In addition, there is 
need for quantitative assessment of information embedded in liver biopsies, 
which are currently evaluated on qualitative or semi-quantitative methods. The 
results of this study are crucial for pathologists, researchers, and clinicians, 
providing a reliable diagnostic tool that reduces interobserver variability and 
improves diagnostic accuracy of these conditions. Potential methodological 
limitations, such as the diversity in scanning techniques and slide colorations, 
were considered, ensuring the robustness and generalizability of the findings.

© 2024 The Author(s).

DOI: 10.1016/j.jhepr.2024.101198
PMCID: PMC11741034
PMID: 39829723

Conflict of interest statement: AG declares consulting services for Ipsen and 
CAMP4 Therapeutics, and speaker fees from Advanz Pharma. JNK declares consulting 
services for Owkin, France; DoMore Diagnostics, Norway, Panakeia, UK and 
Histofy, UK; furthermore, he holds shares in StratifAI GmbH and has received 
honoraria for lectures by AstraZeneca, Bayer, Eisai, MSD, BMS, Roche, Pfizer, 
and Fresenius. AL declares consulting fees from Advanz Pharma, GSK, AlfaSigma, 
Takeda, Ipsen, and Albireo Pharma, and speaker fees from Gilead, Abbvie, MSD, 
Advanz Pharma, AlfaSigma, GSK, and Incyte. AL declares consulting fees from 
Advanz Pharma, GSK, AlfaSigma, Takeda, Ipsen, and Albireo Pharma, and speaker 
fees from Gilead, Abbvie, MSD, Advanz Pharma, AlfaSigma, GSK, and Incyte. MC 
declares consulting services for Advanz Pharma, Cymabay, GSK, Falk, Ipsen, 
Albireo, Mirum Pharma, Perspectum, Echosens, Gentic s.p.a. DV works for Rulex, 
MM is the CEO of Rulex. Please refer to the accompanying ICMJE disclosure forms 
for further details.


510. J Clin Epidemiol. 2025 Apr;180:111672. doi: 10.1016/j.jclinepi.2025.111672. Epub 
2025 Jan 17.

Using artificial intelligence to semi-automate trustworthiness assessment of 
randomized controlled trials: a case study.

Au LS(1), Qu L(2), Nielsen J(1), Ge Z(3), Gurrin LC(4), Mol BW(5), Wang R(6).

Author information:
(1)Department of Obstetrics and Gynaecology, Monash University, Melbourne, 
Victoria, Australia.
(2)Department of Data Science & AI, Monash University, Clayton, Victoria, 
Australia.
(3)AIM for Health Lab, Faculty of IT, Monash University, Clayton, Victoria, 
Australia; Monash-Airdoc Research Lab, Faculty of IT, Monash University, 
Clayton, Victoria, Australia.
(4)Centre for Epidemiology and Biostatistics, School of Population and Global 
Health, The University of Melbourne, Parkville, Australia. Electronic address: 
lgurrin@unimelb.edu.au.
(5)Department of Obstetrics and Gynaecology, Monash University, Melbourne, 
Victoria, Australia; Womens and Children's Program Monash Health, Clayton, 
Melbourne, Victoria, Australia; University Medical Centre Amsterdam, Amsterdam, 
The Netherlands.
(6)Department of Obstetrics and Gynaecology, Monash University, Melbourne, 
Victoria, Australia; NHMRC Clinical Trials Centre, University of Sydney, 
Camperdown, New South Wales, Australia.

BACKGROUND AND OBJECTIVE: Randomized controlled trials (RCTs) are the 
cornerstone of evidence-based medicine. Unfortunately, not all RCTs are based on 
real data. This serious breach of research integrity compromises the reliability 
of systematic reviews and meta-analyses, leading to misinformed clinical 
guidelines and posing a risk to both individual and public health. While methods 
to detect problematic RCTs have been proposed, they are time-consuming and 
labor-intensive. The use of artificial intelligence large language models (LLMs) 
has the potential to accelerate the data collection needed to assess the 
trustworthiness of published RCTs.
METHODS: We present a case study using ChatGPT powered by OpenAI's GPT-4o to 
assess an RCT paper. The case study focuses on applying the trustworthiness in 
randomised controlled trials (TRACT checklist) and automating data table 
extraction to accelerate statistical analysis targeting the trustworthiness of 
the data. We provide a detailed step-by-step outline of the process, along with 
considerations for potential improvements.
RESULTS: ChatGPT completed all tasks by processing the PDF of the selected 
publication and responding to specific prompts. ChatGPT addressed items in the 
TRACT checklist effectively, demonstrating an ability to provide precise "yes" 
or "no" answers while quickly synthesizing information from both the paper and 
relevant online resources. A comparison of results generated by ChatGPT and the 
human assessor showed an 84% level of agreement of (16/19) TRACT items. This 
substantially accelerated the qualitative assessment process. Additionally, 
ChatGPT was able to extract efficiently the data tables as Microsoft Excel 
worksheets and reorganize the data, with three out of four extracted tables 
achieving an accuracy score of 100%, facilitating subsequent analysis and data 
verification.
CONCLUSION: ChatGPT demonstrates potential in semiautomating the trustworthiness 
assessment of RCTs, though in our experience this required repeated prompting 
from the user. Further testing and refinement will involve applying ChatGPT to 
collections of RCT papers to improve the accuracy of data capture and lessen the 
role of the user. The ultimate aim is a completely automated process for large 
volumes of papers that seems plausible given our initial experience.

Copyright © 2025 The Author(s). Published by Elsevier Inc. All rights reserved.

DOI: 10.1016/j.jclinepi.2025.111672
PMID: 39828224 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest B.W.M. is 
supported by an NHMRC Investigator grant (GNT1176437); reports consultancy, 
travel support, and research funding from Merck and Guerbet and consultancy for 
Organon and Norgine; and holds stocks from ObsEva. There are no competing 
interests for any other author.


511. Br J Ophthalmol. 2025 Jul 22;109(8):962-966. doi: 10.1136/bjo-2024-326254.

Can large language models fully automate or partially assist paper selection in 
systematic reviews?

Chen H(#)(1), Jiang Z(#)(1), Liu X(2), Xue CC(3), Yew SME(4)(5), Sheng B(6)(7), 
Zheng YF(3)(8), Wang X(9), Wu Y(10)(11), Sivaprasad S(12), Wong TY(1)(3)(13), 
Chaudhary V(14), Tham YC(15)(4)(5)(16).

Author information:
(1)Tsinghua Medicine, Tsinghua University, Beijing, China.
(2)Institute of Medical Technology, Peking University Health Science Center, 
Beijing, China.
(3)Singapore Eye Research Institute, Singapore National Eye Centre, Singapore.
(4)Centre for Innovation and Precision Eye Health, Yong Loo Lin School of 
Medicine, National University of Singapore, Singapore.
(5)Department of Ophthalmology, Yong Loo Lin School of Medicine, National 
University of Singapore, Singapore.
(6)MOE Key Laboratory of AI, School of Electronic, Information, and Electrical 
Engineering, Shanghai Jiao Tong University, Shanghai, China.
(7)Department of Computer Science and Engineering, School of Electronic, 
Information, and Electrical Engineering, Shanghai Jiao Tong University, 
Shanghai, China.
(8)State Key Laboratory of Ophthalmology, Zhongshan Ophthalmic Center, Sun 
Yat-sen University, Guangzhou, China.
(9)Beijing Advanced Innovation Center for Biomedical Engineering, Key Laboratory 
for Biomechanics and Mechanobiology of Ministry of Education, School of 
Biological Science and Medical Engineering, Beihang University, Beijing, China.
(10)Tsinghua University School of Medicine, Beijing, China.
(11)Department of Health Policy and Management, Johns Hopkins University, 
Baltimore, Maryland, USA.
(12)Moorfields Eye Hospital City Road Campus, London, UK.
(13)School of Clinical Medicine, Beijing Tsinghua Changgung Hospital, Tsinghua 
University, Beijing, China.
(14)Department of Surgery, Division of Ophthalmology, McMaster University, 
Hamilton, Ontario, Canada.
(15)Singapore Eye Research Institute, Singapore National Eye Centre, Singapore 
thamyc@nus.edu.sg.
(16)Ophthalmology and Visual Science Academic Clinical Program, Duke-NUS Medical 
School, Singapore.
(#)Contributed equally

BACKGROUND/AIMS: Large language models (LLMs) have substantial potential to 
enhance the efficiency of academic research. The accuracy and performance of 
LLMs in a systematic review, a core part of evidence building, has yet to be 
studied in detail.
METHODS: We introduced two LLM-based approaches of systematic review: an 
LLM-enabled fully automated approach (LLM-FA) utilising three different GPT-4 
plugins (Consensus GPT, Scholar GPT and GPT web browsing modes) and an 
LLM-facilitated semi-automated approach (LLM-SA) using GPT4's Application 
Programming Interface (API). We benchmarked these approaches using three 
published systematic reviews that reported the prevalence of diabetic 
retinopathy across different populations (general population, pregnant women and 
children).
RESULTS: The three published reviews consisted of 98 papers in total. Across 
these three reviews, in the LLM-FA approach, Consensus GPT correctly identified 
32.7% (32 out of 98) of papers, while Scholar GPT and GPT4's web browsing modes 
only identified 19.4% (19 out of 98) and 6.1% (6 out of 98), respectively. On 
the other hand, the LLM-SA approach not only successfully included 82.7% (81 out 
of 98) of these papers but also correctly excluded 92.2% of 4497 irrelevant 
papers.
CONCLUSIONS: Our findings suggest LLMs are not yet capable of autonomously 
identifying and selecting relevant papers in systematic reviews. However, they 
hold promise as an assistive tool to improve the efficiency of the paper 
selection process in systematic reviews.

© Author(s) (or their employer(s)) 2025. Re-use permitted under CC BY-NC. No 
commercial re-use. See rights and permissions. Published by BMJ Group.

DOI: 10.1136/bjo-2024-326254
PMCID: PMC12320601
PMID: 39814458 [Indexed for MEDLINE]

Conflict of interest statement: Competing interests: TYW declares consulting 
fees from Aldropika Therapeutics, Bayer, Boehringer Ingelheim, Genentech, Iveric 
Bio, Novartis, Plano, Oxurion, Roche, Sanofi and Shanghai Henlius; funding from 
the National Key R&D Program, China (grant number 2022YFC2502802); and being an 
inventor, patent holder and co-founder of the start-up companies EyRiS and 
Visre. All other authors declare no competing interests.


512. JAMIA Open. 2025 Jan 10;8(1):ooae154. doi: 10.1093/jamiaopen/ooae154. 
eCollection 2025 Feb.

Uncertainty estimation in diagnosis generation from large language models: 
next-word probability is not pre-test probability.

Gao Y(1)(2), Myers S(2), Chen S(3)(4), Dligach D(5), Miller T(6), Bitterman 
DS(3)(4), Chen G(7), Mayampurath A(2)(7), Churpek MM(2), Afshar M(2).

Author information:
(1)Department of Biomedical Informatics, University of Colorado Anschutz Medical 
Campus, Aurora, CO 80045, United States.
(2)Department of Medicine, University of Wisconsin-Madison, Madison, WI 53792, 
United States.
(3)Artificial Intelligence in Medicine (AIM) Program, Mass General Brigham, 
Harvard Medical School, Boston, MA 02114, United States.
(4)Department of Radiation Oncology, Brigham and Women's Hospital/Dana-Farber 
Cancer Institute, Boston, MA 02114, United States.
(5)Department of Computer Science, Loyola University Chicago, Chicago, IL 60660, 
United States.
(6)Computational Health Informatics Program, Boston Children's Hospital, Boston, 
MA 02115, United States.
(7)Department of Biostatistics and Medical Informatics, University of 
Wisconsin-Madison, Madison, WI 53792, United States.

OBJECTIVE: To evaluate large language models (LLMs) for pre-test diagnostic 
probability estimation and compare their uncertainty estimation performance with 
a traditional machine learning classifier.
MATERIALS AND METHODS: We assessed 2 instruction-tuned LLMs, Mistral-7B-Instruct 
and Llama3-70B-chat-hf, on predicting binary outcomes for Sepsis, Arrhythmia, 
and Congestive Heart Failure (CHF) using electronic health record (EHR) data 
from 660 patients. Three uncertainty estimation methods-Verbalized Confidence, 
Token Logits, and LLM Embedding+XGB-were compared against an eXtreme Gradient 
Boosting (XGB) classifier trained on raw EHR data. Performance metrics included 
AUROC and Pearson correlation between predicted probabilities.
RESULTS: The XGB classifier outperformed the LLM-based methods across all tasks. 
LLM Embedding+XGB showed the closest performance to the XGB baseline, while 
Verbalized Confidence and Token Logits underperformed.
DISCUSSION: These findings, consistent across multiple models and demographic 
groups, highlight the limitations of current LLMs in providing reliable pre-test 
probability estimations and underscore the need for improved calibration and 
bias mitigation strategies. Future work should explore hybrid approaches that 
integrate LLMs with numerical reasoning modules and calibrated embeddings to 
enhance diagnostic accuracy and ensure fairer predictions across diverse 
populations.
CONCLUSIONS: LLMs demonstrate potential but currently fall short in estimating 
diagnostic probabilities compared to traditional machine learning classifiers 
trained on structured EHR data. Further improvements are needed for reliable 
clinical use.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamiaopen/ooae154
PMCID: PMC11723528
PMID: 39802674

Conflict of interest statement: There are no competing interests to declare.


513. J Am Med Inform Assoc. 2025 Mar 1;32(3):526-534. doi: 10.1093/jamia/ocae314.

Utility of word embeddings from large language models in medical diagnosis.

Yazdani S(1), Henry RC(2), Byrne A(3), Henry IC(4).

Author information:
(1)Department of Pediatrics, David Geffen School of Medicine, University of 
California Los Angeles, Los Angeles, CA 90095, United States.
(2)Department of Civil Engineering, University of Southern California, Los 
Angeles, CA 90089, United States.
(3)San Francisco, CA 94129, United States.
(4)Kennewick, WA 99338, United States.

OBJECTIVE: This study evaluates the utility of word embeddings, generated by 
large language models (LLMs), for medical diagnosis by comparing the semantic 
proximity of symptoms to their eponymic disease embedding ("eponymic condition") 
and the mean of all symptom embeddings associated with a disease ("ensemble 
mean").
MATERIALS AND METHODS: Symptom data for 5 diagnostically challenging pediatric 
diseases-CHARGE syndrome, Cowden disease, POEMS syndrome, Rheumatic fever, and 
Tuberous sclerosis-were collected from PubMed. Using the Ada-002 embedding 
model, disease names and symptoms were translated into vector representations in 
a high-dimensional space. Euclidean and Chebyshev distance metrics were used to 
classify symptoms based on their proximity to both the eponymic condition and 
the ensemble mean of the condition's symptoms.
RESULTS: The ensemble mean approach showed significantly higher classification 
accuracy, correctly classifying between 80% (Cowden disease) to 100% (Tuberous 
sclerosis) of the sample disease symptoms using the Euclidean distance metric. 
In contrast, the eponymic condition approach using Euclidian distance metric and 
Chebyshev distances, in general, showed poor symptom classification performance, 
with erratic results (0%-100% accuracy), largely ranging between 0% and 3% 
accuracy.
DISCUSSION: The ensemble mean captures a disease's collective symptom profile, 
providing a more nuanced representation than the disease name alone. However, 
some misclassifications were due to superficial semantic similarities, 
highlighting the need for LLM models trained on medical corpora.
CONCLUSION: The ensemble mean of symptom embeddings improves classification 
accuracy over the eponymic condition approach. Future efforts should focus on 
medical-specific training of LLMs to enhance their diagnostic accuracy and 
clinical utility.

© The Author(s) 2025. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For commercial 
re-use, please contact reprints@oup.com for reprints and translation rights for 
reprints. All other permissions can be obtained through our RightsLink service 
via the Permissions link on the article page on our site—for further information 
please contact journals.permissions@oup.com.

DOI: 10.1093/jamia/ocae314
PMCID: PMC11833464
PMID: 39786898 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
declare.


514. J Imaging Inform Med. 2025 Oct;38(5):2729-2738. doi: 10.1007/s10278-024-01385-3. 
Epub 2025 Jan 3.

Enhancing Radiographic Diagnosis: CycleGAN-Based Methods for Reducing Cast 
Shadow Artifacts in Wrist Radiographs.

Norris SA(1)(2), Carrion D(3), Ditchfield M(3)(4), Gubser M(5), Seah J(6)(7), 
Badawy MK(3)(8).

Author information:
(1)Monash Imaging, Monash Health, 246 Clayton Rd, Clayton, VIC, 3168, Australia. 
stanley.norris@monashhealth.org.
(2)School of Science, RMIT University, Melbourne, Australia. 
stanley.norris@monashhealth.org.
(3)Monash Imaging, Monash Health, 246 Clayton Rd, Clayton, VIC, 3168, Australia.
(4)Department of Paediatrics, Monash University, Melbourne, Australia.
(5)Division of Radiology and Nuclear Medicine, St. Gallen Cantonal Hospital, St. 
Gallen, Switzerland.
(6)Department of Neuroscience, Monash University, Melbourne, Australia.
(7)Radiology and Nuclear Medicine, Alfred Health, Melbourne, Australia.
(8)Department of Medical Imaging and Radiation Sciences, Monash University, 
Melbourne, Australia.

We extend existing techniques by using generative adversarial network (GAN) 
models to reduce the appearance of cast shadows in radiographs across various 
age groups. We retrospectively collected 11,500 adult and paediatric wrist 
radiographs, evenly divided between those with and without casts. The test 
subset consisted of 750 radiographs with cast and 750 without cast. We extended 
the results from a previous study that employed CycleGAN by enhancing the model 
using a perceptual loss function and a self-attention layer. The CycleGAN model 
which incorporates a self-attention layer and perceptual loss function delivered 
a similar quantitative performance as the original model. This model was applied 
to images from 20 cases where the original reports recommended CT scanning or 
repeat radiographs without the cast, which were then evaluated by radiologists 
for qualitative assessment. The results demonstrated that the generated images 
could improve radiologists' diagnostic confidence, in some cases leading to more 
decisive reports. Where available, the reports from follow-up imaging were 
compared with those produced by radiologists reading AI-generated images. Every 
report, except two, provided identical diagnoses as those associated with 
follow-up imaging. The ability of radiologists to perform robust reporting with 
downsampled AI-enhanced images is clinically meaningful and warrants further 
investigation. Additionally, radiologists were unable to distinguish AI-enhanced 
from unenhanced images. These findings suggest the cast suppression technique 
could be integrated as a tool to augment clinical workflows, with the potential 
benefits of reducing patient doses, improving operational efficiencies, reducing 
delays in diagnoses, and reducing the number of patient visits.

© 2024. The Author(s) under exclusive licence to Society for Imaging Informatics 
in Medicine.

DOI: 10.1007/s10278-024-01385-3
PMCID: PMC12572480
PMID: 39753829 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics Approval: This study was 
exempt from Human Research Ethics Committee review as a retrospective quality 
improvement project. It was consistent with the NHMRC Ethical Considerations in 
Quality Assurance and Evaluation Activities (2014) guideline. Consent to 
Participate: Not applicable. Consent for Publication: Not applicable. Competing 
Interests: The authors declare no competing interests.


515. J Infect Public Health. 2025 Feb;18(2):102627. doi: 10.1016/j.jiph.2024.102627. 
Epub 2024 Dec 18.

Evaluation of ChatGPT-4 for the detection of surgical site infections from 
electronic health records after colorectal surgery: A pilot diagnostic accuracy 
study.

Badia JM(1), Casanova-Portoles D(2), Membrilla E(3), Rubiés C(4), Pujol M(5), 
Sancho J(6).

Author information:
(1)Department of Surgery, Hospital General de Granollers, Granollers, Spain; 
Universitat Internacional de Catalunya. Sant Cugat del Vallès, Barcelona, Spain. 
Electronic address: jmbadiaperez@gmail.com.
(2)Department of Surgery, Hospital General de Granollers, Granollers, Spain; 
Universitat Internacional de Catalunya. Sant Cugat del Vallès, Barcelona, Spain. 
Electronic address: dcasanova@fphag.org.
(3)Department of Surgery, Hospital del Mar, Barcelona, Spain. Electronic 
address: estelabe@gmail.com.
(4)Department of Digital Transformation, Hospital General de 
Granollers, Granollers, Spain. Electronic address: crubies@fphag.org.
(5)VINCat Program, Servei Català de la Salut, Catalonia, Spain; Centro de 
Investigación Biomédica en Red de Enfermedades Infecciosas (CIBERINFEC), 
Instituto de Salud Carlos III, Madrid, Spain. VINCat 
Program, Barcelona, Catalonia, Spain; Department of Infectious Diseases, 
Hospital Universitari de Bellvitge - IDIBELL. L'Hospitalet de Llobregat, Spain. 
Electronic address: mpujol@bellvitgehospital.cat.
(6)Department of Surgery, Hospital del Mar, Barcelona, Spain. Electronic 
address: jsancho@parcdesalutmar.cat.

BACKGROUND: Surveillance of surgical site infection (SSI) relies on manual 
methods that are time-consuming and prone to subjectivity. This study evaluates 
the diagnostic accuracy of ChatGPT for detecting SSI from electronic health 
records after colorectal surgery via comparison with the results of a nationwide 
surveillance programme.
METHODS: This pilot, retrospective, multicentre analysis included 122 patients 
who underwent colorectal surgery. Patient records were reviewed by both manual 
surveillance and ChatGPT, which was tasked with identifying SSI and categorizing 
them as superficial, deep, or organ-space infections. Sensitivity, specificity, 
positive predictive value (PPV), and negative predictive value (NPV) were 
calculated. Receiver operating characteristic (ROC) curve analysis determined 
the model's diagnostic performance.
RESULTS: ChatGPT achieved a sensitivity of 100 %, correctly identifying all SSIs 
detected by manual methods. The specificity was 54 %, indicating the presence of 
false positives. The PPV was 67 %, and the NPV was 100 %. The area under the ROC 
curve was 0.77, indicating good overall accuracy for distinguishing between SSI 
and non-SSI cases. Minor differences in outcomes were observed between colon and 
rectal surgeries, as well as between the hospitals participating in the study.
CONCLUSIONS: ChatGPT shows high sensitivity and good overall accuracy for 
detecting SSI. It appears to be a useful tool for initial screenings and for 
reducing manual review workload. The moderate specificity suggests a need for 
further refinement to reduce the rate of false positives. The integration of 
ChatGPT alongside electronic medical records, antibiotic consumption and imaging 
data results for real-time analysis may further improve the surveillance of SSI.
CLINICALTRIALS: gov Identifier: NCT06556017.

Copyright © 2024 The Author(s). Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.jiph.2024.102627
PMID: 39740340 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest All authors 
declare no conflict of interest relevant to this article.


516. J Am Med Inform Assoc. 2025 Feb 1;32(2):365-374. doi: 10.1093/jamia/ocae310.

CARE-SD: classifier-based analysis for recognizing provider stigmatizing and 
doubt marker labels in electronic health records: model development and 
validation.

Walker A(1), Thorne A(2), Das S(1), Love J(3), Cooper HLF(4), Livingston M 
3rd(4), Sarker A(1)(5).

Author information:
(1)Department of Biomedical Informatics, School of Medicine, Emory University, 
Atlanta, GA 30322, United States.
(2)Department of Infectious Disease, Children's Healthcare of Atlanta, Atlanta, 
GA 30329, United States.
(3)Department of Emergency Medicine, Mount Sinai, New York, NY 10029, United 
States.
(4)Department of Behavioral, Social, Health Education Sciences, Rollins School 
of Public Health, Emory University, Atlanta, GA 30322, United States.
(5)Department of Biomedical Engineering, Georgia Institute of Technology and 
Emory University, Atlanta, GA 30332, United States.

OBJECTIVE: To detect and classify features of stigmatizing and biased language 
in intensive care electronic health records (EHRs) using natural language 
processing techniques.
MATERIALS AND METHODS: We first created a lexicon and regular expression lists 
from literature-driven stem words for linguistic features of stigmatizing 
patient labels, doubt markers, and scare quotes within EHRs. The lexicon was 
further extended using Word2Vec and GPT 3.5, and refined through human 
evaluation. These lexicons were used to search for matches across 18 million 
sentences from the de-identified Medical Information Mart for Intensive Care-III 
(MIMIC-III) dataset. For each linguistic bias feature, 1000 sentence matches 
were sampled, labeled by expert clinical and public health annotators, and used 
to supervised learning classifiers.
RESULTS: Lexicon development from expanded literature stem-word lists resulted 
in a doubt marker lexicon containing 58 expressions, and a stigmatizing labels 
lexicon containing 127 expressions. Classifiers for doubt markers and 
stigmatizing labels had the highest performance, with macro F1-scores of 0.84 
and 0.79, positive-label recall and precision values ranging from 0.71 to 0.86, 
and accuracies aligning closely with human annotator agreement (0.87).
DISCUSSION: This study demonstrated the feasibility of supervised classifiers in 
automatically identifying stigmatizing labels and doubt markers in medical text 
and identified trends in stigmatizing language use in an EHR setting. Additional 
labeled data may help improve lower scare quote model performance.
CONCLUSIONS: Classifiers developed in this study showed high model performance 
and can be applied to identify patterns and target interventions to reduce 
stigmatizing labels and doubt markers in healthcare systems.

© The Author(s) 2024. Published by Oxford University Press on behalf of the 
American Medical Informatics Association.

DOI: 10.1093/jamia/ocae310
PMCID: PMC11756621
PMID: 39724920 [Indexed for MEDLINE]

Conflict of interest statement: The authors have no competing interests to 
share.


517. Comput Biol Med. 2025 Feb;185:109598. doi: 10.1016/j.compbiomed.2024.109598. 
Epub 2024 Dec 20.

PTSP-BERT: Predict the thermal stability of proteins using sequence-based 
bidirectional representations from transformer-embedded features.

Lv Z(1), Wei M(2), Pei H(3), Peng S(2), Li M(2), Jiang L(4).

Author information:
(1)College of Biomedical Engineering, Sichuan University, Chengdu, 610065, 
China. Electronic address: lvzhibin@pku.edu.cn.
(2)College of Biomedical Engineering, Sichuan University, Chengdu, 610065, 
China.
(3)Department of Biomedical Engineering, Johns Hopkins University, MD, 21218, 
USA.
(4)College of Food and Biological Engineering, Chengdu University, Chengdu, 
610106, China; Country Key Laboratory of Coarse Cereal Processing, Ministry of 
Agriculture and Rural Affairs, Chengdu, 610106, China.

Thermophilic proteins, mesophiles proteins and psychrophilic proteins have wide 
industrial applications, as enzymes with different optimal temperatures are 
often needed for different purposes. Convenient methods are needed to determine 
the optimal temperatures for proteins; however, laboratory methods for this 
purpose are time-consuming and laborious, and existing machine learning methods 
can only perform binary classification of thermophilic and non-thermophilic 
proteins, or psychrophilic and non-psychrophilic proteins. Here, we developed a 
deep learning model, PSTP-BERT, based on protein sequences that can directly 
perform Three classes identification of thermophilic, mesophilic, and 
psychrophilic proteins. By comparing BERT-bfd with other deep learning models 
using five-fold cross-validation, we found that BERT-bfd-extracted features 
achieved the highest accuracy under six classifiers. Furthermore, to improve the 
model's accuracy, we used SMOTE (synthetic minority oversampling technique) to 
balance the dataset and light gradient-boosting machine to rank 
BERT-bfd-extracted features according to their weights. We obtained the 
best-performing model with five-fold cross-validation accuracy of 89.59 % and 
independent test accuracy of 85.42 %. The performance of the PSTP-BERT is 
significantly better than that of existing models in Three classes 
identification task. In order to compare with previous binary classification 
models, we used PSTP-BERT to perform binary classification tasks of thermophilic 
and non-thermophilic protein, and psychrophilic and non-psychrophilic protein on 
an independent test set. PSTP-BERT achieved the highest accuracy on both binary 
classification tasks, with an accuracy of 93.33 % for thermophilic protein 
binary classification and 88.33 % for psychrophilic protein binary 
classification. The accuracy of the independent test of the model can reach 
between 89.8 % and 92.9 % after training and optimization of the training set 
with different sequence similarities, and the prediction accuracy of the new 
data can exceed 97 %. For the convenience of future researchers to use and 
reference, we have uploaded source code of PSTP-BERT to GitHub.

Copyright © 2024 Elsevier Ltd. All rights reserved.

DOI: 10.1016/j.compbiomed.2024.109598
PMID: 39708499 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest We have 
nothing to declare.


518. J Imaging Inform Med. 2025 Oct;38(5):2688-2699. doi: 10.1007/s10278-024-01362-w. 
Epub 2024 Dec 20.

Development and Validation of a Modality-Invariant 3D Swin U-Net Transformer for 
Liver and Spleen Segmentation on Multi-Site Clinical Bi-parametric MR Images.

Zhang H(1)(2), Li H(1)(3)(4)(5), Ali R(1), Jia W(1)(6), Pan W(1), Reeder SB(7), 
Harris D(7), Masch W(8), Aslam A(8), Shanbhogue K(9), Parikh NA(5)(10), Dillman 
JR(1)(3)(4), He L(11)(12)(13)(14)(15)(16)(17).

Author information:
(1)Imaging Research Center, Department of Radiology, Cincinnati Children's 
Hospital Medical Center, Cincinnati, OH, USA.
(2)Department of Computer Science, University of Cincinnati, Cincinnati, OH, 
USA.
(3)Artificial Intelligence Imaging Research Center, Cincinnati Children's 
Hospital Medical Center, Cincinnati, OH, USA.
(4)Department of Radiology, University of Cincinnati College of Medicine, 
Cincinnati, OH, USA.
(5)Neurodevelopmental Disorders Prevention Center, Perinatal Institute, 
Cincinnati Children's Hospital Medical Center, Cincinnati, OH, USA.
(6)Department of Biostatistics, University of Cincinnati, Cincinnati, OH, USA.
(7)Department of Radiology, Medical Physics, Biomedical Engineering, Medicine, 
Emergency Medicine, University of Wisconsin, Madison, WI, USA.
(8)Department of Radiology, Michigan Medicine, Ann Arbor, MI, USA.
(9)Department of Radiology, New York University, New York, NY, USA.
(10)Department of Pediatrics, University of Cincinnati College of Medicine, 
Cincinnati, OH, USA.
(11)Imaging Research Center, Department of Radiology, Cincinnati Children's 
Hospital Medical Center, Cincinnati, OH, USA. lili.he@cchmc.org.
(12)Department of Computer Science, University of Cincinnati, Cincinnati, OH, 
USA. lili.he@cchmc.org.
(13)Artificial Intelligence Imaging Research Center, Cincinnati Children's 
Hospital Medical Center, Cincinnati, OH, USA. lili.he@cchmc.org.
(14)Department of Radiology, University of Cincinnati College of Medicine, 
Cincinnati, OH, USA. lili.he@cchmc.org.
(15)Neurodevelopmental Disorders Prevention Center, Perinatal Institute, 
Cincinnati Children's Hospital Medical Center, Cincinnati, OH, USA. 
lili.he@cchmc.org.
(16)Department of Biomedical Engineering, University of Cincinnati, Cincinnati, 
OH, USA. lili.he@cchmc.org.
(17)Department of Biomedical Informatics, University of Cincinnati College of 
Medicine, Cincinnati, OH, USA. lili.he@cchmc.org.

To develop and validate a modality-invariant Swin U-Net Transformer (UNETR) deep 
learning model for liver and spleen segmentation on abdominal T1-weighted (T1w) 
or T2-weighted (T2w) MR images from multiple institutions for pediatric and 
adult patients with known or suspected chronic liver diseases. In this 
IRB-approved retrospective study, clinical abdominal axial T1w and T2w MR images 
from pediatric and adult patients were retrieved from four study sites, 
including Cincinnati Children's Hospital Medical Center (CCHMC), New York 
University (NYU), University of Wisconsin (UW) and University of Michigan / 
Michigan Medicine (UM). The whole liver and spleen were manually delineated as 
the ground truth masks. We developed a modality-invariant 3D Swin UNETR using a 
modality-invariant training strategy, in which each patient's T1w and T2w MR 
images were treated as separate training samples. We conducted both internal and 
external validation experiments. A total of 241 T1w and 339 T2w MR sequences 
from 304 patients (age [mean ± standard deviation], 31.8 ± 20.3 years; 132 [43%] 
female) were included for model development. The Swin UNETR achieved a Dice 
similarity coefficient (DSC) of 0.95 ± 0.02 on T1w images and 0.93 ± 0.05 on T2w 
images for liver segmentation. This is significantly better than the U-Net model 
(0.90 ± 0.05, p < 0.001 and 0.90 ± 0.13, p < 0.001, respectively). The Swin 
UNETR achieved a DSC of 0.88 ± 0.12 on T1w images and 0.93 ± 0.10 on T2w images 
for spleen segmentation, and it significantly outperformed a modality-invariant 
U-Net model (0.80 ± 0.18, p = 0.001 and 0.88 ± 0.12, p = 0.002, respectively). 
Our study demonstrated that a modality-invariant Swin UNETR model can segment 
the liver and spleen on routinely collected clinical bi-parametric abdominal MR 
images from children and adult patients.

© 2024. The Author(s) under exclusive licence to Society for Imaging Informatics 
in Medicine.

DOI: 10.1007/s10278-024-01362-w
PMCID: PMC12572494
PMID: 39707114 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Ethics Approval: Institutional 
Review Board approval was obtained. Consent to Participate: Written informed 
consent was waived by the Institutional Review Board. Consent to Publish: The 
authors affirm that human research participants provided informed consent for 
the publication of the data and results contained in this manuscript. Competing 
Interests: The authors declare no competing interests.


519. J Am Med Inform Assoc. 2025 Feb 1;32(2):357-364. doi: 10.1093/jamia/ocae308.

Lessons learned on information retrieval in electronic health records: a 
comparison of embedding models and pooling strategies.

Myers S(1), Miller TA(2)(3), Gao Y(4), Churpek MM(1), Mayampurath A(1), Dligach 
D(5), Afshar M(1).

Author information:
(1)Department of Medicine, University of Wisconsin-Madison, Madison, WI 53726, 
United States.
(2)Computational Health Informatics Program, Boston Children's Hospital, Boston, 
MA 02215, United States.
(3)Department of Pediatrics, Harvard Medical School, Boston, MA 02215, United 
States.
(4)Department of Biomedical Informatics, University of Colorado-Anschutz, 
Aurora, CO 80045, United States.
(5)Department of Computer Science, Loyola University Chicago, Chicago, IL 60626, 
United States.

OBJECTIVES: Applying large language models (LLMs) to the clinical domain is 
challenging due to the context-heavy nature of processing medical records. 
Retrieval-augmented generation (RAG) offers a solution by facilitating reasoning 
over large text sources. However, there are many parameters to optimize in just 
the retrieval system alone. This paper presents an ablation study exploring how 
different embedding models and pooling methods affect information retrieval for 
the clinical domain.
MATERIALS AND METHODS: Evaluating on 3 retrieval tasks on 2 electronic health 
record (EHR) data sources, we compared 7 models, including medical- and 
general-domain models, specialized encoder embedding models, and off-the-shelf 
decoder LLMs. We also examine the choice of embedding pooling strategy for each 
model, independently on the query and the text to retrieve.
RESULTS: We found that the choice of embedding model significantly impacts 
retrieval performance, with BGE, a comparatively small general-domain model, 
consistently outperforming all others, including medical-specific models. 
However, our findings also revealed substantial variability across datasets and 
query text phrasings. We also determined the best pooling methods for each of 
these models to guide future design of retrieval systems.
DISCUSSION: The choice of embedding model, pooling strategy, and query 
formulation can significantly impact retrieval performance and the performance 
of these models on other public benchmarks does not necessarily transfer to new 
domains. The high variability in performance across different query phrasings 
suggests that the choice of query may need to be tuned and validated for each 
task, or even for each institution's EHR.
CONCLUSION: This study provides empirical evidence to guide the selection of 
models and pooling strategies for RAG frameworks in healthcare applications. 
Further studies such as this one are vital for guiding empirically-grounded 
development of retrieval frameworks, such as in the context of RAG, for the 
clinical domain.

© The Author(s) 2024. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For permissions, 
please email: journals.permissions@oup.com.

DOI: 10.1093/jamia/ocae308
PMCID: PMC11756698
PMID: 39703187 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that they have no known 
competing financial interests or personal relationships that could have appeared 
to influence the work reported in this paper.


520. Child Abuse Negl. 2025 Feb;160:107202. doi: 10.1016/j.chiabu.2024.107202. Epub 
2024 Dec 16.

Case reports unlocked: Harnessing large language models to advance research on 
child maltreatment.

Stoll D(1), Wehrli S(2), Lätsch D(3).

Author information:
(1)School of Social Work, ZHAW Zurich University of Applied Sciences, 
Switzerland. Electronic address: dragan.stoll@zhaw.ch.
(2)School of Life Sciences and Facility Management, ZHAW Zurich University of 
Applied Sciences, Switzerland. Electronic address: samuel.wehrli@zhaw.ch.
(3)School of Social Work, ZHAW Zurich University of Applied Sciences, 
Switzerland.

BACKGROUND: Research on child protective services (CPS) is impeded by a lack of 
high-quality structured data. Crucial information on cases is often documented 
in case files, but only in narrative form. Researchers have applied automated 
language processing to extract structured data from these narratives, but this 
has been limited to classification tasks of fairly low complexity. Large 
language models (LLMs) may work for more challenging tasks.
OBJECTIVE: We aimed to extract structured data from narrative casework reports 
by applying LLMs to distinguish between different subtypes of violence: child 
sexual abuse, child physical abuse, a child witnessing domestic violence, and a 
child being physically aggressive.
METHODS: We developed a four-stage pipeline comprising of (1) text segmentation, 
(2) text segment classification, and subsequent labeling of (3) casework 
reports, and (4) cases. All CPS reports (N = 29,770) between 2008 and 2022 from 
Switzerland's largest CPS provider were collected. 28,223 text segments were 
extracted based on pre-defined keywords. Two human reviewers annotated random 
samples of text segments and reports for training and validation. Model 
performance was compared against human-coded test data.
RESULTS: The best-performing LLM (Mixtral-8x7B) classified text segments with an 
accuracy of 87 %, outperforming agreement between the two human reviewers 
(77 %). The model also correctly labelled casework reports with an accuracy of 
87 %, but only when disregarding non-extracted text segments in stage (1).
CONCLUSIONS: LLMs can replicate human coding of text documents even for highly 
complex tasks that require contextual information. This may considerably advance 
research on CPS. Transparency can be achieved by backtracking labeling decisions 
to individual text segments. Keyword-based text segmentation was identified as a 
weak point, and the potential for bias that may occur at several stages of the 
process requires attention.

Copyright © 2024 The Authors. Published by Elsevier Ltd.. All rights reserved.

DOI: 10.1016/j.chiabu.2024.107202
PMID: 39689392 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no competing interests.


521. J Am Med Inform Assoc. 2026 Jan 1;33(1):79-89. doi: 10.1093/jamia/ocae298.

Using large language models to detect outcomes in qualitative studies of 
adolescent depression.

Xin AW(1), Nielson DM(1), Krause KR(2), Fiorini G(3), Midgley N(3), Pereira 
F(1), Lossio-Ventura JA(1).

Author information:
(1)Machine Learning Core, National Institute of Mental Health, National 
Institutes of Health, Bethesda, MD 20892, United States.
(2)Centre of Research in Epidemiology and Statistics (CRESS UMR 1153), 
Université Paris Cité, Paris 75004, France.
(3)Department of Clinical, Educational and Health Psychology, University 
College, London WC1E 6BT, United Kingdom.

OBJECTIVE: We aim to use large language models (LLMs) to detect mentions of 
nuanced psychotherapeutic outcomes and impacts than previously considered in 
transcripts of interviews with adolescent depression. Our clinical authors 
previously created a novel coding framework containing fine-grained therapy 
outcomes beyond the binary classification (eg, depression vs control) based on 
qualitative analysis embedded within a clinical study of depression. Moreover, 
we seek to demonstrate that embeddings from LLMs are informative enough to 
accurately label these experiences.
MATERIALS AND METHODS: Data were drawn from interviews, where text segments were 
annotated with different outcome labels. Five different open-source LLMs were 
evaluated to classify outcomes from the coding framework. Classification 
experiments were carried out in the original interview transcripts. Furthermore, 
we repeated those experiments for versions of the data produced by breaking 
those segments into conversation turns, or keeping non-interviewer utterances 
(monologues).
RESULTS: We used classification models to predict 31 outcomes and 8 derived 
labels, for 3 different text segmentations. Area under the ROC curve scores 
ranged between 0.6 and 0.9 for the original segmentation and 0.7 and 1.0 for the 
monologues and turns.
DISCUSSION: LLM-based classification models could identify outcomes important to 
adolescents, such as friendships or academic and vocational functioning, in text 
transcripts of patient interviews. By using clinical data, we also aim to better 
generalize to clinical settings compared to studies based on public social media 
data.
CONCLUSION: Our results demonstrate that fine-grained therapy outcome coding in 
psychotherapeutic text is feasible, and can be used to support the 
quantification of important outcomes for downstream uses.

Published by Oxford University Press on behalf of the American Medical 
Informatics Association 2024.

DOI: 10.1093/jamia/ocae298
PMCID: PMC12758459
PMID: 39661754 [Indexed for MEDLINE]

Conflict of interest statement: The authors declare that they have no conflict 
of interest.


522. Med Image Anal. 2025 Apr;101:103395. doi: 10.1016/j.media.2024.103395. Epub 2024 
Nov 25.

Multidimensional Directionality-Enhanced Segmentation via large vision model.

Huang X(1), Yue C(2), Guo Y(2), Huang J(2), Jiang Z(2), Wang M(2), Xu Z(3), 
Zhang G(4), Liu J(5), Zhang T(6), Zheng Z(7), Zhang X(8), He H(9), Jiang S(10), 
Sun Y(11).

Author information:
(1)Hangzhou Dianzi University, Hangzhou, China; School of Electronic Engineering 
and Computer Science, Queen Mary University, London, UK.
(2)Hangzhou Dianzi University, Hangzhou, China.
(3)Department of Paediatrics, University of Cambridge, Cambridge, UK.
(4)College of Engineering, College of Engineering, Peking University, Beijing, 
China.
(5)Hangzhou Dianzi University, Hangzhou, China; School of Remote Sensing and 
Information Engineering, Wuhan University, Wuhan, China. Electronic address: 
jinliu@hdu.edu.cn.
(6)Hangzhou Dianzi University, Hangzhou, China. Electronic address: 
tianyun.zhang@hdu.edu.cn.
(7)Hangzhou Dianzi University, Hangzhou, China. Electronic address: 
zhiwen.zheng@hdu.edu.cn.
(8)Faculty of Information Science and Engineering, Ocean University of China, 
Qingdao, China. Electronic address: x.zhang@ouc.edu.cn.
(9)Hangzhou Dianzi University, Hangzhou, China. Electronic address: 
hehong@hdu.edu.cn.
(10)Hangzhou Dianzi University, Hangzhou, China. Electronic address: 
jiangsw@hdu.edu.cn.
(11)Hangzhou Dianzi University, Hangzhou, China. Electronic address: 
syq@hdu.edu.cn.

Optical Coherence Tomography (OCT) facilitates a comprehensive examination of 
macular edema and associated lesions. Manual delineation of retinal fluid is 
labor-intensive and error-prone, necessitating an automated diagnostic and 
therapeutic planning mechanism. Conventional supervised learning models are 
hindered by dataset limitations, while Transformer-based large vision models 
exhibit challenges in medical image segmentation, particularly in detecting 
small, subtle lesions in OCT images. This paper introduces the Multidimensional 
Directionality-Enhanced Retinal Fluid Segmentation framework (MD-DERFS), which 
reduces the limitations inherent in conventional supervised models by adapting a 
transformer-based large vision model for macular edema segmentation. The 
proposed MD-DERFS introduces a Multi-Dimensional Feature Re-Encoder Unit (MFU) 
to augment the model's proficiency in recognizing specific textures and 
pathological features through directional prior extraction and an Edema Texture 
Mapping Unit (ETMU), a Cross-scale Directional Insight Network (CDIN) furnishes 
a holistic perspective spanning local to global details, mitigating the large 
vision model's deficiencies in capturing localized feature information. 
Additionally, the framework is augmented by a Harmonic Minutiae Segmentation 
Equilibrium loss (LHMSE) that can address the challenges of data imbalance and 
annotation scarcity in macular edema datasets. Empirical validation on the 
MacuScan-8k dataset shows that MD-DERFS surpasses existing segmentation 
methodologies, demonstrating its efficacy in adapting large vision models for 
boundary-sensitive medical imaging tasks. The code is publicly available at 
https://github.com/IMOP-lab/MD-DERFS-Pytorch.git.

Copyright © 2024 Elsevier B.V. All rights reserved.

DOI: 10.1016/j.media.2024.103395
PMID: 39644753 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of competing interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


523. ANZ J Surg. 2025 Mar;95(3):464-496. doi: 10.1111/ans.19337. Epub 2024 Dec 2.

Assessing the efficacy of artificial intelligence to provide peri-operative 
information for patients with a stoma.

Lim B(1), Lirios G(1), Sakalkale A(2), Satheakeerthy S(2), Hayes D(1), Yeung 
JMC(1)(2).

Author information:
(1)Department of Colorectal Surgery, Western Health, Melbourne, Australia.
(2)Department of Surgery, Western Precinct, University of Melbourne, Melbourne, 
Australia.

BACKGROUND: Stomas present significant lifestyle and psychological challenges 
for patients, requiring comprehensive education and support. Current educational 
methods have limitations in offering relevant information to the patient, 
highlighting a potential role for artificial intelligence (AI). This study 
examined the utility of AI in enhancing stoma therapy management following 
colorectal surgery.
MATERIAL AND METHODS: We compared the efficacy of four prominent large language 
models (LLM)-OpenAI's ChatGPT-3.5 and ChatGPT-4.0, Google's Gemini, and Bing's 
CoPilot-against a series of metrics to evaluate their suitability as 
supplementary clinical tools. Through qualitative and quantitative analyses, 
including readability scores (Flesch-Kincaid, Flesch-Reading Ease, and 
Coleman-Liau index) and reliability assessments (Likert scale, DISCERN score and 
QAMAI tool), the study aimed to assess the appropriateness of LLM-generated 
advice for patients managing stomas.
RESULTS: There are varying degrees of readability and reliability across the 
evaluated models, with CoPilot and ChatGPT-4 demonstrating superior performance 
in several key metrics such as readability and comprehensiveness. However, the 
study underscores the infant stage of LLM technology in clinical applications. 
All responses required high school to college level education to comprehend 
comfortably. While the LLMs addressed users' questions directly, the absence of 
incorporating patient-specific factors such as past medical history generated 
broad and generic responses rather than offering tailored advice.
CONCLUSION: The complexity of individual patient conditions can challenge AI 
systems. The use of LLMs in clinical settings holds promise for improving 
patient education and stoma management support, but requires careful 
consideration of the models' capabilities and the context of their use.

© 2024 Royal Australasian College of Surgeons.

DOI: 10.1111/ans.19337
PMID: 39620607 [Indexed for MEDLINE]


524. J Am Med Inform Assoc. 2025 Feb 1;32(2):285-295. doi: 10.1093/jamia/ocae287.

LCD benchmark: long clinical document benchmark on mortality prediction for 
language models.

Yoon W(1)(2), Chen S(1)(2)(3)(4), Gao Y(5), Zhao Z(1)(2), Dligach D(6), 
Bitterman DS(1)(2)(3)(4), Afshar M(5), Miller T(1)(2).

Author information:
(1)Computational Health Informatics Program, Boston Children's Hospital, Boston, 
MA 02215, United States.
(2)Department of Pediatrics, Harvard Medical School, Boston, MA 02115, United 
States.
(3)Artificial Intelligence in Medicine (AIM) Program, Mass General Brigham, 
Harvard Medical School, Boston, MA 02115, United States.
(4)Department of Radiation Oncology, Brigham and Women's Hospital/Dana-Farber 
Cancer Institute, Boston, MA 02215, United States.
(5)Department of Medicine, University of Wisconsin-Madison, Madison, WI 53705, 
United States.
(6)Department of Computer Science, Loyola University Chicago, Chicago, IL 60660, 
United States.

Update of
    medRxiv. 2024 Jul 02:2024.03.26.24304920. doi: 10.1101/2024.03.26.24304920.

OBJECTIVES: The application of natural language processing (NLP) in the clinical 
domain is important due to the rich unstructured information in clinical 
documents, which often remains inaccessible in structured data. When applying 
NLP methods to a certain domain, the role of benchmark datasets is crucial as 
benchmark datasets not only guide the selection of best-performing models but 
also enable the assessment of the reliability of the generated outputs. Despite 
the recent availability of language models capable of longer context, benchmark 
datasets targeting long clinical document classification tasks are absent.
MATERIALS AND METHODS: To address this issue, we propose Long Clinical Document 
(LCD) benchmark, a benchmark for the task of predicting 30-day out-of-hospital 
mortality using discharge notes of Medical Information Mart for Intensive Care 
IV and statewide death data. We evaluated this benchmark dataset using baseline 
models, from bag-of-words and convolutional neural network to instruction-tuned 
large language models. Additionally, we provide a comprehensive analysis of the 
model outputs, including manual review and visualization of model weights, to 
offer insights into their predictive capabilities and limitations.
RESULTS: Baseline models showed 28.9% for best-performing supervised models and 
32.2% for GPT-4 in F1 metrics. Notes in our dataset have a median word count of 
1687.
DISCUSSION: Our analysis of the model outputs showed that our dataset is 
challenging for both models and human experts, but the models can find 
meaningful signals from the text.
CONCLUSION: We expect our LCD benchmark to be a resource for the development of 
advanced supervised models, or prompting methods, tailored for clinical text.

© The Author(s) 2024. Published by Oxford University Press on behalf of the 
American Medical Informatics Association. All rights reserved. For permissions, 
please email: journals.permissions@oup.com.

DOI: 10.1093/jamia/ocae287
PMCID: PMC11756648
PMID: 39602813 [Indexed for MEDLINE]

Conflict of interest statement: None declared.


525. Acad Radiol. 2025 Mar;32(3):1547-1553. doi: 10.1016/j.acra.2024.10.028. Epub 
2024 Nov 16.

Accuracy and Readability of ChatGPT on Potential Complications of Interventional 
Radiology Procedures: AI-Powered Patient Interviewing.

Kaba E(1), Beyazal M(2), Çeliker FB(2), Yel İ(3), Vogl TJ(3).

Author information:
(1)Recep Tayyip Erdogan University, Department of Radiology, Rize, Turkey (E.K., 
M.B., F.B.C.). Electronic address: esatkaba04@gmail.com.
(2)Recep Tayyip Erdogan University, Department of Radiology, Rize, Turkey (E.K., 
M.B., F.B.C.).
(3)University Hospital Frankfurt, Department of Diagnostic and Interventional 
Radiology, Frankfurt, Germany (I.Y., T.J.V.).

RATIONALE AND OBJECTIVES: It is crucial to inform the patient about potential 
complications and obtain consent before interventional radiology procedures. In 
this study, we investigated the accuracy, reliability, and readability of the 
information provided by ChatGPT-4 about potential complications of 
interventional radiology procedures.
MATERIALS AND METHODS: Potential major and minor complications of 25 different 
interventional radiology procedures (8 non-vascular, 17 vascular) were asked to 
ChatGPT-4 chatbot. The responses were evaluated by two experienced 
interventional radiologists (>25 years and 10 years of experience) using a 
5-point Likert scale according to Cardiovascular and Interventional Radiological 
Society of Europe guidelines. The correlation between the two interventional 
radiologists' scoring was evaluated by the Wilcoxon signed-rank test, Intraclass 
Correlation Coefficient (ICC), and Pearson correlation coefficient (PCC). In 
addition, readability and complexity were quantitatively assessed using the 
Flesch-Kincaid Grade Level, Flesch Reading Ease scores, and Simple Measure of 
Gobbledygook (SMOG) index.
RESULTS: Interventional radiologist 1 (IR1) and interventional radiologist 2 
(IR2) gave 104 and 109 points, respectively, out of a potential 125 points for 
the total of all procedures. There was no statistically significant difference 
between the total scores of the two IRs (p = 0.244). The IRs demonstrated high 
agreement across all procedure ratings (ICC=0.928). Both IRs scored 34 out of 40 
points for the eight non-vascular procedures. 17 vascular procedures received 70 
points out of 85 from IR1 and 75 from IR2. The agreement between the two 
observers' assessments was good, with PCC values of 0.908 and 0.896 for 
non-vascular and vascular procedures, respectively. Readability levels were 
overall low. The mean Flesch-Kincaid Grade Level, Flesch Reading Ease scores, 
and SMOG index were 12.51 ± 1.14 (college level) 30.27 ± 8.38 (college level), 
and 14.46 ± 0.76 (college level), respectively. There was no statistically 
significant difference in readability between non-vascular and vascular 
procedures (p = 0.16).
CONCLUSION: ChatGPT-4 demonstrated remarkable performance, highlighting its 
potential to enhance accessibility to information about interventional radiology 
procedures and support the creation of educational materials for patients. Based 
on the findings of our study, while ChatGPT provides accurate information and 
shows no evidence of hallucinations, it is important to emphasize that a high 
level of education and health literacy are required to fully comprehend its 
responses.

Copyright © 2024 The Association of University Radiologists. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.acra.2024.10.028
PMID: 39551684 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


526. Dent Traumatol. 2025 Apr;41(2):178-186. doi: 10.1111/edt.12999. Epub 2024 Nov 2.

Evidence-Based Potential of Generative Artificial Intelligence Large Language 
Models on Dental Avulsion: ChatGPT Versus Gemini.

Tokgöz Kaplan T(1), Cankar M(2).

Author information:
(1)Department of Pedodontics, Faculty of Dentistry, Karabuk University, Karabük, 
Turkey.
(2)Private Clinic, Çorum, Turkey.

BACKGROUND: In this study, the accuracy and comprehensiveness of the answers 
given to questions about dental avulsion by two artificial intelligence-based 
language models, ChatGPT and Gemini, were comparatively evaluated.
MATERIALS AND METHODS: Based on the guidelines of the International Society of 
Dental Traumatology, a total of 33 questions were prepared, including 
multiple-choice questions, binary questions, and open-ended questions as 
technical questions and patient questions about dental avulsion. They were 
directed to ChatGPT and Gemini. Responses were recorded and scored by four 
pediatric dentists. Statistical analyses, including ICC analysis, were performed 
to determine the agreement and accuracy of the responses. The significance level 
was set as p < 0.050.
RESULTS: The mean score of the Gemini model was statistically significantly 
higher than the ChatGPT (p = 0.001). ChatGPT gave more correct answers to 
open-ended questions and T/F questions on dental avulsion; it showed the lowest 
accuracy in the MCQ section. There was no significant difference between the 
responses of the Gemini model to different types of questions on dental avulsion 
and the median scores (p = 0.088). ChatGPT and Gemini were analyzed with the 
Mann-Whitney U test without making a distinction between question types, and 
Gemini answers were found to be statistically significantly more accurate 
(p = 0.004).
CONCLUSIONS: The Gemini and ChatGPT language models based on the IADT guideline 
for dental avulsion undoubtedly show promise. To guarantee the successful 
incorporation of LLMs into practice, it is imperative to conduct additional 
research, clinical validation, and improvements to the models.

© 2024 John Wiley & Sons A/S. Published by John Wiley & Sons Ltd.

DOI: 10.1111/edt.12999
PMID: 39487673 [Indexed for MEDLINE]


527. Acad Radiol. 2025 Feb;32(2):604-611. doi: 10.1016/j.acra.2024.09.042. Epub 2024 
Oct 15.

Large Language Models can Help with Biostatistics and Coding Needed in Radiology 
Research.

Ghosh A(1), Li H(2), Trout AT(3).

Author information:
(1)Department of Radiology, Cincinnati Children's Hospital Medical Center, 
Cincinnati, Ohio (A.G., H.L., A.T.T.); Department of Radiology, Nationwide 
Children's Hospital, Columbus, Ohio (A.G.). Electronic address: 
adarsh.ghosh.rad@gmail.com.
(2)Department of Radiology, Cincinnati Children's Hospital Medical Center, 
Cincinnati, Ohio (A.G., H.L., A.T.T.); Department of Radiology, University of 
Cincinnati College of Medicine, Cincinnati, Ohio (H.L., A.T.T.); Imaging 
Research Center, Cincinnati Children's Hospital Medical Center, Cincinnati, Ohio 
(H.L.).
(3)Department of Radiology, Cincinnati Children's Hospital Medical Center, 
Cincinnati, Ohio (A.G., H.L., A.T.T.); Department of Radiology, University of 
Cincinnati College of Medicine, Cincinnati, Ohio (H.L., A.T.T.); Department of 
Pediatrics, University of Cincinnati College of Medicine, Cincinnati, Ohio 
(A.T.T.).

INTRODUCTION: Original research in radiology often involves handling large 
datasets, data manipulation, statistical tests, and coding. Recent studies show 
that large language models (LLMs) can solve bioinformatics tasks, suggesting 
their potential in radiology research. This study evaluates an LLM's ability to 
provide statistical and deep learning solutions and code for radiology research.
MATERIALS AND METHODS: We used web-based chat interfaces available for 
ChatGPT-4o, ChatGPT-3.5, and Google Gemini. EXPERIMENT 1: BIOSTATISTICS AND DATA 
VISUALIZATION: We assessed each LLMs' ability to suggest biostatistical tests 
and generate R code for the same using a Cancer Imaging Archive dataset. Prompts 
were based on statistical analyses from a peer-reviewed manuscript. The 
generated code was tested in R Studio for correctness, runtime errors and the 
ability to generate the requested visualization. EXPERIMENT 2: DEEP LEARNING: We 
used the RSNA-STR Pneumonia Detection Challenge dataset to evaluate ChatGPT-4o 
and Gemini's ability to generate Python code for transformer-based image 
classification models (Vision Transformer ViT-B/16). The generated code was 
tested in a Jupiter Notebook for functionality and run time errors.
RESULTS: Out of the 8 statistical questions posed, correct statistical answers 
were suggested for 7 (ChatGPT-4o), 6 (ChatGPT-3.5), and 5 (Gemini) scenarios. 
The R code output by ChatGPT-4o had fewer runtime errors (6 out of the 7 total 
codes provided) compared to ChatGPT-3.5 (5/7) and Gemini (5/7). Both ChatGPT4o 
and Gemini were able to generate visualization requested with a few run time 
errors. Iteratively copying runtime errors from the code generated by ChatGPT4o 
into the chat helped resolve them. Gemini initially hallucinated during code 
generation but was able to provide accurate code on restarting the experiment. 
ChatGPT4-o and Gemini successfully generated initial Python code for deep 
learning tasks. Errors encountered during implementation were resolved through 
iterations using the chat interface, demonstrating LLM utility in providing 
baseline code for further code refinement and resolving run time errors.
CONCLUSION: LLMs can assist in coding tasks for radiology research, providing 
initial code for data visualization, statistical tests, and deep learning models 
helping researchers with foundational biostatistical knowledge. While LLM can 
offer a useful starting point, they require users to refine and validate the 
code and caution is necessary due to potential errors, the risk of 
hallucinations and data privacy regulations.
SUMMARY STATEMENT: LLMs can help with coding and statistical problems in 
radiology research. This can help primary authors trouble shoot coding needed in 
radiology research.

Copyright © 2025 The Association of University Radiologists. Published by 
Elsevier Inc. All rights reserved.

DOI: 10.1016/j.acra.2024.09.042
PMID: 39406582 [Indexed for MEDLINE]

Conflict of interest statement: Declaration of Competing Interest The authors 
declare that they have no known competing financial interests or personal 
relationships that could have appeared to influence the work reported in this 
paper.


528. Am J Med Genet A. 2025 Feb;197(2):e63878. doi: 10.1002/ajmg.a.63878. Epub 2024 
Sep 13.

Diagnostic Accuracy of a Custom Large Language Model on Rare Pediatric Disease 
Case Reports.

Young CC(1)(2), Enichen E(1)(2), Rivera C(1)(2), Auger CA(1)(2), Grant N(1)(2), 
Rao A(1)(2), Succi MD(2)(3).

Author information:
(1)Harvard Medical School, Boston, Massachusetts, USA.
(2)Medically Engineered Solutions in Healthcare Incubator, Innovation in 
Operations Research Center, Mass General Brigham, Boston, Massachusetts, USA.
(3)Department of Radiology, Massachusetts General Hospital, Boston, 
Massachusetts, USA.

Accurately diagnosing rare pediatric diseases frequently represent a clinical 
challenge due to their complex and unusual clinical presentations. Here, we 
explore the capabilities of three large language models (LLMs), GPT-4, Gemini 
Pro, and a custom-built LLM (GPT-4 integrated with the Human Phenotype Ontology 
[GPT-4 HPO]), by evaluating their diagnostic performance on 61 rare pediatric 
disease case reports. The performance of the LLMs were assessed for accuracy in 
identifying specific diagnoses, listing the correct diagnosis among a 
differential list, and broad disease categories. In addition, GPT-4 HPO was 
tested on 100 general pediatrics case reports previously assessed on other LLMs 
to further validate its performance. The results indicated that GPT-4 was able 
to predict the correct diagnosis with a diagnostic accuracy of 13.1%, whereas 
both GPT-4 HPO and Gemini Pro had diagnostic accuracies of 8.2%. Further, GPT-4 
HPO showed an improved performance compared with the other two LLMs in 
identifying the correct diagnosis among its differential list and the broad 
disease category. Although these findings underscore the potential of LLMs for 
diagnostic support, particularly when enhanced with domain-specific ontologies, 
they also stress the need for further improvement prior to integration into 
clinical practice.

© 2024 Wiley Periodicals LLC.

DOI: 10.1002/ajmg.a.63878
PMCID: PMC12123583
PMID: 39268988 [Indexed for MEDLINE]

Conflict of interest statement: Conflicts of Interest The authors declare no 
conflicts of interest.


529. J Sex Res. 2025 Nov-Dec;62(9):1905-1919. doi: 10.1080/00224499.2024.2396457. 
Epub 2024 Sep 10.

Large Language Models in an App: Conducting a Qualitative Synthetic Data 
Analysis of How Snapchat's "My AI" Responds to Questions About Sexual Consent, 
Sexual Refusals, Sexual Assault, and Sexting.

Marcantonio TL(1), Avery G(1), Thrash A(1), Leone RM(2).

Author information:
(1)Department of Health Science, College of Human Environmental Sciences, 
University of Alabama.
(2)Department of Health Policy and Behavioral Sciences, School of Public Health, 
Georgia State University.

Comprehensive sexual health education for young people often remains largely 
inaccessible, leaving gaps in knowledge about sexual consent, refusals, sexual 
assault, and sexting. Snapchat's My AI, tailored to its users, offers accessible 
and potentially stigma-free responses to sexual health queries, showing promise 
in addressing these gaps. This study aimed to evaluate My AI's effectiveness in 
handling questions related to sexual communication, assault, and sexting, and 
its response consistency across users. Collaborating with young adults and 
leveraging prior literature, our team formulated questions on sexual consent, 
refusals, sexual assault, and sexting submitted to My AI by 15 researchers. Each 
team member opened a chat with My AI in Snapchat and asked it a set of questions 
in the same order. We then conducted a qualitative content analysis to assess 
for consistency across users. My AI responses emphasized the importance of 
clear, honest consent and refusal communication through physical contact or 
direct conversation. With potential experiences of sexual assault or sexting, it 
advised consultation with a trusted caregiver. While My AI provided similar 
responses across researchers, variations were noted. My AI's responses mirrored 
common sexual health education messages and the responses used language 
reflective of sympathy toward potential assault victims or those who were asked 
for sexual images. My AI's potential to enhance accessibility to sexual health 
information could empower young people in making informed sexual health 
decisions. However, variability in responses may limit impact and comprehension, 
underscoring the importance of educators complementing these tools.

DOI: 10.1080/00224499.2024.2396457
PMCID: PMC11891083
PMID: 39254628 [Indexed for MEDLINE]


530. Am J Perinatol. 2025 Mar;42(4):495-501. doi: 10.1055/a-2405-3703. Epub 2024 Aug 
29.

Identifying Elective Induction of Labor among a Diverse Pregnant Population from 
Electronic Health Records within a Large Integrated Health Care System.

Xie F(1), Fassett MJ(2)(3), Im TM(1), Park D(1), Chiu VY(1), Getahun D(1)(4).

Author information:
(1)Department of Research and Evaluation, Kaiser Permanente Southern California, 
Pasadena, California.
(2)Department of Obstetrics and Gynecology, Kaiser Permanente West Los Angeles 
Medical Center, Los Angeles, California.
(3)Department of Clinical Science, Kaiser Permanente Bernard J. Tyson School of 
Medicine, Pasadena, California.
(4)Department of Health Systems Science, Kaiser Permanente Bernard J. Tyson 
School of Medicine, Pasadena, California.

OBJECTIVE:  Distinguishing between medically indicated induction of labor (iIOL) 
and elective induction of labor (eIOL) is a daunting process for researchers. We 
aimed to develop a Natural Language Processing (NLP) algorithm to identify eIOLs 
from electronic health records (EHRs) within a large integrated health care 
system.
STUDY DESIGN:  We used structured and unstructured data from Kaiser Permanente 
Southern California's EHRs of patients who were <35 years old and had singleton 
deliveries between 37 and 40 gestational weeks. Induction of labor (IOL) 
pregnancies were identified if there was evidence of an IOL diagnosis code, 
procedure code, or documentation in a delivery flowsheet or progress note. A 
comprehensive NLP algorithm was developed and refined through an iterative 
process of chart reviews and adjudications, where IOL-associated reasons 
(medically indicated vs. elective induction) were reviewed. The final algorithm 
was applied to discern the indications of IOLs performed during the study 
period.
RESULTS:  A total of 332,163 eligible pregnancies were identified between 
January 1, 2008, and December 31, 2022. Of these eligible pregnancies, 68,541 
(20.6%) were IOL, of which 6,824 (10.0%) were eIOL. Validation of the NLP 
process against 300 randomly selected pregnancies (100 eIOL, iIOL, and non-IOL 
cases each) yielded a positive predictive value of 83.0% and 88.0% for eIOL and 
iIOL, respectively. The rates of eIOL among the maternal age groups ranged 
between 9.6 and 10.3%, except for the <20 years group (12.2%). Non-Hispanic 
White individuals had the highest rate of eIOL (13.2%), while non-Hispanic 
Asian/Pacific Islanders had the lowest rate of eIOL (7.8%). The rate of eIOL 
increased from 1.0% in the 37-week gestational age (GA) group to 20.6% in the 
40-week GA group.
CONCLUSION:  Findings suggest that the developed NLP algorithm effectively 
identifies eIOL. It can be utilized to support eIOL-related 
pharmacoepidemiological studies, fill in knowledge gaps, and provide content 
more relevant to researchers.
KEY POINTS: · An NLP algorithm was developed to identify indications of IOL.. · 
The study algorithm was successfully implemented within a large integrated 
health care system.. · The study algorithm can be utilized to support 
eIOL-related studies..

Thieme. All rights reserved.

DOI: 10.1055/a-2405-3703
PMID: 39209302 [Indexed for MEDLINE]

Conflict of interest statement: None declared.


531. Med Teach. 2025 Mar;47(3):560-564. doi: 10.1080/0142159X.2024.2363486. Epub 2024 
Jun 20.

Can ChatGPT generate practice question explanations for medical students, a new 
faculty teaching tool?

Tong L(1), Wang J(1), Rapaka S(1), Garg PS(2).

Author information:
(1)Boston University Chobanian and Avedisian School of Medicine, Boston, MA, 
USA.
(2)Medical Education Office and Department of Pediatrics, Boston University 
Chobanian and Avedisian School of Medicine, Boston, MA, USA.

INTRODUCTION: Multiple-choice questions (MCQs) are frequently used for formative 
assessment in medical school but often lack sufficient answer explanations given 
time-restraints of faculty. Chat Generated Pre-trained Transformer (ChatGPT) has 
emerged as a potential student learning aid and faculty teaching tool. This 
study aims to evaluate ChatGPT's performance in answering and providing 
explanations for MCQs.
METHOD: Ninety-four faculty-generated MCQs were collected from the pre-clerkship 
curriculum at a US medical school. ChatGPT's accuracy in answering MCQ's were 
tracked on first attempt without an answer prompt (Pass 1) and after being given 
a prompt for the correct answer (Pass 2). Explanations provided by ChatGPT were 
compared with faculty-generated explanations, and a 3-point evaluation scale was 
used to assess accuracy and thoroughness compared to faculty-generated answers.
RESULTS: On first attempt, ChatGPT demonstrated a 75% accuracy in correctly 
answering faculty-generated MCQs. Among correctly answered questions, 66.4% of 
ChatGPT's explanations matched faculty explanations, and 89.1% captured some key 
aspects without providing inaccurate information. The amount of inaccurately 
generated explanations increases significantly if the questions was not answered 
correctly on the first pass (2.7% if correct on first pass vs. 34.6% if 
incorrect on first pass, p < 0.001).
CONCLUSION: ChatGPT shows promise in assisting faculty and students with 
explanations for practice MCQ's but should be used with caution. Faculty should 
review explanations and supplement to ensure coverage of learning objectives. 
Students can benefit from ChatGPT for immediate feedback through explanations if 
ChatGPT answers the question correctly on the first try. If the question is 
answered incorrectly students should remain cautious of the explanation and seek 
clarification from instructors.

DOI: 10.1080/0142159X.2024.2363486
PMID: 38900675 [Indexed for MEDLINE]


532. J Clin Endocrinol Metab. 2025 Feb 18;110(3):e841-e850. doi: 
10.1210/clinem/dgae235.

Comparing ChatGPT's and Surgeon's Responses to Thyroid-related Questions From 
Patients.

Guo S(1), Li R(2), Li G(1), Chen W(1), Huang J(1), He L(1), Ma Y(1), Wang L(1), 
Zheng H(3), Tian C(4), Zhao Y(5), Pan X(6), Wan H(7), Liu D(8), Li Z(1), Lei 
J(1).

Author information:
(1)Division of Thyroid Surgery, Department of General Surgery, West China 
Hospital, Sichuan University, Chengdu, Sichuan 610041, China.
(2)Health Management Center, General Practice Medical Center, West China 
Hospital, Sichuan University, Chengdu, Sichuan 610041, China.
(3)Department of Thyroid Surgery, General Surgery Ward 7, The First Hospital of 
Lanzhou University, Lanzhou, Gansu 730000, China.
(4)Chengdu Women's and Children's Central Hospital, School of Medicine, 
University of Electronic Science and Technology of China, Chengdu, Sichuan 
610031, China.
(5)Thyroid Surgery, Zhengzhou Central Hospital Affiliated of Zhengzhou 
University, Zhengzhou, Henan 450007, China.
(6)Department of Thyroid Surgery, General Surgery III, Gansu Provincial 
Hospital, Lanzhou, Gansu 730000, China.
(7)Department of Oncology, Sanya People's Hospital, Sanya, Hainan 572000, China.
(8)Department of Vascular Thyroid Surgery, The Second Affiliated Hospital of 
Guangzhou University of Chinese Medicine, Guangzhou, Guangdong 510120, China.

CONTEXT: For some common thyroid-related conditions with high prevalence and 
long follow-up times, ChatGPT can be used to respond to common thyroid-related 
questions.
OBJECTIVE: In this cross-sectional study, we assessed the ability of ChatGPT 
(version GPT-4.0) to provide accurate, comprehensive, compassionate, and 
satisfactory responses to common thyroid-related questions.
METHODS: First, we obtained 28 thyroid-related questions from the Huayitong app, 
which together with the 2 interfering questions eventually formed 30 questions. 
Then, these questions were responded to by ChatGPT (on July 19, 2023), a junior 
specialist, and a senior specialist (on July 20, 2023) separately. Finally, 26 
patients and 11 thyroid surgeons evaluated those responses on 4 dimensions: 
accuracy, comprehensiveness, compassion, and satisfaction.
RESULTS: Among the 30 questions and responses, ChatGPT's speed of response was 
faster than that of the junior specialist (8.69 [7.53-9.48] vs 4.33 [4.05-4.60]; 
P < .001) and the senior specialist (8.69 [7.53-9.48] vs 4.22 [3.36-4.76]; P < 
.001). The word count of the ChatGPT's responses was greater than that of both 
the junior specialist (341.50 [301.00-384.25] vs 74.50 [51.75-84.75]; P < .001) 
and senior specialist (341.50 [301.00-384.25] vs 104.00 [63.75-177.75]; P < 
.001). ChatGPT received higher scores than the junior specialist and senior 
specialist in terms of accuracy, comprehensiveness, compassion, and satisfaction 
in responding to common thyroid-related questions.
CONCLUSION: ChatGPT performed better than a junior specialist and senior 
specialist in answering common thyroid-related questions, but further research 
is needed to validate the logical ability of the ChatGPT for complex thyroid 
questions.

© The Author(s) 2024. Published by Oxford University Press on behalf of the 
Endocrine Society. All rights reserved. For commercial re-use, please contact 
reprints@oup.com for reprints and translation rights for reprints. All other 
permissions can be obtained through our RightsLink service via the Permissions 
link on the article page on our site—for further information please contact 
journals.permissions@oup.com.

DOI: 10.1210/clinem/dgae235
PMID: 38597169 [Indexed for MEDLINE]


533. medRxiv [Preprint]. 2025 Apr 22:2024.02.17.24302976. doi: 
10.1101/2024.02.17.24302976.

Artificial Intelligence-Based Automated Interpretation of Images of 
Electrocardiograms: Development and Multinational Validation of ECG-GPT.

Khunte A(1)(2), Sangha V(1)(3), Oikonomou EK(1), Dhingra LS(1), Aminorroaya 
A(1), Coppi A(1)(4), Shankar SV(1), Rockers E(5), Mortazavi BJ(4)(6), Bhatt 
DL(7), Krumholz HM(1)(4)(8), Al-Kindi S(5), Nadkarni GN(9)(10), Vaid A(9)(10), 
Khera R(1)(4)(11).

Author information:
(1)Section of Cardiovascular Medicine, Department of Internal Medicine, Yale 
School of Medicine, New Haven, CT.
(2)NYU Grossman School of Medicine, New York, NY.
(3)Department of Engineering Science, Oxford University, Oxford, UK.
(4)Center for Outcomes Research and Evaluation, Yale-New Haven Hospital, New 
Haven, CT.
(5)Center for Cardiovascular Computational & Precision Health, Houston Methodist 
DeBakey Heart & Vascular Center, Houston, TX, USA.
(6)Department of Computer Science & Engineering, Texas A&M University, College 
Station, TX.
(7)Mount Sinai Fuster Heart Hospital, Icahn School of Medicine at Mount Sinai, 
New York, NY.
(8)Department of Health Policy and Management, Yale School of Public Health, New 
Haven, CT.
(9)The Charles Bronfman Institute for Personalized Medicine, Icahn School of 
Medicine at Mount Sinai, New York, NY, USA.
(10)The Division of Data Driven and Digital Medicine, Department of Medicine, 
Icahn School of Medicine at Mount Sinai, New York, NY, USA.
(11)Section of Health Informatics, Department of Biostatistics, Yale School of 
Public Health, New Haven, CT.

BACKGROUND: Timely and accurate assessment of electrocardiograms (ECGs) is 
crucial for diagnosing, triaging, and clinically managing patients. Current 
workflows rely on computerized ECG interpretation tools built into ECG signal 
acquisition systems, which use rule-based algorithms that are unreliable and 
frequently not available in low-resource settings. We developed and validated a 
format-independent vision encoder-decoder model - ECG-GPT - that can generate 
free-text, expert-level interpretations directly from 12-lead ECG images.
METHODS: Using 12-lead ECGs and their corresponding diagnosis statements 
collected at the Yale-New Haven Health System (YNHHS) between 2000 and 2022, we 
developed a vision-text transformer model to generate interpretation statements 
from images of ECGs. Using structured clinical assessment, semantic similarity, 
and conventional natural language generation metrics, we validated ECG-GPT 
across 7 geographically distinct health settings. These include (1) 3 large and 
diverse US health systems, (2) consecutive ECGs from a central reading system in 
Minas Gerais, Brazil, (3) the prospective cohort study, UK Biobank, (4) a 
Germany-based, publicly available repository, PTB-XL, and (5) a community 
hospital in Missouri.
RESULTS: Overall, 2.9 million ECGs were used for model development. The model 
performed well in clinical assessment across 26 extracted labels: for atrial 
fibrillation, sinus tachycardia, sinus bradycardia, premature atrial 
contractions, and premature ventricular contractions, AUROCs and AUPRCs ranged 
from 0.80-0.95 and 0.50-0.86, respectively. For left bundle branch block, right 
bundle branch block, first degree atrioventricular block, left anterior 
fascicular block, and left posterior fascicular block, AUROCs and AUPRCs ranged 
from 0.88-0.96 and 0.23-0.86, respectively. Across all 26 conditions, diagnostic 
accuracy ranged between 0.93-0.99. ECG-GPT identified the full context of the 
diagnosis statements with allied conditions. It had a median pairwise cosine 
similarity of 0.90 (IQR 0.83-0.97), significantly greater than the median 
baseline similarity of 0.73 (IQR 0.67-0.78, p<0.001). This separation between 
median pairwise and baseline similarity remained consistent across all 26 
condition-specific subsets. The results were comparable across external 
validation sites.
CONCLUSIONS: We developed and extensively validated a vision encoder-decoder 
model that generates expert-level interpretations from ECG images. This 
represents a scalable and accessible strategy for automated ECG analysis, 
especially in low-resource settings.

DOI: 10.1101/2024.02.17.24302976
PMCID: PMC10889032
PMID: 38405776

Conflict of interest statement: Mr. Khunte, Mr. Sangha, and Dr. Khera are the 
coinventors of U.S. Provisional Patent Application No. 63/428,569. Mr. Sangha 
and Dr. Khera are the coinventors of U.S. Pending Patent Application No. 
63/346,610, and are co-founders of Ensight-AI with Dr. Krumholz. Dr. Khera is 
the coinventor of U.S. Provisional Patent Application No. 63/177,117 (unrelated 
to current work) and is a co-founder of Evidence2Health, a precision health 
platform for evidence-based care. He is also an associate editor at JAMA, and 
received support from the National Institutes of Health (under award 
R01HL167858, R01AG089981, and K23HL153775) and the Doris Duke Charitable 
Foundation (under award, 2022060). He also receives research support, through 
Yale, from Bristol-Myers Squibb, Novo Nordisk, and BridgeBio. Dr. Oikonomou 
receives support from the National Heart, Lung, and Blood Institute of the 
National Institutes of Health (under award 1F32HL170592). He is an academic 
co-founder of Evidence2Health LLC, a co-inventor in patent applications 
(US17/720,068, 63/177,117, 63/580,137, 63/606,203, WO2018078395A1, 
WO2020058713A1) and has been an ad hoc consultant for Caristo Diagnostics Ltd. 
Dr. Nadkarni is a founder of Renalytix, Pensieve, and Verici and provides 
consultancy services to AstraZeneca, Reata, Renalytix, Siemens Healthineer, and 
Variant Bio, and serves a scientific advisory board member for Renalytix and 
Pensieve. He also has equity in Renalytix, Pensieve, and Verici. Dr. Krumholz 
works under contract with the Centers for Medicare & Medicaid Services to 
support quality measurement programs, was a recipient of a research grant from 
Johnson & Johnson, through Yale University, to support clinical trial data 
sharing; was a recipient of a research agreement, through Yale University, from 
the Shenzhen Center for Health Information for work to advance intelligent 
disease prevention and health promotion; collaborates with the National Center 
for Cardiovascular Diseases in Beijing; receives payment from the Arnold & 
Porter Law Firm for work related to the Sanofi clopidogrel litigation, from the 
Martin Baughman Law Firm for work related to the Cook Celect IVC filter 
litigation, and from the Siegfried and Jensen Law Firm for work related to Vioxx 
litigation; chairs a Cardiac Scientific Advisory Board for UnitedHealth; was a 
member of the IBM Watson Health Life Sciences Board; is a member of the Advisory 
Board for Element Science, the Advisory Board for Facebook, and the Physician 
Advisory Board for Aetna; and is the co-founder of Hugo Health, a personal 
health information platform, and co-founder of Refactor Health, a healthcare 
AI-augmented data management company, and Ensight-AI, Inc. Dr. Bhatt discloses 
the following relationships - Advisory Board: Angiowave, Bayer, Boehringer 
Ingelheim, CellProthera, Cereno Scientific, Elsevier Practice Update Cardiology, 
High Enroll, Janssen, Level Ex, McKinsey, Medscape Cardiology, Merck, MyoKardia, 
NirvaMed, Novo Nordisk, PhaseBio, PLx Pharma, Stasys; Board of Directors: 
American Heart Association New York City, Angiowave (stock options), Bristol 
Myers Squibb (stock), DRS.LINQ (stock options), High Enroll (stock); Consultant: 
Broadview Ventures, GlaxoSmithKline, Hims, SFJ, Youngene; Data Monitoring 
Committees: Acesion Pharma, Assistance Publique-Hôpitaux de Paris, Baim 
Institute for Clinical Research (formerly Harvard Clinical Research Institute, 
for the PORTICO trial, funded by St. Jude Medical, now Abbott), Boston 
Scientific (Chair, PEITHO trial), Cleveland Clinic, Contego Medical (Chair, 
PERFORMANCE 2), Duke Clinical Research Institute, Mayo Clinic, Mount Sinai 
School of Medicine (for the ENVISAGE trial, funded by Daiichi Sankyo; for the 
ABILITY-DM trial, funded by Concept Medical; for ALLAY-HF, funded by Alleviant 
Medical), Novartis, Population Health Research Institute; Rutgers University 
(for the NIH-funded MINT Trial); Honoraria: American College of Cardiology 
(Senior Associate Editor, Clinical Trials and News, ACC.org; Chair, ACC 
Accreditation Oversight Committee), Arnold and Porter law firm (work related to 
Sanofi/Bristol-Myers Squibb clopidogrel litigation), Baim Institute for Clinical 
Research (formerly Harvard Clinical Research Institute; RE-DUAL PCI clinical 
trial steering committee funded by Boehringer Ingelheim; AEGIS-II executive 
committee funded by CSL Behring), Belvoir Publications (Editor in Chief, Harvard 
Heart Letter), Canadian Medical and Surgical Knowledge Translation Research 
Group (clinical trial steering committees), CSL Behring (AHA lecture), Cowen and 
Company, Duke Clinical Research Institute (clinical trial steering committees, 
including for the PRONOUNCE trial, funded by Ferring Pharmaceuticals), HMP 
Global (Editor in Chief, Journal of Invasive Cardiology), Journal of the 
American College of Cardiology (Guest Editor; Associate Editor), K2P (Co-Chair, 
interdisciplinary curriculum), Level Ex, Medtelligence/ReachMD (CME steering 
committees), MJH Life Sciences, Oakstone CME (Course Director, Comprehensive 
Review of Interventional Cardiology), Piper Sandler, Population Health Research 
Institute (for the COMPASS operations committee, publications committee, 
steering committee, and USA national co-leader, funded by Bayer), WebMD (CME 
steering committees), Wiley (steering committee); Other: Clinical Cardiology 
(Deputy Editor); Patent: Sotagliflozin (named on a patent for sotagliflozin 
assigned to Brigham and Women’s Hospital who assigned to Lexicon; neither I nor 
Brigham and Women’s Hospital receive any income from this patent); Research 
Funding: Abbott, Acesion Pharma, Afimmune, Aker Biomarine, Alnylam, Amarin, 
Amgen, AstraZeneca, Bayer, Beren, Boehringer Ingelheim, Boston Scientific, 
Bristol-Myers Squibb, Cardax, CellProthera, Cereno Scientific, Chiesi, CinCor, 
Cleerly, CSL Behring, Eisai, Ethicon, Faraday Pharmaceuticals, Ferring 
Pharmaceuticals, Forest Laboratories, Fractyl, Garmin, HLS Therapeutics, 
Idorsia, Ironwood, Ischemix, Janssen, Javelin, Lexicon, Lilly, Medtronic, Merck, 
Moderna, MyoKardia, NirvaMed, Novartis, Novo Nordisk, Otsuka, Owkin, Pfizer, 
PhaseBio, PLx Pharma, Recardio, Regeneron, Reid Hoffman Foundation, Roche, 
Sanofi, Stasys, Synaptic, The Medicines Company, Youngene, 89Bio; Royalties: 
Elsevier (Editor, Braunwald’s Heart Disease); Site Co-Investigator: Abbott, 
Biotronik, Boston Scientific, CSI, Endotronix, St. Jude Medical (now Abbott), 
Philips, SpectraWAVE, Svelte, Vascular Solutions; Trustee: American College of 
Cardiology; Unfunded Research: FlowCo. All other authors declare no relevant 
competing interests.


534. Aesthetic Plast Surg. 2025 Feb;49(3):1011-1012. doi: 10.1007/s00266-023-03805-1. 
Epub 2023 Dec 28.

Dr. GAI: Significance of Generative AI in Plastic Surgery.

Ray PP(1).

Author information:
(1)Department of Computer Applications, Sikkim University, 6th Mile, PO-Tadong, 
Gangtok, Sikkim, 737102, India. ppray@cus.ac.in.

In this letter to the editor, I offer a critique of the article titled 
"Consulting the Digital Doctor: Google Versus ChatGPT as Sources of Information 
on Breast Implant-Associated Anaplastic Large Cell Lymphoma and Breast Implant 
Illness." While acknowledging the authors' pioneering effort to compare 
informational outputs from Google and a generative AI (GAI)-ChatGPT, I raise 
concerns about the methodology, lack of rigorous validation, potential biases, 
and the overstatement of findings. The letter suggests that the authors' 
conclusions about the superiority of ChatGPT in providing high-quality medical 
information may be premature, given the limitations of the study design and the 
evolving nature of artificial intelligence (AI) technology.No Level Assigned 
This journal requires that authors assign a level of evidence to each submission 
to which Evidence-Based Medicine rankings are applicable. This excludes Review 
Articles, Book Reviews, and manuscripts that concern Basic Science, Animal 
Studies, Cadaver Studies, and Experimental Studies. For a full description of 
these Evidence-Based Medicine ratings, please refer to the Table of Contents or 
the online Instructions to Authors www.springer.com/00266.

© 2023. Springer Science+Business Media, LLC, part of Springer Nature and 
International Society of Aesthetic Plastic Surgery.

DOI: 10.1007/s00266-023-03805-1
PMID: 38155290 [Indexed for MEDLINE]

Conflict of interest statement: Declarations. Conflict of interest: The author 
declares that he has no conflicts of interest to disclose. Ethical Approval and 
Consent to Participate: Not applicable. Informed Consent: For this type of study 
informed consent is not required. Statement of Human and Animal Rights: This 
article does not contain any studies with human participants or animals 
performed by any of the authors.